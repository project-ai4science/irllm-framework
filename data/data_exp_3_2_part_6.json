[
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Auto-Encoding Variational Bayes",
    "start_abstract":"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
      ],
      "abstract":[
        "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
      ],
      "categories":[
        "physics.gen-ph"
      ]
    },
    "list":{
      "title":[
        "Introduction of the G$_2$-Ricci Flow: Geometric Implications for\n  Spontaneous Symmetry Breaking and Gauge Boson Masses",
        "Preons, Braid Topology, and Representations of Fundamental Particles",
        "Reversible Imprinting and Retrieval of Quantum Information: Experimental\n  Verification of the Quantum Memory Matrix Hypothesis",
        "A New Method for Calculating the Energies Associated with Particle\n  Reactions",
        "New entropy, thermodynamics of apparent horizon and cosmology",
        "Cylindrical gravastars with Kuchowicz metric potential",
        "Exploring quaternion framework for subluminal to superluminal space\n  transformations in particle dynamics",
        "Ten Equations that Shook the Quantum World: Bose-Einstein Condensation,\n  Superfluidity, and the Quantum-Classical Transition",
        "Scattering problem for the valence electron model potential",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Impossible Galaxies, the Hubble Tension and the Ricci soliton miracle,",
        "What Exactly is Antimatter (Gravitationally Speaking)? A Second Scenario",
        "Local Quantum Mechanical Prediction of the Singlet State",
        "Isometric Gelfand transforms of complete Nevanlinna-Pick spaces",
        "On extensivity of morphisms",
        "On Branch-and-Price for Project Scheduling",
        "Extreme Shape Coexistence Observed in $^{70}$Co",
        "Learning Memory and Material Dependent Constitutive Laws",
        "Exploring the Technology Landscape through Topic Modeling, Expert\n  Involvement, and Reinforcement Learning",
        "Models for the Eremenko-Lyubich class",
        "Subcode Ensemble Decoding of Linear Block Codes",
        "How does non-metricity affect particle creation and evaporation in\n  bumblebee gravity?",
        "A mesh-free hybrid Chebyshev-Tucker tensor format with applications to\n  multi-particle modelling",
        "Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric\n  Properties of Generated Sea Route Graphs",
        "Complex potential and open system applications in heavy-ions and cold\n  atoms",
        "A Pristine-UNIONS view on the Galaxy: Kinematics of the distant spur\n  feature of the Sagittarius stream traced by Blue Horizontal Branch stars",
        "Implementation and verification of coherent error suppression using\n  randomized compiling for Grover's algorithm on a trapped-ion device",
        "Dynkin Systems and the One-Point Geometry"
      ],
      "abstract":[
        "This work introduces the G$_2$-Ricci flow on seven-dimensional manifolds with\nnon-zero torsion and explores its physical implications. By extending the Ricci\nflow to manifolds with G$_2$ structures, we study the evolution of solitonic\nsolutions and their role in spontaneous symmetry breaking in gauge theories. In\nparticular, this model proposes that the masses of the W and Z bosons are\ndetermined not by an external scalar field, as in the Higgs mechanism, but by\nthe intrinsic geometric torsion of the manifold. Furthermore, a possible\nconnection between the geometry of extra dimensions and the curvature of our\nspacetime is explored, with implications for the experimentally observed\npositive cosmological constant. This approach provides an innovative\ninterpretation of fundamental interactions in theoretical physics, opening new\npossibilities for studying extra dimensions and the geometry of\nG$_2$-manifolds.",
        "In particle phenomenology, preon models study compositional rules of standard\nmodel interactions. In spite of empirical success, mathematical underpinnings\nof preon models in terms of group representation theory have not been fully\nworked out. Here, we address this issue while clarifying the relation between\ndifferent preon models. In particular, we focus on two prominent models:\nBilson-Thompson's helon model, and Lambek's 4-vector model. We determine the\nmapping between helon model particle states and representation theory of Lie\nalgebras. Braided ribbon diagrams of the former represent on-shell states of\nspinors of the Lorentz group. Braids correspond to chirality, and twists, to\ncharges. We note that this model captures only the $SU(3)_c\\times U(1)_{em}$\nsector of the standard model. We then map the twists of helon diagrams to the\nweight polytope of $SU(3)_c \\times U(1)_{em}$. The braid structure maps to\nchiral states of fermions. We also show that Lambek's 4-vector can be recovered\nfrom helon diagrams. Alongside, we introduce a new 5-vector representation\nderived from the weight lattice. This representation contains both, the correct\ninteractions found in 4-vectors and the inclusion of chirality found in helons.\nAdditionally, we demonstrate topological analogues of CPT transformations in\nhelon diagrams. Interestingly, the braid diagrams of the helon model are the\nonly ones that are self-consistent with CPT invariance. In contrast to\nfield-theoretic approaches, the compositional character of preon models offers\nan analogous particle-centric perspective on fundamental interactions.",
        "We present a series of quantum computing experiments designed to test a\ncentral prediction of the Quantum Memory Matrix (QMM) hypothesis - that quantum\ninformation can be locally stored in finite-dimensional cells of space-time and\nlater retrieved in a fully unitary and reversible manner. Our work encompasses\nfive distinct experiments: a basic three-qubit imprint-retrieval cycle, an\nextended five-qubit model implementing two parallel cycles, and variations\nincorporating dynamic evolution and controlled error injection. In each case, a\nfield qubit is prepared in an arbitrary superposition, and its state is\nimprinted onto memory qubit(s) via controlled-R_y gates, with subsequent\ncontrolled-SWAP operations retrieving the stored information into output\nqubit(s). Execution on an IBM Quantum Processing Unit using the Qiskit Runtime\nservice yielded significant correlations between the initially prepared field\nstates and the retrieved outputs, with fidelities that, while subject to\nhardware noise and decoherence, consistently demonstrate the reversible and\nunitary nature of the process. These results not only confirm the basic\nimprint-retrieval cycle as predicted by the QMM hypothesis but also establish a\nscalable experimental methodology that may ultimately contribute to resolving\nchallenges such as the black hole information paradox and advancing our\nunderstanding of quantum gravity.",
        "A new method in which the energy and mass of elementary particles can be\ncalculated is presented. Gluon gluon interactions within a single elementary\nparticle are considered, and the number of possible interactions per particle\nis determined. This procedure can be formalized and standardized via a newly\nintroduced microcanonical partition function. The possibility of calculating\nthe energetic relationships provides new and more in-depth insights into the\nreaction possibilities of the particles. In addition, the application of this\nmethod of calculating the partition function to the known quarks themselves\nsuggests that they are composed of even more elementary particles. The\nproperties of these particles match the Rishons of the Harari Shupe preon\nmodel. In combination with the Rishon model, the method presented here for\ncalculating the energetic situation of particle reactions provides a profound\nand new understanding of the processes at an absolute elementary level; this\nopens new possibilities for the calculation and understanding of particle\nreactions and might change our understanding of particle physics in fundamental\nways.",
        "Here, we consider new nonadditive entropy of the apparent horizon\n$S_K=S_{BH}\/(1+\\gamma S_{BH})$ with $S_{BH}$ being the Bekenstein--Hawking\nentropy. This is an alternative of the R\\'{e}nyi and Tsallis entropies, that\nallow us by utilizing the holographic principle to develop of a new model of\nholographic dark energy. When $\\gamma\\rightarrow 0$ our entropy becomes the\nBekenstein--Hawking entropy $S_{BH}$. The generalized Friedmann's equations for\nFriedmann--Lema\\^{i}tre--Robertson--Walker (FLRW) space-time for the barotropic\nmatter fluid with $p=w\\rho$ were obtained. We compute the dark energy pressure\n$p_D$, density energy $\\rho_D$ and the deceleration parameter corresponding to\nour model. From the second modified Friedmann's equation a dynamical\ncosmological constant was obtained. We show that at some model parameters $w$\nand $\\beta$ there are two phases, universe acceleration and deceleration or the\nphase of the eternal inflation. Thus, our model, by virtue of the holographic\nprinciple, can describe the universe inflation and late time of the universe\nacceleration. The holographic dark energy model with the generalized entropy of\nthe apparent horizon can be of interest for new cosmology.",
        "Mazur and Mottola's gravastar model represents one of the few serious\nalternatives to the traditional understanding of the black hole. The gravastar\nis typically regarded as a theoretical alternative for the black hole. This\narticle investiagtes the creation of gravastar(gravitational vacuum star)\nwithin the realm of cylindrically symmetric space-time utilizing the Kuchowicz\nmetric potential. A stable gravastar comprises of three distinct regions,\nstarting with an interior region marked by positive energy density and negative\npressure $(p=-\\rho)$ which is followed by an intermediate thin shell, where the\ninterior negative pressure induces a outward repulsive force at each point on\nthe shell. Ultra-relativistic stiff fluid makes up the thin shell governed by\nthe equation of state(EoS) $(p=\\rho)$, which meets the Zel'dovich criteria. And\nthen comes the region exterior to it which is total vacuum. In this scenario,\nthe central singularity is eliminated and the event horizon is effectively\nsubstituted by the thin bounding shell. Employing the Kuchowicz metric\npotential we have derived the remaining metric functions for the interior\nregion and the shell regions yielding a non-singular solution for both the\nregions. Additionally, we have investigated various characteristics of this\nshell region including its proper shell length, the energy content and entropy.\nThis theoretical model successfully resolves the singularity issue inherent to\nthe black holes. Therefore, this gravastar model presents a viable alternative\nto the traditional black holes, reconciled within the context of Einstein's\ntheory of General Relativity.",
        "The present study explores the behavior of quaternionic four-space algebra\nfor subluminal and superluminal spaces. We formulate the generalized Lorentz\ntransformations for quaternionic subluminal, superluminal, and their combined\nMinkowski spaces. Furthermore, we have studied the relativistic phenomenon of\nquaternionic length contraction, time dilation, velocity addition, and the\nDoppler effect for the combination of subluminal and superluminal space. We\nclaim that the transformation between two superluminal spaces is ultimately a\nsubluminal space; the tachyonic behavior reveals itself in the consequences\nwhen the two frames are in different spaces (i.e., generalized\nsubluminal-superluminal spaces).",
        "The transition from the quantum to the classical world and its relation to\nBose-Einstein condensation and superfluidity is explained in ten equations.",
        "In the paper, in the scattering problem for the valence electron model\npotential a self-adjoint extension is performed and Rutherford formula is\nmodified. The scattering of slow particles for this potential is also discussed\nand the changes caused by the self-adjoint extension in the differential and\nintegral cross-sections of the scattering are studied.",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "The standard model of cosmology has begun to show signs of internal\ninconsistencies under the relentless onslaught of precision data from the James\nWebb Telescope (JWST), the Hubble Telescope (HST) and other space-based\nobservation platforms as well as those from the ground. The most statistically\nsignificant tension, the so-called Hubble Tension currently stands at between 4\n{\\sigma} to 6 {\\sigma} statistical significance. The JWST high resolution\nimages reveal mature and sometimes quenched galaxies at high redshifts (z>6)\nwhich standard cosmological models have failed to both predict and explain.\nThis is the impossible galaxies problem. These short comings of the standard\nmodel of cosmology have led to a plethora of new models attempting to explain\nthe inconsistencies between theory and observation. Thus far, none of them\nadequately address these tensions. In this article, we demonstrate that by\nincluding a Ricci soliton into Einstein's field equations both tensions can be\nadequately resolved. The Ricci soliton is sourced from gravitational energy.",
        "In arXiv:2401.10954 I investigated the consequences of regarding the\nmass-energy of the fundamental fermions (quarks and leptons) and the\nIntermediate Vector Bosons (e.g., photon) as matter, and the fundamental\nantifermions (antiquarks and antileptons) as antimatter within the context of\nan antigravity universe, one where matter and antimatter repel gravitationally.\nHere I consider an alternative scenario in which the Intermediate Vector\nBosons, which are neither particle nor antiparticle, are gravitationally\nattracted to both fundamental fermions and antifermions. This leads to a\nprediction for the free-fall acceleration of antihydrogen of\n$a_{\\bar{H}}=(0.78^{+0.11}_{-0.08})g$ (and most certainly less than $g$) as\nwell as quite different expectations for the free-fall accelerations of the\n$\\mu^+$ and positronium from those derived in arXiv:2401.10954. The cosmology\nwhich results from the premise presented here is little different from the\nstandard cosmology (i.e., the $\\Lambda$CDM model). One significant deviation is\nthat there would be an increased accelerated expansion in the early moments\nafter the Big Bang due to the gravitational repulsion between the fundamental\nfermions and antifermions.",
        "We deduce the quantum mechanical prediction of $-{\\bf a}\\cdot{\\bf b}$ for the\nsinglet spin state employing local measurement functions following Bell's\napproach. Our derivation is corroborated through a computational simulation\nconducted via the Mathematica programming environment.",
        "We show that any complete Nevanlinna-Pick space whose multiplier algebra has\nisometric Gelfand transform (or commutative C*-envelope) is essentially the\nHardy space on the open unit disk.",
        "Extensivity of a category may be described as a property of coproducts in the\ncategory, namely, that they are disjoint and universal. An alternative\nviewpoint is that it is a property of morphisms in a category. This paper\nexplores this point of view through a natural notion of extensive and\ncoextensive morphism. Through these notions, topics in universal algebra, such\nas the strict refinement and Fraser-Horn properties, take categorical form and\nthereby enjoy the benefits of categorical generalisation. On the other hand,\nthe universal algebraic theory surrounding these topics inspire categorical\nresults. One such result we establish in this paper is that a Barr-exact\ncategory is coextensive if and only if every split monomorphism in the category\nis coextensive.",
        "Integer programs for resource-constrained project scheduling problems are\nnotoriously hard to solve due to their weak linear relaxations. Several papers\nhave proposed reformulating project scheduling problems via Dantzig-Wolfe\ndecomposition to strengthen their linear relaxation and decompose large problem\ninstances. The reformulation gives rise to a master problem that has a large\nnumber of variables. Therefore, the master problem is solved by a column\ngeneration procedure embedded in a branching framework, also known as\nbranch-and-price. While branch-and-price has been successfully applied to many\nproblem classes, it turns out to be ineffective for most project scheduling\nproblems. This paper identifies drivers of the ineffectiveness by analyzing the\nstructure of the reformulated problem and the strength of different branching\nschemes. Our analysis shows that the reformulated problem has an unfavorable\nstructure for column generation: It is highly degenerate, slowing down the\nconvergence of column generation, and for many project scheduling problems, it\nyields the same or only slightly stronger linear relaxations as classical\nformulations at the expense of large increases in runtime. Our computational\nexperiments complement our theoretical findings.",
        "The shape of the atomic nucleus is a property which underpins our\nunderstanding of nuclear systems, impacts the limits of nuclear existence, and\nenables probes of physics beyond the Standard Model. Nuclei can adopt a variety\nof shapes, including spheres, axially deformed spheroids, and pear shapes. In\nsome regions of the nuclear chart where a spherical nucleus would naively be\nexpected, deformed nuclear states can result from collective action of\nconstituent protons and neutrons. In a small subset of nuclei both spherical\nand deformed nuclear states have been experimentally observed, a phenomenon\ntermed shape coexistence. We present spectroscopic evidence for the coexistence\nof $J^{\\pi}=1+$ spherical and deformed states in $^{70}$Co, separated by less\nthan 275~keV. This close degeneracy of levels with the same $J^{\\pi}$ and\ndifferent shapes demonstrates an extreme example of shape coexistence resulting\nfrom the interplay of independent particle motion and collective behavior in\nhighly unstable nuclear systems and identifies the Co isotopes as a transition\npoint between deformed ground states observed in the Cr isotopes and spherical\nconfigurations observed in the closed-shell Ni isotopes.",
        "The theory of homogenization provides a systematic approach to the derivation\nof macroscale constitutive laws, obviating the need to repeatedly resolve\ncomplex microstructure. However, the unit cell problem that defines the\nconstitutive model is typically not amenable to explicit evaluation. It is\ntherefore of interest to learn constitutive models from data generated by the\nunit cell problem. Many viscoelastic and elastoviscoplastic materials are\ncharacterized by memory-dependent constitutive laws. In order to amortize the\ncomputational investment in finding such memory-dependent constitutive laws, it\nis desirable to learn their dependence on the material microstructure. While\nprior work has addressed learning memory dependence and material dependence\nseparately, their joint learning has not been considered. This paper focuses on\nthe joint learning problem and proposes a novel neural operator framework to\naddress it.\n  In order to provide firm foundations, the homogenization problem for linear\nKelvin-Voigt viscoelastic materials is studied. The theoretical properties of\nthe cell problem in this Kelvin-Voigt setting are used to motivate the proposed\ngeneral neural operator framework; these theoretical properties are also used\nto prove a universal approximation theorem for the learned macroscale\nconstitutive model. This formulation of learnable constitutive models is then\ndeployed beyond the Kelvin-Voigt setting. Numerical experiments are presented\nshowing that the resulting data-driven methodology accurately learns history-\nand microstructure-dependent linear viscoelastic and nonlinear\nelastoviscoplastic constitutive models, and numerical results also demonstrate\nthat the resulting constitutive models can be deployed in macroscale simulation\nof material deformation.",
        "In today's rapidly evolving technological landscape, organizations face the\nchallenge of integrating external insights into their decision-making processes\nto stay competitive. To address this issue, this study proposes a method that\ncombines topic modeling, expert knowledge inputs, and reinforcement learning\n(RL) to enhance the detection of technological changes. The method has four\nmain steps: (1) Build a relevant topic model, starting with textual data like\ndocuments and reports to find key themes. (2) Create aspect-based topic models.\nExperts use curated keywords to build models that showcase key domain-specific\naspects. (3) Iterative analysis and RL driven refinement: We examine metrics\nsuch as topic magnitude, similarity, entropy shifts, and how models change over\ntime. We optimize topic selection with RL. Our reward function balances the\ndiversity and similarity of the topics. (4) Synthesis and operational\nintegration: Each iteration provides insights. In the final phase, the experts\ncheck these insights and reach new conclusions. These conclusions are designed\nfor use in the firm's operational processes. The application is tested by\nforecasting trends in quantum communication. Results demonstrate the method's\neffectiveness in identifying, ranking, and tracking trends that align with\nexpert input, providing a robust tool for exploring evolving technological\nlandscapes. This research offers a scalable and adaptive solution for\norganizations to make informed strategic decisions in dynamic environments.",
        "If $f$ is in the Eremenko-Lyubich class (transcendental entire functions with\nbounded singular set) then $\\Omega= \\{ z: |f(z)| > R\\}$ and $f|_\\Omega$ must\nsatisfy certain simple topological conditions when $R$ is sufficiently large. A\nmodel $(\\Omega, F)$ is an open set $\\Omega$ and a holomorphic function $F$ on\n$\\Omega$ that satisfy these same conditions. We show that any model can be\napproximated by an Eremenko-Lyubich function in a precise sense. In many cases,\nthis allows the construction of functions in the Eremenko-Lyubich with a\ndesired property to be reduced to the construction of a model with that\nproperty, and this is often much easier to do.",
        "Low-density parity-check (LDPC) codes together with belief propagation (BP)\ndecoding yield exceptional error correction capabilities in the large block\nlength regime. Yet, there remains a gap between BP decoding and maximum\nlikelihood decoding for short block length LDPC codes. In this context,\nensemble decoding schemes yield both reduced latency and good error rates. In\nthis paper, we propose subcode ensemble decoding (SCED), which employs an\nensemble of decodings on different subcodes of the code. To ensure that all\ncodewords are decodable, we use the concept of linear coverings and explore\napproaches for sampling suitable ensembles for short block length LDPC codes.\nMonte-Carlo simulations conducted for three LDPC codes demonstrate that SCED\nimproves decoding performance compared to stand-alone decoding and automorphism\nensemble decoding. In particular, in contrast to existing schemes, e.g.,\nmultiple bases belief propagation and automorphism ensemble decoding, SCED does\nnot require the NP-complete search for low-weight dual codewords or knowledge\nof the automorphism group of the code, which is often unknown.",
        "In this work, we analyze the impact of non-metricity on particle creation and\nthe evaporation process of black holes within the framework of bumblebee\ngravity. In general lines, we compare black holes in the metric formalism [1]\nand the metric-affine approach [2]. Initially, we focus on bosonic particle\nmodes to investigate Hawking radiation. Using the Klein-Gordon equation, we\ncompute the Bogoliubov coefficients and derive the Hawking temperature.\nSubsequently, we examine Hawking radiation as a tunneling process, resolving\ndivergent integrals through the residue method. The analysis is then extended\nto fermionic particle modes, also within the tunneling framework. Particle\ncreation densities are calculated for both bosonic and fermionic cases.\nAdditionally, greybody bounds are estimated for bosonic and fermionic\nparticles. Finally, we explore the evaporation process, considering the final\nstate of the black holes. In a general panorama, non-metricity in bumblebee\ngravity raises particle density for bosons while reducing it for fermions,\nincreases greybody factors (for both bosons and fermions), amplifies the\nemission rate, and accelerates the evaporation process.",
        "In this paper, we introduce a mesh-free two-level hybrid Tucker tensor format\nfor approximation of multivariate functions, which combines the product\nChebyshev interpolation with the ALS-based Tucker decomposition of the tensor\nof Chebyshev coefficients. It allows to avoid the expenses of the\nrank-structured approximation of function-related tensors defined on large\nspacial grids, while benefiting from the Tucker decomposition of the rather\nsmall core tensor of Chebyshev coefficients. This leads to nearly optimal\nTucker rank parameters which are close to the results for well established\nTucker-ALS algorithm applied to the large grid-based tensors. These rank\nparameters inherited from the Tucker-ALS decomposition of the coefficient\ntensor can be much less than the polynomial degrees of the initial Chebyshev\ninterpolant via function independent basis set. Furthermore, the tensor product\nChebyshev polynomials discretized on a tensor grid leads to a low-rank\ntwo-level orthogonal algebraic Tucker tensor that approximates the initial\nfunction with controllable accuracy. It is shown that our techniques could be\ngainfully applied to the long-range part of the electrostatic potential of\nmulti-particle systems approximated in the range-separated tensor format. Error\nand complexity estimates of the proposed methods are presented. We demonstrate\nthe efficiency of the suggested method numerically on examples of the\nlong-range components of multi-particle interaction potentials generated by 3D\nNewton kernel for large bio-molecule systems and lattice-type compounds.",
        "The demand for artificially generated data for the development, training and\ntesting of new algorithms is omnipresent. Quantum computing (QC), does offer\nthe hope that its inherent probabilistic functionality can be utilised in this\nfield of generative artificial intelligence. In this study, we use\nquantum-classical hybrid generative adversarial networks (QuGANs) to\nartificially generate graphs of shipping routes. We create a training dataset\nbased on real shipping data and investigate to what extent QuGANs are able to\nlearn and reproduce inherent distributions and geometric features of this data.\nWe compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),\nwith a special focus on their parameter efficiency. Our results indicate that\nQuGANs are indeed able to quickly learn and represent underlying geometric\nproperties and distributions, although they seem to have difficulties in\nintroducing variance into the sampled data. Compared to classical GANs of\ngreater size, measured in the number of parameters used, some QuGANs show\nsimilar result quality. Our reference to concrete use cases, such as the\ngeneration of shipping data, provides an illustrative example and demonstrate\nthe potential and diversity in which QC can be used.",
        "Since the discovery of the complex potential of quarkonium at high\ntemperatures, quarkonium has been regarded as an open quantum system in the\nquark-gluon plasma. Recently, a similar issue regarding in-medium bound states\nof impurities has also emerged in particle physics and cold atomic physics. We\nwill provide an overview of recent advancements in understanding key quantities\nsuch as complex potential and transport coefficients for heavy impurities in\nfinite temperature QCD and cold atomic systems.",
        "Providing a detailed picture of the Sagittarius (Sgr) stream offers important\nconstraints on the build-up of the Galactic halo as well as its gravitational\npotential at large radii. While several attempts have been made to model the\nstructure of the Sgr stream, no model has yet been able to match all the\nfeatures observed for the stream. Moreover, for several of these features,\nobservational characterisation of their properties is rather limited,\nparticularly at large distances. The aim of this work is to investigate the\nkinematics of the Sgr stream outermost spur feature using blue horizontal\nbranch (BHB) stars. Candidate BHB stars were selected by combining two\napproaches; one capitalising on Pan-STARRS1 3$\\Pi$ griz and u photometry taken\nas part of UNIONS, the other using Pristine Survey CaHK and SDSS ugr\nphotometry. Follow-up optical spectra are obtained using ESO\/VLT\/FORS2 to\nconfirm their BHB nature and obtain line-of-sight (LOS) velocities. Of our 25\ncandidates, 20 stars can be confirmed as bona fide BHB stars. Their LOS\nvelocities, together with the 3D positions of these stars qualitatively match\nwell with Sgr model predictions and trace the outer apocentre of the trailing\narm and its spur feature very nicely. The quantitative offsets that are found\nbetween our data and the different models can be used to provide information\nabout the Galactic gravitational potential at large distances. We present a\nfirst, tentative, analysis in this direction, showing that the model of\nVasiliev et al. (2021) would provide better agreement with our observations if\nthe enclosed mass of the Milky Way within 100 kpc were lowered to\n$(5.3\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$ (versus\n$(5.6\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$). Our selection of BHB stars\nprovides a new view on the outermost structure in 3D positions and LOS\nvelocities of the Sgr debris.",
        "In near-term quantum computations that do not employ error correction, noise\ncan proliferate rapidly, corrupting the quantum state and making results\nunreliable. These errors originate from both decoherence and control\nimprecision. The latter can manifest as coherent noise that is especially\ndetrimental. Here, we study the impact of coherent errors and their mitigation\nunder standard error-reduction techniques, both theoretically and\nexperimentally on a trapped-ion quantum computer. As a representative case\nstudy, we implement a range of Grover's algorithm circuits containing up to 10\nqubits and 26 two-qubit gates. We demonstrate the effectiveness of randomized\ncompiling (RC) and algorithm error detection (ED), where the latter is realized\nvia post-selection on ancillary qubits that ideally return to the ground state\nat the end of each circuit. Our results highlight a synergetic effect:\ncombining RC and ED yields the largest reductions in errors, indicating that\nthese methods can work together to extend the capabilities of near-term quantum\ndevices for moderately deep circuits.",
        "In this note I demonstrate that the collection of Dynkin systems on finite\nsets assembles into a Connes-Consani $\\mathbb{F}_1$-module, with the collection\nof partitions of finite sets as a sub-module. The underlying simplicial set of\nthis $\\mathbb{F}_1$-module is shown to be isomorphic to the delooping of the\nKrasner hyperfield $\\mathbb{K}$, where $1+1=\\{0,1\\}$. The face and degeneracy\nmaps of the underlying simplicial set of the $\\mathbb{F}_1$-module of\npartitions correspond to merging partition blocks and introducing singleton\nblocks, respectively. I also show that the $\\mathbb{F}_1$-module of partitions\ncannot correspond to a set with a binary operation (even partially defined or\nmultivalued) under the ``Eilenberg-MacLane'' embedding. These results imply\nthat the $n$-fold sum of the Dynkin $\\mathbb{F}_1$-module with itself is\nisomorphic to the $\\mathbb{F}_1$-module of the discrete projective geometry on\n$n$ points."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Three-dimensional nanomagnetism",
    "start_abstract":"Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities.",
    "start_categories":[
      "cond-mat.mes-hall"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
      ],
      "abstract":[
        "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "BANet: Bilateral Aggregation Network for Mobile Stereo Matching",
        "SNN-Driven Multimodal Human Action Recognition via Event Camera and\n  Skeleton Data Fusion",
        "Tracking Mouse from Incomplete Body-Part Observations and Deep-Learned\n  Deformable-Mouse Model Motion-Track Constraint for Behavior Analysis",
        "GCP: Guarded Collaborative Perception with Spatial-Temporal Aware\n  Malicious Agent Detection",
        "MarkushGrapher: Joint Visual and Textual Recognition of Markush\n  Structures",
        "Generative Human Geometry Distribution",
        "High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion",
        "Online Dense Point Tracking with Streaming Memory",
        "UniVG: A Generalist Diffusion Model for Unified Image Generation and\n  Editing",
        "MaterialMVP: Illumination-Invariant Material Generation via Multi-view\n  PBR Diffusion",
        "Debiased Prompt Tuning in Vision-Language Model without Annotations",
        "Exploring Transferable Homogeneous Groups for Compositional Zero-Shot\n  Learning",
        "Dynamic watermarks in images generated by diffusion models",
        "The Factorizable Feigin-Frenkel center",
        "A BERT Based Hybrid Recommendation System For Academic Collaboration",
        "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning Abilities",
        "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks",
        "Quantum Feature-Empowered Deep Classification for Fast Mangrove Mapping",
        "Antenna Position and Beamforming Optimization for Movable Antenna\n  Enabled ISAC: Optimal Solutions and Efficient Algorithms",
        "Do Unit Proofs Work? An Empirical Study of Compositional Bounded Model\n  Checking for Memory Safety Verification",
        "A New Statistical Approach to the Performance Analysis of Vision-based\n  Localization",
        "PAID: A Framework of Product-Centric Advertising Image Design",
        "How much should we care about what others know? Jump signals in optimal\n  investment under relative performance concerns",
        "Optimal PMU Placement for Kalman Filtering of DAE Power System Models",
        "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama\n  Generation",
        "Bias Analysis of Experiments for Multi-Item Multi-Period Inventory\n  Control Policies",
        "Robust Phantom-Assisted Framework for Multi-Person Localization and\n  Vital Signs Monitoring Using MIMO FMCW Radar",
        "On the existence of twisted Shalika periods: the Archimedean case"
      ],
      "abstract":[
        "State-of-the-art stereo matching methods typically use costly 3D convolutions\nto aggregate a full cost volume, but their computational demands make mobile\ndeployment challenging. Directly applying 2D convolutions for cost aggregation\noften results in edge blurring, detail loss, and mismatches in textureless\nregions. Some complex operations, like deformable convolutions and iterative\nwarping, can partially alleviate this issue; however, they are not\nmobile-friendly, limiting their deployment on mobile devices. In this paper, we\npresent a novel bilateral aggregation network (BANet) for mobile stereo\nmatching that produces high-quality results with sharp edges and fine details\nusing only 2D convolutions. Specifically, we first separate the full cost\nvolume into detailed and smooth volumes using a spatial attention map, then\nperform detailed and smooth aggregations accordingly, ultimately fusing both to\nobtain the final disparity map. Additionally, to accurately identify\nhigh-frequency detailed regions and low-frequency smooth\/textureless regions,\nwe propose a new scale-aware spatial attention module. Experimental results\ndemonstrate that our BANet-2D significantly outperforms other mobile-friendly\nmethods, achieving 35.3\\% higher accuracy on the KITTI 2015 leaderboard than\nMobileStereoNet-2D, with faster runtime on mobile devices. The extended 3D\nversion, BANet-3D, achieves the highest accuracy among all real-time methods on\nhigh-end GPUs. Code: \\textcolor{magenta}{https:\/\/github.com\/gangweiX\/BANet}.",
        "Multimodal human action recognition based on RGB and skeleton data fusion,\nwhile effective, is constrained by significant limitations such as high\ncomputational complexity, excessive memory consumption, and substantial energy\ndemands, particularly when implemented with Artificial Neural Networks (ANN).\nThese limitations restrict its applicability in resource-constrained scenarios.\nTo address these challenges, we propose a novel Spiking Neural Network\n(SNN)-driven framework for multimodal human action recognition, utilizing event\ncamera and skeleton data. Our framework is centered on two key innovations: (1)\na novel multimodal SNN architecture that employs distinct backbone networks for\neach modality-an SNN-based Mamba for event camera data and a Spiking Graph\nConvolutional Network (SGN) for skeleton data-combined with a spiking semantic\nextraction module to capture deep semantic representations; and (2) a\npioneering SNN-based discretized information bottleneck mechanism for modality\nfusion, which effectively balances the preservation of modality-specific\nsemantics with efficient information compression. To validate our approach, we\npropose a novel method for constructing a multimodal dataset that integrates\nevent camera and skeleton data, enabling comprehensive evaluation. Extensive\nexperiments demonstrate that our method achieves superior performance in both\nrecognition accuracy and energy efficiency, offering a promising solution for\npractical applications.",
        "Tracking mouse body parts in video is often incomplete due to occlusions such\nthat - e.g. - subsequent action and behavior analysis is impeded. In this\nconceptual work, videos from several perspectives are integrated via global\nexterior camera orientation; body part positions are estimated by 3D\ntriangulation and bundle adjustment. Consistency of overall 3D track\nreconstruction is achieved by introduction of a 3D mouse model, deep-learned\nbody part movements, and global motion-track smoothness constraint. The\nresulting 3D body and body part track estimates are substantially more complete\nthan the original single-frame-based body part detection, therefore, allowing\nimproved animal behavior analysis.",
        "Collaborative perception significantly enhances autonomous driving safety by\nextending each vehicle's perception range through message sharing among\nconnected and autonomous vehicles. Unfortunately, it is also vulnerable to\nadversarial message attacks from malicious agents, resulting in severe\nperformance degradation. While existing defenses employ\nhypothesis-and-verification frameworks to detect malicious agents based on\nsingle-shot outliers, they overlook temporal message correlations, which can be\ncircumvented by subtle yet harmful perturbations in model input and output\nspaces. This paper reveals a novel blind area confusion (BAC) attack that\ncompromises existing single-shot outlier-based detection methods. As a\ncountermeasure, we propose GCP, a Guarded Collaborative Perception framework\nbased on spatial-temporal aware malicious agent detection, which maintains\nsingle-shot spatial consistency through a confidence-scaled spatial concordance\nloss, while simultaneously examining temporal anomalies by reconstructing\nhistorical bird's eye view motion flows in low-confidence regions. We also\nemploy a joint spatial-temporal Benjamini-Hochberg test to synthesize\ndual-domain anomaly results for reliable malicious agent detection. Extensive\nexperiments demonstrate GCP's superior performance under diverse attack\nscenarios, achieving up to 34.69% improvements in AP@0.5 compared to the\nstate-of-the-art CP defense strategies under BAC attacks, while maintaining\nconsistent 5-8% improvements under other typical attacks. Code will be released\nat https:\/\/github.com\/CP-Security\/GCP.git.",
        "The automated analysis of chemical literature holds promise to accelerate\ndiscovery in fields such as material science and drug development. In\nparticular, search capabilities for chemical structures and Markush structures\n(chemical structure templates) within patent documents are valuable, e.g., for\nprior-art search. Advancements have been made in the automatic extraction of\nchemical structures from text and images, yet the Markush structures remain\nlargely unexplored due to their complex multi-modal nature. In this work, we\npresent MarkushGrapher, a multi-modal approach for recognizing Markush\nstructures in documents. Our method jointly encodes text, image, and layout\ninformation through a Vision-Text-Layout encoder and an Optical Chemical\nStructure Recognition vision encoder. These representations are merged and used\nto auto-regressively generate a sequential graph representation of the Markush\nstructure along with a table defining its variable groups. To overcome the lack\nof real-world training data, we propose a synthetic data generation pipeline\nthat produces a wide range of realistic Markush structures. Additionally, we\npresent M2S, the first annotated benchmark of real-world Markush structures, to\nadvance research on this challenging task. Extensive experiments demonstrate\nthat our approach outperforms state-of-the-art chemistry-specific and\ngeneral-purpose vision-language models in most evaluation settings. Code,\nmodels, and datasets will be available.",
        "Realistic human geometry generation is an important yet challenging task,\nrequiring both the preservation of fine clothing details and the accurate\nmodeling of clothing-pose interactions. Geometry distributions, which can model\nthe geometry of a single human as a distribution, provide a promising\nrepresentation for high-fidelity synthesis. However, applying geometry\ndistributions for human generation requires learning a dataset-level\ndistribution over numerous individual geometry distributions. To address the\nresulting challenges, we propose a novel 3D human generative framework that,\nfor the first time, models the distribution of human geometry distributions.\nOur framework operates in two stages: first, generating the human geometry\ndistribution, and second, synthesizing high-fidelity humans by sampling from\nthis distribution. We validate our method on two tasks: pose-conditioned 3D\nhuman generation and single-view-based novel pose generation. Experimental\nresults demonstrate that our approach achieves the best quantitative results in\nterms of realism and geometric fidelity, outperforming state-of-the-art\ngenerative methods.",
        "Despite recent advances in Novel View Synthesis (NVS), generating\nhigh-fidelity views from single or sparse observations remains a significant\nchallenge. Existing splatting-based approaches often produce distorted geometry\ndue to splatting errors. While diffusion-based methods leverage rich 3D priors\nto achieve improved geometry, they often suffer from texture hallucination. In\nthis paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion\nmodel designed to synthesize high-fidelity novel views from a single image.\nSpecifically, we propose an aligned synthesis strategy for precise control of\ntarget viewpoints and geometry-consistent view synthesis. To mitigate texture\nhallucination, we design a texture bridge module that enables high-fidelity\ntexture generation through adaptive feature fusion. In this manner, SplatDiff\nleverages the strengths of splatting and diffusion to generate novel views with\nconsistent geometry and high-fidelity details. Extensive experiments verify the\nstate-of-the-art performance of SplatDiff in single-view NVS. Additionally,\nwithout extra training, SplatDiff shows remarkable zero-shot performance across\ndiverse tasks, including sparse-view NVS and stereo video conversion.",
        "Dense point tracking is a challenging task requiring the continuous tracking\nof every point in the initial frame throughout a substantial portion of a\nvideo, even in the presence of occlusions. Traditional methods use optical flow\nmodels to directly estimate long-range motion, but they often suffer from\nappearance drifting without considering temporal consistency. Recent point\ntracking algorithms usually depend on sliding windows for indirect information\npropagation from the first frame to the current one, which is slow and less\neffective for long-range tracking. To account for temporal consistency and\nenable efficient information propagation, we present a lightweight and fast\nmodel with \\textbf{S}treaming memory for dense \\textbf{PO}int \\textbf{T}racking\nand online video processing. The \\textbf{SPOT} framework features three core\ncomponents: a customized memory reading module for feature enhancement, a\nsensory memory for short-term motion dynamics modeling, and a visibility-guided\nsplatting module for accurate information propagation. This combination enables\nSPOT to perform dense point tracking with state-of-the-art accuracy on the CVO\nbenchmark, as well as comparable or superior performance to offline models on\nsparse tracking benchmarks such as TAP-Vid and RoboTAP. Notably, SPOT with\n10$\\times$ smaller parameter numbers operates at least 2$\\times$ faster than\nprevious state-of-the-art models while maintaining the best performance on CVO.\nWe will release the models and codes at: https:\/\/github.com\/DQiaole\/SPOT.",
        "Text-to-Image (T2I) diffusion models have shown impressive results in\ngenerating visually compelling images following user prompts. Building on this,\nvarious methods further fine-tune the pre-trained T2I model for specific tasks.\nHowever, this requires separate model architectures, training designs, and\nmultiple parameter sets to handle different tasks. In this paper, we introduce\nUniVG, a generalist diffusion model capable of supporting a diverse range of\nimage generation tasks with a single set of weights. UniVG treats multi-modal\ninputs as unified conditions to enable various downstream applications, ranging\nfrom T2I generation, inpainting, instruction-based editing, identity-preserving\ngeneration, and layout-guided generation, to depth estimation and referring\nsegmentation. Through comprehensive empirical studies on data mixing and\nmulti-task training, we provide detailed insights into the training processes\nand decisions that inform our final designs. For example, we show that T2I\ngeneration and other tasks, such as instruction-based editing, can coexist\nwithout performance trade-offs, while auxiliary tasks like depth estimation and\nreferring segmentation enhance image editing. Notably, our model can even\noutperform some task-specific models on their respective benchmarks, marking a\nsignificant step towards a unified image generation model.",
        "Physically-based rendering (PBR) has become a cornerstone in modern computer\ngraphics, enabling realistic material representation and lighting interactions\nin 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model\nfor generating PBR textures from 3D meshes and image prompts, addressing key\nchallenges in multi-view material synthesis. Our approach leverages Reference\nAttention to extract and encode informative latent from the input reference\nimages, enabling intuitive and controllable texture generation. We also\nintroduce a Consistency-Regularized Training strategy to enforce stability\nacross varying viewpoints and illumination conditions, ensuring\nillumination-invariant and geometrically consistent results. Additionally, we\npropose Dual-Channel Material Generation, which separately optimizes albedo and\nmetallic-roughness (MR) textures while maintaining precise spatial alignment\nwith the input images through Multi-Channel Aligned Attention. Learnable\nmaterial embeddings are further integrated to capture the distinct properties\nof albedo and MR. Experimental results demonstrate that our model generates PBR\ntextures with realistic behavior across diverse lighting scenarios,\noutperforming existing methods in both consistency and quality for scalable 3D\nasset creation.",
        "Prompt tuning of Vision-Language Models (VLMs) such as CLIP, has demonstrated\nthe ability to rapidly adapt to various downstream tasks. However, recent\nstudies indicate that tuned VLMs may suffer from the problem of spurious\ncorrelations, where the model relies on spurious features (e.g. background and\ngender) in the data. This may lead to the model having worse robustness in\nout-of-distribution data. Standard methods for eliminating spurious correlation\ntypically require us to know the spurious attribute labels of each sample,\nwhich is hard in the real world. In this work, we explore improving the group\nrobustness of prompt tuning in VLMs without relying on manual annotation of\nspurious features. We notice the zero - shot image recognition ability of VLMs\nand use this ability to identify spurious features, thus avoiding the cost of\nmanual annotation. By leveraging pseudo-spurious attribute annotations, we\nfurther propose a method to automatically adjust the training weights of\ndifferent groups. Extensive experiments show that our approach efficiently\nimproves the worst-group accuracy on CelebA, Waterbirds, and MetaShift\ndatasets, achieving the best robustness gap between the worst-group accuracy\nand the overall accuracy.",
        "Conditional dependency present one of the trickiest problems in Compositional\nZero-Shot Learning, leading to significant property variations of the same\nstate (object) across different objects (states). To address this problem,\nexisting approaches often adopt either all-to-one or one-to-one representation\nparadigms. However, these extremes create an imbalance in the seesaw between\ntransferability and discriminability, favoring one at the expense of the other.\nComparatively, humans are adept at analogizing and reasoning in a hierarchical\nclustering manner, intuitively grouping categories with similar properties to\nform cohesive concepts. Motivated by this, we propose Homogeneous Group\nRepresentation Learning (HGRL), a new perspective formulates state (object)\nrepresentation learning as multiple homogeneous sub-group representation\nlearning. HGRL seeks to achieve a balance between semantic transferability and\ndiscriminability by adaptively discovering and aggregating categories with\nshared properties, learning distributed group centers that retain\ngroup-specific discriminative features. Our method integrates three core\ncomponents designed to simultaneously enhance both the visual and prompt\nrepresentation capabilities of the model. Extensive experiments on three\nbenchmark datasets validate the effectiveness of our method.",
        "High-fidelity text-to-image diffusion models have revolutionized visual\ncontent generation, but their widespread use raises significant ethical\nconcerns, including intellectual property protection and the misuse of\nsynthetic media. To address these challenges, we propose a novel multi-stage\nwatermarking framework for diffusion models, designed to establish copyright\nand trace generated images back to their source. Our multi-stage watermarking\ntechnique involves embedding: (i) a fixed watermark that is localized in the\ndiffusion model's learned noise distribution and, (ii) a human-imperceptible,\ndynamic watermark in generates images, leveraging a fine-tuned decoder. By\nleveraging the Structural Similarity Index Measure (SSIM) and cosine\nsimilarity, we adapt the watermark's shape and color to the generated content\nwhile maintaining robustness. We demonstrate that our method enables reliable\nsource verification through watermark classification, even when the dynamic\nwatermark is adjusted for content-specific variations. Source model\nverification is enabled through watermark classification. o support further\nresearch, we generate a dataset of watermarked images and introduce a\nmethodology to evaluate the statistical impact of watermarking on generated\ncontent.Additionally, we rigorously test our framework against various attack\nscenarios, demonstrating its robustness and minimal impact on image quality.\nOur work advances the field of AI-generated content security by providing a\nscalable solution for model ownership verification and misuse prevention.",
        "We prove a factorizable version of the Feigin-Frenkel theorem on the center\nof the completed enveloping algebra of the affine Kac-Moody algebra attached to\na simple Lie algebra at the critical level. On any smooth curve C we consider a\nsheaf of complete topological Lie algebras whose fiber at any point is the\nusual affine algebra at the critical level and consider its sheaf of completed\nenveloping algebras. We show that the center of this sheaf is a factorization\nalgebra and establish that it is canonically isomorphic, in a factorizable\nmanner, with the factorization algebra of functions on Opers on the pointed\ndisk.",
        "Universities serve as a hub for academic collaboration, promoting the\nexchange of diverse ideas and perspectives among students and faculty through\ninterdisciplinary dialogue. However, as universities expand in size,\nconventional networking approaches via student chapters, class groups, and\nfaculty committees become cumbersome. To address this challenge, an\nacademia-specific profile recommendation system is proposed to connect\nlike-minded stakeholders within any university community. This study evaluates\nthree techniques: Term Frequency-Inverse Document Frequency (TF-IDF),\nBidirectional Encoder Representations from Transformers (BERT), and a hybrid\napproach to generate effective recommendations. Due to the unlabelled nature of\nthe dataset, Affinity Propagation cluster-based relabelling is performed to\nunderstand the grouping of similar profiles. The hybrid model demonstrated\nsuperior performance, evidenced by its similarity score, Silhouette score,\nDavies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),\nachieving an optimal balance between diversity and relevance in\nrecommendations. Furthermore, the optimal model has been implemented as a\nmobile application, which dynamically suggests relevant profiles based on\nusers' skills and collaboration interests, incorporating contextual\nunderstanding. The potential impact of this application is significant, as it\npromises to enhance networking opportunities within large academic institutions\nthrough the deployment of intelligent recommendation systems.",
        "Understanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio\nreasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and propose LongAudio, a large and novel dataset for\ntraining ALMs on long audio captioning and question-answering tasks.\nFine-tuning AF2 on LongAudio leads to exceptional performance on our proposed\nLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps:\/\/research.nvidia.com\/labs\/adlr\/AF2\/.",
        "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI.",
        "A mangrove mapping (MM) algorithm is an essential classification tool for\nenvironmental monitoring. The recent literature shows that compared with other\nindex-based MM methods that treat pixels as spatially independent,\nconvolutional neural networks (CNNs) are crucial for leveraging spatial\ncontinuity information, leading to improved classification performance. In this\nwork, we go a step further to show that quantum features provide radically new\ninformation for CNN to further upgrade the classification results. Simply\nspeaking, CNN computes affine-mapping features, while quantum neural network\n(QNN) offers unitary-computing features, thereby offering a fresh perspective\nin the final decision-making (classification). To address the challenging MM\nproblem, we design an entangled spatial-spectral quantum feature extraction\nmodule. Notably, to ensure that the quantum features contribute genuinely novel\ninformation (unaffected by traditional CNN features), we design a separate\nnetwork track consisting solely of quantum neurons with built-in\ninterpretability. The extracted pure quantum information is then fused with\ntraditional feature information to jointly make the final decision. The\nproposed quantum-empowered deep network (QEDNet) is very lightweight, so the\nimprovement does come from the cooperation between CNN and QNN (rather than\nparameter augmentation). Extensive experiments will be conducted to demonstrate\nthe superiority of QEDNet.",
        "In this paper, we propose an integrated sensing and communication (ISAC)\nsystem enabled by movable antennas (MAs), which can dynamically adjust antenna\npositions to enhance both sensing and communication performance for future\nwireless networks. To characterize the benefits of MA-enabled ISAC systems, we\nfirst derive the Cram\\'er-Rao bound (CRB) for angle estimation error, which is\nthen minimized for optimizing the antenna position vector (APV) and beamforming\ndesign, subject to a pre-defined signal-to-noise ratio (SNR) constraint to\nensure the communication performance. In particular, for the case with receive\nMAs only, we provide a closed-form optimal antenna position solution, and show\nthat employing MAs over conventional fixed-position antennas (FPAs) can achieve\na sensing performance gain upper-bounded by 4.77 dB. On the other hand, for the\ncase with transmit MAs only, we develop a boundary traversal breadth-first\nsearch (BT-BFS) algorithm to obtain the global optimal solution in the\nline-of-sight (LoS) channel scenario, along with a lower-complexity boundary\ntraversal depth-first search (BT-DFS) algorithm to find a local optimal\nsolution efficiently. While in the scenario with non-LoS (NLoS) channels, a\nmajorization-minimization (MM) based Rosen's gradient projection (RGP)\nalgorithm with an efficient initialization method is proposed to obtain\nstationary solutions for the considered problem, which can be extended to the\ngeneral case with both transmit and receive MAs. Extensive numerical results\nare presented to verify the effectiveness of the proposed algorithms, and\ndemonstrate the superiority of the considered MA-enabled ISAC system over\nconventional ISAC systems with FPAs in terms of sensing and communication\nperformance trade-off.",
        "Memory safety defects pose a major threat to software reliability, enabling\ncyberattacks, outages, and crashes. To mitigate these risks, organizations\nadopt Compositional Bounded Model Checking (BMC), using unit proofs to formally\nverify memory safety. However, methods for creating unit proofs vary across\norganizations and are inconsistent within the same project, leading to errors\nand missed defects. In addition, unit proofing remains understudied, with no\nsystematic development methods or empirical evaluations.\n  This work presents the first empirical study on unit proofing for memory\nsafety verification. We introduce a systematic method for creating unit proofs\nthat leverages verification feedback and objective criteria. Using this\napproach, we develop 73 unit proofs for four embedded operating systems and\nevaluate their effectiveness, characteristics, cost, and generalizability. Our\nresults show unit proofs are cost-effective, detecting 74\\% of recreated\ndefects, with an additional 9\\% found with increased BMC bounds, and 19 new\ndefects exposed. We also found that embedded software requires small unit\nproofs, which can be developed in 87 minutes and executed in 61 minutes on\naverage. These findings provide practical guidance for engineers and empirical\ndata to inform tooling design.",
        "Many modern wireless devices with accurate positioning needs also have access\nto vision sensors, such as a camera, radar, and Light Detection and Ranging\n(LiDAR). In scenarios where wireless-based positioning is either inaccurate or\nunavailable, using information from vision sensors becomes highly desirable for\ndetermining the precise location of the wireless device. Specifically, vision\ndata can be used to estimate distances between the target (where the sensors\nare mounted) and nearby landmarks. However, a significant challenge in\npositioning using these measurements is the inability to uniquely identify\nwhich specific landmark is visible in the data. For instance, when the target\nis located close to a lamppost, it becomes challenging to precisely identify\nthe specific lamppost (among several in the region) that is near the target.\nThis work proposes a new framework for target localization using range\nmeasurements to multiple proximate landmarks. The geometric constraints\nintroduced by these measurements are utilized to narrow down candidate landmark\ncombinations corresponding to the range measurements and, consequently, the\ntarget's location on a map. By modeling landmarks as a marked Poisson point\nprocess (PPP), we show that three noise-free range measurements are sufficient\nto uniquely determine the correct combination of landmarks in a two-dimensional\nplane. For noisy measurements, we provide a mathematical characterization of\nthe probability of correctly identifying the observed landmark combination\nbased on a novel joint distribution of key random variables. Our results\ndemonstrate that the landmark combination can be identified using ranges, even\nwhen individual landmarks are visually indistinguishable.",
        "Creating visually appealing advertising images is often a labor-intensive and\ntime-consuming process. Is it possible to automatically generate such images\nusing only basic product information--specifically, a product foreground image,\ntaglines, and a target size? Existing methods mainly focus on parts of the\nproblem and fail to provide a comprehensive solution. To address this gap, we\npropose a novel multistage framework called Product-Centric Advertising Image\nDesign (PAID). It consists of four sequential stages to highlight product\nforegrounds and taglines while achieving overall image aesthetics: prompt\ngeneration, layout generation, background image generation, and graphics\nrendering. Different expert models are designed and trained for the first three\nstages: First, we use a visual language model (VLM) to generate background\nprompts that match the products. Next, a VLM-based layout generation model\narranges the placement of product foregrounds, graphic elements (taglines and\ndecorative underlays), and various nongraphic elements (objects from the\nbackground prompt). Following this, we train an SDXL-based image generation\nmodel that can simultaneously accept prompts, layouts, and foreground controls.\nTo support the PAID framework, we create corresponding datasets with over\n50,000 labeled images. Extensive experimental results and online A\/B tests\ndemonstrate that PAID can produce more visually appealing advertising images.",
        "We present a multi-agent and mean-field formulation of a game between\ninvestors who receive private signals informing their investment decisions and\nwho interact through relative performance concerns. A key tool in our model is\na Poisson random measure which drives jumps in both market prices and signal\nprocesses and thus captures common and idiosyncratic noise. Upon receiving a\njump signal, an investor evaluates not only the signal's implications for stock\nprice movements but also its implications for the signals received by her peers\nand for their subsequent investment decisions. A crucial aspect of this\nassessment is the distribution of investor types in the economy. These types\ndetermine their risk aversion, performance concerns, and the quality and\nquantity of their signals. We demonstrate how these factors are reflected in\nthe corresponding HJB equations, characterizing an agent's optimal response to\nher peers' signal-based strategies. The existence of equilibria in both the\nmulti-agent and mean-field game is established using Schauder's Fixed Point\nTheorem under suitable conditions on investor characteristics, particularly\ntheir signal processes. Finally, we present numerical case studies that\nillustrate these equilibria from a financial-economic perspective. This allows\nus to address questions such as how much investors should care about the\ninformation known by their peers.",
        "Optimal sensor placement is essential for minimizing costs and ensuring\naccurate state estimation in power systems. This paper introduces a novel\nmethod for optimal sensor placement for dynamic state estimation of power\nsystems modeled by differential-algebraic equations. The method identifies\noptimal sensor locations by minimizing the steady-state covariance matrix of\nthe Kalman filter, thus minimizing the error of joint differential and\nalgebraic state estimation. The problem is reformulated as a mixed-integer\nsemidefinite program and effectively solved using off-the-shelf numerical\nsolvers. Numerical results demonstrate the merits of the proposed approach by\nbenchmarking its performance in phasor measurement unit placement in comparison\nto greedy algorithms.",
        "We introduce a novel method for generating 360{\\deg} panoramas from text\nprompts or images. Our approach leverages recent advances in 3D generation by\nemploying multi-view diffusion models to jointly synthesize the six faces of a\ncubemap. Unlike previous methods that rely on processing equirectangular\nprojections or autoregressive generation, our method treats each face as a\nstandard perspective image, simplifying the generation process and enabling the\nuse of existing multi-view diffusion models. We demonstrate that these models\ncan be adapted to produce high-quality cubemaps without requiring\ncorrespondence-aware attention layers. Our model allows for fine-grained text\ncontrol, generates high resolution panorama images and generalizes well beyond\nits training set, whilst achieving state-of-the-art results, both qualitatively\nand quantitatively. Project page: https:\/\/cubediff.github.io\/",
        "Randomized experiments, or A\/B testing, are the gold standard for evaluating\ninterventions but are underutilized in the area of inventory management. This\nstudy addresses this gap by analyzing A\/B testing strategies in multi-item,\nmulti-period inventory systems with lost sales and capacity constraints. We\nexamine switchback experiments, item-level randomization, pairwise\nrandomization, and staggered rollouts, analyzing their biases theoretically and\ncomparing them through numerical experiments. Our findings provide actionable\nguidance for selecting experimental designs across various contexts in\ninventory management.",
        "With the rising prevalence of cardiovascular and respiratory disorders and an\naging global population, healthcare systems face increasing pressure to adopt\nefficient, non-contact vital sign monitoring (NCVSM) solutions. This study\nintroduces a robust framework for multi-person localization and vital signs\nmonitoring, using multiple-input-multiple-output frequency-modulated continuous\nwave radar, addressing challenges in real-world, cluttered environments. Two\nkey contributions are presented. First, a custom hardware phantom was developed\nto simulate multi-person NCVSM scenarios, utilizing recorded thoracic impedance\nsignals to replicate realistic cardiopulmonary dynamics. The phantom's design\nfacilitates repeatable and rapid validation of radar systems and algorithms\nunder diverse conditions to accelerate deployment in human monitoring. Second,\naided by the phantom, we designed a robust algorithm for multi-person\nlocalization utilizing joint sparsity and cardiopulmonary properties, alongside\nharmonics-resilient dictionary-based vital signs estimation, to mitigate\ninterfering respiration harmonics. Additionally, an adaptive signal refinement\nprocedure is introduced to enhance the accuracy of continuous NCVSM by\nleveraging the continuity of the estimates. Performance was validated and\ncompared to existing techniques through 12 phantom trials and 12 human trials,\nincluding both single- and multi-person scenarios, demonstrating superior\nlocalization and NCVSM performance. For example, in multi-person human trials,\nour method achieved average respiration rate estimation accuracies of 94.14%,\n98.12%, and 98.69% within error thresholds of 2, 3, and 4 breaths per minute,\nrespectively, and heart rate accuracies of 87.10%, 94.12%, and 95.54% within\nthe same thresholds. These results highlight the potential of this framework\nfor reliable multi-person NCVSM in healthcare and IoT applications.",
        "Let $\\K$ be an archimedean local field. We investigate the existence of the\ntwisted Shalika functionals on irreducible admissible smooth representations of\n$\\GL_{2n}(\\K)$ in terms of their L-parameters. As part of our proof, we\nestablish a Hochschild-Serre spectral sequence for nilpotent normal subgroups\nand a Kunneth formula in the framework of Schwartz homology. We also prove the\nanalogous result for twisted linear periods using theta correspondence. The\nexistence of twisted Shalika functionals on representations of\n$\\GL_{2n}^{+}(\\R)$ is also studied, which is of independent interest."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"MagNet: machine learning enhanced three-dimensional magnetic reconstruction",
    "start_abstract":"Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Three-dimensional nanomagnetism"
      ],
      "abstract":[
        "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
      ],
      "categories":[
        "cond-mat.mes-hall"
      ]
    },
    "list":{
      "title":[
        "Caroli-de Gennes-Matricon Analogs in Full-Shell Hybrid Nanowires",
        "Nonequilibrium Green's Function Formalism Applicable to Discrete\n  Impurities in Semiconductor Nanostructures",
        "Probing $k$-Space Alternating Spin Polarization via the Anomalous Hall\n  Effect",
        "Topological insulator constrictions -- Dirac particles in a\n  magneto-chiral box",
        "Klein Tunneling and Fabry-P\\'erot Resonances in Twisted Bilayer Graphene",
        "Nonvolatile Electric Control of Antiferromagnet CrSBr",
        "Longitudinal Spin Hall Magnetoresistance from Spin Fluctuations",
        "Spin-polarized STM measurement scheme for quantum geometric tensor",
        "Anomalous topological edge modes in a periodically-driven trimer lattice",
        "Gate Tunable Josephson Diode Effect in Josephson Junctions made from\n  InAs Nanosheets",
        "Single-gate tracking behavior in flat-band multilayer graphene devices",
        "Quantum dot-based device for high-performance magnetic microscopy and\n  spin filtering in the Kondo regime",
        "Quantized crystalline-electromagnetic responses in insulators",
        "A new framework for Ljusternik-Schnirelmann theory and its application\n  to planar Choquard equations",
        "Implicit Generative Modeling by Kernel Similarity Matching",
        "k-Sample inference via Multimarginal Optimal Transport",
        "A Drinfeld Presentation of the Queer Super-Yangian",
        "Implicit Bias in Matrix Factorization and its Explicit Realization in a\n  New Architecture",
        "An experimental technique for measuring radial coherence",
        "Probing prethermal nonergodicity through measurement outcomes of\n  monitored quantum dynamics",
        "Four Total Eclipsing Contact Binary Systems: The First Photometric Light\n  Curve Solutions Employing TESS and Gaia Surveys",
        "Time-Variant Vector Field Visualization for Magnetic Fields of Neutron\n  Star Simulations",
        "Quantum Birkhoff Normal Form in the $\\sigma$-Bruno-R\\\"{u}ssmann\n  non-resonant condition",
        "From de Bruijn graphs to variation graphs-relationships between\n  pangenome models",
        "High-accuracy multi-ion spectroscopy with mixed-species Coulomb crystals",
        "A Liouville-type theorem for the p-Laplacian on complete non-compact\n  Riemannian manifolds",
        "Gradient Flows and the Curvature of Theory Space",
        "Causal survival analysis, Estimation of the Average Treatment Effect\n  (ATE): Practical Recommendations"
      ],
      "abstract":[
        "We report tunneling spectroscopy of Andreev subgap states in hybrid nanowires\nwith a thin superconducting full-shell surrounding a semiconducting core. The\ncombination of the quantized fluxoid of the shell and the Andreev reflection at\nthe superconductor-semiconductor interface gives rise to analogs of Caroli-de\nGennes-Matricon (CdGM) states found in Abrikosov vortices in type-II\nsuperconductors. Unlike in metallic superconductors, CdGM analogs in full-shell\nhybrid nanowires manifest as one-dimensional van Hove singularities with energy\nspacings comparable to the superconducting gap and independent of the Fermi\nenergy, making them readily observable. Evolution of these analogs with axial\nmagnetic field, skewed within the Little-Parks lobe structure, is consistent\nwith theory and yields information about the radial distribution and angular\nmomenta of the corresponding subbands.",
        "A new theoretical framework for the nonequilibrium Green's function (NEGF)\nscheme is presented to account for the discrete nature of impurities doped in\nsemiconductor nanostructures. The short-range part of impurity potential is\nincluded as scattering potential in the self-energy due to spatially localized\nimpurity scattering, and the long-range part of impurity potential is treated\nas the self-consistent Hartree potential by coupling with the Poisson equation.\nThe position-dependent impurity scattering rate under inhomogeneous impurity\nprofiles is systematically derived so that its physical meaning is clarified.\nThe position dependence of the scattering rate turns out to be represented by\nthe `center of mass' coordinates in the Wigner coordinates, rather than the\nreal-space coordinates. Consequently, impurity scattering is intrinsically\nnonlocal in space. The proposed framework is applied to cylindrical thin wires\nunder the quasi-one-dimensional (quasi-1D) approximation. We show explicitly\nhow the discrete nature of impurities affects the transport properties such as\nelectrostatic potential, local density of states, carrier density, scattering\nrates, and mobility.",
        "Altermagnets represent a recently discovered class of collinear magnets,\ncharacterized by antiparallel neighboring magnetic moments and alternating-sign\nspin polarization in momentum-space($k$-space). However, experimental methods\nfor probing the $k$-space spin polarization in altermagnets remain limited. In\nthis work, we propose an approach to address this challenge by interfacing an\naltermagnet with the surface of a topological insulator. The massless Dirac\nfermions on the topological insulator surface acquire a mass due to the\ntime-reversal symmetry breaking. The local $k$-space magnetic moment at the\nDirac point directly determines both the sign and magnitude of this Dirac mass,\nresulting in an anomalous Hall effect. By measuring the Hall conductance, we\ncan extract the local $k$-space magnetic moment. Moreover, we can map the\nglobal magnetic moment distribution by tuning the Dirac point position using an\nin-plane magnetic field, thereby revealing the $k$-space spin density of the\naltermagnet. This work establishes the Dirac fermion on the topological\ninsulator surface as a sensitive probe for unveiling spin characters of\naltermagnets and those of other unconventional antiferromagnets.",
        "We study magneto-transport through topological insulator nanowires shaped in\nthe form of a constriction, as can be obtained by etching techniques. The\nmagnetic field is coaxial, potentially turning the nanowire into a\nmagneto-chiral junction. We show in a detailed analytical and numerical study\nthat two main transport regimes emerge, depending on the central narrow region\nbeing short or long as compared to the magnetic length at the junction entrance\nand exit. In both cases the central region hosts Dirac-particle-in-a-box states\ndue to magnetic confinement, whose conductance properties are strongly\ninfluenced by Landau levels at the ends of the constriction. Notably, in the\nlow-energy regime only chiral states with a specific handedness can transport\ncharge across the junction. Based on these properties and general symmetry\nconsiderations we argue that the shaped nanowire should exhibit strong\nmagneto-chiral non-reciprocal transport beyond linear response. We employ a\nnumerical tight-binding implementation of an effective 2D model on a\nnon-homogeneous grid, capable of simulating samples of realistic sizes, and\ntest its soundness against full simulations for scaled-down 3D topological\ninsulator wires.",
        "The paper discusses the Klein tunneling and Fabry-P\\'erot resonances of\ncharge carriers through a rectangular potential barrier in twisted bilayer\ngraphene. Within the framework of the low-energy excitations, the transmission\nprobability and the conductance are obtained depending on the parameters of the\nproblem. Owing to the different chirality in twisted bilayer graphene, the\npropagation of charge carriers exhibits an anisotropic behavior in transmission\nprobability and Fabry-P\\'erot resonances. Moreover, we show that the anisotropy\nof the charge carriers induces asymmetry and deflection in the Fabry-P\\'erot\nresonances and Klein tunneling, and they are extremely sensitive to the height\nof the potential applied. Additionally, we found that the conductance is\nstrongly sensitive to the barrier height but weakly sensitive to the barrier\nwidth. Therefore, it is possible to control the maxima and minima of the\nconductance of charge carriers in twisted bilayer graphene. With our results,\nwe gain an in-depth understanding of tunneling properties in twisted bilayer\ngraphene, which may help in the development and designing of novel electronic\nnanodevices based on anisotropic 2D materials.",
        "van der Waals magnets are emerging as a promising material platform for\nelectric field control of magnetism, offering a pathway towards the elimination\nof external magnetic fields from spintronic devices. A further step is the\nintegration of such magnets with electrical gating components which would\nenable nonvolatile control of magnetic states. However, this approach remains\nunexplored for antiferromagnets, despite their growing significance in\nspintronics. Here, we demonstrate nonvolatile electric field control of\nmagnetoelectric characteristics in van der Waals antiferromagnet CrSBr. We\nintegrate a CrSBr channel in a flash-memory architecture featuring charge\ntrapping graphene multilayers. The electrical gate operation triggers a\nnonvolatile 200 % change in the antiferromagnetic state of CrSBr resistance by\nmanipulating electron accumulation\/depletion. Moreover, the nonvolatile gate\nmodulates the metamagnetic transition field of CrSBr and the magnitude of\nmagnetoresistance. Our findings highlight the potential of manipulating\nmagnetic properties of antiferromagnetic semiconductors in a nonvolatile way.",
        "Spin Hall magnetoresistance (SMR), the variation in resistance in a heavy\nmetal (HM) with the magnetization orientation of an adjacent ferromagnet (FM),\nhas been extensively studied as a powerful tool for probing surface magnetic\nmoments in a variety of magnetic materials. However, the conventional SMR\ntheory assumes rigid magnetization of a fixed magnitude, an assumption that\nbreaks down close to the FM's Curie temperature \\(T_c\\), where the magnetic\nsusceptibility diverges. Here, we report an unconventional SMR effect arising\nfrom the magnetic-field modulation of spin fluctuations in the FM, while its\nmagnetization remaining collinear to the spin Hall accumulation in the HM. In\ncontrast to the conventional SMR, which scales with the magnetization and\nvanishes near $T_{c}$, such ``longitudinal\" SMR (LSMR), though suppressed at\nlow temperatures, becomes critically enhanced at \\(T_c\\), reaching a magnitude\ncomparable to conventional SMR amplitudes. Our findings suggest a promising\nmethod for electrically detecting enhanced spin fluctuations in magnetic\nsystems.",
        "Quantum geometric tensor (QGT) reflects the geometry of the eigenstates of a\nsystem's Hamiltonian. The full characterization of QGT is essential for various\nquantum systems. However, it is challenging to characterize the QGT of the\nsolid-state systems. Here we present a scheme by using spin-polarized STM to\nmeasure QGT of two-dimensional solid-state systems, in which the spin texture\nis extracted from geometric amplitudes of Friedel oscillations induced by the\nintentionally introduced magnetic impurity and then the QGT is derived from the\nmomentum differential of spin texture. The surface states of topological\ninsulator (TISS), as a model spin system, is promising to demonstrate the\nscheme. In a TI slab, the gapped TISS host finite quantum metric and Berry\ncurvature as the symmetric real part and the antisymmetric imaginary part of\nQGT, respectively. Thus, a detailed calculations guide the use of the developed\nscheme to measure the QGT of gapped TISS with or without an external in-plane\nmagnetic field. This study provides a feasible scheme for measuring QGT of\ntwo-dimensional solid-state systems, and hints at the great potential of the\ninformation extraction from the geometric amplitudes of STM and other\nmeasurement.",
        "Periodically driven systems have a longstanding reputation for establishing\nrich topological phenomena beyond their static counterpart. In this work, we\npropose and investigate a periodically driven extended Su-Schrieffer-Heeger\n(SSH) model with three sites per unit cell, obtained by replacing the Pauli\nmatrices with their $3\\times 3$ counterparts. The system is found to support a\nnumber of edge modes over a range of parameter windows, some of which have no\nstatic counterparts. Among these edge modes, of particular interest are those\nwhich are pinned at a specific quasienergy value. Such quasienergy-fixed edge\nmodes arise due to the interplay between topology and chiral symmetry, which\nare typically not expected in a three-band static model due to the presence of\na bulk band at the only chiral-symmetric energy value, i.e., zero. In our\ntime-periodic setting, another chiral-symmetric quasienergy value exists at\nhalf the driving frequency, which is not occupied by a bulk band and could then\nhost chiral-symmetry-protected edge modes ($\\pi$ modes). Finally, we verify the\nrobustness of all edge modes against spatial disorder and briefly discuss the\nprospect of realizing our system in experiments.",
        "We report the observation of Josephson diode effect (JDE) in hybrid devices\nmade from semiconductor InAs nanosheets and superconductor Al contacts. By\napplying an in-plane magnetic field ($B_{\\mathrm{xy}}$), we detect\nnon-reciprocal superconducting switching current as well as non-reciprocal\nsuperconducting retrapping current. The strength of the JDE depends on the\nangle between the in-plane magnetic field and the bias current\n($I_{\\mathrm{b}}$), reaching its maximum when $B_{\\mathrm{xy}} \\perp\nI_{\\mathrm{b}}$ and dropping to nearly zero when $B_{\\mathrm{xy}}\\parallel\nI_{\\mathrm{b}}$. Additionally, the diode efficiency is tunable via an\nelectrostatic gate with a complete suppression at certain gate voltages. Our\nfindings indicate that the observed JDE in InAs nanosheet-based Josephson\njunctions most likely arises from the Rashba spin-orbit interaction (SOI) in\nthe nanosheets. Such gate-tunable JDE in Josephson junctions made from\nsemiconductor material with SOI is useful not only for constructing advanced\nsuperconducting electronics but also for detecting novel superconducting\nstates.",
        "A central feature of many van der Waals (vdW) materials is the ability to\nprecisely control their charge doping, $n$, and electric displacement field,\n$D$, using top and bottom gates. For devices composed of only a few layers, it\nis commonly assumed that $D$ causes the layer-by-layer potential to drop\nlinearly across the structure. Here, we show that this assumption fails for a\nbroad class of crystalline and moir\\'e vdW structures based on Bernal- or\nrhombohedral-stacked multilayer graphene. We find that the electronic\nproperties at the Fermi level are largely dictated by special layer-polarized\nstates arising at Bernal-stacked crystal faces, which typically coexist in the\nsame band with layer-delocalized states. We uncover a novel mechanism by which\nthe layer-delocalized states completely screen the layer-polarized states from\nthe bias applied to the remote gate. This screening mechanism leads to an\nunusual scenario where voltages on either gate dope the band as expected, yet\nthe band dispersion and associated electronic properties remain primarily (and\nsometimes exclusively) governed by the gate closer to the layer-polarized\nstates. Our results reveal a novel electronic mechanism underlying the atypical\nsingle-gate-controlled transport characteristics observed across many flat-band\ngraphitic structures, and provide key theoretical insights essential for\naccurately modeling these systems.",
        "We propose a nanoscale device consisting of a double quantum dot with a full\nexchange and pair hopping interaction. In this design, the current can only\nflow through the upper dot, but is sensitive to the spin state of the lower\ndot. The system is immersed in a highly inhomogeneous magnetic field, and only\nthe bottom dot feels a substantial magnetic field, while the top dot\nexperiences only a residual one.\n  We show that our device exhibits very interesting magnetic field-dependent\ntransport properties at low temperatures. The Kondo effect partially survives\nthe presence of the magnetic field and allows to obtain conductances that\ndiffer by several orders of magnitude for the two spin types across the top\ndot.\n  Interestingly, as a function of the magnetic field, our two-dot device\nchanges from a spin singlet state to a spin triplet state, in which the\namplitudes of the spin-dependent conductances are reversed.\n  Our device is able to discriminate between positive and negative magnetic\nfields with a high sensitivity and is therefore particularly interesting for\nimaging the surface of anti-ferromagnetic (AF) insulating materials with\nalternated surface magnetic field, as well as for spin filtering applications.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "We consider the planar logarithmic Choquard equation $$- \\Delta u + a(x)u +\n(\\log|\\cdot| \\ast u^2)u = 0,\\qquad \\text{in } \\mathbb{R}^2$$ in the strongly\nindefinite and possibly degenerate setting where no sign condition is imposed\non the linear potential $a \\in L^\\infty(\\mathbb{R}^2)$. In particular, we shall\nprove the existence of a sequence of high energy solutions to this problem in\nthe case where $a$ is invariant under $\\mathbb{Z}^2$-translations.\n  The result extends to a more general $G$-equivariant setting, for which we\ndevelop a new variational approach which allows us to find critical points of\nLjusternik-Schnirelmann type. In particular, our method resolves the problem\nthat the energy functional $\\Phi$ associated with the logarithmic Choquard\nequation is only defined on a subspace $X \\subset H^1(\\mathbb{R}^2)$ with the\nproperty that $\\|\\cdot\\|_X$ is not translation invariant. The new approach is\nbased on a new $G$-equivariant version of the Cerami condition and on\ndeformation arguments adapted to a family of suitably constructed scalar\nproducts $\\langle \\cdot, \\cdot \\rangle_u$, $u \\in X$ with the $G$-equivariance\nproperty $\\langle g \\ast v , g \\ast w \\rangle_{g \\ast u} = \\langle v , w\n\\rangle_u.$",
        "Understanding how the brain encodes stimuli has been a fundamental problem in\ncomputational neuroscience. Insights into this problem have led to the design\nand development of artificial neural networks that learn representations by\nincorporating brain-like learning abilities. Recently, learning representations\nby capturing similarity between input samples has been studied to tackle this\nproblem. This approach, however, has thus far been used to only learn\ndownstream features from an input and has not been studied in the context of a\ngenerative paradigm, where one can map the representations back to the input\nspace, incorporating not only bottom-up interactions (stimuli to latent) but\nalso learning features in a top-down manner (latent to stimuli). We investigate\na kernel similarity matching framework for generative modeling. Starting with a\nmodified sparse coding objective for learning representations proposed in prior\nwork, we demonstrate that representation learning in this context is equivalent\nto maximizing similarity between the input kernel and a latent kernel. We show\nthat an implicit generative model arises from learning the kernel structure in\nthe latent space and show how the framework can be adapted to learn manifold\nstructures, potentially providing insights as to how task representations can\nbe encoded in the brain. To solve the objective, we propose a novel Alternate\nDirection Method of Multipliers (ADMM) based algorithm and discuss the\ninterpretation of the optimization process. Finally, we discuss how this\nrepresentation learning problem can lead towards a biologically plausible\narchitecture to learn the model parameters that ties together representation\nlearning using similarity matching (a bottom-up approach) with predictive\ncoding (a top-down approach).",
        "This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for\nsimultaneously comparing $k\\geq 2$ measures supported on finite subsets of\n$\\mathbb{R}^d$, $d \\geq 1$. We derive asymptotic distributions of the optimal\nvalue of the empirical $MOT$ program under the null hypothesis that all $k$\nmeasures are same, and the alternative hypothesis that at least two measures\nare different. We use these results to construct the test of the null\nhypothesis and provide consistency and power guarantees of this $k$-sample\ntest. We consistently estimate asymptotic distributions using bootstrap, and\npropose a low complexity linear program to approximate the test cut-off. We\ndemonstrate the advantages of our approach on synthetic and real datasets,\nincluding the real data on cancers in the United States in 2004 - 2020.",
        "We introduce a Drinfeld presentation for the super-Yangian\n$\\mathrm{Y}(\\mathfrak{q}_n)$ associated with the queer Lie superalgebra\n$\\mathfrak{q}_n$. The Drinfeld generators of $\\mathrm{Y}(\\mathfrak{q}_n)$ are\nobtained by a block version Gauss decomposition of the generator matrix in its\nRTT presentation, and the Drinfeld relations are explicitly computed by\nutilizing a block version of its RTT relations.",
        "Gradient descent for matrix factorization is known to exhibit an implicit\nbias toward approximately low-rank solutions. While existing theories often\nassume the boundedness of iterates, empirically the bias persists even with\nunbounded sequences. We thus hypothesize that implicit bias is driven by\ndivergent dynamics markedly different from the convergent dynamics for data\nfitting. Using this perspective, we introduce a new factorization model:\n$X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while\n$D$ is a diagonal factor allowing the model to span the entire search space.\nOur experiments reveal that this model exhibits a strong implicit bias\nregardless of initialization and step size, yielding truly (rather than\napproximately) low-rank solutions. Furthermore, drawing parallels between\nmatrix factorization and neural networks, we propose a novel neural network\nmodel featuring constrained layers and diagonal components. This model achieves\nstrong performance across various regression and classification tasks while\nfinding low-rank solutions, resulting in efficient and lightweight networks.",
        "Coherence refers to correlations between field vibrations at two separate\npoints in degrees of freedom such as space, time, and polarisation. In the\ncontext of space, coherence theory has been formulated between two transverse\npositions which can be described either in the cartesian coordinates or in the\ncylindrical coordinates. When expressed in cylindrical coordinates, spatial\ncoherence is described in terms of azimuthal and radial coordinates. The\ndescription of spatial coherence in radial degree of freedom has been\nformulated only recently in JOSA A 40, 411 (2023). In the present article, we\ndemonstrate an efficient experimental technique for measuring radial coherence,\nand we report measurement of radial coherence of two different types of\nradially partially coherent optical fields.",
        "Projective measurements are a key element in quantum physics and enable rich\nphenomena in monitored quantum dynamics. Here, we show that the measurement\noutcomes, recorded during monitored dynamics, can provide crucial information\nabout the properties of the monitored dynamical system itself. We demonstrate\nthis for a Floquet model of many-body localization, where we find that the\nprethermal many-body localized regime becomes unstable against rare\nmeasurements, yielding an unusual enhancement of quantum entanglement. Through\nan unsupervised learning and mutual information analysis on the classical\ndataset of measurement outcomes, we find that the information loss in the\nsystem, reflected by the increased entanglement, is compensated by an emergent\nstructure in this classical dataset. Our findings highlight the crucial role of\nmeasurements and corresponding classical outcomes in capturing prethermal\nnonergodicity, offering a promising perspective for applications to other\nmonitored quantum dynamics.",
        "We presented the first photometric light curve solutions of four W Ursae\nMajoris (W UMa)-type contact binary systems. This investigation utilized\nphotometric data from the Transiting Exoplanet Survey Satellite (TESS) and Gaia\nData Release 3 (DR3). We used the PHysics Of Eclipsing BinariEs (PHOEBE) Python\ncode and the Markov Chain Monte Carlo (MCMC) method for these light curve\nsolutions. Only TIC 249064185 among the target systems needed a cold starspot\nto be included in the analysis. Based on the estimated mass ratios for these\ntotal eclipse systems, three of them are categorized as low mass ratio contact\nbinary stars. The absolute parameters of the systems were estimated using the\nGaia DR3 parallax method and the orbital period and semi-major axis ($P-a$)\nempirical relationship. We defined that TIC 318015356 and TIC 55522736 systems\nare A-subtypes, while TIC 249064185 and TIC 397984843 are W-subtypes, depending\non each component's effective temperature and mass. We estimated the initial\nmasses of the stars, the mass lost by the binary system, and the systems' ages.\nWe displayed star positions in the mass-radius, mass-luminosity, and total\nmass-orbital angular momentum diagrams. In addition, our findings indicate a\ngood agreement with the mass-temperature empirical parameter relationship for\nthe primary stars.",
        "We present a novel visualization application designed to explore the\ntime-dependent development of magnetic fields of neutron stars. The strongest\nmagnetic fields in the universe can be found within neutron stars, potentially\nplaying a role in initiating astrophysical jets and facilitating the outflow of\nneutron-rich matter, ultimately resulting in the production of heavy elements\nduring binary neutron star mergers. Since such effects may be dependent on the\nstrength and configuration of the magnetic field, the formation and parameters\nof such fields are part of current research in astrophysics. Magnetic fields\nare investigated using simulations in which various initial configurations are\ntested. However, the long-term configuration is an open question, and current\nsimulations do not achieve a stable magnetic field. Neutron star simulations\nproduce data quantities in the range of several terabytes, which are both\nspatially in 3D and temporally resolved. Our tool enables physicists to\ninteractively explore the generated data. We first convert the data in a\npre-processing step and then we combine sparse vector field visualization using\nstreamlines with dense vector field visualization using line integral\nconvolution. We provide several methods to interact with the data responsively.\nThis allows the user to intuitively investigate data-specific issues.\nFurthermore, diverse visualization techniques facilitate individual exploration\nof the data and enable real-time processing of specific domain tasks, like the\ninvestigation of the time-dependent evolution of the magnetic field. In a\nqualitative study, domain experts tested the tool, and the usability was\nqueried. Experts rated the tool very positively and recommended it for their\ndaily work.",
        "The aim of this paper is to construct a Gevrey quantum Birkhoff normal form\nfor the $h$-differential operator $P_{h}(t),$ where $\nt\\in(-\\frac{1}{2},\\frac{1}{2})$, in the neighborhood of the union $\\Lambda$ of\nKAM tori. This construction commences from an appropriate Birkhoff normal form\nof $H$ around $\\Lambda$ and proceeds under the $\\sigma$-Bruno-R\\\"{u}ssmann\ncondition with $\\sigma>1$.",
        "Pangenomes serve as a framework for joint analysis of genomes of related\norganisms. Several pangenome models were proposed, offering different\nfunctionalities, applications provided by available tools, their efficiency\netc. Among them, two graph-based models are particularly widely used: variation\ngraphs and de Bruijn graphs. In the current paper we propose an axiomatization\nof the desirable properties of a graph representation of a collection of\nstrings. We show the relationship between variation graphs satisfying these\ncriteria and de Bruijn graphs. This relationship can be used to efficiently\nbuild a variation graph representing a given set of genomes, transfer\nannotations between both models, compare the results of analyzes based on each\nmodel etc.",
        "Multi-ion optical clocks offer the possibility of overcoming the low\nsignal-to-noise ratio of single-ion clocks, while still providing low\nsystematic uncertainties. We present simultaneous spectroscopy of up to four\n${}^{115}$In${}^+$ clock ions in a linear Coulomb crystal, sympathetically\ncooled with ${}^{172}$Yb${}^+$ ions. In first clock comparisons, we see\nagreement below $1\\times10^{-17}$ with results obtained using a single In${}^+$\nion, for which we have evaluated the systematic uncertainty to be\n$2.5\\times10^{-18}$. Operation with four clock ions reduces the instability\nfrom $1.6\\times10^{-15}\/\\sqrt{t\/(1\\;\\mathrm{s})}$ to\n$9.2\\times10^{-16}\/\\sqrt{t\/(1\\;\\mathrm{s})}$. We derive a model for\ndecay-related dead time during state preparation, which matches the observed\nscaling of instability with clock ion number $N$, and indicates that\n$1\/\\sqrt{N}$ scaling can be achieved with the addition of a repump laser.",
        "A Liouville-type result for the p-Laplacian on complete Riemannian manifolds\nis proved. As an application are present some results concerning complete\nnon-compact hypersurfaces immersed in a suitable warped product manifold.",
        "The metric and potential associated with the gradient property of\nrenormalisation group flow in multiscalar models in $d=4-\\varepsilon$\ndimensions are studied. The metric is identified with the Zamolodchikov metric\nof nearly marginal operators on the sphere. An explicit form for the associated\nRicci scalar in $d=4-\\varepsilon$ is derived, which shows that the space of\nmultiscalar field theories is curved. The potential is identified with a\nquantity $\\widetilde{F}$ that was previously proposed as a weakly monotonic\nfunction interpolating between the $a$-theorem in four dimensions and the\n$F$-theorem in three dimensions. This implies that the $\\widetilde{F}$-theorem\ncan be extended perturbatively to a theorem about gradient flow in\n$d=4-\\varepsilon$.",
        "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Model-free simulations of turbulent reactive flows",
    "start_abstract":"A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given.",
    "start_categories":[
      "physics.flu-dyn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
      ],
      "abstract":[
        "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "FW-Shapley: Real-time Estimation of Weighted Shapley Values",
        "A Deep-Learning Iterative Stacked Approach for Prediction of Reactive\n  Dissolution in Porous Media",
        "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
        "Learning Versatile Optimizers on a Compute Diet",
        "Single Domain Generalization with Model-aware Parametric Batch-wise\n  Mixup",
        "Position: Curvature Matrices Should Be Democratized via Linear Operators",
        "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
        "Exploring the Potential of Bilevel Optimization for Calibrating Neural\n  Networks",
        "U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer\n  Depression Detection",
        "ColNet: Collaborative Optimization in Decentralized Federated Multi-task\n  Learning Systems",
        "Federated Learning with Reservoir State Analysis for Time Series Anomaly\n  Detection",
        "Spurious Forgetting in Continual Learning of Language Models",
        "Model-Based Exploration in Monitored Markov Decision Processes",
        "Continuous spectrum-shrinking maps and applications to preserver\n  problems",
        "Aligning LLMs with Domain Invariant Reward Models",
        "People Reduce Workers' Compensation for Using Artificial Intelligence\n  (AI)",
        "DnD Filter: Differentiable State Estimation for Dynamic Systems using\n  Diffusion Models",
        "The Commutators of $n$-dimensional Rough Fractional Hardy Operators on\n  Two Weighted Grand Herz-Morrey Spaces with Variable Exponents",
        "Hypernetwork-based approach for optimal composition design in partially\n  controlled multi-agent systems",
        "Science mapping of the Revista General de Informacion y Documentacion\n  (2005-2022)",
        "Error norm estimates for the block conjugate gradient algorithm",
        "TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional\n  Networks to Predict Transcription Factor Binding Sites",
        "Scalable Video Conferencing Using SDN Principles",
        "Mining Diamonds in labeled Transition Systems",
        "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
        "Monte-Carlo based non-line-of-sight underwater wireless optical\n  communication channel modeling and system performance analysis under\n  turbulence",
        "AIoT-based smart traffic management system",
        "Phase space analysis of CCDM cosmologies"
      ],
      "abstract":[
        "Fair credit assignment is essential in various machine learning (ML)\napplications, and Shapley values have emerged as a valuable tool for this\npurpose. However, in critical ML applications such as data valuation and\nfeature attribution, the uniform weighting of Shapley values across subset\ncardinalities leads to unintuitive credit assignments. To address this,\nweighted Shapley values were proposed as a generalization, allowing different\nweights for subsets with different cardinalities. Despite their advantages,\nsimilar to Shapley values, Weighted Shapley values suffer from exponential\ncompute costs, making them impractical for high-dimensional datasets. To tackle\nthis issue, we present two key contributions. Firstly, we provide a weighted\nleast squares characterization of weighted Shapley values. Next, using this\ncharacterization, we propose Fast Weighted Shapley (FW-Shapley), an amortized\nframework for efficiently computing weighted Shapley values using a learned\nestimator. We further show that our estimator's training procedure is\ntheoretically valid even though we do not use ground truth Weighted Shapley\nvalues during training. On the feature attribution task, we outperform the\nlearned estimator FastSHAP by $27\\%$ (on average) in terms of Inclusion AUC.\nFor data valuation, we are much faster (14 times) while being comparable to the\nstate-of-the-art KNN Shapley.",
        "Simulating reactive dissolution of solid minerals in porous media has many\nsubsurface applications, including carbon capture and storage (CCS), geothermal\nsystems and oil & gas recovery. As traditional direct numerical simulators are\ncomputationally expensive, it is of paramount importance to develop faster and\nmore efficient alternatives. Deep-learning-based solutions, most of them built\nupon convolutional neural networks (CNNs), have been recently designed to\ntackle this problem. However, these solutions were limited to approximating one\nfield over the domain (e.g. velocity field). In this manuscript, we present a\nnovel deep learning approach that incorporates both temporal and spatial\ninformation to predict the future states of the dissolution process at a fixed\ntime-step horizon, given a sequence of input states. The overall performance,\nin terms of speed and prediction accuracy, is demonstrated on a numerical\nsimulation dataset, comparing its prediction results against state-of-the-art\napproaches, also achieving a speedup around $10^4$ over traditional numerical\nsimulators.",
        "Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM.",
        "Learned optimization has emerged as a promising alternative to hand-crafted\noptimizers, with the potential to discover stronger learned update rules that\nenable faster, hyperparameter-free training of neural networks. A critical\nelement for practically useful learned optimizers, that can be used\noff-the-shelf after meta-training, is strong meta-generalization: the ability\nto apply the optimizers to new tasks. Recent state-of-the-art work in learned\noptimizers, VeLO (Metz et al., 2022), requires a large number of highly diverse\nmeta-training tasks along with massive computational resources, 4000 TPU\nmonths, to achieve meta-generalization. This makes further improvements to such\nlearned optimizers impractical. In this work, we identify several key elements\nin learned optimizer architectures and meta-training procedures that can lead\nto strong meta-generalization. We also propose evaluation metrics to reliably\nassess quantitative performance of an optimizer at scale on a set of evaluation\ntasks. Our proposed approach, Celo, makes a significant leap in improving the\nmeta-generalization performance of learned optimizers and also outperforms\ntuned state-of-the-art optimizers on a diverse set of out-of-distribution\ntasks, despite being meta-trained for just 24 GPU hours.",
        "Single Domain Generalization (SDG) remains a formidable challenge in the\nfield of machine learning, particularly when models are deployed in\nenvironments that differ significantly from their training domains. In this\npaper, we propose a novel data augmentation approach, named as Model-aware\nParametric Batch-wise Mixup (MPBM), to tackle the challenge of SDG. MPBM\ndeploys adversarial queries generated with stochastic gradient Langevin\ndynamics, and produces model-aware augmenting instances with a parametric\nbatch-wise mixup generator network that is carefully designed through an\ninnovative attention mechanism. By exploiting inter-feature correlations, the\nparameterized mixup generator introduces additional versatility in combining\nfeatures across a batch of instances, thereby enhancing the capacity to\ngenerate highly adaptive and informative synthetic instances for specific\nqueries. The synthetic data produced by this adaptable generator network,\nguided by informative queries, is expected to significantly enrich the\nrepresentation space covered by the original training dataset and subsequently\nenhance the prediction model's generalizability across diverse and previously\nunseen domains. To prevent excessive deviation from the training data, we\nfurther incorporate a real-data alignment-based adversarial loss into the\nlearning process of MPBM, regularizing any tendencies toward undesirable\nexpansions. We conduct extensive experiments on several benchmark datasets. The\nempirical results demonstrate that by augmenting the training set with\ninformative synthesis data, our proposed MPBM method achieves the\nstate-of-the-art performance for single domain generalization.",
        "Structured large matrices are prevalent in machine learning. A particularly\nimportant class is curvature matrices like the Hessian, which are central to\nunderstanding the loss landscape of neural nets (NNs), and enable second-order\noptimization, uncertainty quantification, model pruning, data attribution, and\nmore. However, curvature computations can be challenging due to the complexity\nof automatic differentiation, and the variety and structural assumptions of\ncurvature proxies, like sparsity and Kronecker factorization. In this position\npaper, we argue that linear operators -- an interface for performing\nmatrix-vector products -- provide a general, scalable, and user-friendly\nabstraction to handle curvature matrices. To support this position, we\ndeveloped $\\textit{curvlinops}$, a library that provides curvature matrices\nthrough a unified linear operator interface. We demonstrate with\n$\\textit{curvlinops}$ how this interface can hide complexity, simplify\napplications, be extensible and interoperable with other libraries, and scale\nto large NNs.",
        "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps:\/\/github.com\/KV-Park.",
        "Handling uncertainty is critical for ensuring reliable decision-making in\nintelligent systems. Modern neural networks are known to be poorly calibrated,\nresulting in predicted confidence scores that are difficult to use. This\narticle explores improving confidence estimation and calibration through the\napplication of bilevel optimization, a framework designed to solve hierarchical\nproblems with interdependent optimization levels. A self-calibrating bilevel\nneural-network training approach is introduced to improve a model's predicted\nconfidence scores. The effectiveness of the proposed framework is analyzed\nusing toy datasets, such as Blobs and Spirals, as well as more practical\nsimulated datasets, such as Blood Alcohol Concentration (BAC). It is compared\nwith a well-known and widely used calibration strategy, isotonic regression.\nThe reported experimental results reveal that the proposed bilevel optimization\napproach reduces the calibration error while preserving accuracy.",
        "Machine learning bias in mental health is becoming an increasingly pertinent\nchallenge. Despite promising efforts indicating that multitask approaches often\nwork better than unitask approaches, there is minimal work investigating the\nimpact of multitask learning on performance and fairness in depression\ndetection nor leveraged it to achieve fairer prediction outcomes. In this work,\nwe undertake a systematic investigation of using a multitask approach to\nimprove performance and fairness for depression detection. We propose a novel\ngender-based task-reweighting method using uncertainty grounded in how the\nPHQ-8 questionnaire is structured. Our results indicate that, although a\nmultitask approach improves performance and fairness compared to a unitask\napproach, the results are not always consistent and we see evidence of negative\ntransfer and a reduction in the Pareto frontier, which is concerning given the\nhigh-stake healthcare setting. Our proposed approach of gender-based\nreweighting with uncertainty improves performance and fairness and alleviates\nboth challenges to a certain extent. Our findings on each PHQ-8 subitem task\ndifficulty are also in agreement with the largest study conducted on the PHQ-8\nsubitem discrimination capacity, thus providing the very first tangible\nevidence linking ML findings with large-scale empirical population studies\nconducted on the PHQ-8.",
        "The integration of Federated Learning (FL) and Multi-Task Learning (MTL) has\nbeen explored to address client heterogeneity, with Federated Multi-Task\nLearning (FMTL) treating each client as a distinct task. However, most existing\nresearch focuses on data heterogeneity (e.g., addressing non-IID data) rather\nthan task heterogeneity, where clients solve fundamentally different tasks.\nAdditionally, much of the work relies on centralized settings with a server\nmanaging the federation, leaving the more challenging domain of decentralized\nFMTL largely unexplored. Thus, this work bridges this gap by proposing ColNet,\na framework designed for heterogeneous tasks in decentralized federated\nenvironments. ColNet divides models into the backbone and task-specific layers,\nforming groups of similar clients, with group leaders performing\nconflict-averse cross-group aggregation. A pool of experiments with different\nfederations demonstrated ColNet outperforms the compared aggregation schemes in\ndecentralized settings with label and task heterogeneity scenarios.",
        "With a growing data privacy concern, federated learning has emerged as a\npromising framework to train machine learning models without sharing locally\ndistributed data. In federated learning, local model training by multiple\nclients and model integration by a server are repeated only through model\nparameter sharing. Most existing federated learning methods assume training\ndeep learning models, which are often computationally demanding. To deal with\nthis issue, we propose federated learning methods with reservoir state analysis\nto seek computational efficiency and data privacy protection simultaneously.\nSpecifically, our method relies on Mahalanobis Distance of Reservoir States\n(MD-RS) method targeting time series anomaly detection, which learns a\ndistribution of reservoir states for normal inputs and detects anomalies based\non a deviation from the learned distribution. Iterative updating of statistical\nparameters in the MD-RS enables incremental federated learning (IncFed MD-RS).\nWe evaluate the performance of IncFed MD-RS using benchmark datasets for time\nseries anomaly detection. The results show that IncFed MD-RS outperforms other\nfederated learning methods with deep learning and reservoir computing models\nparticularly when clients' data are relatively short and heterogeneous. We\ndemonstrate that IncFed MD-RS is robust against reduced sample data compared to\nother methods. We also show that the computational cost of IncFed MD-RS can be\nreduced by subsampling from the reservoir states without performance\ndegradation. The proposed method is beneficial especially in anomaly detection\napplications where computational efficiency, algorithm simplicity, and low\ncommunication cost are required.",
        "Recent advancements in large language models (LLMs) reveal a perplexing\nphenomenon in continual learning: despite extensive training, models experience\nsignificant performance declines, raising questions about task alignment and\nunderlying knowledge retention. This study first explores the concept of\n\"spurious forgetting\", proposing that such performance drops often reflect a\ndecline in task alignment rather than true knowledge loss. Through controlled\nexperiments with a synthesized dataset, we investigate the dynamics of model\nperformance during the initial training phases of new tasks, discovering that\nearly optimization steps can disrupt previously established task alignments.\nOur theoretical analysis connects these shifts to orthogonal updates in model\nweights, providing a robust framework for understanding this behavior.\nUltimately, we introduce a Freezing strategy that fix the bottom layers of the\nmodel, leading to substantial improvements in four continual learning\nscenarios. Our findings underscore the critical distinction between task\nalignment and knowledge retention, paving the way for more effective strategies\nin continual learning.",
        "A tenet of reinforcement learning is that rewards are always observed by the\nagent. However, this is not true in many realistic settings, e.g., a human\nobserver may not always be able to provide rewards, a sensor to observe rewards\nmay be limited or broken, or rewards may be unavailable during deployment.\nMonitored Markov decision processes (Mon-MDPs) have recently been proposed as a\nmodel of such settings. Yet, Mon-MDP algorithms developed thus far do not fully\nexploit the problem structure, cannot take advantage of a known monitor, have\nno worst-case guarantees for ``unsolvable'' Mon-MDPs without specific\ninitialization, and only have asymptotic proofs of convergence. This paper\nmakes three contributions. First, we introduce a model-based algorithm for\nMon-MDPs that addresses all of these shortcomings. The algorithm uses two\ninstances of model-based interval estimation, one to guarantee that observable\nrewards are indeed observed, and another to learn the optimal policy. Second,\nempirical results demonstrate these advantages, showing faster convergence than\nprior algorithms in over two dozen benchmark settings, and even more dramatic\nimprovements when the monitor process is known. Third, we present the first\nfinite-sample bound on performance and show convergence to an optimal\nworst-case policy when some rewards are never observable.",
        "For a positive integer $n$ let $\\mathcal{X}_n$ be either the algebra $M_n$ of\n$n \\times n$ complex matrices, the set $N_n$ of all $n \\times n$ normal\nmatrices, or any of the matrix Lie groups $\\mathrm{GL}(n)$, $\\mathrm{SL}(n)$\nand $\\mathrm{U}(n)$. We first give a short and elementary argument that for two\npositive integers $m$ and $n$ there exists a continuous spectrum-shrinking map\n$\\phi : \\mathcal{X}_n \\to M_m$ (i.e.\\ $\\mathrm{sp}(\\phi(X))\\subseteq\n\\mathrm{sp}(X)$ for all $X \\in \\mathcal{X}_n$) if and only if $n$ divides $m$.\nMoreover, in that case we have the equality of characteristic polynomials\n$k_{\\phi(X)}(\\cdot) = k_{X}(\\cdot)^\\frac{m}{n}$ for all $X \\in \\mathcal{X}_n$,\nwhich in particular shows that $\\phi$ preserves spectra. Using this we show\nthat whenever $n \\geq 3$, any continuous commutativity preserving and\nspectrum-shrinking map $\\phi : \\mathcal{X}_n \\to M_n$ is of the form\n$\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$, for some $T\\in\n\\mathrm{GL}(n)$. The analogous results fail for the special unitary group\n$\\mathrm{SU}(n)$, and slightly more elaborate versions hold for the spaces of\nsemisimple elements in either $\\mathrm{GL}(n)$ or $\\mathrm{SL}(n)$, where a\nqualitatively new (and surprising) phenomenon arises: the map sending\n$SNS^{-1}$ to $S^{-1}NS$ for positive invertible $S$ and normal $N$ is also an\nexample. As a consequence, we also recover (a strengthened version of)\n\\v{S}emrl's influential characterization of Jordan automorphisms of $M_n$ via\npreserving properties.",
        "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https:\/\/github.com\/portal-cornell\/dial}.",
        "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
        "This paper proposes the DnD Filter, a differentiable filter that utilizes\ndiffusion models for state estimation of dynamic systems. Unlike conventional\ndifferentiable filters, which often impose restrictive assumptions on process\nnoise (e.g., Gaussianity), DnD Filter enables a nonlinear state update without\nsuch constraints by conditioning a diffusion model on both the predicted state\nand observational data, capitalizing on its ability to approximate complex\ndistributions. We validate its effectiveness on both a simulated task and a\nreal-world visual odometry task, where DnD Filter consistently outperforms\nexisting baselines. Specifically, it achieves a 25\\% improvement in estimation\naccuracy on the visual odometry task compared to state-of-the-art\ndifferentiable filters, and even surpasses differentiable smoothers that\nutilize future measurements. To the best of our knowledge, DnD Filter\nrepresents the first successful attempt to leverage diffusion models for state\nestimation, offering a flexible and powerful framework for nonlinear estimation\nunder noisy measurements.",
        "In this paper, we obtain the boundedness of $m$th order commutators generated\nby the $n$-dimensional fractional Hardy operator with rough kernel and its\nadjoint operator with BMO functions on two weighted grand Herz-Morrey spaces\nwith variable exponents. Replacing Lipschitz functions with BMO functions the\ncorresponding result is also given.",
        "Partially Controlled Multi-Agent Systems (PCMAS) are comprised of\ncontrollable agents, managed by a system designer, and uncontrollable agents,\noperating autonomously. This study addresses an optimal composition design\nproblem in PCMAS, which involves the system designer's problem, determining the\noptimal number and policies of controllable agents, and the uncontrollable\nagents' problem, identifying their best-response policies. Solving this\nbi-level optimization problem is computationally intensive, as it requires\nrepeatedly solving multi-agent reinforcement learning problems under various\ncompositions for both types of agents. To address these challenges, we propose\na novel hypernetwork-based framework that jointly optimizes the system's\ncomposition and agent policies. Unlike traditional methods that train separate\npolicy networks for each composition, the proposed framework generates policies\nfor both controllable and uncontrollable agents through a unified hypernetwork.\nThis approach enables efficient information sharing across similar\nconfigurations, thereby reducing computational overhead. Additional\nimprovements are achieved by incorporating reward parameter optimization and\nmean action networks. Using real-world New York City taxi data, we demonstrate\nthat our framework outperforms existing methods in approximating equilibrium\npolicies. Our experimental results show significant improvements in key\nperformance metrics, such as order response rate and served demand,\nhighlighting the practical utility of controlling agents and their potential to\nenhance decision-making in PCMAS.",
        "A study of the Revista General de Informacion y Documentacion, from 2005 to\n2022. The objective is aimed at qualifying the structure of the research field\nand assessing the trajectory of the thematic areas covered. Applying as\nmethodology the analysis of co-words, the construction of bibliometric networks\nand the creation of scientific maps. 514 documents are extracted from the Web\nof Science (WoS) database. The keywords assigned by the authors of the\ndocuments are selected and divided into three subperiods: 2005-2010, 2011-2016\nand 2017-2022. In the results, 1701 author keywords and 37 bibliometric\nnetworks are obtained. In the period 2005-2010, the structure of the research\nfield is represented on the scientific map with very few central and\nspecialized topics, considering an initial and underdeveloped organization. In\nthe period 2011-2016, the structure of the research field is distributed on the\nscientific map with a more varied number of central and specialized topics, but\nstill insufficient, considering an organization in the process of development.\nIn the period 2017-2022, the structure of the research field is shown on the\nmap with all kinds of family of topics (central, specialized, transversal,\nemerging or disappearing), being valued as a dynamic, complex and heterogeneous\norganization. Regarding the evolution of the thematic areas, the map shows\nsolid progress between the last two periods. The morphology of the thematic\nfield treated in RGID is outlined in three phases: foundation, process of\ndevelopment and consolidation.",
        "In the book [Meurant and Tichy, SIAM, 2024] we discussed the estimation of\nerror norms in the conjugate gradient (CG) algorithm for solving linear systems\n$Ax=b$ with a symmetric positive definite matrix $A$, where $b$ and $x$ are\nvectors. In this paper, we generalize the most important formulas for\nestimating the $A$-norm of the error to the block case. First, we discuss in\ndetail the derivation of various variants of the block CG (BCG) algorithm from\nthe block Lanczos algorithm. We then consider BCG and derive the related block\nGauss and block Gauss-Radau quadrature rules. We show how to obtain lower and\nupper bounds on the $A$-norm of the error of each system, both in terms of the\nquantities computed in BCG and in terms of the underlying block Lanczos\nalgorithm. Numerical experiments demonstrate the behavior of the bounds in\npractical computations.",
        "Transcription factors are proteins that regulate the expression of genes by\nbinding to specific genomic regions known as Transcription Factor Binding Sites\n(TFBSs), typically located in the promoter regions of those genes. Accurate\nprediction of these binding sites is essential for understanding the complex\ngene regulatory networks underlying various cellular functions. In this regard,\nmany deep learning models have been developed for such prediction, but there is\nstill scope of improvement. In this work, we have developed a deep learning\nmodel which uses pre-trained DNABERT, a Convolutional Neural Network (CNN)\nmodule, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale\nConvolutions with Attention (MSCA) module and an output module. The pre-trained\nDNABERT is used for sequence embedding, thereby capturing the long-term\ndependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are\nuseful in extracting higher-order local features. TFBS-Finder is trained and\ntested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies\nas well as cross-cell line validations and comparisons with other models. The\nexperimental results show the superiority of the proposed method in predicting\nTFBSs compared to the existing methodologies. The codes and the relevant\ndatasets are publicly available at\nhttps:\/\/github.com\/NimishaGhosh\/TFBS-Finder\/.",
        "Video-conferencing applications face an unwavering surge in traffic,\nstressing their underlying infrastructure in unprecedented ways. This paper\nrethinks the key building block for conferencing infrastructures -- selective\nforwarding units (SFUs). SFUs relay and adapt media streams between\nparticipants and, today, run in software on general-purpose servers. Our main\ninsight, discerned from dissecting the operation of production SFU servers, is\nthat SFUs largely mimic traditional packet-processing operations such as\ndropping and forwarding. Guided by this, we present Scallop, an SDN-inspired\nSFU that decouples video-conferencing applications into a hardware-based data\nplane for latency-sensitive and frequent media operations, and a software\ncontrol plane for the (infrequent) remaining tasks, such as analyzing feedback\nsignals. Our Tofino-based implementation fully supports WebRTC and delivers\n7-210 times improved scaling over a 32-core commodity server, while reaping\nperformance improvements by cutting forwarding-induced latency by 26 times.",
        "Labeled transition systems can be a great way to visualize the complex\nbehavior of parallel and communicating systems. However, if, during a\nparticular timeframe, no synchronization or communication between processes\noccurs, then multiple parallel sequences of actions are able to interleave\narbitrarily, and the resulting graph quickly becomes too complex for the human\neye to understand easily. With that in mind, we propose an exact formalization\nof these arbitrary interleavings, and an algorithm to find all said\ninterleavings in deterministic LTSs, to reduce the visual complexity of labeled\ntransition systems.",
        "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
        "Compared with line-of-sight (LOS) communication, nonline-of-sight (NLOS)\nunderwater wireless optical communication (UWOC) systems have garnered\nextensive attention because of their heightened suitability for the intricate\nand dynamic underwater environment. In the NLOS channel, photons can reach the\nreceiver by sea surface reflection or particle scattering. However, research\nlacks comprehensive channel models that incorporate sea surface reflection and\nparticle scattering. Moreover, the presence of ocean turbulence introduces\nrandom fluctuations in the received optical signal based on the average light\nintensity. Consequently, this paper adopts the Monte Carlo simulation method\n(MCS) to solve the fading-free impulse response of the joint\nreflection-scattering channel. Furthermore, a weighted double gamma function\n(WDGF) is proposed to characterize the channel impulse response (CIR). Based on\nthe closed CIR model, the average bit error rate and the performance of the\ninterruption probability of the UWOC system under turbulence are analyzed. The\nconclusions obtained are intended to assist in the design and performance\nevaluation of NLOS UWOC systems.",
        "This paper presents a novel AI-based smart traffic management system\nde-signed to optimize traffic flow and reduce congestion in urban environments.\nBy analysing live footage from existing CCTV cameras, this approach eliminates\nthe need for additional hardware, thereby minimizing both deployment costs and\nongoing maintenance expenses. The AI model processes live video feeds to\naccurately count vehicles and assess traffic density, allowing for adaptive\nsignal control that prioritizes directions with higher traffic volumes. This\nreal-time adaptability ensures smoother traffic flow, reduces congestion, and\nminimizes waiting times for drivers. Additionally, the proposed system is\nsimulated using PyGame to evaluate its performance under various traffic\nconditions. The simulation results demonstrate that the AI-based system\nout-performs traditional static traffic light systems by 34%, leading to\nsignificant improvements in traffic flow efficiency. The use of AI to optimize\ntraffic signals can play a crucial role in addressing urban traffic challenges,\noffering a cost-effective, scalable, and efficient solution for modern cities.\nThis innovative system represents a key advancement in the field of smart city\ninfra-structure and intelligent transportation systems.",
        "We perform a detailed investigation of the CCDM (creation of cold dark\nmatter) cosmologies using the powerful techniques of qualitative analysis of\ndynamical systems. Considering a wide variety of the creation rates ranging\nfrom constant to dynamical, we examine the nature of critical points and their\nstability obtained from the individual scenario consisting of only cold dark\nmatter, or cold dark matter plus a second fluid with constant equation of\nstate. According to our analyses, these scenarios predict unstable dark matter\ndominated critical points, stable accelerating attractors dominated either by\ndark matter or the second fluid, scaling attractors in which both dark matter\nand the second fluid co-exist. Along with these critical points, these\nscenarios also indicate the possibility of decelerating attractors or\ndecelerating scaling attractors in the future which are new results in this\ndirection. These altogether suggest that CCDM cosmologies are viable\nalternatives to the mainstream cosmological models."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Physics guided neural networks for spatio-temporal superresolution of turbulent flows",
    "start_abstract":"Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Model-free simulations of turbulent reactive flows"
      ],
      "abstract":[
        "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
      ],
      "categories":[
        "physics.flu-dyn"
      ]
    },
    "list":{
      "title":[
        "Bioinspired Drone Rotors for Reduced Aeroacoustic Noise and Improved\n  Efficiency",
        "Insights of Transitions to Thermoacoustic Instability in Inverse\n  Diffusion Flame using Multifractal Detrended Fluctuation Analysis",
        "On the role of morphology and kinematics of biological swimmers to\n  spread and suppress their odors in the wake",
        "On the Separating Flow Behind a Cylinder: Insights from the Principle of\n  Minimum Pressure Gradient",
        "The origin of vorticity in viscous incompressible flows",
        "Hydrodynamically Beneficial School Configurations in Carangiform\n  Swimmers: Insights from a Flow-Physics Informed Model",
        "UV Radiation Measurement of Nitrogen in shock focusing facility",
        "Investigation of hydrogen under-expanded jets in gaseous propulsion\n  systems",
        "A toy model of turbulent shear flow using vortons",
        "An aerodynamic measurement system to improve the efficiency of wind\n  turbine rotor blades",
        "A Numerical Investigation of Particle Deposition on a Substrate",
        "Reduced Basis Model for Compressible Flow",
        "Bayesian Optimization of the GEKO Turbulence Model for Predicting Flow\n  Separation Over a Smooth Surface",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Freelance Holography, Part II: Moving Boundary in Gauge\/Gravity\n  Correspondence",
        "Dynamically Learning to Integrate in Recurrent Neural Networks",
        "Automated DC Voltage and DC Resistance Real-time Multiple Standard for\n  Artifact Calibration of Calibrators and multimeters",
        "Two almost planetary mass survivors of common envelope evolution",
        "Electrochemically induced hyperfluorescence based on the formation of\n  charge-transfer excimers",
        "Computation of generalised magnetic coordinates asymptotically close to\n  the separatrix",
        "The diffuse extragalactic gamma-ray background radiation: star-forming\n  galaxies are not the dominant component",
        "Tight Analysis of Difference-of-Convex Algorithm (DCA) Improves\n  Convergence Rates for Proximal Gradient Descent",
        "An Open Source Validation System for Continuous Arterial Blood Pressure\n  Measuring Sensors",
        "Updated analysis of neutron magnetic form factor and the nucleon\n  transverse densities",
        "Power residue symbols and the exponential local-global principle",
        "In-medium bottomonium properties from lattice NRQCD calculations with\n  extended meson operators",
        "MXMap: A Multivariate Cross Mapping Framework for Causal Discovery in\n  Dynamical Systems",
        "Adsorption Behavior of Greenhouse Gases on Carbon Nanobelts: A\n  Semi-Empirical Tight-Binding Approach for Environmental Application"
      ],
      "abstract":[
        "The application of unmanned aerial vehicles (UAVs) is surging across several\nindustries, paralleled by growing demand for these UAVs. However, the noise\nemitted by UAVs remains a significant impediment to their widespread use even\nthough in areas such as product delivery, they can be more environmentally\nfriendly than traditional delivery methods. Nature has often been a source of\ninspiration for devices that are efficient and eco-friendly. In the current\nstudy, we leverage the previous work by Seo et al. (Bioinsp. Biomimetics, 16\n(4):046019, 2021) on the aeroacoustics of flapping wing flight in mosquitoes\nand fruit flies to propose and examine a simple strategy for reducing the\naeroacoustic noise from drone rotors. In particular, inspired by these insects,\nwe explore how an increase in the planform area of the rotor could be used to\nreduce the rotation rate and the associated aeroacoustic noise from small-scale\nrotors. The study employs a sharp-interface immersed boundary solver for the\nflow simulations and the aeroacoustic sound is predicted by the Ffowcs\nWilliams-Hawkings equation. Simulations indicate that the simple strategy of\nemploying rotors with larger planform areas could lead not just to reduced\naeroacoustic noise but improved power economy as well.",
        "The inverse diffusion flame (IDF) can experience thermoacoustic instability\ndue to variations in power input or flow conditions. However, the dynamical\ntransitions in IDF that lead to this instability when altering control\nparameters have not been thoroughly investigated. In this study, we explore the\ncontrol parameters through two different approaches and employ multifractal\ndetrended fluctuation analysis to characterize the transitions observed prior\nto the onset of thermoacoustic instability in the inverse diffusion flame. Our\nfindings reveal a loss of multifractality near the region associated with\nthermoacoustic instability, which suggests a more ordered behavior. We\ndetermine that the singularity exponent, the width of the multifractal\nspectrum, and the Hurst exponent are reliable indicators of thermoacoustic\ninstability and serve as effective classifiers of dynamical states in inverse\ndiffusion flames.",
        "Understanding the interplay between hydrodynamics and chemical sensing in\naquatic environments is crucial for unraveling biological swimmers' navigation,\nforaging, and communication strategies. This study investigates the role of\nkinematics and morphologies of fish in dispersion and suppression of odor cues\nin their wake. We employ high-fidelity three-dimensional computational fluid\ndynamics simulations, integrating a sharp-interface immersed-boundary method\nwith an odor transport model. Using carangiform and anguilliform kinematics for\na jackfish and an eel, we analyze the transport of chemical cues in the wake of\nundulatory swimmers at a Reynolds number of 3000 and Strouhal numbers of 0.25\nand 0.4. Our findings reveal that odor plumes closely align with vortex\nstructures, emphasizing a strong coupling between hydrodynamics and chemical\ndispersion. We demonstrate that kinematics, rather than morphology,\npredominantly govern odor transport, with anguilliform motion generating\nbroader, more persistent odor trails. Increasing the amplitude of undulation\nimproves the effectiveness of the odor, driven primarily by convection, while\ndiffusion plays a secondary role. These insights provide a deeper understanding\nof underwater sensing mechanisms and inform the design of bio-inspired robotic\nsystems with improved navigation and chemical detection capabilities.",
        "We study the separating flow over a circular cylinder with two objectives:\n(i) to demonstrate the validity of the condition of matching curvature, and\n(ii) to obtain a reasonable estimate of the separation angle in the subcritical\nregime (Re=10^4-10^5) without explicitly modeling the boundary layer. First, we\nstudy Roshko's free streamline model (1954); it is an ideal flow model with\nsheets of discontinuities that represent the separating shear layers in the\nnear wake region. The model fails to predict the correct separation angle over\na cylinder. Roshko attributed this discrepancy to the condition of matching\ncurvature, which asserts that the curvature of the separating streamline at the\nseparation point must match that of the cylinder. We show that such a condition\nis legitimate and is not the real culprit for the failure of Roshko's model in\npredicting separation. Second, we employ the principle of minimum pressure\ngradient (PMPG), which asserts that, an incompressible flow evolves by\nminimizing the total magnitude of the pressure gradient over the domain.\nEncouraged by the fact that the flow characteristics in the range Re=10^4-10^5\nare fairly independent of Re, we aim to predict the separation angle in this\nregime without modeling the boundary layer -- a task that may seem impossible,\nthough anticipated by Prandtl in his seminal paper (Prandtl 1904). Over the\nfamily of kinematically-admissible, equilibrium flows, we utilize the PMPG to\nsingle out the separating flow with the minimum pressure gradient cost.\nInterestingly, the obtained separation angles match experimental measurements\nover the regime Re=10^4-10^5.",
        "In inviscid, incompressible flows, the evolution of vorticity is exactly\nequivalent to that of an infinitesimal material line-element, and hence\nvorticity can be traced forward or backward in time in a Lagrangian fashion.\nThis elegant and powerful description is not possible in viscous flows due to\nthe action of diffusion. Instead, a stochastic Lagrangian interpretation is\nrequired and was recently introduced, where the origin of vorticity at a point\nis traced back in time as an expectation over the contribution from stochastic\ntrajectories. We herein introduce for the first time an Eulerian, adjoint-based\napproach to quantify the back-in-time origin of vorticity in viscous,\nincompressible flows. The adjoint variable encodes the advection, tilting and\nstretching of the earlier-in-time vorticity that ultimately leads to the target\nvalue. Precisely, the adjoint vorticity is the volume-density of the mean\nLagrangian deformation of the earlier vorticity. The formulation can also\naccount for the injection of vorticity into the domain at solid boundaries. We\ndemonstrate the mathematical equivalence of the adjoint approach and the\nstochastic Lagrangian approach. We then provide an example from turbulent\nchannel flow, where we analyze the origin of high-stress events and relate them\nto Lighthill's mechanism of stretching of near-wall vorticity.",
        "Researchers have long debated which spatial arrangements and swimming\nsynchronizations are beneficial for the hydrodynamic performance of fish in\nschools. In our previous work (Seo and Mittal, Bioinsp. Biomim., Vol. 17,\n066020, 2022), we demonstrated using direct numerical simulations that\nhydrodynamic interactions with the wake of a leading body-caudal fin\ncarangiform swimmer could significantly enhance the swimming performance of a\ntrailing swimmer by augmenting the leading-edge vortex (LEV) on its caudal fin.\nIn this study, we develop a model based on the phenomenology of LEV\nenhancement, which utilizes wake velocity data from direct numerical\nsimulations of a leading fish to predict the trailing swimmer's hydrodynamic\nperformance without additional simulations. This approach enables a\ncomprehensive analysis of the effects of relative positioning, phase\ndifference, flapping amplitude, Reynolds number, and the number of swimmers in\nthe school on thrust enhancement. The results offer several insights regarding\nthe effect of these parameters that have implications for fish schools as well\nas for bio-inspired underwater vehicle applications.",
        "A study on UV radiation in shock focused gas has been carried out in high\ntemperature radiating Air experimentally. Spherical shock wave focusing was\nachieved with the help of a contoured converging section attached to a shock\ntube. The measured radiation is compared with SPECAIR software to find out that\nthe radiation corresponds to the emission of N2+ Meinal transition. The\ntemperature of the radiating gas was estimated from SPECAIR and was found to be\naround 6000K.",
        "Underexpanded jets are present in various engineering applications; in recent\nyears, they have gained special attention because of the development of\ngaseous-fueled propulsion systems. In these apparatuses, the direct injection\nof fuels such as hydrogen in innovative low-emission engines' chambers induces\nturbulent under-expanded jets. In this study, we performed high-fidelity Large\nEddy Simulations of under-expanded hydrogen jets to investigate these flows and\nprovide valuable insights for developing injectors suitable for hydrogen and,\nmore generally, gaseous-fueled propulsion systems. We initially assessed the\nmethod's accuracy, evaluating the convergence and uncertainty of the numerical\nresults and validating them against experimental particle image velocimetry and\nSchlieren data. The simulated jets, the Mach disc dimensions, and the resulting\nvelocity field align closely with the experimental observations. Then we\nanalyzed the jet structure for pressure ratios of 4 to 25 and examined the\neffects of the geometrical configuration of the nozzle on the characteristics\nof the air-fuel mixture obtained. We compared the jets resulting from a\nround-hole nozzle with annular ones resembling outward-opening injectors.",
        "We introduce a novel toy model for shear flows, exploiting the spatial\nintermittency and the scale separation between large-scale flows and\nsmall-scale structures. The model is highly sparse, focusing exclusively on the\nmost intense structures, which are represented by vortons: dynamically\nregularized quasi-singularities that experience rapid distortion from the\nlarge-scale shear. The vortons, in turn, influence the large-scale flow through\nthe sub-grid stress tensor. Despite its simplicity, the model displays an\ninteresting transition between two distinct regimes: (i) a laminar regime,\nwhere dissipation is entirely attributed to the large-scale flow, and the\nvortons dynamics is essentially diffusive, and (ii) a turbulent regime, in\nwhich most of the dissipation arises from the vortons. These regimes correspond\nto different scalings of dissipation and the Grashof number as functions of the\nReynolds number, with power-law relationships that resemble those observed in\nclassical turbulence.",
        "The wind energy sector is growing rapidly with the installation of wind\nturbines with long, slender blades in a diverse range of locations. To enhance\nthe operational performance under specific wind conditions and to validate the\naerodynamic design of flexible blades, it is crucial to obtain comprehensive\ndata on the aerodynamic behaviour of the blades in the field, such as\ntime-resolved pressure distributions, local inflow conditions and dynamic\nresponses of the blade. However, published field measurements are scarce for\nlarge-scale rotor blades due to the complex and costly installation of the\nrequisite measurement systems. In recent work, we developed a wireless and\nself-sufficient aerodynamic measurement system, named Aerosense, which is less\ncomplex and costly than conventional aerodynamic measurement systems. The\nAerosense system uses Micro-Electro-Mechanical Systems (MEMS) sensors to obtain\nlocal aerodynamic pressures, blade motions, and inflow conditions. In this\npaper, we demonstrate the value of Aerosense in understanding the aerodynamic\nbehaviour of rotor blades, using a 7kW wind turbine operating in the field.\nAfter a thorough calibration and correction process, we demonstrate, for\nexample, that the pressure distribution can vary significantly during one\nrotation of the blade, even under stable wind conditions. These variations are\nfound to be due to the misalignment of the wind direction with the wind\nturbine's rotational axis. We therefore conclude that the Aerosense measurement\nsystem is valuable for understanding the aerodynamic loading on rotor blades as\nwell as the influence of the inflow conditions on wind turbine performance.",
        "The deposition of nanometer-scale particles is of significant interest in\nvarious industrial processes. While these particles offer several advantages,\ntheir deposition can have detrimental effects, such as reducing the heat\ntransfer efficiency in nanofluid-based battery cooling systems. In this study,\nwe investigated particle deposition around different square substrate\nconfigurations as well as experimentally obtained complex porous structure in a\ntwo-dimensional setup. The particles modeled as a concentration field using the\nlattice Boltzmann method, with a given external flow following a parabolic\nprofile. Our results revealed that particle deposition around a substrate\nincreases with higher fluid velocity, greater particle concentration, and\nhigher deposition probability. Additionally, placing multiple number of\nsubstrates in the channel resulted in increased deposition on upstream\nsubstrates compared to downstream ones. As particle deposition around upstream\nsubstrates increases, it eventually obstructs the flow to downstream regions,\nthereby affecting the overall system performance. The insights gained from this\nsimplified model of particle deposition will play a crucial role in advancing\nour understanding of deposition processes in complex systems.",
        "Numerical simulations are a valuable research and layout tool for fluid flow\nproblems, yet repeated evaluations of parametrized problems, necessary to solve\noptimization problems, can be very costly. One option to speed up this process\nis to replace the costly CFD model with a cheaper one. These surrogate models\ncan be either data-driven or they can also rely on reduced basis (RB) methods\nto speed up the calculations. In contrast to data-driven surrogate models, the\nlatter are not based on regression techniques but are still aimed at explicitly\nsolving the conservation equations. Their speed-up comes from a strong\nreduction of the solution space, which results in much smaller algebraic\nsystems that need to be solved. Within this work, an RB model, suited for\nslightly compressible flow, is presented and tested on different flow\nconfigurations. The model is stabilized using a Petrov-Galerkin method with\ntrial and test function spaces of different dimensionality to generate stable\nresults for a wide range of Reynolds numbers. The presented model applies to\ngeometrically and physically parametrized flow problems. Finally, a data-driven\napproach was used to extend it to turbulent flows.",
        "This paper applies Bayesian-optimization-RANS (turbo-RANS) to improve\nReynolds-averaged Navier-Stokes (RANS) turbulence models for a\nconverging-diverging channel, a case with adverse pressure gradients and flow\nseparation. Using Bayesian optimization, the Generalized $k$-$\\omega$ (GEKO)\nmodel was calibrated by tuning $C_\\text{SEP}$ and $C_\\text{NW}$ with sparse\ndirect numerical simulation (DNS) data at $Re = 12,600$. The calibration\nfollowed the Generalized Error Distribution-based Calibration Procedure\n(GEDCP), optimizing coefficients based on pressure recovery ($C_p$) and skin\nfriction ($C_f$). The optimized model was evaluated beyond training data.\nStreamwise velocity ($U$) predictions at $Re = 12,600$ were compared to DNS to\nassess improvements in $C_p$ and $C_f$. To test robustness, comparisons were\nmade against large-eddy simulation (LES) data at $Re = 20,580$ for velocity and\nskin friction. Results show that optimized GEKO (turbo-RANS) improves wall\nquantity predictions, particularly reattachment. Improved velocity profiles at\nboth Reynolds numbers suggest Bayesian-optimized coefficients enhance adverse\npressure gradient modeling. The model retains accuracy across different $Re$,\nshowing turbo-RANS' potential in turbulence model corrections that generalize\nacross flows. While skin friction predictions showed limited improvement due to\nconstraints of two-equation models, this study highlights the role of machine\nlearning-assisted RANS calibration in improving predictive accuracy for complex\nflows. The results suggest optimized coefficients from a single dataset can be\napplied across moderate $Re$ variations, improving turbo-RANS' applicability\nfor turbulence model tuning.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "We continue developing the freelance holography program, formulating\ngauge\/gravity correspondence where the gravity side is formulated on a space\nbounded by a generic timelike codimension-one surface inside AdS and arbitrary\nboundary conditions are imposed on the gravity fields on the surface. Our\nanalysis is performed within the Covariant Phase Space Formalism (CPSF). We\ndiscuss how a given boundary condition on the bulk fields on a generic boundary\nevolves as we move the boundary to another boundary inside AdS and work out how\nthis evolution is encoded in deformations of the holographic boundary theory.\nOur analyses here extend the extensively studied T$\\bar{\\text{T}}$-deformation\nby relaxing the boundary conditions at asymptotic AdS or at the cutoff surface\nto be any arbitrary one (besides Dirichlet). We discuss some of the\nimplications of our general freelance holography setting.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience.",
        "An automated temperature-controlled electrical DC voltage and DC resistance\nmultiple reference standard (MRS) has been developed by Measurements\nInternational (MI) with the scientific support from the Istituto Nazionale di\nRicerca Metrologica (INRIM). The MRS includes a 10 V, a 1 {\\Omega}, and a 10\nk{\\Omega} standards selectable via a switch unit. This setup allows the\nartifact calibration of high-end calibrators and multimeters used in\nlow-frequency electrical measurements. The two resistors are high-stability\nstandards from MI, while the 10 V standard is based on a low-noise circuit\ndeveloped by INRIM in collaboration with MI. A key innovation is the internal\nreal-time clock calendar, which displays the calibration values of the MRS\nstandards and their updated values internally calculated. This ensures reliable\nuse of the MRS standards over extended periods between calibrations,\neffectively minimizing uncertainties due to their drift. The standards are\nhoused in a thermal box, minimizing temperature variations. The MRS standards\nmeet the uncertainty requirements defined by calibrators and multimeters\nmanufacturers for artifact calibration and can also serve as laboratory\nreferences or travelling standards for interlaboratory comparisons (ILCs). MI\nis currently commercializing the MRS.",
        "White dwarfs are often found in close binaries with stellar or even\nsubstellar companions. It is generally thought that these compact binaries form\nvia common envelope evolution, triggered by the progenitor of the white dwarf\nexpanding after it evolved off the main-sequence and engulfing its companion.\nTo date, a handful of white dwarfs in compact binaries with substellar\ncompanions have been found, typically with masses greater than around 50\nM$_\\mathrm{Jup}$. Here we report the discovery of two eclipsing white dwarf\nplus brown dwarf binaries containing very low mass brown dwarfs. ZTF J1828+2308\nconsists of a hot ($15900\\pm75$ K) $0.610\\pm0.004$ M$_{\\odot}$ white dwarf in a\n2.7 hour binary with a $0.0186\\pm0.0008$ M$_{\\odot}$ ($19.5\\pm0.8$\nM$_\\mathrm{Jup}$) brown dwarf. ZTF J1230$-$2655 contains a cool ($10000\\pm110$\nK) $0.65\\pm0.02$ M$_{\\odot}$ white dwarf in a 5.7 hour binary with a companion\nthat has a mass of less than 0.0211 M$_{\\odot}$ (22.1 M$_\\mathrm{Jup}$). While\nthe brown dwarf in ZTF J1828+2308 has a radius consistent with its mass and\nage, ZTF J1230$-$2655 contains a roughly 20 per cent overinflated brown dwarf\nfor its age. We are only able to reconstruct the common envelope phase for\neither system if it occurred after the first thermal pulse, when the white\ndwarf progenitor had already lost a significant fraction of its original mass.\nThis is true even for very high common envelope ejection efficiencies\n($\\alpha_\\mathrm{CE}\\sim 1$), unless both systems have extremely low\nmetallicities. It may be that the lowest mass companions can only survive a\ncommon envelope phase if it occurs at this very late stage.",
        "Despite the extensive use of electrochemiluminescence in sensing\napplications, its potential in lighting and display technology has been\nconstrained by the low luminance and short operational lifetime of\nelectrochemiluminescence devices (ECLDs). Here, we demonstrate a substantial\nenhancement in the luminance, efficiency, and operational longevity of ECLDs by\nintroducing electrochemically induced hyperfluorescence (ECiHF) via\nelectrogeneration of charge-transfer (CT) excimers and subsequent energy\ntransfer to fluorescent acceptors. By assuming a double-decker arrangement of\nthe electron donor and acceptor groups, the molecule TpAT-tFFO supports\nsolution-state thermally activated delayed fluorescence from a CT excimer state\nand efficient energy transfer to the rubrene dye TBRb. Optimized ECLDs based on\nthis material combination achieve an unprecedented luminance of 6,220 cd\/m2 and\ntheir operational lifetime (LT50) at an initial luminance of 100 cd\/m2 exceeds\n20 minutes, more than 10-fold longer than other ECLDs with meaningful\nefficiency or brightness. We identify energy level alignment between the\nexcimer and the emitter as a crucial factor for efficient ECiHF. In mixtures\nwith energy gaps > 0.5 eV, electron transfer results in reduced performance and\nrenders the operation strongly dependent on applied voltage and frequency. By\ncontrast, spectroelectrochemical analysis reveals that devices with favorable\nenergy level alignment operate on a pure excimer mechanism across a wide range\nof frequencies. These findings highlight the innovative potential of ECiHF in\nimproving the performance of ECLD, which can be widely applied in future\ncommercial lighting solutions.",
        "Integrals to calculate generalised magnetic coordinates from an input\nmagnetic flux function asymptotically close to the separatrix are presented,\nand implemented in the GPEC\/DCON code suite. These integrals allow\ncharacterisation of the magnetic equilibrium of a diverted tokamak, in magnetic\ncoordinates, arbitrarily close to the last closed flux surface, avoiding the\nnumerical issues associated with calculating diverging field line integrals\nnear a magnetic x-point. These methods provide an important first step in the\ndevelopment of robust asymptotic equilibrium behaviour for spectral 3D MHD\ncodes at the separatrix.",
        "Star-forming galaxies (SFGs) are considered to be an important component of\nthe diffuse extragalactic gamma-ray background (EGB) radiation observed in 0.1\n-- 820 GeV, but their quantitative contribution has not yet been precisely\ndetermined. In this study, we aim to provide the currently most reliable\nestimate of the contribution of SFGs based on careful calibration with\ngamma-ray luminosities of nearby galaxies and physical quantities (star\nformation rate, stellar mass, and size) of galaxies observed by high-redshift\ngalaxy surveys. Our calculations are based on the latest database of particle\ncollision cross-sections and energy spectra of secondary particles, and take\ninto account not only hadronic but also leptonic processes with various\nradiation fields in a galaxy. We find that SFGs are not the dominant component\nof the unresolved EGB measured by Fermi; the largest contribution is around 50%\n-- 60% in the 1 -- 10 GeV region, and the contribution falls rapidly in lower\nand higher energy ranges. This result appears to contradict a previous study,\nwhich claimed that SFGs are the dominant component of the unresolved EGB, and\nthe origin of the discrepancy is examined. In calculations of cosmic-ray\nproduction, propagation, and interaction in a galaxy, we try models developed\nby two independent groups and find that they have little impact on EGB.",
        "We investigate a difference-of-convex (DC) formulation where the second term\nis allowed to be weakly convex. We examine the precise behavior of a single\niteration of the difference-of-convex algorithm (DCA), providing a tight\ncharacterization of the objective function decrease, distinguishing between six\ndistinct parameter regimes.\n  Our proofs, inspired by the performance estimation framework, are notably\nsimplified compared to related prior research. We subsequently derive sublinear\nconvergence rates for the DCA towards critical points, assuming at least one of\nthe functions is smooth.\n  Additionally, we explore the underexamined equivalence between proximal\ngradient descent (PGD) and DCA iterations, demonstrating how DCA, a\nparameter-free algorithm, without the need for a stepsize, serves as a tool for\nstudying the exact convergence rates of PGD.",
        "Measuring the blood pressure waveform is becoming a more frequently studied\narea. The development of sensor technologies opens many new ways to be able to\nmeasure high-quality signals. The development of such an aim-specific sensor\ncan be time-consuming, expensive, and difficult to test or validate with known\nand consistent waveforms. In this paper, we present an open source blood\npressure waveform simulator with an open source Python validation package to\nreduce development costs for early-stage sensor development and research. The\nsimulator mainly consists of 3D printed parts which technology has become a\nwidely available and cheap solution. The core part of the simulator is a 3D\nprinted cam that can be generated based on real blood pressure waveforms. The\nvalidation framework can create a detailed comparison between the signal\nwaveform used to design the cam and the measured time series from the sensor\nbeing validated. The presented simulator proved to be robust and accurate in\nshort- and long-term use, as it produced the signal waveform consistently and\naccurately. To validate this solution, a 3D force sensor was used, which was\nproven earlier to be able to measure high-quality blood pressure waveforms on\nthe radial artery at the wrist. The results showed high similarity between the\nmeasured and the nominal waveforms, meaning that comparing the normalized\nsignals, the RMSE value ranged from $0.0276 \\pm 0.0047$ to $0.0212 \\pm 0.0023$,\nand the Pearson correlation ranged from $0.9933 \\pm 0.0027$ to $0.9978 \\pm\n0.0005$. Our validation framework is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim. Our hardware framework, which allows\nreproduction of the presented solution, is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim-hardware. The entire design is an open\nsource project and was developed using free software.",
        "We provide an updated global extraction of the neutron magnetic form factor,\nincluding new extractions from $^3$H-$^3$He comparisons at Jefferson Lab. Our\nnew global fit addresses discrepancies between previous data sets at modest\nmomentum transfer by separating the uncertainties from world data into\nnormalization and uncorrelated uncertainties. We use this updated global fit,\nalong with previous fits for the other form factors, to extract the neutron and\nproton transverse charge and magnetization densities and their uncertainties.",
        "The exponential local-global principle, or Skolem conjecture, says: Suppose\nthat \\(b\\) is a positive integer, and that the sequence \\((u_{n})_{n =\n-\\infty}^{\\infty}\\) is such that every term is in \\(\\mathbb{Z}[1\/b]\\), the\nlinear recurrence \\(u_{n + d} = a_{1}u_{n + d - 1} + \\cdots + a_{d}u_{n}\\)\nholds for all integers \\(n\\), and every root of \\(x^{d} - a_{1}x^{d - 1} -\na_{2}x^{d - 2} - \\cdots - a_{d}\\) is nonzero and simple; then there is no zero\nterm \\(u_{n}\\) if and only if, for some integer \\(m\\) that is larger than \\(1\\)\nand relatively prime to \\(b\\), every term \\(u_{n}\\) is not in\n\\(m\\mathbb{Z}[1\/b]\\).\n  Particular cases of the conjecture are known, but the general conjecture is\nopen. This paper proves some apparently new quadratic and degenerate cubic\ncases of the exponential local-global principle via power residue symbols.\n  This work was presented at the Stellenbosch Number Theory Conference 2025 in\nJanuary 2025 at Stellenbosch University; much of the work was also presented at\nthe 67th Annual Congress of the South African Mathematical Society in December\n2024 at the University of Pretoria.",
        "We calculate the temperature dependence of bottomonium correlators in\n(2+1)-flavor lattice QCD with the aim to constrain in-medium properties of\nbottomonia at high temperature. The lattice calculations are performed using\nHISQ action with physical strange quark mass and light quark masses twenty\ntimes smaller than the strange quark mass at two lattice spacings $a=0.0493$ fm\nand $0.0602$ fm, and temporal extents $N_{\\tau}=16-30$, corresponding to the\ntemperatures $T=133-250$ MeV. We use a tadpole-improved NRQCD action including\nspin-dependent $v^6$ corrections for the heavy quarks and extended meson\noperators in order to be sensitive to in-medium properties of the bottomonium\nstates of interest. We find that within estimated errors the bottomonium masses\ndo not change compared to their vacuum values for all temperatures under our\nconsideration; however, we find different nonzero widths for the various\nbottomonium states.",
        "Convergent Cross Mapping (CCM) is a powerful method for detecting causality\nin coupled nonlinear dynamical systems, providing a model-free approach to\ncapture dynamic causal interactions. Partial Cross Mapping (PCM) was introduced\nas an extension of CCM to address indirect causality in three-variable systems\nby comparing cross-mapping quality between direct cause-effect mapping and\nindirect mapping through an intermediate conditioning variable. However, PCM\nremains limited to univariate delay embeddings in its cross-mapping processes.\nIn this work, we extend PCM to the multivariate setting, introducing multiPCM,\nwhich leverages multivariate embeddings to more effectively distinguish\nindirect causal relationships. We further propose a multivariate cross-mapping\nframework (MXMap) for causal discovery in dynamical systems. This two-phase\nframework combines (1) pairwise CCM tests to establish an initial causal graph\nand (2) multiPCM to refine the graph by pruning indirect causal connections.\nThrough experiments on simulated data and the ERA5 Reanalysis weather dataset,\nwe demonstrate the effectiveness of MXMap. Additionally, MXMap is compared\nagainst several baseline methods, showing advantages in accuracy and causal\ngraph refinement.",
        "This research investigates the adsorption characteristics of carbon nanobelts\n(CNB) and Mobius carbon nanobelts (MCNB) interacting with various greenhouse\ngases, including NH3, CO2, CO, H2S, CH4, CH3OH, NO2, NO, and COCl2. The study\nemploys semi-empirical tight-binding calculations via xTB software,\ncomplemented by topological analysis using MULTIWFN software. Comparative\nanalysis reveals MCNB's superior adsorption properties, particularly for\nspecific gases. Notable adsorption energies for MCNB were measured at -1.595eV,\n-0.669eV, and -0.637eV for NO, COCl2, and NO2, respectively, significantly\nexceeding the corresponding CNB values of -0.636eV, -0.449eV, and -0.438eV. The\ninvestigation of desorption kinetics demonstrates rapid recovery times\n(sub-millisecond) for most gas-nanobelt interactions, with the notable\nexception of the MCNB+NO system, which exhibits persistent bonding. Topological\nanalysis confirms chemisorption mechanisms for NO, COCl2, and NO2 on both\nnanobelt variants, characterized by complex hybridizations of covalent and\nnon-covalent interactions. Molecular dynamics simulations conducted in both\npacked configurations and dry air mixtures demonstrate the nanobelts' effective\ngas-attracting properties, maintaining consistent capture performance across\ndifferent environmental conditions. These findings establish carbon nanobelts,\nparticularly the Mobius configuration, as promising candidates for greenhouse\ngas capture technologies, offering potential applications in environmental\nremediation and climate change mitigation strategies."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Quality assurance procedures for mass spectrometry untargeted metabolomics. a review",
    "start_abstract":"Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "XGBoost: A Scalable Tree Boosting System"
      ],
      "abstract":[
        "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "TimesBERT: A BERT-Style Foundation Model for Time Series Understanding",
        "Residual Policy Gradient: A Reward View of KL-regularized Objective",
        "Multi-label feature selection based on binary hashing learning and\n  dynamic graph constraints",
        "Achieving Upper Bound Accuracy of Joint Training in Continual Learning",
        "How Your Location Relates to Health: Variable Importance and\n  Interpretable Machine Learning for Environmental and Sociodemographic Data",
        "Adjusted Count Quantification Learning on Graphs",
        "Exploring Geometric Representational Alignment through Ollivier-Ricci\n  Curvature and Ricci Flow",
        "Learning Efficient Positional Encodings with Graph Neural Networks",
        "Efficient Distributed Optimization under Heavy-Tailed Noise",
        "GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time\n  Distribution Shifts",
        "Mechanistic PDE Networks for Discovery of Governing Equations",
        "Simultaneous Latent State Estimation and Latent Linear Dynamics\n  Discovery from Image Observations",
        "On the Expressivity of Selective State-Space Layers: A Multivariate\n  Polynomial Approach",
        "Target Selection for the Redshift-Limited WAVES-Wide with Machine\n  Learning",
        "Auxiliary Discrminator Sequence Generative Adversarial Networks\n  (ADSeqGAN) for Few Sample Molecule Generation",
        "Competing Effects of Local Solvation Structures on Chemical Shift\n  Changes of Liquid Electrolyte",
        "Diffusion Models for Cayley Graphs",
        "Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from\n  Word-In-Context Perspectives",
        "A Comprehensive Reanalysis of K2-18 b's JWST NIRISS+NIRSpec Transmission\n  Spectrum",
        "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles",
        "Equilibrium Moment Analysis of It\\^o SDEs",
        "NaFM: Pre-training a Foundation Model for Small-Molecule Natural\n  Products",
        "DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models",
        "Hypersurfaces passing through the Galois orbit of a point",
        "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Wireless Network Topology Inference: A Markov Chains Approach",
        "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING"
      ],
      "abstract":[
        "Time series analysis is crucial in diverse scenarios. Beyond forecasting,\nconsiderable real-world tasks are categorized into classification, imputation,\nand anomaly detection, underscoring different capabilities termed time series\nunderstanding in this paper. While GPT-style models have been positioned as\nfoundation models for time series forecasting, the BERT-style architecture,\nwhich has made significant advances in natural language understanding, has not\nbeen fully unlocked for time series understanding, possibly attributed to the\nundesirable dropout of essential elements of BERT. In this paper, inspired by\nthe shared multi-granularity structure between multivariate time series and\nmultisentence documents, we design TimesBERT to learn generic representations\nof time series including temporal patterns and variate-centric characteristics.\nIn addition to a natural adaptation of masked modeling, we propose a parallel\ntask of functional token prediction to embody vital multi-granularity\nstructures. Our model is pre-trained on 260 billion time points across diverse\ndomains. Leveraging multi-granularity representations, TimesBERT achieves\nstate-of-the-art performance across four typical downstream understanding\ntasks, outperforming task-specific models and language pre-trained backbones,\npositioning it as a versatile foundation model for time series understanding.",
        "Reinforcement Learning and Imitation Learning have achieved widespread\nsuccess in many domains but remain constrained during real-world deployment.\nOne of the main issues is the additional requirements that were not considered\nduring training. To address this challenge, policy customization has been\nintroduced, aiming to adapt a prior policy while preserving its inherent\nproperties and meeting new task-specific requirements. A principled approach to\npolicy customization is Residual Q-Learning (RQL), which formulates the problem\nas a Markov Decision Process (MDP) and derives a family of value-based learning\nalgorithms. However, RQL has not yet been applied to policy gradient methods,\nwhich restricts its applicability, especially in tasks where policy gradient\nhas already proven more effective. In this work, we first derive a concise form\nof Soft Policy Gradient as a preliminary. Building on this, we introduce\nResidual Policy Gradient (RPG), which extends RQL to policy gradient methods,\nallowing policy customization in gradient-based RL settings. With the view of\nRPG, we rethink the KL-regularized objective widely used in RL fine-tuning. We\nshow that under certain assumptions, KL-regularized objective leads to a\nmaximum-entropy policy that balances the inherent properties and task-specific\nrequirements on a reward-level. Our experiments in MuJoCo demonstrate the\neffectiveness of Soft Policy Gradient and Residual Policy Gradient.",
        "Multi-label learning poses significant challenges in extracting reliable\nsupervisory signals from the label space. Existing approaches often employ\ncontinuous pseudo-labels to replace binary labels, improving supervisory\ninformation representation. However, these methods can introduce noise from\nirrelevant labels and lead to unreliable graph structures. To overcome these\nlimitations, this study introduces a novel multi-label feature selection method\ncalled Binary Hashing and Dynamic Graph Constraint (BHDG), the first method to\nintegrate binary hashing into multi-label learning. BHDG utilizes\nlow-dimensional binary hashing codes as pseudo-labels to reduce noise and\nimprove representation robustness. A dynamically constrained sample projection\nspace is constructed based on the graph structure of these binary\npseudo-labels, enhancing the reliability of the dynamic graph. To further\nenhance pseudo-label quality, BHDG incorporates label graph constraints and\ninner product minimization within the sample space. Additionally, an\n$l_{2,1}$-norm regularization term is added to the objective function to\nfacilitate the feature selection process. The augmented Lagrangian multiplier\n(ALM) method is employed to optimize binary variables effectively.\nComprehensive experiments on 10 benchmark datasets demonstrate that BHDG\noutperforms ten state-of-the-art methods across six evaluation metrics. BHDG\nachieves the highest overall performance ranking, surpassing the next-best\nmethod by an average of at least 2.7 ranks per metric, underscoring its\neffectiveness and robustness in multi-label feature selection.",
        "Continual learning has been an active research area in machine learning,\nfocusing on incrementally learning a sequence of tasks. A key challenge is\ncatastrophic forgetting (CF), and most research efforts have been directed\ntoward mitigating this issue. However, a significant gap remains between the\naccuracy achieved by state-of-the-art continual learning algorithms and the\nideal or upper-bound accuracy achieved by training all tasks together jointly.\nThis gap has hindered or even prevented the adoption of continual learning in\napplications, as accuracy is often of paramount importance. Recently, another\nchallenge, termed inter-task class separation (ICS), was also identified, which\nspurred a theoretical study into principled approaches for solving continual\nlearning. Further research has shown that by leveraging the theory and the\npower of large foundation models, it is now possible to achieve upper-bound\naccuracy, which has been empirically validated using both text and image\nclassification datasets. Continual learning is now ready for real-life\napplications. This paper surveys the main research leading to this achievement,\njustifies the approach both intuitively and from neuroscience research, and\ndiscusses insights gained.",
        "Health outcomes depend on complex environmental and sociodemographic factors\nwhose effects change over location and time. Only recently has fine-grained\nspatial and temporal data become available to study these effects, namely the\nMEDSAT dataset of English health, environmental, and sociodemographic\ninformation. Leveraging this new resource, we use a variety of variable\nimportance techniques to robustly identify the most informative predictors\nacross multiple health outcomes. We then develop an interpretable machine\nlearning framework based on Generalized Additive Models (GAMs) and Multiscale\nGeographically Weighted Regression (MGWR) to analyze both local and global\nspatial dependencies of each variable on various health outcomes. Our findings\nidentify NO2 as a global predictor for asthma, hypertension, and anxiety,\nalongside other outcome-specific predictors related to occupation, marriage,\nand vegetation. Regional analyses reveal local variations with air pollution\nand solar radiation, with notable shifts during COVID. This comprehensive\napproach provides actionable insights for addressing health disparities, and\nadvocates for the integration of interpretable machine learning in public\nhealth.",
        "Quantification learning is the task of predicting the label distribution of a\nset of instances. We study this problem in the context of graph-structured\ndata, where the instances are vertices. Previously, this problem has only been\naddressed via node clustering methods. In this paper, we extend the popular\nAdjusted Classify & Count (ACC) method to graphs. We show that the prior\nprobability shift assumption upon which ACC relies is often not fulfilled and\npropose two novel graph quantification techniques: Structural importance\nsampling (SIS) makes ACC applicable in graph domains with covariate shift.\nNeighborhood-aware ACC improves quantification in the presence of\nnon-homophilic edges. We show the effectiveness of our techniques on multiple\ngraph quantification tasks.",
        "Representational analysis explores how input data of a neural system are\nencoded in high dimensional spaces of its distributed neural activations, and\nhow we can compare different systems, for instance, artificial neural networks\nand brains, on those grounds. While existing methods offer important insights,\nthey typically do not account for local intrinsic geometrical properties within\nthe high-dimensional representation spaces. To go beyond these limitations, we\nexplore Ollivier-Ricci curvature and Ricci flow as tools to study the alignment\nof representations between humans and artificial neural systems on a geometric\nlevel. As a proof-of-principle study, we compared the representations of face\nstimuli between VGG-Face, a human-aligned version of VGG-Face, and\ncorresponding human similarity judgments from a large online study. Using this\ndiscrete geometric framework, we were able to identify local structural\nsimilarities and differences by examining the distributions of node and edge\ncurvature and higher-level properties by detecting and comparing community\nstructure in the representational graphs.",
        "Positional encodings (PEs) are essential for effective graph representation\nlearning because they provide position awareness in inherently\nposition-agnostic transformer architectures and increase the expressive\ncapacity of Graph Neural Networks (GNNs). However, designing powerful and\nefficient PEs for graphs poses significant challenges due to the absence of\ncanonical node ordering and the scale of the graph. {In this work, we identify\nfour key properties that graph PEs should satisfy}: stability, expressive\npower, scalability, and genericness. We find that existing eigenvector-based PE\nmethods often fall short of jointly satisfying these criteria. To address this\ngap, we introduce PEARL, a novel framework of learnable PEs for graphs. Our\nprimary insight is that message-passing GNNs function as nonlinear mappings of\neigenvectors, enabling the design of GNN architectures for generating powerful\nand efficient PEs. A crucial challenge lies in initializing node attributes in\na manner that is both expressive and permutation equivariant. We tackle this by\ninitializing GNNs with random node inputs or standard basis vectors, thereby\nunlocking the expressive power of message-passing operations, while employing\nstatistical pooling functions to maintain permutation equivariance. Our\nanalysis demonstrates that PEARL approximates equivariant functions of\neigenvectors with linear complexity, while rigorously establishing its\nstability and high expressive power. Experimental evaluations show that PEARL\noutperforms lightweight versions of eigenvector-based PEs and achieves\ncomparable performance to full eigenvector-based PEs, but with one or two\norders of magnitude lower complexity. Our code is available at\nhttps:\/\/github.com\/ehejin\/Pearl-PE.",
        "Distributed optimization has become the default training paradigm in modern\nmachine learning due to the growing scale of models and datasets. To mitigate\ncommunication overhead, local updates are often applied before global\naggregation, resulting in a nested optimization approach with inner and outer\nsteps. However, heavy-tailed stochastic gradient noise remains a significant\nchallenge, particularly in attention-based models, hindering effective\ntraining. In this work, we propose TailOPT, an efficient framework designed to\naddress heavy-tailed noise by leveraging adaptive optimization or clipping\ntechniques. We establish convergence guarantees for the TailOPT framework under\nheavy-tailed noise with potentially unbounded gradient variance and local\nupdates. Among its variants, we highlight a memory and communication efficient\ninstantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping\nat both the inner and outer optimizers, achieving adaptive-like performance\n(e.g., Adam) without the cost of maintaining or transmitting additional\ngradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates\nsuperior performance on several language tasks and models, outperforming\nstate-of-the-art methods.",
        "We consider the problem of test-time domain generalization, where a model is\ntrained on several source domains and adjusted on target domains never seen\nduring training. Different from the common methods that fine-tune the model or\nadjust the classifier parameters online, we propose to generate multiple layer\nparameters on the fly during inference by a lightweight meta-learned\ntransformer, which we call \\textit{GeneralizeFormer}. The layer-wise parameters\nare generated per target batch without fine-tuning or online adjustment. By\ndoing so, our method is more effective in dynamic scenarios with multiple\ntarget distributions and also avoids forgetting valuable source distribution\ncharacteristics. Moreover, by considering layer-wise gradients, the proposed\nmethod adapts itself to various distribution shifts. To reduce the\ncomputational and time cost, we fix the convolutional parameters while only\ngenerating parameters of the Batch Normalization layers and the linear\nclassifier. Experiments on six widely used domain generalization datasets\ndemonstrate the benefits and abilities of the proposed method to efficiently\nhandle various distribution shifts, generalize in dynamic scenarios, and avoid\nforgetting.",
        "We present Mechanistic PDE Networks -- a model for discovery of governing\npartial differential equations from data. Mechanistic PDE Networks represent\nspatiotemporal data as space-time dependent linear partial differential\nequations in neural network hidden representations. The represented PDEs are\nthen solved and decoded for specific tasks. The learned PDE representations\nnaturally express the spatiotemporal dynamics in data in neural network hidden\nspace, enabling increased power for dynamical modeling. Solving the PDE\nrepresentations in a compute and memory-efficient way, however, is a\nsignificant challenge. We develop a native, GPU-capable, parallel, sparse, and\ndifferentiable multigrid solver specialized for linear partial differential\nequations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE\nsolver, we propose a discovery architecture that can discover nonlinear PDEs in\ncomplex settings while also being robust to noise. We validate PDE discovery on\na number of PDEs, including reaction-diffusion and Navier-Stokes equations.",
        "The problem of state estimation has a long history with many successful\nalgorithms that allow analytical derivation or approximation of posterior\nfiltering distribution given the noisy observations. This report tries to\nconclude previous works to resolve the problem of latent state estimation given\nimage-based observations and also suggests a new solution to this problem.",
        "Recent advances in efficient sequence modeling have introduced selective\nstate-space layers, a key component of the Mamba architecture, which have\ndemonstrated remarkable success in a wide range of NLP and vision tasks. While\nMamba's empirical performance has matched or surpassed SoTA transformers on\nsuch diverse benchmarks, the theoretical foundations underlying its powerful\nrepresentational capabilities remain less explored. In this work, we\ninvestigate the expressivity of selective state-space layers using multivariate\npolynomials, and prove that they surpass linear transformers in expressiveness.\nConsequently, our findings reveal that Mamba offers superior representational\npower over linear attention-based models for long sequences, while not\nsacrificing their generalization. Our theoretical insights are validated by a\ncomprehensive set of empirical experiments on various datasets.",
        "The forthcoming Wide Area Vista Extragalactic Survey (WAVES) on the 4-metre\nMulti-Object Spectroscopic Telescope (4MOST) has a key science goal of probing\nthe halo mass function to lower limits than possible with previous surveys. For\nthat purpose, in its Wide component, galaxies targetted by WAVES will be\nflux-limited to $Z<21.1$ mag and will cover the redshift range of $z<0.2$, at a\nspectroscopic success rate of $\\sim95\\%$. Meeting this completeness\nrequirement, when the redshift is unknown a priori, is a challenge. We solve\nthis problem with supervised machine learning to predict the probability of a\ngalaxy falling within the WAVES-Wide redshift limit, rather than estimate each\nobject's redshift. This is done by training an XGBoost tree-based classifier to\ndecide if a galaxy should be a target or not. Our photometric data come from\n9-band VST+VISTA observations, including KiDS+VIKING surveys. The redshift\nlabels for calibration are derived from an extensive spectroscopic sample\noverlapping with KiDS and ancillary fields. Our current results indicate that\nwith our approach, we should be able to achieve the completeness of $\\sim95\\%$,\nwhich is the WAVES success criterion.",
        "In this work, we introduce Auxiliary Discriminator Sequence Generative\nAdversarial Networks (ADSeqGAN), a novel approach for molecular generation in\nsmall-sample datasets. Traditional generative models often struggle with\nlimited training data, particularly in drug discovery, where molecular datasets\nfor specific therapeutic targets, such as nucleic acids binders and central\nnervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by\nintegrating an auxiliary random forest classifier as an additional\ndiscriminator into the GAN framework, significantly improves molecular\ngeneration quality and class specificity.\n  Our method incorporates pretrained generator and Wasserstein distance to\nenhance training stability and diversity. We evaluate ADSeqGAN on a dataset\ncomprising nucleic acid-targeting and protein-targeting small molecules,\ndemonstrating its superior ability to generate nucleic acid binders compared to\nbaseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling\nstrategy, ADSeqGAN also significantly improves CNS drug generation, achieving a\nhigher yield than traditional de novo models. Critical assessments, including\ndocking simulations and molecular property analysis, confirm that\nADSeqGAN-generated molecules exhibit strong binding affinities, enhanced\nchemical diversity, and improved synthetic feasibility.\n  Overall, ADSeqGAN presents a novel framework for generative molecular design\nin data-scarce scenarios, offering potential applications in computational drug\ndiscovery. We have demonstrated the successful applications of ADSeqGAN in\ngenerating synthetic nucleic acid-targeting and CNS drugs in this work.",
        "Understanding the solvation structure of electrolytes is critical for\noptimizing the electrochemical performance of rechargeable batteries, as it\ndirectly influences properties such as ionic conductivity, viscosity, and\nelectrochemical stability. The highly complex structures and strong\ninteractions in high-concentration electrolytes make accurate modeling and\ninterpretation of their ``structure-property\" relationships even more\nchallenging with spectroscopic methods. In this study, we present a machine\nlearning-based approach to predict dynamic $^7$Li NMR chemical shifts in\nLiFSI\/DME electrolyte solutions. Additionally, we provide a comprehensive\nstructural analysis to interpret the observed chemical shift behavior in our\nexperiments, particularly the abrupt changes in $^7$Li chemical shifts at high\nconcentrations. Using advanced modeling techniques, we quantitatively establish\nthe relationship between molecular structure and NMR spectra, offering critical\ninsights into solvation structure assignments. Our findings reveal the\ncoexistence of two competing local solvation structures that shift in dominance\nas electrolyte concentration approaches the concentrated limit, leading to\nanomalous reverse of $^7$Li NMR chemical shift in our experiment. This work\nprovides a detailed molecular-level understanding of the intricate solvation\nstructures probed by NMR spectroscopy, leading the way for enhanced electrolyte\ndesign.",
        "We review the problem of finding paths in Cayley graphs of groups and group\nactions, using the Rubik's cube as an example, and we list several more\nexamples of significant mathematical interest. We then show how to formulate\nthese problems in the framework of diffusion models. The exploration of the\ngraph is carried out by the forward process, while finding the target nodes is\ndone by the inverse backward process. This systematizes the discussion and\nsuggests many generalizations. To improve exploration, we propose a ``reversed\nscore'' ansatz which substantially improves over previous comparable\nalgorithms.",
        "In this work, we evaluate annotator disagreement in Word-in-Context (WiC)\ntasks exploring the relationship between contextual meaning and disagreement as\npart of the CoMeDi shared task competition. While prior studies have modeled\ndisagreement by analyzing annotator attributes with single-sentence inputs,\nthis shared task incorporates WiC to bridge the gap between sentence-level\nsemantic representation and annotator judgment variability. We describe three\ndifferent methods that we developed for the shared task, including a feature\nenrichment approach that combines concatenation, element-wise differences,\nproducts, and cosine similarity, Euclidean and Manhattan distances to extend\ncontextual embedding representations, a transformation by Adapter blocks to\nobtain task-specific representations of contextual embeddings, and classifiers\nof varying complexities, including ensembles. The comparison of our methods\ndemonstrates improved performance for methods that include enriched and\ntask-specfic features. While the performance of our method falls short in\ncomparison to the best system in subtask 1 (OGWiC), it is competitive to the\nofficial evaluation results in subtask 2 (DisWiC).",
        "Sub-Neptunes are the most common type of planet in our galaxy. Interior\nstructure models suggest that the coldest sub-Neptunes could host liquid water\noceans underneath their hydrogen envelopes - sometimes called 'hycean' planets.\nJWST transmission spectra of the $\\sim$ 250 K sub-Neptune K2-18 b were recently\nused to report detections of CH$_4$ and CO$_2$, alongside weaker evidence of\n(CH$_3$)$_2$S (dimethyl sulfide, or DMS). Atmospheric CO$_2$ was interpreted as\nevidence for a liquid water ocean, while DMS was highlighted as a potential\nbiomarker. However, these notable claims were derived using a single data\nreduction and retrieval modeling framework, which did not allow for standard\nrobustness tests. Here we present a comprehensive reanalysis of K2-18 b's JWST\nNIRISS SOSS and NIRSpec G395H transmission spectra, including the first\nanalysis of the second-order NIRISS SOSS data. We incorporate multiple\nwell-tested data reduction pipelines and retrieval codes, spanning 60 different\ndata treatments and over 250 atmospheric retrievals. We confirm the detection\nof CH$_4$ ($\\approx$ 4$\\sigma$), with a volume mixing ratio of log CH$_4$ =\n$-1.15^{+0.40}_{-0.52}$, but we find no statistically significant or reliable\nevidence for CO$_2$ or DMS. Finally, we quantify the observed atmospheric\ncomposition using photochemical-climate and interior models, demonstrating that\nour revised composition of K2-18 b can be explained by an oxygen-poor\nmini-Neptune without requiring a liquid water surface or life.",
        "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.",
        "Stochastic differential equations have proved to be a valuable governing\nframework for many real-world systems which exhibit ``noise'' or randomness in\ntheir evolution. One quality of interest in such systems is the shape of their\nequilibrium probability distribution, if such a thing exists. In some cases a\nstraightforward integral equation may yield this steady-state distribution, but\nin other cases the equilibrium distribution exists and yet that integral\nequation diverges. Here we establish a new equilibrium-analysis technique based\non the logic of finite-timestep simulation which allows us to glean information\nabout the equilibrium regardless -- in particular, a relationship between the\nraw moments of the equilibrium distribution. We utilize this technique to\nextract information about one such equilibrium resistant to direct definition.",
        "Natural products, as metabolites from microorganisms, animals, or plants,\nexhibit diverse biological activities, making them crucial for drug discovery.\nNowadays, existing deep learning methods for natural products research\nprimarily rely on supervised learning approaches designed for specific\ndownstream tasks. However, such one-model-for-a-task paradigm often lacks\ngeneralizability and leaves significant room for performance improvement.\nAdditionally, existing molecular characterization methods are not well-suited\nfor the unique tasks associated with natural products. To address these\nlimitations, we have pre-trained a foundation model for natural products based\non their unique properties. Our approach employs a novel pretraining strategy\nthat is especially tailored to natural products. By incorporating contrastive\nlearning and masked graph learning objectives, we emphasize evolutional\ninformation from molecular scaffolds while capturing side-chain information.\nOur framework achieves state-of-the-art (SOTA) results in various downstream\ntasks related to natural product mining and drug discovery. We first compare\ntaxonomy classification with synthesized molecule-focused baselines to\ndemonstrate that current models are inadequate for understanding natural\nsynthesis. Furthermore, by diving into a fine-grained analysis at both the gene\nand microbial levels, NaFM demonstrates the ability to capture evolutionary\ninformation. Eventually, our method is experimented with virtual screening,\nillustrating informative natural product representations that can lead to more\neffective identification of potential drug candidates.",
        "Recent advancements in slow-thinking reasoning models have shown exceptional\nperformance in complex reasoning tasks. However, these models often exhibit\noverthinking-generating redundant reasoning steps for simple problems, leading\nto excessive computational resource usage. While current mitigation strategies\nuniformly reduce reasoning tokens, they risk degrading performance on\nchallenging tasks that require extended reasoning. This paper introduces\nDifficulty-Adaptive Slow-Thinking (DAST), a novel framework that enables models\nto autonomously adjust the length of Chain-of-Thought(CoT) based on problem\ndifficulty. We first propose a Token Length Budget (TLB) metric to quantify\ndifficulty, then leveraging length-aware reward shaping and length preference\noptimization to implement DAST. DAST penalizes overlong responses for simple\ntasks while incentivizing sufficient reasoning for complex problems.\nExperiments on diverse datasets and model scales demonstrate that DAST\neffectively mitigates overthinking (reducing token usage by over 30\\% on\naverage) while preserving reasoning accuracy on complex problems.",
        "Asgarli, Ghioca, and Reichstein recently proved that if $K$ is a field with\n$|K|>2$, then for any positive integers $d$ and $n$, and separable field\nextension $L\/K$ with degree $m=\\binom{n+d}{d}$, there exists a point $P\\in\n\\mathbb{P}^n(L)$ which does not lie on any degree $d$ hypersurface defined over\n$K$. They asked whether the result holds when $|K| = 2$. We answer their\nquestion in the affirmative by combining various ideas from arithmetic\ngeometry. More generally, we show that for each positive integer $r$ and\nseparable field extension $L\/K$ with degree $r$, there exists a point $P \\in\n\\mathbb{P}^n(L)$ such that the vector space of degree $d$ forms over $K$ that\nvanish at $P$ has the expected dimension. We also discuss applications to\nlinear systems of hypersurfaces with special properties.",
        "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "In this work, we address the problem of inferring the topology of a wireless\nnetwork using limited observational data. Specifically, we assume that we can\ndetect when a node is transmitting, but no further information regarding the\ntransmission is available. We propose a novel network estimation procedure\ngrounded in the following abstract problem: estimating the parameters of a\nfinite discrete-time Markov chain by observing, at each time step, which states\nare visited by multiple ``anonymous'' copies of the chain. We develop a\nconsistent estimator that approximates the transition matrix of the chain in\nthe operator norm, with the number of required samples scaling roughly linearly\nwith the size of the state space. Applying this estimation procedure to\nwireless networks, our numerical experiments demonstrate that the proposed\nmethod accurately infers network topology across a wide range of parameters,\nconsistently outperforming transfer entropy, particularly under conditions of\nhigh network congestion.",
        "We introduce STRING: Separable Translationally Invariant Position Encodings.\nSTRING extends Rotary Position Encodings, a recently proposed and widely used\nalgorithm in large language models, via a unifying theoretical framework.\nImportantly, STRING still provides exact translation invariance, including\ntoken coordinates of arbitrary dimensionality, whilst maintaining a low\ncomputational footprint. These properties are especially important in robotics,\nwhere efficient 3D token representation is key. We integrate STRING into Vision\nTransformers with RGB(-D) inputs (color plus optional depth), showing\nsubstantial gains, e.g. in open-vocabulary object detection and for robotics\ncontrollers. We complement our experiments with a rigorous mathematical\nanalysis, proving the universality of our methods."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"XGBoost: A Scalable Tree Boosting System",
    "start_abstract":"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
      ],
      "abstract":[
        "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Petri Net Modeling of Root Hair Response to Phosphate Starvation in\n  Arabidopsis Thaliana",
        "Hierarchical Functional Group Ranking via IUPAC Name Analysis for Drug\n  Discovery: A Case Study on TDP1 Inhibitors",
        "Optimal compound downselection to promote diversity and parallel\n  chemistry",
        "Predicting novel pharmacological activities of compounds using PubChem\n  IDs and machine learning (CID-SID ML model)",
        "In silico clinical trials in drug development: a systematic review",
        "Derivation from kinetic theory and 2-D pattern analysis of chemotaxis\n  models for Multiple Sclerosis",
        "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties",
        "Data-Driven Modeling of Amyloid-beta Targeted Antibodies for Alzheimer's\n  Disease",
        "Temporal Dynamics of Microbial Communities in Anaerobic Digestion:\n  Influence of Temperature and Feedstock Composition on Reactor Performance and\n  Stability",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Stability of 2-class groups in the $\\mathbb{Z}_2$-extension of certain\n  real biquadratic fields",
        "Obstructions for Morin and fold maps: Stiefel-Whitney classes and Euler\n  characteristics of singularity loci",
        "Chiral broadband High Harmonic Generation Source by Vectorial\n  Time-Polarization-Gating",
        "Crystal skeletons: Combinatorics and axioms",
        "Quantum crystal spin Hall effect in two-dimensional altermagnetic\n  systems",
        "Random Variables, Conditional Independence and Categories of Abstract\n  Sample Spaces",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "Constant-Overhead Fault-Tolerant Bell-Pair Distillation using High-Rate\n  Codes",
        "Bounded Dark Energy",
        "Probing Type II Seesaw Leptogenesis Through Lepton Flavor Violation",
        "Strategic Queues with Priority Classes",
        "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep\n  Reinforcement Learning with LLM Evaluation",
        "$\\mathrm{G}_2$-structures with torsion and the deformed Shatashvili-Vafa\n  vertex algebra",
        "A new class of non-stationary Gaussian fields with general smoothness on\n  metric graphs",
        "Isolating the hard core of phaseless inference: the Phase selection\n  formulation"
      ],
      "abstract":[
        "Limited availability of inorganic phosphate (Pi) in soil is an important\nconstraint to plant growth. In order to understand better the underlying\nmechanism of plant response to Pi, the response to phosphate starvation in\nArabidopsis thaliana was investigated through use of Petri Nets, a formal\nlanguage suitable for bio-modeling. A. thaliana displays a range of responses\nto deal with Pi starvation, but special attention was paid to root hair\nelongation in this study. A central player in the root hair pathway is the\ntranscription factor ROOT HAIR DEFECTIVE 6-LIKE 4 (RSL4), which has been found\nto be upregulated during the Pi stress. A Petri Net was created which could\nsimulate the gene regulatory networks responsible for the increase in root hair\nlength, as well as the resulting increase in root hair length. Notably,\ndiscrepancies between the model and the literature suggested an important role\nfor RSL2 in regulating RSL4. In the future, the net designed in the current\nstudy could be used as a platform to develop hypotheses about the interaction\nbetween RSL2 and RSL4.",
        "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
        "Early stage drug discovery and molecular design projects often follow\niterative design-make-test cycles. The selection of which compounds to\nsynthesize from all possible candidate compounds is a complex decision inherent\nto these design cycles that must weigh multiple factors. We build upon the\nalgorithmic downselection framework SPARROW that considers synthetic cost,\nsynthetic feasibility, and compound utility, extending it to address additional\ncritical factors related to the risk of synthesis failure, molecular diversity,\nand parallel chemistry capabilities. These design considerations further align\nalgorithmic compound selection with the true complexity of this decision-making\nprocess, allowing SPARROW to capture a broader set of principles typically\nreliant on expert chemist intuition. The application of these formulations to\nan exemplary case study highlights SPARROW's ability to promote the selection\nof diverse batches of compounds whose syntheses are amenable to parallel\nchemistry.",
        "Significance and Object: The proposed methodology aims to provide time- and\ncost-effective approach for the early stage in drug discovery. The machine\nlearning models developed in this study used only the identification numbers\nprovided by PubChem. Thus, a drug development researcher who has obtained a\nPubChem CID and SID can easily identify new functionality of their compound.\nThe approach was demonstrated, using four bioassay which were on (i) the\nantagonists of human D3 dopamine receptors; (ii) the promoter Rab9 activators;\n(iii) small molecule inhibitors of CHOP to regulate the unfolded protein\nresponse to ER stress; (iv) antagonists of the human M1 muscarinic receptor.\nSolution: The four bioassays used for demonstration of the approach were\nprovided by PubChem. For each bioassay, the generated by PubChem CIDs, SIDs\nwere extracted together with the corresponding activity. The resulting dataset\nwas sifted with the dataset on a water solubility bioassay, remaining only the\ncompounds common for both bioassays. In this way, the inactive compounds were\nreduced. Then, all active compounds were added, and the resulted dataset was\nlater used for machine learning based on scikit learn algorithms. Results: The\naverage values of the ML models` metrics for the four bioassays were: 83.82%\nAccuracy with 5.35 standard deviation; 87.9% Precision with 5.04 standard\ndeviation; 77.1% Recall with 7.65 standard deviation; 82.1% F1 with 6.44\nstandard deviation; 83.4% ROC with 5.09 standard deviation.",
        "In the context of clinical research, computational models have received\nincreasing attention over the past decades. In this systematic review, we aimed\nto provide an overview of the role of so-called in silico clinical trials\n(ISCTs) in medical applications. Exemplary for the broad field of clinical\nmedicine, we focused on in silico (IS) methods applied in drug development,\nsometimes also referred to as model informed drug development (MIDD). We\nsearched PubMed and ClinicalTrials.gov for published articles and registered\nclinical trials related to ISCTs. We identified 202 articles and 48 trials, and\nof these, 76 articles and 19 trials were directly linked to drug development.\nWe extracted information from all 202 articles and 48 clinical trials and\nconducted a more detailed review of the methods used in the 76 articles that\nare connected to drug development. Regarding application, most articles and\ntrials focused on cancer and imaging related research while rare and pediatric\ndiseases were only addressed in 18 and 4 studies, respectively. While some\nmodels were informed combining mechanistic knowledge with clinical or\npreclinical (in-vivo or in-vitro) data, the majority of models were fully\ndata-driven, illustrating that clinical data is a crucial part in the process\nof generating synthetic data in ISCTs. Regarding reproducibility, a more\ndetailed analysis revealed that only 24% (18 out of 76) of the articles\nprovided an open-source implementation of the applied models, and in only 20%\nof the articles the generated synthetic data were publicly available. Despite\nthe widely raised interest, we also found that it is still uncommon for ISCTs\nto be part of a registered clinical trial and their application is restricted\nto specific diseases leaving potential benefits of ISCTs not fully exploited.",
        "In this paper, a class of reaction-diffusion equations for Multiple Sclerosis\nis presented. These models are derived by means of a diffusive limit starting\nfrom a proper kinetic description, taking account of the underlying microscopic\ninteractions among cells. At the macroscopic level, we discuss the necessary\nconditions for Turing instability phenomena and the formation of\ntwo-dimensional patterns, whose shape and stability are investigated by means\nof a weakly nonlinear analysis. Some numerical simulations, confirming and\nextending theoretical results, are proposed for a specific scenario.",
        "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions.",
        "Alzheimer's disease (AD) is driven by the accumulation of amyloid-beta\n(Abeta) proteins in the brain, leading to memory loss and cognitive decline.\nWhile monoclonal antibodies targeting Abetahave been approved, optimizing their\nuse to maximize benefits while minimizing side effects remains a challenge.\nThis study develops a mathematical model to describe Abeta aggregation,\ncapturing its progression from monomers to toxic oligomers, protofibrils, and\nfibrils using mass-action kinetics and coarse-grained modeling. The model is\ncalibrated with experimental data, incorporating parameter estimation and\nsensitivity analysis to ensure accuracy. An optimal control framework is\nintroduced to determine the best drug dosing strategy that reduces toxic Abeta\naggregates while minimizing adverse effects, such as amyloid-related imaging\nabnormalities (ARIA). Results indicate that Donanemab achieves the greatest\nreduction in fibrils. This work provides a quantitative framework for\noptimizing AD treatment strategies, offering insights into balancing\ntherapeutic efficacy and safety.",
        "Anaerobic digestion (AD) offers a sustainable biotechnology to recover\nresources from carbon-rich wastewater, such as food-processing wastewater.\nDespite crude wastewater characterisation, the impact of detailed chemical\nfingerprinting on AD remains underexplored. This study investigated the\ninfluence of fermentation-wastewater composition and operational parameters on\nAD over time to identify critical factors influencing reactor biodiversity and\nperformance. Eighteen reactors were operated under various operational\nconditions using mycoprotein fermentation wastewater. Detailed chemical\nanalysis fingerprinted the molecules in the fermentation wastewater throughout\nAD including sugars, sugar alcohols and volatile fatty acids (VFAs). Sequencing\nrevealed distinct microbiome profiles linked to temperature and reactor\nconfiguration, with mesophilic conditions supporting a more diverse and densely\nconnected microbiome. Significant elevations in Methanomassiliicoccus were\ncorrelated to high butyric acid concentrations and decreased biogas production,\nfurther elucidating the role of this newly discovered methanogen. Dissimilarity\nanalysis demonstrated the importance of individual molecules on microbiome\ndiversity, highlighting the need for detailed chemical fingerprinting in AD\nstudies of microbial trends. Machine learning (ML) models predicting reactor\nperformance achieved high accuracy based on operational parameters and\nmicrobial taxonomy. Operational parameters had the most substantial influence\non chemical oxygen demand removal, whilst Oscillibacter and two Clostridium sp.\nwere highlighted as key factors in biogas production. By integrating detailed\nchemical and biological fingerprinting with ML models this research presents a\nnovel approach to advance our understanding of AD microbial ecology, offering\ninsights for industrial applications of sustainable waste-to-energy systems.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "Greenberg's conjecture on the stability of $\\ell$-class groups in the\ncyclotomic $\\mathbb{Z}_{\\ell}$-extension of a real field has been proven for\nvarious infinite families of real quadratic fields for the prime $\\ell=2$. In\nthis work, we consider an infinite family of real biquadratic fields $K$. With\nsome extensive use of elementary group theoretic and class field theoretic\narguments, we investigate the $2$-class groups of the $n$-th layers $K_n$ of\nthe cyclotomic $\\mathbb{Z}_2$-extension of $K$ and verify Greenberg's\nconjecture. We also relate capitulation of ideal classes of certain\nsub-extensions of $K_n$ to the relative sizes of the $2$-class groups.",
        "For a singularity type $\\eta$, let the $\\eta$-avoiding number of an\n$n$-dimensional manifold $M$ be the lowest $k$ for which there is a map\n$M\\to\\mathbb{R}^{n+k}$ without $\\eta$ type singular points. For instance, the\ncase of $\\eta=\\Sigma^1$ is the case of immersions, which has been extensively\nstudied in the case of real projective spaces. In this paper we study the\n$\\eta$-avoiding number for other singularity types. Our results come in two\nlevels: first we give an abstract reasoning that a non-zero cohomology class is\nsupported on the singularity locus $\\eta(f)$, proving that $\\eta(f)$ cannot be\nempty. Second, we interpret this obstruction as a non-zero invariant of the\nsingularity locus $\\eta(f)$ for generic $f$. The main technique that we employ\nis Sullivan's Stiefel-Whitney classes, which are mod 2, real analogues of the\nChern-Schwartz-MacPherson (CSM) classes. We introduce the Segre-Stiefel-Whitney\nclasses of a singularity ${\\rm s}^{\\rm sw}_\\eta$ whose lowest degree term is\nthe mod 2 Thom polynomial of $\\eta$. Using these techniques we compute some\nuniversal formulas for the Euler characteristic of a singularity locus.",
        "Chiral (highly helical) extreme ultraviolet (XUV) sources are pivotal for\ninvestigating chiroptical phenomena on the ultrafast electronic timescale.\nTable-top, coherent High Harmonic Generation (HHG)-based sources are\nparticularly well-suited for these studies. However, chiral materials, such as\norganic chiral molecules and solid-state magnetic materials, exhibit fine\nspectral features which necessitate broadband radiation for their complete\ninterrogation. The generation of radiation that is both broadband and helical\nthrough HHG presents a seemingly paradoxical challenge: while chiral HHG\nemission requires at least two recollisions occurring along different\ndirections in the polarization plane, the Floquet limit might already be\nreached with as few as three recollisions, resulting in a sparse spectrum\ncharacterized by pronounced discrete harmonic peaks. Here we propose a\nstraightforward scheme that enables the interrogation of fine spectral\nfeatures, in principle restricted only by the resolution of the XUV\nspectrometer, with chiral XUV light. Our method is based on using a vectorial\ntwo-color driver with close central-frequencies with slight symmetry breaking.\nIt integrates the time-gating and polarization-gating techniques to generate a\nvectorial driver which induces well-controlled bursts of recollisions,\noccurring along different directions in the polarization plane. The method\nsatisfies the dual requirements of an XUV source which is both broadband and\nhelical. We perform polarization scan and demonstrate that the broadband XUV\nradiation exhibits rapid modulations in its spectral ellipticity, and fast\nalternation in its spectral helicities. The phase of modulations could be\ncontrolled by introducing a slight symmetry breaking. This allows us to control\nand modulate the XUV polarization state, which should enable the detection of\nchiroptical signals with enhanced sensitivity.",
        "Crystal skeletons were introduced by Maas-Gari\\'epy in 2023 by contracting\nquasi-crystal components in a crystal graph. On the representation theoretic\nlevel, crystal skeletons model the expansion of Schur functions into Gessel's\nquasisymmetric functions. Motivated by questions of Schur positivity, we\nprovide a combinatorial description of crystal skeletons, and prove many new\nproperties, including a conjecture by Maas-Gari\\'epy that crystal skeletons\ngeneralize dual equivalence graphs. We then present a new axiomatic approach to\ncrystal skeletons. We give three versions of the axioms based on\n$GL_n$-branching, $S_n$-branching, and local axioms in analogy to the local\nStembridge axioms for crystals based on novel commutation relations.",
        "In the field of condensed matter physics, time-reversal symmetry provides the\nfoundation for a number of interesting quantum phenomena, in particular the\ntopological materials and the quantum spin Hall physics that have been\nextensively studied in recent years. Here, based on the first-principles\nelectronic-structure calculations, symmetry analysis, and model simulations, we\ndemonstrate that time-reversal symmetry is not fundamentally necessary for the\nquantum spin Hall effect. In altermagnetic materials, as an alternative, it can\nalso be protected by crystal symmetry, which can be referred to as the quantum\ncrystal spin Hall effect.",
        "Two high-level \"pictures\" of probability theory have emerged: one that takes\nas central the notion of random variable, and one that focuses on distributions\nand probability channels (Markov kernels). While the channel-based picture has\nbeen successfully axiomatized, and widely generalized, using the notion of\nMarkov category, the categorical semantics of the random variable picture\nremain less clear. Simpson's probability sheaves are a recent approach, in\nwhich probabilistic concepts like random variables are allowed vary over a site\nof sample spaces. Simpson has identified rich structure on these sites, most\nnotably an abstract notion of conditional independence, and given examples\nranging from probability over databases to nominal sets. We aim bring this\ndevelopment together with the generality and abstraction of Markov categories:\nWe show that for any suitable Markov category, a category of sample spaces can\nbe defined which satisfies Simpson's axioms, and that a theory of probability\nsheaves can be developed purely synthetically in this setting. We recover\nSimpson's examples in a uniform fashion from well-known Markov categories, and\nconsider further generalizations.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "We present a fault-tolerant Bell-pair distillation scheme achieving constant\noverhead through high-rate quantum low-density parity-check (qLDPC) codes. Our\napproach maintains a constant distillation rate equal to the code rate - as\nhigh as $1\/3$ in our implementations - while requiring no additional overhead\nbeyond the physical qubits of the code. Full circuit-level analysis\ndemonstrates fault-tolerance for input Bell pair infidelities below a threshold\n$\\sim 5\\%$, readily achievable with near-term capabilities. Unlike previous\nproposals, our scheme keeps the output Bell pairs encoded in qLDPC codes at\neach node, eliminating decoding overhead and enabling direct use in distributed\nquantum applications through recent advances in qLDPC computation. These\nresults establish qLDPC-based distillation as a practical route toward\nresource-efficient quantum networks and distributed quantum computing.",
        "Recent cosmological observations suggest that the dark energy equation of\nstate may have changed in the latter stages of cosmic history. We introduce a\nquintessence scenario, termed bounded dark energy, capable of explaining this\nfeature in a technically natural way. Our approach is motivated from a\nbottom-up perspective, based on the concept of mirage cut-off, where we\ndemonstrate the stability of the quintessence potential against large quantum\ncorrections. At the same time, the bounded dark energy framework aligns well\nwith top-down considerations motivated from quantum gravity arguments. We\nemploy both human-driven insights and machine learning techniques to identify\nexplicit realizations of bounded dark energy models. We then perform an\nanalysis based on Markov Chain Monte-Carlo to assess their predictions against\nCMB, galaxy surveys, and supernova data, showing that bounded dark energy\nprovides a good fit to current observations. We also discuss how upcoming\nmeasurements can further test and refine our proposal.",
        "Lepton flavor violation (LFV) offers a powerful probe of physics beyond the\nStandard Model, particularly in models addressing neutrino masses and the\nbaryon asymmetry of the universe. In this study, we investigate LFV processes\nwithin the framework of type II seesaw leptogenesis, where the Standard Model\nis extended by an $SU(2)_L$ triplet Higgs field. We focus on key LFV processes\nincluding $\\mu^+\\to e^+\\gamma$, $\\mu^+ \\to e^+e^-e^+$, and $\\mu \\rightarrow e$\nconversion in nuclei, deriving stringent constraints on the parameter space\nfrom current experimental data. We scan the 3$\\sigma$ range of neutrino\noscillation parameters and identify the most conservative bounds consistent\nwith existing measurements. Our results reveal that the MEG experiment\ncurrently provides the strongest constraints in the normal ordering (NO)\nscenario, while the SINDRUM experiment offers comparable sensitivity in the\ninverted ordering (IO) case. Future experiments, such as MEG II, Mu3e, Mu2e,\nand COMET, are predicted to significantly improve the sensitivity, testing\nlarger regions of the parameter space. This work underscores the crucial role\nof LFV experiments in probing type II seesaw leptogenesis, providing an avenue\nto explore the connections between neutrino mass generation, baryogenesis, and\ninflation at experimentally accessible energy scales.",
        "We consider a strategic M\/M\/1 queueing model under a first-come-first-served\nregime, where customers are split into two classes and class $A$ has priority\nover class $B$. Customers can decide whether to join the queue or balk, and, in\ncase they have joined the queue, whether and when to renege. We study the\nequilibrium strategies and compare the equilibrium outcome and the social\noptimum in the two cases where the social optimum is or is not constrained by\npriority.",
        "Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY\/HOLD\/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit\/loss evaluation (60% profit rate), LLM evaluation\n(3.37\/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making.",
        "We construct representations of the deformed Shatashvili-Vafa vertex algebra\n$\\mathrm{SV}_a$, with parameter $a \\in \\mathbb{C}$, as recently proposed in the\nphysics literature by Fiset and Gaberdiel. The geometric input for our\nconstruction are integrable $\\mathrm{G}_2$-structures with closed torsion,\nsolving the heterotic $\\mathrm{G}_2$ system with $\\alpha'=0$ on the group\nmanifolds $S^3\\times T^4$ and $S^3\\times S^3\\times S^1$. From considerations in\nstring theory, one expects the chiral algebra of these backgrounds to include\n$\\mathrm{SV}_a$, and we provide a mathematical realization of this expectation\nby obtaining embeddings of $\\mathrm{SV}_a$ in the corresponding superaffine\nvertex algebra and the chiral de Rham complex. In our examples, the parameter\n$a$ is proportional to the scalar torsion class of the $\\mathrm{G}_2$\nstructure, $a \\sim \\tau_0$, as expected from previous work in the\nsemi-classical limit by the second author, jointly with De la Ossa and\nMarchetto.",
        "The increasing availability of network data has driven the development of\nadvanced statistical models specifically designed for metric graphs, where\nGaussian processes play a pivotal role. While models such as Whittle-Mat\\'ern\nfields have been introduced, there remains a lack of practically applicable\noptions that accommodate flexible non-stationary covariance structures or\ngeneral smoothness. To address this gap, we propose a novel class of\ngeneralized Whittle-Mat\\'ern fields, which are rigorously defined on general\ncompact metric graphs and permit both non-stationarity and arbitrary\nsmoothness. We establish new regularity results for these fields, which extend\neven to the standard Whittle-Mat\\'ern case. Furthermore, we introduce a method\nto approximate the covariance operator of these processes by combining the\nfinite element method with a rational approximation of the operator's\nfractional power, enabling computationally efficient Bayesian inference for\nlarge datasets. Theoretical guarantees are provided by deriving explicit\nconvergence rates for the covariance approximation error, and the practical\nutility of our approach is demonstrated through simulation studies and an\napplication to traffic speed data, highlighting the flexibility and\neffectiveness of the proposed model class.",
        "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency."
      ]
    }
  },
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"The Burgers equation",
    "start_abstract":"The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows.",
    "start_categories":[
      "math.AP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
      ],
      "abstract":[
        "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Rule-Based Conflict-Free Decision Framework in Swarm Confrontation",
        "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform",
        "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
        "Towards a Formal Theory of the Need for Competence via Computational\n  Intrinsic Motivation",
        "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs",
        "Lifelong Learning of Large Language Model based Agents: A Roadmap",
        "Benchmarking Reasoning Robustness in Large Language Models",
        "Driving behavior recognition via self-discovery learning",
        "Generating Counterfactual Explanations Under Temporal Constraints",
        "R-ParVI: Particle-based variational inference through lens of rewards",
        "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
        "Empowering GraphRAG with Knowledge Filtering and Integration",
        "Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven\n  Multi-Trait Essay Scoring",
        "FlashSR: One-step Versatile Audio Super-resolution via Diffusion\n  Distillation",
        "Insights of Transitions to Thermoacoustic Instability in Inverse\n  Diffusion Flame using Multifractal Detrended Fluctuation Analysis",
        "Delta-WKV: A Novel Meta-in-Context Learner for MRI Super-Resolution",
        "AAS2RTO: Automated Alert Streams to Real-Time Observations: Preparing\n  for rapid follow-up of transient objects in the era of LSST",
        "\"Auntie, Please Don't Fall for Those Smooth Talkers\": How Chinese\n  Younger Family Members Safeguard Seniors from Online Fraud",
        "Complex Riemannian spacetime and singularity-free black holes and\n  cosmology",
        "Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and\n  Texture Fusion",
        "Proof-Driven Clause Learning in Neural Network Verification",
        "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project",
        "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
        "Efficient Language Modeling for Low-Resource Settings with Hybrid\n  RNN-Transformer Architectures",
        "Semi-supervised Anomaly Detection with Extremely Limited Labels in\n  Dynamic Graphs",
        "Quantifying the degree of hydrodynamic behaviour in heavy-ion collisions",
        "Funnelling super-resolution STED microscopy through multimode fibres"
      ],
      "abstract":[
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "Traditional rule-based decision-making methods with interpretable advantage,\nsuch as finite state machine, suffer from the jitter or deadlock(JoD) problems\nin extremely dynamic scenarios. To realize agent swarm confrontation, decision\nconflicts causing many JoD problems are a key issue to be solved. Here, we\npropose a novel decision-making framework that integrates probabilistic finite\nstate machine, deep convolutional networks, and reinforcement learning to\nimplement interpretable intelligence into agents. Our framework overcomes state\nmachine instability and JoD problems, ensuring reliable and adaptable decisions\nin swarm confrontation. The proposed approach demonstrates effective\nperformance via enhanced human-like cooperation and competitive strategies in\nthe rigorous evaluation of real experiments, outperforming other methods.",
        "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
        "Decoding strategies significantly influence the quality and diversity of the\ngenerated texts in large language models (LLMs), yet their impact on\ncomputational resource consumption, particularly GPU energy usage, is\ninsufficiently studied. This paper investigates the relationship between text\ngeneration decoding methods and energy efficiency, focusing on the trade-off\nbetween generation quality and GPU energy consumption across diverse tasks and\ndecoding configurations. By benchmarking multiple strategies across different\ntext generation tasks, such as Translation, Code Summarization, and Math\nProblem Solving, we reveal how selecting appropriate decoding techniques with\ntheir tuned hyperparameters affects text quality and has measurable\nimplications for resource utilization, emphasizing the need for balanced\noptimization. To the best of our knowledge, this study is among the first to\nexplore decoding strategies in LLMs through the lens of energy consumption,\noffering actionable insights for designing resource-aware applications that\nmaintain high-quality text generation.",
        "Computational models offer powerful tools for formalising psychological\ntheories, making them both testable and applicable in digital contexts.\nHowever, they remain little used in the study of motivation within psychology.\nWe focus on the \"need for competence\", postulated as a key basic human need\nwithin Self-Determination Theory (SDT) -- arguably the most influential\npsychological framework for studying intrinsic motivation (IM). The need for\ncompetence is treated as a single construct across SDT texts. Yet, recent\nresearch has identified multiple, ambiguously defined facets of competence in\nSDT. We propose that these inconsistencies may be alleviated by drawing on\ncomputational models from the field of artificial intelligence, specifically\nfrom the domain of reinforcement learning (RL). By aligning the aforementioned\nfacets of competence -- effectance, skill use, task performance, and capacity\ngrowth -- with existing RL formalisms, we provide a foundation for advancing\ncompetence-related theory in SDT and motivational psychology more broadly. The\nformalisms reveal underlying preconditions that SDT fails to make explicit,\ndemonstrating how computational models can improve our understanding of IM.\nAdditionally, our work can support a cycle of theory development by inspiring\nnew computational models formalising aspects of the theory, which can then be\ntested empirically to refine the theory. While our research lays a promising\nfoundation, empirical studies of these models in both humans and machines are\nneeded, inviting collaboration across disciplines.",
        "Human reasoning relies on constructing and manipulating mental\nmodels-simplified internal representations of situations that we use to\nunderstand and solve problems. Conceptual diagrams (for example, sketches drawn\nby humans to aid reasoning) externalize these mental models, abstracting\nirrelevant details to efficiently capture relational and spatial information.\nIn contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs)\npredominantly reason through textual representations, limiting their\neffectiveness in complex multi-step combinatorial and planning tasks. In this\npaper, we propose a zero-shot fully automatic framework that enables LMMs to\nreason through multiple chains of self-generated intermediate conceptual\ndiagrams, significantly enhancing their combinatorial planning capabilities.\nOur approach does not require any human initialization beyond a natural\nlanguage description of the task. It integrates both textual and diagrammatic\nreasoning within an optimized graph-of-thought inference framework, enhanced by\nbeam search and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves GPT-4o's performance (for\nexample, from 35.5% to 90.2% in Blocksworld). On more difficult planning\ndomains with solution depths up to 40, our approach outperforms even the\no1-preview reasoning model (for example, over 13% improvement in Parking).\nThese results highlight the value of conceptual diagrams as a complementary\nreasoning medium in LMMs.",
        "Lifelong learning, also known as continual or incremental learning, is a\ncrucial component for advancing Artificial General Intelligence (AGI) by\nenabling systems to continuously adapt in dynamic environments. While large\nlanguage models (LLMs) have demonstrated impressive capabilities in natural\nlanguage processing, existing LLM agents are typically designed for static\nsystems and lack the ability to adapt over time in response to new challenges.\nThis survey is the first to systematically summarize the potential techniques\nfor incorporating lifelong learning into LLM-based agents. We categorize the\ncore components of these agents into three modules: the perception module for\nmultimodal input integration, the memory module for storing and retrieving\nevolving knowledge, and the action module for grounded interactions with the\ndynamic environment. We highlight how these pillars collectively enable\ncontinuous adaptation, mitigate catastrophic forgetting, and improve long-term\nperformance. This survey provides a roadmap for researchers and practitioners\nworking to develop lifelong learning capabilities in LLM agents, offering\ninsights into emerging trends, evaluation metrics, and application scenarios.\nRelevant literature and resources are available at \\href{this\nurl}{https:\/\/github.com\/qianlima-lab\/awesome-lifelong-llm-agent}.",
        "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.",
        "Autonomous driving systems require a deep understanding of human driving\nbehaviors to achieve higher intelligence and safety.Despite advancements in\ndeep learning, challenges such as long-tail distribution due to scarce samples\nand confusion from similar behaviors hinder effective driving behavior\ndetection.Existing methods often fail to address sample confusion adequately,\nas datasets frequently contain ambiguous samples that obscure unique semantic\ninformation.",
        "Counterfactual explanations are one of the prominent eXplainable Artificial\nIntelligence (XAI) techniques, and suggest changes to input data that could\nalter predictions, leading to more favourable outcomes. Existing counterfactual\nmethods do not readily apply to temporal domains, such as that of process\nmining, where data take the form of traces of activities that must obey to\ntemporal background knowledge expressing which dynamics are possible and which\nnot. Specifically, counterfactuals generated off-the-shelf may violate the\nbackground knowledge, leading to inconsistent explanations. This work tackles\nthis challenge by introducing a novel approach for generating temporally\nconstrained counterfactuals, guaranteed to comply by design with background\nknowledge expressed in Linear Temporal Logic on process traces (LTLp). We do so\nby infusing automata-theoretic techniques for LTLp inside a genetic algorithm\nfor counterfactual generation. The empirical evaluation shows that the\ngenerated counterfactuals are temporally meaningful and more interpretable for\napplications involving temporal dependencies.",
        "A reward-guided, gradient-free ParVI method, \\textit{R-ParVI}, is proposed\nfor sampling partially known densities (e.g. up to a constant). R-ParVI\nformulates the sampling problem as particle flow driven by rewards: particles\nare drawn from a prior distribution, navigate through parameter space with\nmovements determined by a reward mechanism blending assessments from the target\ndensity, with the steady state particle configuration approximating the target\ngeometry. Particle-environment interactions are simulated by stochastic\nperturbations and the reward mechanism, which drive particles towards high\ndensity regions while maintaining diversity (e.g. preventing from collapsing\ninto clusters). R-ParVI offers fast, flexible, scalable and stochastic sampling\nand inference for a class of probabilistic models such as those encountered in\nBayesian inference and generative modelling.",
        "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps:\/\/github.com\/HKUDS\/MiniRAG.",
        "In recent years, large language models (LLMs) have revolutionized the field\nof natural language processing. However, they often suffer from knowledge gaps\nand hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances\nLLM reasoning by integrating structured knowledge from external graphs.\nHowever, we identify two key challenges that plague GraphRAG:(1) Retrieving\nnoisy and irrelevant information can degrade performance and (2)Excessive\nreliance on external knowledge suppresses the model's intrinsic reasoning. To\naddress these issues, we propose GraphRAG-FI (Filtering and Integration),\nconsisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering\nemploys a two-stage filtering mechanism to refine retrieved information.\nGraphRAG-Integration employs a logits-based selection strategy to balance\nexternal knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing\nover-reliance on retrievals. Experiments on knowledge graph QA tasks\ndemonstrate that GraphRAG-FI significantly improves reasoning performance\nacross multiple backbone models, establishing a more reliable and effective\nGraphRAG framework.",
        "Multi-trait automated essay scoring (AES) systems provide a fine-grained\nevaluation of an essay's diverse aspects. While they excel in scoring, prior\nsystems fail to explain why specific trait scores are assigned. This lack of\ntransparency leaves instructors and learners unconvinced of the AES outputs,\nhindering their practical use. To address this, we propose a self-explainable\nRationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME\nleverages the reasoning capabilities of large language models (LLMs) by\ndistilling them into a smaller yet effective scorer. This more manageable\nstudent model is optimized to sequentially generate a trait score followed by\nthe corresponding rationale, thereby inherently learning to select a more\njustifiable score by considering the subsequent rationale during training. Our\nfindings indicate that while LLMs underperform in direct AES tasks, they excel\nin rationale generation when provided with precise numerical scores. Thus,\nRaDME integrates the superior reasoning capacities of LLMs into the robust\nscoring accuracy of an optimized smaller model. Extensive experiments\ndemonstrate that RaDME achieves both accurate and adequate reasoning while\nsupporting high-quality multi-trait scoring, significantly enhancing the\ntransparency of AES.",
        "Versatile audio super-resolution (SR) is the challenging task of restoring\nhigh-frequency components from low-resolution audio with sampling rates between\n4kHz and 32kHz in various domains such as music, speech, and sound effects.\nPrevious diffusion-based SR methods suffer from slow inference due to the need\nfor a large number of sampling steps. In this paper, we introduce FlashSR, a\nsingle-step diffusion model for versatile audio super-resolution aimed at\nproducing 48kHz audio. FlashSR achieves fast inference by utilizing diffusion\ndistillation with three objectives: distillation loss, adversarial loss, and\ndistribution-matching distillation loss. We further enhance performance by\nproposing the SR Vocoder, which is specifically designed for SR models\noperating on mel-spectrograms. FlashSR demonstrates competitive performance\nwith the current state-of-the-art model in both objective and subjective\nevaluations while being approximately 22 times faster.",
        "The inverse diffusion flame (IDF) can experience thermoacoustic instability\ndue to variations in power input or flow conditions. However, the dynamical\ntransitions in IDF that lead to this instability when altering control\nparameters have not been thoroughly investigated. In this study, we explore the\ncontrol parameters through two different approaches and employ multifractal\ndetrended fluctuation analysis to characterize the transitions observed prior\nto the onset of thermoacoustic instability in the inverse diffusion flame. Our\nfindings reveal a loss of multifractality near the region associated with\nthermoacoustic instability, which suggests a more ordered behavior. We\ndetermine that the singularity exponent, the width of the multifractal\nspectrum, and the Hurst exponent are reliable indicators of thermoacoustic\ninstability and serve as effective classifiers of dynamical states in inverse\ndiffusion flames.",
        "Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the\nchallenges such as long scan times and expensive equipment by enhancing image\nresolution from low-quality inputs acquired in shorter scan times in clinical\nsettings. However, current SR techniques still have problems such as limited\nability to capture both local and global static patterns effectively and\nefficiently. To address these limitations, we propose Delta-WKV, a novel MRI\nsuper-resolution model that combines Meta-in-Context Learning (MiCL) with the\nDelta rule to better recognize both local and global patterns in MRI images.\nThis approach allows Delta-WKV to adjust weights dynamically during inference,\nimproving pattern recognition with fewer parameters and less computational\neffort, without using state-space modeling. Additionally, inspired by\nReceptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional\nscanning mechanism with time-mixing and channel-mixing structures to capture\nlong-range dependencies while maintaining high-frequency details. Tests on the\nIXI and fastMRI datasets show that Delta-WKV outperforms existing methods,\nimproving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and\ninference times by over 15\\%. These results demonstrate its efficiency and\npotential for clinical use with large datasets and high-resolution imaging.",
        "The upcoming Vera C. Rubin Legacy Survey of Space and Time (LSST) will\ndiscover tens of thousands of astrophysical transients per night, far outpacing\navailable spectroscopic follow-up capabilities. Carefully prioritising\ncandidates for follow-up observations will maximise the scientific return from\nsmall telescopes with a single-object spectrograph. We introduce AAS2RTO, an\nastrophysical transient candidate prioritisation tool written in Python.\nAAS2RTO is flexible in that any number of criteria that consider observed\nproperties of transients can be implemented. The visibility of candidates from\na given observing site is also considered. The prioritised list of candidates\nprovided by AAS2RTO is continually updated when new transient data are made\navailable. Therefore, it can be applied to observing campaigns with a wide\nvariety of scientific motivations. AAS2RTO uses a greedy algorithm to\nprioritise candidates. Candidates are represented by a single numerical value,\nor `score'. Scores are computed by constructing simple numerical factors which\nindividually consider the competing facets of a candidate which make it\nsuitable for follow-up observation. AAS2RTO is currently configured to work\nprimarily with photometric data from the Zwicky Transient Facility (ZTF),\ndistributed by certified LSST community brokers. We provide an example of how\nAAS2RTO can be used by defining a set of criteria to prioritise observations of\ntype Ia supernovae (SNe Ia) close to peak brightness, in preparation for\nobservations with the spectrograph at the Danish-1.54m telescope. Using a\nsample of archival alerts from ZTF, we evaluate the criteria we have designed\nto estimate the number of SNe Ia that we will be able to observe with a 1.5m\ntelescope. Finally, we evaluate the performance of our criteria when applied to\nmock LSST observations of SNe Ia.",
        "Online fraud substantially harms individuals and seniors are\ndisproportionately targeted. While family is crucial for seniors, little\nresearch has empirically examined how they protect seniors against fraud. To\naddress this gap, we employed an inductive thematic analysis of 124 posts and\n16,872 comments on RedNote (Xiaohongshu), exploring the family support\necosystem for senior-targeted online fraud in China. We develop a taxonomy of\nsenior-targeted online fraud from a familial perspective, revealing younger\nmembers often spot frauds hard for seniors to detect, such as unusual charges.\nYounger family members fulfill multiple safeguarding roles, including\npreventative measures, fraud identification, fraud persuasion, loss recovery,\nand education. They also encounter numerous challenges, such as seniors'\nrefusal of help and considerable mental and financial stress. Drawing on these,\nwe develop a conceptual framework to characterize family support in\nsenior-targeted fraud, and outline implications for researchers and\npractitioners to consider the broader stakeholder ecosystem and cultural\naspects.",
        "An approach is presented to address singularities in general relativity using\na complex Riemannian spacetime extension. We demonstrate how this method can be\napplied to both black hole and cosmological singularities, specifically\nfocusing on the Schwarzschild and Kerr black holes and the\nFriedmann-Lema\\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending\nthe relevant coordinates into the complex plane and carefully choosing\nintegration contours, we show that it is possible to regularize these\nsingularities, resulting in physically meaningful, singularity-free solutions\nwhen projected back onto real spacetime. The removal of the singularity at the\nBig Bang allows for a bounce cosmology. This approach offers a potential bridge\nbetween classical general relativity and quantum gravity effects, suggesting a\nway to resolve longstanding issues in gravitational physics without requiring a\nfull theory of quantum gravity.",
        "Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the\ndetection of myocardial scars for post myocardial infarction (MI). LGE MRI\nrequires the injection of a contrast agent, which carries potential side\neffects and increases scanning time and patient discomfort. To address these\nissues, we propose a novel framework that combines cardiac motion observed in\ncine MRI with image texture information to segment the myocardium and scar\ntissue in the left ventricle. Cardiac motion tracking can be formulated as a\nfull cardiac image cycle registration problem, which can be solved via deep\nneural networks. Experimental results prove that the proposed method can\nachieve scar segmentation based on non-contrasted cine images with comparable\naccuracy to LGE MRI. This demonstrates its potential as an alternative to\ncontrast-enhanced techniques for scar detection.",
        "The widespread adoption of deep neural networks (DNNs) requires efficient\ntechniques for safety verification. Existing methods struggle to scale to\nreal-world DNNs, and tremendous efforts are being put into improving their\nscalability. In this work, we propose an approach for improving the scalability\nof DNN verifiers using Conflict-Driven Clause Learning (CDCL) -- an approach\nthat has proven highly successful in SAT and SMT solving. We present a novel\nalgorithm for deriving conflict clauses using UNSAT proofs, and propose several\noptimizations for expediting it. Our approach allows a modular integration of\nSAT solvers and DNN verifiers, and we implement it on top of an interface\ndesigned for this purpose. The evaluation of our implementation over several\nbenchmarks suggests a 2X--3X improvement over a similar approach, with specific\ncases outperforming the state of the art.",
        "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body\/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses.",
        "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https:\/\/www.github.com\/SimonAytes\/SoT.",
        "Transformer-based language models have recently been at the forefront of\nactive research in text generation. However, these models' advances come at the\nprice of prohibitive training costs, with parameter counts in the billions and\ncompute requirements measured in petaflop\/s-decades. In this paper, we\ninvestigate transformer-based architectures for improving model performance in\na low-data regime by selectively replacing attention layers with feed-forward\nand quasi-recurrent neural network layers. We test these architectures on the\nstandard Enwik8 and Wikitext-103 corpora. Our results show that our reduced\narchitectures outperform existing models with a comparable number of\nparameters, and obtain comparable performance to larger models while\nsignificantly reducing the number of parameters.",
        "Semi-supervised graph anomaly detection (GAD) has recently received\nincreasing attention, which aims to distinguish anomalous patterns from graphs\nunder the guidance of a moderate amount of labeled data and a large volume of\nunlabeled data. Although these proposed semi-supervised GAD methods have\nachieved great success, their superior performance will be seriously degraded\nwhen the provided labels are extremely limited due to some unpredictable\nfactors. Besides, the existing methods primarily focus on anomaly detection in\nstatic graphs, and little effort was paid to consider the continuous evolution\ncharacteristic of graphs over time (dynamic graphs). To address these\nchallenges, we propose a novel GAD framework (EL$^{2}$-DGAD) to tackle anomaly\ndetection problem in dynamic graphs with extremely limited labels.\nSpecifically, a transformer-based graph encoder model is designed to more\neffectively preserve evolving graph structures beyond the local neighborhood.\nThen, we incorporate an ego-context hypersphere classification loss to classify\ntemporal interactions according to their structure and temporal neighborhoods\nwhile ensuring the normal samples are mapped compactly against anomalous data.\nFinally, the above loss is further augmented with an ego-context contrasting\nmodule which utilizes unlabeled data to enhance model generalization. Extensive\nexperiments on four datasets and three label rates demonstrate the\neffectiveness of the proposed method in comparison to the existing GAD methods.",
        "Exploiting the first measurements of the same ion species in O+O collisons at\nRHIC and LHC, we propose an experimentally accessible observable to distinguish\nwhether collective behaviour builds up through a hydrodynamic expansion of a\nstrongly interacting QGP or through few rescatterings in a non-equilibrated\ndilute medium. Our procedure allows to disentangle the effects of the initial\nstate geometry and the dynamical response mechanism on the total resulting\nanisotropic flow. We validate the ability of our proposed observable to\ndiscriminate between systems with different interaction rates using results\nfrom event-by-event simulations in kinetic theory in the Relaxation Time\nApproximation (RTA). As a proof of concept, we extract the degree of\nhydrodynamization for Pb+Pb collisions at LHC from experimental data.",
        "Holographic multimode fibre endoscopes have recently shown their ability to\nunveil and monitor deep brain structures with sub-micrometre resolution,\nestablishing themselves as a minimally-invasive technology with promising\napplications in neurobiology. In this approach, holographic control of the\ninput light field entering the multimode fibres is achieved by means of\nwavefront shaping, usually treating the fibre as a complex medium. In contrast\nto other unpredictable and highly scattering complex media, multimode fibres\nfeature symmetries and strong correlations between their input and output\nfields. Both step-index and graded-index multimode fibres offer a specific set\nof such correlations which, when appropriately leveraged, enable generating\nhigh-quality focused pulses with minimal intermodal dispersion. With this, we\nfunnelled pulsed super-resolution STED microscopy with time-gated detection\nthrough a custom multimode fibre probe, combining the correlations of both\nmultimode fibre types. We demonstrate resolution improvements over 3-times\nbeyond the diffraction limit and showcase its applicability in bioimaging. This\nwork provides not only a solution for delivering short pulses through\nstep-index multimode fibre segments but also marks a step towards bringing\nadvanced super-resolution imaging techniques with virtually no depth\nlimitations."
      ]
    }
  },
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
    "start_abstract":"We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "The Burgers equation"
      ],
      "abstract":[
        "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
      ],
      "categories":[
        "math.AP"
      ]
    },
    "list":{
      "title":[
        "Incompressible and vanishing vertical viscosity limit for the\n  compressible Navier-Stokes system with Dirichlet boundary conditions",
        "Regularity properties of certain convolution operators in H\\\"{o}lder\n  spaces",
        "Well-Posedness of Contact Discontinuity Solutions and Vanishing Pressure\n  Limit for the Aw-Rascle Traffic Flow Model",
        "Structural stability of cylindrical supersonic solutions to the steady\n  Euler-Poisson system",
        "Derivation of a Multiscale Ferrofluid Model: Superparamagnetic Behavior\n  due to Fast Spin Flip",
        "Asymptotical Behavior of Global Solutions of the Navier-Stokes-Korteweg\n  Equations with Respect to Capillarity Number at Infinity",
        "Uniqueness of Dirac-harmonic maps from a compact surface with boundary",
        "A semi-adaptive finite difference method for simulating two-sided\n  fractional convection-diffusion quenching problems",
        "Reconstruction of 1-D evolution equations and their initial data from\n  one passive measurement",
        "Spectral multipliers on two-step stratified Lie groups with degenerate\n  group structure",
        "Well-posedness of the nonhomogeneous incompressible\n  Navier-Stokes\/Allen-Cahn system",
        "On isolated singularities of the conformal Gaussian curvature equation\n  and $Q$-curvature equation",
        "The regularity of electronic wave functions in Barron spaces",
        "On the test properties of the Frobenius endomorphism",
        "Twinning in ferromagnetic Heusler Rh2MnSb epitaxial thin films",
        "Cepheids in spectroscopic binary systems -- current status and recent\n  discoveries",
        "Efficient Diffusion Posterior Sampling for Noisy Inverse Problems",
        "Biglobal resolvent analysis of separated flow over a NACA0012 airfoil",
        "Efficient $d$-ary Cuckoo Hashing at High Load Factors by Bubbling Up",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A plastic damage model with mixed isotropic-kinematic hardening for\n  low-cycle fatigue in 7020 aluminum",
        "A Fast Decoding Algorithm for Generalized Reed-Solomon Codes and\n  Alternant Codes",
        "Dirac-type condition for Hamilton-generated graphs",
        "A free boundary approach to the quasistatic evolution of debonding\n  models",
        "The Lagrangian approach to the compressible primitive equations",
        "Chemistry in the Galactic Center",
        "On complex eigenvalues of a real nonsymmetric matrix",
        "Robust triple-q magnetic order with trainable spin vorticity in\n  Na$_2$Co$_2$TeO$_6$"
      ],
      "abstract":[
        "In this paper, we show the incompressible and vanishing vertical viscosity\nlimits for the strong solutions to the isentropic compressible Navier-Stokes\nsystem with anistropic dissipation, in a domain with Dirichlet boundary\nconditions in the general setting of ill-prepared initial data. We establish\nthe uniform regularity estimates with respect to the Mach number $\\epsilon$ and\nthe vertical viscosity $\\nu$ so that the solution exists on a uniform time\ninterval $[0,T_0]$ independent of these parameters. The key steps toward this\ngoal are the careful construction of the approximate solution in the presence\nof both fast oscillations and two kinds of boundary layers together with the\nstability analysis of the remainder. In the process, it is also shown that the\nsolutions of the compressible systems converge to those of the incompressible\nsystem with only horizontal dissipation, after removing the fast waves whose\nhorizontal derivative is bounded in $L_{T_0}^2L_x^2$ by $\\min\\{1,\n(\\epsilon\/\\nu)^{\\frac14}\\}.$",
        "The aim of this paper is to prove a theorem of C.~Miranda on the H\\\"older\nregularity of convolution operators acting on the boundary of an open set in\nthe limiting case in which the open set is of class $C^{1,1}$ and the densities\nare of class $C^{0,1}$. The convolution operators that we consider are\ngeneralizations of those that are associated to layer potential operators,\nwhich are a useful tool for the analysis of boundary value problems.",
        "This paper investigates the well-posedness of contact discontinuity solutions\nand the vanishing pressure limit for the Aw-Rascle traffic flow model with\ngeneral pressure functions. The well-posedness problem is formulated as a free\nboundary problem, where initial discontinuities propagate along linearly\ndegenerate characteristics. By mollifying initial data in Lagrangian\ncoordinates, the problem is reduced to analyzing the limit of classical\nsolutions. To avoid equation degeneracy due to vacuum states, we establish a\nuniform lower bound for density by categorizing discontinuity types at singular\npoints. Using level sets for velocity derivatives, we derive uniform estimates\nfor density and velocity gradients. Through equivalence of coordinate\ntransformations, the well-posedness of contact discontinuity solutions in\noriginal coordinates is rigorously proven. Results show that compressive\ninitial data induce finite-time singularity formation, while rarefactive\ninitial data ensure global existence. Furthermore, we demonstrate that\nsolutions of the Aw-Rascle model converge to those of the pressureless model\nunder vanishing pressure, with matching convergence rates for velocity and\ncharacteristic triangles. By enhancing regularity in non-discontinuous regions,\nwe prove convergence of the Aw-Rascle model's blow-up time to the pressureless\nmodel's blow-up time. Finally, analogous results are extended to the Chaplygin\ngas pressure case.",
        "This paper concerns the structural stability of smooth cylindrically\nsymmetric supersonic Euler-Poisson flows in nozzles. Both three-dimensional and\naxisymmetric perturbations are considered. On one hand, we establish the\nexistence and uniqueness of three-dimensional smooth supersonic solutions to\nthe potential flow model of the steady Euler-Poisson system. On the other hand,\nthe existence and uniqueness of smooth supersonic flows with nonzero vorticity\nto the steady axisymmetric Euler-Poisson system are proved. The problem is\nreduced to solve a nonlinear boundary value problem for a hyperbolic-elliptic\nmixed system. One of the key ingredients in the analysis of three-dimensional\nsupersonic irrotational flows is the well-posedness theory for a linear second\norder hyperbolic-elliptic coupled system, which is achieved by using the\nmultiplier method and the reflection technique to derive the energy estimates.\nFor smooth axisymmetric supersonic flows with nonzero vorticity, the\ndeformation-curl-Poisson decomposition is utilized to reformulate the steady\naxisymmetric Euler-Poisson system as a deformation-curl-Poisson system together\nwith several transport equations, so that one can design a two-layer iteration\nscheme to establish the nonlinear structural stability of the background\nsupersonic flow within the class of axisymmetric rotational flows.",
        "We consider a microscopic model of $N$ magnetic nanoparticles in a Stokes\nflow. We assume that the temperature is above the critical N\\'eel temperature\nsuch that the particles' magnetizations undergo random flip with rate\n$1\/\\varepsilon$. The microscopic system is the modeled through a piecewise\ndeterministic Markov jump process. We show that for large $N$, small particle\nvolume fraction and small $\\varepsilon$, the system can be effectively\ndescribed by a multiscale model.",
        "Vanishing capillarity in the Navier-Stokes-Korteweg (NSK) equations has been\nwidely investigated, in particular, it is well-known that the NSK equations\nconverge to the Navier-Stokes (NS) equations by vanishing capillarity number.\nTo our best knowledge, this paper first investigates the behavior of large\ncapillary number, denoted by $\\kappa^2$, for the global(-in-time) strong\nsolutions with small initial perturbations of the three-dimensional (3D) NSK\nequations in a slab domain with Navier(-slip) boundary condition. Under the\nwell-prepared initial data, we can construct a family of global strong\nsolutions of the 3D incompressible NSK equations with respect to $\\kappa>0$,\nwhere the solutions converge to a unique solution of 2D incompressible NS-like\nequations as $\\kappa$ goes to infinity.",
        "As a commutative version of the supersymmetric nonlinear sigma model,\nDirac-harmonic maps from Riemann surfaces were introduced fifteen years ago.\nThey are critical points of an unbounded conformally invariant functional\ninvolving two fields, a map from a Riemann surface into a Riemannian manifold\nand a section of a Dirac bundle which is the usual spinor bundle twisted with\nthe pull-back of the tangent bundle of the target by the map. As solutions to a\ncoupled nonlinear elliptic system, the existence and regularity theory of\nDirac-harmonic maps has already received much attention, while the general\nuniqueness theory has not been established yet. For uncoupled Dirac-harmonic\nmaps, the map components are harmonic maps. Since the uniqueness theory of\nharmonic maps from a compact surface with boundary is known, it is sufficient\nto consider the uniqueness of the spinor components, which are solutions to the\ncorresponding boundary value problems for a nonlinear Dirac equation. In\nparticular, when the map components belong to $W^{1,p}$ with $p>2$, the spinor\ncomponents are uniquely determined by boundary values and map components. For\ncoupled Dirac-harmonic maps, the map components are not harmonic maps. So the\nuniqueness problem is more difficult to solve. In this paper, we study the\nuniqueness problem on a compact surface with boundary. More precisely, we prove\nthe energy convexity for weakly Dirac-harmonic maps from the unit disk with\nsmall energy. This yields the first uniqueness result about Dirac-harmonic maps\nfrom a surface conformal to the unit disk with small energy and arbitrary\nboundary values.",
        "This paper investigates quenching solutions of an one-dimensional, two-sided\nRiemann-Liouville fractional order convection-diffusion problem. Fractional\norder spatial derivatives are discretized using weighted averaging\napproximations in conjunction with standard and shifted Gr\\\"{u}nwald formulas.\nThe advective term is handled utilizing a straightforward Euler formula,\nresulting in a semi-discretized system of nonlinear ordinary differential\nequations. The conservativeness of the proposed scheme is rigorously proved and\nvalidated through simulation experiments. The study is further advanced to a\nfully discretized, semi-adaptive finite difference method. Detailed analysis is\nimplemented for the monotonicity, positivity and stability of the scheme.\nInvestigations are carried out to assess the potential impacts of the\nfractional order on quenching location, quenching time, and critical length.\nThe computational results are thoroughly discussed and analyzed, providing a\nmore comprehensive understanding of the quenching phenomena modeled through\ntwo-sided fractional order convection-diffusion problems.",
        "We study formally determined inverse problems with passive measurements for\none dimensional evolution equations where the goal is to simultaneously\ndetermine both the initial data as well as the variable coefficients in such an\nequation from the measurement of its solution at a fixed spatial point for a\ncertain amount of time. This can be considered as a one-dimensional model of\nwidely open inverse problems in photo-acoustic and thermo-acoustic tomography.\nWe provide global uniqueness results for wave and heat equations stated on\nbounded or unbounded spatial intervals. Contrary to all previous related\nresults on the subject, we do not impose any genericity assumptions on the\ncoefficients or initial data. Our proofs are based on creating suitable links\nto the well understood spectral theory for 1D Schr\\\"odinger operators. In\nparticular, in the more challenging case of a bounded spatial domain, our proof\nfor the inverse problem partly relies on the following two key ingredients,\nnamely (i) Paley-Wiener type theorems due to Kahane \\cite{Kahane1957SurLF} and\nRemling \\cite{Remling2002SchrdingerOA} that together provide a quantifiable\nlink between support of a compactly supported function and the upper density of\nits vanishing Schr\\\"odinger spectral modes and (ii) a result of Gesztesy and\nSimon \\cite{Gesztesy1999InverseSA} on partial data inverse spectral problems\nfor reconstructing an unknown potential in a 1D Schr\\\"odinger operator from the\nknowledge of only a fraction of its spectrum.",
        "Let $L$ be a sub-Laplacian on a two-step stratified Lie group $G$ of\ntopological dimension $d$. We prove new $L^p$-spectral multiplier estimates\nunder the sharp regularity condition $s>d\\left|1\/p-1\/2\\right|$ in settings\nwhere the group structure of $G$ is degenerate, extending previously known\nresults for the non-degenerate case. Our results include variants of the free\ntwo-step nilpotent group on three generators and Heisenberg-Reiter groups. The\nproof combines restriction type estimates with a detailed analysis of the\nsub-Riemannian geometry of $G$. A key novelty of our approach is the use of a\nrefined spectral decomposition into caps on the unit sphere in the center of\nthe Lie group.",
        "In this paper, we investigate a system coupled by nonhomogeneous\nincompressible Navier-Stokes equations and Allen-Cahn equations describing a\ndiffuse interface for two-phase flow of viscous fluids with different densities\nin a bounded domain $\\Omega\\subset\\mathbb R^d (d=2, 3)$. The mobility is\nallowed to depend on phase variable but non-degenerate. We first prove the\nexistence of global weak solutions to the initial boundary value problem in 2D\nand 3D cases. Then we obtain the existence of local in time strong solutions in\n3D case as well as the global strong solutions in 2D case. Moreover, by\nimposing smallness conditions on the initial data, the 3D local in time strong\nsolution is extended globally, with an exponential decay rate for\nperturbations. At last, we show the weak-strong uniqueness.",
        "In this paper, we study the isolated singularities of the conformal Gaussian\ncurvature equation \\[\n  -\\Delta u = K(x) e^{u} \\quad ~ in ~ B_{1} \\setminus \\{ 0 \\}, \\] where $B_1\n\\setminus \\{ 0 \\} \\subset \\mathbb{R}^2$ is the punctured unit disc. Under the\nassumption that the Gaussian curvature $K \\in L^\\infty(B_1)$ is nonnegative, we\nestablish the asymptotic behavior of solutions near the singularity. When $K\n\\equiv 1$, a similar result has been obtained by Chou and Wan (Pacific J. Math.\n1994) using the method of complex analysis. Our proof is entirely based on the\nPDE method and applies to the general Gaussian curvature $K(x)$. Furthermore,\nour approach is also available for characterizing isolated singularities of the\nconformal $Q$-curvature equation $(-\\Delta)^{\\frac{n}{2}} u = K(x) e^{u}$ in\nany dimension $n\\geq 3$. This equation arises from the prescribing\n$Q$-curvature problem.",
        "The electronic Schr\\\"odinger equation describes the motion of $N$ electrons\nunder Coulomb interaction forces in a field of clamped nuclei. It is proved\nthat its solutions for eigenvalues below the essential spectrum lie in the\nspectral Barron spaces $\\mathcal{B}^s(\\mathbb{R}^{3N})$ for $s<1$. The example\nof the hydrogen ground state shows that this result cannot be improved.",
        "In this paper, we prove two theorems concerning the test properties of the\nFrobenius endomorphism over commutative Noetherian local rings of prime\ncharacteristic $p$. Our first theorem generalizes a result of Funk-Marley on\nthe vanishing of Ext and Tor modules, while our second theorem generalizes one\nof our previous results on maximal Cohen-Macaulay tensor products. In these\nearlier results, we replace $^{e}R$ with a more general module $^{e}M$, where\n$R$ is a Cohen-Macaulay ring, $M$ is a Cohen-Macaulay $R$-module with full\nsupport, and $^{e}M$ is the module viewed as an $R$-module via the $e$-th\niteration of the Frobenius endomorphism. We also provide examples and present\napplications of our results, yielding new characterizations of the regularity\nof local rings.",
        "Epitaxially grown full Heusler alloy of Rh2MnSb thin films were prepared for\nthe first time using DC magnetron sputtering. The films were deposited on MgO\n[001] substrates with a deposition temperature of 600{\\deg}C, 700{\\deg}C, and\n800{\\deg}C. We report the structural, morphological, optical, magneto-optical,\nand magnetic properties of the films with a 200 nm nominal thickness. The\ngrown-at-600{\\deg}C film was close to stoichiometric and exhibited L21 ordering\ntypical for Heusler alloys. The single-phase Rh2MnSb film had a tetragonal\nstructure with lattice parameters close to the bulk material. X-ray\nphotoelectron spectroscopy revealed the metallic character of the film free\nfrom contamination. The tetragonal films exhibited discernible regular twinning\nwith the majority of twin domains with the c-axis perpendicular to the surface\ndue to a substrate constraint. The twin formation was studied by atomic force\nand transmission electron microscopy and by X-ray diffraction. Magnetic\nmeasurements showed TC of about 220-275 K and saturation magnetization of about\n55 emu\/g, close to the bulk material. Magneto-optical Kerr effect measurements\nof the film prepared at 600 {\\deg}C affirmed paramagnetic behavior at room\ntemperature and suggested the half-metallic behavior. The observed properties\nhighlight the potential for further investigations of Rh2MnSb's thin films,\nfocusing on compositional and structural control.",
        "We present a summary of the current knowledge about Cepheids in binary\nsystems. We focus on the most recent findings and discoveries, such as the\nhighly increasing number of confirmed and candidate spectroscopic binary\nCepheids and the progress in determining their physical parameters. This\nincludes new and newly analyzed binary Cepheids in the Milky Way and Magellanic\nClouds. We will provide an update on the project to increase the number of the\nmost valuable Cepheids in double-lined binary (SB2) systems from six to more\nthan 100. To date, we have confirmed 60 SB2 systems, including detecting a\nsignificant orbital motion for 37. We identified systems with orbital periods\nup to five times shorter than the shortest period reported before and systems\nwith mass ratios significantly different from unity (suggesting past binary\ninteractions, including merger events). Both features are essential to\nunderstanding how multiplicity affects the formation and destruction of Cepheid\nprogenitors and how this influences global Cepheid properties. We will also\npresent nine new systems composed of two Cepheids. Only one such double Cepheid\nsystem was known before.",
        "The pretrained diffusion model as a strong prior has been leveraged to\naddress inverse problems in a zero-shot manner without task-specific\nretraining. Different from the unconditional generation, the measurement-guided\ngeneration requires estimating the expectation of clean image given the current\nimage and the measurement. With the theoretical expectation expression, the\ncrucial task of solving inverse problems is to estimate the noisy likelihood\nfunction at the intermediate image sample. Using the Tweedie's formula and the\nknown noise model, the existing diffusion posterior sampling methods perform\ngradient descent step with backpropagation through the pretrained diffusion\nmodel. To alleviate the costly computation and intensive memory consumption of\nthe backpropagation, we propose an alternative maximum-a-posteriori (MAP)-based\nsurrogate estimator to the expectation. With this approach and further density\napproximation, the MAP estimator for linear inverse problem is the solution to\na traditional regularized optimization, of which the loss comprises of data\nfidelity term and the diffusion model related prior term. Integrating the MAP\nestimator into a general denoising diffusion implicit model (DDIM)-like\nsampler, we achieve the general solving framework for inverse problems. Our\napproach highly resembles the existing $\\Pi$GDM without the manifold projection\noperation of the gradient descent direction. The developed method is also\nextended to nonlinear JPEG decompression. The performance of the proposed\nposterior sampling is validated across a series of inverse problems, where both\nVP and VE SDE-based pretrained diffusion models are taken into consideration.",
        "The effects of Reynolds number across $Re=1000$, $2500$, $5000$, and $10000$\non separated flow over a two-dimensional NACA0012 airfoil at an angle of attack\nof $\\alpha=14^\\circ$ are investigated through the biglobal resolvent analysis.\nWe identify modal structures and energy amplifications over a range of\nfrequency, spanwise wavenumber, and discount parameter, providing insights\nacross various timescales. Using temporal discounting, we find that the shear\nlayer dynamics dominates over short time horizons, while the wake dynamics\nbecomes the primary amplification mechanism over long time horizons. Spanwise\neffects also appear over long time horizon, sustained by low frequencies. At a\nfixed timescale, we investigate the influence of Reynolds number on response\nand forcing mode structures, as well as the energy gain over different\nfrequencies. Across all Reynolds numbers, the response modes shift from\nwake-dominated structures at low frequencies to shear layer-dominated\nstructures at higher frequencies. The frequency at which the dominant mechanism\nchanges is independent of the Reynolds number. The response mode structures\nshow similarities across different Reynolds numbers, with local streamwise\nwavelengths only depending on frequency. Comparisons at a different angle of\nattack ($\\alpha=9^\\circ$) show that the transition from wake to shear layer\ndynamics with increasing frequency only occurs if the unsteady flow is\nthree-dimensional. We also study the dominant frequencies associated with wake\nand shear layer dynamics across the angles of attack and Reynolds numbers, and\npresent the characteristic scaling for each mechanism.",
        "A $d$-ary cuckoo hash table is an open-addressed hash table that stores each\nkey $x$ in one of $d$ random positions $h_1(x), h_2(x), \\ldots, h_d(x)$. In the\noffline setting, where all items are given and keys need only be matched to\nlocations, it is possible to support a load factor of $1 - \\epsilon$ while\nusing $d = \\lceil \\ln \\epsilon^{-1} + o(1) \\rceil$ hashes. The online setting,\nwhere keys are moved as new keys arrive sequentially, has the additional\nchallenge of the time to insert new keys, and it has not been known whether one\ncan use $d = O(\\ln \\epsilon^{-1})$ hashes to support $\\poly(\\epsilon^{-1})$\nexpected-time insertions.\n  In this paper, we introduce bubble-up cuckoo hashing, an implementation of\n$d$-ary cuckoo hashing that achieves all of the following properties\nsimultaneously:\n  (1) uses $d = \\lceil \\ln \\epsilon^{-1} + \\alpha \\rceil$ hash locations per\nitem for an arbitrarily small positive constant $\\alpha$.\n  (2) achieves expected insertion time $O(\\delta^{-1})$ for any insertion\ntaking place at load factor $1 - \\delta \\le 1 - \\epsilon$.\n  (3) achieves expected positive query time $O(1)$, independent of $d$ and\n$\\epsilon$.\n  The first two properties give an essentially optimal value of $d$ without\ncompromising insertion time. The third property is interesting even in the\noffline setting: it says that, even though \\emph{negative} queries must take\ntime $d$, positive queries can actually be implemented in $O(1)$ expected time,\neven when $d$ is large.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "The paper at hand presents an in-depth investigation into the fatigue\nbehavior of the high-strength aluminum alloy EN AW-7020 T6 using both\nexperimental and numerical approaches. Two types of specimens are investigated:\na dog-bone specimen subjected to cyclic loading in a symmetric\nstrain-controlled regime, and a compact tension specimen subjected to repeated\nloading and unloading, which leads to damage growth from the notch tip.\nExperimental data from these tests are used to identify the different phases of\nfatigue. Subsequently, a plastic-damage model is developed, incorporating J2\nplasticity with Chaboche-type mixed isotropic-kinematic hardening. A detailed\ninvestigation reveals that the Chaboche model must be blended with a suitable\nisotropic hardening and combined with a proper damage growth model to\naccurately describe cyclic fatigue including large plastic strains up to\nfailure. Multiple back-stress components with independent properties are\nsuperimposed, and exponential isotropic hardening with saturation effects is\nintroduced to improve alignment with experimental results. For damage,\ndifferent stress splits are tested, with the deviatoric\/volumetric split\nproving successful in reproducing the desired degradation in peak stress and\nstiffness. A nonlinear activation function is introduced to ensure smooth\ntransitions between tension and compression. Two damage indices, one for the\ndeviatoric part and one for the volumetric part, are defined, each of which is\ngoverned by a distinct trilinear damage growth function. The governing\ndifferential equation of the problem is regularized by higher-order gradient\nterms to address the ill-posedness induced by softening. Finally, the\nplasticity model is calibrated using finite element simulations of the dog-bone\ntest and subsequently applied to the cyclic loading of the compact tension\nspecimen.",
        "In this paper, it is shown that the syndromes of generalized Reed-Solomon\n(GRS) codes and alternant codes can be characterized in terms of inverse fast\nFourier transform, regardless of code definitions. Then a fast decoding\nalgorithm is proposed, which has a computational complexity of $O(n\\log(n-k) +\n(n-k)\\log^2(n-k))$ for all $(n,k)$ GRS codes and $(n,k)$ alternant codes.\nParticularly, this provides a new decoding method for Goppa codes, which is an\nimportant subclass of alternant codes. When decoding the binary Goppa code with\nlength $8192$ and correction capability $128$, the new algorithm is nearly 10\ntimes faster than traditional methods. The decoding algorithm is suitable for\nthe McEliece cryptosystem, which is a candidate for post-quantum cryptography\ntechniques.",
        "The cycle space $\\mathcal{C}(G)$ of a graph $G$ is defined as the linear\nspace spanned by all cycles in $G$. For an integer $k\\ge 3$, let $\\mathcal{C}_k\n(G)$ denote the subspace of $\\mathcal{C}(G)$ generated by the cycles of length\nexactly $k$. A graph $G$ on $n$ vertices is called Hamilton-generated if\n$\\mathcal{C}_n (G) = \\mathcal{C}(G)$, meaning every cycle in $G$ is a symmetric\ndifference of some Hamilton cycles of $G$. %A necessary condition for this\nproperty is that $n$ must be odd. Heinig (European J. Combin., 2014) showed\nthat for any $\\sigma >0$ and sufficiently large odd $n$, every $n$-vertex graph\nwith minimum degree $(1+ \\sigma)n\/2$ is Hamilton-generated. He further posed\nthe question that whether the minimum degree requirement could be lowered to\nthe Dirac threshold $n\/2$. Recent progress by Christoph, Nenadov, and\nPetrova~(arXiv:2402.01447) reduced the minimum degree condition to $n\/2 + C$\nfor some large constant $C$. In this paper, we resolve Heinig's problem\ncompletely by proving that for sufficiently large odd $n$, every\nHamilton-connected graph $G$ on $n$ vertices with minimum degree at least\n$(n-1)\/2$ is Hamilton-generated. Moreover, this result is tight for the minimum\ndegree and the Hamilton-connected condition. The proof relies on the\nparity-switcher technique introduced by Christoph, et al in their recent work,\nas well as a classification lemma that strengthens a previous result by\nKrivelevich, Lee, and Sudakov~(Trans. Amer. Math. Soc., 2014).",
        "The mechanical process of progressively debonding an adhesive membrane from a\nsubstrate is described as a quasistatic variational evolution of sets and\nherein investigated. Existence of energetic solutions, based on global\nminimisers of a suitable functional together with an energy balance, is\nobtained within the natural class of open sets, improving and simplifying\nprevious results known in literature. The proposed approach relies on an\nequivalent reformulation of the model in terms of the celebrated one-phase\nBernoulli free boundary problem. This point of view allows performing the\nMinimizing Movements scheme in spaces of functions instead of the more\ncomplicated framework of sets. Nevertheless, in order to encompass\nirreversibility of the phenomenon, it remains crucial to keep track of the\ndebonded region at each discrete time-step, thus actually resulting in a\ncoupled algorithm.",
        "This article develops the hydrostatic Lagrangian approach to the compressible\nprimitive equations. A fundamental aspect in the analysis is the investigation\nof the compressible hydrostatic Lam\\'{e} and Stokes operators. Local strong\nwell-posedness for large data and global strong well-posedness for small data\nare established under various assumptions on the pressure law, both in the\npresence and absence of gravity.",
        "Gas and dust in the Galactic Center are subjected to energetic processing by\nintense UV radiation fields, widespread shocks, enhanced rates of cosmic-rays\nand X-rays, and strong magnetic fields. The Giant Molecular Clouds in the\nGalactic Center present a rich chemistry in a wide variety of chemical\ncompounds, some of which are prebiotic. We have conducted unbiased,\nultrasensitive and broadband spectral surveys toward the G+0.693-0.027\nmolecular cloud located in the Galactic Center, which have yielded the\ndiscovery of new complex organic molecules proposed as precursors of the\n\"building blocks\" of life. I will review our current understanding of the\nchemistry in Galactic Center molecular clouds, and summarize the recent\ndetections toward G+0.693-0.027 of key precursors of prebiotic chemistry. All\nthis suggests that the ISM is an important source of prebiotic material that\ncould have contributed to the process of the origin of life on Earth and\nelsewhere in the Universe.",
        "We consider real non-symmetric matrices and their factorisation as a product\nof real symmetric matrices. The number of complex eigenvalues of the original\nmatrix reveals restrictions on such factorisations as we shall prove.",
        "Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$\npossesses novel triple-$\\mathbf{q}$ magnetic order instead of conventional\nsingle-$\\mathbf{q}$ zigzag order. Here we present dedicated experiments in\nsearch for distinct properties expected of the triple-$\\mathbf{q}$ order,\nnamely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking\nfields and fictitious magnetic fields generated by the spin vorticity. In\nstructurally pristine single crystals, we show that $C_3$ symmetry-breaking\nin-plane uniaxial strains do not affect the order's magnetic neutron\ndiffraction signals. We further show that $\\mathbf{c}$-axis propagating light\nexhibits large Faraday rotations in the ordered state due to the spin\nvorticity, the sign of which can be trained via the system's ferrimagnetic\nmoment. These results are in favor of the triple-$\\mathbf{q}$ order in\nNa$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"MRI segmentation of the human brain: challenges, methods, and applications",
    "start_abstract":"Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "U-net: Convolutional networks for biomedical image segmentation"
      ],
      "abstract":[
        "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real\n  and Synthetic Claims",
        "Extending the design space of ontologization practices: Using bCLEARer\n  as an example",
        "Towards AI-assisted Academic Writing",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs",
        "Representation Learning to Advance Multi-institutional Studies with\n  Electronic Health Record Data",
        "ProRCA: A Causal Python Package for Actionable Root Cause Analysis in\n  Real-world Business Scenarios",
        "Unifying and Optimizing Data Values for Selection via\n  Sequential-Decision-Making",
        "RM-PoT: Reformulating Mathematical Problems and Solving via Program of\n  Thoughts",
        "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning",
        "An Expectation-Maximization Algorithm-based Autoregressive Model for the\n  Fuzzy Job Shop Scheduling Problem",
        "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
        "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
        "Robust Evidence for Declining Disruptiveness: Assessing the Role of\n  Zero-Backward-Citation Works",
        "Coresets for Robust Clustering via Black-box Reductions to Vanilla Case",
        "Thermal Radiation Force and Torque on Moving Nanostructures with\n  Anisotropic Optical Response",
        "Efficient Transformed Gaussian Process State-Space Models for\n  Non-Stationary High-Dimensional Dynamical Systems",
        "Wheel-GINS: A GNSS\/INS Integrated Navigation System with a Wheel-mounted\n  IMU",
        "Electron-Chiral Phonon Coupling, Crystal Angular Momentum, and Phonon\n  Chirality",
        "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them",
        "Characterizing the Burst Error Correction Ability of Quantum Cyclic\n  Codes",
        "Global Existence and Nonlinear Stability of Finite-Energy Solutions of\n  the Compressible Euler-Riesz Equations with Large Initial Data of Spherical\n  Symmetry",
        "Probing the hollowing transition of a shell-shaped BEC with collective\n  excitation",
        "Gender Dynamics in Software Engineering: Insights from Research on\n  Concurrency Bug Reproduction",
        "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time\n  Cognitive Task Solving and Reasoning in UAVs",
        "On connected subgraph arrangements",
        "Scale-wise Distillation of Diffusion Models",
        "Revealed Social Networks"
      ],
      "abstract":[
        "Misinformation can be countered with fact-checking, but the process is costly\nand slow. Identifying checkworthy claims is the first step, where automation\ncan help scale fact-checkers' efforts. However, detection methods struggle with\ncontent that is 1) multimodal, 2) from diverse domains, and 3) synthetic. We\nintroduce HintsOfTruth, a public dataset for multimodal checkworthiness\ndetection with $27$K real-world and synthetic image\/claim pairs. The mix of\nreal and synthetic data makes this dataset unique and ideal for benchmarking\ndetection methods. We compare fine-tuned and prompted Large Language Models\n(LLMs). We find that well-configured lightweight text-based encoders perform\ncomparably to multimodal models but the first only focus on identifying\nnon-claim-like content. Multimodal LLMs can be more accurate but come at a\nsignificant computational cost, making them impractical for large-scale\napplications. When faced with synthetic data, multimodal models perform more\nrobustly",
        "Our aim in this paper is to outline how the design space for the\nontologization process is richer than current practice would suggest. We point\nout that engineering processes as well as products need to be designed - and\nidentify some components of the design. We investigate the possibility of\ndesigning a range of radically new practices, providing examples of the new\npractices from our work over the last three decades with an outlier\nmethodology, bCLEARer. We also suggest that setting an evolutionary context for\nontologization helps one to better understand the nature of these new practices\nand provides the conceptual scaffolding that shapes fertile processes. Where\nthis evolutionary perspective positions digitalization (the evolutionary\nemergence of computing technologies) as the latest step in a long evolutionary\ntrail of information transitions. This reframes ontologization as a strategic\ntool for leveraging the emerging opportunities offered by digitalization.",
        "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs.",
        "The adoption of EHRs has expanded opportunities to leverage data-driven\nalgorithms in clinical care and research. A major bottleneck in effectively\nconducting multi-institutional EHR studies is the data heterogeneity across\nsystems with numerous codes that either do not exist or represent different\nclinical concepts across institutions. The need for data privacy further limits\nthe feasibility of including multi-institutional patient-level data required to\nstudy similarities and differences across patient subgroups. To address these\nchallenges, we developed the GAME algorithm. Tested and validated across 7\ninstitutions and 2 languages, GAME integrates data in several levels: (1) at\nthe institutional level with knowledge graphs to establish relationships\nbetween codes and existing knowledge sources, providing the medical context for\nstandard codes and their relationship to each other; (2) between institutions,\nleveraging language models to determine the relationships between\ninstitution-specific codes with established standard codes; and (3) quantifying\nthe strength of the relationships between codes using a graph attention\nnetwork. Jointly trained embeddings are created using transfer and federated\nlearning to preserve data privacy. In this study, we demonstrate the\napplicability of GAME in selecting relevant features as inputs for AI-driven\nalgorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.\nWe then highlight the application of GAME harmonized multi-institutional EHR\ndata in a study of Alzheimer's disease outcomes and suicide risk among patients\nwith mental health disorders, without sharing patient-level data outside\nindividual institutions.",
        "Root Cause Analysis (RCA) is becoming ever more critical as modern systems\ngrow in complexity, volume of data, and interdependencies. While traditional\nRCA methods frequently rely on correlation-based or rule-based techniques,\nthese approaches can prove inadequate in highly dynamic, multi-layered\nenvironments. In this paper, we present a pathway-tracing package built on the\nDoWhy causal inference library. Our method integrates conditional anomaly\nscoring, noise-based attribution, and depth-first path exploration to reveal\nmulti-hop causal chains. By systematically tracing entire causal pathways from\nan observed anomaly back to the initial triggers, our approach provides a\ncomprehensive, end-to-end RCA solution. Experimental evaluations with synthetic\nanomaly injections demonstrate the package's ability to accurately isolate\ntriggers and rank root causes by their overall significance.",
        "Data selection has emerged as a crucial downstream application of data\nvaluation. While existing data valuation methods have shown promise in\nselection tasks, the theoretical foundations and full potential of using data\nvalues for selection remain largely unexplored. In this work, we first\ndemonstrate that data values applied for selection can be naturally\nreformulated as a sequential-decision-making problem, where the optimal data\nvalue can be derived through dynamic programming. We show this framework\nunifies and reinterprets existing methods like Data Shapley through the lens of\napproximate dynamic programming, specifically as myopic reward function\napproximations to this sequential problem. Furthermore, we analyze how\nsequential data selection optimality is affected when the ground-truth utility\nfunction exhibits monotonic submodularity with curvature. To address the\ncomputational challenges in obtaining optimal data values, we propose an\nefficient approximation scheme using learned bipartite graphs as surrogate\nutility models, ensuring greedy selection is still optimal when the surrogate\nutility is correctly specified and learned. Extensive experiments demonstrate\nthe effectiveness of our approach across diverse datasets.",
        "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation.",
        "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market.",
        "The fuzzy job shop scheduling problem (FJSSP) emerges as an innovative\nextension to the job shop scheduling problem (JSSP), incorporating a layer of\nuncertainty that aligns the problem more closely with the complexities of\nreal-world manufacturing environments. This improvement increases the\ncomputational complexity of deriving the solution while improving its\napplicability. In the domain of deterministic scheduling, neural combinatorial\noptimization (NCO) has recently demonstrated remarkable efficacy. However, its\napplication to the realm of fuzzy scheduling has been relatively unexplored.\nThis paper aims to bridge this gap by investigating the feasibility of\nemploying neural networks to assimilate and process fuzzy information for the\nresolution of FJSSP, thereby leveraging the advancements in NCO to enhance\nfuzzy scheduling methodologies. To achieve this, we approach the FJSSP as a\ngenerative task and introduce an expectation-maximization algorithm-based\nautoregressive model (EMARM) to address it. During training, our model\nalternates between generating scheduling schemes from given instances (E-step)\nand adjusting the autoregressive model weights based on these generated schemes\n(M-step). This novel methodology effectively navigates around the substantial\nhurdle of obtaining ground-truth labels, which is a prevalent issue in NCO\nframeworks. In testing, the experimental results demonstrate the superior\ncapability of EMARM in addressing the FJSSP, showcasing its effectiveness and\npotential for practical applications in fuzzy scheduling.",
        "LLM-powered AI agents are an emerging frontier with tremendous potential to\nincrease human productivity. However, empowering AI agents to take action on\ntheir user's behalf in day-to-day tasks involves giving them access to\npotentially sensitive and private information, which leads to a possible risk\nof inadvertent privacy leakage when the agent malfunctions. In this work, we\npropose one way to address that potential risk, by training AI agents to better\nsatisfy the privacy principle of data minimization. For the purposes of this\nbenchmark, by \"data minimization\" we mean instances where private information\nis shared only when it is necessary to fulfill a specific task-relevant\npurpose. We develop a benchmark called AgentDAM to evaluate how well existing\nand future AI agents can limit processing of potentially private information\nthat we designate \"necessary\" to fulfill the task. Our benchmark simulates\nrealistic web interaction scenarios and is adaptable to all existing web\nnavigation agents. We use AgentDAM to evaluate how well AI agents built on top\nof GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation when unnecessary, and show that these agents are often prone to\ninadvertent use of unnecessary sensitive information. We finally propose a\nprompting-based approach that reduces this.",
        "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:\/dxzxy12138.github.io\/PhysReason.",
        "We respond to Holst et al.'s (HATWG) critique that the observed decline in\nscientific disruptiveness demonstrated in Park et al. (PLF) stems from\nincluding works with zero backward citations (0-bcites). Applying their own\nadvocated dataset, metric, and exclusion criteria, we demonstrate statistically\nand practically significant declines in disruptiveness that equal major\nbenchmark transformations in science. Notably, we show that HATWG's own\nregression model -- designed specifically to address their concerns about\n0-bcite works -- reveals highly significant declines for both papers (p<0.001)\nand patents (p<0.001), a finding they neither acknowledge nor interpret. Their\ncritique is undermined by methodological deficiencies, including reliance on\nvisual inspection without statistical assessment, and severe data quality\nissues in their SciSciNet dataset, which contains nearly three times more\n0-bcite papers than our original data. HATWG's departure from established\nscientometric practices -- notably their inclusion of document types and fields\nknown for poor metadata quality -- invalidates their conclusions. Monte Carlo\nsimulations and additional analyses using multiple disruptiveness measures\nacross datasets further validate the robustness of the declining trend. Our\nfindings collectively demonstrate that the observed decline in disruptiveness\nis not an artifact of 0-bcite works but represents a substantive change in\nscientific and technological innovation patterns.",
        "We devise $\\epsilon$-coresets for robust $(k,z)$-Clustering with $m$ outliers\nthrough black-box reductions to vanilla case. Given an $\\epsilon$-coreset\nconstruction for vanilla clustering with size $N$, we construct coresets of\nsize $N\\cdot \\mathrm{poly}\\log(km\\epsilon^{-1}) +\nO_z\\left(\\min\\{km\\epsilon^{-1}, m\\epsilon^{-2z}\\log^z(km\\epsilon^{-1})\n\\}\\right)$ for various metric spaces, where $O_z$ hides $2^{O(z\\log z)}$\nfactors. This increases the size of the vanilla coreset by a small\nmultiplicative factor of $\\mathrm{poly}\\log(km\\epsilon^{-1})$, and the additive\nterm is up to a $(\\epsilon^{-1}\\log (km))^{O(z)}$ factor to the size of the\noptimal robust coreset. Plugging in vanilla coreset results of [Cohen-Addad et\nal., STOC'21], we obtain the first coresets for $(k,z)$-Clustering with $m$\noutliers with size near-linear in $k$ while previous results have size at least\n$\\Omega(k^2)$ [Huang et al., ICLR'23; Huang et al., SODA'25].\n  Technically, we establish two conditions under which a vanilla coreset is as\nwell a robust coreset. The first condition requires the dataset to satisfy\nspecial structures - it can be broken into \"dense\" parts with bounded diameter.\nWe combine this with a new bounded-diameter decomposition that has only $O_z(km\n\\epsilon^{-1})$ non-dense points to obtain the $O_z(km \\epsilon^{-1})$ additive\nbound. Another condition requires the vanilla coreset to possess an extra\nsize-preserving property. We further give a black-box reduction that turns a\nvanilla coreset to the one satisfying the said size-preserving property,\nleading to the alternative $O_z(m\\epsilon^{-2z}\\log^{z}(km\\epsilon^{-1}))$\nadditive bound.\n  We also implement our reductions in the dynamic streaming setting and obtain\nthe first streaming algorithms for $k$-Median and $k$-Means with $m$ outliers,\nusing space $\\tilde{O}(k+m)\\cdot\\mathrm{poly}(d\\epsilon^{-1}\\log\\Delta)$ for\ninputs on the grid $[\\Delta]^d$.",
        "Nanoscale objects moving relative to a thermal radiation bath experience a\ndrag force due to the imbalance in their interaction with the blue- and\nredshifted components of the electromagnetic field. Here, we show that, in\naddition to this drag force, moving nanostructures with an anisotropic optical\nresponse experience a lateral force and a torque that substantially modify\ntheir trajectory. These phenomena emerge from the additional coupling between\nthe electromagnetic field components polarized parallel and perpendicular to\nthe trajectory, enabled by the anisotropic response of the nanostructure. This\nwork unveils the intricate dynamics of anisotropic nanostructures moving in a\nthermal radiation bath.",
        "Gaussian process state-space models (GPSSMs) have emerged as a powerful\nframework for modeling dynamical systems, offering interpretable uncertainty\nquantification and inherent regularization. However, existing GPSSMs face\nsignificant challenges in handling high-dimensional, non-stationary systems due\nto computational inefficiencies, limited scalability, and restrictive\nstationarity assumptions. In this paper, we propose an efficient transformed\nGaussian process state-space model (ETGPSSM) to address these limitations. Our\napproach leverages a single shared Gaussian process (GP) combined with\nnormalizing flows and Bayesian neural networks, enabling efficient modeling of\ncomplex, high-dimensional state transitions while preserving scalability. To\naddress the lack of closed-form expressions for the implicit process in the\ntransformed GP, we follow its generative process and introduce an efficient\nvariational inference algorithm, aided by the ensemble Kalman filter (EnKF), to\nenable computationally tractable learning and inference. Extensive empirical\nevaluations on synthetic and real-world datasets demonstrate the superior\nperformance of our ETGPSSM in system dynamics learning, high-dimensional state\nestimation, and time-series forecasting, outperforming existing GPSSMs and\nneural network-based methods in both accuracy and computational efficiency.",
        "A long-term accurate and robust localization system is essential for mobile\nrobots to operate efficiently outdoors. Recent studies have shown the\nsignificant advantages of the wheel-mounted inertial measurement unit\n(Wheel-IMU)-based dead reckoning system. However, it still drifts over extended\nperiods because of the absence of external correction signals. To achieve the\ngoal of long-term accurate localization, we propose Wheel-GINS, a Global\nNavigation Satellite System (GNSS)\/inertial navigation system (INS) integrated\nnavigation system using a Wheel-IMU. Wheel-GINS fuses the GNSS position\nmeasurement with the Wheel-IMU via an extended Kalman filter to limit the\nlong-term error drift and provide continuous state estimation when the GNSS\nsignal is blocked. Considering the specificities of the GNSS\/Wheel-IMU\nintegration, we conduct detailed modeling and online estimation of the\nWheel-IMU installation parameters, including the Wheel-IMU leverarm and\nmounting angle and the wheel radius error. Experimental results have shown that\nWheel-GINS outperforms the traditional GNSS\/Odometer\/INS integrated navigation\nsystem during GNSS outages. At the same time, Wheel-GINS can effectively\nestimate the Wheel-IMU installation parameters online and, consequently,\nimprove the localization accuracy and practicality of the system. The source\ncode of our implementation is publicly available\n(https:\/\/github.com\/i2Nav-WHU\/Wheel-GINS).",
        "We explicitly derive the wavefunctions of chiral phonons propagating along\nthe helical axis in chiral crystals and clarify the characteristics of\nelectron-phonon interactions in chiral helical crystals. In particular, we\nelucidate how the conservation of not only the crystal momentum (CM) but also\nthe crystal angular momentum (CAM) manifests in the interaction vertex. This\nformulation provides a microscopic framework for describing physical processes\ninvolving chiral phonons. Furthermore, we construct a phononic analogue of\nZilch, a known measure of chirality carried by light, and discuss its\nrelationship with phonon angular momentum.",
        "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs\/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs\/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.",
        "Quantum burst error correction codes (QBECCs) are of great importance to deal\nwith the memory effect in quantum channels. As the most important family of\nQBECCs, quantum cyclic codes (QCCs) play a vital role in the correction of\nburst errors. In this work, we characterize the burst error correction ability\nof QCCs constructed from the Calderbank-Shor-Steane (CSS) and the Hermitian\nconstructions. We determine the burst error correction limit of QCCs and\nquantum Reed-Solomon codes with algorithms in polynomial-time complexities. As\na result, lots of QBECCs saturating the quantum Reiger bound are obtained. We\nshow that quantum Reed-Solomon codes have better burst error correction\nabilities than the previous results. At last, we give the quantum\nerror-trapping decoder (QETD) of QCCs for decoding burst errors. The decoder\nruns in linear time and can decode both degenerate and nondegenerate burst\nerrors. What's more, the numerical results show that QETD can decode much more\ndegenerate burst errors than the nondegenerate ones.",
        "The compressible Euler-Riesz equations are fundamental with wide applications\nin astrophysics, plasma physics, and mathematical biology. In this paper, we\nare concerned with the global existence and nonlinear stability of\nfinite-energy solutions of the multidimensional Euler-Riesz equations with\nlarge initial data of spherical symmetry. We consider both attractive and\nrepulsive interactions for a wide range of Riesz and logarithmic potentials for\ndimensions larger than or equal to two. This is achieved by the inviscid limit\nof the solutions of the corresponding Cauchy problem for the\nNavier-Stokes-Riesz equations. The strong convergence of the vanishing\nviscosity solutions is achieved through delicate uniform estimates in $L^p$. It\nis observed that, even if the attractive potential is super-Coulomb, no\nconcentration is formed near the origin in the inviscid limit. Moreover, we\nprove that the nonlinear stability of global finite-energy solutions for the\nEuler-Riesz equations is unconditional under a spherically symmetric\nperturbation around the steady solutions. Unlike the Coulomb case where the\npotential can be represented locally, the singularity and regularity of the\nnonlocal radial Riesz potential near the origin require careful analysis, which\nis a crucial step. Finally, unlike the Coulomb case, a Gr\\\"onwall type estimate\nis required to overcome the difficulty of the appearance of boundary terms in\nthe sub-Coulomb case and the singularity of the super-Coulomb potential.\nFurthermore, we prove the nonlinear stability of global finite-energy solutions\nfor the compressible Euler-Riesz equations around steady states by employing\nconcentration compactness arguments. Steady states properties are obtained by\nvariational arguments connecting to recent advances in aggregation-diffusion\nequations.",
        "We investigate the hollowing transition of a shell-shaped Bose-Einstein\ncondensate using collective excitations. The shell is created using an\nimmiscible dual-species BEC mixture, with its hollowness controlled by tuning\nthe repulsive interspecies interaction via a Feshbach resonance. Our results\nreveal two distinct monopole modes in which the two condensates oscillate\neither in-phase or out-of-phase. The spectrum of the out-of-phase mode exhibits\na non-monotonic dependence on the interspecies interaction, providing a clear\nsignature of the topology change from a filled to a hollow condensate.\nFurthermore, we find that the critical point of the hollowing transition\ndepends strongly on the number ratio of the two species. Our findings provide a\ndetailed understanding of the topology change in shell-shaped quantum gases and\npave the way for future study of quantum many-body phenomena in curved spaces.",
        "Reproducing concurrency bugs is a complex task due to their unpredictable\nbehavior. Researchers, regardless of gender, are contributing to automating\nthis complex task to aid software developers. While some studies have\ninvestigated gender roles in the broader software industry, limited research\nexists on gender representation specifically among researchers working in\nconcurrent bug reproduction. To address this gap, in this paper, we present a\nliterature review to assess the gender ratio in this field. We also explore\npotential variations in technique selection and bug-type focus across genders.\nOur findings indicate that female researchers are underrepresented compared to\ntheir male counterparts in this area, with a current male-to-female author\nratio of 29:6. Through this study, we emphasize the importance of fostering\ngender equity in software engineering research, ensuring a diversity of\nperspectives in the development of automated bug reproduction tools.",
        "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)\nmodel tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand\nadvanced cognitive abilities. Trained on a dataset comprising over 8,000\nsimulated flight trajectories across three key categories-Human Recognition,\nSymbol Understanding, and Reasoning-the model generates real-time 4D action\ncommands based on first-person visual inputs and textual instructions. To\nfurther enhance performance in intricate scenarios, we propose\nCognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)\nreasoning module to simplify task directives prior to high-frequency control.\nExperimental evaluations using our open-source benchmark, CognitiveDroneBench,\nreveal that while a racing-oriented model (RaceVLA) achieves an overall success\nrate of 31.3%, the base CognitiveDrone model reaches 59.6%, and\nCognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate\nimprovements of up to 30% in critical cognitive tasks, underscoring the\neffectiveness of incorporating advanced reasoning capabilities into UAV control\nsystems. Our contributions include the development of a state-of-the-art VLA\nmodel for UAV control and the introduction of the first dedicated benchmark for\nassessing cognitive tasks in drone operations. The complete repository is\navailable at cognitivedrone.github.io",
        "Recently, Cuntz and K\\\"uhne introduced a particular class of hyperplane\narrangements stemming from a given graph $G$, so called connected subgraph\narrangements $A_G$. In this note we strengthen some of the result from their\nwork and prove new ones for members of this class. For instance, we show that\naspherical members withing this class stem from a rather restricted set of\ngraphs. Specifically, if $A_G$ is an aspherical connected subgraph arrangement,\nthen $A_G$ is free with the unique possible exception when the underlying graph\n$G$ is the complete graph on $4$ nodes.",
        "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
        "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"U-net: Convolutional networks for biomedical image segmentation",
    "start_abstract":"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "MRI segmentation of the human brain: challenges, methods, and applications"
      ],
      "abstract":[
        "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Foliar Uptake of Biocides: Statistical Assessment of Compartmental and\n  Diffusion-Based Models",
        "VenusMutHub: A systematic evaluation of protein mutation effect\n  predictors on small-scale experimental data",
        "A technical review of multi-omics data integration methods: from\n  classical statistical to deep generative approaches",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "Uncertainty and sensitivity analysis of hair growth duration in human\n  scalp follicles under normal and alopecic conditions",
        "Modeling HIF-ILK Interaction Using Continuous Petri Nets",
        "IA generativa aplicada a la detecci\\'on del c\\'ancer a trav\\'es de\n  Resonancia Magn\\'etica",
        "Genomic and pathological analyses of an asymmetric true hermaphroditism\n  case in a female labrador retriever",
        "A Framework for Building Enviromics Matrices in Mixed Models",
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "EMICSS: Added-value annotations for EMDB entries",
        "Operator Learning for Reconstructing Flow Fields from Sparse\n  Measurements: an Energy Transformer Approach",
        "Excitability and oscillations of active droplets",
        "Several classes of linear codes with few weights derived from Weil sums",
        "ByteQC: GPU-Accelerated Quantum Chemistry Package for Large-Scale\n  Systems",
        "What exactly has TabPFN learned to do?",
        "Amplification of turbulence through multiple planar shocks",
        "Diophantine approximation and the subspace theorem",
        "Optimal $L^p$-approximation of convex sets by convex subsets",
        "The Hierarchy of Saturating Matching Numbers",
        "Bayesianize Fuzziness in the Statistical Analysis of Fuzzy Data",
        "Escalation dynamics and the severity of wars",
        "Efficient and stable derivative-free Steffensen algorithm for root\n  finding",
        "Analytical Strategies and Winning Conditions for Elliptic-Orbit\n  Target-Attacker-Defender Game",
        "Parameter Estimation of State Space Models Using Particle Importance\n  Sampling",
        "A family of asymptotically bad wild towers of function fields"
      ],
      "abstract":[
        "The global population increase leads to a high food demand, and to reach this\ntarget products such as pesticides are needed to protect the crops. Research is\nfocusing on the development of new products that can be less harmful to the\nenvironment, and mathematical models are tools that can help to understand the\nmechanism of uptake of pesticides and then guide in the product development\nphase. This paper applies a systematic methodology to model the foliar uptake\nof pesticides, to take into account the uncertainties in the experimental data\nand in the model structure. A comparison between different models is conducted,\nfocusing on the identifiability of model parameters through dynamic sensitivity\nprofiles and correlation analysis. Lastly, data augmentation studies are\nconducted to exploit the model for the design of experiments and to provide a\npractical support to future experimental campaigns, paving the way for further\napplication of model-based design of experiments techniques in the context of\nfoliar uptake.",
        "In protein engineering, while computational models are increasingly used to\npredict mutation effects, their evaluations primarily rely on high-throughput\ndeep mutational scanning (DMS) experiments that use surrogate readouts, which\nmay not adequately capture the complex biochemical properties of interest. Many\nproteins and their functions cannot be assessed through high-throughput methods\ndue to technical limitations or the nature of the desired properties, and this\nis particularly true for the real industrial application scenario. Therefore,\nthe desired testing datasets, will be small-size (~10-100) experimental data\nfor each protein, and involve as many proteins as possible and as many\nproperties as possible, which is, however, lacking. Here, we present\nVenusMutHub, a comprehensive benchmark study using 905 small-scale experimental\ndatasets curated from published literature and public databases, spanning 527\nproteins across diverse functional properties including stability, activity,\nbinding affinity, and selectivity. These datasets feature direct biochemical\nmeasurements rather than surrogate readouts, providing a more rigorous\nassessment of model performance in predicting mutations that affect specific\nmolecular functions. We evaluate 23 computational models across various\nmethodological paradigms, such as sequence-based, structure-informed and\nevolutionary approaches. This benchmark provides practical guidance for\nselecting appropriate prediction methods in protein engineering applications\nwhere accurate prediction of specific functional properties is crucial.",
        "The rapid advancement of high-throughput sequencing and other assay\ntechnologies has resulted in the generation of large and complex multi-omics\ndatasets, offering unprecedented opportunities for advancing precision medicine\nstrategies. However, multi-omics data integration presents significant\nchallenges due to the high dimensionality, heterogeneity, experimental gaps,\nand frequency of missing values across data types. Computational methods have\nbeen developed to address these issues, employing statistical and machine\nlearning approaches to uncover complex biological patterns and provide deeper\ninsights into our understanding of disease mechanisms. Here, we comprehensively\nreview state-of-the-art multi-omics data integration methods with a focus on\ndeep generative models, particularly variational autoencoders (VAEs) that have\nbeen widely used for data imputation and augmentation, joint embedding\ncreation, and batch effect correction. We explore the technical aspects of loss\nfunctions and regularisation techniques including adversarial training,\ndisentanglement and contrastive learning. Moreover, we discuss recent\nadvancements in foundation models and the integration of emerging data\nmodalities, while describing the current limitations and outlining future\ndirections for enhancing multi-modal methodologies in biomedical research.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
        "Oxygen concentration in tumor micro-environment is a well-established signal\nthat can induce aggressive cancer behaviour. In particular, low oxygen levels\n(hypoxia) activate the Hypoxia-Inducible Factor(HIF) pathway which has an array\nof target systems. One of these systems is Integrin-Linked Kinase (ILK)\npathway, which influences key signaling pathways for cell survival,\nproliferation, and migration. Hence, this paper aimed to explore the\ninterconnection between these two pathways. Using the Petri net modeling tool\nSnoopy, an established HIF network model was transformed to be a continuous\nPetri net. Subsequently, the network was expanded to incorporate a feedback\nelement from the ILK pathway to HIF, based on gene expression data. The\nresulting model conserved the oxygen switch response of the original HIF model\nand positively amplified HIF's output. Therefore, this model provides a\nstarting point for establishing a system reflecting crucial effect on\nhypoxia-induced cancer behavior, and could potentially serve as a basis for\nfuture drug development.",
        "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
        "The two main gonadal development disorders in dogs are true hermaphroditism\nand XX male syndrome. True hermaphroditism can be divided into two\nsubcategories: XX sex reversal and XY sex reversal. XX Sry-negative sex\nreversal is more common, and it is characterized by the presence of both\novarian and testicular tissues in an animal. To date, there are 16 cases of\ntrue hermaphroditism reported in the literature, 15 of which are XX true\nhermaphroditism. Hermaphroditism has not been formally documented in labrador\nretrievers, and no case of asymmetric hermaphroditism has been reported in the\nliterature.",
        "This study introduces a framework for constructing enviromics matrices in\nmixed models to integrate genetic and environmental data to enhance phenotypic\npredictions in plant breeding. Enviromics utilizes diverse data sources, such\nas climate and soil, to characterize genotype-by-environment (GxE)\ninteractions. The approach employs block-diagonal structures in the design\nmatrix to incorporate random effects from genetic and envirotypic covariates\nacross trials. The covariance structure is modeled using the Kronecker product\nof the genetic relationship matrix and an identity matrix representing\nenvirotypic effects, capturing genetic and environmental variability. This dual\nrepresentation enables more accurate crop performance predictions across\nenvironments, improving selection strategies in breeding programs. The\nframework is compatible with existing mixed model software, including rrBLUP\nand BGLR, and can be extended for more complex interactions. By combining\ngenetic relationships and environmental influences, this approach offers a\npowerful tool for advancing GxE studies and accelerating the development of\nimproved crop varieties.",
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Motivation: The Electron Microscopy Data Bank (EMDB) is a key repository for\n3D electron microscopy (3DEM) data but lacks comprehensive annotations and\nconnections to most of the related biological, functional, and structural data.\nThis limitation arises from the optional nature of such information to reduce\ndepositor burden and the complexity of maintaining up-to-date external\nreferences, often requiring depositor consent. To address these challenges, we\ndeveloped EMICSS (EMDB Integration with Complexes, Structures, and Sequences),\nan independent system that automatically updates cross-references with over 20\nexternal resources, including UniProt, AlphaFold DB, PubMed, Complex Portal and\nGene Ontology. Results: EMICSS (https:\/\/www.ebi.ac.uk\/emdb\/emicss) annotations\nare accessible in multiple formats for every EMDB entry and its linked\nresources, and programmatically via the EMDB Application Programming Interface\n(API). EMICSS plays a crucial role supporting the EMDB website, with\nannotations being used on entry pages, statistics, and in the search system.\nAvailability and implementation: EMICSS is implemented in Python and it is an\nopen-source, distributed under the EMBL-EBI license, with core code available\non GitHub (https:\/\/github.com\/emdb-empiar\/added_annotations).",
        "Machine learning methods have shown great success in various scientific\nareas, including fluid mechanics. However, reconstruction problems, where full\nvelocity fields must be recovered from partial observations, remain\nchallenging. In this paper, we propose a novel operator learning framework for\nsolving reconstruction problems by using the Energy Transformer (ET), an\narchitecture inspired by associative memory models. We formulate reconstruction\nas a mapping from incomplete observed data to full reconstructed fields. The\nmethod is validated on three fluid mechanics examples using diverse types of\ndata: (1) unsteady 2D vortex street in flow past a cylinder using simulation\ndata; (2) high-speed under-expanded impinging supersonic jets impingement using\nSchlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The\nresults demonstrate the ability of ET to accurately reconstruct complex flow\nfields from highly incomplete data (90\\% missing), even for noisy experimental\nmeasurements, with fast training and inference on a single GPU. This work\nprovides a promising new direction for tackling reconstruction problems in\nfluid mechanics and other areas in mechanics, geophysics, weather prediction,\nand beyond.",
        "In living cells, cycles of formation and dissolution of liquid droplets can\nmediate biological functions such as DNA repair. However, the minimal\nphysicochemical prerequisite for such droplet oscillations remains elusive.\nHere, we present a simple model composed of only two independent chemical\ncomponents with their diffusive and chemical fluxes governed by non-equilibrium\nthermodynamics. There is turnover of fuel that maintains a chemical reaction\naway from equilibrium, leading to active droplets. We find that a single active\ndroplet undergoes a pitchfork-bifurcation in the droplet volume upon increasing\nthe fueling strength. Strikingly, the active droplet becomes excitable upon\nadding a further chemical reaction. For sufficient fueling, the system\nundergoes self-sustained oscillations cycling between droplet formation and\ndissolution. The minimal nature of our model suggests self-sustained active\ndroplets as functional modules for de novo life.",
        "Linear codes with few weights have applications in secret sharing,\nauthentication codes, association schemes and strongly regular graphs. In this\npaper, several classes of $t$-weight linear codes over ${\\mathbb F}_{q}$ are\npresented with the defining sets given by the intersection, difference and\nunion of two certain sets, where $t=3,4,5,6$ and $q$ is an odd prime power. By\nusing Weil sums and Gauss sums, the parameters and weight distributions of\nthese codes are determined completely. Moreover, three classes of optimal codes\nmeeting the Griesmer bound are obtained, and computer experiments show that\nmany (almost) optimal codes can be derived from our constructions.",
        "Applying quantum chemistry algorithms to large-scale systems requires\nsubstantial computational resources scaled with the system size and the desired\naccuracy. To address this, ByteQC, a fully-functional and efficient package for\nlarge-scale quantum chemistry simulations, has been open-sourced at\nhttps:\/\/github.com\/bytedance\/byteqc, leveraging recent advances in\ncomputational power and many-body algorithms.\n  Regarding computational power, several standard algorithms are efficiently\nimplemented on modern GPUs, ranging from mean-field calculations (Hartree-Fock\nand density functional theory) to post-Hartree-Fock methods such as\nM{\\o}ller-Plesset perturbation theory, random phase approximation, coupled\ncluster methods, and quantum Monte Carlo methods. For the algorithmic approach,\nwe also employ a quantum embedding method, which significantly expands the\ntractable system size while preserving high accuracy at the gold-standard\nlevel.\n  All these features have been systematically benchmarked. For standalone\nalgorithms, the benchmark results demonstrate up to a 60$\\times$ speedup when\ncompared to 100-core CPUs. Additionally, the tractable system sizes have been\nsignificantly expanded: 1,610 orbitals for coupled cluster with single and\ndouble excitations (1,380 orbitals with perturbative triple excitations),\n11,040 orbitals for M{\\o}ller-Plesset perturbation theory of second order,\n37,120 orbitals for mean-field calculations under open boundary conditions, and\nover 100,000 orbitals for periodic boundary conditions. For the advanced\nquantum embedding feature, two representative examples are demonstrated: the\nwater cluster problem (2,752 orbitals) and a water monomer adsorbed on a boron\nnitride surface (3,929 orbitals), achieving the gold-standard accuracy.",
        "TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform\nin-context learning on fresh tabular classification problems, was presented at\nthe last ICLR conference. To better understand its behavior, we treat it as a\nblack-box function approximator generator and observe its generated function\napproximations on a varied selection of training datasets. Exploring its\nlearned inductive biases in this manner, we observe behavior that is at turns\neither brilliant or baffling. We conclude this post with thoughts on how these\nresults might inform the development, evaluation, and application of prior-data\nfitted networks (PFNs) in the future.",
        "We study the amplification of isotropic, incompressible turbulence through\nmultiple planar, collisional shocks, using analytical linear theory. There are\ntwo limiting cases we explore. The first assumes shocks occur rapidly in time\nsuch that the turbulence does not evolve between shocks. Whereas the second\ncase allows enough time for turbulence to isotropize between each shock. For\nthe latter case, through a quasi-equation-of-state, we show that the weak\nmulti-shock limit is agnostic to the distinction between thermal and vortical\nturbulent pressures, like an isotropic volumetric compression. When turbulence\ndoes not return to isotropy between shocks, the generated anisotropy -- itself\na function of shock strength -- can feedback on amplification by further\nshocks, altering choices for maximal or minimal amplification. In addition for\nthis case, we find that amplification is sensitive to the shock ordering. We\nmap how choices of shock strength can impact these amplification differences\ndue to ordering, finding, for example, shock pairs which lead to identical mean\npost-shock fields (density, temperature, pressure) but maximally distinct\nturbulent amplification.",
        "Diophantine approximation explores how well irrational numbers can be\napproximated by rationals, with foundational results by Dirichlet, Hurwitz, and\nLiouville culminating in Roth's theorem. Schmidt's subspace theorem extends\nRoth's results to higher dimensions, with profound implications to Diophantine\nequations and transcendence theory. This article provides a self-contained and\naccessible exposition of Roth's theorem and Schlickewei's refinement of the\nsubspace theorem, with an emphasis on proofs. The arguments presented are\nclassical and approachable for readers with a background in algebraic number\ntheory, serving as a streamlined, yet condensed reference for these fundamental\nresults.",
        "Given a convex set $\\Omega$ of $\\mathbb{R}^n$, we consider the shape\noptimization problem of finding a convex subset $\\omega\\subset \\Omega$, of a\ngiven measure, minimizing the $p$-distance functional $$\\mathcal{J}_p(\\omega)\n:= \\left(\\int_{\\mathbb{S}^{n-1}} |h_\\Omega-h_\\omega|^p\nd\\mathcal{H}^{n-1}\\right)^{\\frac{1}{p}},$$ where $1 \\le p <\\infty$ and\n$h_\\omega$ and $h_\\Omega$ are the support functions of $\\omega$ and the fixed\ncontainer $\\Omega$, respectively.\n  We prove the existence of solutions and show that this minimization problem\n$\\Gamma$-converges, when $p$ tends to $+\\infty$, towards the problem of finding\na convex subset $\\omega\\subset \\Omega$, of a given measure, minimizing the\nHausdorff distance to the convex $\\Omega$.\n  In the planar case, we show that the free parts of the boundary of the\noptimal shapes, i.e., those that are in the interior of $\\Omega$, are given by\npolygonal lines.\n  Still in the $2-d$ setting, from a computational perspective, the classical\nmethod based on optimizing Fourier coefficients of support functions is not\nefficient, as it is unable to efficiently capture the presence of segments on\nthe boundary of optimal shapes. We subsequently propose a method combining\nFourier analysis and a recent numerical scheme, allowing to obtain accurate\nresults, as demonstrated through numerical experiments.",
        "In this paper, we study three matching problems all of which came up quite\nrecently in the field of machine teaching. The cost of a matching is defined in\nsuch a way that, for some formal model of teaching, it equals (or bounds) the\nnumber of labeled examples needed to solve a given teaching task. We show how\nthe cost parameters associated with these problems depend on each other and how\nthey are related to other well known combinatorial parameters (like, for\ninstance, the VC-dimension).",
        "Fuzzy data, prevalent in social sciences and other fields, capture\nuncertainties arising from subjective evaluations and measurement imprecision.\nDespite significant advancements in fuzzy statistics, a unified inferential\nregression-based framework remains undeveloped. Hence, we propose a novel\napproach for analyzing bounded fuzzy variables within a regression framework.\nBuilding on the premise that fuzzy data result from a process analogous to\nstatistical coarsening, we introduce a conditional probabilistic approach that\nlinks observed fuzzy statistics (e.g., mode, spread) to the underlying,\nunobserved statistical model, which depends on external covariates. The\ninferential problem is addressed using Approximate Bayesian methods, mainly\nthrough a Gibbs sampler incorporating a quadratic approximation of the\nposterior distribution. Simulation studies and applications involving external\nvalidations are employed to evaluate the effectiveness of the proposed approach\nfor fuzzy data analysis. By reintegrating fuzzy data analysis into a more\ntraditional statistical framework, this work provides a significant step toward\nenhancing the interpretability and applicability of fuzzy statistical methods\nin many applicative contexts.",
        "Although very large wars remain an enduring threat in global politics, we\nlack a clear understanding of how some wars become large and costly, while most\ndo not. There are three possibilities: large conflicts start with and maintain\nintense fighting, they persist over a long duration, or they escalate in\nintensity over time. Using detailed within-conflict data on civil and\ninterstate wars 1946--2008, we show that escalation dynamics -- variations in\nfighting intensity within an armed conflict -- play a fundamental role in\nproducing large conflicts and are a generic feature of both civil and\ninterstate wars. However, civil wars tend to deescalate when they become very\nlarge, limiting their overall severity, while interstate wars exhibit a\npersistent risk of continual escalation. A non-parametric model demonstrates\nthat this distinction in escalation dynamics can explain the differences in the\nhistorical sizes of civil vs. interstate wars, and explain Richardson's Law\ngoverning the frequency and severity of interstate conflicts over the past 200\nyears. Escalation dynamics also drive enormous uncertainty in forecasting the\neventual sizes of both hypothetical and ongoing civil wars, indicating a need\nto better understand the causes of escalation and deescalation within\nconflicts. The close relationship between the size, and hence the cost, of an\narmed conflict and its potential for escalation has broad implications for\ntheories of conflict onset or termination and for risk assessment in\ninternational relations.",
        "We explore a family of numerical methods, based on the Steffensen divided\ndifference iterative algorithm, that do not evaluate the derivative of the\nobjective functions. The family of methods achieves second-order convergence\nwith two function evaluations per iteration with marginal additional\ncomputational cost. An important side benefit of the method is the improvement\nin stability for different initial conditions compared to the vanilla\nSteffensen method. We present numerical results for scalar functions, fields,\nand scalar fields. This family of methods outperforms the Steffensen method\nwith respect to standard quantitative metrics in most cases.",
        "This paper proposes an analytical framework for the orbital\nTarget-Attacker-Defender game with a non-maneuvering target along elliptic\norbits. Focusing on the linear quadratic game, we derive an analytical solution\nto the matrix Riccati equation, which yields analytical Nash-equilibrium\nstrategies for all players. Based on the analytical strategies, we derive the\nanalytical form of the necessary and sufficient winning conditions for the\nattacker. The simulation results show good consistency between the analytical\nand numerical methods, exhibiting 0.004$\\%$ relative error in the cost\nfunction. The analytical method achieves over 99.9$\\%$ reduction in CPU time\ncompared to the conventional numerical method, strengthening the advantage of\ndeveloping the analytical strategies. Furthermore, we verify the proposed\nwinning conditions and investigate the effects of eccentricity on the game\noutcomes. Our analysis reveals that for games with hovering initial states, the\ninitial position of the defender should be constrained inside a mathematically\ndefinable set to ensure that the attacker wins the game. This constrained set\nfurthermore permits geometric interpretation through our proposed framework.\nThis work establishes the analytical framework for orbital\nTarget-Attacker-Defender games, providing fundamental insights into the\nsolution analysis of the game.",
        "State-space models have been used in many applications, including\neconometrics, engineering, medical research, etc. The maximum likelihood\nestimation (MLE) of the static parameter of general state-space models is not\nstraightforward because the likelihood function is intractable. It is popular\nto use the sequential Monte Carlo(SMC) method to perform gradient ascent\noptimisation in either offline or online fashion. One problem with existing\nonline SMC methods for MLE is that the score estimators are inconsistent, i.e.\nthe bias does not vanish with increasing particle size. In this paper, two SMC\nalgorithms are proposed based on an importance sampling weight function to use\neach set of generated particles more efficiently. The first one is an offline\nalgorithm that locally approximates the likelihood function using importance\nsampling, where the locality is adapted by the effective sample size (ESS). The\nsecond one is a semi-online algorithm that has a computational cost linear in\nthe particle size and uses score estimators that are consistent. We study its\nconsistency and asymptotic normality. Their computational superiority is\nillustrated in numerical studies for long time series.",
        "In a previous work general conditions were given to prove the infiniteness of\nthe genus of certain towers of function fields over a perfect field. It was\nshown that many examples where particular cases of those general results. In\nthis paper the genus of a family of wild towers of function fields will be\nconsidered together with a result with less restrictive sufficient conditions\nfor a wild tower to have infinite genus."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b1",
    "start_title":"Microreactors gain wider use as alternative to batch production",
    "start_abstract":"The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Reliability of coherent systems whose operating life is defined by the\n  lifetime and power of the components",
        "Semicomplete multipartite weakly distance-regular digraphs",
        "Left invertible quasi-isometric liftings",
        "On the compactness of the support of solitary waves of the complex\n  saturated nonlinear Schr{\\\"o}dinger equation and related problems",
        "Affineness on Noetherian graded rings, algebras and Hopf algebras",
        "On Robust Aggregation for Distributed Data",
        "Constrained polynomial roots and a modulated approach to Schur stability",
        "Biased branching random walks on Bienaym\\'e--Galton--Watson trees",
        "Distal Causal Excursion Effects: Modeling Long-Term Effects of\n  Time-Varying Treatments in Micro-Randomized Trials",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Crossover from ballistic transport to normal diffusion: a kinetic view",
        "On Traces in Categories of Contractions (Extended Abstract)",
        "Local-global principle for isogenies of elliptic curves over quadratic\n  fields",
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors"
      ],
      "abstract":[
        "We consider systems whose lifetime is measured by the time of physical\ndegradation of components, as well as the degree of power each component\ncontributes to the system. The lifetimes of the components of the system are\nrandom variables. The power that each component contributes to the system is\nthe product of a random variable and a time-decreasing stable function. The\noperational reliability of these systems is investigated and shown that it is\ndetermined by the joint lifetime functions of the order statistics and their\nconcomitants. In addition to general formulas, examples are given using some\nknown life distributions, and graphs of the operation life functions are shown.",
        "A digraph is semicomplete multipartite if its underlying graph is a complete\nmultipartite graph. As a special case of semicomplete multipartite digraphs,\nJ{\\o}rgensen et al. \\cite{JG14} initiated the study of doubly regular team\ntournaments. As a natural extension, we introduce doubly regular team\nsemicomplete multipartite digraphs and show that such digraphs fall into three\ntypes. Furthermore, we give a characterization of all semicomplete multipartite\ncommutative weakly distance-regular digraphs.",
        "Quasi-isometric liftings similar to isometries, for the operators similar to\ncontractions in Hilbert spaces, are investigated. The existence of such\nliftings is established, and their applications are explored for specific\noperator classes, including quasicontractions. A particular focus is placed on\noperators that admit left invertible minimal quasi-isometric liftings. These\noperators are characterized within the framework of $A$-contractions, and the\nmatrix structures of their liftings are analyzed, highlighting parallels with\nthe isometric liftings of contractions.",
        "We study the vectorial stationary Schr\\\"odinger equation $-\\Delta\nu+a\\,U+b\\,u=F,$ with a saturated nonlinearity $U=u\/|u|$ and with some complex\ncoefficients $(a,b)\\in\\mathbb{C}^2$. Besides the existence and uniqueness of\nsolutions for the Dirichlet and Neumann problems, we prove the compactness of\nthe support of the solution, under suitable conditions on $(a,b)$ and even when\nthe source in the right hand side $F(x)$ is not vanishing for large values of\n$|x|.$ The proof of the compactness of the support uses a local energy method,\ngiven the impossibility of applying the maximum principle. We also consider the\nassociate Schr\\\"{o}dinger-Poisson system when coupling with a simple magnetic\nfield. Among other consequences, our results give a rigorous proof of the\nexistence of ``solitons with compact support\" claimed, without any proof, by\nseveral previous authors.",
        "In this note, we show that every Noetherian graded ring with an affine degree\nzero part is affine. As a result, a Noetherian graded Hopf algebra whose degree\nzero component is a commutative or a cocommutative Hopf subalgebra is affine.\nMoreover, we show that the braided Hopf algebra of a Noetherian graded Hopf\nalgebra is affine.",
        "When data are stored across multiple locations, directly pooling all the data\ntogether for statistical analysis may be impossible due to communication costs\nand privacy concerns. Distributed computing systems allow the analysis of such\ndata, by getting local servers to separately process their own statistical\nanalyses and using a central processor to aggregate the local statistical\nresults. Naive aggregation of local statistics using simple or weighted\naverages, is vulnerable to contamination within a distributed computing system.\nThis paper develops and investigates a Huber-type aggregation method for\nlocally computed M-estimators to handle contamination in the local estimates.\nOur implementation of this aggregation method requires estimating the\nasymptotic variance-covariance matrix of the M-estimator, which we accomplish\nusing a robust spatial median approach. Theoretically, the Huber-type\naggregation achieves the same convergence rate as if all the data were pooled.\nWe establish its asymptotic normality for making inferences, including\njustifying a two-step approach for detecting contamination in the distributed\ncomputing system. Extensive simulation studies are conducted to validate the\ntheoretical results and the usefulness of our proposed approach is demonstrated\non U.S. airline data.",
        "It is common in stability analysis to linearize a system and investigate the\nspectrum of the Jacobian matrix. This approach faces the challenge of\ndetermining the matrix spectrum when the coefficients depend on parameters or\nwhen the characteristic polynomial is more than quartic. In this paper, we\nreverse the classical process and use the authors' work on global stability to\nfind sufficient conditions on the coefficients that ensure the zeros of the\ncharacteristic polynomial are in the open unit disk. This leads to an algorithm\nthat begins by testing the $\\ell_1$-norm of the polynomial, and if it is not\nless than two, perform an iteration process that can be implemented with\nmoderate effort. We give examples that show the effectiveness of our method\nwhen compared with the Jury's algorithm. Last, we formalize our constructions\nin terms of semialgebraic sets.",
        "We study $\\lambda$-biased branching random walks on\nBienaym\\'e--Galton--Watson trees in discrete time. We consider the maximal\ndisplacement at time $n$, $\\max_{\\vert u \\vert =n} \\vert X(u) \\vert$, and show\nthat it almost surely grows at a deterministic, linear speed. We characterize\nthis speed with the help of the large deviation rate function of the\n$\\lambda$-biased random walk of a single particle. A similar result is given\nfor the minimal displacement at time $n$, $\\min_{\\vert u \\vert =n} \\vert X(u)\n\\vert$.",
        "Micro-randomized trials (MRTs) play a crucial role in optimizing digital\ninterventions. In an MRT, each participant is sequentially randomized among\ntreatment options hundreds of times. While the interventions tested in MRTs\ntarget short-term behavioral responses (proximal outcomes), their ultimate goal\nis to drive long-term behavior change (distal outcomes). However, existing\ncausal inference methods, such as the causal excursion effect, are limited to\nproximal outcomes, making it challenging to quantify the long-term impact of\ninterventions. To address this gap, we introduce the distal causal excursion\neffect (DCEE), a novel estimand that quantifies the long-term effect of\ntime-varying treatments. The DCEE contrasts distal outcomes under two excursion\npolicies while marginalizing over most treatment assignments, enabling a\nparsimonious and interpretable causal model even with a large number of\ndecision points. We propose two estimators for the DCEE -- one with\ncross-fitting and one without -- both robust to misspecification of the outcome\nmodel. We establish their asymptotic properties and validate their performance\nthrough simulations. We apply our method to the HeartSteps MRT to assess the\nimpact of activity prompts on long-term habit formation. Our findings suggest\nthat prompts delivered earlier in the study have a stronger long-term effect\nthan those delivered later, underscoring the importance of intervention timing\nin behavior change. This work provides the critically needed toolkit for\nscientists working on digital interventions to assess long-term causal effects\nusing MRT data.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "The crossover between dispersion patterns has been frequently observed in\nvarious systems. Inspired by the pathway-based kinetic model for E. coli\nchemotaxis that accounts for the intracellular adaptation process and noise, we\npropose a kinetic model that can exhibit a crossover from ballistic transport\nto normal diffusion at the population level. At the particle level, this\nframework aligns with a stochastic individual-based model. Using numerical\nsimulations and rigorous asymptotic analysis, we demonstrate this crossover\nboth analytically and computationally. Notably, under suitable scaling, the\nmodel reveals two distinct limits in which the macroscopic densities exhibit\neither ballistic transport or normal diffusion.",
        "Traced monoidal categories are used to model processes that can feed their\noutputs back to their own inputs, abstracting iteration. The category of finite\ndimensional Hilbert spaces with the direct sum tensor is not traced. But\nsurprisingly, in 2014, Bartha showed that the monoidal subcategory of\nisometries is traced. The same holds for coisometries, unitary maps, and\ncontractions. This suggests the possibility of feeding outputs of quantum\nprocesses back to their own inputs, analogous to iteration. In this paper, we\nshow that Bartha's result is not specifically tied to Hilbert spaces, but works\nin any dagger additive category with Moore-Penrose pseudoinverses (a natural\ndagger-categorical generalization of inverses).",
        "In this paper, we prove that the local-global principle of $11$-isogenies for\nelliptic curves over quadratic fields does not fail. This gives a positive\nanswer to a conjecture by Banwait and Cremona. The proof is based on the\ndetermination of the set of quadratic points on the modular curve\n$X_{D_{10}}(11)$.",
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b8",
    "start_title":"How flocculation can explain coexistence in the chemostat",
    "start_abstract":"We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Reliability of coherent systems whose operating life is defined by the\n  lifetime and power of the components",
        "Semicomplete multipartite weakly distance-regular digraphs",
        "Left invertible quasi-isometric liftings",
        "On the compactness of the support of solitary waves of the complex\n  saturated nonlinear Schr{\\\"o}dinger equation and related problems",
        "Affineness on Noetherian graded rings, algebras and Hopf algebras",
        "On Robust Aggregation for Distributed Data",
        "Constrained polynomial roots and a modulated approach to Schur stability",
        "Biased branching random walks on Bienaym\\'e--Galton--Watson trees",
        "Distal Causal Excursion Effects: Modeling Long-Term Effects of\n  Time-Varying Treatments in Micro-Randomized Trials",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Crossover from ballistic transport to normal diffusion: a kinetic view",
        "On Traces in Categories of Contractions (Extended Abstract)",
        "Local-global principle for isogenies of elliptic curves over quadratic\n  fields",
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors"
      ],
      "abstract":[
        "We consider systems whose lifetime is measured by the time of physical\ndegradation of components, as well as the degree of power each component\ncontributes to the system. The lifetimes of the components of the system are\nrandom variables. The power that each component contributes to the system is\nthe product of a random variable and a time-decreasing stable function. The\noperational reliability of these systems is investigated and shown that it is\ndetermined by the joint lifetime functions of the order statistics and their\nconcomitants. In addition to general formulas, examples are given using some\nknown life distributions, and graphs of the operation life functions are shown.",
        "A digraph is semicomplete multipartite if its underlying graph is a complete\nmultipartite graph. As a special case of semicomplete multipartite digraphs,\nJ{\\o}rgensen et al. \\cite{JG14} initiated the study of doubly regular team\ntournaments. As a natural extension, we introduce doubly regular team\nsemicomplete multipartite digraphs and show that such digraphs fall into three\ntypes. Furthermore, we give a characterization of all semicomplete multipartite\ncommutative weakly distance-regular digraphs.",
        "Quasi-isometric liftings similar to isometries, for the operators similar to\ncontractions in Hilbert spaces, are investigated. The existence of such\nliftings is established, and their applications are explored for specific\noperator classes, including quasicontractions. A particular focus is placed on\noperators that admit left invertible minimal quasi-isometric liftings. These\noperators are characterized within the framework of $A$-contractions, and the\nmatrix structures of their liftings are analyzed, highlighting parallels with\nthe isometric liftings of contractions.",
        "We study the vectorial stationary Schr\\\"odinger equation $-\\Delta\nu+a\\,U+b\\,u=F,$ with a saturated nonlinearity $U=u\/|u|$ and with some complex\ncoefficients $(a,b)\\in\\mathbb{C}^2$. Besides the existence and uniqueness of\nsolutions for the Dirichlet and Neumann problems, we prove the compactness of\nthe support of the solution, under suitable conditions on $(a,b)$ and even when\nthe source in the right hand side $F(x)$ is not vanishing for large values of\n$|x|.$ The proof of the compactness of the support uses a local energy method,\ngiven the impossibility of applying the maximum principle. We also consider the\nassociate Schr\\\"{o}dinger-Poisson system when coupling with a simple magnetic\nfield. Among other consequences, our results give a rigorous proof of the\nexistence of ``solitons with compact support\" claimed, without any proof, by\nseveral previous authors.",
        "In this note, we show that every Noetherian graded ring with an affine degree\nzero part is affine. As a result, a Noetherian graded Hopf algebra whose degree\nzero component is a commutative or a cocommutative Hopf subalgebra is affine.\nMoreover, we show that the braided Hopf algebra of a Noetherian graded Hopf\nalgebra is affine.",
        "When data are stored across multiple locations, directly pooling all the data\ntogether for statistical analysis may be impossible due to communication costs\nand privacy concerns. Distributed computing systems allow the analysis of such\ndata, by getting local servers to separately process their own statistical\nanalyses and using a central processor to aggregate the local statistical\nresults. Naive aggregation of local statistics using simple or weighted\naverages, is vulnerable to contamination within a distributed computing system.\nThis paper develops and investigates a Huber-type aggregation method for\nlocally computed M-estimators to handle contamination in the local estimates.\nOur implementation of this aggregation method requires estimating the\nasymptotic variance-covariance matrix of the M-estimator, which we accomplish\nusing a robust spatial median approach. Theoretically, the Huber-type\naggregation achieves the same convergence rate as if all the data were pooled.\nWe establish its asymptotic normality for making inferences, including\njustifying a two-step approach for detecting contamination in the distributed\ncomputing system. Extensive simulation studies are conducted to validate the\ntheoretical results and the usefulness of our proposed approach is demonstrated\non U.S. airline data.",
        "It is common in stability analysis to linearize a system and investigate the\nspectrum of the Jacobian matrix. This approach faces the challenge of\ndetermining the matrix spectrum when the coefficients depend on parameters or\nwhen the characteristic polynomial is more than quartic. In this paper, we\nreverse the classical process and use the authors' work on global stability to\nfind sufficient conditions on the coefficients that ensure the zeros of the\ncharacteristic polynomial are in the open unit disk. This leads to an algorithm\nthat begins by testing the $\\ell_1$-norm of the polynomial, and if it is not\nless than two, perform an iteration process that can be implemented with\nmoderate effort. We give examples that show the effectiveness of our method\nwhen compared with the Jury's algorithm. Last, we formalize our constructions\nin terms of semialgebraic sets.",
        "We study $\\lambda$-biased branching random walks on\nBienaym\\'e--Galton--Watson trees in discrete time. We consider the maximal\ndisplacement at time $n$, $\\max_{\\vert u \\vert =n} \\vert X(u) \\vert$, and show\nthat it almost surely grows at a deterministic, linear speed. We characterize\nthis speed with the help of the large deviation rate function of the\n$\\lambda$-biased random walk of a single particle. A similar result is given\nfor the minimal displacement at time $n$, $\\min_{\\vert u \\vert =n} \\vert X(u)\n\\vert$.",
        "Micro-randomized trials (MRTs) play a crucial role in optimizing digital\ninterventions. In an MRT, each participant is sequentially randomized among\ntreatment options hundreds of times. While the interventions tested in MRTs\ntarget short-term behavioral responses (proximal outcomes), their ultimate goal\nis to drive long-term behavior change (distal outcomes). However, existing\ncausal inference methods, such as the causal excursion effect, are limited to\nproximal outcomes, making it challenging to quantify the long-term impact of\ninterventions. To address this gap, we introduce the distal causal excursion\neffect (DCEE), a novel estimand that quantifies the long-term effect of\ntime-varying treatments. The DCEE contrasts distal outcomes under two excursion\npolicies while marginalizing over most treatment assignments, enabling a\nparsimonious and interpretable causal model even with a large number of\ndecision points. We propose two estimators for the DCEE -- one with\ncross-fitting and one without -- both robust to misspecification of the outcome\nmodel. We establish their asymptotic properties and validate their performance\nthrough simulations. We apply our method to the HeartSteps MRT to assess the\nimpact of activity prompts on long-term habit formation. Our findings suggest\nthat prompts delivered earlier in the study have a stronger long-term effect\nthan those delivered later, underscoring the importance of intervention timing\nin behavior change. This work provides the critically needed toolkit for\nscientists working on digital interventions to assess long-term causal effects\nusing MRT data.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "The crossover between dispersion patterns has been frequently observed in\nvarious systems. Inspired by the pathway-based kinetic model for E. coli\nchemotaxis that accounts for the intracellular adaptation process and noise, we\npropose a kinetic model that can exhibit a crossover from ballistic transport\nto normal diffusion at the population level. At the particle level, this\nframework aligns with a stochastic individual-based model. Using numerical\nsimulations and rigorous asymptotic analysis, we demonstrate this crossover\nboth analytically and computationally. Notably, under suitable scaling, the\nmodel reveals two distinct limits in which the macroscopic densities exhibit\neither ballistic transport or normal diffusion.",
        "Traced monoidal categories are used to model processes that can feed their\noutputs back to their own inputs, abstracting iteration. The category of finite\ndimensional Hilbert spaces with the direct sum tensor is not traced. But\nsurprisingly, in 2014, Bartha showed that the monoidal subcategory of\nisometries is traced. The same holds for coisometries, unitary maps, and\ncontractions. This suggests the possibility of feeding outputs of quantum\nprocesses back to their own inputs, analogous to iteration. In this paper, we\nshow that Bartha's result is not specifically tied to Hilbert spaces, but works\nin any dagger additive category with Moore-Penrose pseudoinverses (a natural\ndagger-categorical generalization of inverses).",
        "In this paper, we prove that the local-global principle of $11$-isogenies for\nelliptic curves over quadratic fields does not fail. This gives a positive\nanswer to a conjecture by Banwait and Cremona. The proof is based on the\ndetermination of the set of quadratic points on the modular curve\n$X_{D_{10}}(11)$.",
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Effect of bioclogging in porous media on complex conductivity signatures",
    "start_abstract":"Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1",
        "b8"
      ],
      "title":[
        "Microreactors gain wider use as alternative to batch production",
        "How flocculation can explain coexistence in the chemostat"
      ],
      "abstract":[
        "The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
        "We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth."
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Leveraging 13C NMR spectroscopic data derived from SMILES to predict the\n  functionality of small biomolecules by machine learning: a case study on\n  human Dopamine D1 receptor antagonists",
        "Toxicological Evaluation of Phytochemicals and Heavy Metals in Ficus\n  exasperata Vahl (Sandpaper) Leaves obtained in Birnin Kebbi, Nigeria",
        "From FAIR to CURE: Guidelines for Computational Models of Biological\n  Systems",
        "Comprehensive Analysis of Bioactive Peptides from Cuminum cyminum L.\n  Seeds: Sequence Identification and Pharmacological Evaluation",
        "Higher serum 25(OH)D concentration is associated with lower risk of\n  metabolic syndrome among Aboriginal and Torres Strait Islander peoples in\n  Australia",
        "Cytogenetic, Hematobiochemical, and Histopathological Assessment of\n  Albino Rats (Rattus norvegicus) Fed on Gluten Extracts",
        "Co-Translational mRNA Decay in Plants: Recent advances and future\n  directions",
        "Human Genome Book: Words, Sentences and Paragraphs",
        "Data Sharing in the PRIMED Consortium: Design, implementation, and\n  recommendations for future policymaking",
        "Conditional Success of Adaptive Therapy: The Role of Treatment-Holiday\n  Thresholds Revealed by Mathematical Modeling",
        "An Asymptotic Analysis of Bivalent Monoclonal Antibody-Antigen Binding",
        "Toward a General Theory for the Scaling and Universality of Thermal\n  Responses in Biology",
        "Reverse Markov Learning: Multi-Step Generative Models for Complex\n  Distributions",
        "CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in\n  Cities",
        "Malware Detection based on API calls",
        "The Southern Photometrical Local Universe Survey (S-PLUS): searching for\n  metal-poor dwarf galaxies",
        "Integrating Spatiotemporal Vision Transformer into Digital Twins for\n  High-Resolution Heat Stress Forecasting in Campus Environments",
        "The Regular Ricci-Inverse Cosmology with Multiple Anticurvature Scalars",
        "Compliance while resisting: a shear-thickening fluid controller for\n  physical human-robot interaction",
        "Benchmarking Vision-Language Models on Optical Character Recognition in\n  Dynamic Video Environments",
        "Adaptive Negative Damping Control for User-Dependent Multi-Terrain\n  Walking Assistance with a Hip Exoskeleton",
        "MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency\n  Focusing on Explainability Techniques",
        "Typographic Attacks in a Multi-Image Setting",
        "DDRM-PR: Fourier Phase Retrieval using Denoising Diffusion Restoration\n  Models",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Adaptive Entanglement Routing with Deep Q-Networks in Quantum Networks",
        "Radio pulse search from Aql X-1"
      ],
      "abstract":[
        "This study contributes to ongoing research which aims to predict small\nbiomolecule functionality using Carbon-13 Nuclear Magnetic Resonance ($^{13}$C\nNMR) spectrum data and machine learning (ML). The approach was demonstrated\nusing a bioassay on human dopamine D1 receptor antagonists. The Simplified\nMolecular Input Line Entry System (SMILES) notations of compounds in this\nbioassay were extracted and converted into spectroscopic data by software\ndesigned for this purpose. The resulting data was then used for ML with\nscikit-learn algorithms. The ML models were trained by 27,756 samples and\ntested by 5,466. From the estimators K-Nearest neighbor, Decision Tree\nClassifier, Random Forest Classifier, Gradient Boosting Classifier, XGBoost\nClassifier, and Support Vector Classifier, the last performed the best,\nachieving 71.5 % accuracy, 77.4 % precision, 60.6% recall, 68 % F1, 71.5 % ROC,\nand 0.749 cross-validation score with 0.005 standard deviation. The methodology\ncan be applied to predict any functionality of any compound when relevant data\nare available. It was hypothesized also that increasing the number of samples\nwould increase accuracy. In addition to the SMILES $^{13}$C NMR spectrum ML\nmodel, the time- , and cost-efficient CID_SID ML model was developed. This\nmodel allows researchers who have developed a compound and obtained its PubChem\nCID and SID to check whether their compound is also a human dopamine D1\nreceptor antagonist based solely on the PubChem identifiers. The metrics of the\nCID_SID ML model were 80.2% accuracy, 86.3% precision, 70.4% recall, 77.6% F1,\n79.9% ROC, five-fold cross-validation score of 0.8071 with 0.0047 Standard\ndeviation.",
        "Objective: Ficus exasperata Vahl (Sandpaper tree) is extensively used in\nNigeria to treat diseases, but a dearth of documentation about its toxicity\nexists. This information is crucial because pollutants can contaminate\nmedicinal plants. This study determined the heavy metal and phytochemical\ncontent of methanolic leaf extract of F exasperata obtained in Birnin Kebbi,\nNigeria. Material and Methods: The lethality of the plant was also assessed\nusing 70 wild shrimps divided equally into seven groups. Group 1 (negative\ncontrol), groups 2 and 3 (positive controls) were exposed to 500 and 1000ppm of\nformaldehyde, respectively; and groups 4-7 were exposed to 1000, 2000, 4000,\nand 8000ppm of extracts, respectively, for 96 hours. Results: The\nphytochemistry revealed high levels of flavonoids and saponins and moderate\nlevels of tannins and phenols. The heavy metal analysis revealed non-tolerable\nlevels of cadmium, copper, and lead, while zinc was within the tolerable limit.\nThe negative control recorded 10% mortality, 1000 and 2000 ppm (20% each),\n4000ppm (70%), and 8000 ppm (100%). Conclusion: These results inferred safe\ndoses of the plant's extract in low and medium concentrations but toxic and\nfatal at high doses over a period of time. Consumers are advised to seek an\nexpert's guidance before using it.",
        "Guidelines for managing scientific data have been established under the FAIR\nprinciples requiring that data be Findable, Accessible, Interoperable, and\nReusable. In many scientific disciplines, especially computational biology,\nboth data and models are key to progress. For this reason, and recognizing that\nsuch models are a very special type of 'data', we argue that computational\nmodels, especially mechanistic models prevalent in medicine, physiology and\nsystems biology, deserve a complementary set of guidelines. We propose the CURE\nprinciples, emphasizing that models should be Credible, Understandable,\nReproducible, and Extensible. We delve into each principle, discussing\nverification, validation, and uncertainty quantification for model credibility;\nthe clarity of model descriptions and annotations for understandability;\nadherence to standards and open science practices for reproducibility; and the\nuse of open standards and modular code for extensibility and reuse. We outline\nrecommended and baseline requirements for each aspect of CURE, aiming to\nenhance the impact and trustworthiness of computational models, particularly in\nbiomedical applications where credibility is paramount. Our perspective\nunderscores the need for a more disciplined approach to modeling, aligning with\nemerging trends such as Digital Twins and emphasizing the importance of data\nand modeling standards for interoperability and reuse. Finally, we emphasize\nthat given the non-trivial effort required to implement the guidelines, the\ncommunity moves to automate as many of the guidelines as possible.",
        "Cuminum cyminum L. (cumin) is a medicinal and edible plant widely used in\ntraditional Chinese medicine (TCM) for treating various ailments, including\ndiarrhea, abdominal pain, inflammation, asthma, and diabetes. While previous\nresearch has primarily focused on its essential oils, studies on its\nprotein-derived bioactive peptides remain limited. In this study, we employed\nan innovative extraction method to isolate peptides from cumin seeds for the\nfirst time and screened their biological activities, revealing significant\nantimicrobial, antioxidant, and hypoglycemic properties. Guided by bioactivity,\nwe utilized advanced separation and structural identification techniques,\nincluding Matrix-Assisted Laser Desorption\/Ionization Time-of-Flight Mass\nSpectrometry (MALDI-TOF\/TOF MS\/MS), to systematically purify and characterize\ncumin-derived peptides. A total of 479 unique peptide sequences were identified\nusing Mascot software and the SwissProt\/UniProt_Bos databases. Among these, 15\nhighly bioactive peptides were selected for further analysis based on\nbioactivity and toxicity predictions using PeptideRanker and ToxinPred.\nStructural characterization revealed key features, such as {\\alpha}-helices and\n\\b{eta}-sheets, associated with their multifunctional activities. This study\nprovides the first comprehensive analysis of bioactive peptides from Cuminum\ncyminum L. seeds, elucidating their potential as antimicrobial, antioxidant,\nand hypoglycemic agents. These findings not only clarify the pharmacological\nbasis of cumin's traditional uses but also lay a theoretical foundation for the\ndevelopment of novel therapeutic agents from this medicinal plant.",
        "Although previous observational studies have shown associations between serum\n25-hydroxyvitamin D (25(OH)D) concentration and metabolic syndrome, this\nassociation has not yet been investigated among Aboriginal and Torres Strait\nIslander peoples. We aimed to investigate the association between serum 25(OH)D\nconcentration and metabolic syndrome and its risk factors in this population\ngroup. We used cross-sectional data from the 2012-2013 Australian Aboriginal\nand Torres Strait Islander Health Survey. Metabolic syndrome is defined as\nhaving 3 or more risk factors: elevated waist circumference, elevated\ntriglycerides, low high-density lipoprotein (HDL) cholesterol, elevated blood\npressure, or elevated fasting blood glucose. We used binomial logistic\nregression to test associations between serum 25(OH)D concentration and\nmetabolic syndrome, and multiple linear regression to test associations between\nserum 25(OH)D concentration and each risk factor. We included the following\ncovariates: age, sex, smoking status, education level, socio-economic status,\nremoteness of location, season, and body mass index (BMI). After adjusting for\ncovariates, we found that each 10 nmol\/L increase in serum 25(OH)D\nconcentration was statistically significantly associated with a 16% lower risk\nof metabolic syndrome (odds ratio: 0.84, 95% confidence interval: 0.76, 0.92)\nand a 2.1 cm (95% confidence interval: 1.65, 2.57) lower waist circumference\n(BMI was not included in the model for waist circumference). We found small\ninverse associations between serum 25(OH)D concentration and all other risk\nfactors except systolic blood pressure. Given that higher serum 25(OH)D\nconcentration may confer metabolic health benefits, promoting vitamin D\nsufficiency may be beneficial for this population.",
        "Background: Literature shows that most of the information on the toxicity of\ngluten is generated from survey and observational studies, resulting in\ninconsistent outcomes and a decrease in the acceptability of gluten-rich foods.\nTo determine gluten's safety, an in-depth in vitro and in vivo toxicological\nexamination is required. This enables scientists to come up with ameliorative\nstrategies if it turns out to have side effects, and consumers' trust can be\nrestored. Objectives: The objective of this study was to assess the toxicity of\ngluten extracts on albino rats (Rattus norvegicus). Materials and Methods:\nTwenty-four rats were randomly selected and divided into four groups, each\ncomprising six rats. Group 1 (control) rats were fed on pellet feeds and groups\n2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts,\nrespectively. The rats' body weights and reactions were observed for 90 days\nbefore blood samples were collected for hematobiochemical and micronucleus\ntests. Histopathological examinations of the liver and kidneys were also\nperformed. Results: There was no difference (P > 0.05) in body weight, blood\nglucose level, or micronuclei between the control and treated rats. The\nlymphocytes, alkaline phosphatase, alanine transaminase, total protein, and\ncalcium ions of the test rats were all significantly (P < 0.05) altered but\nremained within the normal ranges. Other hematobiochemical parameters,\nincluding packed cell volume, hemoglobin, white and red blood cells, aspartate\ntransaminase, albumin, sodium ions, potassium ions, chloride ions, and urea,\nrevealed no marked changes. The treated rats' livers and kidneys showed no\nhistopathological changes. Conclusion: Gluten had no adverse effects. However,\nit altered hematobiochemical parameters, particularly the lymphocytes, alkaline\nphosphatase, alanine transaminase, total protein, and calcium ions.",
        "Tight regulation of messenger RNA (mRNA) stability is essential to ensure\naccurate gene expression in response to developmental and environmental cues.\nmRNA stability is controlled by mRNA decay pathways, which have traditionally\nbeen proposed to occur independently of translation. However, the recent\ndiscovery of a co-translational mRNA decay pathway (also known as CTRD) reveals\nthat mRNA translation and decay can be coupled. While being translated, a mRNA\ncan be targeted for degradation. This pathway was first described in yeast and\nrapidly identified in several plant species. This review explores recent\nadvances in our understanding of CTRD in plants, emphasizing its regulation and\nits importance for development and stress response. The different metrics used\nto assess CTRD activity are also presented. Furthermore, this review outlines\nfuture directions to explore the importance of mRNA decay in maintaining mRNA\nhomeostasis in plants.",
        "Since the completion of the human genome sequencing project in 2001,\nsignificant progress has been made in areas such as gene regulation editing and\nprotein structure prediction. However, given the vast amount of genomic data,\nthe segments that can be fully annotated and understood remain relatively\nlimited. If we consider the genome as a book, constructing its equivalents of\nwords, sentences, and paragraphs has been a long-standing and popular research\ndirection. Recently, studies on transfer learning in large language models have\nprovided a novel approach to this challenge.Multilingual transfer ability,\nwhich assesses how well models fine-tuned on a source language can be applied\nto other languages, has been extensively studied in multilingual pre-trained\nmodels. Similarly, the transfer of natural language capabilities to \"DNA\nlanguage\" has also been validated. Building upon these findings, we first\ntrained a foundational model capable of transferring linguistic capabilities\nfrom English to DNA sequences. Using this model, we constructed a vocabulary of\nDNA words and mapped DNA words to their English equivalents.Subsequently, we\nfine-tuned this model using English datasets for paragraphing and sentence\nsegmentation to develop models capable of segmenting DNA sequences into\nsentences and paragraphs. Leveraging these models, we processed the GRCh38.p14\nhuman genome by segmenting, tokenizing, and organizing it into a \"book\"\ncomprised of genomic \"words,\" \"sentences,\" and \"paragraphs.\" Additionally,\nbased on the DNA-to-English vocabulary mapping, we created an \"English version\"\nof the genomic book. This study offers a novel perspective for understanding\nthe genome and provides exciting possibilities for developing innovative tools\nfor DNA search, generation, and analysis.",
        "Sharing diverse genomic and other biomedical datasets is critical to advance\nscientific discoveries and their equitable translation to improve human health.\nHowever, data sharing remains challenging in the context of legacy datasets,\nevolving policies, multi-institutional consortium science, and international\nstakeholders. The NIH-funded Polygenic Risk Methods in Diverse Populations\n(PRIMED) Consortium was established to improve the performance of polygenic\nrisk estimates for a broad range of health and disease outcomes with global\nimpacts. Improving polygenic risk score performance across genetically diverse\npopulations requires access to large, diverse cohorts. We report on the design\nand implementation of data sharing policies and procedures developed in PRIMED\nto aggregate and analyze data from multiple, heterogeneous sources while\nadhering to existing data sharing policies for each integrated dataset. We\ndescribe two primary data sharing mechanisms: coordinated dbGaP applications\nand a Consortium Data Sharing Agreement, as well as provide alternatives when\nindividual-level data cannot be shared within the Consortium (e.g., federated\nanalyses). We also describe technical implementation of Consortium data sharing\nin the NHGRI Analysis Visualization and Informatics Lab-space (AnVIL) cloud\nplatform, to share derived individual-level data, genomic summary results, and\nmethods workflows with appropriate permissions. As a Consortium making\nsecondary use of pre-existing data sources, we also discuss challenges and\npropose solutions for release of individual- and summary-level data products to\nthe broader scientific community. We make recommendations for ongoing and\nfuture policymaking with the goal of informing future consortia and other\nresearch activities.",
        "Adaptive therapy (AT) improves cancer treatment by controlling the\ncompetition between sensitive and resistant cells through treatment holidays.\nThis study highlights the critical role of treatment-holiday thresholds in AT\nfor tumors composed of drug-sensitive and resistant cells. Using a\nLotka-Volterra model, the research compares AT with maximum tolerated dose\ntherapy and intermittent therapy, showing that AT's success largely depends on\nthe threshold at which treatment is paused and resumed, as well as on the\ncompetition between sensitive and resistant cells. Three scenarios of\ncomparison between AT and other therapies are identified: uniform-decline,\nconditional-improve, and uniform-improve, illustrating that optimizing the\ntreatment-holiday threshold is crucial for AT effectiveness. Tumor composition,\nincluding initial tumor burden and the proportion of resistant cells,\ninfluences outcomes. Adjusting threshold values enables AT to suppress\nresistant subclones, preserving sensitive cells, ultimately improving\nprogression-free survival. These findings emphasize the importance of\npersonalized treatment strategies potentially enhancing long-term therapeutic\noutcomes.",
        "Ligand-receptor interactions are fundamental to many biological processes.\nFor example in antibody-based immunotherapies, the dynamics of an antibody\nbinding with its target antigen directly influence the potency and efficacy of\nmonoclonal antibody (mAb) therapies. In this paper, we present an asymptotic\nanalysis of an ordinary differential equation (ODE) model of bivalent\nantibody-antigen binding in the context of mAb cancer therapies, highlighting\nthe added complexity associated with bivalency of the antibody. To understand\nwhat drives the complex temporal dynamics of bivalent antibody-antigen binding,\nwe construct asymptotic approximations to the model's solutions at different\ntimescales and antibody concentrations that are in good agreement with\nnumerical simulations of the full model. We show how the dynamics differ\nbetween two scenarios; a region where unbound antigens are abundant, and one\nwhere the number of unbound antigens is small such that the dominant balance\nwithin the model equations changes. Of particular importance to the potency and\nefficacy of mAb treatments are the values of quantities such as antigen\noccupancy and bound antibody number. We use the results of our asymptotic\nanalysis to approximate the long-time values of these quantities that could be\ncombined with experimental data to facilitate parameter estimation.",
        "We developed a theory showing that under appropriate normalizations and\nrescalings, temperature response curves show a remarkably regular behavior and\nfollow a general, universal law. The impressive universality of temperature\nresponse curves remained hidden due to various curve-fitting models not\nwell-grounded in first principles. In addition, this framework has the\npotential to explain the origin of different scaling relationships in thermal\nperformance in biology, from molecules to ecosystems. Here, we summarize the\nbackground, principles and assumptions, predictions, implications, and possible\nextensions of this theory.",
        "Learning complex distributions is a fundamental challenge in contemporary\napplications. Generative models, such as diffusion models, have demonstrated\nremarkable success in overcoming many limitations of traditional statistical\nmethods. Shen and Meinshausen (2024) introduced engression, a generative\napproach based on scoring rules that maps noise (and covariates, if available)\ndirectly to data. While effective, engression struggles with highly complex\ndistributions, such as those encountered in image data. In this work, we extend\nengression to improve its capability in learning complex distributions. We\npropose a framework that defines a general forward process transitioning from\nthe target distribution to a known distribution (e.g., Gaussian) and then\nlearns a reverse Markov process using multiple engression models. This reverse\nprocess reconstructs the target distribution step by step. Our approach\nsupports general forward processes, allows for dimension reduction, and\nnaturally discretizes the generative process. As a special case, when using a\ndiffusion-based forward process, our framework offers a method to discretize\nthe training and inference of diffusion models efficiently. Empirical\nevaluations on simulated and climate data validate our theoretical insights,\ndemonstrating the effectiveness of our approach in capturing complex\ndistributions.",
        "Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy\ngeneration from fossil fuels for industry, automobile, and domestic\nrequirements. Forecasting the evolution of CO in real-time can enable the\ndeployment of effective early warning systems and intervention strategies.\nHowever, the computational cost associated with the physics and chemistry-based\nsimulation makes it prohibitive to implement such a model at the city and\ncountry scale. To address this challenge, here, we present a machine learning\nmodel based on neural operator, namely, Complex Neural Operator for Air Quality\n(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this\nby developing a country-level model for short-term (hourly) and long-term\n(72-hour) forecasts of CO concentrations. Our model outperforms\nstate-of-the-art models such as Fourier neural operators (FNO) and provides\nreliable predictions for both short and long-term forecasts. We further analyse\nthe capability of the model to capture extreme events and generate forecasts in\nurban cities in India. Interestingly, we observe that the model predicts the\nnext hour CO concentrations with R2 values greater than 0.95 for all the cities\nconsidered. The deployment of such a model can greatly assist the governing\nbodies to provide early warning, plan intervention strategies, and develop\neffective strategies by considering several what-if scenarios. Altogether, the\npresent approach could provide a fillip to real-time predictions of CO\npollution in urban cities.",
        "Malware attacks pose a significant threat in today's interconnected digital\nlandscape, causing billions of dollars in damages. Detecting and identifying\nfamilies as early as possible provides an edge in protecting against such\nmalware. We explore a lightweight, order-invariant approach to detecting and\nmitigating malware threats: analyzing API calls without regard to their\nsequence. We publish a public dataset of over three hundred thousand samples\nand their function call parameters for this task, annotated with labels\nindicating benign or malicious activity. The complete dataset is above 550GB\nuncompressed in size. We leverage machine learning algorithms, such as random\nforests, and conduct behavioral analysis by examining patterns and anomalies in\nAPI call sequences. By investigating how the function calls occur regardless of\ntheir order, we can identify discriminating features that can help us identify\nmalware early on. The models we've developed are not only effective but also\nefficient. They are lightweight and can run on any machine with minimal\nperformance overhead, while still achieving an impressive F1-Score of over\n85\\%. We also empirically show that we only need a subset of the function call\nsequence, specifically calls to the ntdll.dll library, to identify malware. Our\nresearch demonstrates the efficacy of this approach through empirical\nevaluations, underscoring its accuracy and scalability. The code is open source\nand available at Github along with the dataset on Zenodo.",
        "The metal content of a galaxy's interstellar medium reflects the interplay\nbetween different evolutionary processes such as feedback from massive stars\nand the accretion of gas from the intergalactic medium. Despite the expected\nabundance of low-luminosity galaxies, the low-mass and low-metallicity regime\nremains relatively understudied. Since the properties of their interstellar\nmedium resemble those of early galaxies, identifying such objects in the Local\nUniverse is crucial to understand the early stages of galaxy evolution. We used\nthe DR3 catalog of the Southern Photometric Local Universe Survey (S-PLUS) to\nselect low-metallicity dwarf galaxy candidates based on color selection\ncriteria typical of metal-poor, star-forming, low-mass systems. The final\nsample contains approximately 50 candidates. Spectral energy distribution\nfitting of the 12 S-PLUS bands reveals that $\\sim$ 90\\% of the candidates are\nbest fit by models with very low stellar metallicities. We obtained long-slit\nobservations with the Gemini Multi-Object Spectrograph to follow-up a pilot\nsample and confirm whether these galaxies have low metallicities. We find\noxygen abundances in the range $7.35<$ 12 + log(O\/H) $< 7.93$ (5\\% to 17\\% of\nthe solar value), confirming their metal-poor nature. Most targets are outliers\nin the mass-metallicity relation, i.e. they display a low metal content\nrelative to their observed stellar masses. In some cases, perturbed optical\nmorphologies might give evidence of dwarf-dwarf interactions or mergers. These\nresults suggest that the low oxygen abundances may be associated with an\nexternal event causing the accretion of metal-poor gas, which dilutes the\noxygen abundance in these systems.",
        "Extreme heat events exacerbated by climate change pose significant challenges\nto urban resilience and planning. This study introduces a climate-responsive\ndigital twin framework integrating the Spatiotemporal Vision Transformer\n(ST-ViT) model to enhance heat stress forecasting and decision-making. Using a\nTexas campus as a testbed, we synthesized high-resolution physical model\nsimulations with spatial and meteorological data to develop fine-scale human\nthermal predictions. The ST-ViT-powered digital twin enables efficient,\ndata-driven insights for planners, policymakers, and campus stakeholders,\nsupporting targeted heat mitigation strategies and advancing climate-adaptive\nurban design.",
        "We investigate the modified gravity in which the Lagrangian of gravity is a\nfunction of the trace of the n-th matrix power of Ricci tensor in a\nFriedmann-Lemaitre-Robertson-Walker(FLRW) spacetime. When n is negative, the\ninverse of Ricci tensor, also called the anticurvature tensor, will be\nintroduced. We design a new class of Ricci-inverse theory containing two\nanticurvature scalars and resulting to be free from the singularity problem.",
        "Physical human-robot interaction (pHRI) is widely needed in many fields, such\nas industrial manipulation, home services, and medical rehabilitation, and puts\nhigher demands on the safety of robots. Due to the uncertainty of the working\nenvironment, the pHRI may receive unexpected impact interference, which affects\nthe safety and smoothness of the task execution. The commonly used linear\nadmittance control (L-AC) can cope well with high-frequency small-amplitude\nnoise, but for medium-frequency high-intensity impact, the effect is not as\ngood. Inspired by the solid-liquid phase change nature of shear-thickening\nfluid, we propose a Shear-thickening Fluid Control (SFC) that can achieve both\nan easy human-robot collaboration and resistance to impact interference. The\nSFC's stability, passivity, and phase trajectory are analyzed in detail, the\nfrequency and time domain properties are quantified, and parameter constraints\nin discrete control and coupled stability conditions are provided. We conducted\nsimulations to compare the frequency and time domain characteristics of L-AC,\nnonlinear admittance controller (N-AC), and SFC, and validated their dynamic\nproperties. In real-world experiments, we compared the performance of L-AC,\nN-AC, and SFC in both fixed and mobile manipulators. L-AC exhibits weak\nresistance to impact. N-AC can resist moderate impacts but not high-intensity\nones, and may exhibit self-excited oscillations. In contrast, SFC demonstrated\nsuperior impact resistance and maintained stable collaboration, enhancing\ncomfort in cooperative water delivery tasks. Additionally, a case study was\nconducted in a factory setting, further affirming the SFC's capability in\nfacilitating human-robot collaborative manipulation and underscoring its\npotential in industrial applications.",
        "This paper introduces an open-source benchmark for evaluating Vision-Language\nModels (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video\nenvironments. We present a curated dataset containing 1,477 manually annotated\nframes spanning diverse domains, including code editors, news broadcasts,\nYouTube videos, and advertisements. Three state of the art VLMs - Claude-3,\nGemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as\nEasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),\nCharacter Error Rate (CER), and Accuracy. Our results highlight the strengths\nand limitations of VLMs in video-based OCR tasks, demonstrating their potential\nto outperform conventional OCR models in many scenarios. However, challenges\nsuch as hallucinations, content security policies, and sensitivity to occluded\nor stylized text remain. The dataset and benchmarking framework are publicly\navailable to foster further research.",
        "Hip exoskeletons are known for their versatility in assisting users across\nvaried scenarios. However, current assistive strategies often lack the\nflexibility to accommodate for individual walking patterns and adapt to diverse\nlocomotion environments. In this work, we present a novel control strategy that\nadapts the mechanical impedance of the human-exoskeleton system. We design the\nhip assistive torques as an adaptive virtual negative damping, which is able to\ninject energy into the system while allowing the users to remain in control and\ncontribute voluntarily to the movements. Experiments with five healthy subjects\ndemonstrate that our controller reduces the metabolic cost of walking compared\nto free walking (average reduction of 7.2%), and it preserves the lower-limbs\nkinematics. Additionally, our method achieves minimal power losses from the\nexoskeleton across the entire gait cycle (less than 2% negative mechanical\npower out of the total power), ensuring synchronized action with the users'\nmovements. Moreover, we use Bayesian Optimization to adapt the assistance\nstrength and allow for seamless adaptation and transitions across multi-terrain\nenvironments. Our strategy achieves efficient power transmission under all\nconditions. Our approach demonstrates an individualized, adaptable, and\nstraightforward controller for hip exoskeletons, advancing the development of\nviable, adaptive, and user-dependent control laws.",
        "As artificial intelligence (AI) systems become increasingly integrated into\ncritical domains, ensuring their responsible design and continuous development\nis imperative. Effective AI quality management (QM) requires tools and\nmethodologies that address the complexities of the AI lifecycle. In this paper,\nwe propose an approach for AI lifecycle planning that bridges the gap between\ngeneric guidelines and use case-specific requirements (MQG4AI). Our work aims\nto contribute to the development of practical tools for implementing\nResponsible AI (RAI) by aligning lifecycle planning with technical, ethical and\nregulatory demands. Central to our approach is the introduction of a flexible\nand customizable Methodology based on Quality Gates, whose building blocks\nincorporate RAI knowledge through information linking along the AI lifecycle in\na continuous manner, addressing AIs evolutionary character. For our present\ncontribution, we put a particular emphasis on the Explanation stage during\nmodel development, and illustrate how to align a guideline to evaluate the\nquality of explanations with MQG4AI, contributing to overall Transparency.",
        "Large Vision-Language Models (LVLMs) are susceptible to typographic attacks,\nwhich are misclassifications caused by an attack text that is added to an\nimage. In this paper, we introduce a multi-image setting for studying\ntypographic attacks, broadening the current emphasis of the literature on\nattacking individual images. Specifically, our focus is on attacking image sets\nwithout repeating the attack query. Such non-repeating attacks are stealthier,\nas they are more likely to evade a gatekeeper than attacks that repeat the same\nattack text. We introduce two attack strategies for the multi-image setting,\nleveraging the difficulty of the target image, the strength of the attack text,\nand text-image similarity. Our text-image similarity approach improves attack\nsuccess rates by 21% over random, non-specific methods on the CLIP model using\nImageNet while maintaining stealth in a multi-image scenario. An additional\nexperiment demonstrates transferability, i.e., text-image similarity calculated\nusing CLIP transfers when attacking InstructBLIP.",
        "Diffusion models have demonstrated their utility as learned priors for\nsolving various inverse problems. However, most existing approaches are limited\nto linear inverse problems. This paper exploits the efficient and unsupervised\nposterior sampling framework of Denoising Diffusion Restoration Models (DDRM)\nfor the solution of nonlinear phase retrieval problem, which requires\nreconstructing an image from its noisy intensity-only measurements such as\nFourier intensity. The approach combines the model-based alternating-projection\nmethods with the DDRM to utilize pretrained unconditional diffusion priors for\nphase retrieval. The performance is demonstrated through both simulations and\nexperimental data. Results demonstrate the potential of this approach for\nimproving the alternating-projection methods as well as its limitations.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "The quantum internet holds transformative potential for global communication\nby harnessing the principles of quantum information processing. Despite\nsignificant advancements in quantum communication technologies, the efficient\ndistribution of critical resources, such as qubits, remains a persistent and\nunresolved challenge. Conventional approaches often fall short of achieving\noptimal resource allocation, underscoring the necessity for more effective\nsolutions. This study proposes a novel reinforcement learning-based adaptive\nentanglement routing framework designed to enable resource allocation tailored\nto the specific demands of quantum applications. The introduced QuDQN model\nutilizes reinforcement learning to optimize the management of quantum networks,\nallocate resources efficiently, and enhance entanglement routing. The model\nintegrates key considerations, including fidelity requirements, network\ntopology, qubit capacity, and request demands.",
        "We present 12 observations of the accreting millisecond X-ray pulsar Aql X-1,\ntaken from August 2022 to October 2023 using the Five-hundred-meter Aperture\nSpherical Radio Telescope at 1250 MHz. These observations covered both the\nquiescence and X-ray outburst states, as determined by analyzing the X-ray data\nfrom the Neutron Star Interior Composition Explorer and the Monitor of All-sky\nX-ray Image. Periodicity and single-pulse searches were conducted for each\nobservation, but no pulsed signals were detected. The obtained upper limit flux\ndensities are in the range of 2.86-5.73 uJy, which provide the lowest limits to\ndate. We discuss several mechanisms that may prevent detection, suggesting that\nAql X-1 may be in the radio-ejection state during quiescence, where the radio\npulsed emissions are absorbed by the matter surrounding the system."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"ChatGPT Hallucinates Non-existent Citations: Evidence from Economics",
    "start_abstract":"In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2",
    "start_categories":[
      "q-fin.ST"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Accuracy of Chatbots in Citing Journal Articles"
      ],
      "abstract":[
        "This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Comprehensive Metapath-based Heterogeneous Graph Transformer for\n  Gene-Disease Association Prediction",
        "FAIR: Facilitating Artificial Intelligence Resilience in Manufacturing\n  Industrial Internet",
        "Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education",
        "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning",
        "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue",
        "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
        "DeclareAligner: A Leap Towards Efficient Optimal Alignments for\n  Declarative Process Model Conformance Checking",
        "Kriging and Gaussian Process Interpolation for Georeferenced Data\n  Augmentation",
        "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM",
        "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
        "AI Generations: From AI 1.0 to AI 4.0",
        "Towards AI-assisted Academic Writing",
        "Second bounded cohomology of knot quandles",
        "PatentLMM: Large Multimodal Model for Generating Descriptions for Patent\n  Figures",
        "Conditioning on Local Statistics for Scalable Heterogeneous Federated\n  Learning",
        "Vertex-Minimal Triangulation of Complexes with Homology",
        "Epistemic Logic Programs: Non-Ground and Counting Complexity",
        "AudioSpa: Spatializing Sound Events with Text",
        "An upper bound on the size of a code with $s$ distances",
        "MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome\n  Prediction",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "QORT-Former: Query-optimized Real-time Transformer for Understanding Two\n  Hands Manipulating Objects",
        "DGNN: A Neural PDE Solver Induced by Discontinuous Galerkin Methods",
        "Blockchain with proof of quantum work",
        "Modularity of preferential attachment graphs",
        "Harmonic Structure of the Brunel spectra",
        "A maximum concurrence criterion to investigate absolutely maximally\n  entangled states"
      ],
      "abstract":[
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "Discovering gene-disease associations is crucial for understanding disease\nmechanisms, yet identifying these associations remains challenging due to the\ntime and cost of biological experiments. Computational methods are increasingly\nvital for efficient and scalable gene-disease association prediction.\nGraph-based learning models, which leverage node features and network\nrelationships, are commonly employed for biomolecular predictions. However,\nexisting methods often struggle to effectively integrate node features,\nheterogeneous structures, and semantic information. To address these\nchallenges, we propose COmprehensive MEtapath-based heterogeneous graph\nTransformer(COMET) for predicting gene-disease associations. COMET integrates\ndiverse datasets to construct comprehensive heterogeneous networks,\ninitializing node features with BioGPT. We define seven Metapaths and utilize a\ntransformer framework to aggregate Metapath instances, capturing global\ncontexts and long-distance dependencies. Through intra- and inter-metapath\naggregation using attention mechanisms, COMET fuses latent vectors from\nmultiple Metapaths to enhance GDA prediction accuracy. Our method demonstrates\nsuperior robustness compared to state-of-the-art approaches. Ablation studies\nand visualizations validate COMET's effectiveness, providing valuable insights\nfor advancing human health research.",
        "Artificial intelligence (AI) systems have been increasingly adopted in the\nManufacturing Industrial Internet (MII). Investigating and enabling the AI\nresilience is very important to alleviate profound impact of AI system failures\nin manufacturing and Industrial Internet of Things (IIoT) operations, leading\nto critical decision making. However, there is a wide knowledge gap in defining\nthe resilience of AI systems and analyzing potential root causes and\ncorresponding mitigation strategies. In this work, we propose a novel framework\nfor investigating the resilience of AI performance over time under hazard\nfactors in data quality, AI pipelines, and the cyber-physical layer. The\nproposed method can facilitate effective diagnosis and mitigation strategies to\nrecover AI performance based on a multimodal multi-head self latent attention\nmodel. The merits of the proposed method are elaborated using an MII testbed of\nconnected Aerosol Jet Printing (AJP) machines, fog nodes, and Cloud with\ninference tasks via AI pipelines.",
        "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
        "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications.",
        "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics.",
        "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps:\/\/spinbench.github.io\/",
        "In many engineering applications, processes must be followed precisely,\nmaking conformance checking between event logs and declarative process models\ncrucial for ensuring adherence to desired behaviors. This is a critical area\nwhere Artificial Intelligence (AI) plays a pivotal role in driving effective\nprocess improvement. However, computing optimal alignments poses significant\ncomputational challenges due to the vast search space inherent in these models.\nConsequently, existing approaches often struggle with scalability and\nefficiency, limiting their applicability in real-world settings. This paper\nintroduces DeclareAligner, a novel algorithm that uses the A* search algorithm,\nan established AI pathfinding technique, to tackle the problem from a fresh\nperspective leveraging the flexibility of declarative models. Key features of\nDeclareAligner include only performing actions that actively contribute to\nfixing constraint violations, utilizing a tailored heuristic to navigate\ntowards optimal solutions, and employing early pruning to eliminate\nunproductive branches, while also streamlining the process through\npreprocessing and consolidating multiple fixes into unified actions. The\nproposed method is evaluated using 8,054 synthetic and real-life alignment\nproblems, demonstrating its ability to efficiently compute optimal alignments\nby significantly outperforming the current state of the art. By enabling\nprocess analysts to more effectively identify and understand conformance\nissues, DeclareAligner has the potential to drive meaningful process\nimprovement and management.",
        "Data augmentation is a crucial step in the development of robust supervised\nlearning models, especially when dealing with limited datasets. This study\nexplores interpolation techniques for the augmentation of geo-referenced data,\nwith the aim of predicting the presence of Commelina benghalensis L. in\nsugarcane plots in La R{\\'e}union. Given the spatial nature of the data and the\nhigh cost of data collection, we evaluated two interpolation approaches:\nGaussian processes (GPs) with different kernels and kriging with various\nvariograms. The objectives of this work are threefold: (i) to identify which\ninterpolation methods offer the best predictive performance for various\nregression algorithms, (ii) to analyze the evolution of performance as a\nfunction of the number of observations added, and (iii) to assess the spatial\nconsistency of augmented datasets. The results show that GP-based methods, in\nparticular with combined kernels (GP-COMB), significantly improve the\nperformance of regression algorithms while requiring less additional data.\nAlthough kriging shows slightly lower performance, it is distinguished by a\nmore homogeneous spatial coverage, a potential advantage in certain contexts.",
        "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval\/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.",
        "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:\/dxzxy12138.github.io\/PhysReason.",
        "This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
        "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
        "In this paper, we explore the bounded cohomology of quandles and its\napplications to knot theory. We establish two key results that provide\nsufficient conditions for the infinite dimensionality of the second bounded\ncohomology of quandles. The first condition involves a subspace of homogeneous\ngroup quasimorphisms on the inner automorphism group of the quandle, whereas\nthe second condition concerns the vanishing of the stable commutator length on\na subgroup of this inner automorphism group. As topological applications, we\nshow that the second bounded cohomology of the quandle of any non-split link\nwhose link group is non-solvable as well as the quandle of any split link, is\ninfinite dimensional. From these results, we conclude that the second bounded\ncohomology of the knot quandle detects the unknot. On the algebraic side, we\nprove that the second bounded cohomology of a free product of quandles is\ninfinite dimensional if the inner automorphism group of at least one of the\nfree factors is amenable. This leads to the result that the second bounded\ncohomology of free quandles of rank greater than one, as well as their\ncanonical quotients, is infinite dimensional.",
        "Writing comprehensive and accurate descriptions of technical drawings in\npatent documents is crucial to effective knowledge sharing and enabling the\nreplication and protection of intellectual property. However, automation of\nthis task has been largely overlooked by the research community. To this end,\nwe introduce PatentDesc-355K, a novel large-scale dataset containing ~355K\npatent figures along with their brief and detailed textual descriptions\nextracted from more than 60K US patent documents. In addition, we propose\nPatentLMM - a novel multimodal large language model specifically tailored to\ngenerate high-quality descriptions of patent figures. Our proposed PatentLMM\ncomprises two key components: (i) PatentMME, a specialized multimodal vision\nencoder that captures the unique structural elements of patent figures, and\n(ii) PatentLLaMA, a domain-adapted version of LLaMA fine-tuned on a large\ncollection of patents. Extensive experiments demonstrate that training a vision\nencoder specifically designed for patent figures significantly boosts the\nperformance, generating coherent descriptions compared to fine-tuning\nsimilar-sized off-the-shelf multimodal models. PatentDesc-355K and PatentLMM\npave the way for automating the understanding of patent figures, enabling\nefficient knowledge sharing and faster drafting of patent documents. We make\nthe code and data publicly available.",
        "Federated learning is a distributed machine learning approach where multiple\nclients collaboratively train a model without sharing their local data, which\ncontributes to preserving privacy. A challenge in federated learning is\nmanaging heterogeneous data distributions across clients, which can hinder\nmodel convergence and performance due to the need for the global model to\ngeneralize well across diverse local datasets. We propose to use local\ncharacteristic statistics, by which we mean some statistical properties\ncalculated independently by each client using only their local training\ndataset. These statistics, such as means, covariances, and higher moments, are\nused to capture the characteristics of the local data distribution. They are\nnot shared with other clients or a central node. During training, these local\nstatistics help the model learn how to condition on the local data\ndistribution, and during inference, they guide the client's predictions. Our\nexperiments show that this approach allows for efficient handling of\nheterogeneous data across the federation, has favorable scaling compared to\napproaches that directly try to identify peer nodes that share distribution\ncharacteristics, and maintains privacy as no additional information needs to be\ncommunicated.",
        "For a given pair of numbers $(d,k)$, we establish a lower bound on the number\nof vertices in pure $d$-dimensional simplicial complexes with non-trivial\nhomology in dimension $k$, and prove that this bound is tight. Furthermore, we\nsolve the problem under the additional constraint of strong connectivity with\nrespect to any intermediate dimension.",
        "Answer Set Programming (ASP) is a prominent problem-modeling and solving\nframework, whose solutions are called answer sets. Epistemic logic programs\n(ELP) extend ASP to reason about all or some answer sets. Solutions to an ELP\ncan be seen as consequences over multiple collections of answer sets, known as\nworld views. While the complexity of propositional programs is well studied,\nthe non-ground case remains open. This paper establishes the complexity of\nnon-ground ELPs. We provide a comprehensive picture for well-known program\nfragments, which turns out to be complete for the class NEXPTIME with access to\noracles up to \\Sigma^P_2. In the quantitative setting, we establish complexity\nresults for counting complexity beyond #EXP. To mitigate high complexity, we\nestablish results in case of bounded predicate arity, reaching up to the fourth\nlevel of the polynomial hierarchy. Finally, we provide ETH-tight runtime\nresults for the parameter treewidth, which has applications in quantitative\nreasoning, where we reason on (marginal) probabilities of epistemic literals.",
        "Text-to-audio (TTA) systems have recently demonstrated strong performance in\nsynthesizing monaural audio from text. However, the task of generating binaural\nspatial audio from text, which provides a more immersive auditory experience by\nincorporating the sense of spatiality, have not been explored yet. In this\nwork, we introduce text-guided binaural audio generation. As an early effort,\nwe focus on the scenario where a monaural reference audio is given\nadditionally. The core problem is to associate specific sound events with their\ndirections, thereby creating binaural spatial audio. The challenge lies in the\ncomplexity of textual descriptions and the limited availability of\nsingle-source sound event datasets. To address this, we propose AudioSpa, an\nend-to-end model that applies large language models to process both acoustic\nand textual information. We employ fusion multi-head attention (FMHA) to\nintegrate text tokens, which enhances the generation capability of the\nmultimodal learning. Additionally, we propose a binaural source localization\nmodel to assess the quality of the generated audio. Finally, we design a data\naugmentation strategy to generate diverse datasets, which enables the model to\nspatialize sound events across various spatial positions. Experimental results\ndemonstrate that our model is able to put sounds at the specified locations\naccurately. It achieves competitive performance in both localization accuracy\nand signal distortion. Our demonstrations are available at\nhttps:\/\/linfeng-feng.github.io\/AudioSpa-demo.",
        "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.",
        "Clinical trials are the gold standard for assessing the effectiveness and\nsafety of drugs for treating diseases. Given the vast design space of drug\nmolecules, elevated financial cost, and multi-year timeline of these trials,\nresearch on clinical trial outcome prediction has gained immense traction.\nAccurate predictions must leverage data of diverse modes such as drug\nmolecules, target diseases, and eligibility criteria to infer successes and\nfailures. Previous Deep Learning approaches for this task, such as HINT, often\nrequire wet lab data from synthesized molecules and\/or rely on prior knowledge\nto encode interactions as part of the model architecture. To address these\nlimitations, we propose a light-weight attention-based model, MEXA-CTP, to\nintegrate readily-available multi-modal data and generate effective\nrepresentations via specialized modules dubbed \"mode experts\", while avoiding\nhuman biases in model design. We optimize MEXA-CTP with the Cauchy loss to\ncapture relevant interactions across modes. Our experiments on the Trial\nOutcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon\nexisting approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,\nand 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to\nquantify the effectiveness of each component in our proposed method.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "Significant advancements have been achieved in the realm of understanding\nposes and interactions of two hands manipulating an object. The emergence of\naugmented reality (AR) and virtual reality (VR) technologies has heightened the\ndemand for real-time performance in these applications. However, current\nstate-of-the-art models often exhibit promising results at the expense of\nsubstantial computational overhead. In this paper, we present a query-optimized\nreal-time Transformer (QORT-Former), the first Transformer-based real-time\nframework for 3D pose estimation of two hands and an object. We first limit the\nnumber of queries and decoders to meet the efficiency requirement. Given\nlimited number of queries and decoders, we propose to optimize queries which\nare taken as input to the Transformer decoder, to secure better accuracy: (1)\nwe propose to divide queries into three types (a left hand query, a right hand\nquery and an object query) and enhance query features (2) by using the contact\ninformation between hands and an object and (3) by using three-step update of\nenhanced image and query features with respect to one another. With proposed\nmethods, we achieved real-time pose estimation performance using just 108\nqueries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing\nstate-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right\nhand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)\nand 10.4% (object), our method excels in accuracy. Additionally, it sets the\nstate-of-the-art in interaction recognition, maintaining real-time efficiency\nwith an off-the-shelf action recognition module.",
        "We propose a general framework for the Discontinuous Galerkin-induced Neural\nNetwork (DGNN), inspired by the Interior Penalty Discontinuous Galerkin Method\n(IPDGM). In this approach, the trial space consists of piecewise neural network\nspace defined over the computational domain, while the test function space is\ncomposed of piecewise polynomials. We demonstrate the advantages of DGNN in\nterms of accuracy and training efficiency across several numerical examples,\nincluding stationary and time-dependent problems. Specifically, DGNN easily\nhandles high perturbations, discontinuous solutions, and complex geometric\ndomains.",
        "We propose a blockchain architecture in which mining requires a quantum\ncomputer. The consensus mechanism is based on proof of quantum work, a\nquantum-enhanced alternative to traditional proof of work that leverages\nquantum supremacy to make mining intractable for classical computers. We have\nrefined the blockchain framework to incorporate the probabilistic nature of\nquantum mechanics, ensuring stability against sampling errors and hardware\ninaccuracies. To validate our approach, we implemented a prototype blockchain\non four D-Wave$^{\\rm TM}$ quantum annealing processors geographically\ndistributed within North America, demonstrating stable operation across\nhundreds of thousands of quantum hashing operations. Our experimental protocol\nfollows the same approach used in the recent demonstration of quantum supremacy\n[1], ensuring that classical computers cannot efficiently perform the same\ncomputation task. By replacing classical machines with quantum systems for\nmining, it is possible to significantly reduce the energy consumption and\nenvironmental impact traditionally associated with blockchain mining. Beyond\nserving as a proof of concept for a meaningful application of quantum\ncomputing, this work highlights the potential for other near-term quantum\ncomputing applications using existing technology.",
        "Modularity is a graph parameter measuring how clearly the set of graph\nvertices may be partitioned into subsets of high edge density. It indicates the\npresence of community structure in the graph. We study its value for a random\npreferential attachment model $G_n^h$ introduced by Barab\\'asi and Albert in\n1999. A graph $G_n^h$ is created from some finite starting graph by adding new\nvertices one by one. A new vertex always connects to $h\\geq1$ already existing\nvertices and those are chosen with probability proportional to their current\ndegrees. We prove that modularity of $G_n^h$ is with high probability upper\nbounded by a function tending to $0$ with $h$ tending to infinity. This\nresolves the conjecture of Prokhorenkova, Pralat and Raigorodskii from 2016. As\na byproduct we obtain novel concentration results for the volume and the edge\ndensity parameters of subsets of $G_n^h$.",
        "Electromagnetic emissions, known as Brunel radiations, are produced in\nplasmas through the coupling between the free electron density and ultrafast\nionizing laser pulses. The radiation spectrum generated in laser-gas\ninteractions is here investigated from a local current model for laser drivers\nwith two frequency components - or \"colors\" - being not necessarily integers of\none another. We provide a general description of this spectrum by deriving\nanalytically the convolution product of the Fourier transforms of the electron\ndensity and of the laser electric field. Our analysis reveals that the only\nknowledge of the optical field extrema in time domain is sufficient to\nreproduce faithfully the numerically-computed Brunel spectrum and justify the\nemergence of various resonance frequencies. The classical combination of two\nlaser harmonics, i.e., a fundamental and its second harmonic, is also\naddressed.",
        "We propose a straightforward method to determine the maximal entanglement of\npure states using the criterion of maximal I-concurrence, a measure of\nentanglement. The square of concurrence for a bipartition $X|X^\\prime$ of a\npure state is defined as $E^2_{X| X ^\\prime}=2[1-tr({\\rho_X}^2)]$. From this,\nwe can infer that the concurrence $E_{X| X ^\\prime}$ reaches its maximum when\n$tr({\\rho_X}^2)$ is minimized. Using this approach, we identify numerous\nAbsolutely Maximally Entangled (AME) pure states that exhibit maximal\nentanglement across all possible bipartitions. Conditions are derived for pure\nstates to achieve maximal mixedness in all bipartitions, revealing that any\npure state with an odd number of subsystem coefficients does not meet the AME\ncriterion. Furthermore, we obtain equal maximal multipartite entangled pure\nstates across all bipartitions using our maximal concurrence criterion."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Accuracy of Chatbots in Citing Journal Articles",
    "start_abstract":"This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "ChatGPT Hallucinates Non-existent Citations: Evidence from Economics"
      ],
      "abstract":[
        "In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2"
      ],
      "categories":[
        "q-fin.ST"
      ]
    },
    "list":{
      "title":[
        "A mixture transition distribution approach to portfolio optimization",
        "Impermanent loss and Loss-vs-Rebalancing II",
        "Evaluating the resilience of ESG investments in European Markets during\n  turmoil periods",
        "Considerations on the use of financial ratios in the study of family\n  businesses",
        "Shapley-Scarf Markets with Objective Indifferences",
        "A multi-factor model for improved commodity pricing: Calibration and an\n  application to the oil market",
        "Analyzing Communicability and Connectivity in the Indian Stock Market\n  During Crises",
        "Pricing American options under rough volatility using deep-signatures\n  and signature-kernels",
        "Dynamic Factor Correlation Model",
        "Matrix H-theory approach to stock market fluctuations",
        "Will artificial intelligence accelerate or delay the race between\n  nuclear energy technology budgeting and net-zero emissions?",
        "Stochastic Optimal Control of Iron Condor Portfolios for Profitability\n  and Risk Management",
        "Analysis of the Impact of the Union Budget Announcements on the Indian\n  Stock Market: A Fractal Perspective",
        "Existence of optimal controls for stochastic partial differential\n  equations with fully local monotone coefficients",
        "Spin-polarized STM measurement scheme for quantum geometric tensor",
        "Mechanism of tulip flame formation in highly reactive and low reactive\n  gas mixtures",
        "Poincar\\'{e} sphere engineering of dynamical ferroelectric topological\n  solitons",
        "An Optimal Transport approach to arbitrage correction: Application to\n  volatility Stress-Tests",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "Balancing Flexibility and Interpretability: A Conditional Linear Model\n  Estimation via Random Forest",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Bring the noise: exact inference from noisy simulations in collider\n  physics",
        "Quantum Hamiltonian Descent for Non-smooth Optimization",
        "On the spatial distribution of luminous blue variables in the M33 galaxy",
        "The existence of pyramidal Steiner triple systems over abelian groups",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Rotating and non-linear magnetic-charged black hole with an anisotropic\n  matter field",
        "SUSY transformation as the coupler of non-interacting systems"
      ],
      "abstract":[
        "Understanding the dependencies among financial assets is critical for\nportfolio optimization. Traditional approaches based on correlation networks\noften fail to capture the nonlinear and directional relationships that exist in\nfinancial markets. In this study, we construct directed and weighted financial\nnetworks using the Mixture Transition Distribution (MTD) model, offering a\nricher representation of asset interdependencies. We apply local assortativity\nmeasures--metrics that evaluate how assets connect based on similarities or\ndifferences--to guide portfolio selection and allocation. Using data from the\nDow Jones 30, Euro Stoxx 50, and FTSE 100 indices constituents, we show that\nportfolios optimized with network-based assortativity measures consistently\noutperform the classical mean-variance framework. Notably, modalities in which\nassets with differing characteristics connect enhance diversification and\nimprove Sharpe ratios. The directed nature of MTD-based networks effectively\ncaptures complex relationships, yielding portfolios with superior risk-adjusted\nreturns. Our findings highlight the utility of network-based methodologies in\nfinancial decision-making, demonstrating their ability to refine portfolio\noptimization strategies. This work thus underscores the potential of leveraging\nadvanced financial networks to achieve enhanced performance, offering valuable\ninsights for practitioners and setting a foundation for future research.",
        "This paper examines the relationship between impermanent loss (IL) and\nloss-versus-rebalancing (LVR) in automated market makers (AMMs). Our main focus\nis on statistical properties, the impact of fees, the role of block times, and,\nrelated to the latter, the continuous time limit. We find there are three\nrelevant regimes: (i) very short times where LVR and IL are identical; (ii)\nintermediate time where LVR and IL show distinct distribution functions but are\nconnected via the central limit theorem exhibiting the same expectation value;\n(iii) long time behavior where both the distribution functions and averages are\ndistinct. Subsequently, we study how fees change this dynamics with a special\nfocus on competing time scales like block times and 'arbitrage times'.",
        "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
        "Most empirical works that study the financing decisions of family businesses\nuse financial ratios. These data present asymmetry, non-normality,\nnon-linearity and even dependence on the results of the choice of which\naccounting figure goes to the numerator and denominator of the ratio. This\narticle uses compositional data analysis (CoDa) as well as classical analysis\nstrategies to compare the structure of balance sheet liabilities between family\nand non-family businesses, showing the sensitivity of the results to the\nmethodology used. The results prove the need to use appropriate methodologies\nto advance the academic discipline.",
        "In many object allocation problems, some of the objects may effectively be\nindistinguishable from each other, such as with dorm rooms or school seats. In\nsuch cases, it is reasonable to assume that agents are indifferent between\nidentical copies of the same object. We call this setting ``objective\nindifferences.'' Top trading cycles (TTC) with fixed tie-breaking has been\nsuggested and used in practice to deal with indifferences in object allocation\nproblems. Under general indifferences, TTC with fixed tie-breaking is not\nPareto efficient nor group strategy-proof. Furthermore, it may not select the\ncore, even when it exists. Under objective indifferences, agents are always and\nonly indifferent between copies of the same object. In this setting, TTC with\nfixed tie-breaking maintains Pareto efficiency, group strategy-proofness, and\ncore selection. In fact, we present domain characterization results which\ntogether show that objective indifferences is the most general setting where\nTTC with fixed tie-breaking maintains these important properties.",
        "We present a new model for commodity pricing that enhances accuracy by\nintegrating four distinct risk factors: spot price, stochastic volatility,\nconvenience yield, and stochastic interest rates. While the influence of these\nfour variables on commodity futures prices is well recognized, their combined\neffect has not been addressed in the existing literature. We fill this gap by\nproposing a model that effectively captures key stylized facts including a\ndynamic correlation structure and time-varying risk premiums. Using a Kalman\nfilter-based framework, we achieve simultaneous estimation of parameters while\nfiltering state variables through the joint term structure of futures prices\nand bond yields. We perform an empirical analysis focusing on crude oil\nfutures, where we benchmark our model against established approaches. The\nresults demonstrate that the proposed four-factor model effectively captures\nthe complexities of futures term structures and outperforms existing models.",
        "In financial networks, information does not always follow the shortest path\nbetween two nodes but may also take alternate routes. Communicability, a\nnetwork measure, resolves this complexity and, in diffusion-like processes,\nprovides a reliable measure of the ease with which information flows between\nnodes. As a result, communicability appears to be an important measure for\ndetecting disturbances in connectivity within financial systems, similar to\ninstability caused by periods of high volatility. This study investigates the\nevolution of communicability measures in the stock networks during periods of\ncrises, showing how systemic shocks strengthen the pairwise interdependence\nbetween stocks in the financial market. In this study, the permutation test\nreveals that approximately 83.5 per cent of stock pairs were found to be\nstatistically significant at the significance level of 0.001 and have an\nincrease in the shortest communicability path length during the crisis than the\nnormal days, indicating enhanced interdependence and heightened information\nflow in the market. Furthermore, we show that when employed as features in the\nclassification model, the network shortest path-based measures, along with\ncommunicability measures, are able to accurately classify between the times\nperiods of market stability and volatility. Additionally, our results show that\nthe geometric measures perform better in terms of classification accuracy than\ntopological measures. These findings provide important insights into market\nbehaviour during times of increased volatility and advance our understanding of\nthe financial market crisis.",
        "We extend the signature-based primal and dual solutions to the optimal\nstopping problem recently introduced in [Bayer et al.: Primal and dual optimal\nstopping with signatures, to appear in Finance & Stochastics 2025], by\nintegrating deep-signature and signature-kernel learning methodologies. These\napproaches are designed for non-Markovian frameworks, in particular enabling\nthe pricing of American options under rough volatility. We demonstrate and\ncompare the performance within the popular rough Heston and rough Bergomi\nmodels.",
        "We introduce a new dynamic factor correlation model with a novel\nvariation-free parametrization of factor loadings. The model is applicable to\nhigh dimensions and can accommodate time-varying correlations, heterogeneous\nheavy-tailed distributions, and dependent idiosyncratic shocks, such as those\nobserved in returns on stocks in the same subindustry. We apply the model to a\n\"small universe\" with 12 asset returns and to a \"large universe\" with 323 asset\nreturns. The former facilitates a comprehensive empirical analysis and\ncomparisons and the latter demonstrates the flexibility and scalability of the\nmodel.",
        "We introduce matrix H theory, a framework for analyzing collective behavior\narising from multivariate stochastic processes with hierarchical structure. The\ntheory models the joint distribution of the multiple variables (the measured\nsignal) as a compound of a large-scale multivariate distribution with the\ndistribution of a slowly fluctuating background. The background is\ncharacterized by a hierarchical stochastic evolution of internal degrees of\nfreedom, representing the correlations between stocks at different time scales.\nAs in its univariate version, the matrix H-theory formalism also has two\nuniversality classes: Wishart and inverse Wishart, enabling a concise\ndescription of both the background and the signal probability distributions in\nterms of Meijer G-functions with matrix argument. Empirical analysis of daily\nreturns of stocks within the S&P500 demonstrates the effectiveness of matrix H\ntheory in describing fluctuations in stock markets. These findings contribute\nto a deeper understanding of multivariate hierarchical processes and offer\npotential for developing more informed portfolio strategies in financial\nmarkets.",
        "This study explores the impact of nuclear energy technology budgeting and\nartificial intelligence on carbon dioxide (CO2) emissions in 20 OECD economies.\nUnlike previous research that relied on conventional panel techniques, we\nutilize the Method of Moment Quantile Regression panel data estimation\ntechniques. This approach provides quantile-specific insights while addressing\nissues of endogeneity and heteroscedasticity, resulting in a more nuanced and\nrobust understanding of complex relationships. A novel aspect of this research\nwork is introducing the moderating effect of artificial intelligence on the\nrelationship between nuclear energy and CO2 emissions. The results found that\nthe direct impact of artificial intelligence on CO2 emissions is significant,\nwhile the effect of nuclear energy technology budgeting is not. Additionally,\nartificial intelligence moderates the relationship between nuclear energy\ntechnology budgeting and CO2 emissions, aiding nuclear energy in reducing\ncarbon emissions across OECD countries. Our findings indicate that\ntransitioning to a low-carbon future is achievable by replacing fossil fuel\nenergy sources with increased integration of artificial intelligence to promote\nnuclear energy technologies. This study demonstrates that energy innovations\ncan serve as effective climate-resilience strategies to mitigate the impacts of\nclimate change.",
        "Previous research on option strategies has primarily focused on their\nbehavior near expiration, with limited attention to the transient value process\nof the portfolio. In this paper, we formulate Iron Condor portfolio\noptimization as a stochastic optimal control problem, examining the impact of\nthe control process \\( u(k_i, \\tau) \\) on the portfolio's potential\nprofitability and risk. By assuming the underlying price process as a bounded\nmartingale within $[K_1, K_2]$, we prove that the portfolio with a strike\nstructure of $k_1 < k_2 = K_2 < S_t < k_3 = K_3 < k_4$ has a submartingale\nvalue process, which results in the optimal stopping time aligning with the\nexpiration date $\\tau = T$. Moreover, we construct a data generator based on\nthe Rough Heston model to investigate general scenarios through simulation. The\nresults show that asymmetric, left-biased Iron Condor portfolios with $\\tau =\nT$ are optimal in SPX markets, balancing profitability and risk management.\nDeep out-of-the-money strategies improve profitability and success rates at the\ncost of introducing extreme losses, which can be alleviated by using an optimal\nstopping strategy. Except for the left-biased portfolios $\\tau$ generally falls\nwithin the range of [50\\%,75\\%] of total duration. In addition, we validate\nthese findings through case studies on the actual SPX market, covering bullish,\nsideways, and bearish market conditions.",
        "The stock market closely monitors macroeconomic policy announcements, such as\nannual budget events, due to their substantial influence on various economic\nparticipants. These events tend to impact the stock markets initially before\naffecting the real sector. Our study aims to analyze the effects of the budget\non the Indian stock market, specifically focusing on the announcement for the\nyear 2024. We will compare this with the years 2023, 2022, and 2020, assessing\nits impact on the NIFTY50 index using average abnormal return (AAR) and\ncumulative average abnormal return (CAAR) over a period of -15 and +15 days,\nincluding the budget day. This study utilizes an innovative approach involving\nthe fractal interpolation function, paired with fractal dimensional analysis,\nto study the fluctuations arising from budget announcements. The fractal\nperspective on the data offers an effective framework for understanding complex\nvariations.",
        "This paper deals with a stochastic optimal feedback control problem for the\ncontrolled stochastic partial differential equations. More precisely, we\nestablish the existence of stochastic optimal feedback control for the\ncontrolled stochastic partial differential equations with fully monotone\ncoefficients by a minimizing sequence for the control problem. Using the\nFaedo-Galerkin approximations, the uniform estimates and the tightness in some\nappropriate space for the Faedo-Galerkin approximating solution can be obtain\nto prove the well-posedness of the controlled stochastic partial differential\nequations with fully monotone coefficients. The results obtained in the present\npaper may be applied to various types of controlled stochastic partial\ndifferential equations, such as the controlled stochastic convection diffusion\nequation.",
        "Quantum geometric tensor (QGT) reflects the geometry of the eigenstates of a\nsystem's Hamiltonian. The full characterization of QGT is essential for various\nquantum systems. However, it is challenging to characterize the QGT of the\nsolid-state systems. Here we present a scheme by using spin-polarized STM to\nmeasure QGT of two-dimensional solid-state systems, in which the spin texture\nis extracted from geometric amplitudes of Friedel oscillations induced by the\nintentionally introduced magnetic impurity and then the QGT is derived from the\nmomentum differential of spin texture. The surface states of topological\ninsulator (TISS), as a model spin system, is promising to demonstrate the\nscheme. In a TI slab, the gapped TISS host finite quantum metric and Berry\ncurvature as the symmetric real part and the antisymmetric imaginary part of\nQGT, respectively. Thus, a detailed calculations guide the use of the developed\nscheme to measure the QGT of gapped TISS with or without an external in-plane\nmagnetic field. This study provides a feasible scheme for measuring QGT of\ntwo-dimensional solid-state systems, and hints at the great potential of the\ninformation extraction from the geometric amplitudes of STM and other\nmeasurement.",
        "The early stages of flame dynamics and the development and evolution of tulip\nflames in closed tubes of various aspect ratios and in a semi-open tube are\nstudied by solving the fully compressible reactive Navier-Stokes equations\nusing a high-order numerical method coupled to detailed chemical models in a\nstoichiometric hydrogen\/air and methane\/air mixtures. The use of adaptive mesh\nrefinement provides adequate resolution of the flame reaction zone, pressure\nwaves, and flame-pressure wave interactions. The purpose of this study is to\ngain a deeper insight into the influence of chemical kinetics on the combustion\nregimes leading to the formation of a tulip flame and its subsequent evolution.\nThe simulations highlight the effect of flame thickness, flame velocity, and\nreaction order on the intensity of the rarefaction wave generated by the flame\nduring the deceleration phase, which is the principal physical mechanism of\ntulip flame formation. The obtained results explain most of the experimentally\nobserved features of tulip flame formation, e.g. faster tulip flame formation\nwith deeper tulip shape for faster flames compared to slower flames.",
        "Geometric representation lays the basis for understanding and flexible tuning\nof topological transitions in many physical systems. An example is given by the\nPoincar\\'{e} sphere (PS) that provides an intuitive and continuous\nparameterization of the spin or orbital angular momentum (OAM) light states.\nHere, we apply this geometric construction to understand and continuously\nencode dynamical topologies of ferroelectric solitons driven by OAM-tunable\nlight. We show that: (1) PS engineering enables controlled creation of dynamic\npolar antiskyrmions that are rarely found in ferroelectrics; (2) We link such\ntopological transition to the tuning of the light beam as a ``knob'' from OAM\n(PS pole) to non-OAM (PS equator) modes; (3) Intermediate OAM-state structured\nlight results in new ferroelectric topologies of temporally hybrid\nskyrmion-antiskyrmion states. Our study offers new approaches of robust control\nand flexible tuning of topologies of matter using structured light.",
        "We present a method based on optimal transport to remove arbitrage\nopportunities within a finite set of option prices. The method is notably\nintended for regulatory stress-tests, which impose to apply important local\ndistortions to implied volatility surfaces. The resulting stressed option\nprices are naturally associated to a family of signed marginal measures: we\nformulate the process of removing arbitrage as a projection onto the subset of\nmartingale measures with respect to a Wasserstein metric in the space of signed\nmeasures. We show how this projection problem can be recast as an optimal\ntransport problem; in view of the numerical solution, we apply an entropic\nregularization technique. For the regularized problem, we derive a strong\nduality formula, show convergence results as the regularization parameter\napproaches zero, and formulate a multi-constrained Sinkhorn algorithm, where\neach iteration involves, at worse, finding the root of an explicit scalar\nfunction. The convergence of this algorithm is also established. We compare our\nmethod with the existing approach by [Cohen, Reisinger and Wang, Appl.\\ Math.\\\nFin.\\ 2020] across various scenarios and test cases.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "Traditional parametric econometric models often rely on rigid functional\nforms, while nonparametric techniques, despite their flexibility, frequently\nlack interpretability. This paper proposes a parsimonious alternative by\nmodeling the outcome $Y$ as a linear function of a vector of variables of\ninterest $\\boldsymbol{X}$, conditional on additional covariates\n$\\boldsymbol{Z}$. Specifically, the conditional expectation is expressed as\n$\\mathbb{E}[Y|\\boldsymbol{X},\\boldsymbol{Z}]=\\boldsymbol{X}^{T}\\boldsymbol{\\beta}(\\boldsymbol{Z})$,\nwhere $\\boldsymbol{\\beta}(\\cdot)$ is an unknown Lipschitz-continuous function.\nWe introduce an adaptation of the Random Forest (RF) algorithm to estimate this\nmodel, balancing the flexibility of machine learning methods with the\ninterpretability of traditional linear models. This approach addresses a key\nchallenge in applied econometrics by accommodating heterogeneity in the\nrelationship between covariates and outcomes. Furthermore, the heterogeneous\npartial effects of $\\boldsymbol{X}$ on $Y$ are represented by\n$\\boldsymbol{\\beta}(\\cdot)$ and can be directly estimated using our proposed\nmethod. Our framework effectively unifies established parametric and\nnonparametric models, including varying-coefficient, switching regression, and\nadditive models. We provide theoretical guarantees, such as pointwise and\n$L^p$-norm rates of convergence for the estimator, and establish a pointwise\ncentral limit theorem through subsampling, aiding inference on the function\n$\\boldsymbol\\beta(\\cdot)$. We present Monte Carlo simulation results to assess\nthe finite-sample performance of the method.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "We rely on Monte Carlo (MC) simulations to interpret searches for new physics\nat the Large Hadron Collider (LHC) and elsewhere. These simulations result in\nnoisy and approximate estimators of selection efficiencies and likelihoods. In\nthis context we pioneer an exact-approximate computational method -\nexact-approximate Markov Chain Monte Carlo - that returns exact inferences\ndespite noisy simulations. To do so, we introduce an unbiased estimator for a\nPoisson likelihood. We demonstrate the new estimator and new techniques in\nexamples based on a search for neutralinos and charginos at the LHC using a\nsimplified model. We find attractive performance characteristics - exact\ninferences are obtained for a similar computational cost to approximate ones\nfrom existing methods and inferences are robust with respect to the number of\nevents generated per point.",
        "Non-smooth optimization models play a fundamental role in various\ndisciplines, including engineering, science, management, and finance. However,\nclassical algorithms for solving such models often struggle with convergence\nspeed, scalability, and parameter tuning, particularly in high-dimensional and\nnon-convex settings. In this paper, we explore how quantum mechanics can be\nleveraged to overcome these limitations. Specifically, we investigate the\ntheoretical properties of the Quantum Hamiltonian Descent (QHD) algorithm for\nnon-smooth optimization in both continuous and discrete time. First, we propose\ncontinuous-time variants of the general QHD algorithm and establish their\nglobal convergence and convergence rate for non-smooth convex and strongly\nconvex problems through a novel Lyapunov function design. Furthermore, we prove\nthe finite-time global convergence of continuous-time QHD for non-smooth\nnon-convex problems under mild conditions (i.e., locally Lipschitz). In\naddition, we propose discrete-time QHD, a fully digitized implementation of QHD\nvia operator splitting (i.e., product formula). We find that discrete-time QHD\nexhibits similar convergence properties even with large time steps. Finally,\nnumerical experiments validate our theoretical findings and demonstrate the\ncomputational advantages of QHD over classical non-smooth non-convex\noptimization algorithms.",
        "In the current paper, we present a study of the spatial distribution of\nluminous blue variables (LBVs) and various LBV candidates (cLBVs) with respect\nto OB associations in the M33 galaxy. The identification of blue star groups\nwas based on the LGGS data and was carried out by two clustering algorithms\nwith initial parameters determined during simulations of random stellar fields.\nWe have found that the distribution of distances to the nearest OB association\nobtained for the LBV\/cLBV sample is close to that for massive stars with\n$M_{\\rm init}>20\\,M_\\odot$ and Wolf-Rayet stars. This result is in good\nagreement with the standard assumption that luminous blue variables represent\nan intermediate stage in the evolution of the most massive stars. However, some\nobjects from the LBV\/cLBV sample, particularly Fe$\\,$II-emission stars,\ndemonstrated severe isolation compared to other massive stars, which, together\nwith certain features of their spectra, implicitly indicates that the nature of\nthese objects and other LBVs\/cLBVs may differ radically.",
        "A Steiner triple system STS$(v)$ is called $f$-pyramidal if it has an\nautomorphism group fixing $f$ points and acting sharply transitively on the\nremaining ones. In this paper, we focus on the STSs that are $f$-pyramidal over\nsome abelian group. Their existence has been settled only for the smallest\nadmissible values of $f$, that is, $f=0,1,3$.\n  In this paper, we complete this result and determine, for every $f>3$, the\nspectrum of values $(f,v)$ for which there is an $f$-pyramidal STS$(v)$ over an\nabelian group. This result is obtained by constructing difference families\nrelative to a suitable partial spread.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We present the solution of a non-linear magnetic-charged black hole with an\nanisotropic matter field and further extend it to obtain the corresponding\nrotating black hole solution using the modified Newman-Janis algorithm. The\nevent horizon and ergosphere of the rotating black hole are studied in terms of\nthe perspective of geometric properties, revealing that the rotating black hole\ncan have up to three horizons. The first law of thermodynamics and the\nsquared-mass formula for the rotating black hole are derived from a\nthermodynamic perspective, based on which we obtain the thermodynamic\nquantities and study the thermodynamic stability of the rotating black hole.\nAdditionally, we calculate the Penrose process for the rotating black hole,\nindicating the influence of various black hole parameters on the maximal\nefficiency of the Penrose process.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Multi-Task Bayesian Optimization",
    "start_abstract":"Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study",
        "Genomic and pathological analyses of an asymmetric true hermaphroditism\n  case in a female labrador retriever",
        "Beyond Cortisol! Physiological Indicators of Welfare for Dogs: Deficits,\n  Misunderstandings and Opportunities",
        "EMICSS: Added-value annotations for EMDB entries",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "Towards Precision Oncology: Predicting Mortality and Relapse-Free\n  Survival in Head and Neck Cancer Using Clinical Data",
        "Spatial Analysis of Neuromuscular Junctions Activation in\n  Three-Dimensional Histology-based Muscle Reconstructions",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Targeting Neurodegeneration: Three Machine Learning Methods for G9a\n  Inhibitors Discovery Using PubChem and Scikit-learn",
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model"
      ],
      "abstract":[
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population.",
        "The two main gonadal development disorders in dogs are true hermaphroditism\nand XX male syndrome. True hermaphroditism can be divided into two\nsubcategories: XX sex reversal and XY sex reversal. XX Sry-negative sex\nreversal is more common, and it is characterized by the presence of both\novarian and testicular tissues in an animal. To date, there are 16 cases of\ntrue hermaphroditism reported in the literature, 15 of which are XX true\nhermaphroditism. Hermaphroditism has not been formally documented in labrador\nretrievers, and no case of asymmetric hermaphroditism has been reported in the\nliterature.",
        "This paper aims to initiate new conversations about the use of physiological\nindicators when assessing the welfare of dogs. There are significant concerns\nabout construct validity - whether the measures used accurately reflect\nwelfare. The goal is to provide recommendations for future inquiry and\nencourage debate. We acknowledge that the scientific understanding of animal\nwelfare has evolved and bring attention to the shortcomings of commonly used\nbiomarkers like cortisol. These indicators are frequently used in isolation and\nwith limited salient dog descriptors, so fail to reflect the canine experience\nadequately. Using a systems approach, we explore various physiological systems\nand alternative indicators, such as heart rate variability and oxidative\nstress, to address this limitation. It is essential to consider factors like\nage, body weight, breed, and sex when interpreting these biomarkers correctly,\nand researchers should report on these in their studies. This discussion\nidentifies possible indicators for both positive and negative experiences. In\nconclusion, we advocate for a practical, evidence-based approach to assessing\nindicators of canine welfare, including non-invasive collection methods. We\nacknowledge the complexity of evaluating experiential responses in dogs across\ndifferent situations and the need for continued work to improve practices and\nrefine terminology. This will enhance our ability to accurately understand\nwelfare and improve the wellbeing of dogs, serving to inform standards of\nanimal welfare assessment. We hope this will promote more fundamental research\nin canine physiology to improve construct validity, leading to better\npractices, ultimately improving the lives of dogs.",
        "Motivation: The Electron Microscopy Data Bank (EMDB) is a key repository for\n3D electron microscopy (3DEM) data but lacks comprehensive annotations and\nconnections to most of the related biological, functional, and structural data.\nThis limitation arises from the optional nature of such information to reduce\ndepositor burden and the complexity of maintaining up-to-date external\nreferences, often requiring depositor consent. To address these challenges, we\ndeveloped EMICSS (EMDB Integration with Complexes, Structures, and Sequences),\nan independent system that automatically updates cross-references with over 20\nexternal resources, including UniProt, AlphaFold DB, PubMed, Complex Portal and\nGene Ontology. Results: EMICSS (https:\/\/www.ebi.ac.uk\/emdb\/emicss) annotations\nare accessible in multiple formats for every EMDB entry and its linked\nresources, and programmatically via the EMDB Application Programming Interface\n(API). EMICSS plays a crucial role supporting the EMDB website, with\nannotations being used on entry pages, statistics, and in the search system.\nAvailability and implementation: EMICSS is implemented in Python and it is an\nopen-source, distributed under the EMBL-EBI license, with core code available\non GitHub (https:\/\/github.com\/emdb-empiar\/added_annotations).",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Head and neck squamous cell carcinoma (HNSCC) presents significant challenges\nin clinical oncology due to its heterogeneity and high mortality rates. This\nstudy aims to leverage clinical data and machine learning (ML) principles to\npredict key outcomes for HNSCC patients: mortality, and relapse-free survival.\nUtilizing data sourced from the Cancer Imaging Archive, an extensive pipeline\nwas implemented to ensure robust model training and evaluation. Ensemble and\nindividual classifiers, including XGBoost, Random Forest, and Support Vectors,\nwere employed to develop predictive models. The study identified key clinical\nfeatures influencing HNSCC mortality outcomes and achieved predictive accuracy\nand ROC-AUC values exceeding 90\\% across tasks. Support Vector Machine strongly\nexcelled in relapse-free survival, with an recall value of 0.99 and precision\nof 0.97. Key clinical features including loco-regional control, smoking and\ntreatment type, were identified as critical predictors of patient outcomes.\nThis study underscores the medical impact of using ML-driven insights to refine\nprognostic accuracy and optimize personalized treatment strategies in HNSCC.",
        "Histology has long been a foundational technique for studying anatomical\nstructures through tissue slicing. Advances in computational methods now enable\nthree dimensional (3D) reconstruction of organs from histology images,\nenhancing the analysis of structural and functional features. Here, we present\na novel multimodal computational method to reconstruct rodent muscles in 3D\nusing classical image processing and data analysis techniques, analyze their\nstructural features and correlate them to previously recorded\nelectrophysiological data. The algorithm analyzes spatial distribution patterns\nof features identified through histological staining, normalizing them across\nmultiple samples. Further, the algorithm successfully correlates spatial\npatterns with high density epimysial ElectroMyoGraphy (hdEMG) recordings,\nproviding a multimodal perspective on neuromuscular dynamics, linking spatial\nand electrophysiological information. The code was validated by looking at the\ndistribution of NeuroMuscular Junctions (NMJs) in naive soleus muscles and\ncompared the distributions and patterns observed with ones observed in previous\nliterature. Our results showed consistency with the expected results,\nvalidating our method for features and pattern recognition. The multimodal\naspect was shown in a naive soleus muscle, where a strong correlation was found\nbetween motor unit locations derived via hdEMG, and NMJ locations obtained from\nhistology, highlighting their spatial relationship. This multimodal analysis\ntool integrates 3D structural data with electrophysiological activity, opening\nnew avenues in muscle diagnostics, regenerative medicine, and personalized\ntherapies where spatial insights could one day predict electrophysiological\nbehavior or vice versa.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "In light of the increasing interest in G9a's role in neuroscience, three\nmachine learning (ML) models, that are time efficient and cost effective, were\ndeveloped to support researchers in this area. The models are based on data\nprovided by PubChem and performed by algorithms interpreted by the scikit-learn\nPython-based ML library. The first ML model aimed to predict the efficacy\nmagnitude of active G9a inhibitors. The ML models were trained with 3,112 and\ntested with 778 samples. The Gradient Boosting Regressor perform the best,\nachieving 17.81% means relative error (MRE), 21.48% mean absolute error (MAE),\n27.39% root mean squared error (RMSE) and 0.02 coefficient of determination\n(R2) error. The goal of the second ML model called a CID_SID ML model, utilised\nPubChem identifiers to predict the G9a inhibition probability of a small\nbiomolecule that has been primarily designed for different purposes. The ML\nmodels were trained with 58,552 samples and tested with 14,000. The most\nsuitable classifier for this case study was the Extreme Gradient Boosting\nClassifier, which obtained 78.1% accuracy, 84.3% precision,69.1% recall, 75.9%\nF1-score and 8.1% Receiver-operating characteristic (ROC). The third ML model\nbased on the Random Forest Classifier algorithm led to the generation of a list\nof descending-ordered functional groups based on their importance to the G9a\ninhibition. The model was trained with 19,455 samples and tested with 14,100.\nThe probability of this rank was 70% accuracy.",
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Taking the Human Out of the Loop: A Review of Bayesian Optimization",
    "start_abstract":"Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study",
        "Genomic and pathological analyses of an asymmetric true hermaphroditism\n  case in a female labrador retriever",
        "Beyond Cortisol! Physiological Indicators of Welfare for Dogs: Deficits,\n  Misunderstandings and Opportunities",
        "EMICSS: Added-value annotations for EMDB entries",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "Towards Precision Oncology: Predicting Mortality and Relapse-Free\n  Survival in Head and Neck Cancer Using Clinical Data",
        "Spatial Analysis of Neuromuscular Junctions Activation in\n  Three-Dimensional Histology-based Muscle Reconstructions",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Targeting Neurodegeneration: Three Machine Learning Methods for G9a\n  Inhibitors Discovery Using PubChem and Scikit-learn",
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model"
      ],
      "abstract":[
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population.",
        "The two main gonadal development disorders in dogs are true hermaphroditism\nand XX male syndrome. True hermaphroditism can be divided into two\nsubcategories: XX sex reversal and XY sex reversal. XX Sry-negative sex\nreversal is more common, and it is characterized by the presence of both\novarian and testicular tissues in an animal. To date, there are 16 cases of\ntrue hermaphroditism reported in the literature, 15 of which are XX true\nhermaphroditism. Hermaphroditism has not been formally documented in labrador\nretrievers, and no case of asymmetric hermaphroditism has been reported in the\nliterature.",
        "This paper aims to initiate new conversations about the use of physiological\nindicators when assessing the welfare of dogs. There are significant concerns\nabout construct validity - whether the measures used accurately reflect\nwelfare. The goal is to provide recommendations for future inquiry and\nencourage debate. We acknowledge that the scientific understanding of animal\nwelfare has evolved and bring attention to the shortcomings of commonly used\nbiomarkers like cortisol. These indicators are frequently used in isolation and\nwith limited salient dog descriptors, so fail to reflect the canine experience\nadequately. Using a systems approach, we explore various physiological systems\nand alternative indicators, such as heart rate variability and oxidative\nstress, to address this limitation. It is essential to consider factors like\nage, body weight, breed, and sex when interpreting these biomarkers correctly,\nand researchers should report on these in their studies. This discussion\nidentifies possible indicators for both positive and negative experiences. In\nconclusion, we advocate for a practical, evidence-based approach to assessing\nindicators of canine welfare, including non-invasive collection methods. We\nacknowledge the complexity of evaluating experiential responses in dogs across\ndifferent situations and the need for continued work to improve practices and\nrefine terminology. This will enhance our ability to accurately understand\nwelfare and improve the wellbeing of dogs, serving to inform standards of\nanimal welfare assessment. We hope this will promote more fundamental research\nin canine physiology to improve construct validity, leading to better\npractices, ultimately improving the lives of dogs.",
        "Motivation: The Electron Microscopy Data Bank (EMDB) is a key repository for\n3D electron microscopy (3DEM) data but lacks comprehensive annotations and\nconnections to most of the related biological, functional, and structural data.\nThis limitation arises from the optional nature of such information to reduce\ndepositor burden and the complexity of maintaining up-to-date external\nreferences, often requiring depositor consent. To address these challenges, we\ndeveloped EMICSS (EMDB Integration with Complexes, Structures, and Sequences),\nan independent system that automatically updates cross-references with over 20\nexternal resources, including UniProt, AlphaFold DB, PubMed, Complex Portal and\nGene Ontology. Results: EMICSS (https:\/\/www.ebi.ac.uk\/emdb\/emicss) annotations\nare accessible in multiple formats for every EMDB entry and its linked\nresources, and programmatically via the EMDB Application Programming Interface\n(API). EMICSS plays a crucial role supporting the EMDB website, with\nannotations being used on entry pages, statistics, and in the search system.\nAvailability and implementation: EMICSS is implemented in Python and it is an\nopen-source, distributed under the EMBL-EBI license, with core code available\non GitHub (https:\/\/github.com\/emdb-empiar\/added_annotations).",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Head and neck squamous cell carcinoma (HNSCC) presents significant challenges\nin clinical oncology due to its heterogeneity and high mortality rates. This\nstudy aims to leverage clinical data and machine learning (ML) principles to\npredict key outcomes for HNSCC patients: mortality, and relapse-free survival.\nUtilizing data sourced from the Cancer Imaging Archive, an extensive pipeline\nwas implemented to ensure robust model training and evaluation. Ensemble and\nindividual classifiers, including XGBoost, Random Forest, and Support Vectors,\nwere employed to develop predictive models. The study identified key clinical\nfeatures influencing HNSCC mortality outcomes and achieved predictive accuracy\nand ROC-AUC values exceeding 90\\% across tasks. Support Vector Machine strongly\nexcelled in relapse-free survival, with an recall value of 0.99 and precision\nof 0.97. Key clinical features including loco-regional control, smoking and\ntreatment type, were identified as critical predictors of patient outcomes.\nThis study underscores the medical impact of using ML-driven insights to refine\nprognostic accuracy and optimize personalized treatment strategies in HNSCC.",
        "Histology has long been a foundational technique for studying anatomical\nstructures through tissue slicing. Advances in computational methods now enable\nthree dimensional (3D) reconstruction of organs from histology images,\nenhancing the analysis of structural and functional features. Here, we present\na novel multimodal computational method to reconstruct rodent muscles in 3D\nusing classical image processing and data analysis techniques, analyze their\nstructural features and correlate them to previously recorded\nelectrophysiological data. The algorithm analyzes spatial distribution patterns\nof features identified through histological staining, normalizing them across\nmultiple samples. Further, the algorithm successfully correlates spatial\npatterns with high density epimysial ElectroMyoGraphy (hdEMG) recordings,\nproviding a multimodal perspective on neuromuscular dynamics, linking spatial\nand electrophysiological information. The code was validated by looking at the\ndistribution of NeuroMuscular Junctions (NMJs) in naive soleus muscles and\ncompared the distributions and patterns observed with ones observed in previous\nliterature. Our results showed consistency with the expected results,\nvalidating our method for features and pattern recognition. The multimodal\naspect was shown in a naive soleus muscle, where a strong correlation was found\nbetween motor unit locations derived via hdEMG, and NMJ locations obtained from\nhistology, highlighting their spatial relationship. This multimodal analysis\ntool integrates 3D structural data with electrophysiological activity, opening\nnew avenues in muscle diagnostics, regenerative medicine, and personalized\ntherapies where spatial insights could one day predict electrophysiological\nbehavior or vice versa.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "In light of the increasing interest in G9a's role in neuroscience, three\nmachine learning (ML) models, that are time efficient and cost effective, were\ndeveloped to support researchers in this area. The models are based on data\nprovided by PubChem and performed by algorithms interpreted by the scikit-learn\nPython-based ML library. The first ML model aimed to predict the efficacy\nmagnitude of active G9a inhibitors. The ML models were trained with 3,112 and\ntested with 778 samples. The Gradient Boosting Regressor perform the best,\nachieving 17.81% means relative error (MRE), 21.48% mean absolute error (MAE),\n27.39% root mean squared error (RMSE) and 0.02 coefficient of determination\n(R2) error. The goal of the second ML model called a CID_SID ML model, utilised\nPubChem identifiers to predict the G9a inhibition probability of a small\nbiomolecule that has been primarily designed for different purposes. The ML\nmodels were trained with 58,552 samples and tested with 14,000. The most\nsuitable classifier for this case study was the Extreme Gradient Boosting\nClassifier, which obtained 78.1% accuracy, 84.3% precision,69.1% recall, 75.9%\nF1-score and 8.1% Receiver-operating characteristic (ROC). The third ML model\nbased on the Random Forest Classifier algorithm led to the generation of a list\nof descending-ordered functional groups based on their importance to the G9a\ninhibition. The model was trained with 19,455 samples and tested with 14,100.\nThe probability of this rank was 70% accuracy.",
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials",
    "start_abstract":"We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b7"
      ],
      "title":[
        "Multi-Task Bayesian Optimization",
        "Taking the Human Out of the Loop: A Review of Bayesian Optimization"
      ],
      "abstract":[
        "Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
        "Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Decentralized Strategies for Backward Linear-Quadratic Mean Field Games\n  and Teams",
        "Regularity of the Product of Two Relaxed Cutters with Relaxation\n  Parameters Beyond Two",
        "Online Learning-Based Predictive Control for Nonlinear System",
        "Convergence of projected stochastic approximation algorithm",
        "State-Dependent Uncertainty Modeling in Robust Optimal Control Problems\n  through Generalized Semi-Infinite Programming",
        "Entropic optimal transport with congestion aversion Application to\n  relocation of drones",
        "A consensus-based optimization method for nonsmooth nonconvex programs\n  with approximated gradient descent scheme",
        "Rank conditions for exactness of semidefinite relaxations in polynomial\n  optimization",
        "Differentiation of inertial methods for optimizing smooth parametric\n  function",
        "Chance constraints transcription and failure risk estimation for\n  stochastic trajectory optimization",
        "Quantization Of Probability Measures In Maximum~Mean~Discrepancy\n  Distance",
        "Complete systems of inequalities relating the perimeter, the area and\n  the Cheeger constant of planar domains",
        "Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven\n  Deep Reinforcement Learning",
        "Fluctuations of non-local branching Markov processes",
        "Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu",
        "Differential virial analysis: a new technique to determine the dynamical\n  state of molecular clouds",
        "Quantum geometry of non-Hermitian systems",
        "Gravity-induced collisions of uncharged cloud droplets in an electric\n  field",
        "Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential\n  Recommendation",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs",
        "A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer",
        "Ways of Seeing, and Selling, AI Art",
        "Project portfolio planning in the pharmaceutical industry -- strategic\n  objectives and quantitative optimization",
        "Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation",
        "A Possible Four-Month Periodicity in the Activity of FRB 20240209A",
        "Hybrid Near\/Far-Field Frequency-Dependent Beamforming via Joint\n  Phase-Time Arrays"
      ],
      "abstract":[
        "This paper studies a new class of linear-quadratic mean field games and teams\nproblem, where the large-population system satisfies a class of $N$ weakly\ncoupled linear backward stochastic differential equations (BSDEs), and $z_i$ (a\npart of solution of BSDE) enter the state equations and cost functionals. By\nvirtue of stochastic maximum principle and optimal filter technique, we obtain\na Hamiltonian system first, which is a fully coupled forward-backward\nstochastic differential equation (FBSDE). Decoupling the Hamiltonian system, we\nderive a feedback form optimal strategy by introducing Riccati equations,\nstochastic differential equation (SDE) and BSDE. Finally, we provide a\nnumerical example to simulate our results.",
        "We study the product of two relaxed cutters having a common fixed point. We\nassume that one of the relaxation parameters is greater than two so that the\ncorresponding relaxed cutter is no longer quasi-nonexpansive, but rather\ndemicontractive. We show that if both of the operators are (weakly\/linearly)\nregular, then under certain conditions, the resulting product inherits the same\ntype of regularity. We then apply these results to proving convergence in the\nweak, norm and linear sense of algorithms that employ such products.",
        "In this paper, we propose an online learning-based predictive control (LPC)\napproach designed for nonlinear systems that lack explicit system dynamics.\nUnlike traditional model predictive control (MPC) algorithms that rely on known\nsystem models to optimize controller outputs, our proposed algorithm integrates\na reinforcement learning component to learn optimal policies in real time from\nthe offline dataset and real-time data. Additionally, an optimal control\nproblem (OCP)-based optimization framework is incorporated to enhance real-time\ncomputational efficiency while ensuring stability during online operation.\nMoreover, we rigorously establish the super-linear convergence properties of\nthe algorithm. Finally, extensive simulations are performed to evaluate the\nfeasibility and effectiveness of the proposed approach.",
        "We study the Robbins-Monro stochastic approximation algorithm with\nprojections on a hyperrectangle and prove its convergence. This work fills a\ngap in the convergence proof of the classic book by Kushner and Yin. Using the\nODE method, we show that the algorithm converges to stationary points of a\nrelated projected ODE. Our results provide a better theoretical foundation for\nstochastic optimization techniques, including stochastic gradient descent and\nits proximal version. These results extend the algorithm's applicability and\nrelax some assumptions of previous research.",
        "Generalized semi-infinite programs (generalized SIPs) are problems featuring\na finite number of decision variables but an infinite number of constraints.\nThey differ from standard SIPs in that their constraint set itself depends on\nthe choice of the decision variable. Generalized SIPs can be used to model\nrobust optimal control problems where the uncertainty itself is a function of\nthe state or control input, allowing for a less conservative alternative to\nassuming a uniform uncertainty set over the entire decision space. In this\nwork, we demonstrate how any generalized SIP can be converted to an\nexistence-constrained SIP through a reformulation of the constraints and solved\nusing a local reduction approach, which approximates the infinite constraint\nset by a finite number of scenarios. This transformation is then exploited to\nsolve nonlinear robust optimal control problems with state-dependent\nuncertainties. We showcase our proposed approach on a planar quadrotor\nsimulation where it recovers the true generalized SIP solution and outperforms\na SIP-based approach with uniform uncertainty bounds.",
        "We present a mathematical framework for tempo-spatial entropic optimal\ntransport, motivated by the problem of efficiently routing drones back to\nlogistics centers. To address collision risk, we incorporate a convex penalty\nterm into the transport model. We propose the Sinkhorn-Frank-Wolfe algorithm, a\nnumerically efficient method with theoretical convergence guarantees, and\ndemonstrate its effectiveness through experiments on synthetic datasets. Our\napproach provides a foundation for optimizing large-scale autonomous drone\nlogistics while ensuring safe and efficient transportation.",
        "In this paper, we are interested in finding the global minimizer of a\nnonsmooth nonconvex unconstrained optimization problem. By combining the\ndiscrete consensus-based optimization (CBO) algorithm and the gradient descent\nmethod, we develop a novel CBO algorithm with an extra gradient descent scheme\nevaluated by the forward-difference technique on the function values, where\nonly the objective function values are used in the proposed algorithm. First,\nwe prove that the proposed algorithm can exhibit global consensus in an\nexponential rate in two senses and possess a unique global consensus point.\nSecond, we evaluate the error estimate between the objective function value on\nthe global consensus point and its global minimum. In particular, as the\nparameter $\\beta$ tends to $\\infty$, the error converges to zero and the\nconvergence rate is $\\mathcal{O}\\left(\\frac{\\log\\beta}{\\beta}\\right)$. Third,\nunder some suitable assumptions on the objective function, we provide the\nnumber of iterations required for the mean square error in expectation to reach\nthe desired accuracy. It is worth underlining that the theoretical analysis in\nthis paper does not use the mean-field limit. Finally, we illustrate the\nimproved efficiency and promising performance of our novel CBO method through\nsome experiments on several nonconvex benchmark problems and the application to\ntrain deep neural networks.",
        "We consider the Moment-SOS hierarchy in polynomial optimization. We first\nprovide a sufficient condition to solve the truncated K-moment problem\nassociated with a given degree-$2n$ pseudo-moment sequence $\\phi$ n and a\nsemi-algebraic set $K \\subset \\mathbb{R}^d$. Namely, let $2v$ be the maximum\ndegree of the polynomials that describe $K$. If the rank $r$ of its associated\nmoment matrix is less than $nv + 1$, then $\\phi^n$ has an atomic representing\nmeasure supported on at most $r$ points of $K$. When used at step-$n$ of the\nMoment-SOS hierarchy, it provides a sufficient condition to guarantee its\nfinite convergence (i.e., the optimal value of the corresponding degree-n\nsemidefinite relaxation of the hierarchy is the global minimum). For Quadratic\nConstrained Quadratic Problems (QCQPs) one may also recover global minimizers\nfrom the optimal pseudo-moment sequence. Our condition is in the spirit of\nBlekherman's rank condition and while on the one-hand it is more restrictive,\non the other hand it applies to constrained POPs as it provides a localization\non $K$ for the representing measure.",
        "In this paper, we consider the minimization of a $C^2-$smooth and strongly\nconvex objective depending on a given parameter, which is usually found in many\npractical applications. We suppose that we desire to solve the problem with\nsome inertial methods which cover a broader existing well-known inertial\nmethods. Our main goal is to analyze the derivative of this algorithm as an\ninfinite iterative process in the sense of ``automatic'' differentiation. This\nprocedure is very common and has gain more attention recently. From a pure\noptimization perspective and under some mild premises, we show that any\nsequence generated by these inertial methods converge to the unique minimizer\nof the problem, which depends on the parameter. Moreover, we show a local\nlinear convergence rate of the generated sequence. Concerning the\ndifferentiation of the scheme, we prove that the derivative of the sequence\nwith respect to the parameter converges to the derivative of the limit of the\nsequence showing that any sequence is <<derivative stable>>. Finally, we\ninvestigate the rate at which the convergence occurs. We show that, this is\nlocally linear with an error term tending to zero.",
        "Space exploration has advanced significantly, with missions increasingly\nusing complex dynamical systems. Optimal trajectory design is crucial,\ninvolving the minimization of objective functions while ensuring robustness\nagainst measurement and control errors. Recent research has focused on\nstochastic solvers that address uncertainties through chance constraints, which\nare relaxed hard constraints allowing for a given failure risk. This study\nintroduces three novel, general, multidimensional transcription methods for\nchance constraints: the spectral radius, first-order, and d-th order methods.\nAdditionally, we introduce failure risk estimation techniques and a\nconservatism metric to enable comprehensive comparison with existing\napproaches. Applications to aerospace test cases demonstrate the effectiveness\nof the proposed transcriptions, highlighting that state-of-the-art methods\nsignificantly overestimate risk. Notably, the d-th order transcription\ndramatically outperforms the other methods, particularly in high-dimensional\nscenarios. This work shows that spectral radius-based methods are overly\nconservative and computationally intensive, while the first-order and d-th\norder methods offer practical and efficient alternatives.",
        "Accurate approximation of probability measures is essential in numerical\napplications. This paper explores the quantization of probability measures\nusing the maximum mean discrepancy (MMD) distance as a guiding metric. We first\ninvestigate optimal approximations by determining the best weights, followed by\naddressing the problem of optimal facility locations.\n  To facilitate efficient computation, we reformulate the nonlinear objective\nas expectations over a product space, enabling the use of stochastic\napproximation methods. For the Gaussian kernel, we derive closed-form\nexpressions to develop a deterministic optimization approach. By integrating\nstochastic approximation with deterministic techniques, our framework achieves\nprecise and efficient quantization of continuous distributions, with\nsignificant implications for machine learning and signal processing\napplications.",
        "The object of the paper is to find complete systems of inequalities relating\nthe perimeter $P$, the area $|\\cdot|$ and the Cheeger constant $h$ of planar\nsets. To do so, we study the so called Blaschke--Santal\\'o diagram of the\ntriplet $(P,h,|\\cdot|)$ for different classes of domains: simply connected\nsets, convex sets and convex polygons with at most $N$ sides. We completely\ndetermine the diagram in the latter cases except for the class of convex\n$N$-gons when $N\\ge 5$ is odd: therein, we show that the boundary of the\ndiagram is given by the graphs of two continuous and strictly increasing\nfunctions. An explicit formula for the lower one and a numerical method to\nobtain the upper one is provided. At last, some applications of the results are\npresented.",
        "Given the complexity of multi-tenant cloud environments and the need for\nreal-time threat mitigation, Security Operations Centers (SOCs) must integrate\nAI-driven adaptive defenses against Advanced Persistent Threats (APTs).\nHowever, SOC analysts struggle with countering adaptive adversarial tactics,\nnecessitating intelligent decision-support frameworks. To enhance human-AI\ncollaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep\nQ-Network (CHT-DQN) framework that models SOC analysts' decision-making against\nAI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1,\nanticipating attacker strategies, while the APT bot (attacker) follows a\nlevel-0 exploitative policy. By incorporating CHT into DQN, our framework\nenhances SOC defense strategies via Attack Graph (AG)-based reinforcement\nlearning. Simulation experiments across varying AG complexities show that\nCHT-DQN achieves higher data protection and lower action discrepancies compared\nto standard DQN. A theoretical lower bound analysis further validates its\nsuperior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon\nMechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven\ntransition probabilities align better with adaptive attackers, improving data\nprotection. Additionally, human decision patterns exhibit risk aversion after\nfailure and risk-seeking behavior after success, aligning with Prospect Theory.\nThese findings underscore the potential of integrating cognitive modeling into\ndeep reinforcement learning to enhance SOC operations and develop real-time\nadaptive cloud security mechanisms.",
        "The aim of this paper is to study the fluctuations of a general class of\nsupercritical branching Markov processes with non-local branching mechanisms.\nWe show the existence of three regimes according to the size of the spectral\ngap associated with the expectation semigroup of the branching process and\nestablish functional central limit theorems within each regime.",
        "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.",
        "Since molecular clouds form stars, at least some parts of them must be in a\nstate of collapse. However, there is a long-standing debate as to whether that\ncollapse is local, involving only a small fraction of the cloud mass, or\nglobal, with most mass in a state of collapse up to the moment when it is\ndispersed by stellar feedback. In principle it is possible to distinguish these\npossibilities from clouds' virial ratios, which should be a factor of two\nlarger for collapse than for equilibrium, but systematic uncertainties have\nthus far prevented such measurements. Here we propose a new analysis method to\novercome this limitation: while the absolute value of a cloud's virial ratio is\ntoo uncertain to distinguish global from local collapse, the differential\nchange in virial ratio as a function of surface density is also diagnostic of\nclouds' dynamical state, and can be measured with far fewer systematic\nuncertainties. We demonstrate the basic principles of the method using simple\nanalytic models of supported and collapsing clouds, validate it from full 3D\nsimulations, and discuss possible challenges in applying the method to real\ndata. We then provide a preliminary application of the technique to recent\nobservations of the molecular clouds in Andromeda, showing that most of them\nare inconsistent with being in a state of global collapse.",
        "The Berry curvature characterizes one aspect of the geometry of quantum\nstates. It materializes, among other consequences, as an anomalous velocity of\nwave packets. In non-Hermitian systems, wave packet dynamics is enriched by\nadditional terms that can be expressed as generalizations of the Berry\nconnection to non-orthogonal eigenstates. Here, we contextualize these\nanomalous non-Hermitian contributions by showing that they directly arise from\nthe geometry of the underlying quantum states as corrections to the distance\nbetween left and perturbed right eigenstates. By calculating the electric\nsusceptibility for a single-band wave packet and comparing it with the wave\npacket's localization, we demonstrate that these terms can, in some\ncircumstances, lead to a violation of fluctuation-dissipation relations in\nnon-Hermitian systems. We discuss experimental signatures in terms of response\nfunctions and transport signatures.",
        "We investigate the collisions of uncharged, conducting droplets settling\nunder gravity in the presence of an external electric field. Previous studies\nhave derived a near-field asymptotic expression for the electric-field-induced\nattraction, suggesting that this force can overcome lubrication resistance and\ndrive surface-to-surface contact between two spherical conductors within a\nfinite time. However, for droplets moving in air, traditional lubrication\ntheory breaks down when the inter-droplet gap approaches the mean free path of\nair molecules. To account for this, we incorporate non-continuum hydrodynamic\neffects to estimate the gravity-driven collision efficiency under\nelectric-field-induced forces. This study examines how an external electric\nfield influences the trajectories of settling droplet pairs of unequal sizes.\nBy analyzing their motion, we compute collision efficiencies and explore their\ndependence on droplet size ratio, electric field strength, the angle between\nthe field and gravity, and key dimensionless parameters governing\nelectric-field-induced and van der Waals forces. Our findings reveal that\nelectric-field-induced forces significantly enhance collision efficiency,\nhighlighting their critical role in droplet coalescence dynamics.",
        "Traditional sequential recommendation (SR) methods heavily rely on explicit\nitem IDs to capture user preferences over time. This reliance introduces\ncritical limitations in cold-start scenarios and domain transfer tasks, where\nunseen items and new contexts often lack established ID mappings. To overcome\nthese limitations, recent studies have shifted towards leveraging text-only\ninformation for recommendation, thereby improving model generalization and\nadaptability across domains. Although promising, text-based SR faces unique\ndifficulties: items' text descriptions often share semantic similarities that\nlead to clustered item representations, compromising their uniformity, a\nproperty essential for promoting diversity and enhancing generalization in\nrecommendation systems. In this paper, we explore a novel framework to improve\nthe uniformity of item representations in text-based SR. Our analysis reveals\nthat items within a sequence exhibit marked semantic similarity, meaning they\nare closer in representation than items overall, and that this effect is more\npronounced for less popular items, which form tighter clusters compared to\ntheir more popular counterparts. Based on these findings, we propose UniT, a\nframework that employs three pairwise item sampling strategies: Unified General\nSampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven\nSampling Strategy. Each strategy applies varying degrees of repulsion to\nselectively adjust the distances between item pairs, thereby refining\nrepresentation uniformity while considering both sequence context and item\npopularity. Extensive experiments on multiple real-world datasets demonstrate\nthat our proposed approach outperforms state-of-the-art models, validating the\neffectiveness of UniT in enhancing both representation uniformity and\nrecommendation accuracy.The source code is available at\nhttps:\/\/github.com\/ccwwhhh\/Model-Rec.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.",
        "The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.",
        "In early 2025, Augmented Intelligence - Christie's first AI art auction -\ndrew criticism for showcasing a controversial genre. Amid wider legal\nuncertainty, artists voiced concerns over data mining practices, notably with\nrespect to copyright. The backlash could be viewed as a microcosm of AI's\ncontested position in the creative economy. Touching on the auction's\npresentation, reception, and results, this paper explores how, among social\ndissonance, machine learning finds its place in the artworld. Foregrounding\nresponsible innovation, the paper provides a balanced perspective that\nchampions creators' rights and brings nuance to this polarised debate. With a\nfocus on exhibition design, it centres framing, which refers to the way a piece\nis presented to influence consumer perception. Context plays a central role in\nshaping our understanding of how good, valuable, and even ethical an artwork\nis. In this regard, Augmented Intelligence situates AI art within a\nsurprisingly traditional framework, leveraging hallmarks of \"high art\" to\nestablish the genre's cultural credibility. Generative AI has a clear economic\ndimension, converging questions of artistic merit with those of monetary worth.\nScholarship on ways of seeing, or framing, could substantively inform the\ninterpretation and evaluation of creative outputs, including assessments of\ntheir aesthetic and commercial value.",
        "Many pharmaceutical companies face concerns with the maintenance of desired\nrevenue levels. Sales forecasts for the current portfolio of products and\nprojects may indicate a decline in revenue as the marketed products approach\npatent expiry. To counteract the potential downturn in revenue, and to\nestablish revenue growth, an in-flow of new projects into the development\nphases is required. In this article, we devise an approach with which the\nin-flow of new projects could be optimized, while adhering to the objectives\nand constraints set on revenue targets, budget limitations and strategic\nconsiderations on the composition of the company's portfolio.",
        "With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients from\ndistant galaxies. While most FRBs are singular events, repeaters emit multiple\nbursts, with only two-FRB 121102 and FRB 180916B-showing periodic activity (160\nand 16 days, respectively). FRB 20240209A, discovered by CHIME-FRB, is\nlocalized to the outskirts of a quiescent elliptical galaxy (z = 0.1384). We\ndiscovered a periodicity of ~ 126 days in the activity of the FRB 20240209A,\npotentially adding to the list of extremely rare periodic repeating FRBs. We\nused auto-correlation and Lomb-Scargle periodogram analyses, validated with\nrandomized control samples, to confirm the periodicity. The FRB's location in\nan old stellar population disfavors young progenitor models, instead pointing\nto scenarios involving globular clusters, late-stage magnetars, or low-mass\nX-ray binaries (LMXBs). Though deep X-ray or polarimetric observations are not\navailable, the localization of the FRB and a possible periodicity points to\nprogenitors likely to be a binary involving a compact object and a stellar\ncompanion or a precessing or rotating old neutron star.",
        "Joint phase-time arrays (JPTA) emerge as a cost-effective and\nenergy-efficient architecture for frequency-dependent beamforming in wideband\ncommunications by utilizing both true-time delay units and phase shifters. This\npaper exploits the potential of JPTA to simultaneously serve multiple users in\nboth near- and far-field regions with a single radio frequency chain. The goal\nis to jointly optimize JPTA-based beamforming and subband allocation to\nmaximize overall system performance. To this end, we formulate a system utility\nmaximization problem, including sum-rate maximization and proportional fairness\nas special cases. We develop a 3-step alternating optimization (AO) algorithm\nand an efficient deep learning (DL) method for this problem. The DL approach\nincludes a 2-layer convolutional neural network, a 3-layer graph attention\nnetwork (GAT), and a normalization module for resource and beamforming\noptimization. The GAT efficiently captures the interactions between resource\nallocation and analog beamformers. Simulation results confirm that JPTA\noutperforms conventional phased arrays (PA) in enhancing user rate and strikes\na good balance between PA and fully-digital approach in energy efficiency.\nEmploying a logarithmic utility function for user rates ensures greater\nfairness than maximizing sum-rates. Furthermore, the DL network achieves\ncomparable performance to the AO approach, while having orders of magnitude\nlower computational complexity."
      ]
    }
  },
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots",
    "start_abstract":"Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods.",
    "start_categories":[
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal"
      ],
      "abstract":[
        "Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "The FFT Strikes Again: An Efficient Alternative to Self-Attention",
        "Whenever, Wherever: Towards Orchestrating Crowd Simulations with\n  Spatio-Temporal Spawn Dynamics",
        "It's My Data Too: Private ML for Datasets with Multi-User Training\n  Examples",
        "Smoothing ADMM for Non-convex and Non-smooth Hierarchical Federated\n  Learning",
        "GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering\n  and Large Language Models for Explainable Classification",
        "Al-Khwarizmi: Discovering Physical Laws with Foundation Models",
        "Efficient and Sharp Off-Policy Learning under Unobserved Confounding",
        "Food Delivery Time Prediction in Indian Cities Using Machine Learning\n  Models",
        "LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic\n  Health Records",
        "Uncovering Utility Functions from Observed Outcomes",
        "Continuous K-Max Bandits",
        "Reinforcement Learning with Segment Feedback",
        "A Structured Reasoning Framework for Unbalanced Data Classification\n  Using Probabilistic Models",
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities"
      ],
      "abstract":[
        "Conventional self-attention mechanisms exhibit quadratic complexity in\nsequence length, making them challenging to scale for long inputs. We present\nFFTNet, an adaptive spectral filtering framework that uses the Fast Fourier\nTransform (FFT) to achieve global token mixing in \\(\\mathcal{O}(n\\log n)\\)\ntime. By mapping inputs into the frequency domain, FFTNet exploits\northogonality and energy preservation-guaranteed by Parseval's theorem-to\nefficiently model long-range dependencies. Our main theoretical contributions\ninclude 1) An adaptive spectral filter that highlights salient frequency\ncomponents, 2) A hybrid scheme combining local windowing with a global FFT\nbranch, 3) Nonlinear feature transformations applied in both the frequency and\ntoken domains. Experiments on Long Range Arena and ImageNet validate our\ntheoretical insights and demonstrate superior performance over fixed\nFourier-based and standard attention models.",
        "Realistic crowd simulations are essential for immersive virtual environments,\nrelying on both individual behaviors (microscopic dynamics) and overall crowd\npatterns (macroscopic characteristics). While recent data-driven methods like\ndeep reinforcement learning improve microscopic realism, they often overlook\ncritical macroscopic features such as crowd density and flow, which are\ngoverned by spatio-temporal spawn dynamics, namely, when and where agents enter\na scene. Traditional methods, like random spawn rates, stochastic processes, or\nfixed schedules, are not guaranteed to capture the underlying complexity or\nlack diversity and realism. To address this issue, we propose a novel approach\ncalled nTPP-GMM that models spatio-temporal spawn dynamics using Neural\nTemporal Point Processes (nTPPs) that are coupled with a spawn-conditional\nGaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate\nour approach by orchestrating crowd simulations of three diverse real-world\ndatasets with nTPP-GMM. Our experiments demonstrate the orchestration with\nnTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios\nand allow crowd analysis.",
        "We initiate a study of algorithms for model training with user-level\ndifferential privacy (DP), where each example may be attributed to multiple\nusers, which we call the multi-attribution model. We first provide a carefully\nchosen definition of user-level DP under the multi-attribution model. Training\nin the multi-attribution model is facilitated by solving the contribution\nbounding problem, i.e. the problem of selecting a subset of the dataset for\nwhich each user is associated with a limited number of examples. We propose a\ngreedy baseline algorithm for the contribution bounding problem. We then\nempirically study this algorithm for a synthetic logistic regression task and a\ntransformer training task, including studying variants of this baseline\nalgorithm that optimize the subset chosen using different techniques and\ncriteria. We find that the baseline algorithm remains competitive with its\nvariants in most settings, and build a better understanding of the practical\nimportance of a bias-variance tradeoff inherent in solutions to the\ncontribution bounding problem.",
        "This paper presents a hierarchical federated learning (FL) framework that\nextends the alternating direction method of multipliers (ADMM) with smoothing\ntechniques, tailored for non-convex and non-smooth objectives. Unlike\ntraditional hierarchical FL methods, our approach supports asynchronous updates\nand multiple updates per iteration, enhancing adaptability to heterogeneous\ndata and system settings. Additionally, we introduce a flexible mechanism to\nleverage diverse regularization functions at each layer, allowing customization\nto the specific prior information within each cluster and accommodating\n(possibly) non-smooth penalty objectives. Depending on the learning goal, the\nframework supports both consensus and personalization: the total variation norm\ncan be used to enforce consensus across layers, while non-convex penalties such\nas minimax concave penalty (MCP) or smoothly clipped absolute deviation (SCAD)\nenable personalized learning. Experimental results demonstrate the superior\nconvergence rates and accuracy of our method compared to conventional\napproaches, underscoring its robustness and versatility for a wide range of FL\nscenarios.",
        "This paper introduces GPT-HTree, a framework combining hierarchical\nclustering, decision trees, and large language models (LLMs) to address this\nchallenge. By leveraging hierarchical clustering to segment individuals based\non salient features, resampling techniques to balance class distributions, and\ndecision trees to tailor classification paths within each cluster, GPT-HTree\nensures both accuracy and interpretability. LLMs enhance the framework by\ngenerating human-readable cluster descriptions, bridging quantitative analysis\nwith actionable insights.",
        "Inferring physical laws from data is a central challenge in science and\nengineering, including but not limited to healthcare, physical sciences,\nbiosciences, social sciences, sustainability, climate, and robotics. Deep\nnetworks offer high-accuracy results but lack interpretability, prompting\ninterest in models built from simple components. The Sparse Identification of\nNonlinear Dynamics (SINDy) method has become the go-to approach for building\nsuch modular and interpretable models. SINDy leverages sparse regression with\nL1 regularization to identify key terms from a library of candidate functions.\nHowever, SINDy's choice of candidate library and optimization method requires\nsignificant technical expertise, limiting its widespread applicability. This\nwork introduces Al-Khwarizmi, a novel agentic framework for physical law\ndiscovery from data, which integrates foundational models with SINDy.\nLeveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach\nautomates physical law discovery, incorporating prior knowledge and iteratively\nrefining candidate solutions via reflection. Al-Khwarizmi operates in two\nsteps: it summarizes system observations-comprising textual descriptions, raw\ndata, and plots-followed by a secondary step that generates candidate feature\nlibraries and optimizer configurations to identify hidden physics laws\ncorrectly. Evaluating our algorithm on over 198 models, we demonstrate\nstate-of-the-art performance compared to alternatives, reaching a 20 percent\nincrease against the best-performing alternative.",
        "We develop a novel method for personalized off-policy learning in scenarios\nwith unobserved confounding. Thereby, we address a key limitation of standard\npolicy learning: standard policy learning assumes unconfoundedness, meaning\nthat no unobserved factors influence both treatment assignment and outcomes.\nHowever, this assumption is often violated, because of which standard policy\nlearning produces biased estimates and thus leads to policies that can be\nharmful. To address this limitation, we employ causal sensitivity analysis and\nderive a statistically efficient estimator for a sharp bound on the value\nfunction under unobserved confounding. Our estimator has three advantages: (1)\nUnlike existing works, our estimator avoids unstable minimax optimization based\non inverse propensity weighted outcomes. (2) Our estimator is statistically\nefficient. (3) We prove that our estimator leads to the optimal\nconfounding-robust policy. Finally, we extend our theory to the related task of\npolicy improvement under unobserved confounding, i.e., when a baseline policy\nsuch as the standard of care is available. We show in experiments with\nsynthetic and real-world data that our method outperforms simple plug-in\napproaches and existing baselines. Our method is highly relevant for\ndecision-making where unobserved confounding can be problematic, such as in\nhealthcare and public policy.",
        "Accurate prediction of food delivery times significantly impacts customer\nsatisfaction, operational efficiency, and profitability in food delivery\nservices. However, existing studies primarily utilize static historical data\nand often overlook dynamic, real-time contextual factors crucial for precise\nprediction, particularly in densely populated Indian cities. This research\naddresses these gaps by integrating real-time contextual variables such as\ntraffic density, weather conditions, local events, and geospatial data\n(restaurant and delivery location coordinates) into predictive models. We\nsystematically compare various machine learning algorithms, including Linear\nRegression, Decision Trees, Bagging, Random Forest, XGBoost, and LightGBM, on a\ncomprehensive food delivery dataset specific to Indian urban contexts. Rigorous\ndata preprocessing and feature selection significantly enhanced model\nperformance. Experimental results demonstrate that the LightGBM model achieves\nsuperior predictive accuracy, with an R2 score of 0.76 and Mean Squared Error\n(MSE) of 20.59, outperforming traditional baseline approaches. Our study thus\nprovides actionable insights for improving logistics strategies in complex\nurban environments. The complete methodology and code are publicly available\nfor reproducibility and further research.",
        "Lab tests are fundamental for diagnosing diseases and monitoring patient\nconditions. However, frequent testing can be burdensome for patients, and test\nresults may not always be immediately available. To address these challenges,\nwe propose LabTOP, a unified model that predicts lab test outcomes by\nleveraging a language modeling approach on EHR data. Unlike conventional\nmethods that estimate only a subset of lab tests or classify discrete value\nranges, LabTOP performs continuous numerical predictions for a diverse range of\nlab items. We evaluate LabTOP on three publicly available EHR datasets and\ndemonstrate that it outperforms existing methods, including traditional machine\nlearning models and state-of-the-art large language models. We also conduct\nextensive ablation studies to confirm the effectiveness of our design choices.\nWe believe that LabTOP will serve as an accurate and generalizable framework\nfor lab test outcome prediction, with potential applications in clinical\ndecision support and early detection of critical conditions.",
        "Determining consumer preferences and utility is a foundational challenge in\neconomics. They are central in determining consumer behaviour through the\nutility-maximising consumer decision-making process. However, preferences and\nutilities are not observable and may not even be known to the individual making\nthe choice; only the outcome is observed in the form of demand. Without the\nability to observe the decision-making mechanism, demand estimation becomes a\nchallenging task and current methods fall short due to lack of scalability or\nability to identify causal effects. Estimating these effects is critical when\nconsidering changes in policy, such as pricing, the impact of taxes and\nsubsidies, and the effect of a tariff. To address the shortcomings of existing\nmethods, we combine revealed preference theory and inverse reinforcement\nlearning to present a novel algorithm, Preference Extraction and Reward\nLearning (PEARL) which, to the best of our knowledge, is the only algorithm\nthat can uncover a representation of the utility function that best\nrationalises observed consumer choice data given a specified functional form.\nWe introduce a flexible utility function, the Input-Concave Neural Network\nwhich captures complex relationships across goods, including cross-price\nelasticities. Results show PEARL outperforms the benchmark on both noise-free\nand noisy synthetic data.",
        "We study the $K$-Max combinatorial multi-armed bandits problem with\ncontinuous outcome distributions and weak value-index feedback: each base arm\nhas an unknown continuous outcome distribution, and in each round the learning\nagent selects $K$ arms, obtains the maximum value sampled from these $K$ arms\nas reward and observes this reward together with the corresponding arm index as\nfeedback. This setting captures critical applications in recommendation\nsystems, distributed computing, server scheduling, etc. The continuous $K$-Max\nbandits introduce unique challenges, including discretization error from\ncontinuous-to-discrete conversion, non-deterministic tie-breaking under limited\nfeedback, and biased estimation due to partial observability. Our key\ncontribution is the computationally efficient algorithm DCK-UCB, which combines\nadaptive discretization with bias-corrected confidence bounds to tackle these\nchallenges. For general continuous distributions, we prove that DCK-UCB\nachieves a $\\widetilde{\\mathcal{O}}(T^{3\/4})$ regret upper bound, establishing\nthe first sublinear regret guarantee for this setting. Furthermore, we identify\nan important special case with exponential distributions under full-bandit\nfeedback. In this case, our proposed algorithm MLE-Exp enables\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret upper bound through maximal\nlog-likelihood estimation, achieving near-minimax optimality.",
        "Standard reinforcement learning (RL) assumes that an agent can observe a\nreward for each state-action pair. However, in practical applications, it is\noften difficult and costly to collect a reward for each state-action pair.\nWhile there have been several works considering RL with trajectory feedback, it\nis unclear if trajectory feedback is inefficient for learning when trajectories\nare long. In this work, we consider a model named RL with segment feedback,\nwhich offers a general paradigm filling the gap between per-state-action\nfeedback and trajectory feedback. In this model, we consider an episodic Markov\ndecision process (MDP), where each episode is divided into $m$ segments, and\nthe agent observes reward feedback only at the end of each segment. Under this\nmodel, we study two popular feedback settings: binary feedback and sum\nfeedback, where the agent observes a binary outcome and a reward sum according\nto the underlying reward function, respectively. To investigate the impact of\nthe number of segments $m$ on learning performance, we design efficient\nalgorithms and establish regret upper and lower bounds for both feedback\nsettings. Our theoretical and experimental results show that: under binary\nfeedback, increasing the number of segments $m$ decreases the regret at an\nexponential rate; in contrast, surprisingly, under sum feedback, increasing $m$\ndoes not reduce the regret significantly.",
        "This paper studies a Markov network model for unbalanced data, aiming to\nsolve the problems of classification bias and insufficient minority class\nrecognition ability of traditional machine learning models in environments with\nuneven class distribution. By constructing joint probability distribution and\nconditional dependency, the model can achieve global modeling and reasoning\noptimization of sample categories. The study introduced marginal probability\nestimation and weighted loss optimization strategies, combined with\nregularization constraints and structured reasoning methods, effectively\nimproving the generalization ability and robustness of the model. In the\nexperimental stage, a real credit card fraud detection dataset was selected and\ncompared with models such as logistic regression, support vector machine,\nrandom forest and XGBoost. The experimental results show that the Markov\nnetwork performs well in indicators such as weighted accuracy, F1 score, and\nAUC-ROC, significantly outperforming traditional classification models,\ndemonstrating its strong decision-making ability and applicability in\nunbalanced data scenarios. Future research can focus on efficient model\ntraining, structural optimization, and deep learning integration in large-scale\nunbalanced data environments and promote its wide application in practical\napplications such as financial risk control, medical diagnosis, and intelligent\nmonitoring.",
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats."
      ]
    }
  }
]