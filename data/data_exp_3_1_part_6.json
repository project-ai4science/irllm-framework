[
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease",
    "start_abstract":"<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b17"
      ],
      "title":[
        "Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
        "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"
      ],
      "abstract":[
        "Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
        "Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach."
      ],
      "categories":[
        "cs.CV",
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "A mathematical perspective on the paradox that chemotherapy sometimes\n  works backwards",
        "An Efficient Quantum Approximate Optimization Algorithm with Fixed\n  Linear Ramp Schedule for Truss Structure Optimization",
        "The COSMOS-Web ring: Spectroscopic confirmation of the background source\n  at z = 5.1",
        "Financial Fraud Detection with Entropy Computing",
        "Survey on Question Answering over Visually Rich Documents: Methods,\n  Challenges, and Trends",
        "Cracking Vector Search Indexes",
        "Fineness and smoothness of a KSBA moduli of marked cubic surfaces",
        "Aspects of Complexity in Quantum Evolutions on the Bloch Sphere",
        "Fluctuation-driven topological Hall effect in room-temperature itinerant\n  helimagnet Fe3Ga4",
        "Invariant and non-invariant almost complex structures on compact\n  quotients of Lie groups",
        "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
        "Self-supervised conformal prediction for uncertainty quantification in\n  Poisson imaging problems",
        "Dynamic Manipulation of Multiphase Fluid in Microgravity Using\n  Photoresponsive Surfactant",
        "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning\n  and Reflection",
        "EXALT: EXplainable ALgorithmic Tools for Optimization Problems",
        "Disentangled Latent Spaces for Reduced Order Models using Deterministic\n  Autoencoders",
        "Simulation and Harmonic Analysis of k-Space Readout (SHAKER)",
        "Skillful High-Resolution Ensemble Precipitation Forecasting with an\n  Integrated Deep Learning Framework",
        "Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth\n  Models",
        "DICE: Distilling Classifier-Free Guidance into Text Embeddings",
        "Optimal Rebate Design: Incentives, Competition and Efficiency in Auction\n  Markets",
        "220 GHz RIS-Aided Multi-user Terahertz Communication System: Prototype\n  Design and Over-the-Air Experimental Trials",
        "Tomographic Signatures of Interacting Majorana and Andreev States in\n  Superconductor-Semiconductor Transmon Qubits",
        "VerifBFL: Leveraging zk-SNARKs for A Verifiable Blockchained Federated\n  Learning",
        "Symmetry violation-driven hysteresis loops as measurands for\n  noise-resilient sensors",
        "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
        "Topological-to-Topological Transition Induced by On-Site Nonlinearity in\n  a One-Dimensional Topological Insulator",
        "Using gradient of Lagrangian function to compute efficient channels for\n  the ideal observer"
      ],
      "abstract":[
        "Doctors are well aware that sometimes cancer treatments not only fail, but\neven work backwards, i.e. they make the treated tumor grow. In this work we\npresent a mathematical perspective on this paradox in the case of chemotherapy,\nby studying a minimally parameterized mathematical model for the system\ncomposed of the tumor and the surrounding vasculature. To this end, we will use\na system of two well-established nonlinear ordinary differential equations,\nwhich incorporates the cytotoxic (via the Norton-Simon hypothesis) and\nantiangiogenic effects of chemotherapy. Finally, we provide two theoretical\nways to avoid these anomalies.",
        "This study proposes a novel structural optimization framework based on\nquantum variational circuits, in which the multiplier acting on the\ncross-sectional area of each rod in a truss structure as an updater is used as\na design variable. Specifically, we employ a classical processor for structural\nanalysis with the finite element method, and the Quantum Approximate\nOptimization Algorithm (QAOA) is subsequently performed to update the\ncross-sectional area so that the compliance is minimized. The advantages of\nthis framework can be seen in three key aspects. First, by defining design\nvariables as multipliers, rather than simply reducing the design variable to a\nbinary candidate of inclusion or exclusion (corresponding to qubit states, ``0\"\nand ``1\"), it provides greater flexibility in adjusting the cross-sectional\narea of the rod at each iteration of the optimization process. Second, the\nmultipliers acting on rods are encoded with on-off encoding, eliminating\nadditional constraints in the convergence judgement. As a result, the objective\nfunction is in a simple format, enabling efficient optimization using\nQAOA.Third, a fixed linear ramp schedule (FLRS) for variational parameter\nsetting bypasses the classical optimization process, thereby improving the\noperational efficiency of the framework. In the two structural cases\ninvestigated in this study, the proposed approach highlights the feasibility\nand applicability potential of quantum computing in advancing engineering\ndesign and optimization. Numerical experiments have demonstrated the\neffectiveness of this framework, providing a firm foundation for future\nresearch on quantum-assisted optimization methods in engineering fields.",
        "We report the spectroscopic confirmation of the background source of the most\ndistant Einstein ring known to date, the COSMOS-Web ring. This system consists\nof a complete Einstein ring at $z=5.1$, lensed by a massive early-type galaxy\nat $z\\sim2$. The redshift $z=5.1043\\pm0.0004$ is unambiguously identified with\nour NOEMA and Keck\/MOSFIRE spectroscopy, where the NOEMA observations reveal\nthe CO(4-3) and CO(5-4) lines at $>8\\,\\sigma$, and the MOSFIRE data detect\n[O\\textsc{ii}] at $\\sim 6\\,\\sigma$. Using multi-wavelength photometry spanning\nnear-infrared to radio bands, we find that the lensed galaxy is a dust-obscured\nstarburst ($M_{\\star} \\sim 1.8\\times10^{10}\\,{\\rm M_{\\odot}}$, ${\\rm\nSFR_{IR}\\sim 60\\,{\\rm M_{\\odot}} ~yr^{-1}}$) with high star-formation\nefficiency (gas depletion time $\\tau_{\\rm dep}<100~$Myr) as indicated by the\n[C\\textsc{i}](1-0) non-detection. The redshift confirmation revalidates that\nthe total lens mass budget within the Einstein radius is fully accounted for by\nthe stellar and dark matter components, without the need of modifying the\ninitial mass function or dark matter distribution profile. This work paves the\nway for detailed studies and future follow-ups of this unique lensing system,\nproviding an ideal laboratory for studying mass distribution at $z\\sim2$ and\nphysical conditions of star formation at $z\\sim5$.",
        "We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.",
        "The field of visually-rich document understanding, which involves interacting\nwith visually-rich documents (whether scanned or born-digital), is rapidly\nevolving and still lacks consensus on several key aspects of the processing\npipeline. In this work, we provide a comprehensive overview of state-of-the-art\napproaches, emphasizing their strengths and limitations, pointing out the main\nchallenges in the field, and proposing promising research directions.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "By work of Gallardo-Kerr-Schaffler, it is known that Naruki's\ncompactification of the moduli space of marked cubic surfaces is isomorphic to\nthe normalization of the Koll\\'ar, Shepherd-Barron, and Alexeev\ncompactification parametrizing pairs\n$\\left(S,\\left(\\frac{1}{9}+\\epsilon\\right)D\\right)$, with $D$ the sum of the\n$27$ marked lines on $S$, and their stable degenerations. In the current paper,\nwe show that the normalization assumption is not necessary as we prove that\nthis KSBA compactification is smooth. Additionally, we show it is a fine moduli\nspace. This is done by studying the automorphisms and the\n$\\mathbb{Q}$-Gorenstein obstructions of the stable pairs parametrized by it.",
        "We enhance our quantitative comprehension of the complexity associated with\nboth time-optimal and time sub-optimal quantum Hamiltonian evolutions that\nconnect arbitrary source and target states on the Bloch sphere, as recently\npresented in Nucl. Phys. B1010, 116755 (2025). Initially, we examine each\nunitary Schrodinger quantum evolution selected through various metrics, such as\npath length, geodesic efficiency, speed efficiency, and the curvature\ncoefficient of the corresponding quantum-mechanical trajectory that connects\nthe source state to the target state on the Bloch sphere. Subsequently, we\nevaluate the selected evolutions using our proposed measure of complexity, as\nwell as in relation to the concept of complexity length scale. The choice of\nboth time-optimal and time sub-optimal evolutions, along with the selection of\nsource and target states, enables us to conduct pertinent sanity checks that\nseek to validate the physical relevance of the framework supporting our\nproposed complexity measure. Our research suggests that, in general, efficient\nquantum evolutions possess a lower complexity than their inefficient\ncounterparts. However, it is important to recognize that complexity is not\nsolely determined by length; in fact, longer trajectories that are adequately\ncurved may exhibit a complexity that is less than or equal to that of shorter\ntrajectories with a lower curvature coefficient.",
        "The topological Hall effect (THE) is a hallmark of a non-trivial geometric\nspin arrangement in a magnetic metal, originating from a finite scalar spin\nchirality (SSC). The associated Berry phase is often a consequence of\nnon-coplanar magnetic structures identified by multiple k-vectors. For single-k\nmagnetic structures however with zero SSC, the emergence of a finite\ntopological Hall signal presents a conceptual challenge. Here, we report that a\nfluctuation-driven mechanism involving chiral magnons is responsible for the\nobserved THE in a low-symmetry compound, monoclinic Fe3Ga4. Through neutron\nscattering experiments, we discovered several nontrivial magnetic phases in\nthis system. In our focus is the helical spiral phase at room temperature,\nwhich transforms into a transverse conical state in applied magnetic field,\nsupporting a significant THE signal up to and above room temperature. Our work\noffers a fresh perspective in the search for novel materials with intertwined\ntopological magnetic and transport properties.",
        "In this paper we briefly survey the classical problem of understanding which\nLie algebras admit a complex structure, put in the broader perspective of\nalmost complex structures with special properties. We focus on the different\nbehavior of invariant and non-invariant structures, with a special attention to\ntheir canonical bundle and Kodaira dimension. We provide new examples of\ncomputations of Kodaira dimension of invariant and non-invariant structures.",
        "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
        "Image restoration problems are often ill-posed, leading to significant\nuncertainty in reconstructed images. Accurately quantifying this uncertainty is\nessential for the reliable interpretation of reconstructed images. However,\nimage restoration methods often lack uncertainty quantification capabilities.\nConformal prediction offers a rigorous framework to augment image restoration\nmethods with accurate uncertainty quantification estimates, but it typically\nrequires abundant ground truth data for calibration. This paper presents a\nself-supervised conformal prediction method for Poisson imaging problems which\nleverages Poisson Unbiased Risk Estimator to eliminate the need for ground\ntruth data. The resulting self-calibrating conformal prediction approach is\napplicable to any Poisson linear imaging problem that is ill-conditioned, and\nis particularly effective when combined with modern self-supervised image\nrestoration techniques trained directly on measurement data. The proposed\nmethod is demonstrated through numerical experiments on image denoising and\ndeblurring; its performance are comparable to supervised conformal prediction\nmethods relying on ground truth data.",
        "Control of bubble motion is essential for improving efficiency and creating\nnew functionalities in electrochemistry, heat transfer, and biomedical systems.\nPhotoresponsive surfactants enable bubble manipulation by creating surface\ntension gradients, inducing a photo-Marangoni flow under illumination, without\nneeding any engineered substrates, by leveraging a reversible switch in\nmolecular conformation. Although previous studies have demonstrated bubble\nmanipulation using photo-responsive surfactants, a comprehensive understanding\nof how fluid behavior is affected by critical parameters, such as bubble size,\nillumination, photo-switching kinetics, concentration, and adsorption\ndesorption kinetics, remains elusive. Advances have been limited by the complex\nmultiphysics processed involved, and by the fact that earth-bound experiments\ncannot study bubble photo-Marangoni dynamics without interference from bubble\nbuoyancy and photo-thermal convection. We elucidate the factors enabling fast\nphoto-Marangoni-driven bubble motion, by performing microgravity experiments,\nenabled by a bespoke photo-surfactant, complemented by a detailed modeling\nframework. We identify an optimal bubble size for migration, since smaller and\nlarger bubbles incur weaker photo-Marangoni stresses and larger drag,\nrespectively. Surfactants that switch rapidly under illumination drive fast\nmigration, provided their reverse switch (in darkness) is much slower, yet not\nnegligible. These foundational results enable the synthesis of next-generation\nphoto-surfactants and photo-Marangoni manipulation across multiphase fluid\nsystems.",
        "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\n\\url{https:\/\/github.com\/Reallm-Labs\/InfiGUIAgent}.",
        "Algorithmic solutions have significant potential to improve decision-making\nacross various domains, from healthcare to e-commerce. However, the widespread\nadoption of these solutions is hindered by a critical challenge: the lack of\nhuman-interpretable explanations. Current approaches to Explainable AI (XAI)\npredominantly focus on complex machine learning models, often producing brittle\nand non-intuitive explanations. This project proposes a novel approach to\ndeveloping explainable algorithms by starting with optimization problems,\nspecifically the assignment problem. The developed software library enriches\nbasic algorithms with human-understandable explanations through four key\nmethodologies: generating meaningful alternative solutions, creating robust\nsolutions through input perturbation, generating concise decision trees and\nproviding reports with comprehensive explanation of the results. Currently\ndeveloped tools are often designed with specific clustering algorithms in mind,\nwhich limits their adaptability and flexibility to incorporate alternative\ntechniques. Additionally, many of these tools fail to integrate expert\nknowledge, which could enhance the clustering process by providing valuable\ninsights and context. This lack of adaptability and integration can hinder the\neffectiveness and robustness of the clustering outcomes in various\napplications. The represents a step towards making algorithmic solutions more\ntransparent, trustworthy, and accessible. By collaborating with industry\npartners in sectors such as sales, we demonstrate the practical relevance and\ntransformative potential of our approach.",
        "Data-driven reduced-order models based on autoencoders generally lack\ninterpretability compared to classical methods such as the proper orthogonal\ndecomposition. More interpretability can be gained by disentangling the latent\nvariables and analyzing the resulting modes. For this purpose, probabilistic\n$\\beta$-variational autoencoders ($\\beta$-VAEs) are frequently used in\ncomputational fluid dynamics and other simulation sciences. Using a benchmark\nperiodic flow dataset, we show that competitive results can be achieved using\nnon-probabilistic autoencoder approaches that either promote orthogonality or\npenalize correlation between latent variables. Compared to probabilistic\nautoencoders, these approaches offer more robustness with respect to the choice\nof hyperparameters entering the loss function. We further demonstrate the\nability of a non-probabilistic approach to identify a reduced number of active\nlatent variables by introducing a correlation penalty, a function also known\nfrom the use of $\\beta$-VAE. The investigated probabilistic and\nnon-probabilistic autoencoder models are finally used for the dimensionality\nreduction of aircraft ditching loads, which serves as an industrial application\nin this work.",
        "In the realm of neuroimaging research, the demand for efficient and accurate\nsimulation tools for functional magnetic resonance imaging (fMRI) data is ever\nincreasing. We present SHAKER, a comprehensive MATLAB package for simulating\ncomplex-valued fMRI time series data that will advance understanding and\nimplementation of the MR signal equation and related physics principles to fMRI\nsimulation. The core objective of the package is to provide researchers with a\nuser-friendly MATLAB graphical user interface (GUI) tool capable of generating\ncomplex-valued fMRI time series data. This tool will allow researchers to input\nvarious parameters related to the MRI scan and receive simulated k-space data\nwith ease, facilitating a deeper understanding of the intricacies of the\ngeneration and interpretation of fMRI data.",
        "High-resolution precipitation forecasts are crucial for providing accurate\nweather prediction and supporting effective responses to extreme weather\nevents. Traditional numerical models struggle with stochastic subgrid-scale\nprocesses, while recent deep learning models often produce blurry results. To\naddress these challenges, we propose a physics-inspired deep learning framework\nfor high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble\nprecipitation forecasting. Trained on ERA5 and CMPA high-resolution\nprecipitation datasets, the framework integrates deterministic and\nprobabilistic components. The deterministic model, based on a 3D\nSwinTransformer, captures average precipitation at mesoscale resolution and\nincorporates strategies to enhance performance, particularly for moderate to\nheavy rainfall. The probabilistic model employs conditional diffusion in latent\nspace to account for uncertainties in residual precipitation at convective\nscales. During inference, ensemble members are generated by repeatedly sampling\nlatent variables, enabling the model to represent precipitation uncertainty.\nOur model significantly enhances spatial resolution and forecast accuracy. Rank\nhistogram shows that the ensemble system is reliable and unbiased. In a case\nstudy of heavy precipitation in southern China, the model outputs align more\nclosely with observed precipitation distributions than ERA5, demonstrating\nsuperior capability in capturing extreme precipitation events. Additionally,\n5-day real-time forecasts show good performance in terms of CSI scores.",
        "Glioblastoma, a highly aggressive brain tumor, poses major challenges due to\nits poor prognosis and high morbidity rates. Partial differential\nequation-based models offer promising potential to enhance therapeutic outcomes\nby simulating patient-specific tumor behavior for improved radiotherapy\nplanning. However, model calibration remains a bottleneck due to the high\ncomputational demands of optimization methods like Monte Carlo sampling and\nevolutionary algorithms. To address this, we recently introduced an approach\nleveraging a neural forward solver with gradient-based optimization to\nsignificantly reduce calibration time. This approach requires a highly accurate\nand fully differentiable forward model. We investigate multiple architectures,\nincluding (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a\n3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best\noverall results, excelling in both tumor outline matching and voxel-level\nprediction of tumor cell concentration. It halved the MSE relative to the\nbaseline model and achieved the highest Dice score across all tumor cell\nconcentration thresholds. Our study demonstrates significant enhancement in\nforward solver performance and outlines important future research directions.",
        "Text-to-image diffusion models are capable of generating high-quality images,\nbut these images often fail to align closely with the given text prompts.\nClassifier-free guidance (CFG) is a popular and effective technique for\nimproving text-image alignment in the generative process. However, using CFG\nintroduces significant computational overhead and deviates from the established\ntheoretical foundations of diffusion models. In this paper, we present\nDIstilling CFG by enhancing text Embeddings (DICE), a novel approach that\nremoves the reliance on CFG in the generative process while maintaining the\nbenefits it provides. DICE distills a CFG-based text-to-image diffusion model\ninto a CFG-free version by refining text embeddings to replicate CFG-based\ndirections. In this way, we avoid the computational and theoretical drawbacks\nof CFG, enabling high-quality, well-aligned image generation at a fast sampling\nspeed. Extensive experiments on multiple Stable Diffusion v1.5 variants, SDXL\nand PixArt-$\\alpha$ demonstrate the effectiveness of our method. Furthermore,\nDICE supports negative prompts for image editing to improve image quality\nfurther. Code will be available soon.",
        "This study explores the design of an efficient rebate policy in auction\nmarkets, focusing on a continuous-time setting with competition among market\nparticipants. In this model, a stock exchange collects transaction fees from\nauction investors executing block trades to buy or sell a risky asset, then\nredistributes these fees as rebates to competing market makers submitting limit\norders. Market makers influence both the price at which the asset trades and\ntheir arrival intensity in the auction. We frame this problem as a\nprincipal-multi-agent problem and provide necessary and sufficient conditions\nto characterize the Nash equilibrium among market makers. The exchange's\noptimization problem is formulated as a high-dimensional\nHamilton-Jacobi-Bellman equation with Poisson jump processes, which is solved\nusing a verification result. To numerically compute the optimal rebate and\ntransaction fee policies, we apply the Deep BSDE method. Our results show that\noptimal transaction fees and rebate structures improve market efficiency by\nnarrowing the spread between the auction clearing price and the asset's\nfundamental value, while ensuring a minimal gain for both market makers indexed\non the price of the asset on a coexisting limit order book.",
        "Terahertz (THz) communication technology is regarded as a promising enabler\nfor achieving ultra-high data rate transmission in next-generation\ncommunication systems. To mitigate the high path loss in THz systems, the\ntransmitting beams are typically narrow and highly directional, which makes it\ndifficult for a single beam to serve multiple users simultaneously. To address\nthis challenge, reconfigurable intelligent surfaces (RIS), which can\ndynamically manipulate the wireless propagation environment, have been\nintegrated into THz communication systems to extend coverage. Existing works\nmostly remain theoretical analysis and simulation, while prototype validation\nof RIS-assisted THz communication systems is scarce. In this paper, we designed\na liquid crystal-based RIS operating at 220 GHz supporting both single-user and\nmulti-user communication scenarios, followed by a RIS-aided THz communication\nsystem prototype. To enhance the system performance, we developed a beamforming\nmethod including a real-time power feedback control, which is compatible with\nboth single-beam and multibeam modes. To support simultaneous multi-user\ntransmission, we designed an OFDM-based resource allocation scheme. In our\nexperiments, the received power gain with RIS is no less than 10 dB in the\nsingle-beam mode, and no less than 5 dB in the multi-beam mode. With the\nassistance of RIS, the achievable rate of the system could reach 2.341 Gbps\nwith 3 users sharing 400 MHz bandwidth and the bit error rate (BER) of the\nsystem decreased sharply. Finally, an image transmission experiment was\nconducted to vividly show that the receiver could recover the transmitted\ninformation correctly with the help of RIS. The experimental results also\ndemonstrated that the received signal quality was enhanced through power\nfeedback adjustments.",
        "Semiconductor-based Josephson junctions embedded within a Cooper-pair-box can\nhost complex many-body states, such as interacting Andreev states and\npotentially other quasi-particles of topological origin. Here, we study the\ninsights that could be revealed from a tomographic reconstruction of the\nCooper-pair charge distribution of the junction prepared in its ground state.\nWe posit that interacting and topological states can be identified from\ndistinct signatures within the probability distribution of the charge states.\nFurthermore, the comprehensive dataset provides direct access to information\ntheory metrics elucidating the entanglement between the charge sector of the\nsuperconductor and the microscopic degrees of freedom in the junction. We\ndemonstrate how these metrics serve to further classify differences between the\ntypes of excitations in the junction.",
        "Blockchain-based Federated Learning (FL) is an emerging decentralized machine\nlearning paradigm that enables model training without relying on a central\nserver. Although some BFL frameworks are considered privacy-preserving, they\nare still vulnerable to various attacks, including inference and model\npoisoning. Additionally, most of these solutions employ strong trust\nassumptions among all participating entities or introduce incentive mechanisms\nto encourage collaboration, making them susceptible to multiple security flaws.\nThis work presents VerifBFL, a trustless, privacy-preserving, and verifiable\nfederated learning framework that integrates blockchain technology and\ncryptographic protocols. By employing zero-knowledge Succinct Non-Interactive\nArgument of Knowledge (zk-SNARKs) and incrementally verifiable computation\n(IVC), VerifBFL ensures the verifiability of both local training and\naggregation processes. The proofs of training and aggregation are verified\non-chain, guaranteeing the integrity and auditability of each participant's\ncontributions. To protect training data from inference attacks, VerifBFL\nleverages differential privacy. Finally, to demonstrate the efficiency of the\nproposed protocols, we built a proof of concept using emerging tools. The\nresults show that generating proofs for local training and aggregation in\nVerifBFL takes less than 81s and 2s, respectively, while verifying them\non-chain takes less than 0.6s.",
        "Sublinear resonant deviations from an exceptional point degeneracy (EPD) has\nbeen recently promoted as a sensing scheme. However, there is still an ongoing\ndebate whether the sensitivity advantage is negated by an increase in\nfundamental noise - especially when active elements induce self-oscillations.\nIn this case, nonlinearities are crucial in stabilizing amplifying modes and\nmitigating noise effects. A drawback is the formation of hysteresis loops that\nsignal a transition to unstable modes. This can only be alleviated by precise\ncavity symmetry management. Here, utilizing two coupled nonlinear RLC tanks\nwith balanced amplification and attenuation, we demonstrate that an explicit\nsymmetry violation, induced by sweeping the resonant detuning of the RLC tanks,\nreveals a hysteresis loop near the EPD whose width scales sublinearly with the\ninter-tank coupling. Our proposal re-envisions this disadvantageous feature as\na sensing protocol with diverging sensitivity, enhanced signal-to-noise ratio,\nand self-calibration without requiring delicate symmetry control. As such, it\nopens new avenues in metrology as well as for optical or RF switching and\ntriggering.",
        "Decoding strategies significantly influence the quality and diversity of the\ngenerated texts in large language models (LLMs), yet their impact on\ncomputational resource consumption, particularly GPU energy usage, is\ninsufficiently studied. This paper investigates the relationship between text\ngeneration decoding methods and energy efficiency, focusing on the trade-off\nbetween generation quality and GPU energy consumption across diverse tasks and\ndecoding configurations. By benchmarking multiple strategies across different\ntext generation tasks, such as Translation, Code Summarization, and Math\nProblem Solving, we reveal how selecting appropriate decoding techniques with\ntheir tuned hyperparameters affects text quality and has measurable\nimplications for resource utilization, emphasizing the need for balanced\noptimization. To the best of our knowledge, this study is among the first to\nexplore decoding strategies in LLMs through the lens of energy consumption,\noffering actionable insights for designing resource-aware applications that\nmaintain high-quality text generation.",
        "Recent studies have extended the notion of band topology to nonlinear systems\nby defining nonlinear counterparts of eigenvalue problems. They have found the\nnonlinearity-induced topological transition, while it has required complicated\nnonlinearity such as off-diagonal one. Thus, the existence of\nnonlinearity-induced transitions has been unclear under homogeneous on-site\nnonlinearity, which is ubiquitously found in nature. We here reveal that such\non-site nonlinearity can induce transitions of topological modes, where\ntopological modes converging to zero begin to converge to nonzero values. Since\nsuch nonlinearity-induced transition remains the bulk band topology unchanged,\nwe can regard it as a transition from a conventional topological mode to one\nunique to nonlinear systems. We analyze a nonlinear eigenvalue problem by\nrewriting it to a dynamical system in the spatial direction and clarify that\nthe nonlinearity-induced transition is a result of the bifurcation in the\nspatial dynamics. We also propose a possible setup to observe the\nnonlinearity-induced transition that uses a gradual amplification of nonlinear\nwaves. These results provide a general designing principle of topological\ninsulators controlled by nonlinearity.",
        "It is widely accepted that the Bayesian ideal observer (IO) should be used to\nguide the objective assessment and optimization of medical imaging systems. The\nIO employs complete task-specific information to compute test statistics for\nmaking inference decisions and performs optimally in signal detection tasks.\nHowever, the IO test statistic typically depends non-linearly on the image data\nand cannot be analytically determined. The ideal linear observer, known as the\nHotelling observer (HO), can sometimes be used as a surrogate for the IO.\nHowever, when image data are high dimensional, HO computation can be difficult.\nEfficient channels that can extract task-relevant features have been\ninvestigated to reduce the dimensionality of image data to approximate IO and\nHO performance. This work proposes a novel method for generating efficient\nchannels by use of the gradient of a Lagrangian-based loss function that was\ndesigned to learn the HO. The generated channels are referred to as the\nLagrangian-gradient (L-grad) channels. Numerical studies are conducted that\nconsider binary signal detection tasks involving various backgrounds and\nsignals. It is demonstrated that channelized HO (CHO) using L-grad channels can\nproduce significantly better signal detection performance compared to the CHO\nusing PLS channels. Moreover, it is shown that the proposed L-grad method can\nachieve significantly lower computation time compared to the PLS method."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)",
    "start_abstract":"Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm\n  With Cuckoo Filter",
        "Polyregular Model Checking",
        "Quasinormal modes of nonthermal fixed points",
        "Constrained multi-fidelity Bayesian optimization with automatic stop\n  condition",
        "A Probabilistic WxChallenge Proposal",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration\n  of Large and Small Language Model",
        "MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting\n  and Attention Distillation",
        "Movable Antenna Enhanced DF and AF Relaying Systems: Performance\n  Analysis and Optimization",
        "Connection between planetary He I $\\lambda$10830 \\AA\\ absorption and\n  extreme-ultraviolet emission of planet-host stars",
        "Low-Complexity Event Detection and Identification in Coherent\n  Correlation OTDR Measurements",
        "Assortment optimization given basket shopping behavior using the Ising\n  model",
        "NavG: Risk-Aware Navigation in Crowded Environments Based on\n  Reinforcement Learning with Guidance Points",
        "Leveraging Randomness in Model and Data Partitioning for Privacy\n  Amplification",
        "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home",
        "Improving discrepancy by moving a few points",
        "Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch",
        "Effect of spin on the dynamics of multi-component trans-relativistic\n  accretion flows around Kerr black holes",
        "Supercooled phase transitions in conformal dark sectors explain NANOGrav\n  data",
        "Advancing Tumor Budding Detection with Fourier Ptychography Microscopy",
        "Enhancing 5G O-RAN Communication Efficiency Through AI-Based Latency\n  Forecasting",
        "Thiolation and PEGylation of silicon carbide nanoparticle",
        "Geodesic Connectedness on Statistical Manifolds with Divisible Cubic\n  Forms",
        "Noetherianity of polynomial rings up to group actions",
        "From obstacle to opportunity: uncovering the silver lining of pileup",
        "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
        "New Co-Simulation Variants for Emissions and Cost Reduction of\n  Sustainable District Heating Planning",
        "Bursting Filter Bubble: Enhancing Serendipity Recommendations with\n  Aligned Large Language Models",
        "3d Mirrors and Phase Diagrams of Abelian Gauge Theories"
      ],
      "abstract":[
        "Although retrieval-augmented generation(RAG) significantly improves\ngeneration quality by retrieving external knowledge bases and integrating\ngenerated content, it faces computational efficiency bottlenecks, particularly\nin knowledge retrieval tasks involving hierarchical structures for Tree-RAG.\nThis paper proposes a Tree-RAG acceleration method based on the improved Cuckoo\nFilter, which optimizes entity localization during the retrieval process to\nachieve significant performance improvements. Tree-RAG effectively organizes\nentities through the introduction of a hierarchical tree structure, while the\nCuckoo Filter serves as an efficient data structure that supports rapid\nmembership queries and dynamic updates. The experiment results demonstrate that\nour method is much faster than naive Tree-RAG while maintaining high levels of\ngenerative quality. When the number of trees is large, our method is hundreds\nof times faster than naive Tree-RAG. Our work is available at\nhttps:\/\/github.com\/TUPYP7180\/CFT-RAG-2025.",
        "We reduce the model checking problem for a subset of Python to the\nsatisfiability of a first-order formula over finite words, which is known to be\ndecidable. The reduction is based on the theory of polyregular functions, a\nrecently developed generalization of regular languages to polynomial output\nstring-to-string functions. We implemented this reduction in a verification\ntool called PolyCheck, that can use both automata-based solvers and classical\nSMT solvers as backends.",
        "Quasinormal modes play a prominent role in relaxation of diverse physical\nsystems to equilibria, ranging from astrophysical black holes to tiny droplets\nof quark-gluon plasma at RHIC and LHC accelerators. We propose that a novel\nkind of quasinormal modes govern the direct approach to self-similar time\nevolution of nonthermal fixed points, whose relevance ranges from high energy\nphysics to cold atom gases. We utilize black hole perturbation theory\ntechniques to compute the spectrum of these far from equilibrium quasinormal\nmodes for a kinetic theory with a Focker-Planck collision kernel in isotropic\nand homogeneous states. Our conclusion is that quasinormal modes of nonthermal\nfixed points give rise to a tower of progressively more decaying power-law\ncontributions. A byproduct of our analysis is a precise determination and\nimproved understanding of the distribution function characterizing nonthermal\nfixed points.",
        "Bayesian optimization (BO) is increasingly employed in critical applications\nto find the optimal design with minimal cost. While BO is known for its sample\nefficiency, relying solely on costly high-fidelity data can still result in\nhigh costs. This is especially the case in constrained search spaces where BO\nmust not only optimize but also ensure feasibility. A related issue in the BO\nliterature is the lack of a systematic stopping criterion. To solve these\nchallenges, we develop a constrained cost-aware multi-fidelity BO (CMFBO)\nframework whose goal is to minimize overall sampling costs by utilizing\ninexpensive low-fidelity sources while ensuring feasibility. In our case, the\nconstraints can change across the data sources and may be even black-box\nfunctions. We also introduce a systematic stopping criterion that addresses the\nlong-lasting issue associated with BO's convergence assessment. Our framework\nis publicly available on GitHub through the GP+ Python package and herein we\nvalidate it's efficacy on multiple benchmark problems.",
        "The national forecasting competition WxChallenge, brainchild of Brad Illston\nat the University of Oklahoma in 2005, has become a cherished institution\nplayed across the United States each year. Participants include students,\nfaculty, alumni, and industry professionals. However, forecasts are given as\nscalar values without expression of uncertainty, probabilities being a keystone\nof meteorological forecasting today, and previous attempts to add probabilistic\nelements to WxChallenge have failed partly due to challenges in making\nprobability forecasting accessible to all, and inability to combine scores with\ndifferent units while also appropriately rewarding forecasts using proper\nscoring rules. Much of the competition's maintenance relies on dedicated\nvolunteers, highlighting need for more automation. Hence I propose three new\nfeatures: (1) automated forecast problems based on morning ensemble guidance,\nforming prediction baselines, thresholds over which the players demonstrate\nskill in their later forecast; (2) a spread betting game, where the players\nallocate 100 confidence credits to the over-under for exceeding a percentile\n(e.g., 50pc) threshold of a variable (e.g., maximum temperature) derived from\nthe ensemble baseline; and (3) a game where players distribute 100 confidence\ncredits across bins of a continuous variable (e.g., accumulated precipitation)\napproximating a probability mass function. Forecasts are evaluated using\nShannon information gained over the baseline forecast, yielding additive units\nof bits that allow score combinations of different variables and units.\nInformation gain parallels the Brier Score and is likewise a sound measure of\nskill due its punishment of hedging. This proposal objective is to augment\nWxChallenge with two new probabilistic games that are accessible,\nscientifically sound, enjoyable, and optional.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Automated log analysis is crucial to ensure high availability and reliability\nof complex systems. The advent of LLMs in NLP has ushered in a new era of\nlanguage model-driven automated log analysis, garnering significant interest.\nWithin this field, two primary paradigms based on language models for log\nanalysis have become prominent. Small Language Models (SLMs) follow the\npre-train and fine-tune paradigm, focusing on the specific log analysis task\nthrough fine-tuning on supervised datasets. On the other hand, LLMs following\nthe in-context learning paradigm, analyze logs by providing a few examples in\nprompt contexts without updating parameters. Despite their respective\nstrengths, we notice that SLMs are more cost-effective but less powerful,\nwhereas LLMs with large parameters are highly powerful but expensive and\ninefficient. To trade-off between the performance and inference costs of both\nmodels in automated log analysis, this paper introduces an adaptive log\nanalysis framework known as AdaptiveLog, which effectively reduces the costs\nassociated with LLM while ensuring superior results. This framework\ncollaborates an LLM and a small language model, strategically allocating the\nLLM to tackle complex logs while delegating simpler logs to the SLM.\nSpecifically, to efficiently query the LLM, we propose an adaptive selection\nstrategy based on the uncertainty estimation of the SLM, where the LLM is\ninvoked only when the SLM is uncertain. In addition, to enhance the reasoning\nability of the LLM in log analysis tasks, we propose a novel prompt strategy by\nretrieving similar error-prone cases as the reference, enabling the model to\nleverage past error experiences and learn solutions from these cases. Extensive\nexperiments demonstrate that AdaptiveLog achieves state-of-the-art results\nacross different tasks, elevating the overall accuracy of log analysis while\nmaintaining cost efficiency.",
        "In recent years, attention-based models have excelled across various domains\nbut remain vulnerable to backdoor attacks, often from downloading or\nfine-tuning on poisoned datasets. Many current methods to mitigate backdoors in\nNLP models rely on the pre-trained (unfine-tuned) weights, but these methods\nfail in scenarios where the pre-trained weights are not available. In this\nwork, we propose MBTSAD, which can mitigate backdoors in the language model by\nutilizing only a small subset of clean data and does not require pre-trained\nweights. Specifically, MBTSAD retrains the backdoored model on a dataset\ngenerated by token splitting. Then MBTSAD leverages attention distillation, the\nretrained model is the teacher model, and the original backdoored model is the\nstudent model. Experimental results demonstrate that MBTSAD achieves comparable\nbackdoor mitigation performance as the methods based on pre-trained weights\nwhile maintaining the performance on clean data. MBTSAD does not rely on\npre-trained weights, enhancing its utility in scenarios where pre-trained\nweights are inaccessible. In addition, we simplify the min-max problem of\nadversarial training and visualize text representations to discover that the\ntoken splitting method in MBTSAD's first step generates Out-of-Distribution\n(OOD) data, leading the model to learn more generalized features and eliminate\nbackdoor patterns.",
        "Movable antenna (MA) has been deemed as a promising technology to flexibly\nreconfigure wireless channels by adjusting the antenna positions in a given\nlocal region. In this paper, we investigate the application of the MA\ntechnology in both decode-and-forward (DF) and amplify-and-forward (AF)\nrelaying systems, where a relay is equipped with multiple MAs to assist in the\ndata transmission between two single-antenna nodes. For the DF relaying system,\nour objective is to maximize the achievable rate at the destination by jointly\noptimizing the positions of the MAs in two stages for receiving signals from\nthe source and transmitting signals to the destination, respectively. To drive\nessential insights, we first derive a closed-form upper bound on the maximum\nachievable rate of the DF relaying system. Then, a low-complexity algorithm\nbased on projected gradient ascent (PGA) and alternating optimization (AO) is\nproposed to solve the antenna position optimization problem. For the AF\nrelaying system, our objective is to maximize the achievable rate by jointly\noptimizing the two-stage MA positions as well as the AF beamforming matrix at\nthe relay, which results in a more challenging optimization problem due to the\nintricate coupling variables. To tackle this challenge, we first reveal the\nhidden separability among the antenna position optimization in the two stages\nand the beamforming optimization. Based on such separability, we derive a\nclosed-form upper bound on the maximum achievable rate of the AF relaying\nsystem and propose a low-complexity algorithm to obtain a high-quality\nsuboptimal solution to the considered problem. Simulation results validate the\nefficacy of our theoretical analysis and demonstrate the superiority of the\nMA-enhanced relaying systems to the conventional relaying systems with\nfixed-position antennas (FPAs) and other benchmark schemes.",
        "Context. The detection of the He I 10830 A triplet in exoplanet atmospheres\nhas opened a new window for probing planetary properties, including atmospheric\nescape. Unlike Lyman alpha, the triplet is less affected by ISM absorption.\nSufficient XUV stellar irradiation may trigger the formation of the He I\ntriplet via photoionization and posterior recombination processes in the planet\natmospheres. Only a weak trend between stellar XUV and the planetary He I\nstrength has been observed so far. Aims. We aim to confirm this mechanism for\nproducing the He I absorption in exoplanetary atmospheres by examining a sample\nof planetary systems. Methods. We obtained homogeneous measurements of the\nplanetary He I line EW and consistently computed the stellar XUV ionizing\nirradiation. We first derived new coronal models for the planet-host stars. We\nused updated data from the X-exoplanets database, archival X-ray spectra of\nM-type stars (including AU Mic and Proxima Cen), and new XMM-Newton X-ray data\nobtained for the CARMENES project. These data were complemented at longer\nwavelengths with publicly available HST, FUSE, and EUVE spectra. A total of 75\nstars are carefully analyzed to obtain a new calibration between X-ray and EUV\nemission. Results. Two distinct relationships between stellar X-ray emission\n(5-100 A) and EUV_H (100-920 A) or EUV_He (100-504 A) radiation are obtained to\nscale the emission from late-type stellar coronae. A total of 48 systems with\nreported planetary He I 10830 A studies, exhibit a robust relationship between\nthe planetary He I feature and the ionizing XUV_He received by the planet,\ncorrected by stellar and planetary radii, and the planet's gravitational\npotential. Some outliers could be explained by a different atmospheric\ncomposition or the lack of planetary gaseous atmospheres. This relation may be\nused to predict the He I 10830 A absorption in exoplanet atmospheres.",
        "Pairing coherent correlation OTDR with low-complexity analysis methods, we\ninvestigate the detection of fast temperature changes and vibrations in optical\nfibers. A localization accuracy of ~2 m and extraction of vibration amplitudes\nand frequencies is demonstrated.",
        "In markets where customers tend to purchase baskets of products rather than\nsingle products, assortment optimization is a major challenge for retailers.\nRemoving a product from a retailer's assortment can result in a severe drop in\naggregate demand if this product is a complement to other products. Therefore,\naccounting for the complementarity effect is essential when making assortment\ndecisions. In this paper, we develop a modeling framework designed to address\nthis problem. We model customers' choices using a Markov random field -- in\nparticular, the Ising model -- which captures pairwise demand dependencies as\nwell as the individual attractiveness of each product. Using the Ising model\nallows us to leverage existing methodologies for various purposes including\nparameter estimation and efficient simulation of customer choices. We formulate\nthe assortment optimization problem under this model and show that its decision\nversion is NP-hard. We also provide multiple theoretical insights into the\nstructure of the optimal assortments based on the graphical representation of\nthe Ising model, and propose several heuristic algorithms that can be used to\nobtain high-quality solutions to the assortment optimization problem. Our\nnumerical analysis demonstrates that the developed simulated annealing\nprocedure leads to an expected profit gain of 15% compared to offering an\nunoptimized assortment (where all products are included) and around 5% compared\nto using a revenue-ordered heuristic algorithm.",
        "Motion planning in navigation systems is highly susceptible to upstream\nperceptual errors, particularly in human detection and tracking. To mitigate\nthis issue, the concept of guidance points--a novel directional cue within a\nreinforcement learning-based framework--is introduced. A structured method for\nidentifying guidance points is developed, consisting of obstacle boundary\nextraction, potential guidance point detection, and redundancy elimination. To\nintegrate guidance points into the navigation pipeline, a\nperception-to-planning mapping strategy is proposed, unifying guidance points\nwith other perceptual inputs and enabling the RL agent to effectively leverage\nthe complementary relationships among raw laser data, human detection and\ntracking, and guidance points. Qualitative and quantitative simulations\ndemonstrate that the proposed approach achieves the highest success rate and\nnear-optimal travel times, greatly improving both safety and efficiency.\nFurthermore, real-world experiments in dynamic corridors and lobbies validate\nthe robot's ability to confidently navigate around obstacles and robustly avoid\npedestrians.",
        "We study how inherent randomness in the training process -- where each sample\n(or client in federated learning) contributes only to a randomly selected\nportion of training -- can be leveraged for privacy amplification. This\nincludes (1) data partitioning, where a sample participates in only a subset of\ntraining iterations, and (2) model partitioning, where a sample updates only a\nsubset of the model parameters. We apply our framework to model parallelism in\nfederated learning, where each client updates a randomly selected subnetwork to\nreduce memory and computational overhead, and show that existing methods, e.g.\nmodel splitting or dropout, provide a significant privacy amplification gain\nnot captured by previous privacy analysis techniques. Additionally, we\nintroduce Balanced Iteration Subsampling, a new data partitioning method where\neach sample (or client) participates in a fixed number of training iterations.\nWe show that this method yields stronger privacy amplification than Poisson\n(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness\nin the training process, which is structured rather than i.i.d. and interacts\nwith data in complex ways, can be systematically leveraged for significant\nprivacy amplification.",
        "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.",
        "We show how to improve the discrepancy of an iid sample by moving only a few\npoints. Specifically, modifying \\( O(m) \\) sample points on average reduces the\nKolmogorov-Smirnov distance to the population distribution to \\(1\/m\\).",
        "Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.",
        "We investigate the axially symmetric accretion of low angular momentum\nhydrodynamic matter onto a rotating black hole. The gravitational field under\nconsideration is assumed to be described by a pseudo-Newtonian Kerr potential.\nThe accreting matter consists of different species defined by a relativistic\nequation of state with a variable adiabatic index.We construct and solve the\nhydrodynamical conservation equations governing such a flow, and find out the\ncorresponding stationary integral solutions. We find that depending on the\nvalues of initial boundary conditions, accretion flow may exhibit\nmulti-transonic behaviour, and a standing shock may form. We investigate, in\nminute detail, how the spin angular momentum of the black hole, as well as the\ncomposition of the accreting matter influence the dynamics of accretion flow\nand the astrophysics of shock formation in the aforementioned accreting black\nhole systems.",
        "According to recent lore, it is difficult to explain the evidence for a\nstochastic gravitational wave background obtained by pulsar timing arrays with\nsupercooled first-order phase transitions (FOPTs). We demonstrate that\nsupercooled FOPTs in dark U(1)' models with a conformal dark sector easily\nexplain the nHz signal at NANOGrav.",
        "Background: Tumour budding is an independent predictor of metastasis and\nprognosis in colorectal cancer and is a vital part of the pathology\nspecification report. In a conventional pathological section observation\nprocess, pathologists have to repeatedly switch from 10x objective to 20x\nobjective several times to localize and image the target region. Besides the\nswitching operations, repeated manual or electro-mechanical focusing is also\nvery time-consuming, affecting the total time for pathological diagnosis. In\naddition, It is usually necessary to remove the manually marked symbols on the\nstained pathology slides used for classification and management before\nobservation. Methods: In this paper, we utilize Fourier ptychographic\nmicroscopy (FPM) in the pathological diagnosis process to realize large\nspace-bandwidth product imaging, quantitative phase imaging, and digital\nrefocusing in the observation process without any mechanical operations, which\ncan therefore simplify the above-mentioned cumbersome diagnostic processes. We\nfirst verify the effectiveness and efficiency of the proposed method with\nseveral typical pathological sections. Then, instead of manually erasing, we\nalso prove that FP framework can digitally remove the artificial markers with\nits digital refocusing ability. Results: At last, we demonstrated pathologists\ncan achieve 100% diagnostic accuracy with FPM imaging results. Conclusions: The\nproposed method can greatly simplify the process of pathological diagnosis, and\nthe related addon hardware system does not require expensive components, which\nmakes it have great potential for promotion in the field of pathological\ndiagnosis.",
        "The increasing complexity and dynamic nature of 5G open radio access networks\n(O-RAN) pose significant challenges to maintaining low latency, high\nthroughput, and resource efficiency. While existing methods leverage machine\nlearning for latency prediction and resource management, they often lack\nreal-world scalability and hardware validation. This paper addresses these\nlimitations by presenting an artificial intelligence-driven latency forecasting\nsystem integrated into a functional O-RAN prototype. The system uses a\nbidirectional long short-term memory model to predict latency in real time\nwithin a scalable, open-source framework built with FlexRIC. Experimental\nresults demonstrate the model's efficacy, achieving a loss metric below 0.04,\nthus validating its applicability in dynamic 5G environments.",
        "In this study, we implement thiol termination on the surface of\nfew-nanometer-sized silicon carbide (SiC) nanoparticles (NPs) to enable further\napplications, such as fluorescent biomarkers. Various spectroscopic techniques\nare employed to monitor the effectiveness of the surface treatment.\nAdditionally, a thiol-Michael addition reaction is performed by conjugating\n4-arm PEG-maleimide molecules to the thiol groups of SiC NPs, further\ndemonstrating the reactivity of thiol-terminated SiC NPs. These thiolated SiC\nNPs, both with and without conjugated molecules, open new avenues in\nbiotechnology.",
        "The class of statistical manifolds with divisible cubic forms arises from\naffine differential geometry. We examine the geodesic connectedness of affine\nconnections on this class of statistical manifolds. In information geometry,\nthe geodesic connectedness of the affine connections are often assumed, as in\nthe generalized Pythagorean theorem. In Riemannian geometry, the geodesic\nconnectedness of the Levi-Civita connection follows from its geodesic\ncompleteness by the well-known Hopf-Rinow theorem. However, the geodesic\nconnectedness of general affine connections is more challenging to achieve,\neven for the Levi-Civita connection in pseudo-Riemannian geometry or for affine\nconnections on compact manifolds. By analogy with the Hopf-Rinow theorem in\nRiemannian geometry, we establish the geodesic connectedness of the affine\nconnections on statistical manifolds with divisible cubic forms from their\ngeodesic completeness. As an application, we establish a Cartan-Hadamard type\ntheorem for statistical manifolds.",
        "Let $k$ be a commutative Noetherian ring, and $k[S]$ the polynomial ring with\nindeterminates parameterized by elements in a set $S$. We show that $k[S]$ is\nNoetherian up to actions of permutation groups on $S$ satisfying certain\ncombinatorial conditions. Moreover, there is a special linear order on every\ninfinite $S$ such that $k[S]$ is Noetherian up to the action of the\norder-preserving permutation group, and the existence of such a linear order is\nequivalent to the Axiom of Choice. These Noetherian results are proved via a\nsheaf theoretic approach and the work of Nagel-R\\\"{o}mer.",
        "The lack of evidence for Beyond Standard Model (BSM) particles might be due\nto their light mass and very weak interactions, as exemplified by BSM\nlong-lived particles (LLPs). Such particles can be produced from $B$ or $D$\nhadron decays. Typically, the high values of pileup (PU) in hadron colliders\nare expected to pose a major challenge in light new physics searches. We\npropose a fresh perspective that counters this conventional wisdom: instead of\nviewing PU solely as an impediment, we highlight its potential benefits in\nsearches for light LLPs from $B$ or $D$ hadron decays at HL-LHC and FCC-hh. In\nparticular, certain forward detectors in LHC experiments, such as the Zero\nDegree Calorimeters (ZDC), which are currently not utilized for LLP searches,\ncan be repurposed with strategic modifications to play a crucial role in this\nendeavor. Leveraging a combination of forward and central detectors, along with\nsmart strategies for triggering and offline analysis, we demonstrate the\npotential for exploring light LLPs in high PU scenarios.",
        "We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer\nmodel, which we refer to as a metagenomic foundation model, on a novel corpus\nof diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base\npairs. This dataset is sourced from a large collection of human wastewater\nsamples, processed and sequenced using deep metagenomic (next-generation)\nsequencing methods. Unlike genomic models that focus on individual genomes or\ncurated sets of specific species, the aim of METAGENE-1 is to capture the full\ndistribution of genomic information present within this wastewater, to aid in\ntasks relevant to pandemic monitoring and pathogen detection. We carry out\nbyte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic\nsequences, and then pretrain our model. In this paper, we first detail the\npretraining dataset, tokenization strategy, and model architecture,\nhighlighting the considerations and design choices that enable the effective\nmodeling of metagenomic data. We then show results of pretraining this model on\nour metagenomic dataset, providing details about our losses, system metrics,\nand training stability over the course of pretraining. Finally, we demonstrate\nthe performance of METAGENE-1, which achieves state-of-the-art results on a set\nof genomic benchmarks and new evaluations focused on human-pathogen detection\nand genomic sequence embedding, showcasing its potential for public health\napplications in pandemic monitoring, biosurveillance, and early detection of\nemerging health threats.",
        "Classical heating of residential areas is very energy-intensive, so\nalternatives are needed, including renewable energies and advanced heating\ntechnologies. Thus, the present paper introduces a new methodology for\ncomprehensive variant analysis for future district heating planning, aiming at\noptimizing emissions and costs. For this, an extensive Modelica-based modeling\nstudy comprising models of heating center, heat grid pipelines and heating\ninterface units to buildings are coupled in co-simulations. These enable a\ncomparative analysis of the economic feasibility and sustainability for various\ntechnologies and energy carriers to be carried out. The new modular and highly\nparameterizable building model serves for validation of the introduced heat\ngrid model. The results show that bio-methane as an energy source reduces\ncarbon equivalent emissions by nearly 70% compared to conventional natural gas\nheating, and the use of hydrogen as an energy source reduces carbon equivalent\nemissions by 77% when equipped with a heat pump. In addition, the use of ground\nsource heat pumps has a high economic viability when economic benefits are\ntaken into account. The study findings highlight the importance of strategic\nplanning and flexible design in the early stages of district development in\norder to achieve improved energy efficiency and a reduced carbon footprint.",
        "Recommender systems (RSs) often suffer from the feedback loop phenomenon,\ne.g., RSs are trained on data biased by their recommendations. This leads to\nthe filter bubble effect that reinforces homogeneous content and reduces user\nsatisfaction. To this end, serendipity recommendations, which offer unexpected\nyet relevant items, are proposed. Recently, large language models (LLMs) have\nshown potential in serendipity prediction due to their extensive world\nknowledge and reasoning capabilities. However, they still face challenges in\naligning serendipity judgments with human assessments, handling long user\nbehavior sequences, and meeting the latency requirements of industrial RSs. To\naddress these issues, we propose SERAL (Serendipity Recommendations with\nAligned Large Language Models), a framework comprising three stages: (1)\nCognition Profile Generation to compress user behavior into multi-level\nprofiles; (2) SerenGPT Alignment to align serendipity judgments with human\npreferences using enriched training data; and (3) Nearline Adaptation to\nintegrate SerenGPT into industrial RSs pipelines efficiently. Online\nexperiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and\ntransactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user\nexperience without much impact on overall revenue. Now, it has been fully\ndeployed in the \"Guess What You Like\" of the Taobao App homepage.",
        "This paper presents new developments in the study of 3d mirror symmetry and\nthe phase structure of Abelian gauge theories. Previous works identified 3d\nmirrors for a specific class of theories, termed ``simple\" Abelian theories.\nThis work extends this framework by proposing 3d mirrors for ``non-simple\"\nAbelian theories with both discrete and continuous gauge group factors. The\nproposal is supported by evidence from an exact operator map between the\nHiggs\/Coulomb branch of one theory and the Coulomb\/Higgs branch of its 3d\nmirror. Further support is provided by explicit Hilbert series computations. An\nalgorithm for computing the Hasse (phase) diagram of the Higgs branch of both\nsimple and non-simple Abelian theories is introduced, uncovering a recently\ndiscovered family of isolated singularities among the elementary slices. A\nbottom-up algorithm for computing the Coulomb branch Hasse diagram of these\ntheories is also introduced, and the two algorithms are tested against each\nother via 3d mirror symmetry."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)"
      ],
      "abstract":[
        "Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Modular Forms and Certain ${}_2F_1(1)$ Hypergeometric Series",
        "Signal amplification in a solid-state quantum sensor via asymmetric\n  time-reversal of many-body dynamics",
        "Flavor dependence of Energy-energy correlators",
        "Integral Ricci Curvature for Graphs",
        "Optimal Low degree hardness for Broadcasting on Trees",
        "Geodesics for Discrete manifolds",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Unique continuation for locally uniformly distributed measures",
        "Isospin sum rules for bottom-baryon weak decays",
        "Sampling Binary Data by Denoising through Score Functions",
        "Consonance in music -- the Pythagorean approach revisited",
        "Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase\n  retrieval",
        "The Andreadakis Problem for the McCool groups",
        "Intrinsic superconducting diode effect and nonreciprocal\n  superconductivity in rhombohedral graphene multilayers",
        "On cyclotomic nearly-doubly-regular tournaments",
        "Anatomy of Spin Wave Polarization in Ferromagnets",
        "Periodic elements in finite type Artin-Tits groups and stability\n  conditions",
        "Impact of phonon lifetimes on the single-photon indistinguishability in\n  quantum emitters based on 2D materials",
        "Optimization of the Woodcock Particle Tracking Method Using Neural\n  Network",
        "Machine learning algorithms to predict stroke in China based on causal\n  inference of time series analysis",
        "Biases in Edge Language Models: Detection, Analysis, and Mitigation",
        "Pressure suppresses the density wave order in kagome metal\n  LuNb$_6$Sn$_6$",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "Slow magnetic quantum oscillations in the c-axis magnetoresistance of\n  UTe$_2$",
        "Fractons from covariant higher-rank 3D BF theory",
        "A Perspective on Symbolic Machine Learning in Physical Sciences",
        "Simultaneously decoding the unknown stationary state and function\n  parameters for mean field games",
        "Central series' and ($n$)-isoclinism of skew left braces",
        "Analog of the Carnot engine for fluctuating diffusivity in living cells"
      ],
      "abstract":[
        "Using the framework relating hypergeometric motives to modular forms, we\ndefine an explicit family of weight 2 Hecke eigenforms with complex\nmultiplication. We use the theory of ${}_2F_1(1)$ hypergeometric series and\nRamanujan's theory of alternative bases to compute the exact central $L$-value\nof these Hecke eigenforms in terms of special beta values. We also show the\nintegral Fourier coefficients can be written in terms of Jacobi sums,\nreflecting a motivic relation between the hypergeometric series and the modular\nforms.",
        "Electronic spins of nitrogen vacancy (NV) centers in diamond constitute a\npromising system for micro- and nano-scale magnetic sensing, due to their\noperation under ambient conditions, ease of placement in close proximity to\nsensing targets, and biological compatibility. At high densities, the\nelectronic spins interact through dipolar coupling, which typically limits but\ncan also potentially enhance sensing performance. Here we report the\nexperimental demonstration of many-body signal amplification in a solid-state,\nroom temperature quantum sensor. Our approach utilizes time-reversed\ntwo-axis-twisting interactions, engineered through dynamical control of the\nquantization axis and Floquet engineering in a two-dimensional ensemble of NV\ncenters. Strikingly, we observe that the optimal amplification occurs when the\nbackward evolution time equals twice the forward evolution time, in sharp\ncontrast to the conventional Loschmidt echo. These observations can be\nunderstood as resulting from an underlying time-reversed mirror symmetry of the\nmicroscopic dynamics, providing key insights into signal amplification and\nopening the door towards entanglement-enhanced practical quantum sensing.",
        "Energy-energy correlators (EECs) within high energy jets serve as a key\nexperimentally accessible quantity to probe the scale and structure of the\nquark-gluon plasma (QGP) in relativistic heavy-ion collisions. The CMS\nCollaboration's first measurement of the modification to the EEC within single\ninclusive jets in Pb+Pb collisions relative to p+p collisions reveals a\nsignificant enhancement at small angles, which may arise from jet transverse\nmomentum $p_T$ selection biases due to jet energy loss. We investigate the\ndependence of jet EECs on the flavor of the initiating parton. The EEC\ndistribution of a gluon jet is broader and the peak of transition from\nperturbative to non-perturbative regime occurs at a larger angle than a quark\njet. Such flavor dependence leads to the different EECs for $\\gamma$-jets and\nsingle inclusive jets due to their different flavor composition. It is also\nresponsible for a colliding energy dependence of EECs of single inclusive jets\nat fixed jet energy. We also investigate the impact of flavor composition\nvariation on the $p_T$ dependence of the jet EEC. We further propose that a\nchange in the gluon jet fraction in A+A collisions compared to p+p can also\ncontribute to a non-negligible enhancement of the medium modified EEC at small\nangles. Using the \\textsc{Jewel} model, we predict the reduction of the gluon\njet fraction in A+A collisions and estimate its impact on the EEC.",
        "We introduce the notion of integral Ricci curvature $I_{\\kappa_0}$ for\ngraphs, which measures the amount of Ricci curvature below a given threshold\n$\\kappa_0$. We focus our attention on the Lin-Lu-Yau Ricci curvature. As\napplications, we prove a Bonnet-Myers-type diameter estimate, a Moore-type\nestimate on the number of vertices of a graph in terms of the maximum degree\n$d_M$ and diameter $D$, and a Lichnerowicz-type estimate for the first\neigenvalue $\\lambda_1$ of the Graph Laplacian, generalizing the results\nobtained by Lin, Lu, and Yau. All estimates are uniform, depending only on\ngeometric parameters like $\\kappa_0$, $I_{\\kappa_0}$, $d_M$, or $D$, and do not\nrequire the graphs to be positively curved.",
        "Broadcasting on trees is a fundamental model from statistical physics that\nplays an important role in information theory, noisy computation and\nphylogenetic reconstruction within computational biology and linguistics. While\nthis model permits efficient linear-time algorithms for the inference of the\nroot from the leaves, recent work suggests that non-trivial computational\ncomplexity may be required for inference.\n  The inference of the root state can be performed using the celebrated Belief\nPropagation (BP) algorithm, which achieves Bayes-optimal performance. Although\nBP runs in linear time using real arithmetic operations, recent research\nindicates that it requires non-trivial computational complexity using more\nrefined complexity measures.\n  Moitra, Mossel, and Sandon demonstrated such complexity by constructing a\nMarkov chain for which estimating the root better than random guessing (for\ntypical inputs) is $NC^1$-complete. Kohler and Mossel constructed chains where,\nfor trees with $N$ leaves, achieving better-than-random root recovery requires\npolynomials of degree $N^{\\Omega(1)}$. The papers above raised the question of\nwhether such complexity bounds hold generally below the celebrated\nKesten-Stigum bound.\n  In a recent work, Huang and Mossel established a general degree lower bound\nof $\\Omega(\\log N)$ below the Kesten-Stigum bound. Specifically, they proved\nthat any function expressed as a linear combination of functions of at most\n$O(log N)$ leaves has vanishing correlation with the root. In this work, we get\nan exponential improvement of this lower bound by establishing an\n$N^{\\Omega(1)}$ degree lower bound, for any broadcast process in the whole\nregime below the Kesten-Stigum bound.",
        "The geodesic flow on a finite discrete q-manifold with or without boundary is\ndefined as as a permutation of its ordered q-simplices. This allows to define\ngeodesic sheets and a notion of sectional curvature.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "In this note we show that the support of a locally $k$-uniform measure in\n$\\mathbb R^{n+1}$ satisfies a kind of unique continuation property. As a\nconsequence, we show that locally uniformly distributed measures satisfy a\nweaker unique continuation property. This continues work of Kirchheim and\nPreiss (Math. Scand. 2002) and David, Kenig and Toro (Comm. Pure Appl. Math.\n2001) and lends additional evidence to the conjecture proposed by Kowalski and\nPreiss (J. Reine Angew. Math. 1987) that each connected component of the\nsupport of a locally $n$-uniform measure in $\\mathbb R^{n+1}$ is contained in\nthe zero set of a quadratic polynomial.",
        "Isospin symmetry, as the most precise flavor symmetry, can be used to extract\ninformation about hadronic dynamics. The effective Hamiltonian operators of\nbottom quark weak decays are zero under a series of isospin lowering operators\n$I_-^n$, which permits us to generate isospin sum rules without the\nWigner-Eckhart invariants. In this work, we derive hundreds of isospin sum\nrules for the two- and three-body non-leptonic decays of bottom baryons. They\nprovide hints for new decay modes and the isospin partners of pentaquark\nstates.",
        "Gaussian smoothing combined with a probabilistic framework for denoising via\nthe empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are\nthe two key ingredients in the success of score-based generative models in\nEuclidean spaces. Smoothing holds the key for easing the problem of learning\nand sampling in high dimensions, denoising is needed for recovering the\noriginal signal, and TMF ties these together via the score function of noisy\ndata. In this work, we extend this paradigm to the problem of learning and\nsampling the distribution of binary data on the Boolean hypercube by adopting\nBernoulli noise, instead of Gaussian noise, as a smoothing device. We first\nderive a TMF-like expression for the optimal denoiser for the Hamming loss,\nwhere a score function naturally appears. Sampling noisy binary data is then\nachieved using a Langevin-like sampler which we theoretically analyze for\ndifferent noise levels. At high Bernoulli noise levels sampling becomes easy,\nakin to log-concave sampling in Euclidean spaces. In addition, we extend the\nsequential multi-measurement sampling of Saremi et al. (2024) to the binary\nsetting where we can bring the \"effective noise\" down by sampling multiple\nnoisy measurements at a fixed noise level, without the need for continuous-time\nstochastic processes. We validate our formalism and theoretical findings by\nexperiments on synthetic data and binarized images.",
        "The Pythagorean school attributed consonance in music to simplicity of\nfrequency ratios between musical tones. In the last two centuries, the\nconsonance curves developed by Helmholtz, Plompt and Levelt shifted focus to\npsycho-acoustic considerations in perceiving consonances. The appearance of\npeaks of these curves at the ratios considered by the Pythagorean school, and\nwhich were a consequence of an attempt to understand the world by nice\nmathematical proportions, remained a curiosity. This paper addresses this\ncuriosity, by describing a mathematical model of musical sound, along with a\nmathematical definition of consonance. First, we define pure, complex and mixed\ntones as mathematical models of musical sound. By a sequence of numerical\nexperiments and analytic calculations, we show that continuous cosine\nsimilarity, abbreviated as cosim, applied to these models quantifies the\nelusive concept of consonance as a frequency ratio which gives a local maximum\nof the cosim function. We prove that these maxima occur at the ratios\nconsidered as consonant in classical music theory. Moreover, we provide a\nsimple explanation why the number of musical intervals considered as consonant\nby musicians is finite, but has been increasing over the centuries.\nSpecifically, our formulas show that the number of consonant intervals changes\nwith the depth of the tone (the number of harmonics present).",
        "The injectivity of ReLU layers in neural networks, the recovery of vectors\nfrom clipped or saturated measurements, and (real) phase retrieval in\n$\\mathbb{R}^n$ allow for a similar problem formulation and characterization\nusing frame theory. In this paper, we revisit all three problems with a unified\nperspective and derive lower Lipschitz bounds for ReLU layers and clipping\nwhich are analogous to the previously known result for phase retrieval and are\noptimal up to a constant factor.",
        "In this short paper, we show that the McCool group does not satisfy the\nAndreadakis equality from degree $7$, and we give a lower bound for the size of\nthe difference between the two relevant filtrations. As a consequence, we see\nthat the Andreadakis problem for the McCool group does not stabilize.",
        "Rhombohedral tetralayer graphene has recently emerged as an exciting platform\nfor a possible chiral superconducting state. Here, we theoretically demonstrate\nand study the emergence of nonreciprocal superconductivity and an intrinsic\nsuperconducting diode effect in this system. Our results are based on a fully\nself-consistent framework for determining the superconducting order parameter\nfrom a Kohn-Luttinger mechanism to superconductivity and show that large diode\nefficiencies, $\\sim$ 60%, are achievable and highly tunable by an external\ndisplacement field. Moreover, we also find that the diodicity shows a\ncharacteristic angular dependence with multiple enhanced lobes, which depend on\nthe Fermi surface structure of the underlying normal state. Hence, our results\nsuggest that the intrinsic superconducting diode effect could provide insights\ninto the type of Fermi surface topology from which superconductivity arises.",
        "Nearly-doubly-regular tournaments have played significant roles in extremal\ngraph theory. In this note, we construct new cyclotomic nearly-doubly-regular\ntournaments and determine their spectrum by establishing a new connection\nbetween cyclotomic nearly-doubly-regular tournaments and almost difference sets\nfrom combinatorial design theory. Furthermore, under the celebrated\nHardy-Littlewood conjecture F in analytic number theory, our results confirm\nthe conjecture due to Sergey Savchenko (J. Graph Theory {\\bf 83} (2016),\n44--77) on the existence of infinitely many nearly-doubly-regular tournaments\nwith the canonical spectrum.",
        "Spin waves in ferromagnetic materials are predominantly characterized by\nright-handed circular polarization due to symmetry breaking induced by net\nmagnetization. However, magnetic interactions, including the external magnetic\nfield, Heisenberg exchange, Dzyaloshinskii-Moriya interaction, and\ndipole-dipole interaction, can modify this behavior, leading to elliptical\npolarization. This study provides a systematic analysis of these interactions\nand their influence on spin wave polarization, establishing principles to\npredict traits such as polarization degree and orientation based on equilibrium\nmagnetization textures. The framework is applied to diverse magnetic\nconfigurations, including spin spirals, domain walls, and Skyrmions, offering a\ncomprehensive yet simple approach to understanding polarization dynamics in\nferromagnetic systems.",
        "Periodic elements in finite type Artin--Tits groups are elements some\npositive power of which is central. We give a dynamical characterisation of\nperiodic elements via their action on the corresponding 2-Calabi--Yau category\nand on its space of (fusion equivariant) Bridgeland stability conditions. The\nmain theorem is that an element $\\beta$ is periodic if and only if $\\beta$ has\na fixed point in the stability manifold.",
        "Localized excitons in two-dimensional (2D) materials are considered as\npromising sources of single photons on demand. The photon indistinguishability\nas key figure of merit for quantum information processing is strongly\ninfluenced by the coupling of charge excitations to lattice vibrations of the\nsurrounding semiconductor material. Here, we quantify the impact of\nexciton-acoustic-phonon-interaction and cavity QED effects on photon\nindistinguishability in a Hong-Ou-Mandel setup by solving fully quantum\nmechanical equations for the coupled QD-cavity-phonon system including\nnon-Markovian effects. We find a strong reduction of indistinguishability\ncompared to 3D systems due to increased exciton-phonon coupling efficiency.\nMoreover, we show that the coherence properties of photons are significantly\ninfluenced by the finite phonon lifetime in the surrounding material giving\nrise to pure dephasing. Only if these limitations are overcome, localized\nexcitons in 2D semiconductors can become a new avenue for quantum light\nsources.",
        "The acceptance rate in Woodcock tracking algorithm is generalized to an\narbitrary position-dependent variable $q(x)$. A neural network is used to\noptimize $q(x)$, and the FOM value is used as the loss function. This idea\ncomes from physics informed neural network(PINN), where a neural network is\nused to represent the solution of differential equations. Here the neural\nnetwork $q(x)$ should solve the functional equations that optimize FOM. For a\n1d transmission problem with Gaussian absorption cross section, we observe a\nsignificant improvement of the FOM value compared to the constant $q$ case and\nthe original Woodcock method. Generalizations of the neural network\nWoodcock(NNW) method to 3d voxel models are waiting to be explored.",
        "Participants: This study employed a combination of Vector Autoregression\n(VAR) model and Graph Neural Networks (GNN) to systematically construct dynamic\ncausal inference. Multiple classic classification algorithms were compared,\nincluding Random Forest, Logistic Regression, XGBoost, Support Vector Machine\n(SVM), K-Nearest Neighbor (KNN), Gradient Boosting, and Multi Layer Perceptron\n(MLP). The SMOTE algorithm was used to undersample a small number of samples\nand employed Stratified K-fold Cross Validation. Results: This study included a\ntotal of 11,789 participants, including 6,334 females (53.73%) and 5,455 males\n(46.27%), with an average age of 65 years. Introduction of dynamic causal\ninference features has significantly improved the performance of almost all\nmodels. The area under the ROC curve of each model ranged from 0.78 to 0.83,\nindicating significant difference (P < 0.01). Among all the models, the\nGradient Boosting model demonstrated the highest performance and stability.\nModel explanation and feature importance analysis generated model\ninterpretation that illustrated significant contributors associated with risks\nof stroke. Conclusions and Relevance: This study proposes a stroke risk\nprediction method that combines dynamic causal inference with machine learning\nmodels, significantly improving prediction accuracy and revealing key health\nfactors that affect stroke. The research results indicate that dynamic causal\ninference features have important value in predicting stroke risk, especially\nin capturing the impact of changes in health status over time on stroke risk.\nBy further optimizing the model and introducing more variables, this study\nprovides theoretical basis and practical guidance for future stroke prevention\nand intervention strategies.",
        "The integration of large language models (LLMs) on low-power edge devices\nsuch as Raspberry Pi, known as edge language models (ELMs), has introduced\nopportunities for more personalized, secure, and low-latency language\nintelligence that is accessible to all. However, the resource constraints\ninherent in edge devices and the lack of robust ethical safeguards in language\nmodels raise significant concerns about fairness, accountability, and\ntransparency in model output generation. This paper conducts a comparative\nanalysis of text-based bias across language model deployments on edge, cloud,\nand desktop environments, aiming to evaluate how deployment settings influence\nmodel fairness. Specifically, we examined an optimized Llama-2 model running on\na Raspberry Pi 4; GPT 4o-mini, Gemini-1.5-flash, and Grok-beta models running\non cloud servers; and Gemma2 and Mistral models running on a MacOS desktop\nmachine. Our results demonstrate that Llama-2 running on Raspberry Pi 4 is\n43.23% and 21.89% more prone to showing bias over time compared to models\nrunning on the desktop and cloud-based environments. We also propose the\nimplementation of a feedback loop, a mechanism that iteratively adjusts model\nbehavior based on previous outputs, where predefined constraint weights are\napplied layer-by-layer during inference, allowing the model to correct bias\npatterns, resulting in 79.28% reduction in model bias.",
        "Dancing tins pair up,\n  But compressing the framework\n  Thwarts the displacements.\n  The density waves that develop in kagome metals ScV$_6$Sn$_6$ and\nLuNb$_6$Sn$_6$ at low temperature appear to arise from under-filled atomic\ncolumns within a V-Sn or Nb-Sn scaffolding. Compressing this network with\napplied pressure in ScV$_6$Sn$_6$ suppressed the structural transition\ntemperature by constraining atomic rattling and inhibiting the shifts that\ndefine the structural modulation. We predicted that the density wave transition\nin LuNb$_6$Sn$_6$ at 68 K would be suppressed by pressure as well. In this\nbrief study we examine the pressure dependence of the density wave transition\nby remeasuring resistance vs temperature up to 2.26 GPa. We found the\ntransition temperature is smoothly depressed and disappears around 1.9 GPa.\nThis result not only addresses our prediction, but strengthens the rattling\nchains origin of structural instabilities in the HfFe$_6$Ge$_6$-type kagome\nmetals.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "Details of the electronic band structure in unconventional superconductors\nare key to the understanding of their fundamental ground state. The potential\nspin-triplet superconductor UTe$_2$, with $T_\\mathrm{c}\\approx 2.1\\,$K, has\nattracted attention recently. Its main Fermi surface consists of weakly\ncorrugated, two-dimensional Fermi-surface cylinders that run along the\ncrystallographic $c$ axis. In addition, there is evidence for the presence of\nan additional small three-dimensional band. This has been discussed\ncontroversially as it may be essential for the realization of superconductivity\nin UTe$_2$. Here, we investigate the angle-resolved magnetoresistance and Hall\neffect in bulk crystalline samples with current along the $c$ axis in fields up\nto $60\\,$T. We observe low-frequency magnetic quantum oscillations with light\neffective masses that are most pronounced for magnetic field applied along the\n$a$ axis. Two distinct frequencies indicate two separate changes in the\nFermi-surface topology, likely connected with Lifshitz transitions. We discuss\nthe origin of these oscillations in terms of magnetic breakdown, quantum\ninterference, and other potential mechanisms.",
        "In this paper we study the 3D gauge theory of two tensor gauge fields:\n$a_{\\mu\\nu}(x)$, which we take symmetric, and $B_{\\mu\\nu}(x)$, with no symmetry\non its indices. The corresponding invariant action is a higher-rank BF-like\nmodel, which is first considered from a purely field theoretical point of view,\nand the propagators with their poles and the degrees of freedom are studied.\nOnce matter is introduced, a fracton behaviour naturally emerges. We show that\nour theory can be mapped to the low-energy effective field theory describing\nthe Rank-2 Toric Code (R2TC). This relation between our covariant BF-like\ntheory and the R2TC is a higher-rank generalization of the equivalence between\nthe ordinary 3D BF theory and the Kitaev's Toric Code. In the last part of the\npaper we analyze the case in which the field $B_{\\mu\\nu}(x)$ is a symmetric\ntensor. It turns out that the obtained BF-like action can be cast into the sum\nof two rank-2 Chern-Simons actions, thus generalizing the ordinary abelian\ncase. Therefore, this represents a higher-rank generalization of the ordinary\n3D BF theory, which well describes the low-energy physics of quantum spin Hall\ninsulators in two spatial dimensions.",
        "Machine learning is rapidly making its pathway across all of the natural\nsciences, including physical sciences. The rate at which ML is impacting\nnon-scientific disciplines is incomparable to that in the physical sciences.\nThis is partly due to the uninterpretable nature of deep neural networks.\nSymbolic machine learning stands as an equal and complementary partner to\nnumerical machine learning in speeding up scientific discovery in physics. This\nperspective discusses the main differences between the ML and scientific\napproaches. It stresses the need to develop and apply symbolic machine learning\nto physics problems equally, in parallel to numerical machine learning, because\nof the dual nature of physics research.",
        "Mean field games (MFGs) offer a versatile framework for modeling large-scale\ninteractive systems across multiple domains. This paper builds upon a previous\nwork, by developing a state-of-the-art unified approach to decode or design the\nunknown stationary state of MFGs, in addition to the underlying parameter\nfunctions governing their behavior. This result is novel, even in the general\nrealm of inverse problems for nonlinear PDEs. By enabling agents to distill\ncrucial insights from observed data and unveil intricate hidden structures and\nunknown states within MFG systems, our approach surmounts a significant\nobstacle, enhancing the applicability of MFGs in real-world scenarios. This\nadvancement not only enriches our understanding of MFG dynamics but also\nbroadens the scope for their practical deployment in various contexts.",
        "The aim of this article is to advance the knowledge on the theory of skew\nleft braces. We introduce a sub class of skew left braces, which we denote by\n$\\mathcal{I}_n$, $n \\ge 1$, such that elements of the annihilator and lower\ncentral series' interact 'nicely' with respect to commutation. That allows us\nto define a concept of $n$-isoclinism of skew left braces in $\\mathcal{I}_n$,\nby using a concept of brace commutator words, which we have introduced. We\nprove various results on $1$-isoclinism (isoclinism) of skew left braces\nanalogous to results in group theory. For any two symmetric $n$-isoclinic skew\nleft braces $A$ and $B$, we prove that, there exist skew left braces $C$ and\n$R$ such that both $A$ and $B$ are $n$-isoclinic to both $C$ and $R$ and (i)\n$A$ and $B$ are quotient skew left braces of $C$; (ii) $A$ and $B$ are sub skew\nleft braces of $R$. Connections between a skew left brace and the group which\noccurs as a natural semi-direct product of additive and multiplicative groups\nof the skew left brace are investigated, and it is proved that $n$-isoclinism\nis preserved from braces to groups. We also show that various nilpotency\nconcepts on skew left braces are invariant under $n$-isoclinism.",
        "Recently, a formal analogy between the fluctuating diffusivity and\nthermodynamics has been proposed based on phenomena of heterogeneous diffusion\nobserved in living cells. This not only offers the analogs of the quantity of\nheat and work as well as the internal energy but also achieves that of the\nClausius inequality for the entropy concerning diffusivity fluctuations. Here,\na discussion is developed about constructing a heat-like engine in terms of the\nfluctuating diffusivity. The engine constitutes two kinds of processes with the\naverage diffusivity or the average local temperature being kept fixed, along\nwhich the fluctuation distribution obeys an exponential law. The efficiency of\nthe engine in a cycle, which quantifies how much the diffusivity change as the\nanalog of work can be extracted, is found to formally coincide with that of\nCarnot's. During the cycle, the total change of the entropy is also shown to\nvanish."
      ]
    }
  }
]