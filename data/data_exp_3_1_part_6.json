[
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b23",
    "start_title":"DARWIN Series: Domain Specific Large Language Models for Natural Science",
    "start_abstract":"Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b29"
      ],
      "title":[
        "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
      ],
      "abstract":[
        "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Non-Affine Extensions of the Raychaudhuri Equation in the K-essence\n  Framework",
        "Time-series attribution maps with regularized contrastive learning",
        "Ion-kinetic-energy sampling in a 22-pole trap using ring-electrode\n  evaporation",
        "Improved Quantum Computation using Operator Backpropagation",
        "Reservoir Computing and Photoelectrochemical Sensors: A Marriage of\n  Convenience",
        "On the localization length of finite-volume random block Schr\\\"odinger\n  operators",
        "Theory of neutrino slow flavor evolution. Part II. Space-time evolution\n  of linear instabilities",
        "Thermodynamic analysis and shadow bound of black holes surrounded by a\n  dark matter halo",
        "Canonical forms of oriented matroids",
        "Beyond surveys: A High-Precision Wealth Inequality Mapping of China's\n  Rural Households Derived from Satellite and Street View Imageries",
        "More on the corner-vector construction for spherical designs",
        "Enabling GPU Portability into the Numba-JITed Monte Carlo Particle\n  Transport Code MC\/DC",
        "BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid\n  Dynamics",
        "A new practical and effective source-independent full-waveform inversion\n  with a velocity-distribution supported deep image prior: Applications to two\n  real datasets",
        "Particle-based plasma simulation using a graph neural network",
        "A Statistical Interpretation of Multi-Item Rating and Recommendation\n  Problems",
        "Conjecture on Supersequence Lower Bound related to Connell Sequence",
        "A spectral clustering-type algorithm for the consistent estimation of\n  the Hurst distribution in moderately high dimensions",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case",
        "Complete Hamiltonian Framework of Relativistic Hierarchical Triple\n  Systems: Capabilities and Limitations of Secular Perturbation Theory",
        "Collapsing of K3 Surfaces and Special K\\\"ahler Structures",
        "Wavelength-Dependent Photodissociation of Iodomethylbutane",
        "Finding Quasars Behind the Galactic Plane: Spectroscopic Identifications\n  of ~1300 New Quasars at |b|<=20 degree from LAMOST DR10",
        "First-Ever Deployment of a SiPM-on-Tile Calorimeter in a Collider: A\n  Parasitic Test with 200 GeV $pp$ Collisions at RHIC",
        "Time-Resolved Measurements of Cumulative Effects in Gas Dynamics Induced\n  by High-Repetition-Rate Femtosecond Laser Filamentation",
        "Pseudo-Goldstone Dark Matter from Primordial Black Holes: Gravitational\n  Wave Signatures and Implications for KM3-230213A Event at KM3NeT",
        "Quantum Schrodinger bridges: large deviations and time-symmetric\n  ensembles",
        "Dissecting stellar populations with manifold learning I. Validation of\n  the method on a synthetic Milky Way-like galaxy",
        "Late-time cosmic acceleration from quantum gravity"
      ],
      "abstract":[
        "The Raychaudhuri equation (RE) governs the evolution of geodesic congruences\nin curved spacetime. Here, we extend the classical RE by incorporating\nnon-affine parametrization within the framework of k-essence scalar field\ndynamics. The non-affine parametrization introduces deviations from purely\ngeodesic congruences (motion), allowing investigation of non-gravitational\ninteractions and external forces. Using a DBI-type k-essence Lagrangian, we\nanalyze the behavior of non-geodesic flow curves in the background FLRW metric,\nelucidating their role in cosmic acceleration and structure formation. The\nemergent metric formalism is used to derive a modified RE, revealing new\ngeometric and dynamical features induced by the k-essence field. The\ncosmological implications of our model are studied by constraining key\nparameters using observational data from the PANTHEON+SHOES+BAO and Hubble\ndatasets. Our results suggest that non-affine parametrization, coupled with\nk-essence dynamics, can provide a viable explanation for late-time cosmic\nacceleration while addressing the Hubble tension. Further, we reinterpret the\nmodified RE as an anti-damped harmonic oscillator, revealing quantum-like\neffects in cosmic expansion. These results suggest a deep connection between\nscalar field dynamics and modified gravity, offering new perspectives on the\nnature of the universe's expansion history.",
        "Gradient-based attribution methods aim to explain decisions of deep learning\nmodels but so far lack identifiability guarantees. Here, we propose a method to\ngenerate attribution maps with identifiability guarantees by developing a\nregularized contrastive learning algorithm trained on time-series data plus a\nnew attribution method called Inverted Neuron Gradient (collectively named\nxCEBRA). We show theoretically that xCEBRA has favorable properties for\nidentifying the Jacobian matrix of the data generating process. Empirically, we\ndemonstrate robust approximation of zero vs. non-zero entries in the\nground-truth attribution map on synthetic datasets, and significant\nimprovements across previous attribution methods based on feature ablation,\nShapley values, and other gradient-based methods. Our work constitutes a first\nexample of identifiable inference of time-series attribution maps and opens\navenues to a better understanding of time-series data, such as for neural\ndynamics and decision-processes within neural networks.",
        "We present an experimental method for the characterization of the kinetic\nenergies of ions confined in a 22-pole radio frequency trap by inducing a small\npotential barrier using the surrounding ring electrodes, allowing the selective\nextraction of ions. Energy sampling experiments have been performed on buffer\ngas thermalized He$^+$ ions at trap temperatures between 10-180 K, resulting in\ndistinct extraction curves as a function of the potential barrier, and a\ndifferentiated behavior depending on the escape time from the trap. The\nexperiments are complemented by Monte Carlo simulations of the ion trajectories\ninside the calculated trap potential and allow us to investigate the properties\nof the sampling method, the role of ion motion coupling, and the impact of\nresidual buffer gas collisions on the observed results. The technique has also\nbeen successfully applied to identify energetic H$_3^+$ ions produced in an\nexothermic reaction inside the trap. Upon calibration, this method can provide\nrelative kinetic energy distributions or be used to filter the maximum desired\nkinetic energy of the ions inside the trap.",
        "Decoherence of quantum hardware is currently limiting its practical\napplications. At the same time, classical algorithms for simulating quantum\ncircuits have progressed substantially. Here, we demonstrate a hybrid framework\nthat integrates classical simulations with quantum hardware to improve the\ncomputation of an observable's expectation value by reducing the quantum\ncircuit depth. In this framework, a quantum circuit is partitioned into two\nsubcircuits: one that describes the backpropagated Heisenberg evolution of an\nobservable, executed on a classical computer, while the other is a\nSchr\\\"odinger evolution run on quantum processors. The overall effect is to\nreduce the depths of the circuits executed on quantum devices, trading this\nwith classical overhead and an increased number of circuit executions. We\ndemonstrate the effectiveness of this method on a Hamiltonian simulation\nproblem, achieving more accurate expectation value estimates compared to using\nquantum hardware alone.",
        "Sensing technology is an important aspect of information processing. Current\ndevelopment in artificial intelligence systems (especially those aimed at\nmedical and environmental applications) requires a lot of data on the chemical\ncomposition of biological fluids or environmental samples. These complex\nmatrices require advanced sensing devices, and photoelectrochemical ones seem\nto have potential to overcome at least some of the obstacles. Furthermore, the\ndevelopment of artificial intelligence (AI) technology for autonomous robotics\nrequires technology mimicking human senses, also those operating at the\nmolecular level, such as gustation and olfaction. Again, photoelectrochemical\nsensing can provide some suitable solutions. In this review, we introduce the\nidea of integration of photoelectrochemical sensors with some unconventional\ncomputing paradigm - reservoir computing. This approach should not only boost\nthe performance of the sensors itself, but also open new pathways through\nscience. Integration of sensing devices with computing systems will also\ncontribute to a better understanding (or at least mimicking) of the human\nsenses and neuromorphic sensory information processing. Although reservoir\nsystems can be considered magic \"black boxes\" and their operation is at the\nsame time simple and hard to comprehend, this combination is expected to open a\nnew era of effective information harvesting and processing systems.",
        "We study a general class of random block Schr\\\"odinger operators (RBSOs) in\ndimensions 1 and 2, which naturally extend the Anderson model by replacing the\nrandom potential with a random block potential. Specifically, we focus on two\nRBSOs -- the block Anderson and Wegner orbital models -- defined on the\n$d$-dimensional torus $(\\mathbb Z\/L\\mathbb Z)^d$. They take the form $H=V +\n\\lambda \\Psi$, where $V$ is a block potential with i.i.d. $W^d\\times W^d$\nGaussian diagonal blocks, $\\Psi$ describes interactions between neighboring\nblocks, and $\\lambda>0$ is a coupling parameter. We normalize the blocks of\n$\\Psi$ so that each block has a Hilbert-Schmidt norm of the same order as the\nblocks of $V$. Assuming $W\\ge L^\\delta$ for a small constant $\\delta>0$ and\n$\\lambda\\gg W^{-d\/2}$, we establish the following results. In dimension $d=2$,\nwe prove delocalization and quantum unique ergodicity for bulk eigenvectors,\nwhich, combined with the localization result from arXiv:1608.02922 under the\ncondition $\\lambda\\ll W^{-d\/2}$, rigorously establishes the Anderson\nlocalization-delocalization transition as $\\lambda$ crosses the critical\nthreshold $W^{-d\/2}$. In dimension $d=1$, we show that the localization length\nof bulk eigenvectors is at least of order $(W\\lambda)^2$, which is conjectured\nto be the correct scaling for the localization length.",
        "Slow flavor evolution (defined as driven by neutrino masses and not\nnecessarily ``slow'') is receiving fresh attention in the context of compact\nastrophysical environments. In Part~I of this series, we have studied the\nslow-mode dispersion relation following our recently developed analogy to\nplasma waves. The concept of resonance between flavor waves in the linear\nregime and propagating neutrinos is the defining feature of this approach. It\nis best motivated for weak instabilities, which probably is the most relevant\nregime in self-consistent astrophysical environments because these will try to\neliminate the cause of instability. We here go beyond the dispersion relation\nalone (which by definition applies to infinite media) and consider the group\nvelocities of unstable modes that determines whether the instability relaxes\nwithin the region where it first appears (absolute), or away from it\n(convective). We show that all weak instabilities are convective so that their\nfurther evolution is not local. Therefore, studying their consequences\nnumerically in small boxes from given initial conditions may not always be\nappropriate.",
        "We perform the thermodynamic analysis of a black hole (BH) immersed in a dark\nmatter halo (DMH), showing that the BH could not be in thermal equilibrium with\nthe DMH in any regions outside the event horizon. This means that the\nthermodynamic influence of the environment (DMH) is relatively small on the BH\nand it does not alter the nature of BH with negative heat capacity. The\nNewtonian ($1\/a_0$) approximation gives us a correct thermodynamic description\nfor the BH surrounded by DMH because the first law of thermodynamics and Smarr\nformula are satisfied. We use the Newtonian Helmholtz free energy to show that\nthere is no phase transition to other BH with positive heat capacity surrounded\nby a DMH. We investigate the shadow bound of favored region for the BH immersed\nin the DMH by introducing three critical impact parameters.",
        "Positive geometries are semialgebraic sets equipped with a canonical\ndifferential form whose residues mirror the boundary structure of the geometry.\nEvery full-dimensional projective polytope is a positive geometry. Motivated by\nthe canonical forms of polytopes, we construct a canonical form for any tope of\nan oriented matroid, inside the Orlik--Solomon algebra of the underlying\nmatroid. Using these canonical forms, we construct bases for the Orlik--Solomon\nalgebra of a matroid, and for the Aomoto cohomology. These bases of canonical\nforms are a foundational input in the theory of matroid amplitudes introduced\nby the second author.",
        "Wide coverage and high-precision rural household wealth data is an important\nsupport for the effective connection between the national macro rural\nrevitalization policy and micro rural entities, which helps to achieve precise\nallocation of national resources. However, due to the large number and wide\ndistribution of rural areas, wealth data is difficult to collect and scarce in\nquantity. Therefore, this article attempts to integrate \"sky\" remote sensing\nimages with \"ground\" village street view imageries to construct a fine-grained\n\"computable\" technical route for rural household wealth. With the intelligent\ninterpretation of rural houses as the core, the relevant wealth elements of\nimage data were extracted and identified, and regressed with the household\nwealth indicators of the benchmark questionnaire to form a high-precision\ntownship scale wealth prediction model (r=0.85); Furthermore, a national and\ntownship scale map of rural household wealth in China was promoted and drawn.\nBased on this, this article finds that there is a \"bimodal\" pattern in the\ndistribution of wealth among rural households in China, which is reflected in a\npolarization feature of \"high in the south and low in the north, and high in\nthe east and low in the west\" in space. This technological route may provide\nalternative solutions with wider spatial coverage and higher accuracy for\nhigh-cost manual surveys, promote the identification of shortcomings in rural\nconstruction, and promote the precise implementation of rural policies.",
        "This paper explores a full generalization of the classical corner-vector\nmethod for constructing weighted spherical designs, which we call the {\\it\ngeneralized corner-vector method}. First we establish a uniform upper bound for\nthe degree of designs obtained from the proposed method. Our proof is a hybrid\nargument that employs techniques in analysis and combinatorics, especially a\nfamous result by Xu(1998) on the interrelation between spherical designs and\nsimplical designs, and the cross-ratio comparison method for Hilbert identities\nintroduced by Nozaki and Sawa(2013). We extensively study conditions for the\nexistence of designs obtained from our method, and present many curious\nexamples of degree $7$ through $13$, some of which are, to our surprise,\ncharacterized in terms of integral lattices.",
        "The Center for Exascale Monte Carlo Neutron Transport is developing Monte\nCarlo \/ Dynamic Code (MC\/DC) as a portable Monte Carlo neutron transport\npackage for rapid numerical methods exploration on CPU- and GPU-based\nhigh-performance computers. In this paper, we describe MC\/DC's current\nevent-based GPU algorithm as well as the just-in-time (JIT) compilation scheme\nwe use to enable GPU operability on Nvidia and AMD GPUs from MC\/DC's Python\nsource. To analyze performance, we conduct runtime tests of the C5G7\nk-eigenvalue benchmark problem and a continuous-energy infinite pin cell on\nNvidia Tesla V100 GPU, AMD MI250X GPU, and the AMD MI300A APU and make\ncomparison to a dual-socket Intel Xeon Sapphire Rapid CPU node. We found that\nfor the multi-group C5G7 benchmark problem, we respectively see a 15$\\times$,\n0.7$\\times$, 12$\\times$ speedup on a V100, MI250X, and MI300A over 112 Intel\nXeon CPU cores. For the continuous-energy infinite pin-cell benchmark, we found\nspeedups of 5$\\times$, 3$\\times$, 4$\\times$ on a V100, MI250X, and MI300A,\nrespectively, over the same CPU node.",
        "We introduce BCAT, a PDE foundation model designed for autoregressive\nprediction of solutions to two dimensional fluid dynamics problems. Our\napproach uses a block causal transformer architecture to model next frame\npredictions, leveraging previous frames as contextual priors rather than\nrelying solely on sub-frames or pixel-based inputs commonly used in image\ngeneration methods. This block causal framework more effectively captures the\nspatial dependencies inherent in nonlinear spatiotemporal dynamics and physical\nphenomena. In an ablation study, next frame prediction demonstrated a 2.9x\naccuracy improvement over next token prediction. BCAT is trained on a diverse\nrange of fluid dynamics datasets, including incompressible and compressible\nNavier-Stokes equations across various geometries and parameter regimes, as\nwell as the shallow-water equations. The model's performance was evaluated on 6\ndistinct downstream prediction tasks and tested on about 8K trajectories to\nmeasure robustness on a variety of fluid dynamics simulations. BCAT achieved an\naverage relative error of 1.92% across all evaluation tasks, outperforming\nprior approaches on standard benchmarks.",
        "Full-waveform inversion (FWI) is an advanced technique for reconstructing\nhigh-resolution subsurface physical parameters by progressively minimizing the\ndiscrepancy between observed and predicted seismic data. However, conventional\nFWI encounters challenges in real data applications, primarily due to its\nconventional objective of direct measurements of the data misfit. Accurate\nestimation of the source wavelet is essential for effective data fitting,\nalongside the need for low-frequency data and a reasonable initial model to\nprevent cycle skipping. Additionally, wave equation solvers often struggle to\naccurately simulate the amplitude of observed data in real applications. To\naddress these challenges, we introduce a correlation-based source-independent\nobjective function for FWI that aims to mitigate source uncertainty and\namplitude dependency, which effectively enhances its practicality for real data\napplications. We develop a deep-learning framework constrained by this new\nobjective function with a velocity-distribution supported deep image prior,\nwhich reparameterizes velocity inversion into trainable parameters within an\nautoencoder, thereby reducing the nonlinearity in the conventional FWI's\nobjective function. We demonstrate the superiority of our proposed method using\nsynthetic data from benchmark velocity models and, more importantly, two real\ndatasets. These examples highlight its effectiveness and practicality even\nunder challenging conditions, such as missing low frequencies, a crude initial\nvelocity model, and an incorrect source wavelet.",
        "A surrogate model for particle-in-cell plasma simulations based on a graph\nneural network is presented. The graph is constructed in such a way as to\nenable the representation of electromagnetic fields on a fixed spatial grid.\nThe model is applied to simulate beams of electrons in one dimension over a\nwide range of temperatures, drift momenta and densities, and is shown to\nreproduce two-stream instabilities - a common and fundamental plasma\ninstability. Qualitatively, the characteristic phase-space mixing of\ncounterpropagating electron beams is observed. Quantitatively, the model's\nperformance is evaluated in terms of the accuracy of its predictions of number\ndensity distributions, the electric field, and their Fourier decompositions,\nparticularly the growth rate of the fastest-growing unstable mode, as well as\nparticle position, momentum distributions, energy conservation and run time.\nThe model achieves high accuracy with a time step longer than conventional\nsimulation by two orders of magnitude. This work demonstrates that complex\nplasma dynamics can be learned and shows promise for the development of fast\ndifferentiable simulators suitable for solving forward and inverse problems in\nplasma physics.",
        "Ordinal user-provided ratings across multiple items are frequently\nencountered in both scientific and commercial applications. Whilst recommender\nsystems are known to do well on these type of data from a predictive point of\nview, their typical reliance on large sample sizes and frequent lack of\ninterpretability and uncertainty quantification limits their applicability in\ninferential problems. Taking a fully Bayesian approach, this article introduces\na novel statistical method that is designed with interpretability and\nuncertainty quantification in mind. Whilst parametric assumptions ensure that\nthe method is applicable to data with modest sample sizes, the model is\nsimultaneously designed to remain flexible in order to handle a wide variety of\nsituations. Model performance, i.e. parameter estimation and prediction, is\nshown by means of a simulation study, both on simulated data and against\ncommonly used recommender systems on real data. These simulations indicate that\nthe proposed method performs competitively. Finally, to illustrate the\napplicability of the proposed method on real life problems that are of interest\nto economists, the method is applied on speed dating data, where novel insights\ninto the partner preference problem are obtained. An R package containing the\nproposed methodology can be found on\nhttps:\/\/CRAN.R-project.org\/package=StatRec.",
        "This paper proves the minimum size of a supersequence over a set of eight\nelements is 52. This disproves a conjecture that the lower bound of the\nsupersequence is the partial sum of the geometric Connell sequence. By studying\nthe internal distribution of individual elements within sub-strings of the\nsupersequence called segments, the proof provides important results on the\ninternal structure that could help to understand the general lower bound\nproblem for finite sets.",
        "Scale invariance (fractality) is a prominent feature of the large-scale\nbehavior of many stochastic systems. In this work, we construct an algorithm\nfor the statistical identification of the Hurst distribution (in particular,\nthe scaling exponents) undergirding a high-dimensional fractal system. The\nalgorithm is based on wavelet random matrices, modified spectral clustering and\na model selection step for picking the value of the clustering precision\nhyperparameter. In a moderately high-dimensional regime where the dimension,\nthe sample size and the scale go to infinity, we show that the algorithm\nconsistently estimates the Hurst distribution. Monte Carlo simulations show\nthat the proposed methodology is efficient for realistic sample sizes and\noutperforms another popular clustering method based on mixed-Gaussian modeling.\nWe apply the algorithm in the analysis of real-world macroeconomic time series\nto unveil evidence for cointegration.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift.",
        "Relativistic secular perturbation theory has ignited significant interest in\nuncovering intricate cross-term effects, especially the interplay between 1PN\nand quadrupole terms. While most existing studies rely on the Lagrangian\nplanetary perturbation method for computing cross terms, a comprehensive\nHamiltonian framework for the field has been missing. In this work, we\nintroduce a framework based on von Zeipel transformation, utilizing two\nsequential canonical transformations to systematically compute cross terms to\narbitrary orders. Our results reveal secular cross terms up to\nquadrupole-squared order, showcasing remarkable consistency with both the\nLagrangian method [1] and the effective-field-theory approach [2]. We present\nleading-order periodic cross terms arising from the interactions between 1PN\nand quadrupole, and present estimates of higher-order cross terms. It is\ndemonstrated that this method not only accurately predicts the long-term\nevolution of hierarchical systems but also captures fast oscillations observed\nin N-body simulations. We identify and validate resonances caused by\nquadrupole-squared effects, highlighting both consistencies and discrepancies\nwhen compared to N-body simulations. These discrepancies underscore the\nimportance of mean-motion resonances, a factor overlooked in current secular\nperturbation frameworks. Finally, we provide a comprehensive review of the\nsubtleties and limitations inherent to secular perturbation theory, paving the\nway for future research and advancements in this field.",
        "In this article, we classify all 2-dimensional Gromov-Hausdorff limit spaces\nof hyperk\\\"ahler K3 surfaces and explore their relations with Jacobian elliptic\nK3 surfaces.",
        "Ultrashort XUV pulses of the Free-Electron-LASer in Hamburg (FLASH) were used\nto investigate laser-induced fragmentation patterns of the prototypical chiral\nmolecule 1-iodo-2-methyl-butane (C$_5$H$_{11}$I) in a pump-probe scheme. Ion\nvelocity-map images and mass spectra of optical-laser-induced fragmentation\nwere obtained for subsequent FEL exposure with photon energies of 63 eV and 75\neV. These energies specifically address the iodine 4d edge of neutral and\nsingly charged iodine, respectively. The presented ion spectra for two optical\npump-laser wavelengths, i.e., 800 nm and 267 nm, reveal substantially different\ncationic fragment yields in dependence on the wavelength and intensity. For the\ncase of 800-nm-initiated fragmentation, the molecule dissociates notably slower\nthan for the 267-nm pump. The results underscore the importance of considering\noptical-laser wavelength and intensity in the dissociation dynamics of this\nprototypical chiral molecule that is a promising candidate for future studies\nof its asymmetric nature.",
        "Quasars behind the Galactic plane (GPQs) are excellent tracers to probe the\nchemistry and kinematics of the interstellar\/intergalactic medium (ISM\/IGM) of\nthe Milky Way along sight lines via absorption line spectroscopy. Moreover, the\nquasars located at low Galactic latitudes will fill the gap in the spatial\ndistribution of known quasars near the Galactic plane, and can be used to\nconstruct an astrometric reference frame for accurate measurements of proper\nmotions (PMs) of stars, and substructures of the Milky Way. We started a survey\nof background quasars in the low Galactic latitude region since the LAMOST\nphase II survey in 2017. Quasar candidates have been selected from the optical\nand infrared photometric data of Pan-STARRS1 and WISE surveys based on their\nvariability and color properties. In this paper, we present a sample of 1982\nspectroscopically confirmed GPQs with |b| <= 20 degree based on LAMOST Data\nRelease 10 (DR10). Among them, 1338 are newly discovered. Most GPQs are located\naround 240<l<90 degree, and the spatial distributions are non-uniform. These\nGPQs have a magnitude distribution with a peak at i-mag 19.0, and mostly around\n18.0-19.5mag. The peak of redshift distributions is around ~1.5, and most GPQs\nhave redshifts between 0.3 and 2.5. Our finding demonstrates the potential\ndiscovery space for the GPQs from the spectroscopic surveys and the promising\napplications for future research.",
        "We describe the testing of a prototype SiPM-on-tile iron-scintillator\ncalorimeter at the Relativistic Heavy Ion Collider (RHIC) during its 200 GeV\n$pp$ run in 2024. The prototype, measuring $20 \\times 20 \\, \\text{cm}^{2}$ and\n24 radiation lengths in depth, was positioned in the STAR experimental hall,\napproximately 8 m from the interaction point and 65 cm from the beam line,\ncovering a pseudorapidity range of about $3.1<\\eta<3.4$. By using the dark\ncurrent of a reference SiPM as a radiation monitor, we estimate that the\nprototype was exposed to a fluence of about $10^{10}$ 1-MeV\n$n_{\\mathrm{eq}}$\/cm$^2$. Channel-by-channel calibration was performed in a\ndata-driven way with the signature from minimum-ionizing particles during\nbeam-on conditions. A Geant4 detector simulation, with inputs from the Pythia8\nevent generator, describes measurements of energy spectra and hit\nmultiplicities reasonably well. These results mark the first deployment,\ncommissioning, calibration, and long-term operation of a SiPM-on-tile\ncalorimeter in a collider environment. This experimental campaign will guide\ndetector designs and operational strategies for the ePIC detector at the future\nEIC, as well as other applications.",
        "The advent of high-average-power, ultrafast ytterbium-based lasers allows us\nto generate laser filaments at repetition rates ranging from 10s of kHz up to\n100s of kHz. At such high repetition rates, the inter-pulse time lies below the\ntime required for the total diffusion of the deposited heat by each laser\npulse, leading to cumulative hydrodynamic effects that have so far been rarely\nstudied. Here, we present, to the best of our knowledge, the first experimental\ntime-resolved measurements of these dynamics in air for laser repetition rates\nbetween 1 kHz and 100 kHz. We measure the change in the air refractive index\ncaused by the localized heat deposition and the length of the\nfilament-generated plasma channel, with which we can infer the corresponding\nchange in air density. We observe that at repetition rates above 10 kHz,\nstationary density depletions with vanishing dynamics emerge. Our findings are\nof wide relevance for the fields of high-repetition-rate laser filamentation\nand its applications, as well as THz generation from laser-induced plasma\nsources.",
        "In many well-motivated new physics models, the pseudo-Nambu-Goldstone boson\n(pNGB) from U(1) symmetry breaking emerges as a promising dark matter\ncandidate. Its coupling, suppressed by the symmetry breaking scale, prevents\nthermal equilibrium in the early Universe for high scale symmetry breaking.\nThus, pNGB dark matter is predominantly produced via non-thermal mechanisms,\nsuch as the freeze-in process through a new portal coupling. In this work, we\nexplore a novel mechanism for the production of pNGB dark matter even with\nfeeble Higgs portal coupling-arising from Hawking radiation or superradiance of\nprimordial black holes. We systematically investigate the production of light\nand heavy pNGB dark matter, both for Schwarzschild and Kerr black holes. We\nalso discuss its potential gravitational wave signatures from domain wall\ncollapse, density perturbations, and Hawking radiation. If the ultraviolet (UV)\nmodel is considered, the recent $\\mathcal{O}$(100) PeV neutrino event\nKM3-230213A at KM3NeT can be naturally explained.",
        "Quantum counterparts of Schrodinger's classical bridge problem have been\naround for the better part of half a century. During that time, several quantum\napproaches to this multifaceted classical problem have been introduced. In the\npresent work, we unify, extend, and interpret several such approaches through a\nclassical large deviations perspective. To this end, we consider time-symmetric\nensembles that are pre- and post-selected before and after a Markovian\nexperiment is performed. The Schrodinger bridge problem is that of finding the\nmost likely joint distribution of initial and final outcomes that is consistent\nwith obtained endpoint results. The derived distribution provides quantum\nMarkovian dynamics that bridge the observed endpoint states in the form of\ndensity matrices. The solution retains its classical structure in that density\nmatrices can be expressed as the product of forward-evolving and\nbackward-evolving matrices. In addition, the quantum Schrodinger bridge allows\ninference of the most likely distribution of outcomes of an intervening\nmeasurement with unknown results. This distribution may be written as a product\nof forward- and backward-evolving expressions, in close analogy to the\nclassical setting, and in a time-symmetric way. The derived results are\nillustrated through a two-level amplitude damping example.",
        "Different stellar populations may be identified through differences in\nchemical, kinematic, and chronological properties, suggesting the interplay of\nvarious physical mechanisms that led to their origin and subsequent evolution.\nAs such, the identification of stellar populations is key for gaining insight\ninto the evolutionary history of the Milky Way galaxy. This task is complicated\nby the fact that stellar populations share significant overlap in their\nchrono-chemo-kinematic properties, hindering efforts to identify and define\nstellar populations. Our goal is to offer a novel and effective methodology\nthat can provide deeper insight into the nonlinear and nonparametric properties\nof the multidimensional physical parameters that define stellar populations.\nFor this purpose we explore the ability of manifold learning to differentiate\nstellar populations with minimal assumptions about their number and nature.\nManifold learning is an unsupervised machine learning technique that seeks to\nintelligently identify and disentangle manifolds hidden within the input data.\nTo test this method, we make use of Gaia DR3-like synthetic stellar samples\ngenerated from the FIRE-2 cosmological simulations. These represent red-giant\nstars constrained by asteroseismic data from TESS. We reduce the 5-dimensional\ninput chrono-chemo-kinematic parameter space into 2-dimensional latent space\nembeddings generated by manifold learning. We then study these embeddings to\nassess how accurately they represent the original data and whether they contain\nmeaningful information that can be used to discern stellar populations. We\nconclude that manifold learning possesses promising abilities to differentiate\nstellar populations when considering realistic observational constraints.",
        "We deepen the analysis of the cosmological acceleration produced by quantum\ngravity dynamics in the formalism of group field theory condensate cosmology,\ntreated at the coarse-grained level via a phenomenological model, in the\nlanguage of hydrodynamics on minisuperspace. Specifically, we conduct a\ndetailed analysis of the late-time evolution, which shows a phantom-like phase\nfollowed by an asymptotic De Sitter expansion. We argue that the model\nindicates a recent occurrence of the phantom crossing and we extract a more\nprecise expression for the effective cosmological constant, linking its value\nto other parameters in the model and to the scale of the quantum bounce in the\nearly universe evolution. Additionally, we show how the phantom phase produced\nby our quantum gravity dynamics increases the inferred value of the current\nHubble parameter based on observed data, indicating a possible quantum gravity\nmechanism for alleviating the Hubble tension. Our results represent a concrete\nexample of how quantum gravity can provide an explanation for large-scale\ncosmological puzzles, in an emergent spacetime scenario."
      ]
    }
  },
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b29",
    "start_title":"Study of Fermion pair production in e+e- collisions at 130-183 GeV",
    "start_abstract":"The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b23"
      ],
      "title":[
        "DARWIN Series: Domain Specific Large Language Models for Natural Science"
      ],
      "abstract":[
        "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Reframing Dense Action Detection (RefDense): A Paradigm Shift in Problem\n  Solving & a Novel Optimization Strategy",
        "The Architecture and Evaluation of Bayesian Neural Networks",
        "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "The Strong Cosmic Censorship Conjecture",
        "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "Adaptive Camera Sensor for Vision Models",
        "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret\n  in Noise-Free Gaussian Process Bandits",
        "Elliptic curves in game theory",
        "StaICC: Standardized Evaluation for Classification Task in In-context\n  Learning",
        "End-to-end Training for Text-to-Image Synthesis using Dual-Text\n  Embeddings",
        "ExplainReduce: Summarising local explanations via proxies",
        "Polynomial invariants of $\\operatorname{GL}_{2}$: Conjugation over\n  finite fields",
        "Continuity of asymptotic entropy on wreath products",
        "Uncertainty Guarantees on Automated Precision Weeding using Conformal\n  Prediction",
        "REGNav: Room Expert Guided Image-Goal Navigation",
        "Intolerable Risk Threshold Recommendations for Artificial Intelligence",
        "LoRa Fine Synchronization with Two-Pass Time and Frequency Offset\n  Estimation",
        "Partial Channel Network: Compute Fewer, Perform Better",
        "Comparison of 2D Regular Lattices for the CPWL Approximation of\n  Functions",
        "DCAT: Dual Cross-Attention Fusion for Disease Classification in\n  Radiological Images with Uncertainty Estimation",
        "Strichartz estimates for the half Klein-Gordon equation on\n  asymptotically flat backgrounds and applications to cubic Dirac equations",
        "Improving the Transferability of Adversarial Attacks by an Input\n  Transpose",
        "From Deep Additive Kernel Learning to Last-Layer Bayesian Neural\n  Networks via Induced Prior Approximation",
        "Generating with Fairness: A Modality-Diffused Counterfactual Framework\n  for Incomplete Multimodal Recommendations",
        "A Scalable System for Visual Analysis of Ocean Data",
        "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in\n  Large Language Models",
        "Linear-Time User-Level DP-SCO via Robust Statistics",
        "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training"
      ],
      "abstract":[
        "Dense action detection involves detecting multiple co-occurring actions while\naction classes are often ambiguous and represent overlapping concepts. We argue\nthat handling the dual challenge of temporal and class overlaps is too complex\nto effectively be tackled by a single network. To address this, we propose to\ndecompose the task of detecting dense ambiguous actions into detecting dense,\nunambiguous sub-concepts that form the action classes (i.e., action entities\nand action motions), and assigning these sub-tasks to distinct sub-networks. By\nisolating these unambiguous concepts, the sub-networks can focus exclusively on\nresolving a single challenge, dense temporal overlaps. Furthermore,\nsimultaneous actions in a video often exhibit interrelationships, and\nexploiting these relationships can improve the method performance. However,\ncurrent dense action detection networks fail to effectively learn these\nrelationships due to their reliance on binary cross-entropy optimization, which\ntreats each class independently. To address this limitation, we propose\nproviding explicit supervision on co-occurring concepts during network\noptimization through a novel language-guided contrastive learning loss. Our\nextensive experiments demonstrate the superiority of our approach over\nstate-of-the-art methods, achieving substantial improvements of 3.8% and 1.7%\non average across all metrics on the challenging benchmark datasets, Charades\nand MultiTHUMOS.",
        "As modern neural networks get more complex, specifying a model with high\npredictive performance and sound uncertainty quantification becomes a more\nchallenging task. Despite some promising theoretical results on the true\nposterior predictive distribution of Bayesian neural networks, the properties\nof even the most commonly used posterior approximations are often questioned.\nComputational burdens and intractable posteriors expose miscalibrated Bayesian\nneural networks to poor accuracy and unreliable uncertainty estimates.\nApproximate Bayesian inference aims to replace unknown and intractable\nposterior distributions with some simpler but feasible distributions. The\ndimensions of modern deep models coupled with the lack of identifiability make\nMarkov chain Monte Carlo tremendously expensive and unable to fully explore the\nmultimodal posterior. On the other hand, variational inference benefits from\nimproved computational complexity but lacks the asymptotical guarantees of\nsampling-based inference and tends to concentrate around a single mode. The\nperformance of both approaches heavily depends on architectural choices; this\npaper aims to shed some light on this, by considering the computational costs,\naccuracy and uncertainty quantification in different scenarios including large\nwidth and out-of-sample data. To improve posterior exploration, different model\naveraging and ensembling techniques are studied, along with their benefits on\npredictive performance. In our experiments, variational inference overall\nprovided better uncertainty quantification than Markov chain Monte Carlo;\nfurther, stacking and ensembles of variational approximations provided\ncomparable to Markov chain Monte Carlo accuracy at a much-reduced cost.",
        "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "In the wake of major breakthroughs in General Relativity during the 1960s,\nRoger Penrose introduced Strong Cosmic Censorship, a profound conjecture\nregarding the deterministic nature of the theory. Penrose's proposal has since\nopened far-reaching new mathematical avenues, revealing connections to\nfundamental questions about black holes and the nature of gravitational\nsingularities. We review recent advances arising from modern techniques in the\ntheory of partial differential equations as applied to Strong Cosmic\nCensorship, maintaining a focus on the context of gravitational collapse that\ngave birth to the conjecture.",
        "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https:\/\/github.com\/Robin-WZQ\/IBA.",
        "Domain shift remains a persistent challenge in deep-learning-based computer\nvision, often requiring extensive model modifications or large labeled datasets\nto address. Inspired by human visual perception, which adjusts input quality\nthrough corrective lenses rather than over-training the brain, we propose Lens,\na novel camera sensor control method that enhances model performance by\ncapturing high-quality images from the model's perspective rather than relying\non traditional human-centric sensor control. Lens is lightweight and adapts\nsensor parameters to specific models and scenes in real-time. At its core, Lens\nutilizes VisiT, a training-free, model-specific quality indicator that\nevaluates individual unlabeled samples at test time using confidence scores\nwithout additional adaptation costs. To validate Lens, we introduce ImageNet-ES\nDiverse, a new benchmark dataset capturing natural perturbations from varying\nsensor and lighting conditions. Extensive experiments on both ImageNet-ES and\nour new ImageNet-ES Diverse show that Lens significantly improves model\naccuracy across various baseline schemes for sensor control and model\nmodification while maintaining low latency in image captures. Lens effectively\ncompensates for large model size differences and integrates synergistically\nwith model improvement techniques. Our code and dataset are available at\ngithub.com\/Edw2n\/Lens.git.",
        "We study the noise-free Gaussian Process (GP) bandits problem, in which the\nlearner seeks to minimize regret through noise-free observations of the\nblack-box objective function lying on the known reproducing kernel Hilbert\nspace (RKHS). Gaussian process upper confidence bound (GP-UCB) is the\nwell-known GP-bandits algorithm whose query points are adaptively chosen based\non the GP-based upper confidence bound score. Although several existing works\nhave reported the practical success of GP-UCB, the current theoretical results\nindicate its suboptimal performance. However, GP-UCB tends to perform well\nempirically compared with other nearly optimal noise-free algorithms that rely\non a non-adaptive sampling scheme of query points. This paper resolves this gap\nbetween theoretical and empirical performance by showing the nearly optimal\nregret upper bound of noise-free GP-UCB. Specifically, our analysis shows the\nfirst constant cumulative regret in the noise-free settings for the squared\nexponential kernel and Mat\\'ern kernel with some degree of smoothness.",
        "We investigate Spohn curves, the algebro-geometric models of dependency\nequilibria for $2 \\times 2$ normal-form games. These curves arise as the\nintersection of two quadrics in $\\mathbb{P}^3$ and are generically elliptic\ncurves. We compute and verify the $j$-invariant for elliptic curves arising as\nthe intersection of quadrics in $\\mathbb P^3$ using two different\nimplementations: by computing the Aronhold invariants and the discriminant (in\nMathematica) and using algorithms for the arithmetic of elliptic curves\n(in-built in Pari\/GP). We define an equivalency of generic $2\\times 2$ games\nbased on the $j$-invariant of the Spohn curve. Additionally, we examine the\nreduction of Spohn curves to plane curves and analyze conditions under which\nthey are reducible. Notably, we prove that the real points are dense on the\nSpohn curve in all cases. Our examples and computations are further supported\nby Macaulay2.",
        "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing.",
        "Text-to-Image (T2I) synthesis is a challenging task that requires modeling\ncomplex interactions between two modalities ( i.e., text and image). A common\nframework adopted in recent state-of-the-art approaches to achieving such\nmultimodal interactions is to bootstrap the learning process with pre-trained\nimage-aligned text embeddings trained using contrastive loss. Furthermore,\nthese embeddings are typically trained generically and reused across various\nsynthesis models. In contrast, we explore an approach to learning text\nembeddings specifically tailored to the T2I synthesis network, trained in an\nend-to-end fashion. Further, we combine generative and contrastive training and\nuse two embeddings, one optimized to enhance the photo-realism of the generated\nimages, and the other seeking to capture text-to-image alignment. A\ncomprehensive set of experiments on three text-to-image benchmark datasets\n(Oxford-102, Caltech-UCSD, and MS-COCO) reveal that having two separate\nembeddings gives better results than using a shared one and that such an\napproach performs favourably in comparison with methods that use text\nrepresentations from a pre-trained text encoder trained using a discriminative\napproach. Finally, we demonstrate that such learned embeddings can be used in\nother contexts as well, such as text-to-image manipulation.",
        "Most commonly used non-linear machine learning methods are closed-box models,\nuninterpretable to humans. The field of explainable artificial intelligence\n(XAI) aims to develop tools to examine the inner workings of these closed\nboxes. An often-used model-agnostic approach to XAI involves using simple\nmodels as local approximations to produce so-called local explanations;\nexamples of this approach include LIME, SHAP, and SLISEMAP. This paper shows\nhow a large set of local explanations can be reduced to a small \"proxy set\" of\nsimple models, which can act as a generative global explanation. This reduction\nprocedure, ExplainReduce, can be formulated as an optimisation problem and\napproximated efficiently using greedy heuristics.",
        "Consider the conjugation action of $\\operatorname{GL}_{2}(K)$ on the\npolynomial ring $K[X_{2 \\times 2}]$. When $K$ is an infinite field, the ring of\ninvariants is a polynomial ring generated by the trace and the determinant. We\ndescribe the ring of invariants when $K$ is a finite field, and show that it is\na hypersurface.",
        "We prove the continuity of asymptotic entropy as a function of the step\ndistribution for non-degenerate probability measures with finite entropy on\nwreath products $ A \\wr B = \\bigoplus_B A \\rtimes B $, where $A$ is any\ncountable group and $B$ is a countable hyper-FC-central group that contains a\nfinitely generated subgroup of at least cubic growth. As one step in proving\nthe above, we show that on any countable group $G$ the probability that the\n$\\mu$-random walk on $G$ never returns to the identity is continuous in $\\mu$,\nfor measures $\\mu$ such that the semigroup generated by the support of $\\mu$\ncontains a finitely generated subgroup of at least cubic growth. Finally, we\nshow that among random walks on a group $G$ that admit a separable completely\nmetrizable space $X$ as a model for their Poisson boundary, the weak continuity\nof the associated harmonic measures on $X$ implies the continuity of the\nasymptotic entropy. This result recovers the continuity of asymptotic entropy\non known cases, such as Gromov hyperbolic groups and acylindrically hyperbolic\ngroups, and extends it to new classes of groups, including linear groups and\ngroups acting on $\\mathrm{CAT}(0)$ spaces.",
        "Precision agriculture in general, and precision weeding in particular, have\ngreatly benefited from the major advancements in deep learning and computer\nvision. A large variety of commercial robotic solutions are already available\nand deployed. However, the adoption by farmers of such solutions is still low\nfor many reasons, an important one being the lack of trust in these systems.\nThis is in great part due to the opaqueness and complexity of deep neural\nnetworks and the manufacturers' inability to provide valid guarantees on their\nperformance. Conformal prediction, a well-established methodology in the\nmachine learning community, is an efficient and reliable strategy for providing\ntrustworthy guarantees on the predictions of any black-box model under very\nminimal constraints. Bridging the gap between the safe machine learning and\nprecision agriculture communities, this article showcases conformal prediction\nin action on the task of precision weeding through deep learning-based image\nclassification. After a detailed presentation of the conformal prediction\nmethodology and the development of a precision spraying pipeline based on a\n''conformalized'' neural network and well-defined spraying decision rules, the\narticle evaluates this pipeline on two real-world scenarios: one under\nin-distribution conditions, the other reflecting a near out-of-distribution\nsetting. The results show that we are able to provide formal, i.e. certifiable,\nguarantees on spraying at least 90% of the weeds.",
        "Image-goal navigation aims to steer an agent towards the goal location\nspecified by an image. Most prior methods tackle this task by learning a\nnavigation policy, which extracts visual features of goal and observation\nimages, compares their similarity and predicts actions. However, if the agent\nis in a different room from the goal image, it's extremely challenging to\nidentify their similarity and infer the likely goal location, which may result\nin the agent wandering around. Intuitively, when humans carry out this task,\nthey may roughly compare the current observation with the goal image, having an\napproximate concept of whether they are in the same room before executing the\nactions. Inspired by this intuition, we try to imitate human behaviour and\npropose a Room Expert Guided Image-Goal Navigation model (REGNav) to equip the\nagent with the ability to analyze whether goal and observation images are taken\nin the same room. Specifically, we first pre-train a room expert with an\nunsupervised learning technique on the self-collected unlabelled room images.\nThe expert can extract the hidden room style information of goal and\nobservation images and predict their relationship about whether they belong to\nthe same room. In addition, two different fusion approaches are explored to\nefficiently guide the agent navigation with the room relation knowledge.\nExtensive experiments show that our REGNav surpasses prior state-of-the-art\nworks on three popular benchmarks.",
        "Frontier AI models -- highly capable foundation models at the cutting edge of\nAI development -- may pose severe risks to public safety, human rights,\neconomic stability, and societal value in the coming years. These risks could\narise from deliberate adversarial misuse, system failures, unintended cascading\neffects, or simultaneous failures across multiple models.\n  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI\nindustry organizations signed the Frontier AI Safety Commitments, and 27\nnations and the EU issued a declaration on their intent to define these\nthresholds. To fulfill these commitments, organizations must determine and\ndisclose ``thresholds at which severe risks posed by a model or system, unless\nadequately mitigated, would be deemed intolerable.''\n  To assist in setting and operationalizing intolerable risk thresholds, we\noutline key principles and considerations; for example, to aim for ``good, not\nperfect'' thresholds in the face of limited data on rapidly advancing AI\ncapabilities and consequently evolving risks. We also propose specific\nthreshold recommendations, including some detailed case studies, for a subset\nof risks across eight risk categories: (1) Chemical, Biological, Radiological,\nand Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)\nPersuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,\nand (8) Socioeconomic Disruption. Our goal is to serve as a starting point or\nsupplementary resource for policymakers and industry leaders, encouraging\nproactive risk management that prioritizes preventing intolerable risks (ex\nante) rather than merely mitigating them after they occur (ex post).",
        "LoRa is currently one of the most widely used low-power wide-area network\n(LPWAN) technologies. The physical layer leverages a chirp spread spectrum\nmodulation to achieve long-range communication with low power consumption.\nSynchronization at long distances is a challenging task as the spread signal\ncan lie multiple orders of magnitude below the thermal noise floor. Multiple\nresearch works have proposed synchronization algorithms for LoRa under\ndifferent hardware impairments. However, the impact of sampling frequency\noffset (SFO) has mostly either been ignored or tracked only during the data\nphase, but it often harms synchronization. In this work, we extend existing\nsynchronization algorithms for LoRa to estimate and compensate SFO already in\nthe preamble and show that this early compensation has a critical impact on the\nestimation of other impairments such as carrier frequency offset and sampling\ntime offset. Therefore it is critical to recover long-range signals.",
        "Designing a module or mechanism that enables a network to maintain low\nparameters and FLOPs without sacrificing accuracy and throughput remains a\nchallenge. To address this challenge and exploit the redundancy within feature\nmap channels, we propose a new solution: partial channel mechanism (PCM).\nSpecifically, through the split operation, the feature map channels are divided\ninto different parts, with each part corresponding to different operations,\nsuch as convolution, attention, pooling, and identity mapping. Based on this\nassumption, we introduce a novel partial attention convolution (PATConv) that\ncan efficiently combine convolution with visual attention. Our exploration\nindicates that the PATConv can completely replace both the regular convolution\nand the regular visual attention while reducing model parameters and FLOPs.\nMoreover, PATConv can derive three new types of blocks: Partial\nChannel-Attention block (PAT_ch), Partial Spatial-Attention block (PAT_sp), and\nPartial Self-Attention block (PAT_sf). In addition, we propose a novel dynamic\npartial convolution (DPConv) that can adaptively learn the proportion of split\nchannels in different layers to achieve better trade-offs. Building on PATConv\nand DPConv, we propose a new hybrid network family, named PartialNet, which\nachieves superior top-1 accuracy and inference speed compared to some SOTA\nmodels on ImageNet-1K classification and excels in both detection and\nsegmentation on the COCO dataset. Our code is available at\nhttps:\/\/github.com\/haiduo\/PartialNet.",
        "We investigate the approximation error of functions with continuous and\npiecewise-linear (CPWL) representations. We focus on the CPWL search spaces\ngenerated by translates of box splines on two-dimensional regular lattices. We\ncompute the approximation error in terms of the stepsize and angles that define\nthe lattice. Our results show that hexagonal lattices are optimal, in the sense\nthat they minimize the asymptotic approximation error.",
        "Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.",
        "The aim of this paper is to establish the $L^2_t$-endpoint Strichartz\nestimate for (half) Klein-Gordon equations on a weakly asymptotically flat\nspace-time. As an application we prove small data global well-posedness and\nscattering for massive cubic Dirac equations in the full subcritical range in\nthis setting.\n  Crucial ingredient is a parametrix contruction following the work of\nMetcalfe-Tataru and Xue and complements Strichartz estimates obtained by\nZheng-Zhang. The proof of the global result for the cubic Dirac equation\nfollows the strategy developed by Machihara-Nakanishi-Ozawa in the Euclidean\nsetting.",
        "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle perturbations applied to inputs that are often imperceptible\nto humans yet lead to incorrect model predictions. In black-box scenarios,\nhowever, existing adversarial examples exhibit limited transferability and\nstruggle to effectively compromise multiple unseen DNN models. Previous\nstrategies enhance the cross-model generalization of adversarial examples by\nintroducing versatility into adversarial perturbations, thereby improving\ntransferability. However, further refining perturbation versatility often\ndemands intricate algorithm development and substantial computation\nconsumption. In this work, we propose an input transpose method that requires\nalmost no additional labor and computation costs but can significantly improve\nthe transferability of existing adversarial strategies. Even without adding\nadversarial perturbations, our method demonstrates considerable effectiveness\nin cross-model attacks. Our exploration finds that on specific datasets, a mere\n$1^\\circ$ left or right rotation might be sufficient for most adversarial\nexamples to deceive unseen models. Our further analysis suggests that this\ntransferability improvement triggered by rotating only $1^\\circ$ may stem from\nvisible pattern shifts in the DNN's low-level feature maps. Moreover, this\ntransferability exhibits optimal angles that, when identified under\nunrestricted query conditions, could potentially yield even greater\nperformance.",
        "With the strengths of both deep learning and kernel methods like Gaussian\nProcesses (GPs), Deep Kernel Learning (DKL) has gained considerable attention\nin recent years. From the computational perspective, however, DKL becomes\nchallenging when the input dimension of the GP layer is high. To address this\nchallenge, we propose the Deep Additive Kernel (DAK) model, which incorporates\ni) an additive structure for the last-layer GP; and ii) induced prior\napproximation for each GP unit. This naturally leads to a last-layer Bayesian\nneural network (BNN) architecture. The proposed method enjoys the\ninterpretability of DKL as well as the computational advantages of BNN.\nEmpirical results show that the proposed approach outperforms state-of-the-art\nDKL methods in both regression and classification tasks.",
        "Incomplete scenario is a prevalent, practical, yet challenging setting in\nMultimodal Recommendations (MMRec), where some item modalities are missing due\nto various factors. Recently, a few efforts have sought to improve the\nrecommendation accuracy by exploring generic structures from incomplete data.\nHowever, two significant gaps persist: 1) the difficulty in accurately\ngenerating missing data due to the limited ability to capture modality\ndistributions; and 2) the critical but overlooked visibility bias, where items\nwith missing modalities are more likely to be disregarded due to the\nprioritization of items' multimodal data over user preference alignment. This\nbias raises serious concerns about the fair treatment of items. To bridge these\ntwo gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)\nframework for incomplete multimodal recommendations. MoDiCF features two key\nmodules: a novel modality-diffused data completion module and a new\ncounterfactual multimodal recommendation module. The former, equipped with a\nparticularly designed multimodal generative framework, accurately generates and\niteratively refines missing data from learned modality-specific distribution\nspaces. The latter, grounded in the causal perspective, effectively mitigates\nthe negative causal effects of visibility bias and thus assures fairness in\nrecommendations. Both modules work collaboratively to address the two\naforementioned significant gaps for generating more accurate and fair results.\nExtensive experiments on three real-world datasets demonstrate the superior\nperformance of MoDiCF in terms of both recommendation accuracy and fairness.\nThe code and processed datasets are released at\nhttps:\/\/github.com\/JinLi-i\/MoDiCF.",
        "Oceanographers rely on visual analysis to interpret model simulations,\nidentify events and phenomena, and track dynamic ocean processes. The ever\nincreasing resolution and complexity of ocean data due to its dynamic nature\nand multivariate relationships demands a scalable and adaptable visualization\ntool for interactive exploration. We introduce pyParaOcean, a scalable and\ninteractive visualization system designed specifically for ocean data analysis.\npyParaOcean offers specialized modules for common oceanographic analysis tasks,\nincluding eddy identification and salinity movement tracking. These modules\nseamlessly integrate with ParaView as filters, ensuring a user-friendly and\neasy-to-use system while leveraging the parallelization capabilities of\nParaView and a plethora of inbuilt general-purpose visualization\nfunctionalities. The creation of an auxiliary dataset stored as a Cinema\ndatabase helps address I\/O and network bandwidth bottlenecks while supporting\nthe generation of quick overview visualizations. We present a case study on the\nBay of Bengal (BoB) to demonstrate the utility of the system and scaling\nstudies to evaluate the efficiency of the system.",
        "Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness.",
        "User-level differentially private stochastic convex optimization (DP-SCO) has\ngarnered significant attention due to the paramount importance of safeguarding\nuser privacy in modern large-scale machine learning applications. Current\nmethods, such as those based on differentially private stochastic gradient\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\nutility due to the need to privatize every intermediate iterate. In this work,\nwe introduce a novel linear-time algorithm that leverages robust statistics,\nspecifically the median and trimmed mean, to overcome these challenges. Our\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\nwith gradient estimation based on robust statistics, thereby significantly\nreducing the gradient estimation noise for privacy purposes and enhancing the\nprivacy-utility trade-off. By sidestepping the repeated privatization required\nby previous methods, our algorithm not only achieves an improved theoretical\nprivacy-utility trade-off but also maintains computational efficiency. We\ncomplement our algorithm with an information-theoretic lower bound, showing\nthat our upper bound is optimal up to logarithmic factors and the dependence on\n$\\epsilon$. This work sets the stage for more robust and efficient\nprivacy-preserving techniques in machine learning, with implications for future\nresearch and application in the field.",
        "Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https:\/\/github.com\/penfever\/wildchat-50m."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy",
    "start_abstract":"ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications.",
    "start_categories":[
      "astro-ph.CO"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "ImageNet: A large-scale hierarchical image database"
      ],
      "abstract":[
        "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Transformers trained on proteins can learn to attend to Euclidean\n  distance",
        "ProgCo: Program Helps Self-Correction of Large Language Models",
        "Unveiling and Causalizing CoT: A Causal Pespective",
        "Visual Attention Exploration in Vision-Based Mamba Models",
        "Kernel EDMD for data-driven nonlinear Koopman MPC with stability\n  guarantees",
        "\"Once Upon a Time...\" Literary Narrative Connectedness Progresses with\n  Grade Level: Potential Impact on Reading Fluency and Literacy Skills",
        "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language\n  Embedding Registration",
        "EILID: Execution Integrity for Low-end IoT Devices",
        "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles",
        "Targetless Intrinsics and Extrinsic Calibration of Multiple LiDARs and\n  Cameras with IMU using Continuous-Time Estimation",
        "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery",
        "On singular supports in mixed characteristic",
        "Addressing Out-of-Label Hazard Detection in Dashcam Videos: Insights\n  from the COOOL Challenge",
        "Algebraic families of higher dimensional $\\mathbb{A}^{1}$-contractible\n  affine varieties non-isomorphic to affine spaces",
        "On the Semantic Security of NTRU -- with a gentle introduction to\n  cryptography",
        "RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head\n  Avatars",
        "Discrete Gaussian Process Representations for Optimising UAV-based\n  Precision Weed Mapping",
        "Data re-uploading in Quantum Machine Learning for time series:\n  application to traffic forecasting",
        "Explicit bounds on the transcendental Brauer group of K3 surfaces with\n  principal complex multiplication",
        "N-player and mean field games among fund managers considering excess\n  logarithmic returns",
        "Bounded-Confidence Models of Multi-Dimensional Opinions with\n  Topic-Weighted Discordance",
        "Identity-aware Feature Decoupling Learning for Clothing-change Person\n  Re-identification",
        "P4sim: Programming Protocol-independent Packet Processors in ns-3",
        "Dollarized Economies in Latin America. An Inflationary Analysis of Pre,\n  During and Post Pandemic",
        "Sakshm AI: Advancing AI-Assisted Coding Education for Engineering\n  Students in India Through Socratic Tutoring and Comprehensive Feedback",
        "Understanding and Rectifying Safety Perception Distortion in VLMs",
        "On a Gelfand-Tsetlin representation of $\\mathfrak{sl}_3$ in the space of\n  sections of a local system with two monodromy parameters",
        "Normalization through Fine-tuning: Understanding Wav2vec 2.0 Embeddings\n  for Phonetic Analysis",
        "A Transformer-Based Framework for Greek Sign Language Production using\n  Extended Skeletal Motion Representations"
      ],
      "abstract":[
        "While conventional Transformers generally operate on sequence data, they can\nbe used in conjunction with structure models, typically SE(3)-invariant or\nequivariant graph neural networks (GNNs), for 3D applications such as protein\nstructure modelling. These hybrids typically involve either (1)\npreprocessing\/tokenizing structural features as input for Transformers or (2)\ntaking Transformer embeddings and processing them within a structural\nrepresentation. However, there is evidence that Transformers can learn to\nprocess structural information on their own, such as the AlphaFold3 structural\ndiffusion model. In this work we show that Transformers can function\nindependently as structure models when passed linear embeddings of coordinates.\nWe first provide a theoretical explanation for how Transformers can learn to\nfilter attention as a 3D Gaussian with learned variance. We then validate this\ntheory using both simulated 3D points and in the context of masked token\nprediction for proteins. Finally, we show that pre-training protein Transformer\nencoders with structure improves performance on a downstream task, yielding\nbetter performance than custom structural models. Together, this work provides\na basis for using standard Transformers as hybrid structure-language models.",
        "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
        "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved.",
        "State space models (SSMs) have emerged as an efficient alternative to\ntransformer-based models, offering linear complexity that scales better than\ntransformers. One of the latest advances in SSMs, Mamba, introduces a selective\nscan mechanism that assigns trainable weights to input tokens, effectively\nmimicking the attention mechanism. Mamba has also been successfully extended to\nthe vision domain by decomposing 2D images into smaller patches and arranging\nthem as 1D sequences. However, it remains unclear how these patches interact\nwith (or attend to) each other in relation to their original 2D spatial\nlocation. Additionally, the order used to arrange the patches into a sequence\nalso significantly impacts their attention distribution. To better understand\nthe attention between patches and explore the attention patterns, we introduce\na visual analytics tool specifically designed for vision-based Mamba models.\nThis tool enables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout a Mamba model.\nUsing the tool, we also investigate the impact of different patch-ordering\nstrategies on the learned attention, offering further insights into the model's\nbehavior.",
        "Extended dynamic mode decomposition (EDMD) is a popular data-driven method to\npredict the action of the Koopman operator, i.e., the evolution of an\nobservable function along the flow of a dynamical system. In this paper, we\nleverage a recently-introduced kernel EDMD method for control systems for\ndata-driven model predictive control. Building upon pointwise error bounds\nproportional in the state, we rigorously show practical asymptotic stability of\nthe origin w.r.t. the MPC closed loop without stabilizing terminal conditions.\nThe key novelty is that we avoid restrictive invariance conditions. Last, we\nverify our findings by numerical simulations.",
        "Selecting an appropriate book is crucial for fostering reading habits in\nchildren. While children exhibit varying levels of complexity when generating\noral narratives, the question arises: do children's books also differ in\nnarrative complexity? This study explores the narrative dynamics of literary\ntexts used in schools, focusing on how their complexity evolves across\ndifferent grade levels. Using Word-Recurrence Graph Analysis, we examined a\ndataset of 1,627 literary texts spanning 13 years of education. The findings\nreveal significant exponential growth in connectedness, particularly during the\nfirst three years of schooling, mirroring patterns observed in children's oral\nnarratives. These results highlight the potential of literary texts as a tool\nto support the development of literacy skills.",
        "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene\nunderstanding leveraging 3D Gaussian Splatting. Unlike existing\nlanguage-embedded 3DGS methods, which rely on a rendering process, our method\ndirectly associates language-aligned CLIP embeddings with 3D Gaussians for\nholistic 3D scene understanding. The key of our method is a language feature\nregistration technique where CLIP embeddings are assigned to the dominant\nGaussians intersected by each pixel-ray. Moreover, we integrate Product\nQuantization (PQ) trained on general large-scale image data to compactly\nrepresent embeddings without per-scene optimization. Experiments demonstrate\nthat our approach significantly outperforms existing approaches in 3D\nperception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D\nobject localization, and 3D object selection tasks. For video results, please\nvisit : https:\/\/drsplat.github.io\/",
        "Prior research yielded many techniques to mitigate software compromise for\nlow-end Internet of Things (IoT) devices. Some of them detect software\nmodifications via remote attestation and similar services, while others\npreventatively ensure software (static) integrity. However, achieving run-time\n(dynamic) security, e.g., control-flow integrity (CFI), remains a challenge.\n  Control-flow attestation (CFA) is one approach that minimizes the burden on\ndevices. However, CFA is not a real-time countermeasure against run-time\nattacks since it requires communication with a verifying entity. This poses\nsignificant risks if safety- or time-critical tasks have memory\nvulnerabilities.\n  To address this issue, we construct EILID - a hybrid architecture that\nensures software execution integrity by actively monitoring control-flow\nviolations on low-end devices. EILID is built atop CASU, a prevention-based\n(i.e., active) hybrid Root-of-Trust (RoT) that guarantees software\nimmutability. EILID achieves fine-grained backward-edge and function-level\nforward-edge CFI via semi-automatic code instrumentation and a secure shadow\nstack.",
        "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps:\/\/anonymous.4open.science\/r\/remember-B0B8\/.",
        "Accurate spatiotemporal calibration is a prerequisite for multisensor fusion.\nHowever, sensors are typically asynchronous, and there is no overlap between\nthe fields of view of cameras and LiDARs, posing challenges for intrinsic and\nextrinsic parameter calibration. To address this, we propose a calibration\npipeline based on continuous-time and bundle adjustment (BA) capable of\nsimultaneous intrinsic and extrinsic calibration (6 DOF transformation and time\noffset). We do not require overlapping fields of view or any calibration board.\nFirstly, we establish data associations between cameras using Structure from\nMotion (SFM) and perform self-calibration of camera intrinsics. Then, we\nestablish data associations between LiDARs through adaptive voxel map\nconstruction, optimizing for extrinsic calibration within the map. Finally, by\nmatching features between the intensity projection of LiDAR maps and camera\nimages, we conduct joint optimization for intrinsic and extrinsic parameters.\nThis pipeline functions in texture-rich structured environments, allowing\nsimultaneous calibration of any number of cameras and LiDARs without the need\nfor intricate sensor synchronization triggers. Experimental results demonstrate\nour method's ability to fulfill co-visibility and motion constraints between\nsensors without accumulating errors.",
        "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https:\/\/app.aios.foundation,\nthe code is at https:\/\/github.com\/agiresearch\/Cerebrum, and video is at\nhttps:\/\/app.aios.foundation\/video-demo.",
        "We fix an excellent regular noetherian scheme $S$ over ${\\mathbf Z}_{(p)}$\nsatisfying a certain finiteness condition. For a constructible \\'etale sheaf\n${\\cal F}$ on a regular scheme $X$ of finite type over $S$, we introduce a\nvariant of the singular support relatively to $S$ and prove the existence of a\nsaturated relative variant of the singular support by adopting the method of\nBeilinson using the Radon transform. We may deduce the existence of the\nsingular support itself, if we admit an expected property on the micro support\nof tensor product and if the scheme $X$ is sufficiently ramified over the base\n$S$.",
        "This paper presents a novel approach for hazard analysis in dashcam footage,\naddressing the detection of driver reactions to hazards, the identification of\nhazardous objects, and the generation of descriptive captions. We first\nintroduce a method for detecting driver reactions through speed and sound\nanomaly detection, leveraging unsupervised learning techniques. For hazard\ndetection, we employ a set of heuristic rules as weak classifiers, which are\ncombined using an ensemble method. This ensemble approach is further refined\nwith differential privacy to mitigate overconfidence, ensuring robustness\ndespite the lack of labeled data. Lastly, we use state-of-the-art\nvision-language models for hazard captioning, generating descriptive labels for\nthe detected hazards. Our method achieved the highest scores in the Challenge\non Out-of-Label in Autonomous Driving, demonstrating its effectiveness across\nall three tasks. Source codes are publicly available at\nhttps:\/\/github.com\/ffyyytt\/COOOL_2025.",
        "We construct algebraic families of smooth affine $\\mathbb{A}^1$-contractible\nvarieties of every dimension $n\\geq 4$ over fields of characteristic zero which\nare non-isomorphic to affine spaces and potential counterexamples to the\nZariski Cancellation Problem. We further prove that these families of varieties\nare also counter examples to the generalized Cancellation problem.",
        "This paper provides an explanation of NTRU, a post quantum encryption scheme,\nwhile also providing a gentle introduction to cryptography. NTRU is a very\nefficient lattice based cryptosystem that appears to be safe against attacks by\nquantum computers. NTRU's efficiency suggests that it is a strong candidate as\nan alternative to RSA, ElGamal, and ECC for the post quantum world. The paper\nbegins with an introduction to cryptography and security proofs for\ncryptographic schemes before explaining the NTRU cryptosystem and culminating\nwith a proof that the original presentation of NTRU is not IND-CPA secure. We\nwill conclude by mentioning padding schemes to NTRU that are provably IND-CCA2\nsecure in the random oracle model. The paper is designed to be accessible to\nanyone with minimal background in abstract algebra and number theory - no\nprevious knowledge of cryptography is assumed. Given the author's lack of\nfamiliarity with the subject, this paper aims to be an expository work rather\nthan to provide new insights to the subject matter.",
        "We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for\nreconstructing photorealistic, animatable head avatars at speeds sufficient for\non-the-fly reconstruction. Unlike prior approaches that utilize linear bases\nfrom 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps\ntracked 3DMM parameters into reduced blendshape weights with an MLP, leading to\na compact set of blendshape bases. The learned compact base composition\neffectively captures essential facial details for specific individuals, and\ndoes not rely on the fixed base composition weights of 3DMM, leading to\nenhanced reconstruction quality and higher efficiency. To further expedite the\nreconstruction process, we develop a novel color initialization estimation\nmethod and a batch-parallel Gaussian rasterization process, achieving\nstate-of-the-art quality with training throughput of about 630 images per\nsecond. Moreover, we propose a local-global sampling strategy that enables\ndirect on-the-fly reconstruction, immediately reconstructing the model as video\nstreams in real time while achieving quality comparable to offline settings.\nOur source code is available at https:\/\/github.com\/gapszju\/RGBAvatar.",
        "Accurate agricultural weed mapping using UAVs is crucial for precision\nfarming applications. Traditional methods rely on orthomosaic stitching from\nrigid flight paths, which is computationally intensive and time-consuming.\nGaussian Process (GP)-based mapping offers continuous modelling of the\nunderlying variable (i.e. weed distribution) but requires discretisation for\npractical tasks like path planning or visualisation. Current implementations\noften default to quadtrees or gridmaps without systematically evaluating\nalternatives. This study compares five discretisation methods: quadtrees,\nwedgelets, top-down binary space partition (BSP) trees using least square error\n(LSE), bottom-up BSP trees using graph merging, and variable-resolution\nhexagonal grids. Evaluations on real-world weed distributions measure visual\nsimilarity, mean squared error (MSE), and computational efficiency. Results\nshow quadtrees perform best overall, but alternatives excel in specific\nscenarios: hexagons or BSP LSE suit fields with large, dominant weed patches,\nwhile quadtrees are optimal for dispersed small-scale distributions. These\nfindings highlight the need to tailor discretisation approaches to weed\ndistribution patterns (patch size, density, coverage) rather than relying on\ndefault methods. By choosing representations based on the underlying\ndistribution, we can improve mapping accuracy and efficiency for precision\nagriculture applications.",
        "Accurate traffic forecasting plays a crucial role in modern Intelligent\nTransportation Systems (ITS), as it enables real-time traffic flow management,\nreduces congestion, and improves the overall efficiency of urban transportation\nnetworks. With the rise of Quantum Machine Learning (QML), it has emerged a new\nparadigm possessing the potential to enhance predictive capabilities beyond\nwhat classical machine learning models can achieve. In the present work we\npursue a heuristic approach to explore the potential of QML, and focus on a\nspecific transport issue. In particular, as a case study we investigate a\ntraffic forecast task for a major urban area in Athens (Greece), for which we\npossess high-resolution data. In this endeavor we explore the application of\nQuantum Neural Networks (QNN), and, notably, we present the first application\nof quantum data re-uploading in the context of transport forecasting. This\ntechnique allows quantum models to better capture complex patterns, such as\ntraffic dynamics, by repeatedly encoding classical data into a quantum state.\nAside from providing a prediction model, we spend considerable effort in\ncomparing the performance of our hybrid quantum-classical neural networks with\nclassical deep learning approaches. Our results show that hybrid models achieve\ncompetitive accuracy with state-of-the-art classical methods, especially when\nthe number of qubits and re-uploading blocks is increased. While the classical\nmodels demonstrate lower computational demands, we provide evidence that\nincreasing the complexity of the quantum model improves predictive accuracy.\nThese findings indicate that QML techniques, and specifically the data\nre-uploading approach, hold promise for advancing traffic forecasting models\nand could be instrumental in addressing challenges inherent in ITS\nenvironments.",
        "Let $X$ be a K3 surface defined over a number field $k$, with principal\ncomplex multiplication by a CM field $E$. We find explicit bounds, in terms of\n$k$ and $E$, on the size of the transcendental Brauer group\n$\\operatorname{Br}(X)\/\\operatorname{Br}_1(X)$ of $X$. Bounding the size of this\ngroup is important for computing the Brauer--Manin obstruction, which is\nconjectured by Skorobogatov to be the only obstruction to the Hasse principle\nfor K3 surfaces. Our methods are built on top of earlier work by Valloni, who\nrelated the group $\\operatorname{Br}(X)\/\\operatorname{Br}_1(X)$ to the\narithmetic structure of the CM field $E$. It is from this arithmetic structure\nthat we deduce our bounds.",
        "This paper studies the competition among multiple fund managers with relative\nperformance over the excess logarithmic return. Fund managers compete with each\nother and have expected utility or mean-variance criteria for excess\nlogarithmic return.\n  Each fund manager possesses a unique risky asset, and all fund managers can\nalso invest in a public risk-free asset and a public risk asset. We construct\nboth an $n$-player game and a mean field game (MFG) to address the competition\nproblem under these two criteria. We explicitly define and rigorously solve the\nequilibrium and mean field equilibrium (MFE) for each criteria. In the four\nmodels, the excess logarithmic return as the evaluation criterion of the fund\nleads to the { allocation fractions} being constant. The introduction of the\npublic risky asset yields different outcomes, with competition primarily\naffecting the investment in public assets, particularly evident in the MFG. We\ndemonstrate that the MFE of the MFG represents the limit of the $n$-player\ngame's equilibrium as the competitive scale $n$ approaches infinity. Finally,\nthe sensitivity analyses of the equilibrium are given.",
        "People's opinions on a wide range of topics often evolve over time through\ntheir interactions with others. Models of opinion dynamics primarily focus on\none-dimensional opinions which represent opinions on one topic. However,\nopinions on various topics are rarely isolated; instead, they can be\ninterdependent and exhibit correlations. In a bounded-confidence model (BCM) of\nopinion dynamics, agents influence each other's opinions only if their opinions\nare sufficiently similar. We extend classical agent-based BCMs -- namely, the\nHegeselmann--Krause BCM, which has synchronous interactions, and the\nDeffuant--Weisbuch BCM, which has asynchronous interactions -- to a\nmultidimensional setting, in which the opinions are multidimensional vectors\nrepresenting opinions of different topics and opinions on different topics are\ninterdependent. To measure opinion differences between agents, we introduce\ntopic-weighted discordance functions that account for opinion differences in\nall topics. We use the regions of receptiveness to characterize the\nsteady-state opinion clusters and provide an analytical approach to compute\nthese regions. In addition, we numerically simulate our models on various\nnetworks with initial opinions drawn from a variety of distributions. When\ninitial opinions are correlated across different topics, our topic-weighted\nBCMs yield significantly different results in both transient and steady states\ncompared to baseline models, where the dynamics of each opinion topic are\nindependent.",
        "Clothing-change person re-identification (CC Re-ID) has attracted increasing\nattention in recent years due to its application prospect. Most existing works\nstruggle to adequately extract the ID-related information from the original RGB\nimages. In this paper, we propose an Identity-aware Feature Decoupling (IFD)\nlearning framework to mine identity-related features. Particularly, IFD\nexploits a dual stream architecture that consists of a main stream and an\nattention stream. The attention stream takes the clothing-masked images as\ninputs and derives the identity attention weights for effectively transferring\nthe spatial knowledge to the main stream and highlighting the regions with\nabundant identity-related information. To eliminate the semantic gap between\nthe inputs of two streams, we propose a clothing bias diminishing module\nspecific to the main stream to regularize the features of clothing-relevant\nregions. Extensive experimental results demonstrate that our framework\noutperforms other baseline models on several widely-used CC Re-ID datasets.",
        "Programmable data planes enable users to design data plane algorithms for\nnetwork devices, providing extensive flexibility for network customization.\nProgramming Protocol-Independent Packet Processors (P4) has become the most\nwidely adopted abstraction, programming language, and framework for data plane\nprogramming. However, existing simulation platforms lack high-performance\nsupport for P4-based networks. This paper introduces P4sim, a high-performance\nP4-driven simulation framework built on bmv2 and NS4, seamlessly integrated\nwith ns-3. It improves queue modeling, time scheduling, and P4 architecture\nsupport, extending compatibility to V1model, PSA, and PNA. P4sim enables\nefficient packet processing, accurate time tracking, and seamless interaction\nbetween P4-enabled hosts and switches. We evaluate the P4sim in terms of\nperformance and queue management and demonstrate its capabilities using two\ncommon use cases: Basic Tunneling and Load Balancing. The results highlight the\nP4sim as a powerful tool for advancing research and education in programmable\nnetworks.",
        "Given the hyperinflation that most of the Latin American countries suffered\nin the 90 and their decision towards adopting dollarization and in most cases\nkeeping their own currency, this paper analyzes the effectiveness of\ndollarization as a protective mechanism against economic disruptions in Latin\nAmerican countries. It assesses the context that led Latin American dollarized\ncountries to dollarize and analyzes CPI, GDP, and the poverty rates pre,\nduring, and postpandemic in Latin American countries, considering those that\nare dollarized and those that are not, and evaluating its relation to the US.\nInterviews were carried out with experts in the field. It assesses the\nadvantages and disadvantages of dollarization regarding global crises. The data\nwas compared and analyzed to check if there were patterns that support the\npaper objective which is that dollarization might serve as a protective\nmechanism against economic disruption. It was found that dollarization protects\nthe economy against inflation, however, it does not fully protect the economy\nwhen considering economic performance and poverty. In conclusion, this research\nconcludes that dollarization does not completely serve as a protective\nmechanism against economic disruptions nonetheless, it found that a bigger role\nis played by domestic policies and government action.",
        "The advent of Large Language Models (LLMs) is reshaping education,\nparticularly in programming, by enhancing problem-solving, enabling\npersonalized feedback, and supporting adaptive learning. Existing AI tools for\nprogramming education struggle with key challenges, including the lack of\nSocratic guidance, direct code generation, limited context retention, minimal\nadaptive feedback, and the need for prompt engineering. To address these\nchallenges, we introduce Sakshm AI, an intelligent tutoring system for learners\nacross all education levels. It fosters Socratic learning through Disha, its\ninbuilt AI chatbot, which provides context-aware hints, structured feedback,\nand adaptive guidance while maintaining conversational memory and supporting\nlanguage flexibility. This study examines 1170 registered participants,\nanalyzing platform logs, engagement trends, and problem-solving behavior to\nassess Sakshm AI's impact. Additionally, a structured survey with 45 active\nusers and 25 in-depth interviews was conducted, using thematic encoding to\nextract qualitative insights. Our findings reveal how AI-driven Socratic\nguidance influences problem-solving behaviors and engagement, offering key\nrecommendations for optimizing AI-based coding platforms. This research\ncombines quantitative and qualitative insights to inform AI-assisted education,\nproviding a framework for scalable, intelligent tutoring systems that improve\nlearning outcomes. Furthermore, Sakshm AI represents a significant step toward\nSustainable Development Goal 4 Quality Education, providing an accessible and\nstructured learning tool for undergraduate students, even without expert\nguidance. This is one of the first large-scale studies examining AI-assisted\nprogramming education across multiple institutions and demographics.",
        "Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.",
        "We construct a Gelfand-Tsetlin representation of $\\mathfrak{sl}_3$ in the\nspace of sections of a local system. The local system lives on an open part of\nthe flag variety given by the intersection of three translates of the big cell\nand has two complex monodromy parameters. We analyze the structure of this\nrepresentation.",
        "Phonetic normalization plays a crucial role in speech recognition and\nanalysis, ensuring the comparability of features derived from raw audio data.\nHowever, in the current paradigm of fine-tuning pre-trained large transformer\nmodels, phonetic normalization is not deemed a necessary step; instead, it is\nimplicitly executed within the models. This study investigates the\nnormalization process within transformer models, especially wav2vec 2.0.\nThrough a comprehensive analysis of embeddings from models fine-tuned for\nvarious tasks, our results demonstrate that fine-tuning wav2vec 2.0 effectively\nachieves phonetic normalization by selectively suppressing task-irrelevant\ninformation. We found that models fine-tuned for multiple tasks retain\ninformation for both tasks without compromising performance, and that\nsuppressing task-irrelevant information is not necessary for effective\nclassification. These findings provide new insights into how phonetic\nnormalization can be flexibly achieved in speech models and how it is realized\nin human speech perception.",
        "Sign Languages are the primary form of communication for Deaf communities\nacross the world. To break the communication barriers between the Deaf and\nHard-of-Hearing and the hearing communities, it is imperative to build systems\ncapable of translating the spoken language into sign language and vice versa.\nBuilding on insights from previous research, we propose a deep learning model\nfor Sign Language Production (SLP), which to our knowledge is the first attempt\non Greek SLP. We tackle this task by utilizing a transformer-based architecture\nthat enables the translation from text input to human pose keypoints, and the\nopposite. We evaluate the effectiveness of the proposed pipeline on the Greek\nSL dataset Elementary23, through a series of comparative analyses and ablation\nstudies. Our pipeline's components, which include data-driven gloss generation,\ntraining through video to text translation and a scheduling algorithm for\nteacher forcing - auto-regressive decoding seem to actively enhance the quality\nof produced SL videos."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"ImageNet: A large-scale hierarchical image database",
    "start_abstract":"The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
      ],
      "abstract":[
        "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
      ],
      "categories":[
        "astro-ph.CO"
      ]
    },
    "list":{
      "title":[
        "Improved Decoding of Tanner Codes",
        "Anomaly Detection to identify Transients in LSST Time Series Data",
        "Cherenkov detector with wavelength-shifting fiber readout for muon\n  tomography applications",
        "Diagnosing Quantum Many-body Chaos in Non-Hermitian Quantum Spin Chain\n  via Krylov Complexity",
        "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems",
        "Energy Reconstruction of Non-fiducial Electron-Positron Events in the\n  DAMPE Experiment Using Convolutional Neural Networks",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "Impact of pH and chloride content on the biodegradation of magnesium\n  alloys for medical implants: An in vitro and phase-field study",
        "Concentration of Measure for Distributions Generated via Diffusion\n  Models",
        "A Comprehensive Framework for Electroweak Phase Transitions: Thermal\n  History and Dynamics from Bubble Nucleation to Percolation",
        "On the inverse-closedness of operator-valued matrices with polynomial\n  off-diagonal decay",
        "Multivariate Frequent Stability and Diam-Mean Equicontinuity",
        "Global existence for semi-linear hyperbolic equations in a neighbourhood\n  of future null infinity",
        "Growth Laws and Universality in 2-TIPS: Microscopic and Coarse grained\n  approach",
        "High pressure structural and lattice dynamics study of\n  {\\alpha}-In$_2$Se$_3$",
        "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 {\\deg}C",
        "Physical Interpretations of Integration Constants in the Solutions of\n  Einstein Equations",
        "Dynamic Routing in Space-Ground Integrated Quantum Networks",
        "Breakdown of broken-symmetry approach to exchange interaction",
        "Multimodal Stock Price Prediction",
        "Fundamental Oscillations of Massive Boson Stars and Distinguishability",
        "Properties of high-redshift Type II supernovae discovered by the JADES\n  transient survey",
        "Phylogenetic Corrections and Higher-Order Sequence Statistics in Protein\n  Families: The Potts Model vs MSA Transformer",
        "Late-time behaviors of scalar field modes for a collapsing null shell\n  spacetime and for the Unruh state in Schwarzschild spacetime",
        "Enhanced collective vibrations in granular materials",
        "Unstable accretion in TW Hya: 3D simulations and comparisons with\n  observations",
        "Quantum Transport in Reduced Graphene Oxide Measured by Scanning Probe\n  Microscopy",
        "Upgrades and maintenance of the CRYRING@ESR electron cooler for improved\n  internal electron target operation",
        "Bose-Bose gases with nonuniversal corrections to the interactions: a\n  droplet phase"
      ],
      "abstract":[
        "In this paper, we present improved decoding algorithms for expander-based\nTanner codes.\n  We begin by developing a randomized linear-time decoding algorithm that,\nunder the condition that $ \\delta d_0 > 2 $, corrects up to $ \\alpha n $ errors\nfor a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \\alpha, \\delta)\n$-bipartite expander with $n$ left vertices, and $ C_0 \\subseteq \\mathbb{F}_2^d\n$ is a linear inner code with minimum distance $ d_0 $. This result improves\nupon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024),\nwhich required $ \\delta d_0 > 3 $.\n  We further derandomize the algorithm to obtain a deterministic linear-time\ndecoding algorithm with the same decoding radius. Our algorithm improves upon\nthe previous deterministic algorithm of Cheng et al.\\ by achieving a decoding\nradius of $ \\alpha n $, compared with the previous radius of $\n\\frac{2\\alpha}{d_0(1 + 0.5c\\delta) }n$.\n  Additionally, we investigate the size-expansion trade-off introduced by the\nrecent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to\nprovide new bounds on the minimum distance of Tanner codes. Specifically, we\nprove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately\n$f_\\delta^{-1} \\left( \\frac{1}{d_0} \\right) \\alpha n $, where $ f_\\delta(\\cdot)\n$ is the Size-Expansion Function. As another application, we improve the\ndecoding radius of our decoding algorithms from $\\alpha n$ to approximately\n$f_\\delta^{-1}(\\frac{2}{d_0})\\alpha n$.",
        "We introduce a novel approach to detecting microlensing events and other\ntransients in light curves, utilising the isolation forest (iForest) algorithm\nfor anomaly detection. Focusing on the Legacy Survey of Space and Time by the\nVera C. Rubin Observatory, we show that an iForest trained on signal-less light\ncurves can efficiently identify microlensing events by different types of dark\nobjects and binaries, as well as variable stars. We further show that the\niForest has real-time applicability through a drip-feed analysis, demonstrating\nits potential as a valuable tool for LSST alert brokers to efficiently\nprioritise and classify transient candidates for follow-up observations.",
        "Cherenkov detectors have been extensively developed and utilized in various\nscientific fields, including particle physics, astrophysics, and nuclear\nengineering. These detectors operate based on Cherenkov radiation, which is\nemitted when a charged particle traverses a dielectric medium at a velocity\ngreater than the phase velocity of light in that medium. In this work, we\npresent the development of a Cherenkov radiation detector designed for a muon\ntomography system with high spatial resolution, employing wavelength-shifting\n(WLS) fiber readout. The detector consists of two large-area Cherenkov\nradiators, each measuring 1 m x 1 m, with each read out by WLS fibers arranged\northogonally to determine the x and y coordinates of muon hit positions. The\nsystem is modeled using the GEANT4 simulation package, and the achieved\nposition resolution is 1.8 mm+-0.1 (FWHM). This design enables precise tracking\nof muon trajectories, making it suitable for high-resolution imaging\napplications in muon tomography.",
        "We investigate the phase transitions from chaotic to non-chaotic dynamics in\na quantum spin chain with a local non-Hermitian disorder, which can be realized\nwith a Rydberg atom array setting. As the disorder strength increases, the\nemergence of non-chaotic dynamics is qualitatively captured through the\nsuppressed growth of Krylov complexity, and quantitatively identified through\nthe reciprocity breaking of Krylov space. We further find that the localization\nin Krylov space generates another transition in the weak disorder regime,\nsuggesting a weak ergodicity breaking. Our results closely align with\nconventional methods, such as the entanglement entropy and complex level\nspacing statistics, and pave the way to explore non-Hermitian phase transitions\nusing Krylov complexity and associated metrics.",
        "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.",
        "The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "The individual contributions of pH and chloride concentration to the\ncorrosion kinetics of bioabsorbable magnesium (Mg) alloys remain unresolved\ndespite their significant roles as driving factors in Mg corrosion. This study\ndemonstrates and quantifies hitherto unknown separate effects of pH and\nchloride content on the corrosion of Mg alloys pertinent to biomedical implant\napplications. The experimental setup designed for this purpose enables the\nquantification of the dependence of corrosion on pH and chloride concentration.\nThe in vitro tests conclusively demonstrate that variations in chloride\nconcentration, relevant to biomedical applications, have a negligible effect on\ncorrosion kinetics. The findings identify pH as a critical factor in the\ncorrosion of bioabsorbable Mg alloys. A variationally consistent phase-field\nmodel is developed for assessing the degradation of Mg alloys in biological\nfluids. The model accurately predicts the corrosion performance of Mg alloys\nobserved during the experiments, including their dependence on pH and chloride\nconcentration. The capability of the framework to account for mechano-chemical\neffects during corrosion is demonstrated in practical orthopaedic applications\nconsidering bioabsorbable Mg alloy implants for bone fracture fixation and\nporous scaffolds for bone tissue engineering. The strategy has the potential to\nassess the in vitro and in vivo service life of bioabsorbable Mg-based\nbiomedical devices.",
        "We show via a combination of mathematical arguments and empirical evidence\nthat data distributions sampled from diffusion models satisfy a Concentration\nof Measure Property saying that any Lipschitz $1$-dimensional projection of a\nrandom vector is not too far from its mean with high probability. This implies\nthat such models are quite restrictive and gives an explanation for a fact\npreviously observed in the literature that conventional diffusion models cannot\ncapture \"heavy-tailed\" data (i.e. data $\\mathbf{x}$ for which the norm\n$\\|\\mathbf{x}\\|_2$ does not possess a sub-Gaussian tail) well. We then proceed\nto train a generalized linear model using stochastic gradient descent (SGD) on\nthe diffusion-generated data for a multiclass classification task and observe\nempirically that a Gaussian universality result holds for the test error.\n  In other words, the test error depends only on the first and second order\nstatistics of the diffusion-generated data in the linear setting. Results of\nsuch forms are desirable because they allow one to assume the data itself is\nGaussian for analyzing performance of the trained classifier. Finally, we note\nthat current approaches to proving universality do not apply to this case as\nthe covariance matrices of the data tend to have vanishing minimum singular\nvalues for the diffusion-generated data, while the current proofs assume that\nthis is not the case (see Subsection 3.4 for more details). This leaves\nextending previous mathematical universality results as an intriguing open\nquestion.",
        "The electroweak phase transition (EWPT) is crucial for cosmology and particle\nphysics, with a profound impact on electroweak baryogenesis, symmetry breaking,\nand gravitational wave (GW) signals. However, many studies overlook key aspects\nof EWPT dynamics, leading to misidentified patterns and overestimated GW\nsignals. To address these gaps, we present a comprehensive framework for\nanalyzing EWPTs, focusing on the vacuum's thermal history and dynamics from\nbubble nucleation to percolation. Using the $\\mathbb{Z}_2$-odd real scalar\nsinglet model, we demonstrate the occurrence of spontaneous $\\mathbb{Z}_2$\nsymmetry breaking in the high-temperature vacuum, leading to diverse EWPT\nprocesses, including multi-step transitions and inverse symmetry breaking. We\nidentify four distinct EWPT patterns, each characterized by unique\nsymmetry-breaking mechanisms and associated with bubbles exhibiting distinct\nfield configurations, which can be analyzed using a formalism based on energy\ndensity distributions developed here. A key finding is that bubble nucleation\nfails in extremely strong phase transitions (PTs) with low nucleation rates, or\nin ultra-fast PTs involving inverse $s$-bubbles that collapse instantly upon\nformation, both of which lead to false vacuum trapping and the absence of\nobservable GW signals. In first-order PTs where nucleation succeeds, stronger\ntransitions occur later in the universe's evolution, while weaker transitions\nproceed more rapidly. Multi-step transitions involving (inverse) $\\mathbb{Z}_2$\nsymmetry breaking give rise to complex transition sequences and exotic bubble\ndynamics, such as sequential nucleation or the coexistence of bubbles from\ndifferent vacua -- phenomena with significant implications for GW spectra, dark\nmatter, and baryogenesis. This work advances our understanding of EWPT dynamics\nand lays the groundwork for future studies of EWPTs in BSM physics.",
        "We give a self-contained proof of a recently established\n$\\mathcal{B}(\\mathcal{H})$-valued version of Jaffards Lemma. That is, we show\nthat the Jaffard algebra of $\\mathcal{B}(\\mathcal{H})$-valued matrices, whose\noperator norms of their respective entries decay polynomially off the diagonal,\nis a Banach algebra which is inverse-closed in the Banach algebra\n$\\mathcal{B}(\\ell^2(X;\\mathcal{H}))$ of all bounded linear operators on\n$\\ell^2(X;\\mathcal{H})$, the Bochner-space of square-summable\n$\\mathcal{H}$-valued sequences.",
        "In this paper, we introduce and investigate multivariate versions of frequent\nstability and diam-mean equicontinuity. Given a natural number $m > 1$, we call\nthose notions \"frequent $m$-stability\" and \"diam-mean $m$-equicontinuity\". We\nuse these dynamical rigidity properties to characterise systems whose factor\nmap to the maximal equicontinuous factor (MEF) is finite-to-one for a residual\nset, called \"almost finite-to-one extensions\", or a set of full measure, called\n\"almost surely finite-to-one extensions\". In the case of a $\\sigma$-compact,\nlocally compact, abelian acting group it is shown that frequently\n$(m+1)$-stable systems are equivalently characterised as almost $m$-to-one\nextensions of their MEF. Similarly, it is shown that a system is diam-mean\n$(m+1)$-equicontinuous if and only if it is an almost surely $m$-to-one\nextension of its MEF.",
        "In this paper, we establish the global existence of a semi-linear class of\nhyperbolic equations in 3+1 dimensions, that satisfy the bounded weak null\ncondition. We propose a conformal compactification of the future directed\nnull-cone in Minkowski spacetime, enabling us to establish the solution to the\nwave equation in a neighbourhood of future null infinity. Using this framework,\nwe formulate a conformal symmetric hyperbolic Fuchsian system of equations. The\nexistence of solutions to this Fuchsian system follows from an application of\nthe existence theory developed in [1], and [2].",
        "Two temperature induced phase separation(2-TIPS) is a phenomenon observed in\nmixtures of active and passive particles modeled by scalar activity where the\ntemperature of the particle is proportional to its activity. The binary mixture\nof 'hot' and 'cold' particles phase separate when the relative temperature\ndifference between hot and cold particles defined as activity $\\chi$ exceeds a\ndensity dependent critical value. The study of kinetics in 2-TIPS, a\nnon-equilibrium phase separation, is of fundamental importance in statistical\nphysics. In this paper, we investigate 2-TIPS kinetics using molecular dynamics\n(MD) and coarse-grained (CG) modeling in 3D and 2D. The coarse-grained model\ncouples two passive Model B equations for hot and cold particles, with coupling\nterms emulating the energy transfer between them by raising the temperature of\ncold particles and lowering that of hot particles, a key observation from the\nMD simulations. MD simulations reveal that at high densities, phase separation\nbegins immediately after the quench, forming bi-continuous domains rich in hot\nor cold particles, similar to spinodal decomposition in passive systems. These\ninterconnected domains are also observed in the coarse-grained model for the\nmixture's critical composition. Both MD and CG models show dynamic scaling of\nthe correlation function, indicating self-similar domain growth. Regardless of\ndimensionality, both methods report algebraic growth in domain length with a\ngrowth exponent of $1\/3$, known as the Lifshitz-Slyozov exponent, widely\nobserved in passive systems. Our results demonstrate that the universality of\nphase separation kinetics observed in passive systems also extends to the\nnon-equilibrium binary mixture undergoing 2-TIPS.",
        "Layered $\\alpha$-In$_2$Se$_3$has been studied using a concomitant in-situ\nsynchrotron angle dispersive powder x-ray diffraction and Raman spectroscopy\nstudy in a diamond anvil cell up to 60+ GPa, at room temperature. Helium, that\nremains fairly hydrostatic up to the highest pressure in this study, was used\nas the pressure-transmitting medium. The results from both experimental methods\nreveal a pressure-induced structural phase transition from\n$\\alpha$-In$_2$Se$_3$ to a monoclinic $\\beta$'-In2Se3 structure at $\\approx$1\nGPa, in agreement with previous studies. Based on our detailed measurements\nusing both experimental techniques and F-f formalism, the $\\beta$'-In$_2$Se$_3$\nstructure remains stable up to 45 GPa, without a clear indication of a phase\ntransition towards the previously reported $\\beta$-In2Se3 phase. Above this\npressure, In$_2$Se$_3$ adopts a disordered solid-solution-like orthorhombic\nstructure, phase IV. The results are discussed in comparison with the relevant\nprevious studies of $\\alpha$-In$_2$Se$_3$ under pressure.",
        "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2\/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
        "As in other partial differential equations, one ends up with some arbitrary\nconstants or arbitrary functions when one integrates Einstein's equations, or\nmore generally field equations of any other gravity. Interpretation of these\narbitrary constants and functions as some physical quantities that can in\nprinciple be measured is a non-trivial matter. Concentrating on the case of\nconstants, one usually identifies them as conserved mass, momentum, angular\nmomentum, center of mass, or some other hairs of the solution. This can be done\nvia the Arnowitt-Deser-Misner (ADM)-type construction based on pure geometry;\nand the solution is typically a black hole. Hence, one talks about the black\nhole mass and angular momentum etc. Here we show that there are several\nmisunderstandings: First of all, the physical interpretation of the constants\nof a given geometry depends not only on pure geometry, i.e. the metric, but\nalso on the theory under-consideration. This becomes quite important especially\nwhen there is a cosmological constant. Secondly, one usually assigns the\nmaximally symmetric spacetime, say the flat or the (anti)-de Sitter spacetime,\nto have a zero mass and angular momentum and linear momentum. This declares the\nmaximally symmetric spacetime to be the vacuum of the theory, but such an\nassignment depends on the coordinates in the ADM-type constructions and their\nextensions: in fact, one can introduce large gauge transformations (new\ncoordinates) which map, say, the flat spacetime to flat spacetime but the\nresultant flat spacetime can have a nontrivial mass and angular momentum, if\nthe new coordinates are such that the metric components do not decay properly.\nThese issues, which are often overlooked, will be examined in detail, and a\nresolution, via the use of a divergence-free rank $(0,4)$-tensor will be shown\nfor the case of anti-de Sitter spacetimes.",
        "Quantum networks emerge as fundamental frameworks for addressing various\nlarge-scale problems. There are two primary architectures: space-based quantum\nnetworks, which deploy satellites with free space channels to interconnect\nusers, and ground-based quantum networks, which utilize optical fibers to\ninterconnect users. In this paper, we explore space-ground integrated quantum\nnetworks that incorporate both satellites and optical fibers into the\ninfrastructure. This integrated network features three forms of communication:\nusing only free space links, only ground links, or a hybrid usage of free space\nand ground links. We formulate the routing problem in space-ground integrated\nquantum networks as an integer programming and propose two solutions: using a\nlinear relaxation and a greedy algorithm. The linear relaxation algorithm\nallows timely scheduling of additional entanglement purification, whereas the\ngreedy algorithm enables quick scheduling. Simulation results demonstrate their\neffective balancing between network throughput and communication fidelity.",
        "Broken-symmetry (BS) approaches are widely employed to evaluate Heisenberg\nexchange parameters, primarily in combination with DFT calculations. For many\nmagnetic materials, BS-DFT calculations give reasonable estimations of exchange\nparameters although systematic failures have also been reported. While the\nlatter were attributed to deficiencies of approximate exchange-correlation\nfunctional, we prove here by treating a simple model system that the\nbroken-symmetry methodology has serious problems. Detailed analysis clarifies\nthe intrinsic issue with the broken-symmetry treatment of low-spin states. It\nshows, in particular, that the error in the BS calculation of exchange\nparameter scales with the degree of covalency between the magnetic and the\nbridging orbitals. As a possible tool to overcome this intrinsic drawback of\nsingle-determinant BS approaches, we propose their extension to a minimal\nmulticonfigurational version.",
        "In an era where financial markets are heavily influenced by many static and\ndynamic factors, it has become increasingly critical to carefully integrate\ndiverse data sources with machine learning for accurate stock price prediction.\nThis paper explores a multimodal machine learning approach for stock price\nprediction by combining data from diverse sources, including traditional\nfinancial metrics, tweets, and news articles. We capture real-time market\ndynamics and investor mood through sentiment analysis on these textual data\nusing both ChatGPT-4o and FinBERT models. We look at how these integrated data\nstreams augment predictions made with a standard Long Short-Term Memory (LSTM\nmodel) to illustrate the extent of performance gains. Our study's results\nindicate that incorporating the mentioned data sources considerably increases\nthe forecast effectiveness of the reference model by up to 5%. We also provide\ninsights into the individual and combined predictive capacities of these\nmodalities, highlighting the substantial impact of incorporating sentiment\nanalysis from tweets and news articles. This research offers a systematic and\neffective framework for applying multimodal data analytics techniques in\nfinancial time series forecasting that provides a new view for investors to\nleverage data for decision-making.",
        "Massive Boson Stars are self-gravitating configurations of self-interacting\nscalar fields. The equation of state of massive boson stars and their masses,\nradii, modeled by a self-interacting scalar field with potential of the form\n$V(\\phi) = \\frac{1}{2}m^2|\\phi|^2 + \\frac{1}{4}\\lambda |\\phi|^4$ are known to\nfollow scaling relations. The non-radial fundamental oscillations of such\nmassive BSs have been studied only for a few select model parameters so far. In\nthis work, we demonstrate for the first time that the $f$-mode characteristics\nalso follow a scaling in the strong interaction limit ($\\lambda \\gg\nm^2\/M_{Pl}^2$). This opens up the outstanding prospect of studying the\n$f$-modes of massive BSs throughout the scalar DM parameter space. We study the\nimplications of this finding by carrying out a detailed study of massive BS\n$f$-modes in a separate work. Here, having introduced this scaling, we use it\nto compare boson star oscillations with the neutron star and black hole\nquasinormal modes, thus providing a smoking gun for the distinguishability of\nBSs using gravitational waves.",
        "In this work we estimate the explosion and progenitor properties of six Type\nII supernovae (SNe) at 0.675 <= z <= 3.61 discovered by the JWST Advanced Deep\nExtragalactic Survey (JADES) transient survey by modeling their light curves.\nThis high-redshift Type II SN sample allows us to compare low-redshift Type II\nSNe to their high-redshift counterparts. Two Type II SNe are found to have high\nexplosion energies of 3e51 erg, while the other four Type II SNe are estimated\nto have typical explosion energies found in the local Universe [(0.5-2)e51\nerg]. The fraction of Type II SNe with high explosion energies might be higher\nat high redshifts because of, e.g., lower metallicity, but it is still\ndifficult to draw a firm conclusion because of the small sample size and\npotential observational biases. We found it difficult to constrain the\nprogenitor masses for Type II SNe in our sample because of the sparse\nlight-curve data. We found two Type II SN light curves can be better reproduced\nby introducing confined, dense circumstellar matter. Thus, the confined, dense\ncircumstellar matter frequently observed in nearby Type II SNe is likely to\nexist in Type II SNe at high redshifts as well. Two Type II SNe are estimated\nto have high host galaxy extinctions, showing the ability of JWST to discover\ndust-obscured SNe at high redshifts. More high-redshift Type II SNe are\nrequired to investigate the differences in the properties of Type II SNe near\nand far, but here we show the first glimpse into the high-redshift population\nof Type II SNe.",
        "Recent generative learning models applied to protein multiple sequence\nalignment (MSA) datasets include simple and interpretable physics-based Potts\ncovariation models and other machine learning models such as MSA-Transformer\n(MSA-T). The best models accurately reproduce MSA statistics induced by the\nbiophysical constraints within proteins, raising the question of which\nfunctional forms best model the underlying physics. The Potts model is usually\nspecified by an effective potential including pairwise residue-residue\ninteraction terms, but it has been suggested that MSA-T can capture the effects\ninduced by effective potentials which include more than pairwise interactions\nand implicitly account for phylogenetic structure in the MSA. Here we compare\nthe ability of the Potts model and MSA-T to reconstruct higher-order sequence\nstatistics reflecting complex biological sequence constraints. We find that the\nmodel performance depends greatly on the treatment of phylogenetic\nrelationships between the sequences, which can induce non-biophysical\nmutational covariation in MSAs. When using explicit corrections for\nphylogenetic dependencies, we find the Potts model outperforms MSA-T in\ndetecting epistatic interactions of biophysical origin.",
        "The behaviors of the modes for a massless minimally coupled scalar field are\ninvestigated for the Unruh state for Schwarzschild spacetime and the \"in\"\nvacuum state for a spacetime in which a spherically symmetric null shell\ncollapses to form a nonrotating black hole. In both cases there are two\ndifferent sets of solutions to the mode equation that make up the state. For\nboth spacetimes, one set of modes oscillates forever with no damping of the\noscillations and the other set approaches zero at late times. The difference\nbetween a mode that oscillates forever in the null-shell spacetime and the\ncorresponding mode for the Unruh state vanishes as a power law in time. The\nmodes that approach zero at late times also vanish at late times as a power law\nin time. In all cases the power-law damping is preceded by a period of\noscillations that appear to be due to quasi-normal modes.",
        "Granular materials are defined as collections of macroscopic dissipative\nparticles. Although these systems are ubiquitous in our lives, the nature and\nthe causes of their non-trivial collective dynamics still remain elusive and\nhave attracted significant interest in non-equilibrium physics. Here, we focus\non the vibrational dynamics of granular materials. While the vibrational\ndynamics of random packings have been examined concerning the jamming\ntransition, previous research has overlooked the role of contact dissipations.\nWe conducted numerical and analytical investigations into the vibrational\ndynamics of random packings influenced by the normal dissipative force, which\nis the simplest model for contact dissipations. Our findings reveal that the\nkinetic energy per mode diverges in the low-frequency range, following the\nscaling law $\\mathcal{K}_l \\propto \\omega^{-2}_l$ with the frequency\n$\\omega_l$, indicating that low-frequency modes experience strong excitation\nand that the equipartition of energy is violated. Additionally, the spatial\nstructure factor of the velocity field displays the scaling law $S_v(q) \\propto\nq^{-2}$ with the wavenumber $q$, which signifies that the velocity field has an\ninfinitely long range. We demonstrate that these phenomena arise from the\neffects of weaker damping on softer modes, where the particle displacements\nparallel to the contacts are minimal in the low-frequency modes, rendering\nnormal dissipation ineffective at dampening these modes.",
        "We investigate the origin of photometric variability in the classical T Tauri\nstar TW Hya by comparing light curves obtained by TESS and ground-based\ntelescopes with light curves created using three-dimensional (3D)\nmagnetohydrodynamic (MHD) simulations. TW Hya is modeled as a rotating star\nwith a dipole magnetic moment, slightly tilted about the rotational axis. We\nobserved that for various model parameters, matter accretes in the unstable\nregime and produces multiple hot spots on the star's surface, which leads to\nstochastic-looking light curves similar to the observed ones. Wavelet and\nFourier spectra of observed and modeled light curves show multiple\nquasiperiodic oscillations (QPOs) with quasiperiods from less than 0.1 to 9\ndays. Models show that variation in the strength and tilt of the dipole\nmagnetosphere leads to different periodograms, where the period of the star may\ndominate or be hidden. The amplitude of QPOs associated with the stellar period\ncan be smaller than that of other QPOs if the tilt of the dipole magnetosphere\nis small and when the unstable regime is stronger. In models with small\nmagnetospheres, the short-period QPOs associated with rotation of the inner\ndisc dominate and can be mistaken for a stellar period. We show that\nlonger-period (5-9 days) QPOs can be caused by waves forming beyond the\ncorotation radius.",
        "We report combined scanning probe microscopy and transport measurements to\ninvestigate the local electronic transport properties of reduced graphene oxide\n(rGO) devices. We demonstrate that the quantum transport properties in these\nmaterials can be significantly tuned by the electrostatic potential applied by\nan atomic force microscope (AFM) conducting tip. Scanning gate microscopy\nmeasurements show a distinct p-type response, where the AFM tip locally gates\nthe rGO, thereby modulating the transport current. Additional scanning\nimpedance microscopy measurements indicate shifts in the Fermi energy under\ndifferent gating conditions, highlighting the strong influence of local\nelectrostatic potentials on the transport characteristics of rGO. We\ndemonstrate that the interplay between the tip-induced Fermi level shifts and\ndefect-mediated scattering processes plays a key role in determining the\nobserved transport behavior. Our findings emphasize the crucial role of\nscattering mechanisms, particularly resonant scattering caused by impurities or\nstructural defects, in determining low-dimensional transport behavior in rGO.\nNotably, rGO exhibits resonant scattering effects akin to those seen in\none-dimensional systems like carbon nanotubes, despite its two-dimensional\nstructure. These insights advance our current understanding of charge transport\nin rGO, and have important implications for its use in nanoscale electronics,\nflexible sensors, and tunable optoelectronic devices.",
        "The electron cooler of the CRYRING@ESR storage ring at the GSI-FAIR\naccelerator complex is a unique instrument, which not only provides beam\ncooling of the stored ions, but also serves as a low-energy electron target for\nDielectronic Recombination experiments. The minimisation of vacuum\ncontamination and the response to rapid energy changes are key requirements of\nthe cooler in electron target mode. Therefore, a test bench was prepared to\nstudy the outgassing behaviour of the electron gun components and a setup was\nconstructed to evaluate the drift-electrode-modulation of the acceleration\nvoltage of the cooler. The vacuum studies showed that the electron gun cathode\nwas severely malfunctioning and resulted in its replacement. The\ndrift-electrode-modulation of the acceleration voltage showed significant\nimprovements compared to direct modulation of the terminal voltage of the\ncooler.",
        "Through an effective quantum field theory within Bogoliubov's framework and\ntaking into account nonuniversal effects of the interatomic potential we\nanalytically derive the leading Gaussian zero- and finite-temperature\ncorrections to the equation of state of ultracold interacting Bose-Bose gases.\nWe calculate the ground-state energy per particle at zero and low temperature\nfor three-, two- and one-dimensional two-component bosonic gases. By tuning the\nnonuniversal contribution to the interactions we address and establish\nconditions under which the formation and stability of a self-bound liquidlike\nphase or droplet with nonuniversal corrections to the interactions DNUC) is\nfavorable. At zero temperature in three-dimensions and considering the\nnonuniversal corrections to the attractive interactions as a fitting parameter\nthe energy per particle for DNUC is in good agreement with some diffusion Monte\nCarlo results. In two dimensions the DNUC present small deviations regarding\nconventional droplets. For the one-dimensional DNUC the handling of the\nnonuniversal effects to the interactions achieves a qualitative agreement with\nthe trend of some available Monte Carlo data in usual droplets. We also\nintroduce some improved Gross-Pitaevskii equations to describe self-trapped\nDNUC in three, two and one dimension. We briefly discuss some aspects at low\ntemperature regarding nonuniversal corrections to the interactions in Bose-Bose\ngases. We derive the dependencies on the nonuniversal contribution to the\ninteractions but also on the difference between intra- and inter-species\ncoupling constants. This last dependence crucially affect the three- and the\ntwo-dimensional DNUC driving thus to a thermal-induced instability. This\nthermal instability is also present in one-dimensional Bose-Bose gases, but it\nis not relevant on the formation of DNUC..."
      ]
    }
  },
  {
    "id":2411.17595,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Machine learning model to predict oncologic outcomes for drugs in randomized clinical trials",
    "start_abstract":"Abstract Predicting oncologic outcome is challenging due to the diversity of cancer histologies and complex network underlying biological factors. In this study, we determine whether machine learning (ML) can extract meaningful associations between clinical trial, drug\u2010related biomarker molecular profile information. We analyzed therapeutic trials corresponding 1102 outcomes from 104 758 patients with advanced colorectal adenocarcinoma, pancreatic melanoma nonsmall\u2010cell lung cancer. For each intervention arm, a dataset following attributes was curated: line treatment, number cytotoxic chemotherapies, small\u2010molecule inhibitors, or monoclonal antibody agents, drug class, alteration status arm's population, type, probability sensitivity (PDS) (integrating genomic, transcriptomic proteomic biomarkers in population interest) outcome. A total 467 progression\u2010free survival (PFS) 369 overall (OS) data points were used as training sets build our ML (random forest) model. Cross\u2010validation for PFS OS, obtaining correlation coefficients ( r ) 0.82 0.70, respectively (outcome vs model's parameters). 156 110 OS test sets. The Spearman s predicted actual statistically significant (PFS: = 0.879, OS: 0.878, P &lt; .0001). better arm 81% N 59\/73, z 5.24, .0001) 71% (OS: 37\/52, 2.91, .004) randomized trials. success algorithm predict may be exploitable model optimize trial design pharmaceutical agents.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Lint: Llm interaction network for clinical trial outcome prediction"
      ],
      "abstract":[
        "Clinical trial outcome prediction aims to predict the success probability of a clinical trial that reaches its desirable endpoint. Most of the effort focuses on developing machine learning models for making accurate predictions with diverse data sources, including clinical trial descriptions, drug molecules, and target disease conditions. Accurate trial outcome prediction helps trial planning and asset portfolio prioritization. Previous works have focused on small-molecule drugs; however, biologics are a quickly growing intervention type that lacks information that is traditionally known for drugs, like molecular properties. Additionally, traditional methods like graph neural networks are much more difficult to apply to biologics data which are a fast-growing type of drug. To address these points, we propose a Language Interaction Network (LINT), a novel method for trial outcome prediction using only free-text descriptions. We validate the effectiveness of LINT with thorough experiments across three trial phases. Specifically, LINT obtains 0.770, 0.740, and 0.748 ROC-AUC scores on phase I, II, and III, respectively, for clinical trials with biologic interventions."
      ],
      "categories":[
        "Clinical trials"
      ]
    },
    "list":{
      "title":[
        "Orbital Wigner functions and quantum transport in multiband systems",
        "Determination of $\\alpha_s(M_Z)$ via a high-precision effective coupling\n  $\\alpha^{g_1}_s(Q)$",
        "Some Constructions on Quantum Principal Bundles",
        "Dielectric nanotomography based on electrostatic force microscopy: A\n  numerical analysis",
        "Monotonicity of the Relative Entropy and the Two-sided Bogoliubov\n  Inequality in von Neumann Algebras",
        "Beyond Freeze-Out: A Novel Freeze-in Mechanism for Dark Matter via\n  Supercooled Phase Transitions",
        "Quantum Perspectivism vs Nietzschean Perspectivism",
        "Topological blocking at the Bi(111) surface due to surface relaxation",
        "Gate Tunable Josephson Diode Effect in Josephson Junctions made from\n  InAs Nanosheets",
        "Intrinsic regularity in the discrete log-Sobolev inequality",
        "Stability of the long-range corrected exchange-correlation functional in\n  time-dependent density-functional theory",
        "Study of amorphous alumina coatings for next-generation nuclear\n  reactors: hightemperature in-situ and post-mortem Raman spectroscopy and\n  X-ray diffraction",
        "Inside Out: Externalizing Assumptions in Data Analysis as Validation\n  Checks",
        "Quantum trajectories and Page-curve entanglement dynamics",
        "Thermodynamic properties of fcc lead: A scalar and fully relativistic\n  first principle study",
        "PLS-based approach for fair representation learning",
        "High Order Boundary Extrapolation Technique for Finite Difference\n  Methods on Complex Domains with Cartesian Meshes",
        "Relative phase between $s_{\\pm}$ superconducting order parameter\n  components in a two-band model with impurities",
        "Blind free deconvolution over one-parameter sparse families via\n  eigenmatrix",
        "Modelling lined rock caverns subject to hydrogen embrittlement and\n  cyclic pressurisation in fractured rock masses",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case",
        "Overdamped van der Waals Josephson junctions by area engineering",
        "Supertetragonal BaSnO3 induced giant ferroelectricity in SrTiO3\/BaSnO3\n  superlattices",
        "Entangled mixed-state datasets generation by quantum machine learning",
        "Avoiding Overfitting in Variable-Order Markov Models: a Cross-Validation\n  Approach",
        "Evidence of a diffuse, extended continuum source in quasars from the\n  relative sizes of the broad line region and the UV-optical continuum source\n  measured with microlensing",
        "The Gordon-Litherland pairing and its many applications",
        "The Convergence of Dynamic Routing between Capsules",
        "Magnon cat states in a cavity-magnon-qubit system via two-magnon driving\n  and dissipation"
      ],
      "abstract":[
        "Traditional theories of electron transport in crystals are based on the\nBoltzmann equation and do not capture physics arising from quantum coherence.\nWe introduce a transport formalism based on ''orbital Wigner functions'', which\naccurately captures quantum coherent physics in multiband fermionic systems. We\nillustrate the power of this approach compared to traditional semiclassical\ntransport theory by testing it numerically against microscopic simulations of\none-dimensional, non-interacting, two-band systems -- the simplest systems\ncapable of exhibiting inter-orbital coherence. We show that orbital Wigner\nfunctions accurately capture strongly non-equilibrium features of electron\ndynamics that lie beyond conventional Boltzmann theory, such as the ballistic\ntransport of a relative phase between microscopic orbitals and topological\nThouless pumping of charge both at non-zero temperature and away from the\nadiabatic limit. Our approach is motivated in part by modern ultracold atom\nexperiments that can prepare and measure far-from-equilibrium charge transport\nand phase coherence in multiband fermionic systems, calling for correspondingly\nprecise theories of transport. The quantitative accuracy exhibited by our\napproach, together with its capacity to capture nontrivial physics even at the\nballistic scale, establishes orbital Wigner functions as an ideal starting\npoint for developing a fully systematic theory of transport in crystals.",
        "We propose a novel method to determine the strong coupling of quantum\nchromodynamics (QCD) and fix its running behavior at all scales by using the\nBjorken sum rules (BSR). The BSR defines an effective coupling\n$\\alpha^{g_1}_s(Q)$ which includes the nonperturbative high-twist corrections\nand perturbative QCD (pQCD) corrections to the leading-twist part. For the\nleading-twist part of $\\alpha^{g_1}_s(Q)$, we adopt the infinite-order\nscale-setting procedure of the principle of maximum conformality\n($\\rm{PMC}_\\infty$) to deal with its pQCD corrections, which reveals the\nintrinsic conformality of series and eliminates conventional renormalization\nscheme-and-scale ambiguities. Using the $\\rm{PMC}_\\infty$ approach, we not only\neliminate \\textit{the first kind of residual scale dependence} due to\nuncalculated higher-order terms, but also resolve the previous\n``self-consistence problem\". The holographic light-front QCD model is used for\n$\\alpha^{g_1}_s(Q)$ in the infrared region, which also reveals a conformal\nbehavior at $Q\\to 0$. As a combination, we obtain a precise $\\alpha^{g_1}_s(Q)$\nat all scales, which matches well with the known experimental data with\n$p$-value $\\sim99\\%$, we determine the strong coupling constant at the critical\nscale $M_Z$, $\\alpha_s(M_Z)=0.1191\\pm{0.0012}\\mp0.0006$, where the first error\ncomes from $\\Delta\\kappa$ of LFHQCD model and the second error is from\n\\textit{the second kind of residual scale dependence} that is negligible.",
        "This paper works as an appendix of the paper titled Geometry of Associated\nQuantum Vector Bundles and the Quantum Gauge Group. Here, we are going to prove\nfour statements in the theory of quantum principal bundles:: 1) The universal\ndifferential envelope $\\ast$--calculus of a matrix (compact) Lie group, for the\nclassical bicovariant $\\ast$--First Order Differential Calculus, is the algebra\nof differential forms. 2) An example of a quantum principal bundle in which the\nspace of base forms is not generated by the base space. 3) The group\nisomorphism between convolution-invertible maps and covariant left module\nisomorphisms at the level of differential calculus 4) The way the maps $\\{T^V_k\n\\}$ from Remark 3.1 look in differential geometry.",
        "Electrostatic force microscopy (EFM) can image nanoscale objects buried below\nthe surface. Here, we theoretically show that this capability can be used to\nobtain nanotomographic information, i.e., the physical dimensions and\ndielectric properties, of buried nano-objects. These results constitute a first\nstep toward implementing a nondestructive dielectric nanotomography technique\nbased on EFM with applications in materials sciences and life sciences.",
        "This text studies, on the one hand, certain monotonicity properties of the\nAraki-Uhlmann relative entropy and, on the other hand, unbounded perturbation\ntheory of KMS-states which facilitates a proof of the two-sided Bogoliubov\ninequality in general von Neumann algebras. After introducing the necessary\nbackground from the theory of operator algebras and Tomita-Takesaki modular\ntheory, the relative entropy functional is defined and its basic properties are\nstudied. In particular, a full and detailed proof of Uhlmann's important\nmonotonicity theorem for the relative entropy is provided. This theorem will\nthen be used to derive a number of monotonicity inequalities for the relative\nentropy of normal functionals induced by vectors of the form $V \\varOmega, V\n\\varPhi \\in \\mathcal{H}$, where $V \\in \\mathscr{B}(\\mathcal{H})$ is a suitable\ntransformation. After that, an introduction to perturbation theory in von\nNeumann algebras is given, with an emphasis on unbounded perturbations of\nKMS-states following the framework of Derezi\\'{n}ski-Jak\\v{s}i\\'{c}-Pillet.\nThis mathematical apparatus will then be used to extend the two-sided\nBogoliubov inequality for the relative free energy, which was very recently\nproved for quantum-mechanical systems, to arbitrary von Neumann algebras.",
        "We investigate a novel freeze-in mechanism for Weakly Interacting Massive\nParticles (WIMPs), facilitated by a supercooled first-order phase transition\n(FOPT) in the early universe. Unlike the conventional freeze-out and freeze-in\nscenarios, this mechanism allows WIMPs to acquire their relic abundance through\na non-equilibrium production process following a rapid entropy injection. We\nexplore multiple dark matter (DM) candidates, including vector, fermionic, and\nscalar-mediated models, and identify fermionic DM with a pseudoscalar mediator\nas the most viable candidate for this mechanism. The study demonstrates that\nFOPT dilutes preexisting DM density and simultaneously induces a rapid mass\nincrease, preventing thermal re-equilibration and enabling DM production via\nfreeze-in. Additionally, we analyze the gravitational wave (GW) signals\nassociated with FOPT, identifying parameter regions detectable by future GW\nobservatories such as LISA and UDECIGO. This framework offers a compelling\nalternative to traditional WIMP and Feebly Interacting Massive Particle (FIMP)\nscenarios, providing new avenues for DM model building and experimental\nverification.",
        "This is a work of hard physical philosophy, where Quantum Perspectivism is\nshown to function as both an interpretation of quantum mechanics and a physical\nmodel for understanding Nietzsche's perspectivism. This framework combines\nquantum logic, the principle of complementarity, and contextuality to examine\nhow perspectives construct reality. In this model, measurements correspond to\nPerspectives and Meta-Perspectives, represented as Boolean subalgebras and\nHilbert sub-lattices within the Hilbert lattice, respectively. The Hilbert\nlattice itself is reinterpreted as Jung's Unus Mundus, a unified ontological\nreality. A metaphysical observation, made by a metaphysical observer, of a\ngiven system (World) is identified with the set of all corresponding\nmeta-perspectives in the Hilbert lattice\/Unus Mundus, the ocean of reality.\n  Perspectives, likened to islands in this ocean, correspond to single\nmeasurements of a system, capturing the logical structure of observed\nproperties. Meta-perspectives, analogous to continents, represent the synthesis\nof multiple measurements, providing a broader yet inherently incomplete\nunderstanding of the system. This structure emphasizes the complementarity and\ncontextual dependencies of measurements while exposing the limitations of\nclassical objectivity in the quantum domain. Advocating for a perspectival view\nof both the world and truth, Quantum Perspectivism unites quantum mechanics and\nNietzschean philosophy into a cohesive framework for exploring the interplay\nbetween consciousness, observation, and reality.",
        "The topological characteristics of Bi and its alloys with Sb have fueled\nintense debate since the prediction of three-dimensional topological\ninsulators. However, a definitive resolution has not been reached to date.\nHere, we provide theoretical evidence that surface relaxation conceals the\nunderlying bulk topology of pure Bi. Using density functional theory\ncalculations for thin Bi(111) films (up to 17 bilayers), we first demonstrate a\nsubstantial inter-bilayer expansion near the surface. Motivated by this\nfinding, we extend our analysis to thick Bi(111) films (up to 250 bilayers)\nincorporating relaxation layers, within the framework of a relativistic\nempirical tight-binding model. Our results reveal that these relaxation layers\ntopologically block the emergence of surface state and significantly suppress\nthe one-particle spectrum of surface states, thereby obscuring the experimental\nidentification of Bi's topological properties. This phenomenon, which we term\n\"topological blocking\", provides crucial insights into the long-standing\ndifficulty of observing surface states of Bi(111) at the $\\bar{M}$ point.\nFurthermore, it establishes a framework for understanding and predicting the\ntopological behavior in systems where surface relaxation disrupts the bulk-edge\ncorrespondence.",
        "We report the observation of Josephson diode effect (JDE) in hybrid devices\nmade from semiconductor InAs nanosheets and superconductor Al contacts. By\napplying an in-plane magnetic field ($B_{\\mathrm{xy}}$), we detect\nnon-reciprocal superconducting switching current as well as non-reciprocal\nsuperconducting retrapping current. The strength of the JDE depends on the\nangle between the in-plane magnetic field and the bias current\n($I_{\\mathrm{b}}$), reaching its maximum when $B_{\\mathrm{xy}} \\perp\nI_{\\mathrm{b}}$ and dropping to nearly zero when $B_{\\mathrm{xy}}\\parallel\nI_{\\mathrm{b}}$. Additionally, the diode efficiency is tunable via an\nelectrostatic gate with a complete suppression at certain gate voltages. Our\nfindings indicate that the observed JDE in InAs nanosheet-based Josephson\njunctions most likely arises from the Rashba spin-orbit interaction (SOI) in\nthe nanosheets. Such gate-tunable JDE in Josephson junctions made from\nsemiconductor material with SOI is useful not only for constructing advanced\nsuperconducting electronics but also for detecting novel superconducting\nstates.",
        "The chain rule lies at the heart of the powerful Gamma calculus for Markov\ndiffusions on manifolds, providing remarkable connections between several\nfundamental notions such as Bakry-\\'Emery curvature, entropy decay, and\nhypercontractivity. For Markov chains on finite state spaces, approximate\nversions of this chain rule have recently been put forward, with an extra cost\nthat depends on the log-Lipschitz regularity of the considered observable.\nMotivated by those findings, we here investigate the regularity of extremizers\nin the discrete log-Sobolev inequality. Specifically, we show that their\nlog-Lipschitz constant is bounded by a universal multiple of $\\log d$, where\n$d$ denotes the inverse of the smallest non-zero transition probability. As a\nconsequence, we deduce that the log-Sobolev constant of any reversible Markov\nchain on a finite state space is at least a universal multiple of $\\kappa\/\\log\nd$, where $\\kappa$ is the Bakry-\\'Emery curvature. This is a sharp discrete\nanalogue of what is perhaps the most emblematic application of the\nBakry-\\'Emery theory for diffusions. We also obtain a very simple proof of the\nmain result in \\cite{MR4620718}, which asserts that the log-Sobolev constant\nand its modified version agree up to a $\\log d$ factor. Our work consolidates\nthe role of the sparsity parameter $\\log d$ as a universal cost for\ntransferring results from Markov diffusions to discrete chains.",
        "Excitonic effects in the optical absorption spectra of solids can be\ndescribed with time-dependent density-functional theory (TDDFT) in the\nlinear-response regime, using a simple class of approximate, long-range\ncorrected (LRC) exchange-correlation functionals. It was recently demonstrated\nthat the LRC approximation can also be employed in real-time TDDFT to describe\nexciton dynamics. Here, we investigate the numerical stability of the\ntime-dependent LRC approach using a two-dimensional model solid. It is found\nthat the time-dependent Kohn-Sham equation with an LRC vector potential becomes\nmore and more prone to instabilities for increasing exciton binding energies.\nThe origin of these instabilities is traced back to time-averaged violations of\nthe zero-force theorem, which leads to a simple and robust numerical\nstabilization scheme. This explains and justifies a recently proposed method by\nDewhurst et al., arXiv:2401.16140.",
        "The present work focuses on the investigation of the thermal stability and\nstructural integrity of amorphous alumina coatings intended for use as\nprotective coatings on cladding tubes in Generation IV nuclear reactors,\nspecifically in the Lead-cooled Fast Reactor (LFR) type. Hightemperature Raman\nspectroscopy and high-temperature X-ray diffraction analyses were carried out\nup to 1050 C on a 5 um coating deposited by the pulsed laser deposition (PLD)\ntechnique on a 316L steel substrate. The experiments involved the in-situ\nexamination of structural changes in the material under increasing temperature,\nalong with ex-situ Raman imaging of the surface and cross-section of the\ncoating after thermal treatments of different lengths. As it was expected, the\npresence of alpha-alumina was detected with the addition of other polymorphs,\ngamma- and theta-Al2O3, found in the material after longer high-temperature\nexposure. The use of two structural analysis methods and two lasers excitation\nwavelengths with Raman spectroscopy allowed us to detect all the mentioned\nphases despite different mode activity. Alumina analysis was based on the\nemission spectra, while substrate oxidation products were identified through\nthe structural bands. The experiments depicted a dependence of the phase\ncomposition of oxidation products and alumina's degree of crystallization on\nthe length of the treatment. Nevertheless, the observed structural changes did\nnot occur rapidly, and the coating's integrity remained intact. Moreover,\noxidation signs occurred locally at temperatures exceeding the LFR reactor's\nworking temperature, confirming the material's great potential as a protective\ncoating in the operational conditions of LFR nuclear reactors.",
        "In data analysis, unexpected results often prompt researchers to revisit\ntheir procedures to identify potential issues. While some researchers may\nstruggle to identify the root causes, experienced researchers can often quickly\ndiagnose problems by checking a few key assumptions. These checked assumptions,\nor expectations, are typically informal, difficult to trace, and rarely\ndiscussed in publications. In this paper, we introduce the term *analysis\nvalidation checks* to formalize and externalize these informal assumptions. We\nthen introduce a procedure to identify a subset of checks that best predict the\noccurrence of unexpected outcomes, based on simulations of the original data.\nThe checks are evaluated in terms of accuracy, determined by binary\nclassification metrics, and independence, which measures the shared information\namong checks. We demonstrate this approach with a toy example using step count\ndata and a generalized linear model example examining the effect of particulate\nmatter air pollution on daily mortality.",
        "We consider time dynamics of entanglement entropy between a filled fermionic\nsystem and an empty reservoir. We consider scenarios (i) where the system is\nsubjected to a dephasing mechanism and the reservoir is clean, thereby\nemulating expansion of effectively interacting fermions in vacuum, and (ii)\nwhere both the system and the reservoir are subjected to dephasing and thereby\nenabling us to address how the entanglement between the part of the effectively\ninteracting system and its complement evolves in time. We consider two\ndifferent kinds of quantum trajectory approaches, namely stochastic unitary\nunraveling and quantum state diffusion. For both protocols, we observe and\ncharacterize the full Page curve-like dynamics for the entanglement entropy.\nDepending on the protocol and the setup, we observe very distinct\ncharacteristics of the Page curve and the associated Page time and Page value.\nWe also compute the number of fermions leaking to the reservoir and the\nassociated current and shed light on their plausible connections with\nentanglement entropy. Our findings are expected to hold for a wide variety of\ngeneric interacting quantum systems.",
        "This study investigates the thermodynamic properties of face-centered cubic\nlead (fcc-Pb) using ab-initio methods within the quasi-harmonic approximation\n(QHA), examining the influence of spin-orbit coupling (SOC) and the\nexchange-correlation functionals. Two types of ultrasoft pseudopotential\n(US-PP) are considered: one that excludes (scalar relativistic PP) and one that\nincludes the SOC effects (fully relativistic PP). Further, for each PP, we test\nthe performance of three popular exchange-correlation functionals:\nPerdew-Burke-Ernzerhof generalized gradient approximation (PBE) (Perdew et al.\nPhys. Rev. Lett. 77, 3865 (1996)), PBE modified for dense solids (PBEsol)\n(Perdew et al. Phys. Rev. Lett. 100, 136406 (2008)), and local density\napproximation (LDA) (Perdew et al. Phys. Rev. B 23, 5048 (1981)). We calculate\nthe Helmholtz free energy, incorporating lattice vibrations (phonons) and\nelectronic excitation contributions. The estimated equation of state (at 4 K\nand 301 K), phonon dispersions (at 100 K and 300 K), mode-Gr\\\"uneisen\nparameters ({\\gamma}q{\\eta}) (at 100 K), volume thermal expansion coefficient\n(\\b{eta}), isobaric heat capacity (CP), bulk modulus (BS), and thermodynamic\naverage Gr\\\"uneisen parameter ({\\gamma}) are compared with the available\nexperimental and theoretical studies. Moreover, the 0 K pressure-dependent\nelastic constant-coefficient (Cij) of fcc lead and Pugh ratio, Debye\ntemperature, and longitudinal and transverse sound velocities for\npolycrystalline lead are presented. The contributions of electronic excitations\nin all the thermodynamic properties are found to be negligible. With increasing\npressure, the role of spin-orbit effects decreases but does not vanish. Our\nfindings demonstrate that SOC leads to results distinct from the SR approach,\nbut agreement with the experiment is not consistently improved by including\nSOC.",
        "We revisit the problem of fair representation learning by proposing Fair\nPartial Least Squares (PLS) components. PLS is widely used in statistics to\nefficiently reduce the dimension of the data by providing representation\ntailored for the prediction. We propose a novel method to incorporate fairness\nconstraints in the construction of PLS components. This new algorithm provides\na feasible way to construct such features both in the linear and the non linear\ncase using kernel embeddings. The efficiency of our method is evaluated on\ndifferent datasets, and we prove its superiority with respect to standard fair\nPCA method.",
        "The application of suitable numerical boundary conditions for hyperbolic\nconservation laws on domains with complex geometry has become a problem with\ncertain difficulty that has been tackled in different ways according to the\nnature of the numerical methods and mesh type. In this paper we present a\ntechnique for the extrapolation of information from the interior of the\ncomputational domain to ghost cells designed for structured Cartesian meshes\n(which, as opposed to non-structured meshes, cannot be adapted to the\nmorphology of the domain boundary). This technique is based on the application\nof Lagrange interpolation with a filter for the detection of discontinuities\nthat permits a data dependent extrapolation, with higher order at smooth\nregions and essentially non oscillatory properties near discontinuities.",
        "We obtain solutions for Eliashberg equations within the Nambu representation\nfor a two-band model of iron-based superconductors with nonmagnetic impurities.\nTwo cases of a transition between $s_{\\pm}$ and $s_{++}$ states are considered:\n(i) the transition is accompanied by the abrupt change of the order parameter\nsign within one of the bands and (ii) the change is smooth. For both cases, we\nstudied the role of a gauge defined by the coefficients preceding the Pauli\nmatrices $\\hat\\tau_1$ and $\\hat\\tau_2$ in a self-energy expansion, which\ncorrespond to the components of the order parameter. We show that the absolute\nvalue of the order parameter is conserved for solutions in the clean and in the\nBorn limits. In an intermediate case, between the Born and unitary limits,\nresult depends on the solution for the clean limit. We show that a common gauge\nfor the Eliashberg equations in which one of the order parameter components\nvanishes is essential for adequate description of the multiband superconducting\nsystems.",
        "This note considers the blind free deconvolution problems of sparse spectral\nmeasures from one-parameter families. These problems pose significant\nchallenges since they involve nonlinear sparse recovery. The main technical\ntool is the eigenmatrix method for solving unstructured sparse recovery\nproblems. The key idea is to turn the nonlinear inverse problem into a linear\ninverse problem by leveraging the R-transform for free addition and the\nS-transform for free product. The resulting linear problem is solved with the\neigenmatrix method tailored to the domain of the parametric family. Numerical\nresults are provided for both the additive and multiplicative free\ndeconvolutions.",
        "The technology of lined rock cavern (LRC) with great geographical flexibility\nis a promising, cost-effective solution to underground hydrogen storage.\nHowever, the air-tight steel tanks used in this technology are susceptible to\nmaterial degradation due to hydrogen embrittlement (HE), potentially leading to\nleakage and structural failure, especial for LRCs constructed in complex\ngeological conditions. In this paper, we develop a 2D multiscale numerical\nmodel based on the finite element method to assess the impact of HE on the LRC\nperformance in fractured rock masses under cyclic gas pressurisation. Within\nthis framework, a large-scale model is used to simulate the deformation and\ndamage evolution of both fractured rock and an LRC under in-situ stresses and\ninternal gas pressurisation, while a small-scale model captures HE in the steel\nlining of the LRC. Our simulations reveal that damage in the rock, concrete,\nand steel degradation is strongly affected by pre-existing fractures and\nin-situ stresses. Our results also reveal the presence of a strong positive\nfeedback between hydrogen concentration and stress redistribution in the steel\nlining. Moreover, a comparison between models with and without considering HE\nilluminates that hydrogen concentration significantly contributes to steel\ndegradation, particularly during the long-term LRC operation, highlighting the\ncritical role of HE in the safety and performance of the LRC. The findings and\ninsights obtained from our work have important implications for the design\noptimisation and performance assessment of LRCs for sustainable underground\nhydrogen storage.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift.",
        "Van der Waals (vW) Josephson junctions (JJs) realized by stacking materials\nsuch as few-layered NbSe2, offers a new landscape to realize superconducting\nquantum devices with superior properties owing to its crystalline nature and\ndefect-free junctions. For quantum technology, overdamped JJs are highly\nsought-after, whose realization demands precise control of junction capacitance\nby engineering the junction area using microfabrication techniques. NbSe2 is\nhighly reactive and susceptible to damage during microfabrication processes. In\nthis manuscript, we demonstrate both underdamped and overdamped NbSe2-NbSe2 JJs\nby controlling the junction area. We devise a minimally invasive\nmicrofabrication procedure, post-junction formation, to precisely control the\njunction area. The McCumber parameter characterizing the damping is extracted\nfrom the electrical transport measurements down to 130 mK. The results show\nthat our sample fabrication recipe has preserved the material qualities and\npaved the way for the realization of scalable JJ devices on NbSe2 and similar\nsystems.",
        "Perovskite BaSnO3 has an Sn s-orbital conduction band minimum, which makes it\nof interest as a transparent-conducting oxide parent compound but also\ncontraindicates the ferroelectric instability characteristic of the related\ncompound BaTiO3. In this work, we studied the effect of (001) compressive\nstrain on BaSnO3 using first-principles methods. We found that, with low\ncompressive strain, symmetry breaking takes cubic BaSnO3 to a nonpolar\ntetragonal state, with a first-order phase transition to a hidden\nhighly-polarized ferroelectric supertetragonal state at about -5%. Based on the\nfacts that the mismatch of lattice constant in experiment between BaSnO3 and\nSrTiO3 is about -5.2% and coherent growth of BaSnO3 on SrTiO3 has been\nexperimentally realized for BaSnO3 layers thinner than 3 unit-cells, we studied\na series of SrTiO3\/BaSnO3 superlattices with one or two unit-cells of BaSnO3\nand several unit-cells of SrTiO3. We found that the superlattices are\nferroelectric with large polarizations. We propose that the origin of\nferroelectricity in the superlattices is the mechanical and electrical coupling\nof the BaSnO3 and SrTiO3 layers, with polarized supertetragonal state of BaSnO3\ninduced by compressive-strain from the SrTiO3 layers and polarization of the\nSrTiO3 layers by the polar BaSnO3 layers. Due to the distinctive electronic\nstates in the BaSnO3 layers, the realization of ferroelectricity holds promise\nfor the design of novel electronic devices.",
        "The advancement of classical machine learning is inherently linked to the\nestablishment and progression of classical dataset. In quantum machine learning\n(QML), there is an analogous imperative for the development of quantum\nentangled datasets comprised with huge quantity and high quality. Especially\nfor multipartite mixed-state datasets, due to the lack of suitable entanglement\ncriteria, previous researchers often could only perform classification tasks on\ndatasets extended based on Werner states or other well-structured states. This\npaper is dedicated to provide a method for generating mixed-state datasets for\nentangled-separable classification tasks. This method is based on supervised\nquantum machine learning and the concentratable entanglement measures. It\nfurthers the assembly of quantum entangled datasets, inspires the discovery of\nnew entanglement criteria with both classical and quantum machine learning, and\nprovides a valuable resource for benchmarking QML models, thereby opening new\navenues for exploring the rich structure of quantum entanglement in mixed\nstates. Additionally, we benchmark several machine learning models using this\ndataset, offering guidance and suggestions for the selection of QML models.",
        "Higher$\\text{-}$order Markov chain models are widely used to represent agent\ntransitions in dynamic systems, such as passengers in transport networks. They\ncapture transitions in complex systems by considering not only the current\nstate but also the path of previously visited states. For example, the\nlikelihood of train passengers traveling from Paris (current state) to Rome\ncould increase significantly if their journey originated in Italy (prior\nstate). Although this approach provides a more faithful representation of the\nsystem than first$\\text{-}$order models, we find that commonly used\nmethods$-$relying on Kullback$\\text{-}$Leibler divergence$-$frequently overfit\nthe data, mistaking fluctuations for higher$\\text{-}$order dependencies and\nundermining forecasts and resource allocation. Here, we introduce DIVOP\n(Detection of Informative Variable$\\text{-}$Order Paths), an algorithm that\nemploys cross$\\text{-}$validation to robustly distinguish meaningful\nhigher$\\text{-}$order dependencies from noise. In both synthetic and\nreal$\\text{-}$world datasets, DIVOP outperforms two\nstate$\\text{-}$of$\\text{-}$the$\\text{-}$art algorithms by achieving higher\nprecision, recall, and sparser representations of the underlying dynamics. When\napplied to global corporate ownership data, DIVOP reveals that tax havens\nappear in 82$\\%$ of all significant higher$\\text{-}$order dependencies,\nunderscoring their outsized influence in corporate networks. By mitigating\noverfitting, DIVOP enables more reliable multi$\\text{-}$step predictions and\ndecision$\\text{-}$making, paving the way toward deeper insights into the hidden\nstructures that drive modern interconnected systems.",
        "Microlensing by stars in the lens galaxy of a gravitationally lensed quasar\nis a phenomenon that can selectively magnify quasar subregions, producing\nobservable changes in the continuum brightness or distortions in the emission\nline profiles. Hence, microlensing allows us to probe the inner quasar regions.\nIn this paper, we report measurements of the ratio of the broad emission line\nregion (BLR) radius to the continuum source radius in eight lensed quasars, for\nthe CIV, MgII, and H$\\alpha$ emission lines and their respective underlying\ncontinua at $\\lambda\\lambda$ 1550\\AA , 2800\\AA , and 6563 \\AA . The\nmicrolensing-induced line profile distortions and continuum magnifications were\nobserved in the same single-epoch datasets, and simultaneously compared with\nmicrolensing simulations. We found that, on average, the inner radius of the\nBLR starts at the end of the UV-optical continuum source, independently of the\nline ionization and the wavelength of the continuum. The half-light radius of\nthe BLR is, on average, a factor of six larger than the half-light radius of\nthe continuum source, independently of the quasar's bolometric luminosity. We\nalso found a correlation between the BLR radius and the continuum source\nradius, supporting the idea that the dominant contribution to the UV-optical\ncontinuum may come from the BLR itself. Our results independently confirm the\nresults of reverberation mapping studies, and extend them to higher-redshift,\nhigher-luminosity quasars.",
        "Gordon and Litherland's paper $\\textit{On the Signature of a link}$\nintroduced a bilinear form that simultaneously unifies both the quadratic forms\nof Trotter and Goeritz. This remarkable pairing of combinatorics and topology\nhas had widespread application in low-dimensional topology. In this expository\nnote, we give a picture proof (via Kirby diagrams) of their main result and\ndiscuss the numerous ways their theorem has been put to good use.",
        "Capsule networks(CapsNet) are recently proposed neural network models with\nnew processing layers, specifically for entity representation and discovery of\nimages. It is well known that CapsNet have some advantages over traditional\nneural networks, especially in generalization capability. At the same time,\nsome studies report negative experimental results. The causes of this\ncontradiction have not been thoroughly analyzed. The preliminary experimental\nresults show that the behavior of routing algorithms does not always produce\ngood results as expected, and in most cases, different routing algorithms do\nnot change the classification results, but simply polarize the link strength,\nespecially when they continue to repeat without stopping. To realize the true\npotential of the CapsNet, deep mathematical analysis of the routing algorithms\nis crucial. In this paper, we will give the objective function that is\nminimized by the dynamic routing algorithm, which is a concave function. The\ndynamic routing algorithm can be regarded as nonlinear gradient method to\nsolving an optimization algorithm under linear constraints, and its convergence\ncan be strictly proved mathematically. Furthermore, the mathematically rigorous\nproof of the convergence is given for this class of iterative routing\nprocedures. We analyze the relation between the objective function and the\nconstraints solved by the dynamic routing algorithm in detail, and perform the\ncorresponding routing experiment to analyze the effect of our convergence\nproof.",
        "We propose an efficient method for dissipative generation of magnonic cat\nstates in a cavity-magnon-qubit hybrid system by exploiting a two-magnon\ndriving and dissipation mechanism. When both the magnon and qubit are driven, a\ncoherent nonlinear two-magnon interaction is induced, wherein the qubit and the\nmagnon mode exchange energy through magnon pairs. The dissipation of the qubit\nis exploited to steer the magnon mode into a quantum superposition of distinct\ncoherent states, where the magnon mode evolves into either an even or odd cat\nstate, depending on the parity of the magnon initial state. For the case where\nthe magnon initial state is a superposition state, e.g., of $|0\\rangle$ and\n$|1\\rangle$, the magnon mode can evolve into a weighted mixture of the even and\nodd cat states. We also find that magnon squeezed states may emerge during the\nshort-time evolution, showcasing the capability of our mechanism in preparing\ndiverse magnon non-classical states. Magnonic cat and squeezed states are\nmacroscopic quantum states and find applications in macroscopic quantum studies\nand quantum sensing, e.g., in the dark matter search using ferromagnetic axion\nhaloscopes."
      ]
    }
  },
  {
    "id":2411.17595,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Lint: Llm interaction network for clinical trial outcome prediction",
    "start_abstract":"Clinical trial outcome prediction aims to predict the success probability of a clinical trial that reaches its desirable endpoint. Most of the effort focuses on developing machine learning models for making accurate predictions with diverse data sources, including clinical trial descriptions, drug molecules, and target disease conditions. Accurate trial outcome prediction helps trial planning and asset portfolio prioritization. Previous works have focused on small-molecule drugs; however, biologics are a quickly growing intervention type that lacks information that is traditionally known for drugs, like molecular properties. Additionally, traditional methods like graph neural networks are much more difficult to apply to biologics data which are a fast-growing type of drug. To address these points, we propose a Language Interaction Network (LINT), a novel method for trial outcome prediction using only free-text descriptions. We validate the effectiveness of LINT with thorough experiments across three trial phases. Specifically, LINT obtains 0.770, 0.740, and 0.748 ROC-AUC scores on phase I, II, and III, respectively, for clinical trials with biologic interventions.",
    "start_categories":[
      "Clinical trials"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Machine learning model to predict oncologic outcomes for drugs in randomized clinical trials"
      ],
      "abstract":[
        "Abstract Predicting oncologic outcome is challenging due to the diversity of cancer histologies and complex network underlying biological factors. In this study, we determine whether machine learning (ML) can extract meaningful associations between clinical trial, drug\u2010related biomarker molecular profile information. We analyzed therapeutic trials corresponding 1102 outcomes from 104 758 patients with advanced colorectal adenocarcinoma, pancreatic melanoma nonsmall\u2010cell lung cancer. For each intervention arm, a dataset following attributes was curated: line treatment, number cytotoxic chemotherapies, small\u2010molecule inhibitors, or monoclonal antibody agents, drug class, alteration status arm's population, type, probability sensitivity (PDS) (integrating genomic, transcriptomic proteomic biomarkers in population interest) outcome. A total 467 progression\u2010free survival (PFS) 369 overall (OS) data points were used as training sets build our ML (random forest) model. Cross\u2010validation for PFS OS, obtaining correlation coefficients ( r ) 0.82 0.70, respectively (outcome vs model's parameters). 156 110 OS test sets. The Spearman s predicted actual statistically significant (PFS: = 0.879, OS: 0.878, P &lt; .0001). better arm 81% N 59\/73, z 5.24, .0001) 71% (OS: 37\/52, 2.91, .004) randomized trials. success algorithm predict may be exploitable model optimize trial design pharmaceutical agents."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Uniform local well-posedness and liviscid limit for the KdV-Burgers\n  equation on $\\mathbb{T}$",
        "What is a Social Media Bot? A Global Comparison of Bot and Human\n  Characteristics",
        "SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding",
        "FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous\n  User Data",
        "Low-energy insulating reconstructions of Si(111)-7x7 surface with and\n  without stacking fault discovered by graph theory",
        "Arbitrary control of the flow of light using pseudomagnetic fields in\n  photonic crystals at telecommunication wavelengths",
        "Advancing Medical Representation Learning Through High-Quality Data",
        "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models",
        "Output-Feedback Full-State Targeting Model Predictive Control for\n  Station-Keeping on Near-Rectilinear Halo Orbits",
        "The multiple Markov theorem on Angelesco sets",
        "Impact of Nonreciprocal Hopping on Localization in Non-Hermitian\n  Quasiperiodic Systems",
        "Efficient Gradient-Based Inference for Manipulation Planning in Contact\n  Factor Graphs",
        "Enhanced collective vibrations in granular materials",
        "Efficient Traffic Prediction Through Spatio-Temporal Distillation",
        "Forgetting Any Data at Any Time: A Theoretically Certified Unlearning\n  Framework for Vertical Federated Learning",
        "Minimax Rate-Optimal Inference for Individualized Quantile Treatment\n  Effects in High-dimensional Models",
        "Anomalous exchange correlation of quasiparticles with entangled Nambu\n  spinors",
        "Photometric Determination of Unresolved Main-sequence Binaries in the\n  Pleiades: Binary Fraction and Mass Ratio Distribution",
        "Stone Soup Multi-Target Tracking Feature Extraction For Autonomous\n  Search And Track In Deep Reinforcement Learning Environment",
        "Load-Balancing versus Anycast: A First Look at Operational Challenges",
        "Decentralized Personalization for Federated Medical Image Segmentation\n  via Gossip Contrastive Mutual Learning",
        "The Query\/Hit Model for Sequential Hypothesis Testing",
        "Cryogenic Nano-Imaging of Excitons in a Monolayer Semiconductor",
        "Searching for continuous gravitational waves from slowly spinning\n  neutron stars with DECIGO, Big Bang Observer, Einstein Telescope and Cosmic\n  Explorer",
        "Construction of a Small-Scale Vacuum Generation System and Using It as\n  an Educational Device to Demonstrate Features of the Vacuum",
        "Understanding and Mitigating Membership Inference Risks of Neural\n  Ordinary Differential Equations",
        "Singularities of two-dimensional Nijenhuis operators",
        "Overcoming user-rate limit of quantum network",
        "ELIZA Reanimated: The world's first chatbot restored on the world's\n  first time sharing system"
      ],
      "abstract":[
        "This article investigates the uniform well-posedness and inviscid limit\nproblem for the Korteweg-de Vries-Burgers equation on a torus, we consider the\nKdV-Burgers equation $$\\partial_t\nu(t,x)+\\partial_x^3u(t,x)-\\varepsilon\\partial_x^2u(t,x)=\\dfrac{1}{2}\\partial_x(u(t,x))^2,\n\\quad u(0)=\\phi, $$ where $\\varepsilon\\in(0, 1]$ represents the diffusion\ncoefficient, and $u(t,x):\\mathbb{R}^{+}\\times\\mathbb{T}\\rightarrow \\mathbb{R}$\nis a real-valued function, we show that it is uniformly local well-posed in\n$H^s$ with $s\\geq 0$ for all $\\varepsilon\\in[0,1]$. Moreover, we prove that\nthere exists some $T>0$ such that for any $s\\geq 0$, its solution converges in\n$C([0,T];H^s)$ to that of the KdV equation if $\\varepsilon$ tends to $0$.",
        "Chatter on social media is 20% bots and 80% humans. Chatter by bots and\nhumans is consistently different: bots tend to use linguistic cues that can be\neasily automated while humans use cues that require dialogue understanding.\nBots use words that match the identities they choose to present, while humans\nmay send messages that are not related to the identities they present. Bots and\nhumans differ in their communication structure: sampled bots have a star\ninteraction structure, while sampled humans have a hierarchical structure.\nThese conclusions are based on a large-scale analysis of social media tweets\nacross ~200mil users across 7 events. Social media bots took the world by storm\nwhen social-cybersecurity researchers realized that social media users not only\nconsisted of humans but also of artificial agents called bots. These bots wreck\nhavoc online by spreading disinformation and manipulating narratives. Most\nresearch on bots are based on special-purposed definitions, mostly predicated\non the event studied. This article first begins by asking, \"What is a bot?\",\nand we study the underlying principles of how bots are different from humans.\nWe develop a first-principle definition of a social media bot. With this\ndefinition as a premise, we systematically compare characteristics between bots\nand humans across global events, and reflect on how the software-programmed bot\nis an Artificial Intelligent algorithm, and its potential for evolution as\ntechnology advances. Based on our results, we provide recommendations for the\nuse and regulation of bots. Finally, we discuss open challenges and future\ndirections: Detect, to systematically identify these automated and potentially\nevolving bots; Differentiate, to evaluate the goodness of the bot in terms of\ntheir content postings and relationship interactions; Disrupt, to moderate the\nimpact of malicious bots.",
        "The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest\nand collision-free paths for multiple agents in a known, potentially\nobstacle-ridden environment. It is the core challenge for robotic deployments\nin large-scale logistics and transportation. Decentralized learning-based\napproaches have shown great potential for addressing the MAPF problems,\noffering more reactive and scalable solutions. However, existing learning-based\nMAPF methods usually rely on agents making decisions based on a limited field\nof view (FOV), resulting in short-sighted policies and inefficient cooperation\nin complex scenarios. There, a critical challenge is to achieve consensus on\npotential movements between agents based on limited observations and\ncommunications. To tackle this challenge, we introduce a new framework that\napplies sheaf theory to decentralized deep reinforcement learning, enabling\nagents to learn geometric cross-dependencies between each other through local\nconsensus and utilize them for tightly cooperative decision-making. In\nparticular, sheaf theory provides a mathematical proof of conditions for\nachieving global consensus through local observation. Inspired by this, we\nincorporate a neural network to approximately model the consensus in latent\nspace based on sheaf theory and train it through self-supervised learning.\nDuring the task, in addition to normal features for MAPF as in previous works,\neach agent distributedly reasons about a learned consensus feature, leading to\nefficient cooperation on pathfinding and collision avoidance. As a result, our\nproposed method demonstrates significant improvements over state-of-the-art\nlearning-based MAPF planners, especially in relatively large and complex\nscenarios, demonstrating its superiority over baselines in various simulations\nand real-world robot experiments.",
        "Mobile agents have attracted tremendous research participation recently.\nTraditional approaches to mobile agent training rely on centralized data\ncollection, leading to high cost and limited scalability. Distributed training\nutilizing federated learning offers an alternative by harnessing real-world\nuser data, providing scalability and reducing costs. However, pivotal\nchallenges, including the absence of standardized benchmarks, hinder progress\nin this field.\n  To tackle the challenges, we introduce FedMABench, the first benchmark for\nfederated training and evaluation of mobile agents, specifically designed for\nheterogeneous scenarios. FedMABench features 6 datasets with 30+ subsets, 8\nfederated algorithms, 10+ base models, and over 800 apps across 5 categories,\nproviding a comprehensive framework for evaluating mobile agents across diverse\nenvironments. Through extensive experiments, we uncover several key insights:\nfederated algorithms consistently outperform local training; the distribution\nof specific apps plays a crucial role in heterogeneity; and, even apps from\ndistinct categories can exhibit correlations during training. FedMABench is\npublicly available at: https:\/\/github.com\/wwh0411\/FedMABench with the datasets\nat: https:\/\/huggingface.co\/datasets\/wwh0411\/FedMABench.",
        "The 7x7 reconstruction of Si(111) surface is one of the most fascinating\nconfiguration in nature, whose STM image has been well-understood by the famous\ndimer-adatom-stacking-fault model (DAS). However, the electronic property of\nthe DAS model is always confirmed to be metallic by first-principles\ncalculations, while some experiments detected insulating features. It is still\nchallenge to predict DAS-like reconstructions through traditional method to\nsolve such a puzzle. Here, we show that 7x7 reconstructions can be quickly\ndiscovered by graph theory as implemented in the graph-space based RG2 code for\ncrystal structure prediction. Two groups of reconstructions with (DAS-d8-T12,\nDAS-d8-T9H3-A, DAS-d8-T9H3-B and DAS-d8-T6H6) and without (AB-d10-T12,\nAB-d10-T9H3, AA-d10-T12 and AA-d10-T9H3) stacking-fault are discovered. They\npossess energetic stabilities comparable to the well-known DAS (DAS-d8-T12) and\nshow similar STM patterns, providing a plausible explanation for the\nexperimentally observed 7x7 reconstruction on the Si(111) surface. The\nfirst-principles calculations show that DAS-d8-T12, DAS-d8-T6H6, AB-d10-T12,\nand AA-d10-T12 are metallic, while DAS-d8-T9H3-A, DAS-d8-T9H3-B, AB-d10-T9H3\nand AA-d10-T9H3 are insulating phases with gaps of 0.043 eV, 0.182 eV, 0.043 eV\nand 0.059 eV, respectively. Our work demonstrates the predictability of the\nSi(111)-7x7 reconstruction and provides the structural candidates for\nunderstanding the experimentally observed metal-to-insulator transition.",
        "In photonics, the idea of controlling light in a similar way that magnetic\nfields control electrons has always been attractive. It can be realized by\nsynthesizing pseudomagnetic fields (PMFs) in photonic crystals (PhCs). Previous\nworks mainly focus on the Landau levels and the robust transport of the chiral\nstates. More versatile control over light using complex nonuniform PMFs such as\nthe flexible splitting and routing of light has been elusive, which hinders\ntheir application in practical photonic integrated circuits. Here we propose an\nuniversal and systematic methodology to design nonuniform PMFs and arbitrarily\ncontrol the flow of light in silicon PhCs at telecommunication wavelengths. As\nproofs of concept, a low-loss S-bend and a highly efficient 50:50 power\nsplitter based on PMFs are experimentally demonstrated. A high-speed data\ntransmission experiment is performed on these devices to prove their\napplicability in real communication systems. The proposed method offers a new\nparadigm for the exploration of fundamental physics and the development of\nnovel nanophotonic devices.",
        "Despite the growing scale of medical Vision-Language datasets, the impact of\ndataset quality on model performance remains under-explored. We introduce\nOpen-PMC, a high-quality medical dataset from PubMed Central, containing 2.2\nmillion image-text pairs, enriched with image modality annotations, subfigures,\nand summarized in-text references. Notably, the in-text references provide\nricher medical context, extending beyond the abstract information typically\nfound in captions. Through extensive experiments, we benchmark Open-PMC against\nlarger datasets across retrieval and zero-shot classification tasks. Our\nresults show that dataset quality-not just size-drives significant performance\ngains. We complement our benchmark with an in-depth analysis of feature\nrepresentation. Our findings highlight the crucial role of data curation\nquality in advancing multimodal medical AI. We release Open-PMC, along with the\ntrained models and our codebase.",
        "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
        "We develop a model predictive control (MPC) policy for station-keeping (SK)\non a Near-Rectilinear Halo Orbit (NRHO). The proposed policy achieves\nfull-state tracking of a reference NRHO via a two-maneuver control horizon\nplaced one revolution apart. Our method abides by the typical mission\nrequirement that at most one maneuver is used for SK during each NRHO\nrevolution. Simultaneously, the policy has sufficient controllability for\nfull-state tracking, making it immune to phase deviation issues in the\nalong-track direction of the reference NRHO, a common drawback of existing SK\nmethods with a single maneuver per revolution. We report numerical simulations\nwith a navigation filter to demonstrate the MPC's performance with output\nfeedback. Our approach successfully maintains the spacecraft's motion in the\nvicinity of the reference in both space and phase, with tighter tracking than\nstate-of-the-art SK methods and comparable delta-V performance.",
        "By addressing a long-standing open problem, listed in a highly regarded\ncollection of open questions in the field and described as a \"worthwhile\nresearch project\", this note extends Markov's theorem (Markoff, Math. Ann.,\n27:177-182, 1886) on the variation of zeros of orthogonal polynomials on the\nreal line to the setting of multiple orthogonal polynomials on Angelesco sets.\nThe analysis reveals that the only distinction from the classical 1886 result\nlies in establishing sufficient conditions for a given\n$\\mathcal{Z}$-matrix--which, in the Markov case, is the identity matrix--to be\nan $\\mathcal{M}$-matrix. In contrast to most existing studies, which often\npresent highly technical proofs for specific results, this note seeks to\nprovide a simple proof of a general result without imposing restrictions on the\nweight functions (such as their potential \"classical\" nature), the number of\nintervals, or the structure of the partition.",
        "We study the non-Hermitian Aubry-Andr\\'e-Harper model, incorporating complex\nphase modulation, unmodulated and modulated nonreciprocal hopping. Using\nAvila's global theory, we derive analytical phase boundaries and map out the\nphase diagrams, revealing extended, localized, critical, and skin phases unique\nto non-Hermitian systems. For complex phase modulation, we determine\nlocalization lengths through Lyapunov exponents and show that topological\ntransitions align with localization transitions. In the nonreciprocal case, we\nuse similarity transformations to confirm phase boundaries consistent with\nAvila's theory and uncover asymmetric localization behaviors. Importantly,\nmodulated nonreciprocal hopping transforms both extended and critical phases\ninto skin phases under open boundary conditions. These results highlight the\ninterplay between topology, localization, and non-Hermitian effects, offering\nnew perspectives on quasiperiodic systems.",
        "This paper presents a framework designed to tackle a range of planning\nproblems arise in manipulation, which typically involve complex\ngeometric-physical reasoning related to contact and dynamic constraints. We\nintroduce the Contact Factor Graph (CFG) to graphically model these diverse\nfactors, enabling us to perform inference on the graphs to approximate the\ndistribution and sample appropriate solutions. We propose a novel approach that\ncan incorporate various phenomena of contact manipulation as differentiable\nfactors, and develop an efficient inference algorithm for CFG that leverages\nthis differentiability along with the conditional probabilities arising from\nthe structured nature of contact. Our results demonstrate the capability of our\nframework in generating viable samples and approximating posterior\ndistributions for various manipulation scenarios.",
        "Granular materials are defined as collections of macroscopic dissipative\nparticles. Although these systems are ubiquitous in our lives, the nature and\nthe causes of their non-trivial collective dynamics still remain elusive and\nhave attracted significant interest in non-equilibrium physics. Here, we focus\non the vibrational dynamics of granular materials. While the vibrational\ndynamics of random packings have been examined concerning the jamming\ntransition, previous research has overlooked the role of contact dissipations.\nWe conducted numerical and analytical investigations into the vibrational\ndynamics of random packings influenced by the normal dissipative force, which\nis the simplest model for contact dissipations. Our findings reveal that the\nkinetic energy per mode diverges in the low-frequency range, following the\nscaling law $\\mathcal{K}_l \\propto \\omega^{-2}_l$ with the frequency\n$\\omega_l$, indicating that low-frequency modes experience strong excitation\nand that the equipartition of energy is violated. Additionally, the spatial\nstructure factor of the velocity field displays the scaling law $S_v(q) \\propto\nq^{-2}$ with the wavenumber $q$, which signifies that the velocity field has an\ninfinitely long range. We demonstrate that these phenomena arise from the\neffects of weaker damping on softer modes, where the particle displacements\nparallel to the contacts are minimal in the low-frequency modes, rendering\nnormal dissipation ineffective at dampening these modes.",
        "Graph neural networks (GNNs) have gained considerable attention in recent\nyears for traffic flow prediction due to their ability to learn spatio-temporal\npattern representations through a graph-based message-passing framework.\nAlthough GNNs have shown great promise in handling traffic datasets, their\ndeployment in real-life applications has been hindered by scalability\nconstraints arising from high-order message passing. Additionally, the\nover-smoothing problem of GNNs may lead to indistinguishable region\nrepresentations as the number of layers increases, resulting in performance\ndegradation. To address these challenges, we propose a new knowledge\ndistillation paradigm termed LightST that transfers spatial and temporal\nknowledge from a high-capacity teacher to a lightweight student. Specifically,\nwe introduce a spatio-temporal knowledge distillation framework that helps\nstudent MLPs capture graph-structured global spatio-temporal patterns while\nalleviating the over-smoothing effect with adaptive knowledge distillation.\nExtensive experiments verify that LightST significantly speeds up traffic flow\npredictions by 5X to 40X compared to state-of-the-art spatio-temporal GNNs, all\nwhile maintaining superior accuracy.",
        "Privacy concerns in machine learning are heightened by regulations such as\nthe GDPR, which enforces the \"right to be forgotten\" (RTBF), driving the\nemergence of machine unlearning as a critical research field. Vertical\nFederated Learning (VFL) enables collaborative model training by aggregating a\nsample's features across distributed parties while preserving data privacy at\neach source. This paradigm has seen widespread adoption in healthcare, finance,\nand other privacy-sensitive domains. However, existing VFL systems lack robust\nmechanisms to comply with RTBF requirements, as unlearning methodologies for\nVFL remain underexplored. In this work, we introduce the first VFL framework\nwith theoretically guaranteed unlearning capabilities, enabling the removal of\nany data at any time. Unlike prior approaches -- which impose restrictive\nassumptions on model architectures or data types for removal -- our solution is\nmodel- and data-agnostic, offering universal compatibility. Moreover, our\nframework supports asynchronous unlearning, eliminating the need for all\nparties to be simultaneously online during the forgetting process. These\nadvancements address critical gaps in current VFL systems, ensuring compliance\nwith RTBF while maintaining operational flexibility.We make all our\nimplementations publicly available at\nhttps:\/\/github.com\/wangln19\/vertical-federated-unlearning.",
        "The quantification of treatment effects plays an important role in a wide\nrange of applications, including policy making and bio-pharmaceutical research.\nIn this article, we study the quantile treatment effect (QTE) while addressing\ntwo specific types of heterogeneities: (a) personalized heterogeneity, which\ncaptures the varying treatment effects for different individuals, and (b)\nquantile heterogeneity, which accounts for how the impact of covariates varies\nacross different quantile levels. A well-designed debiased estimator for the\nindividualized quantile treatment effect (IQTE) is proposed to capture such\nheterogeneities effectively. We show that this estimator converges weakly to a\nGaussian process as a function of the quantile levels and propose valid\nstatistical inference methods, including the construction of confidence\nintervals and the development of hypothesis testing decision rules. In\naddition, the minimax optimality frameworks for these inference procedures are\nestablished. Specifically, we derive the minimax optimal rates for the expected\nlength of confidence intervals and the magnitude of the detection boundary for\nhypothesis testing procedures, illustrating the superiority of the proposed\nestimator. The effectiveness of our methods is demonstrated through extensive\nsimulations and an analysis of the National Health and Nutrition Examination\nSurvey (NHANES) datasets.",
        "Entanglement of spin degree of freedom can drastically alter the orbital\nexchange symmetry of electrons, switching their bunching and antibunching\nbehaviors and the resultant current correlations in the Hanbury-Brown-Twiss\ninterferometry. Here, we investigate the exchange correlation of quasiparticles\nwith entanglement encoded in the Nambu spinors, or the electron-hole degree of\nfreedom. In contrast to the conventional correspondence between spin\nentanglement and current correlation, we find that singlet (triplet)\nentanglement of Nambu spinors results in suppressed (enhanced) current\ncorrelation. This effect arises because the charge degree of freedom itself\nencodes the entanglement. We propose implementing this phenomenon in the edge\nstates of a quantum Hall system, where the entangled states of the Nambu\nspinors can be continuously tuned by gate voltages. Our study reveals a novel\nrelationship between entanglement and charge correlations, offering an\neffective approach for detecting entanglement of Nambu spinors.",
        "Accurate determination of binary fractions ($f_{\\rm b}$) and mass ratio ($q$)\ndistributions is crucial for understanding the dynamical evolution of open\nclusters. We present an improved multiband fitting technique to enhance the\nanalysis of binary properties. This approach enables an accurate photometric\ndetermination of $f_{\\rm b}$ and $q$ distribution in a cluster. The detectable\nmass ratio can be down to the $q_{\\rm lim}$, limited by the minimum stellar\nmass in theoretical models. First, we derived an empirical model for magnitudes\nof Gaia DR3 and 2MASS bands that match the photometry of single stars in the\nPleiades. We then performed a multiband fitting for each cluster member,\nderiving the probability density function (PDF) of its primary mass\n($\\mathcal{M}_1$) and $q$ in the Bayesian framework. 1154 main-sequence (MS)\nsingle stars or unresolved MS+MS binaries are identified as members of the\nPleiades. By stacking their PDFs, we conducted a detailed analysis of binary\nproperties of the cluster. We found the $f_{\\rm b}$ of this sample is $0.34 \\pm\n0.02$. The $q$ distribution exhibits a three-segment power-law profile: an\ninitial increase, followed by a decrease, and then another increase. This\ndistribution can be interpreted as a fiducial power-law profile with an\nexponent of -1.0 that is determined in the range of $0.3 < q < 0.8$, but with a\ndeficiency of binaries at lower $q$ and an excess at higher $q$. The variations\nof $f_{\\rm b}$ and $q$ with $\\mathcal{M}_1$ reveal a complex binary\ndistribution within the Pleiades, which might be attributed to a combination of\nprimordial binary formation mechanisms, dynamical interactions, and the\nobservational limit of photometric binaries imposed by $q_{\\rm lim}\n(\\mathcal{M}_1)$.",
        "Management of sensing resources is a non-trivial problem for future military\nair assets with future systems deploying heterogeneous sensors to generate\ninformation of the battlespace. Machine learning techniques including deep\nreinforcement learning (DRL) have been identified as promising approaches, but\nrequire high-fidelity training environments and feature extractors to generate\ninformation for the agent. This paper presents a deep reinforcement learning\ntraining approach, utilising the Stone Soup tracking framework as a feature\nextractor to train an agent for a sensor management task. A general framework\nfor embedding Stone Soup tracker components within a Gymnasium environment is\npresented, enabling fast and configurable tracker deployments for RL training\nusing Stable Baselines3. The approach is demonstrated in a sensor management\ntask where an agent is trained to search and track a region of airspace\nutilising track lists generated from Stone Soup trackers. A sample\nimplementation using three neural network architectures in a search-and-track\nscenario demonstrates the approach and shows that RL agents can outperform\nsimple sensor search and track policies when trained within the Gymnasium and\nStone Soup environment.",
        "Load Balancing (LB) is a routing strategy that increases performance by\ndistributing traffic over multiple outgoing links. In this work, we introduce a\nnovel methodology to detect the influence of LB on anycast routing, which can\nbe used by operators to detect network regions that experience anycast routing\ninstability. We use our methodology to measure the effects of LB-behavior on\nanycast routing at a global scale, covering both IPv4 and IPv6. Our results\nshow that LB-induced anycast routing instability is widespread. The results\nalso show our method can detect LB implementations on the global Internet,\nincluding detection and classification of Points-of-Presence (PoP) and egress\nselection techniques deployed by hypergiants, cloud providers, and network\noperators. We observe LB-induced routing instability directs distinct flows to\ndifferent anycast sites with significant latency inflation. In cases with two\npaths between an anycast instance and a load-balanced destination, we observe\nan average RTT difference of 30 ms with 8% of load-balanced destinations seeing\nRTT differences of over 100 ms. Being able to detect these cases can help\nanycast operators significantly improve their service for affected clients.",
        "Federated Learning (FL) presents a promising avenue for collaborative model\ntraining among medical centers, facilitating knowledge exchange without\ncompromising data privacy. However, vanilla FL is prone to server failures and\nrarely achieves optimal performance on all participating sites due to\nheterogeneous data distributions among them. To overcome these challenges, we\npropose Gossip Contrastive Mutual Learning (GCML), a unified framework to\noptimize personalized models in a decentralized environment, where Gossip\nProtocol is employed for flexible and robust peer-to-peer communication. To\nmake efficient and reliable knowledge exchange in each communication without\nthe global knowledge across all the sites, we introduce deep contrast mutual\nlearning (DCML), a simple yet effective scheme to encourage knowledge transfer\nbetween the incoming and local models through collaborative training on local\ndata. By integrating DCML with other efforts to optimize site-specific models\nby leveraging useful information from peers, we evaluated the performance and\nefficiency of the proposed method on three publicly available datasets with\ndifferent segmentation tasks. Our extensive experimental results show that the\nproposed GCML framework outperformed both centralized and decentralized FL\nmethods with significantly reduced communication overhead, indicating its\npotential for real-world deployment.",
        "This work introduces the Query\/Hit (Q\/H) learning model. The setup consists\nof two agents. One agent, Alice, has access to a streaming source, while the\nother, Bob, does not have direct access to the source. Communication occurs\nthrough sequential Q\/H pairs: Bob sends a sequence of source symbols (queries),\nand Alice responds with the waiting time until each query appears in the source\nstream (hits). This model is motivated by scenarios with communication,\ncomputation, and privacy constraints that limit real-time access to the source.\nThe error exponent for sequential hypothesis testing under the Q\/H model is\ncharacterized, and a querying strategy, the Dynamic Scout-Sentinel Algorithm\n(DSSA), is proposed. The strategy employs a mutual information neural estimator\nto compute the error exponent associated with each query and to select the\nquery with the highest efficiency. Extensive empirical evaluations on both\nsynthetic and real-world datasets -- including mouse movement trajectories,\ntypesetting patterns, and touch-based user interactions -- are provided to\nevaluate the performance of the proposed strategy in comparison with baselines,\nin terms of probability of error, query choice, and time-to-detection.",
        "Excitons, Coulomb bound electron-hole pairs, dominate the optical response of\ntwo-dimensional semiconductors across near-infrared and visible frequencies due\nto their large binding energy and prominent oscillator strength. Previous\nmeasurements of excitons in 2D semiconductors have primarily relied on\nfar-field optical spectroscopy techniques which are diffraction limited to\nseveral hundred nanometers. To precisely image nanoscale spatial disorder\nrequires an order of magnitude increase in resolution capabilities. Here, we\npresent a study of the exciton spectra of monolayer MoSe2 in the visible range\nusing a cryogenic scattering-type scanning near field optical microscope\n(s-SNOM) operating down to 11 K. By mapping the spatial variation in the\nexciton resonance across an hBN encapsulated MoSe2 monolayer, we achieve sub-50\nnm spatial resolution and energy resolution below 1 meV. We further investigate\nthe material's near-field spectra and dielectric function, demonstrating the\nability of cryogenic visible s-SNOM to reveal nanoscale disorder. Comparison to\nroom temperature measurements illustrate the enhanced capabilities of cryogenic\ns-SNOM to reveal fine-scale material heterogeneity.",
        "We consider stably rotating highly magnetised neutron stars and glitching\npulsars. We discuss the prospects for detecting continuous gravitational waves\nfrom these sources below 20 Hz with next-generation ground-based facilities\nsuch as the Einstein Telescope and Cosmic Explorer and space-based\nobservatories such as DECIGO and Big Bang Observer. We demonstrate that these\nconstitute interesting science targets. We use a robust sensitivity estimation\nmethod for future searches based on demonstrated performance. We show that the\nspin-down upper limit on the gravitational wave amplitude of more than 90% of\nall highly magnetised pulsars and magnetars suitable for a years-long fully\ncoherent search, exceeds the smallest gravitational wave amplitude estimated\ndetectable with DECIGO and Big Bang Observer. We find that the hidden magnetar\ncandidate PSR J1852+0040 can be detected by Cosmic Explorer if it is emitting\nat least at 20% of its spin-down luminosity. Finally, post-glitch transient\ncontinuous gravitational waves from magnetars are an interesting target for\ndeci-Hz detectors, with all but one of the recorded glitches giving rise to a\nspin-down limit signal above the smallest detectable level.",
        "We developed a vacuum generation system composed of a reciprocating\ncompressor (3 tons of refrigeration) with an inverted-function that is ready to\nbe hooked flexibly to a gas-tight container to create an evacuated enclosed\natmosphere, without strict limitation of the size of that container. The\nevacuated container (or vacuum chamber) can serve in different purposes such as\neducational demonstration of the vacuum properties, extraction of perfumes from\nherbal resources, and preserving food. We tested the device and found it can\nreach a vacuum level of 26 inches of mercury in an environment with an\natmospheric pressure of 28.5 inches of mercury. We compared the performance of\nour vacuum device to a rotary-vane vacuum pump of 1\/4 horsepowers and found\nthat the vacuum pump reaches a set test vacuum level of 25 inches of mercury\nbefore the compressor. We then demonstrated experimentally some features of the\nvacuum using the inverted compressor or the vane vacuum pump. These experiments\nserve some topics in physics for school students as well as two core subjects\nof mechanical engineering, namely fluid mechanics and thermodynamics.",
        "Neural ordinary differential equations (NODEs) are an emerging paradigm in\nscientific computing for modeling dynamical systems. By accurately learning\nunderlying dynamics in data in the form of differential equations, NODEs have\nbeen widely adopted in various domains, such as healthcare, finance, computer\nvision, and language modeling. However, there remains a limited understanding\nof the privacy implications of these fundamentally different models,\nparticularly with regard to their membership inference risks.\n  In this work, we study the membership inference risks associated with NODEs.\nWe first comprehensively evaluate NODEs against membership inference attacks.\nWe show that NODEs are twice as resistant to these privacy attacks compared to\nconventional feedforward models such as ResNets. By analyzing the variance in\nmembership risks across different NODE models, we identify the factors that\ncontribute to their lower risks. We then demonstrate, both theoretically and\nempirically, that membership inference risks can be further mitigated by\nutilizing a stochastic variant of NODEs: Neural stochastic differential\nequations (NSDEs). We show that NSDEs are differentially-private (DP) learners\nthat provide the same provable privacy guarantees as DP-SGD, the de-facto\nmechanism for training private models. NSDEs are also effective in mitigating\nexisting membership inference attacks, demonstrating risks comparable to\nprivate models trained with DP-SGD while offering an improved privacy-utility\ntrade-off. Moreover, we propose a drop-in-replacement strategy that efficiently\nintegrates NSDEs into conventional feedforward models to enhance their privacy.",
        "A Nijenhuis operator $L$ is a $(1,1)$-tensor field on a smooth manifold $M$\nwith vanishing Nijenhuis torsion ${ {\\mathcal N_L}}$. At each point $x\\in M$,\nthe algebraic type of $L(x)$ is characterized by its Jordan normal form. In\nthis paper, we study singularities of a two-dimensional Nijenhuis operator in\nthe case when its trace has a non-zero differential at the singular point. A\ndescription of such singularities reduces to studying the smoothness of some\nfunction, which is a fraction depending on partial derivatives of the\ndeterminant of $L$. We completely describe singularities for some special\nclasses of functions. We also obtained interesting examples of Nijenhuis\noperators and their singularities.",
        "Quantum networks revolutionize the way of information transmission and are an\nessential step in building a quantum internet. Generally, the information\ncapacity per user-channel in a quantum network drastically decreases with the\nincrease of network capacity, making it difficultly scale to large-user\nscenarios. To break this limit, we develop a quantum network architecture in\nwhich the information capacity per user-channel is independent of the network\ncapacity (NCI-QN), and all previous quantum networks can be regarded as special\nexamples. Three aspects are investigated. Firstly, a quantum network scheme\nformulated in a comprehensive multi-mode time-frequency representation is\npresented. Then, information characteristics of the proposed quantum network\nare delineated by expanding the well-known Pirandola-Laurenza-Ottaviani-Banchi\n(PLOB) bound and the Holevo bound from the linear combination of point-to-point\nlinks to a complex network architecture, demonstrating clear proofs of the\nnetwork capacity independence. Finally, a practical NCI-QN with a network\ncapacity of 19 is experimentally demonstrated using optical frequency comb in\nquantum key distribution network scenarios, in which the security under the\nasymptotic case, finite-size effect, composable security, and composable\nfinite-size security are verified, with a secret key rate up to 8.75 Gbps. This\nachievement overcomes the user-rate limit of quantum network, which is the\nkeystone for the development of the quantum internet.",
        "ELIZA, created by Joseph Weizenbaum at MIT in the early 1960s, is usually\nconsidered the world's first chatbot. It was developed in MAD-SLIP on MIT's\nCTSS, the world's first time-sharing system, on an IBM 7094. We discovered an\noriginal ELIZA printout in Prof. Weizenbaum's archives at MIT, including an\nearly version of the famous DOCTOR script, a nearly complete version of the\nMAD-SLIP code, and various support functions in MAD and FAP. Here we describe\nthe reanimation of this original ELIZA on a restored CTSS, itself running on an\nemulated IBM 7094. The entire stack is open source, so that any user of a\nunix-like OS can run the world's first chatbot on the world's first\ntime-sharing system."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Brain\u2013Computer Interface Spellers: A Review",
    "start_abstract":"A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
      ],
      "abstract":[
        "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Multilingual Non-Autoregressive Machine Translation without Knowledge\n  Distillation",
        "PanopticSplatting: End-to-End Panoptic Gaussian Splatting",
        "Emergent effects of scaling on the functional hierarchies within large\n  language models",
        "CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models",
        "Evaluating Developer-written Unit Test Case Reduction for Java -- A\n  Replication Study",
        "Non-Gaussianities as a Signature of Quantumness of Quantum Cosmology",
        "Propagation of Gravitational Waves on a Geometric Condensate background\n  of $(R + \\alpha R^{2})$ Origin",
        "A Search for Eclipse Cycles Similar to the Hypersaros: Columbus and the\n  Lunar Eclipse of March 14, 2025",
        "Realistic Clothed Human and Object Joint Reconstruction from a Single\n  Image",
        "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "CoopDETR: A Unified Cooperative Perception Framework for 3D Detection\n  via Object Query",
        "Predicting Human Choice Between Textually Described Lotteries",
        "A Study of the Efficacy of Generative Flow Networks for Robotics and\n  Machine Fault-Adaptation",
        "Estimating relapse time distribution from longitudinal biomarker\n  trajectories using iterative regression and continuous time Markov processes",
        "Spatial Context-Driven Positive Pair Sampling for Enhanced\n  Histopathology Image Classification",
        "Some results of CCD-photometry of variable stars at the Astronomical\n  Institute of Karazin Kharkiv National University",
        "$CP$ violation in the $HZZ$ vertex and left-right asymmetries",
        "Riemannian Metric Learning: Closer to You than You Imagine",
        "Broadband transient full-Stokes luminescence spectroscopy with high\n  sensitivity",
        "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
        "Increasing the distance of topological codes with time vortex defects",
        "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts",
        "Adaptivity and Convergence of Probability Flow ODEs in Diffusion\n  Generative Models",
        "Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion\n  Detection with Multilingual Models",
        "qReduMIS: A Quantum-Informed Reduction Algorithm for the Maximum\n  Independent Set Problem",
        "Promising High Temperature Thermoelectric Performance of Alkali\n  Metal-based Zintl phases X$_2$AgY (X = Na, K; Y = Sb, Bi): Insights from\n  First-Principles Studies",
        "Representation-based Reward Modeling for Efficient Safety Alignment of\n  Large Language Model"
      ],
      "abstract":[
        "Multilingual neural machine translation (MNMT) aims at using one single model\nfor multiple translation directions. Recent work applies non-autoregressive\nTransformers to improve the efficiency of MNMT, but requires expensive\nknowledge distillation (KD) processes. To this end, we propose an M-DAT\napproach to non-autoregressive multilingual machine translation. Our system\nleverages the recent advance of the directed acyclic Transformer (DAT), which\ndoes not require KD. We further propose a pivot back-translation (PivotBT)\napproach to improve the generalization to unseen translation directions.\nExperiments show that our M-DAT achieves state-of-the-art performance in\nnon-autoregressive MNMT.",
        "Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models.",
        "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.",
        "The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps:\/\/github.com\/szc12153\/CLDyB.",
        "Abstract: Failing test case reduction can promote efficient debugging because\na developer may not need to observe components that are not relevant to\ninducing failure. Failing test case reduction can also improve the efficiency\nof fault localization. These considerations have prompted researchers to study\nthe reduction process, the reduction output, and the removed entities. Christi\net al. studied test reduction using a tool called ReduSharptor for C# tests.\nThey considered the test to be an Abstract Syntax Tree (AST). Based on that,\nthey studied the reduction outcome and removed entities in terms of Leaf nodes\nand Non-Leaf nodes of the AST. They claimed that (1) leaf nodes are removed in\nlarge numbers, and (2) the probability of removal is slightly higher than\nnon-leaf nodes. We replicate their results using a different test case\nreduction tool, ReduJavator, for Java unit tests. We evaluate test reduction\nusing 30 randomly chosen bugs from the Defects4J database and 30 mutants for 6\nopen-source projects. Our results confirm their first claim: leaf nodes are\nremoved in large numbers. Our results are inconclusive regarding their second\nclaim; we cannot confirm that the probability of removal is higher for non-leaf\nnodes.",
        "We show that the consistent application of the rules of quantum mechanics to\ncosmological systems inevitably results in the so-called multiverse states in\nwhich neither the background spacetime nor the inhomogeneous perturbation are\nin definite states. We study the multiverse states as perturbations to the\nusually employed so-called Born-Oppenheimer states that are products of a wave\nfunction of the background and a wave function of the perturbation. The\nobtained corrections involve integrals over \\emph{virtual backgrounds} that\nrepresent the effect of quantum background fluctuations on the perturbation\nstate. They resemble loop corrections in quantum field theory. This approach\ndemonstrates the inevitable existence of very specific non-Gaussian features in\nprimordial fluctuations. We express the resulting non-Gaussian perturbation as\na nonlinear function of the Gaussian perturbation obtained within the\nBorn-Oppenheimer approximation, and compute its trispectrum, to show that the\nmultiverse scenario leads to testable and distinct signatures in cosmological\nperturbations. Our approach applies both to inflationary and alternative\ncosmologies.",
        "In this paper we propose a new paradigm for cosmology: a time dependent\nscalar condensate background originated from the quadratic $(R + \\alpha R^2)$\nStarobinski model, where $R$ is the Ricci scalar and $\\alpha$ the coupling\nconstant. In weak gravity limit the system decouples into a conventional\ngraviton and a higher derivative scalar. It was shown earlier through works\nfrom our group, \\cite{ssg,sg,us}, that the latter can sustain an oscillatory\nlowest energy configuration or a {\\it{Geometric Condensate}} as it consists\nentirely of metric degrees of freedom. In the present work, we study\nGravitational Wave propagation in this condensate background. We show that the\nexplicit time dependent nature of the condensate can generate curvature and\nradiation-like contributions in the scale factor evolution in FLRW cosmology.\nSubsequently the condensate leaves its signature on the Gravitational Wave\nprofile as it propagates in the condensate modified FLRW spacetime. The wave\nprofile is calculated analytically in terms of Whittaker functions. The main\nnovelty of the Geometric Condensate scheme is that no external (condensate)\nmatter from outside has been considered.",
        "The total lunar eclipse on March 14, 2025 UT occurs nearly exactly 521 years\n(one Hypersaros) after a similar eclipse on March 1, 1504 UT that is renowned\nfor its importance to the voyage of Columbus to Jamaica. Eclipses separated by\na Hypersaros have similar depths, appear very close to the same location in the\nsky, and occur at nearly the same time of year. This paper summarizes the\nresults from a search for analogous cycles within the Five Millennium Catalogs\nof Lunar and Solar Eclipses. Under the two simple constraints of similar\neclipse dates relative to the vernal equinox and similar paths of the Moon\nthrough the Earth's shadow, the most common time intervals between lunar\neclipses separated by less than 1000 years are the 521-year Hypersaros and a\n633-yr period of the Icosa-Inex-Triple-Saros (IITS). Notable cycles at longer\nperiods occur at 1154, 1284, 1787, 1917, and 2308 years.",
        "Recent approaches to jointly reconstruct 3D humans and objects from a single\nRGB image represent 3D shapes with template-based or coarse models, which fail\nto capture details of loose clothing on human bodies. In this paper, we\nintroduce a novel implicit approach for jointly reconstructing realistic 3D\nclothed humans and objects from a monocular view. For the first time, we model\nboth the human and the object with an implicit representation, allowing to\ncapture more realistic details such as clothing. This task is extremely\nchallenging due to human-object occlusions and the lack of 3D information in 2D\nimages, often leading to poor detail reconstruction and depth ambiguity. To\naddress these problems, we propose a novel attention-based neural implicit\nmodel that leverages image pixel alignment from both the input human-object\nimage for a global understanding of the human-object scene and from local\nseparate views of the human and object images to improve realism with, for\nexample, clothing details. Additionally, the network is conditioned on semantic\nfeatures derived from an estimated human-object pose prior, which provides 3D\nspatial information about the shared space of humans and objects. To handle\nhuman occlusion caused by objects, we use a generative diffusion model that\ninpaints the occluded regions, recovering otherwise lost details. For training\nand evaluation, we introduce a synthetic dataset featuring rendered scenes of\ninter-occluded 3D human scans and diverse objects. Extensive evaluation on both\nsynthetic and real-world datasets demonstrates the superior quality of the\nproposed human-object reconstructions over competitive methods.",
        "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https:\/\/github.com\/taco-group\/Re-Align.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Cooperative perception enhances the individual perception capabilities of\nautonomous vehicles (AVs) by providing a comprehensive view of the environment.\nHowever, balancing perception performance and transmission costs remains a\nsignificant challenge. Current approaches that transmit region-level features\nacross agents are limited in interpretability and demand substantial bandwidth,\nmaking them unsuitable for practical applications. In this work, we propose\nCoopDETR, a novel cooperative perception framework that introduces object-level\nfeature cooperation via object query. Our framework consists of two key\nmodules: single-agent query generation, which efficiently encodes raw sensor\ndata into object queries, reducing transmission cost while preserving essential\ninformation for detection; and cross-agent query fusion, which includes Spatial\nQuery Matching (SQM) and Object Query Aggregation (OQA) to enable effective\ninteraction between queries. Our experiments on the OPV2V and V2XSet datasets\ndemonstrate that CoopDETR achieves state-of-the-art performance and\nsignificantly reduces transmission costs to 1\/782 of previous methods.",
        "Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap.",
        "Advancements in robotics have opened possibilities to automate tasks in\nvarious fields such as manufacturing, emergency response and healthcare.\nHowever, a significant challenge that prevents robots from operating in\nreal-world environments effectively is out-of-distribution (OOD) situations,\nwherein robots encounter unforseen situations. One major OOD situations is when\nrobots encounter faults, making fault adaptation essential for real-world\noperation for robots. Current state-of-the-art reinforcement learning\nalgorithms show promising results but suffer from sample inefficiency, leading\nto low adaptation speed due to their limited ability to generalize to OOD\nsituations. Our research is a step towards adding hardware fault tolerance and\nfast fault adaptability to machines. In this research, our primary focus is to\ninvestigate the efficacy of generative flow networks in robotic environments,\nparticularly in the domain of machine fault adaptation. We simulated a robotic\nenvironment called Reacher in our experiments. We modify this environment to\nintroduce four distinct fault environments that replicate real-world\nmachines\/robot malfunctions. The empirical evaluation of this research\nindicates that continuous generative flow networks (CFlowNets) indeed have the\ncapability to add adaptive behaviors in machines under adversarial conditions.\nFurthermore, the comparative analysis of CFlowNets with reinforcement learning\nalgorithms also provides some key insights into the performance in terms of\nadaptation speed and sample efficiency. Additionally, a separate study\ninvestigates the implications of transferring knowledge from pre-fault task to\npost-fault environments. Our experiments confirm that CFlowNets has the\npotential to be deployed in a real-world machine and it can demonstrate\nadaptability in case of malfunctions to maintain functionality.",
        "Biomarker measurements obtained by blood sampling are often used as a\nnon-invasive means of monitoring tumour progression in cancer patients.\nDiseases evolve dynamically over time, and studying longitudinal observations\nof specific biomarkers can help to understand patients response to treatment\nand predict disease progression. We propose a novel iterative regression-based\nmethod to estimate changes in patients status within a cohort that includes\ncensored patients, and illustrate it on clinical data from myeloma cases. We\nformulate the relapse time estimation problem in the framework of Piecewise\nDeterministic Markov processes (PDMP), where the Euclidean component is a\nsurrogate biomarker for patient state. This approach enables continuous-time\nestimation of the status-change dates, which in turn allows for accurate\ninference of the relapse time distribution. A key challenge lies in the partial\nobservability of the process, a complexity that has been rarely addressed in\nprevious studies. . We evaluate the performance of our procedure through a\nsimulation study and compare it with different approaches. This work is a proof\nof concept on biomarker trajectories with simple behaviour, but our method can\neasily be extended to more complex dynamics.",
        "Deep learning has demonstrated great promise in cancer classification from\nwhole-slide images (WSIs) but remains constrained by the need for extensive\nannotations. Annotation-free methods, such as multiple instance learning (MIL)\nand self-supervised learning (SSL), have emerged to address this challenge;\nhowever, current SSL techniques often depend on synthetic augmentations or\ntemporal context, which may not adequately capture the intricate spatial\nrelationships inherent to histopathology. In this work, we introduce a novel\nspatial context-driven positive pair sampling strategy for SSL that leverages\nthe natural coherence of adjacent patches in WSIs. By constructing biologically\nrelevant positive pairs from spatially proximate patches, our approach\nharnesses inherent spatial coherence to enhance patch-level representations,\nultimately boosting slide-level classification performance. Experiments on\nmultiple datasets reveal that our strategy improves classification accuracy by\n5\\% to 10\\% over the standard method, paving the way for more clinically\nrelevant AI models in cancer diagnosis. The code is available at\nhttps:\/\/anonymous.4open.science\/r\/contextual-pairs-E72F\/.",
        "We presented photometric observations for the one UV Ceti type and three W\nUrsae Majoris-type variable stars. The flare of the UV Ceti type star lasted\nabout two hours, and the star changed magnitude to 3.9 within about two\nminutes. The values of color indices V-R, the rotational periods and the\ncomposite lightcurves have been obtained for the EW stars. Using a relation of\nan absolute magnitude-period obtained by Mateo and Rucinski (2017) and\ninterstellar extinction from the three-dimensional map of Milky Way dust\n(http:\/\/argonaut.skymaps.info) and Green et al. (2019), we have calculated the\nabsolute magnitudes of the EW stars and distances to them. The parallaxes\nobtained from our data differ from those given in Gaia DR 3, which may be due\nto insufficient quality calibration of the absolute magnitude-period relation\nand with the estimations of interstellar extinction.",
        "We calculate new contributions to the $HZZ$ vertex from the Flavor Changing\nNeutral Current (FCNC) of the Higgs and $Z$ bosons. It is found that the\n$h_2^V$ and $h_3^V$ ($V=H$, $Z$) form factors can be induced through these\ncouplings, and we present our results in terms of the Passarino-Veltman scalar\nfunctions. Using the current limits on $H\\overline{t}c$ and $Z\\overline{t}c$\ncouplings, we determine that the new contributions to the $CP$-conserving form\nfactor $h_2^V$ are small in comparison to the Standard Model (SM) predictions.\nHowever, for the $CP$-violating form factor $h_3^V$, the contributions can\nreach values as large as $10^{-6}$, five orders of magnitude larger than in the\nSM. Furthermore, we examine how these results influence the left-right\nasymmetries in the processes $H^\\ast\\to ZZ$ and $Z^\\ast\\to ZH$. Our findings\nindicate that significant deviations from the SM predictions may arise when\nFCNC contributions are considered.",
        "Riemannian metric learning is an emerging field in machine learning,\nunlocking new ways to encode complex data structures beyond traditional\ndistance metric learning. While classical approaches rely on global distances\nin Euclidean space, they often fall short in capturing intrinsic data geometry.\nEnter Riemannian metric learning: a powerful generalization that leverages\ndifferential geometry to model the data according to their underlying\nRiemannian manifold. This approach has demonstrated remarkable success across\ndiverse domains, from causal inference and optimal transport to generative\nmodeling and representation learning. In this review, we bridge the gap between\nclassical metric learning and Riemannian geometry, providing a structured and\naccessible overview of key methods, applications, and recent advances. We argue\nthat Riemannian metric learning is not merely a technical refinement but a\nfundamental shift in how we think about data representations. Thus, this review\nshould serve as a valuable resource for researchers and practitioners\ninterested in exploring Riemannian metric learning and convince them that it is\ncloser to them than they might imagine-both in theory and in practice.",
        "Materials emitting circularly polarized light (CPL) are highly sought after\nfor applications ranging from efficient displays to quantum information\ntechnologies. Established methods for time-resolved CPL characterization have\nsignificant limitations, preventing in-depth photophysical insight necessary\nfor materials development. We have designed and built a high-sensitivity (noise\nlevel 10^-4), broadband (ca. 400-900 nm), transient (ns resolution, ms range)\nfull-Stokes (CPL and linear polarizations) spectroscopy setup. We demonstrate\nits broad applicability by measuring compounds with low dissymmetry factors\nacross various timescales, as well as tracking the temporal evolution of linear\npolarization components alongside associated CPL artefacts. We have written\nopen-source software and used stock optical components to make transient CPL\nspectroscopy practically accessible to a wide audience, enabling the study of\nchiral materials in numerous diverse applications.",
        "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https:\/\/github.com\/sail-sg\/LongSpec.",
        "We propose modifying topological quantum error correcting codes by\nincorporating space-time defects, termed ``time vortices,'' to reduce the\nnumber of physical qubits required to achieve a desired logical error rate. A\ntime vortex is inserted by adding a spatially varying delay to the periodic\nmeasurement sequence defining the code such that the delay accumulated on a\nhomologically non-trivial cycle is an integer multiple of the period. We\nanalyze this construction within the framework of the Floquet color code and\noptimize the embedding of the code on a torus along with the choice of the\nnumber of time vortices inserted in each direction. Asymptotically, the\nvortexed code requires less than half the number of qubits as the vortex-free\ncode to reach a given code distance. We benchmark the performance of the\nvortexed Floquet color code by Monte Carlo simulations with a circuit-level\nnoise model and demonstrate that the smallest vortexed code (with $30$ qubits)\noutperforms the vortex-free code with $42$ qubits.",
        "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps:\/\/github.com\/KongLongGeFDU\/PFDial.",
        "Score-based generative models, which transform noise into data by learning to\nreverse a diffusion process, have become a cornerstone of modern generative AI.\nThis paper contributes to establishing theoretical guarantees for the\nprobability flow ODE, a widely used diffusion-based sampler known for its\npractical efficiency. While a number of prior works address its general\nconvergence theory, it remains unclear whether the probability flow ODE sampler\ncan adapt to the low-dimensional structures commonly present in natural image\ndata. We demonstrate that, with accurate score function estimation, the\nprobability flow ODE sampler achieves a convergence rate of $O(k\/T)$ in total\nvariation distance (ignoring logarithmic factors), where $k$ is the intrinsic\ndimension of the target distribution and $T$ is the number of iterations. This\ndimension-free convergence rate improves upon existing results that scale with\nthe typically much larger ambient dimension, highlighting the ability of the\nprobability flow ODE sampler to exploit intrinsic low-dimensional structures in\nthe target distribution for faster sampling.",
        "This paper describes the system submitted by Team A to SemEval 2025 Task 11,\n``Bridging the Gap in Text-Based Emotion Detection.'' The task involved\nidentifying the perceived emotion of a speaker from text snippets, with each\ninstance annotated with one of six emotions: joy, sadness, fear, anger,\nsurprise, or disgust. A dataset provided by the task organizers served as the\nfoundation for training and evaluating our models. Among the various approaches\nexplored, the best performance was achieved using multilingual embeddings\ncombined with a fully connected layer. This paper details the system\narchitecture, discusses experimental results, and highlights the advantages of\nleveraging multilingual representations for robust emotion detection in text.",
        "We propose and implement a quantum-informed reduction algorithm for the\nmaximum independent set problem that integrates classical kernelization\ntechniques with information extracted from quantum devices. Our larger\nframework consists of dedicated application, algorithm, and hardware layers,\nand easily generalizes to the maximum weight independent set problem. In this\nhybrid quantum-classical framework, which we call qReduMIS, the quantum\ncomputer is used as a co-processor to inform classical reduction logic about\nfrozen vertices that are likely (or unlikely) to be in large independent sets,\nthereby opening up the reduction space after removal of targeted subgraphs. We\nsystematically assess the performance of qReduMIS based on experiments with up\nto 231 qubits run on Rydberg quantum hardware available through Amazon Braket.\nOur experiments show that qReduMIS can help address fundamental performance\nlimitations faced by a broad set of (quantum) solvers including Rydberg quantum\ndevices. We outline implementations of qReduMIS with alternative platforms,\nsuch as superconducting qubits or trapped ions, and we discuss potential future\nextensions.",
        "In the quest for novel thermoelectric materials to harvest waste\nenvironmental heat, we investigate alkali metal-based Zintl phases X$_2$AgY (X\n= Na, K, and Y = Sb, Bi) utilizing first-principles methods. We obtain\nsignificantly low lattice thermal conductivity values ranging 0.9-0.5 W\nm$^{-1}$ K$^{-1}$ at 300~K, challenging established thermoelectric materials\nsuch as SnSe, PbTe, Bi$_2$Te$_3$ as well as other Zintl phases. We trace such\nastonishingly low values to lattice anharmonicity, large phonon scattering\nphase space, low phonon velocities, and lifetimes. In K-based materials, the\nlow phonon velocities are further linked to flattened phonon modes arising from\nthe gap in the optical spectrum. Furthermore, the existence of bonding\nheterogeneity could hamper heat conduction in these materials. In addition, an\navoided crossing in the phonon dispersions suggesting rattling behavior,\nobserved in all materials except Na$_2$AgSb, suppresses the dispersion of\nacoustic modes, further reducing the phonon velocities. When combined with\nelectrical transport calculations, the materials exhibit high figure of merit\nvalues at 700~K, i.e., $ZT\\sim2.1$ for Na$_2$AgSb, $1.7$ for Na$_2$AgBi, $0.9$\nfor K$_2$AgSb, and $1.0$ for K$_2$AgBi. Our predicted $ZT$ values are\ncompetitive with state-of-the-art thermoelectric materials such as\nMg$_3$Sb$_2$, ZrCoBi, PbTe, SnSe, and as well as with contemporary Zintl\nphases. Our findings underscore the potential of light alkali metal atoms\ncombined with Ag-Bi\/Sb type frameworks to achieve superior thermoelectric\nperformance, paving the way for material design for specific operating\nconditions.",
        "Reinforcement Learning (RL) algorithms for safety alignment of Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO), encounter the\nchallenge of distribution shift. Current approaches typically address this\nissue through online sampling from the target policy, which requires\nsignificant computational resources. In this paper, we hypothesize that during\noff-policy training, while the ranking order of output generated by policy\nchanges, their overall distribution remains relatively stable. This stability\nallows the transformation of the sampling process from the target policy into a\nre-ranking of preference data. Building on this hypothesis, We propose a new\nframework that leverages the model's intrinsic safety judgment capability to\nextract reward signals, which are then used to calculate label confidence for\npreferences reordering. Extensive experimental results and theoretical analysis\ndemonstrate that the proposed method effectively addresses the distribution\nshift issue, remarkably enhancing the safety performance while reducing about\n300x computational overheads."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication",
    "start_abstract":"Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Brain\u2013Computer Interface Spellers: A Review"
      ],
      "abstract":[
        "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Applying a star formation model calibrated on high-resolution\n  interstellar medium simulations to cosmological simulations of galaxy\n  formation",
        "Neural network-based prediction of particle-induced fission cross\n  sections for r-process nucleosynthesis trained with dynamical reaction models",
        "Improving the trivial bound for $\\ell$-torsion in class groups",
        "Real-time simulation of jet energy loss and entropy production in\n  high-energy scattering with matter",
        "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA). III. NGC 288\n  as the first Splashed globular cluster",
        "Compare Similarities Between DNA Sequences Using Permutation-Invariant\n  Quantum Kernel",
        "Asymptotic behavior of clusters in hierarchical species sampling models",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "B-fields And dust in interstelLar fiLAments using Dust POLarization\n  (BALLAD-POL): III. Grain alignment and disruption mechanisms in G34.43+0.24\n  using polarization observations from JCMT\/POL-2",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Nearsightedness in Materials with Indirect Band Gap",
        "A tracking algorithm for finite-size particles",
        "Female and Combined Male-Female Injury Risk Functions for the Anterior\n  Pelvis Under Frontal Lap Belt Loading Conditions",
        "Cohering Disaggregation and Uncertainty Quantification for Spatially\n  Misaligned Data",
        "Cooperation and competing of antipolar charge order and superconducting\n  states in a quasi-2D superatomic metallic crystal",
        "Adaptive GSIS for rarefied gas flow simulations",
        "Exploring Generative Networks for Manifolds with Non-Trivial Topology",
        "On localizing subcategories of Lie superalgebra representations",
        "Optimised Graph Convolution for Calorimetry Event Classification",
        "Image Data Augmentation for the TAIGA-IACT Experiment with Conditional\n  Generative Adversarial Networks",
        "Stokes flow in the electronic fluid with odd viscosity",
        "Bifurcations in Bosonic Stars: chains and rings from spherical solutions",
        "A-priori estimates for generalized Korteweg-de Vries equations in\n  $H^{-1}(\\mathbb{R})$",
        "Data Sharing in the PRIMED Consortium: Design, implementation, and\n  recommendations for future policymaking",
        "Space compatibility of emerging, wide-bandgap, ultralow-loss integrated\n  photonics",
        "Efficient parameter inference in networked dynamical systems via steady\n  states: A surrogate objective function approach integrating mean-field and\n  nonlinear least squares",
        "On a class of globally analytic Hypoelliptic operators with non-negative\n  characteristic form",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "Twenty years of Ne\\v{s}et\\v{r}il's classification programme of Ramsey\n  classes"
      ],
      "abstract":[
        "Modern high-resolution simulations of the interstellar medium (ISM) have\nshown that key factors in governing star formation are the competing influences\nof radiative dissipation, pressure support driven by stellar feedback, and the\nrelentless pull of gravity. Cosmological simulations of galaxy formation, such\nas IllustrisTNG or ASTRID, are however not able to resolve this physics in\ndetail and therefore need to rely on approximate treatments. These have often\ntaken the form of empirical subgrid models of the ISM expressed in terms of an\neffective equation of state (EOS) that relates the mean ISM pressure to the\nmean gas density. Here we seek to improve these heuristic models by directly\nfitting their key ingredients to results of the high-resolution TIGRESS\nsimulations, which have shown that the dynamical equilibrium of the ISM can be\nunderstood in terms of a pressure-regulated, feedback modulated (PRFM) model\nfor star formation. Here we explore a simple subgrid model that draws on the\nPRFM concept but uses only local quantities. It accurately reproduces PRFM for\npure gas disks, while it predicts slightly less star formation than PRFM in the\npresence of an additional thin stellar disk. We compare the properties of this\nmodel with the older Springel and Hernquist and TNG prescriptions, and apply\nall three to isolated simulations of disk galaxies as well as to a set of\nhigh-resolution zoom-in simulations carried out with a novel 'multi-zoom'\ntechnique that we introduce in this study. The softer EOS implied by TIGRESS\nproduces substantially thinner disk galaxies, which has important ramifications\nfor disk stability and galaxy morphology. The total stellar mass of galaxies is\nhowever hardly modified at low redshift, reflecting the dominating influence of\nlarge-scale gaseous inflows and outflows to galaxies, which are not sensitive\nto the EOS itself",
        "Large-scale computations of fission properties play a crucial role in nuclear\nreaction network calculations simulating rapid neutron-capture process\n(r-process) nucleosynthesis. Due to the large number of fissioning nuclei\ncontributing to the r-process, a description of particle-induced fission\nreactions is computationally challenging. In this work, we use theoretical\ncalculations based on the INCL+ABLA models to train neural networks (NN). The\nresults for the prediction of proton-induced spallation reactions, in\nparticular fission, utilizing a large variety of NN models across the\nhyper-parameter space are presented, which are relevant for r-process\ncalculations.",
        "For any number field $K$ with $D_K=|\\mathrm{Disc}(K)|$ and any integer $\\ell\n\\geq 2$, we improve over the commonly cited trivial bound\n$|\\mathrm{Cl}_K[\\ell]| \\leq |\\mathrm{Cl}_K| \\ll_{[K:\\mathbb{Q}],\\varepsilon}\nD_K^{1\/2+\\varepsilon}$ on the $\\ell$-torsion subgroup of the class group of $K$\nby showing that $|\\mathrm{Cl}_K[\\ell]| = o_{[K:\\mathbb{Q}],\\ell}(D_K^{1\/2})$.\nIn fact, we obtain an explicit log-power saving. This is the first general\nunconditional saving over the trivial bound that holds for all $K$ and all\n$\\ell$.",
        "In analogy to high-energy nuclear scattering experiments, we study a\nreal-time scattering process between a propagating state and a dense target in\n$1+1$-d massive QED. In our setup, we identify three distinct regimes that\nqualitatively characterize the evolution: for a dilute medium, the incoming\nprobe state evolves nearly ballistically; in an intermediate setting, it\ntraverses the matter, locally exciting it; and for dense targets, one\napproaches a black-disk limit, where the matter acts as a strong wall\npotential. We find evidence that the probe's energy loss rate scales linearly\nwith the path length in the medium, and we study how the entanglement entropy\nreveals the mixing between the probe and medium states. With the goal of one\nday replicating high-energy nuclear experiments in quantum devices, we briefly\ndiscuss how the current tensor network-based simulations can be translated to a\nquantum simulator.",
        "The globular clusters (GCs) system of the Milky Way (MW) comprises a mixture\nof both in situ and accreted clusters. Tracing the origin of GCs provides\ninvaluable insights into the formation history of the MW. However, reconciling\ndiverse strands of evidence is often challenging: a notable example is NGC 288,\nwhere despite significant efforts in the literature, the available\nchrono-chemodynamical data have yet to provide a definitive conclusion\nregarding its origin. On one side, all post-Gaia dynamical studies indicate an\naccreted origin for NGC 288 from in the Gaia-Sausage-Enceladus (GSE) dwarf\ngalaxy. On the other, NGC 288 has been found to be 2.5 Gyr older than other GSE\nGCs at the same metallicity, this suggesting a different, possibly in situ\norigin. In this work, we address the unresolved question on the origin of NGC\n288 by analyzing its chrono-chemical properties in an unprecedentedly\nhomogeneous framework. First, we compare the location of NGC 288 in the\nage-metallicity plane with that of other two in situ GCs at similar\nmetallicity, namely NGC 6218 and NGC 6362. The age estimates obtained within\nthe homogeneous framework of the CARMA collaboration show that the three\nclusters are coeval, this reinforcing the contrast with the dynamical\ninterpretation. Then, we compare the abundances with the sample of in situ and\naccreted clusters at similar metallicity presented in Ceccarelli et al. 2024,\nfinding again consistency with the chemistry of in situ systems. To reconcile\nthese results with its orbital properties, we propose a scenario where NGC 288\nformed in the proto-disc of the MW, and then was dynamically heated by the\ninteraction with the GSE merger, a fate similar to that of proto-disc stars\nexperiencing the so-called Splash event. NGC 288 therefore demonstrates the\nimportance of a homogeneous chrono-chemodynamical information in the\ninterpretation of the origin of MW GCs.",
        "Computing the similarity between two DNA sequences is of vital importance in\nbioscience. However, traditional computational methods can be\nresource-intensive due to the enormous sequence length encountered in practice.\nRecently, applied quantum algorithms have been anticipated to provide potential\nadvantages over classical approaches. In this paper, we propose a\npermutation-invariant variational quantum kernel method specifically designed\nfor DNA comparison. To represent the four nucleotide bases in DNA sequences\nwith quantum states, we introduce a novel, theoretically motivated encoding\nscheme: the four distinct bases are encoded using the states of symmetric,\ninformationally complete, positive operator-valued measures (SIC-POVMs). This\nencoding ensures mutual equality: each pair of symbols is equidistant on the\nBloch sphere. Also, since permutation invariance is inherent to common DNA\nsimilarity measures such as Levenshtein distance, we realize it by using a\nspecially designed parameterized quantum layer. We show that our novel encoding\nmethod and parameterized layers used in the quantum kernel model can\neffectively capture the symmetric characteristics of the pairwise DNA sequence\ncomparison task. We validate our model through numerical experiments, which\nyield promising results on length-$8$ DNA sequences.",
        "Consider a sample of size $N$ from a population governed by a hierarchical\nspecies sampling model. We study the large $N$ asymptotic behavior of the\nnumber ${\\bf K}_N$ of clusters and the number ${\\bf M}_{r,N}$ of clusters with\nfrequency $r$ in the sample. In particular, we show almost sure and $L^p$\nconvergence for ${\\bf M}_{r,N}$, obtain Gaussian fluctuation theorems for ${\\bf\nK}_N$, and establish large deviation principles for both ${\\bf K}_N$ and ${\\bf\nM}_{r,N}$. Our approach relies on a random sample size representation of the\nnumber of clusters through the corresponding non-hierarchical species sampling\nmodel.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "Polarization of starlight and thermal dust emission due to aligned\nnon-spherical grains helps us to trace magnetic field (B-field) morphology in\nmolecular clouds and to study grain alignment mechanisms. In this work, we\nstudy grain alignment and disruption mechanisms in a filamentary infrared dark\ncloud G34.43+0.24 using thermal dust polarization observations from JCMT\/POL-2\nat 850 $\\mu\\text{m}$. We study in three sub-regions as North harboring MM3\ncore, Center harboring MM1 and MM2 cores and South having no core. We find the\ndecrease in polarization fraction P with increasing total intensity and gas\ncolumn density, known as polarization hole. To disentangle the effect of\nmagnetic field tangling on the polarization hole, we estimate the polarization\nangle dispersion function. We find depolarizations in North and Center regions\nare due to decrease in net alignment efficiency of grains but in South region,\neffect of magnetic field tangling is significant to cause depolarization. To\ntest whether RAdiative Torque (RAT) mechanism can reproduce the observational\ndata, we calculate minimum alignment and disruption sizes of grains using RAT\ntheory and our study finds that RAT alignment mechanism can explain the\ndepolarizations in North and Center regions where B-field tangling effect is\nless important, except for core regions. We find hints of RAdiative Torque\nDisruption (RAT-D) in the core regions of MM3 in North, MM1 and MM2 in Center.\nWe also find that the high P value of around 8-20% in the outer regions of the\nfilament can be explained potentially by magnetically enhanced RAT alignment\nmechanism.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "We investigate the nearsightedness property in the linear tight binding model\nat zero Fermi-temperature. We focus on the decay property of the density matrix\nfor materials with indirect band gaps. By representing the density matrix in\nreciprocal space, we establish a qualitatively sharp estimate for the\nexponential decay rate in homogeneous systems. An extending result under\nperturbations is also derived. This work refines the estimates presented in\n(Ortner, Thomas & Chen 2020), particularly for systems with small band gaps.",
        "Particle-wall interactions play a crucially important role in various\napplications such as microfluidic devices for cell sorting, particle\nseparation, entire class of hydrodynamic filtration and its derivatives, etc.\nYet, accurate implementation of interactions between wall and finite-size\nparticle is not trivial when working with the currently available particle\ntracking algorithms\/packages as they typically work with point-wise particles.\nHerein, we report a particle tracking algorithm that takes into account\ninteractions between particles of finite size and solid objects existing inside\ncomputational domain. A particle is modeled as a set of circumferential points\non its perimeter. While fluid-particle interactions are captured during the\ntrack of particle center, interactions between particle and nearby solid\nobjects are modeled explicitly by examining circumferential points and applying\na reflection scheme as needed to ensure impenetrability of solid objects. We\nalso report a modified variant of auxiliary structured grid method to locate\nhosting cells, which in conjunction with a boundary condition scheme enables\nthe capture of interactions between particle and solid objects. As a\nproof-of-concept, we numerically and experimentally study the motion of\nparticles within a microfluidic deterministic lateral displacement device. The\nmodeling results successfully demonstrate the zig-zag and bumping displacement\nmodes observed in our experiments. We also study a microfluidic device with\npinched flow numerically and validate our results against experimental data\nfrom the literature. By demonstrating an almost 8x speedup on a system with 8\nPerformance threads, our investigations suggest that the particle tracking\nalgorithm and its implementation code can benefit from parallel processing on\nmulti-thread systems by using the OpenMP application programming interface.",
        "Purpose: Iliac wing fractures due to lap belt loading have been observed in\nlaboratory settings for 50 years and recent data suggest they are also\noccurring in the field. Automated driving systems (ADS) and other occupant\ncompartment advancements are expected to offer enhanced flexibility in seating\norientation, which could place a greater reliance on the seatbelt to restrain\noccupants. Such changes may increase seatbelt loads and create new challenges\nin successfully restraining occupants and mitigating injury to areas such as\nthe pelvis. Injury criteria exist for component-level male iliac wing fractures\nresulting from frontal lap belt loading, but not for females. Methods: This\nstudy explored female iliac wing fracture tolerance in the same loading\nenvironment as a previous study that explored the fracture tolerance of\nisolated male iliac wings. Male and female fracture data were combined to\nevaluate the effect of sex. Injury risk functions were created by fitting\nWeibull survival models to data that integrated censored and exact failure\nobservations. Results: Twenty female iliac wings were tested; fourteen of them\nsustained fracture with known failure forces (exact), but the remaining six\nwings either (1) did not fracture, or (2) fractured after an event that changed\nthe boundary conditions (right censored). The fracture tolerance of the tested\nspecimens ranged widely (1134 - 8759 N) and averaged 4240 N (SD 2516 N).\nConclusion: Female data and combined male-female data were analyzed. Age was\nthe only covariate investigated in this study that had a statistically\nsignificant effect and improved the predictive performance of the models.",
        "Spatial misalignment problems arise from both data aggregation and attempts\nto align misaligned data, leading to information loss. We propose a Bayesian\ndisaggregation framework that links misaligned data to a continuous domain\nmodel using an iteratively linearised integration method via integrated nested\nLaplace approximation (INLA). The framework supports point pattern and\naggregated count models under four covariate field scenarios: \\textit{Raster at\nFull Resolution (RastFull), Raster Aggregation (RastAgg), Polygon Aggregation\n(PolyAgg), and Point Values (PointVal)}. The first three involve aggregation,\nwhile the latter two have incomplete fields. For PolyAgg and PointVal, we\nestimate the full covariate field using \\textit{Value Plugin, Joint\nUncertainty, and Uncertainty Plugin} methods, with the latter two accounting\nfor uncertainty propagation. These methods demonstrate superior performance,\nand remain more robust even under model misspecification (i.e.\\ modelling a\nnonlinear field as linear).\n  In landslide studies, landslide occurrences are often aggregated into counts\nbased on slope units, reducing spatial detail. The results indicate that point\npattern observations and full-resolution covariate fields should be\nprioritized. For incomplete fields, methods incorporating uncertainty\npropagation are preferred. This framework supports landslide susceptibility and\nother spatial mapping, integrating seamlessly with INLA-extension packages.",
        "Exploring various unexpected new quantum states and their corresponding\nextraordinary physics in low dimensional quantum materials, and investing them\ninto application fields, is a primary concern of condensed matter physics.\nPreviously, we performed joint experiments and theoretical calculations to\ninvestigate a superatomic crystal of Au6Te12Se8 with low symmetry, which stacks\nthrough non-covalent inter-cube quasi-bonds. Au6Te12Se8 exhibits a triple-cube\ncharge density wave and spatially polarized metallic states interweaving at 9\nK. In addition, it undergoes a BKT phase transition at 2.8 K to form a\nquasi-two-dimensional superconducting state. The subsequent high-pressure\nexperimental results indicate that the two quantum states above compete with\nsuperconductivity. Here, we experimentally revealed competition and coexistence\namong superconductivity, triple-cube charge density waves, and polarized\nmetallic states during further temperature-decreasing process, as examined\nusing ultra-low temperature scanning tunneling microscopy\/spectroscopy,\ntransport measurement, and Raman spectra. An extraordinary inversion symmetry\nbroken emerged inside the triple-cube-period of the original CDW at 300 mK,\nleading to the polarized metallic states transforming into parallel\npolarization states. The evidence of spontaneous polarization coexisting with\nsuperconductivity in real space is extremely rare and has been revealed by STM.\nIn addition, transport and Raman measurements indicate that there are multiple\nphase transition temperatures from 80K to that below the superconducting\ntransition temperature of Au6Te12Se8, which may also contain more unknown\nexotic quantum states, providing a platform for further exploration of\nintriguing physical properties.",
        "The parallel solver of the general synthetic iterative scheme (GSIS), as\nrecently developed by Zhang \\textit{et. al.} in Comput. Fluids 281 (2024)\n106374, is an efficient method to find the solution of the Boltzmann equation\ndeterministically. However, it consumes a significant computational memory due\nto the discretization of molecular velocity space in hypersonic flows. In this\npaper, we address this issue by introducing the adaptive GSIS, where the\nBoltzmann equation is applied only in rarefied regions when the local Knudsen\nnumber exceeds a reference value, $\\text{Kn}{ref}$. In contrast, the\nNavier-Stokes equations, with and without the high-order corrections to the\nconstitutive relations, are applied in the continuum and rarefied regimes,\nrespectively. Numerical results indicate that setting $\\text{Kn}{ref}=0.01$\nyields acceptable outcomes. With the adaptive GSIS, the computational memory\nand time can be significantly reduced in near-continuum flows, e.g. 24 and 7\ntimes, respectively. in the simulation of rarefied gas flow passing the\nInternational Space Station.",
        "The expressive power of neural networks in modelling non-trivial\ndistributions can in principle be exploited to bypass topological freezing and\ncritical slowing down in simulations of lattice field theories. Some popular\napproaches are unable to sample correctly non-trivial topology, which may lead\nto some classes of configurations not being generated. In this contribution, we\npresent a novel generative method inspired by a model previously introduced in\nthe ML community (GFlowNets). We demonstrate its efficiency at exploring\nergodically configuration manifolds with non-trivial topology through\napplications such as triple ring models and two-dimensional lattice scalar\nfield theory.",
        "We state and prove a stratification result that allows us to classify the\ntensor ideal localizing subcategories for the stable module category\n$\\text{Stab}(\\mathcal{C}_{(\\mathfrak{g}, \\mathfrak{g}_{\\bar 0})})$ of Lie\nsuperalgbera representations which are semisimple as representations of\n$\\mathfrak{g}_{\\bar 0}$ under the hypotheses that $\\mathfrak{g}$ is a classical\nLie superalgebra with a splitting detecting subalgebra $\\mathfrak{z} \\leq\n\\mathfrak{g}$, as well as a natural hypothesis on realization of supports. This\nextends the work of the author and Nakano where a similar classification was\nobtained for the stable category of modules over a detecting subalgebra\nemploying stratification in the sense of Benson, Iyengar, and Krause. Our new\nresult involves making use of a more general stratification framework in weakly\nNoetherian contexts developed by Barthel, Heard, and Sanders using the\nBalmer-Favi notion of support for big objects in tensor triangulated\ncategories, as well as the recently developed homological stratification of\nBarthel, Heard, Sanders, and Zou in using the homological spectrum.",
        "In the recent years, high energy physics discoveries have been driven by the\nincreasing of luminosity and\/or detector granularity. This evolution gives\naccess to bigger statistics and data samples, but can make it hard to process\nresults with current methods and algorithms. Graph convolution networks, have\nbeen shown to be powerful tools to address these challenges. We present our\ngraph convolution framework for particle identification and energy regression\nin high granularity calorimeters. In particular, we introduce our algorithm for\noptimised graph construction in resource constrained environments. We also\nintroduce our implementation of graph convolution and pooling layers. We\nobserve satisfying accuracies, and discuss possible application to other high\ngranularity particle detector challenges.",
        "Modern Imaging Atmospheric Cherenkov Telescopes (IACTs) generate a huge\namount of data that must be classified automatically, ideally in real time.\nCurrently, machine learning-based solutions are increasingly being used to\nsolve classification problems. However, these classifiers require proper\ntraining data sets to work correctly. The problem with training neural networks\non real IACT data is that these data need to be pre-labeled, whereas such\nlabeling is difficult and its results are estimates. In addition, the\ndistribution of incoming events is highly imbalanced. Firstly, there is an\nimbalance in the types of events, since the number of detected gamma quanta is\nsignificantly less than the number of protons. Secondly, the energy\ndistribution of particles of the same type is also imbalanced, since\nhigh-energy particles are extremely rare. This imbalance results in poorly\ntrained classifiers that, once trained, do not handle rare events correctly.\nUsing only conventional Monte Carlo event simulation methods to solve this\nproblem is possible, but extremely resource-intensive and time-consuming. To\naddress this issue, we propose to perform data augmentation with artificially\ngenerated events of the desired type and energy using conditional generative\nadversarial networks (cGANs), distinguishing classes by energy values. In the\npaper, we describe a simple algorithm for generating balanced data sets using\ncGANs. Thus, the proposed neural network model produces both imbalanced data\nsets for physical analysis as well as balanced data sets suitable for training\nother neural networks.",
        "We investigate the transition between elastic and viscous regimes for\ntime-reversal broken Weyl semimetals. In these materials, Hall transport occurs\nthrough two parallel channels: the Fermi sea and the Fermi surface. The Fermi\nsea part remains unaffected by electron-electron scattering, whereas the Fermi\nsurface is influenced by it. We model the disorder by dilute impenetrable\nspherical impurities. We analyze the flow of an electronic fluid with a finite\nodd viscosity in the presence of such disorder and compute the conductivity\ntensor. We find that in the generic case of finite intrinsic conductivity, the\nHall angle in the viscous regime is parametrically suppressed compared to the\nelastic regime. In the special case where the intrinsic conductivity vanishes,\nthe ratio between the transverse and the longitudinal resistivities matches the\nratio between the odd and even components of the viscosity tensor.",
        "We study the bifurcation phenomena between spherical and axisymmetric bosonic\nstars. By numerically solving for the zero-modes of spherical bosonic stars\nunder specific axially symmetric perturbations, we discover that excited state\nspherical bosonic stars bifurcate into two types of axisymmetric bosonic stars\nunder $\\ell=2$ perturbations, with matter distributions resembling chains and\nrings, respectively. Meanwhile, $\\ell=4$ axisymmetric perturbations lead\nspherical scalar bosonic stars to bifurcate into a new type of axisymmetric\nbosonic stars, exhibiting a mixed chain-like and ring-like matter distribution,\nwhich we refer to as gyroscope-like. Additionally, for the first time, we have\nconstructed chains of scalar bosonic stars with 7 constituents and their\ncorresponding ring-like scalar bosonic stars. Our results provide an\nexplanation for the bifurcations in bosonic stars from the perspective of\nperturbations, and by analyzing physical quantities such as quadrupoles and\nenergy densities we systematically discuss the impact of axisymmetric\nperturbations on spherical bosonic stars.",
        "We prove local-in-time a-priori estimates in $H^{-1}(\\mathbb{R})$ for a\nfamily of generalized Korteweg--de Vries equations. This is the first estimate\nfor any non-integrable perturbation of the KdV equation that matches the\nregularity of the sharp well-posedness theory for KdV. In particular, we show\nthat our analysis applies to models for long waves in a shallow channel of\nwater with an uneven bottom.\n  The proof of our main result is based upon a bootstrap argument for the\nrenormalized perturbation determinant coupled with a local smoothing norm.",
        "Sharing diverse genomic and other biomedical datasets is critical to advance\nscientific discoveries and their equitable translation to improve human health.\nHowever, data sharing remains challenging in the context of legacy datasets,\nevolving policies, multi-institutional consortium science, and international\nstakeholders. The NIH-funded Polygenic Risk Methods in Diverse Populations\n(PRIMED) Consortium was established to improve the performance of polygenic\nrisk estimates for a broad range of health and disease outcomes with global\nimpacts. Improving polygenic risk score performance across genetically diverse\npopulations requires access to large, diverse cohorts. We report on the design\nand implementation of data sharing policies and procedures developed in PRIMED\nto aggregate and analyze data from multiple, heterogeneous sources while\nadhering to existing data sharing policies for each integrated dataset. We\ndescribe two primary data sharing mechanisms: coordinated dbGaP applications\nand a Consortium Data Sharing Agreement, as well as provide alternatives when\nindividual-level data cannot be shared within the Consortium (e.g., federated\nanalyses). We also describe technical implementation of Consortium data sharing\nin the NHGRI Analysis Visualization and Informatics Lab-space (AnVIL) cloud\nplatform, to share derived individual-level data, genomic summary results, and\nmethods workflows with appropriate permissions. As a Consortium making\nsecondary use of pre-existing data sources, we also discuss challenges and\npropose solutions for release of individual- and summary-level data products to\nthe broader scientific community. We make recommendations for ongoing and\nfuture policymaking with the goal of informing future consortia and other\nresearch activities.",
        "Integrated photonics has revolutionized optical communication, sensing, and\ncomputation, offering miniaturized and lightweight solutions for spacecraft\nwith limited size and payload. Novel chip-scale instruments based on\nultralow-loss integrated photonic platforms, including lasers, frequency combs\nand atomic traps, have been developed for space applications. Therefore,\nquantifying the space compatibility of ultralow-loss photonic integrated\ncircuits (PICs), particularly their radiation resistance, is critical. This\nstudy experimentally evaluates the radiation resistance of ultralow-loss\nSi$_3$N$_4$, 4H-SiC, and LiNbO$_3$ PICs under intense $\\gamma$-ray and\nhigh-energy proton irradiation. Results show that proton irradiation with $1.1\n\\times 10^{10}$ $\\mathrm{p\/cm^2}$ total flux does not significantly increase\noptical loss or alter the refractive index of these PICs, while $\\gamma$-ray\nirradiation with 1.2 Mrad accumulated dose only marginally increases their\noptical loss. These findings provide preliminary evidence of the excellent\nspace compatibility of ultralow-loss Si$_3$N$_4$, 4H-SiC, and LiNbO$_3$ PICs,\nhighlighting their potential for compact and lightweight space systems.",
        "In networked dynamical systems, inferring governing parameters is crucial for\npredicting nodal dynamics, such as gene expression levels, species abundance,\nor population density. While many parameter estimation techniques rely on\ntime-series data, particularly systems that converge over extreme time ranges,\nonly noisy steady-state data is available, requiring a new approach to infer\ndynamical parameters from noisy observations of steady states. However, the\ntraditional optimization process is computationally demanding, requiring\nrepeated simulation of coupled ordinary differential equations (ODEs). To\novercome these limitations, we introduce a surrogate objective function that\nleverages decoupled equations to compute steady states, significantly reducing\ncomputational complexity. Furthermore, by optimizing the surrogate objective\nfunction, we obtain steady states that more accurately approximate the ground\ntruth than noisy observations and predict future equilibria when topology\nchanges. We empirically demonstrate the effectiveness of the proposed method\nacross ecological, gene regulatory, and epidemic networks. Our approach\nprovides an efficient and effective way to estimate parameters from\nsteady-state data and has the potential to improve predictions in networked\ndynamical systems.",
        "The global analytic hypoellipticity is proved for a class of second order\npartial differential equations with non-negative characteristic form globally\ndefined on the torus. The class considered in this work generalizes at some\ndegree the class of sum of squares considered by Bove-Chinni and also by\nCordaro-Himonas.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "In the 1970s, structural Ramsey theory emerged as a new branch of\ncombinatorics. This development came with the isolation of the concepts of the\n$\\mathbf{A}$-Ramsey property and Ramsey class. Following the influential\nNe\\v{s}et\\v{r}il-R\\\"{o}dl theorem, several Ramsey classes have been identified.\nIn the 1980s Ne\\v{s}et\\v{r}il, inspired by a seminar of Lachlan, discovered a\ncrucial connection between Ramsey classes and Fra\\\"{\\i}ss\\'{e} classes and, in\nhis 1989 paper, connected the classification programme of homogeneous\nstructures to structural Ramsey theory. In 2005, Kechris, Pestov, and\nTodor\\v{c}evi\\'{c} revitalized the field by connecting Ramsey classes to\ntopological dynamics. This breakthrough motivated Ne\\v{s}et\\v{r}il to propose a\nprogram for classifying Ramsey classes. We review the progress made on this\nprogram in the past two decades, list open problems, and discuss recent\nextensions to new areas, namely the extension property for partial\nautomorphisms (EPPA), and big Ramsey structures."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials",
    "start_abstract":"The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "A deep-learning approach to realizing functionality in nanoelectronic devices"
      ],
      "abstract":[
        "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "On conductor submonoids of factorial monoids",
        "Depth of powers of edge ideals of edge-weighted integrally closed cycles",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Spatial-temporal models for forest inventory data",
        "Supervised Manifold Learning for Functional Data",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Encrypted Vector Similarity Computations Using Partially Homomorphic\n  Encryption: Applications and Performance Analysis",
        "FinTSB: A Comprehensive and Practical Benchmark for Financial Time\n  Series Forecasting",
        "Integrative Learning of Intensity Fluctuations of Quantum Dots under\n  Excitation via a Tailored Mixture Hidden Markov Model",
        "Contracting Strategies for Electrolyzers to Secure Grid Connection: The\n  Dutch Case",
        "Naked Eye Three-dimensional Display System Based on Time-multiplexed\n  Technology",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Unfitted boundary algebraic equation method based on difference\n  potentials and lattice Green's function in 3D",
        "PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery",
        "Ultrafast pulsed laser evaluation of Single Event Transients in\n  opto-couplers",
        "ScamFerret: Detecting Scam Websites Autonomously with Large Language\n  Models",
        "A Family of Semi-norms in $C^*$-algebras",
        "Domain Adaptation for Japanese Sentence Embeddings with Contrastive\n  Learning based on Synthetic Sentence Generation",
        "Graph Generative Pre-trained Transformer",
        "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment",
        "City Models: Past, Present and Future Prospects",
        "CAT: Content-Adaptive Image Tokenization",
        "Supervised Quadratic Feature Analysis: An Information Geometry Approach\n  to Dimensionality Reduction",
        "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG",
        "A Novel Phenomenological Model of Equalization-enhanced Phase Noise",
        "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
        "Digital Twin Calibration with Model-Based Reinforcement Learning",
        "Teacher-student training improves accuracy and efficiency of machine\n  learning inter-atomic potentials",
        "CoT-Valve: Length-Compressible Chain-of-Thought Tuning"
      ],
      "abstract":[
        "We give affirmative answers to Conjecture 4.16 in [1] and to Conjecture 2.3\nin [3].",
        "This paper gives some exact formulas for the depth of powers of the edge\nideal of an edge-weighted integrally closed cycle.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "The USDA Forest Inventory and Analysis (FIA) program conducts a national\nforest inventory for the United States through a network of permanent field\nplots. FIA produces estimates of area averages\/totals for plot-measured forest\nvariables through design-based inference, assuming a fixed population and a\nprobability sample of field plot locations. The fixed-population assumption and\ncharacteristics of the FIA sampling scheme make it difficult to estimate change\nin forest variables over time using design-based inference. We propose\nspatial-temporal models based on Gaussian processes as a flexible tool for\nforest inventory data, capable of inferring forest variables and change thereof\nover arbitrary spatial and temporal domains. It is shown to be beneficial for\nthe covariance function governing the latent Gaussian process to account for\nvariation at multiple scales, separating spatially local variation from\necosystem-scale variation. We demonstrate a model for forest biomass density,\ninferring 20 years of biomass change within two US National Forests.",
        "Classification is a core topic in functional data analysis. A large number of\nfunctional classifiers have been proposed in the literature, most of which are\nbased on functional principal component analysis or functional regression. In\ncontrast, we investigate this topic from the perspective of manifold learning.\nIt is assumed that functional data lie on an unknown low-dimensional manifold,\nand we expect that better classifiers can be built upon the manifold structure.\nTo this end, we propose a novel proximity measure that takes the label\ninformation into account to learn the low-dimensional representations, also\nknown as the supervised manifold learning outcomes. When the outcomes are\ncoupled with multivariate classifiers, the procedure induces a family of new\nfunctional classifiers. In theory, we show that our functional classifier\ninduced by the $k$-NN classifier is asymptotically optimal. In practice, we\nshow that our method, coupled with several classical multivariate classifiers,\nachieves outstanding classification performance compared to existing functional\nclassifiers in both synthetic and real data examples.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "This paper explores the use of partially homomorphic encryption (PHE) for\nencrypted vector similarity search, with a focus on facial recognition and\nbroader applications like reverse image search, recommendation engines, and\nlarge language models (LLMs). While fully homomorphic encryption (FHE) exists,\nwe demonstrate that encrypted cosine similarity can be computed using PHE,\noffering a more practical alternative. Since PHE does not directly support\ncosine similarity, we propose a method that normalizes vectors in advance,\nenabling dot product calculations as a proxy. We also apply min-max\nnormalization to handle negative dimension values.\n  Experiments on the Labeled Faces in the Wild (LFW) dataset use DeepFace's\nFaceNet128d, FaceNet512d, and VGG-Face (4096d) models in a two-tower setup.\nPre-encrypted embeddings are stored in one tower, while an edge device captures\nimages, computes embeddings, and performs encrypted-plaintext dot products via\nadditively homomorphic encryption. We implement this with LightPHE, evaluating\nPaillier, Damgard-Jurik, and Okamoto-Uchiyama schemes, excluding others due to\nperformance or decryption complexity. Tests at 80-bit and 112-bit security\n(NIST-secure until 2030) compare PHE against FHE (via TenSEAL), analyzing\nencryption, decryption, operation time, cosine similarity loss, key\/ciphertext\nsizes.\n  Results show PHE is less computationally intensive, faster, and produces\nsmaller ciphertexts\/keys, making it well-suited for memory-constrained\nenvironments and real-world privacy-preserving encrypted similarity search.",
        "Financial time series (FinTS) record the behavior of human-brain-augmented\ndecision-making, capturing valuable historical information that can be\nleveraged for profitable investment strategies. Not surprisingly, this area has\nattracted considerable attention from researchers, who have proposed a wide\nrange of methods based on various backbones. However, the evaluation of the\narea often exhibits three systemic limitations: 1. Failure to account for the\nfull spectrum of stock movement patterns observed in dynamic financial markets.\n(Diversity Gap), 2. The absence of unified assessment protocols undermines the\nvalidity of cross-study performance comparisons. (Standardization Deficit), and\n3. Neglect of critical market structure factors, resulting in inflated\nperformance metrics that lack practical applicability. (Real-World Mismatch).\nAddressing these limitations, we propose FinTSB, a comprehensive and practical\nbenchmark for financial time series forecasting (FinTSF). To increase the\nvariety, we categorize movement patterns into four specific parts, tokenize and\npre-process the data, and assess the data quality based on some sequence\ncharacteristics. To eliminate biases due to different evaluation settings, we\nstandardize the metrics across three dimensions and build a user-friendly,\nlightweight pipeline incorporating methods from various backbones. To\naccurately simulate real-world trading scenarios and facilitate practical\nimplementation, we extensively model various regulatory constraints, including\ntransaction fees, among others. Finally, we conduct extensive experiments on\nFinTSB, highlighting key insights to guide model selection under varying market\nconditions. Overall, FinTSB provides researchers with a novel and comprehensive\nplatform for improving and evaluating FinTSF methods. The code is available at\nhttps:\/\/github.com\/TongjiFinLab\/FinTSBenchmark.",
        "Semiconductor nano-crystals, known as quantum dots (QDs), have garnered\nsignificant interest in various scientific fields due to their unique\nfluorescence properties. One captivating characteristic of QDs is their ability\nto emit photons under continuous excitation. The intensity of photon emission\nfluctuates during the excitation, and such a fluctuation pattern can vary\nacross different dots even under the same experimental conditions. What adding\nto the complication is that the processed intensity series are non-Gaussian and\ntruncated due to necessary thresholding and normalization. As such,\nconventional approaches in the chemistry literature, typified by single-dot\nanalysis of raw intensity data with Gaussian hidden Markov models (HMM), cannot\nmeet the many analytical challenges and may fail to capture any novel yet rare\nfluctuation patterns among QDs. Collaborating with scientists in the chemistry\nfield, we have developed an integrative learning approach to simultaneously\nanalyzing intensity series of multiple QDs. Our approach still inherits the HMM\nas the skeleton to model the intensity fluctuations of each dot, and based on\nthe data structure and the hypothesized collective behaviors of the QDs, our\napproach asserts that (i) under each hidden state, the normalized intensity\nfollows a 0\/1 inflated Beta distribution, (ii) the state distributions are\nshared across all the QDs, and (iii) the patterns of transitions can vary\nacross QDs. These unique features allow for a precise characterization of the\nintensity fluctuation patterns and facilitate the clustering of the QDs. With\nexperimental data collected on 128 QDs, our methods reveal several QD clusters\ncharacterized by unique transition patterns across three intensity states. The\nresults provide deeper insight into QD behaviors and their design\/application\npotentials.",
        "In response to increasing grid congestion in the Netherlands, non-firm\nconnection and transport agreements (CTAs) and capacity restriction contracts\n(CRCs) have been introduced, allowing consumer curtailment in exchange for grid\ntariff discounts or per-MW compensations. This study examines the interaction\nbetween an electrolyzer project, facing sizing and contracting decisions, and a\nnetwork operator, responsible for contract activations and determining grid\nconnection capacity, under the new Dutch regulations. The interaction is\nmodeled using two bilevel optimization problems with alternating\nleader-follower roles. Results highlight a trade-off between CRC income and\nnon-firm CTA tariff discounts, showing that voluntary congestion management by\nthe network operator increases electrolyzer profitability at CRC prices below\n10 euro per MW but reduces it at higher prices. Furthermore, the network\noperator benefits more from reacting to the electrolyzer owner's CTA decisions\nthan from leading the interaction at CRC prices above 10 euro per MW. Ignoring\nthe other party's optimization problem overestimates profits for both the\nnetwork operator and the electrolyzer owner, emphasizing the importance of\ncoordinated decision-making.",
        "Our group is developing a multi-user eye-tracked 3D display, an evolution of\nthe single-user eye-tracked 3D display that we have already successfully\ndeveloped. This display utilizes a slanted lenticular setup, where multiple\nperspective views are shown across the viewing field. Due to the constraints of\nthe lenticular lens parameters, identical views are repeated across the field,\nlimiting eye tracking to a single user. However, this limitation can be\naddressed using spatio-temporal multiplexing, where view zone groups are\npresented sequentially with a high frame rate liquid crystal display (LCD) and\ndriver, in combination with a synchronized directional light emitting diode\n(LED) array. In this paper, we describe the operation and results of the\nbacklight drive electronics, where a prototype using a white LED illumination\nmatrix, a simplified LCD panel, and a linear Fresnel lens array serves as a\ntest bed.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "This work presents an unfitted boundary algebraic equation (BAE) method for\nsolving three-dimensional elliptic partial differential equations on complex\ngeometries using finite difference on structured meshes. We demonstrate that\nreplacing finite auxiliary domains with free-space LGFs streamlines the\ncomputation of difference potentials, enabling matrix-free implementations and\nsignificant cost reductions. We establish theoretical foundations by showing\nthe equivalence between direct formulations in difference potentials framework\nand indirect single\/double layer formulations and analyzing their spectral\nproperties. The spectral analysis demonstrates that discrete double layer\nformulations provide better-conditioned systems for iterative solvers,\nsimilarly as in boundary integral method. The method is validated through\nmatrix-free numerical experiments on both Poisson and modified Helmholtz\nequations in 3D implicitly defined geometries, showing optimal convergence\nrates and computational efficiency. This framework naturally extends to\nunbounded domains and provides a foundation for applications to more complex\nsystems like Helmholtz and Stokes equations.",
        "Recent advancements in language-guided diffusion models for image editing are\noften bottle-necked by cumbersome prompt engineering to precisely articulate\ndesired changes. An intuitive alternative calls on guidance from in-the-wild\nimage exemplars to help users bring their imagined edits to life. Contemporary\nexemplar-based editing methods shy away from leveraging the rich latent space\nlearnt by pre-existing large text-to-image (TTI) models and fall back on\ntraining with curated objective functions to achieve the task. Though somewhat\neffective, this demands significant computational resources and lacks\ncompatibility with diverse base models and arbitrary exemplar count. On further\ninvestigation, we also find that these techniques restrict user control to only\napplying uniform global changes over the entire edited region. In this paper,\nwe introduce a novel framework for progressive exemplar-driven editing with\noff-the-shelf diffusion models, dubbed PIXELS, to enable customization by\nproviding granular control over edits, allowing adjustments at the pixel or\nregion level. Our method operates solely during inference to facilitate\nimitative editing, enabling users to draw inspiration from a dynamic number of\nreference images, or multimodal prompts, and progressively incorporate all the\ndesired changes without retraining or fine-tuning existing TTI models. This\ncapability of fine-grained control opens up a range of new possibilities,\nincluding selective modification of individual objects and specifying gradual\nspatial changes. We demonstrate that PIXELS delivers high-quality edits\nefficiently, leading to a notable improvement in quantitative metrics as well\nas human evaluation. By making high-quality image editing more accessible,\nPIXELS has the potential to enable professional-grade edits to a wider audience\nwith the ease of using any open-source image generation model.",
        "We build a 1064 nm fiber laser system-based testing facility for emulating\nSETs in different electronics components and ICs. Using these facilities, we\ntested the 4N35 optocoupler to observe SETs for the first time.",
        "With the rise of sophisticated scam websites that exploit human psychological\nvulnerabilities, distinguishing between legitimate and scam websites has become\nincreasingly challenging. This paper presents ScamFerret, an innovative agent\nsystem employing a large language model (LLM) to autonomously collect and\nanalyze data from a given URL to determine whether it is a scam. Unlike\ntraditional machine learning models that require large datasets and feature\nengineering, ScamFerret leverages LLMs' natural language understanding to\naccurately identify scam websites of various types and languages without\nrequiring additional training or fine-tuning. Our evaluation demonstrated that\nScamFerret achieves 0.972 accuracy in classifying four scam types in English\nand 0.993 accuracy in classifying online shopping websites across three\ndifferent languages, particularly when using GPT-4. Furthermore, we confirmed\nthat ScamFerret collects and analyzes external information such as web content,\nDNS records, and user reviews as necessary, providing a basis for identifying\nscam websites from multiple perspectives. These results suggest that LLMs have\nsignificant potential in enhancing cybersecurity measures against sophisticated\nscam websites.",
        "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
        "Several backbone models pre-trained on general domain datasets can encode a\nsentence into a widely useful embedding. Such sentence embeddings can be\nfurther enhanced by domain adaptation that adapts a backbone model to a\nspecific domain. However, domain adaptation for low-resource languages like\nJapanese is often difficult due to the scarcity of large-scale labeled\ndatasets. To overcome this, this paper introduces SDJC (Self-supervised Domain\nadaptation for Japanese sentence embeddings with Contrastive learning) that\nutilizes a data generator to generate sentences, which have the same syntactic\nstructure to a sentence in an unlabeled specific domain corpus but convey\ndifferent semantic meanings. Generated sentences are then used to boost\ncontrastive learning that adapts a backbone model to accurately discriminate\nsentences in the specific domain. In addition, the components of SDJC like a\nbackbone model and a method to adapt it need to be carefully selected, but no\nbenchmark dataset is available for Japanese. Thus, a comprehensive Japanese STS\n(Semantic Textual Similarity) benchmark dataset is constructed by combining\ndatasets machine-translated from English with existing datasets. The\nexperimental results validates the effectiveness of SDJC on two domain-specific\ndownstream tasks as well as the usefulness of the constructed dataset.\nDatasets, codes and backbone models adapted by SDJC are available on our github\nrepository https:\/\/github.com\/ccilab-doshisha\/SDJC.",
        "Graph generation is a critical task in numerous domains, including molecular\ndesign and social network analysis, due to its ability to model complex\nrelationships and structured data. While most modern graph generative models\nutilize adjacency matrix representations, this work revisits an alternative\napproach that represents graphs as sequences of node set and edge set. We\nadvocate for this approach due to its efficient encoding of graphs and propose\na novel representation. Based on this representation, we introduce the Graph\nGenerative Pre-trained Transformer (G2PT), an auto-regressive model that learns\ngraph structures via next-token prediction. To further exploit G2PT's\ncapabilities as a general-purpose foundation model, we explore fine-tuning\nstrategies for two downstream applications: goal-oriented generation and graph\nproperty prediction. We conduct extensive experiments across multiple datasets.\nResults indicate that G2PT achieves superior generative performance on both\ngeneric graph and molecule datasets. Furthermore, G2PT exhibits strong\nadaptability and versatility in downstream tasks from molecular design to\nproperty prediction.",
        "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
        "We attempt to take a comprehensive look at the challenges of representing the\nspatio-temporal structures and dynamic processes defining a city's overall\ncharacteristics. For the task of urban planning and urban operation, we take\nthe stance that even if the necessary representations of these structures and\nprocesses can be achieved, the most important representation of the relevant\nmindsets of the citizens are, unfortunately, mostly neglected.\n  After a review of major \"traditional\" urban models of structures behind urban\nscale, form, and dynamics, we turn to major recent modeling approaches\ntriggered by recent advances in AI that enable multi-modal generative models.\nSome of these models can create representations of geometries, networks and\nimages, and reason flexibly at a human-compatible semantic level. They provide\nhuge amounts of knowledge extracted from Terabytes of text and image documents\nand cover the required rich representation spectrum including geographic\nknowledge by different knowledge sources, degrees of granularity and scales.\n  We then discuss what these new opportunities mean for the modeling challenges\nposed by cities, in particular with regard to the role and impact of citizens\nand their interactions within the city infrastructure. We propose to integrate\nthese possibilities with existing approaches, such as agent-based models, which\nopens up new modeling spaces including rich citizen models which are able to\nalso represent social interactions.\n  Finally, we put forward some thoughts about a vision of a \"social AI in a\ncity ecosystem\" that adds relevant citizen models to state-of-the-art\nstructural and process models. This extended city representation will enable\nurban planners to establish citizen-oriented planning of city infrastructures\nfor human culture, city resilience and sustainability.",
        "Most existing image tokenizers encode images into a fixed number of tokens or\npatches, overlooking the inherent variability in image complexity. To address\nthis, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts\nrepresentation capacity based on the image content and encodes simpler images\ninto fewer tokens. We design a caption-based evaluation system that leverages\nlarge language models (LLMs) to predict content complexity and determine the\noptimal compression ratio for a given image, taking into account factors\ncritical to human perception. Trained on images with diverse compression\nratios, CAT demonstrates robust performance in image reconstruction. We also\nutilize its variable-length latent representations to train Diffusion\nTransformers (DiTs) for ImageNet generation. By optimizing token allocation,\nCAT improves the FID score over fixed-ratio baselines trained with the same\nflops and boosts the inference throughput by 18.5%.",
        "Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Despite\nthe availability of methods for learning complex non-linear features (e.g. Deep\nLearning), there is an enduring demand for dimensionality reduction methods\nthat learn linear features due to their interpretability, low computational\ncost, and broad applicability. However, there is a gap between methods that\noptimize linear separability (e.g. LDA), and more flexible but computationally\nexpensive methods that optimize over arbitrary class boundaries (e.g.\nmetric-learning methods). Here, we present Supervised Quadratic Feature\nAnalysis (SQFA), a dimensionality reduction method for learning linear features\nthat maximize the differences between class-conditional first- and second-order\nstatistics, which allow for quadratic discrimination. SQFA exploits the\ninformation geometry of second-order statistics in the symmetric positive\ndefinite manifold. We show that SQFA features support quadratic\ndiscriminability in real-world problems. We also provide a theoretical link,\nbased on information geometry, between SQFA and the Quadratic Discriminant\nAnalysis (QDA) classifier.",
        "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench.",
        "We show that equalization-enhanced phase noise manifests as a time-varying,\nfrequency-dependent phase error, which can be modeled and reversed by a\ntime-varying all-pass finite impulse response filter.",
        "Capturing high dynamic range (HDR) scenes is one of the most important issues\nin camera design. Majority of cameras use exposure fusion technique, which\nfuses images captured by different exposure levels, to increase dynamic range.\nHowever, this approach can only handle images with limited exposure difference,\nnormally 3-4 stops. When applying to very high dynamic scenes where a large\nexposure difference is required, this approach often fails due to incorrect\nalignment or inconsistent lighting between inputs, or tone mapping artifacts.\nIn this work, we propose UltraFusion, the first exposure fusion technique that\ncan merge input with 9 stops differences. The key idea is that we model the\nexposure fusion as a guided inpainting problem, where the under-exposed image\nis used as a guidance to fill the missing information of over-exposed highlight\nin the over-exposed region. Using under-exposed image as a soft guidance,\ninstead of a hard constrain, our model is robust to potential alignment issue\nor lighting variations. Moreover, utilizing the image prior of the generative\nmodel, our model also generates natural tone mapping, even for very\nhigh-dynamic range scene. Our approach outperforms HDR-Transformer on latest\nHDR benchmarks. Moreover, to test its performance in ultra high dynamic range\nscene, we capture a new real-world exposure fusion benchmark, UltraFusion\nDataset, with exposure difference up to 9 stops, and experiments show that\n\\model~can generate beautiful and high-quality fusion results under various\nscenarios. An online demo is provided at\nhttps:\/\/openimaginglab.github.io\/UltraFusion\/.",
        "This paper presents a novel methodological framework, called the\nActor-Simulator, that incorporates the calibration of digital twins into\nmodel-based reinforcement learning for more effective control of stochastic\nsystems with complex nonlinear dynamics. Traditional model-based control often\nrelies on restrictive structural assumptions (such as linear state transitions)\nand fails to account for parameter uncertainty in the model. These issues\nbecome particularly critical in industries such as biopharmaceutical\nmanufacturing, where process dynamics are complex and not fully known, and only\na limited amount of data is available. Our approach jointly calibrates the\ndigital twin and searches for an optimal control policy, thus accounting for\nand reducing model error. We balance exploration and exploitation by using\npolicy performance as a guide for data collection. This dual-component approach\nprovably converges to the optimal policy, and outperforms existing methods in\nextensive numerical experiments based on the biopharmaceutical manufacturing\ndomain.",
        "Machine learning inter-atomic potentials (MLIPs) are revolutionizing the\nfield of molecular dynamics (MD) simulations. Recent MLIPs have tended towards\nmore complex architectures trained on larger datasets. The resulting increase\nin computational and memory costs may prohibit the application of these MLIPs\nto perform large-scale MD simulations. Here, we present a teacher-student\ntraining framework in which the latent knowledge from the teacher (atomic\nenergies) is used to augment the students' training. We show that the\nlight-weight student MLIPs have faster MD speeds at a fraction of the memory\nfootprint compared to the teacher models. Remarkably, the student models can\neven surpass the accuracy of the teachers, even though both are trained on the\nsame quantum chemistry dataset. Our work highlights a practical method for\nMLIPs to reduce the resources required for large-scale MD simulations.",
        "Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"A deep-learning approach to realizing functionality in nanoelectronic devices",
    "start_abstract":"Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
      ],
      "abstract":[
        "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Skeletal Torus Actions and GKM Structures on Quiver Grassmannians of\n  String Representations",
        "Data Augmentation and Regularization for Learning Group Equivariance",
        "Nitrogen-Vacancy Centers in Epitaxial Laterally Overgrown Diamond:\n  Towards Up-scaling of Color Center-based Quantum Technologies",
        "Towards a complexity-theoretic dichotomy for TQFT invariants",
        "Stabilization of quantum properties under intrinsic decoherence in\n  presence of external magnetic fields",
        "Searching for Inflationary Physics with the CMB Trispectrum: 3.\n  Constraints from Planck",
        "Directional optical parametric amplification in a hyperbolic\n  metamaterial",
        "Laser cooled 137BaF molecules for measuring nuclear-spin-dependent\n  parity violation",
        "Interplay of ALP Couplings at a Muon Collider",
        "Change of some cropping systems in a long-term trial comparing different\n  systems: rationale and implications for statistical analysis",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Deep Learning-Powered Electrical Brain Signals Analysis: Advancing\n  Neurological Diagnostics",
        "Learning by Confusion: The Phase Diagram of the Holstein Model",
        "Decaying turbulence beneath surface waves",
        "Thermodynamic uncertainty relations for three-terminal systems with\n  broken time-reversal symmetry",
        "Observation of Fermi acceleration with cold atoms",
        "Various Architectures of Colloidal Cu3(MoO4)2(OH)2 and Cu3Mo2O9; Thermal\n  Stability, Photoluminescence and Magnetic Properties of Cu3(MoO4)2(OH)2 and\n  Cu3Mo2O9 Nanosheets",
        "Odd spanning trees of a graph",
        "Theory of Magnon Purcell Effect in Cavity Magnonic System",
        "Instabilities of the kinematic state of the atmospheres of some single\n  C-rich post-AGB stars",
        "Stochastic Optimal Control of Iron Condor Portfolios for Profitability\n  and Risk Management",
        "Narrowline Laser Cooling and Spectroscopy of Molecules via Stark States",
        "Interfacial superconductivity and a Se-vacancy ordered insulating phase\n  in the FeSe\/PbOx heterostructures",
        "Objective Mackey and Tambara functors via parametrized categories",
        "Doubly-Robust Functional Average Treatment Effect Estimation",
        "Global existence for multi-dimensional partially diffusive systems",
        "Introducing APERTURE: A GPU-based General Relativistic Particle-in-Cell\n  Simulation Framework",
        "Convergence Analysis of alpha-SVRG under Strong Convexity",
        "Driven Polymer Translocation through a Nanopore from a Confining Channel"
      ],
      "abstract":[
        "Quiver Grassmannians of equioriented type $\\texttt{A}$ and nilpotent\nequioriented type $\\tilde{\\texttt{A}}$ quiver representations are\nGKM-varieties. In particular, they have a cellular decomposition and admit a\ntorus action with finitely many fixed points and one-dimensional orbits (i.e.\nskeletal action). We examine the case of string representations and provide a\nclassification of all corresponding quiver Grassmannians with a GKM-variety\nstructure.",
        "In many machine learning tasks, known symmetries can be used as an inductive\nbias to improve model performance. In this paper, we consider learning group\nequivariance through training with data augmentation. We summarize results from\na previous paper of our own, and extend the results to show that equivariance\nof the trained model can be achieved through training on augmented data in\ntandem with regularization.",
        "Providing high-quality, single-crystal diamond (SCD) with a large area is\ndesirable for up-scaling quantum technology applications that rely on color\ncenters in diamond. Growth methods aiming to increase the area of SCD are an\nactive research area. Native color centers offer a sensitive probe for local\ncrystal quality in such novel materials e.g., via their reaction to stress. In\nthis work, we investigate individual native nitrogen-vacancy (NV) centers in\nSCD layers manufactured via laterally overgrowing hole arrays in a\nheteroepitaxially grown large-scale substrate. Heteroepitaxy has become a\ncommon tool for growing large SCDs; however, achieving the high crystal quality\nneeded for quantum applications remains a challenge. In the overgrown layer, we\nidentify NV centers with spin-decoherence times in the order of hundreds of\nmicroseconds, comparable to high-purity homoepitaxial SCD. We quantify the\neffective crystal strain in different regions of the overgrown layer,\nindicating a low stress overall and a stress reduction in the diamond layer\nabove the holes.",
        "We show that for any fixed $(2+1)$-dimensional TQFT over $\\mathbb{C}$ of\neither Turaev-Viro-Barrett-Westbury or Reshetikhin-Turaev type, the problem of\n(exactly) computing its invariants on closed 3-manifolds is either solvable in\npolynomial time, or else it is $\\#\\mathsf{P}$-hard to (exactly) contract\ncertain tensors that are built from the TQFT's fusion category. Our proof is an\napplication of a dichotomy result of Cai and Chen [J. ACM, 2017] concerning\nweighted constraint satisfaction problems over $\\mathbb{C}$. We leave for\nfuture work the issue of reinterpreting the conditions of Cai and Chen that\ndistinguish between the two cases (i.e. $\\#\\mathsf{P}$-hard tensor contractions\nvs. polynomial time invariants) in terms of fusion categories. We expect that\nwith more effort, our reduction can be improved so that one gets a dichotomy\ndirectly for TQFTs' invariants of 3-manifolds rather than more general tensors\nbuilt from the TQFT's fusion category.",
        "The dynamical behavior of quantum state properties under intrinsic\ndecoherence models can be modified by the presence of external magnetic fields.\nAlthough generically external magnetic fields are detrimental to preserve\nquantumness in the presence of intrinsic decoherence, judicious adjustment of\nthe magnetic field can stabilize such features. This stabilization arises from\nnovel resonances between energy eigenstates resulting from the presence of an\nexternal magnetic field. Here, we present our findings using as a model system\ntwo spin 1-particles confined in a double-well potential under intrinsic\ndecoherence. We stress, however, that our results are generic and independent\non the used model.",
        "Is there new physics hidden in the four-point function of the cosmic\nmicrowave background (CMB)? We conduct a detailed analysis of the Planck PR4\ntemperature and polarization trispectrum for $\\ell\\in[2,2048]$. Using the\ntheoretical and computational tools developed in Paper 1 and Paper 2, we search\nfor 33 template amplitudes, encoding a variety of effects from inflationary\nself-interactions to particle exchange. We find no evidence for primordial\nnon-Gaussianity and set stringent constraints on both phenomenological\namplitudes and couplings in the inflationary Lagrangian. Due to the use of\noptimal estimators and polarization data, our constraints are highly\ncompetitive. For example, we find $\\sigma(g_{\\rm NL}^{\\rm loc})=4.8\\times 10^4$\nand $\\tau_{\\rm NL}^{\\rm loc} <1500$ (95\\% CL), a factor of two improvement on\nEffective Field Theory amplitudes, and a $43\\sigma$ detection of gravitational\nlensing. Many templates are analyzed for the first time, such as\ndirection-dependent trispectra and the collapsed limit of the `cosmological\ncollider', across a range of masses and spins. We perform a variety of\nvalidation tests; whilst our results are stable, the most relevant systematics\nare found to be lensing bias, residual foregrounds, and mismatch between\nsimulations and data. The techniques discussed in this series can be extended\nto future datasets, allowing the primordial Universe to be probed at even\nhigher sensitivity.",
        "Optical parametric amplification (OPA) comprises essentially a nonlinear\nfour-wave mixing process in which a \"pump\" and a \"signal\" field give rise to an\n\"idler\" field under certain phase-matching conditions. Here we use a photonic\ncrystal waveguide strongly-coupled with an excitonic reservoir to generate this\nprocess between different guided modes at optical wavelengths. Differently from\nclassical nonlinear optical crystals, where the pump and idler photons travel\nalmost collinearly, our exciton-polaritons are naturally separated in the\nwaveguide due to their opposite group velocities. Due to the high efficiency of\nthe process we can generate the idler field of the parametric process by\npumping with a continuous wave laser and choose its direction of propagation in\nthe waveguide by adjusting the angle of incidence of the seed laser. We show\nthe OPA process to be robust against surface defects of the waveguide and can\nlead to simple-to-fabricate devices compared to microcavities that take\nadvantage of strong signal-idler correlations in a propagating geometry. Our\nresults closely agree with mean-field numerical simulations.",
        "We demonstrate optical cycling and transverse laser cooling of a beam of\nfermionic 137BaF molecules. Their high masses and nuclear spins make these\nmolecules sensitive probes for parity violation and properties of the weak\ninteraction. However, the nuclear spins also lead to a quasi-closed cycling\ntransition currently involving up to 112 levels, which significantly exceeds\nthe complexity in other laser-cooled molecules. Optical cycling and cooling are\nfacilitated through carefully designed optical spectra tailored to this\nmolecular structure. Our results pave the way for efficient state preparation,\ndetection, and cooling in precision measurements using this species and other\nsimilar species.",
        "Axion-like particles can couple to Standard Model gluons, electroweak gauge\nbosons, and massive fermions. A future multi-TeV muon collider provides a\nfavorable environment to probe axion-like particles through multiple production\nchannels, including vector boson fusion via electroweak gauge boson couplings\nand the top-associated production mediated by direct fermionic couplings.\nMotivated by the quality issue of the QCD axion, we focus on axion-like\nparticles with masses and decay constants around the TeV scale. We explore how\ndifferent axion-like particle couplings shape its production and decay modes,\nrevealing a rich and intricate phenomenological landscape.",
        "The project Agriculture 4.0 without chemical synthetical plant protection\n(NOcsPS) tests a number of cropping systems that avoid the use of chemical\nsynthetical pesticides while at the same time using mineral fertilizers. The\nexperiment started in 2020 (sowing fall 2019). In 2024 (sowing fall 2023), some\nof the cropping systems were modified. Analysis of this experiment may be done\nusing linear mixed models. In order to include the data from 2020-2023 in joint\nanalyses with the data collected for the modified systems from 2024 onwards,\nthe mixed modelling approach needs to be reconsidered. In this paper, we\ndevelop models for this purpose. A key feature is the use of network\nmeta-analytic concepts that allow a combination of direct and indirect\ncomparisons among systems from the different years. The approach is first\nillustrated using a toy example. This is followed by detailed analyses of data\nfrom two the two trials sites Dahnsdorf and Hohenheim.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "Neurological disorders represent significant global health challenges,\ndriving the advancement of brain signal analysis methods. Scalp\nelectroencephalography (EEG) and intracranial electroencephalography (iEEG) are\nwidely used to diagnose and monitor neurological conditions. However, dataset\nheterogeneity and task variations pose challenges in developing robust deep\nlearning solutions. This review systematically examines recent advances in deep\nlearning approaches for EEG\/iEEG-based neurological diagnostics, focusing on\napplications across 7 neurological conditions using 46 datasets. We explore\ntrends in data utilization, model design, and task-specific adaptations,\nhighlighting the importance of pre-trained multi-task models for scalable,\ngeneralizable solutions. To advance research, we propose a standardized\nbenchmark for evaluating models across diverse datasets to enhance\nreproducibility. This survey emphasizes how recent innovations can transform\nneurological diagnostics and enable the development of intelligent, adaptable\nhealthcare solutions.",
        "We employ the \"learning by confusion\" technique, an unsupervised machine\nlearning approach for detecting phase transitions, to analyze quantum Monte\nCarlo simulations of the two-dimensional Holstein model--a fundamental model\nfor electron-phonon interactions on a lattice. Utilizing a convolutional neural\nnetwork, we conduct a series of binary classification tasks to identify\nHolstein critical points based on the neural network's learning accuracy. We\nfurther evaluate the effectiveness of various training datasets, including\nsnapshots of phonon fields and other measurements resolved in imaginary time,\nfor predicting distinct phase transitions and crossovers. Our results culminate\nin the construction of the finite-temperature phase diagram of the Holstein\nmodel.",
        "This paper explores decaying turbulence beneath surface waves that is\ninitially isotropic and shear-free. We start by presenting phenomenology\nrevealed by wave-averaged numerical simulations: an accumulation of angular\nmomentum in coherent vortices, suppression of kinetic energy dissipation, and\nthe development of depth-alternating jets. We interpret these features through\nan analogy with rotating turbulence (Holm 1996), wherein the curl of the Stokes\ndrift, $\\nabla \\times \\mathbf{u}^S$, takes on the role of the background\nvorticity (for example, $(f_0 + \\beta y) \\mathbf{\\hat z}$ on the\n$\\beta$-plane). We pursue this thread further by showing that a two-equation\nmodel proposed by (Bardina et al. 1985) for rotating turbulence reproduces the\nsimulated evolution of volume-integrated kinetic energy. This success of the\ntwo-equation model -- which explicitly parameterizes wave-driven suppression of\nkinetic energy dissipation -- carries implications for modeling turbulent\nmixing in the ocean surface boundary layer. We conclude with a discussion about\na wave-averaged analogue of the Rossby number appearing in the two-equation\nmodel, which we term the ``pseudovorticity number'' after the pseudovorticity\n$\\nabla \\times \\mathbf{u}^S$. The pseudovorticity number is related to the\nLangmuir number in an integral sense.",
        "We investigate the thermodynamic uncertainty relations (TURs) in steady-state\ntransport for three-terminal systems within the linear response regime,\nspecifically in the presence of broken time-reversal symmetry. To quantify the\nTUR, we introduce a dimensionless trade-off parameter $Q_J$, and derive new\nbounds of $Q_J$ for both particle and heat currents under a strong constraint\non the Onsager coefficients. Furthermore, we determine a universal lower bound\n$Q_J^{bound}\\geq1.5$ for three-terminal systems in the linear response regime\nwhen the time-reversal symmetry is broken.",
        "Cosmic rays are deemed to be generated by a process known as ``Fermi\nacceleration\", in which charged particles scatter against magnetic fluctuations\nin astrophysical plasmas. The process itself is however universal, has both\nclassical and quantum formulations, and is at the basis of dynamical systems\nwith interesting mathematical properties, such as the celebrated Fermi-Ulam\nmodel. Despite its effectiveness in accelerating particles, Fermi acceleration\nhas so far eluded unambiguous verifications in laboratory settings. Here, we\nrealize the first fully controllable Fermi accelerator by colliding ultracold\natoms against engineered movable potential barriers. We demonstrate that our\nFermi accelerator, which is only 100 um in size, can produce ultracold atomic\njets with velocities above half a meter per second. Adding dissipation, we also\nexperimentally test Bell's general argument for the ensuing energy spectra,\nwhich is at the basis of any model of cosmic ray acceleration. On the one hand,\nour work effectively opens the window to the study of high energy astrophysics\nwith cold atoms, offering new capabilities for the understanding of phenomena\nsuch as diffusive acceleration at collisionless shocks. On the other, the\nperformance of our Fermi accelerator is competitive with those of best-in-class\naccelerating methods used in quantum technology and quantum colliders, but with\nsubstantially simpler implementation and virtually no upper limit.",
        "The lindgrenite compounds [Cu3(MoO4)2(OH)2] with various architectures and\nhigh crystallinity were prepared by a simple surfactant-assisted hydrothermal\nmethod. Then, the Cu3Mo2O9 samples were prepared by calcination of the\nas-synthesized Cu3(MoO4)2(OH)2. The resulting samples have high crystallinity,\ncolloidal properties, high-yield, large-scale production capability with using\nof nontoxic and inexpensive reagents and water as an environmentally solvent.\nThe scanning electron microscope studies show that the as-prepared lindgrenite\nnanostructures are well crystallized with rod, sheet and hollow sphere\nmorphologies. Meanwhile, the photoluminescence and magnetic properties of the\nnanosheet samples have been investigated that the both of Cu3(MoO4)2(OH)2 and\nCu3Mo2O9 samples have super paramagnetic behavior at room temperature and in\ncomparison with previous works, Cu3(MoO4)2(OH)2 and Cu3Mo2O9 samples\nsynthesized by the surfactant-assisted hydrothermal method in this work have a\nvery obvious red-shifted PL emission and high intensity.",
        "A graph $G=(V,E)$ is said to be odd (or even, resp.) if $d_G(v)$ is odd (or\neven, resp.) for any $v\\in V$. Trivially, the order of an odd graph must be\neven. In this paper, we show that every 4-edge connected graph of even order\nhas a connected odd factor. A spanning tree $T$ of $G$ is called a\nhomeomorphically irreducible spanning tree (HIST by simply) if $T$ contains no\nvertex of degree two. Trivially, an odd spanning tree must be a HIST. In 1990,\nAlbertson, Berman, Hutchinson, and Thomassen showed that every connected graph\nof order $n$ with $\\delta(G)\\geq \\min\\{\\frac n 2, 4\\sqrt{2n}\\}$ contains a\nHIST.\n  We show that every complete bipartite graph with both parts being even has no\nodd spanning tree, thereby for any even integer $n$ divisible by 4, there\nexists a graph of order $n$ with the minimum degree $\\frac n 2$ having no odd\nspanning tree. Furthermore, we show that every graph of order $n$ with\n$\\delta(G)\\geq \\frac n 2 +1$ has an odd spanning tree. We also characterize all\nsplit graphs having an odd spanning tree. As an application, for any graph $G$\nwith diameter at least 4, $\\overline{G}$ has a spanning odd double star.\nFinally, we also give a necessary and sufficient condition for a triangle-free\ngraph $G$ whose complement contains an odd spanning tree. A number of related\nopen problems are proposed.",
        "We conduct a systematic analysis of cavity effects on the decay dynamics of\nan open magnonic system. The Purcell effect on the magnon oscillator decay is\nthoroughly examined for both driven and non-driven scenarios. Analytical\nconditions are determined to distinguish between strong and weak coupling\nregimes, corresponding to oscillatory and pure decay behaviors respectively.\nAdditionally, our theory also predicts the decay of the photon mode within the\ncavity-magnonic open system, demonstrating excellent agreement with existing\nexperimental data. Our findings and methodologies can provide valuable insights\nfor advancing research in cavity magnonic quantum control, quantum information\nprocessing, and the development of magnonic quantum devices.",
        "To search for and study the instabilities in the atmospheres of selected\npost-AGB stars, we have performed a long-term high-resolution spectroscopy\n(R=60000) with the spectrograph NES of the 6-meter BTA telescope. Low-amplitude\npulsations, splitting and\/or asymmetry of the absorption profiles with a low\nexcitation potential, as well as variability of a complex H$\\alpha$ profile\nhave been registered in the optical spectra of single stars associated with the\nIR sources IRASz02229+6208, IRAS 04296+3429, IRAS 07134+1005, IRAS 07430+1115,\nIRAS 19500-1709, IRAS 22223+4327, and IRAS 23304+6147 that had previously\nundergone the 3-d dredge-up. The maximum pulsation amplitude A$_{\\rm Vr}$ was\ndetected for the stars in the IRAS 07134+1005 and IRAS 19500-1709 systems,\nwhich have the maximum temperatures among the stars studied. Stratification of\nradial velocity in the atmosphere was found for two stars in the sample. The\nluminosity of the studied stars was estimated based on the intensity of the IR\noxygen triplet OI(7774). Moreover, a luminosity of log${\\rm\n(L\/L_{\\odot})}\\approx$3.1 was obtained for the star in the IRAS 07430+1115\nsystem within the typical values for post-AGB stars luminosity, which\neliminates the paradox of the luminosity and the initial mass of this object.",
        "Previous research on option strategies has primarily focused on their\nbehavior near expiration, with limited attention to the transient value process\nof the portfolio. In this paper, we formulate Iron Condor portfolio\noptimization as a stochastic optimal control problem, examining the impact of\nthe control process \\( u(k_i, \\tau) \\) on the portfolio's potential\nprofitability and risk. By assuming the underlying price process as a bounded\nmartingale within $[K_1, K_2]$, we prove that the portfolio with a strike\nstructure of $k_1 < k_2 = K_2 < S_t < k_3 = K_3 < k_4$ has a submartingale\nvalue process, which results in the optimal stopping time aligning with the\nexpiration date $\\tau = T$. Moreover, we construct a data generator based on\nthe Rough Heston model to investigate general scenarios through simulation. The\nresults show that asymmetric, left-biased Iron Condor portfolios with $\\tau =\nT$ are optimal in SPX markets, balancing profitability and risk management.\nDeep out-of-the-money strategies improve profitability and success rates at the\ncost of introducing extreme losses, which can be alleviated by using an optimal\nstopping strategy. Except for the left-biased portfolios $\\tau$ generally falls\nwithin the range of [50\\%,75\\%] of total duration. In addition, we validate\nthese findings through case studies on the actual SPX market, covering bullish,\nsideways, and bearish market conditions.",
        "The electronic energy level structure of yttrium monoxide (YO) provides\nlong-lived excited $^{2}\\Delta$ states ideal for high-precision molecular\nspectroscopy, narrowline laser cooling at the single photon-recoil limit, and\nstudying dipolar physics with unprecedented interaction strength. We use\nultracold laser-cooled YO molecules to study the Stark effect in the metastable\nA$^{\\prime}\\,^{2}\\Delta_{3\/2}\\,J=3\/2$ state by high-resolution laser\nspectroscopy. We determined the absolute transition frequency from this\nmetastable state to the X$\\,^2\\Sigma^+$ electronic ground state with a\nfractional uncertainty of 9 $\\times$ 10$^{-12}$. In the presence of weak\nelectric fields a linear Stark effect is observed in the\nA$^{\\prime}\\,^{2}\\Delta_{3\/2}$ state owing to the large electric dipole moment\nand near degenerate $\\Lambda$-doublet states. A quasi-closed photon cycling\nscheme is identified involving a narrowline transition to a single Stark state,\nand implemented in free space to demonstrate the first narrowline laser cooling\nof a molecule, reducing the temperature of sub-Doppler cooled YO in two\ndimensions.",
        "The discovery of high-temperature superconductivity in FeSe\/SrTiO3 has\nsparked significant interests in exploring new superconducting systems with\nengineered interfaces. Here, using molecular beam epitaxy growth, we\nsuccessfully fabricate FeSe\/PbOx heterostructures and discover\nsuperconductivities in three different monolayer FeSe-related interfaces. We\nobserve superconducting gaps of 13~14 meV in the monolayer FeSe films grown on\ntwo different phases of PbOx. Moreover, we discover a new insulating Fe10Se9\nphase with an ordered $\\sqrt{5}\\times\\sqrt{5}$ Se-vacancy structure. Our\nfirst-principles calculation suggests that this new insulating phase originates\nfrom electronic correlation. Intriguingly, an additional monolayer FeSe film\ngrown on the insulating Fe10Se9 also exhibits superconductivity with the gap\nsize of 5 meV. Our results suggest that the work function differences between\nthe monolayer FeSe and the substrates, which can induce band bending and charge\ntransfer, are crucial for the interfacial superconductivity.",
        "The first word in the title is intended in a sense suggested by Lawvere and\nSchanuel whereby finite sets are objective natural numbers. At the objective\nlevel, the axioms defining abstract Mackey and Tambara functors are\ncategorically familiar. The first step was taken by Harald Lindner in 1976 when\nhe recognized that Mackey functors, defined as pairs of functors, were single\nfunctors with domain a category of spans. We define objective Mackey and\nobjective Tambara functors as parametrized categories which have local finite\nproducts and satisfy some parametrized completeness and cocompleteness\nrestriction. However, we can replace the original parametrizing base for\nobjective Mackey functors by a bicategory of spans while the replacement for\nobjective Tambara functors is a bicategory obtained by iterating the span\nconstruction; these iterated spans are polynomials. There is an objective\nMackey functor of ordinary Mackey functors. We show that there is a\ndistributive law relating objective Mackey functors to objective Tambara\nfunctors analogous to the distributive law relating abelian groups to\ncommutative rings. We remark on hom enrichment matters involving the 2-category\n$\\mathrm{Cat}_{+}$ of categories admitting finite coproducts and functors\npreserving them, both as a closed base and as a skew-closed base.",
        "Understanding causal relationships in the presence of complex, structured\ndata remains a central challenge in modern statistics and science in general.\nWhile traditional causal inference methods are well-suited for scalar outcomes,\nmany scientific applications demand tools capable of handling functional data\n-- outcomes observed as functions over continuous domains such as time or\nspace. Motivated by this need, we propose DR-FoS, a novel method for estimating\nthe Functional Average Treatment Effect (FATE) in observational studies with\nfunctional outcomes. DR-FoS exhibits double robustness properties, ensuring\nconsistent estimation of FATE even if either the outcome or the treatment\nassignment model is misspecified. By leveraging recent advances in functional\ndata analysis and causal inference, we establish the asymptotic properties of\nthe estimator, proving its convergence to a Gaussian process. This guarantees\nvalid inference with simultaneous confidence bands across the entire functional\ndomain. Through extensive simulations, we show that DR-FoS achieves robust\nperformance under a wide range of model specifications. Finally, we illustrate\nthe utility of DR-FoS in a real-world application, analyzing functional\noutcomes to uncover meaningful causal insights in the SHARE (Survey of Health,\nAging and Retirement in Europe) dataset.",
        "In this work, we explore the global existence of strong solutions for a class\nof partially diffusive hyperbolic systems within the framework of critical\nhomogeneous Besov spaces. Our objective is twofold: first, to extend our recent\nfindings on the local existence presented in J.-P. Adogbo and R. Danchin. Local\nwell-posedness in the critical regularity setting for hyperbolic systems with\npartial diffusion. arXiv:2307.05981, 2024, and second, to refine and enhance\nthe analysis of Kawashima (S. Kawashima. Systems of a hyperbolic parabolic type\nwith applications to the equations of magnetohydrodynamics. PhD thesis, Kyoto\nUniversity, 1983).\n  To address the distinct behaviors of low and high frequency regimes, we\nemploy a hybrid Besov norm approach that incorporates different regularity\nexponents for each regime. This allows us to meticulously analyze the\ninteractions between these regimes, which exhibit fundamentally different\ndynamics.\n  A significant part of our methodology is based on the study of a Lyapunov\nfunctional, inspired by the work of Beauchard and Zuazua (K. Beauchard and E.\nZuazua. Large time asymptotics for partially dissipative hyperbolic system.\nArch. Rational Mech. Anal, 199:177-227, 2011.) and recent contributions (T.\nCrin-Barat and R. Danchin. Partially dissipative hyperbolic systems in the\ncritical regularity setting: the multi-dimensional case. J. Math. Pures Appl.\n(9), 165:1-41, 2022). To effectively handle the high-frequency components, we\nintroduce a parabolic mode with better smoothing properties, which plays a\ncentral role in our analysis.\n  Our results are particularly relevant for important physical systems, such as\nthe magnetohydrodynamics (MHD) system and the Navier-Stokes-Fourier equations.",
        "Low-luminosity Active Galactic Nuclei (AGN) are believed to be surrounded by\na collisionless, highly magnetized accretion flow. As a result,\nParticle-in-Cell simulations are the best tools to study the immediate vicinity\nof the event horizons of these supermassive black holes. We present a GPU-based\ngeneral relativistic particle-in-cell (GRPIC) code framework called Aperture.\nAperture is developed in C++, with compute kernels written in CUDA and HIP to\ntake advantage of the massive acceleration modern GPUs enable. The code is\norganized in a fully modular way, allowing easy extensions to new physics\nproblems. In this paper, we describe in detail the particle pusher, field\nsolver, and charge-conserving current deposition algorithms employed in\nAperture, and present test cases to validate their correctness. Then, we apply\nthe code to study spark gaps and plasma injection in black hole magnetospheres.\nWe find that the apparent location and time-evolution of the gap depend on the\nobserver. Our results reconcile the previous conflicting findings from 1D and\n2D simulations in the literature.",
        "Stochastic first-order methods for empirical risk minimization employ\ngradient approximations based on sampled data in lieu of exact gradients. Such\nconstructions introduce noise into the learning dynamics, which can be\ncorrected through variance-reduction techniques. There is increasing evidence\nin the literature that in many modern learning applications noise can have a\nbeneficial effect on optimization and generalization. To this end, the recently\nproposed variance-reduction technique, alpha-SVRG [Yin et al., 2023] allows for\nfine-grained control of the level of residual noise in the learning dynamics,\nand has been reported to empirically outperform both SGD and SVRG in modern\ndeep learning scenarios. By focusing on strongly convex environments, we first\nprovide a unified convergence rate expression for alpha-SVRG under fixed\nlearning rate, which reduces to that of either SGD or SVRG by setting alpha=0\nor alpha=1, respectively. We show that alpha-SVRG has faster convergence rate\ncompared to SGD and SVRG under suitable choice of alpha. Simulation results on\nlinear regression validate our theory.",
        "We consider the dynamics of pore-driven polymer translocation through a\nnanopore to semi-infinite space when the chain is initially confined and\nequilibrated in a narrow channel. To this end, we use Langevin dynamics (LD)\nsimulations and iso-flux tension propagation (IFTP) theory to characterize\nlocal and global dynamics of the translocating chain. The dynamics of the\nprocess can be described by the IFTP theory in very good agreement with the LD\nsimulations for all values of confinement in the channel. The theory reveals\nthat for channels with size comparable to or less than the end-to-end distance\nof the unconfined chain, in which the blob theory works, the scaling form of\nthe translocation time depends on both the chain contour length as well as the\nchannel width. %originating from the confinement of the spatial fluctuations of\nthe chain inside the channel. Conversely, for a very narrow channel the\ntranslocation time only depends on the chain contour length and is similar to\nthat of a rod due to the absence of spatial chain fluctuations."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge",
    "start_abstract":"Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
      ],
      "abstract":[
        "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Top eigenvalue statistics of diluted Wishart matrices",
        "Quantum oscillations in a dipolar excitonic insulator",
        "Comparison theorems for the minimum eigenvalue of a random\n  positive-semidefinite matrix",
        "Emergent supercounterfluid and quantum phase diagram of two-component\n  interacting bosons in one-dimensional optical lattice",
        "Multivariate spatial models for small area estimation of\n  species-specific forest inventory parameters",
        "Bayesian optimization of electron energy from laser wakefield\n  accelerator",
        "Quantum model reduction for continuous-time quantum filters",
        "Meson Mixing Bounds on $Z^{\\prime}$ Mass in the Alignment Limit:\n  Establishing the Phenomenological Viability of the 331 Model",
        "Right-censored models on massive data",
        "A spectral boundary element method for acoustic interference problems",
        "Variational quantum thermalizers based on weakly-symmetric nonunitary\n  multi-qubit operations",
        "AI-assisted hyper-dimensional broadband quantum memory with efficiency\n  above 90% in warm atoms",
        "The putative center in NGC 1052",
        "Hopfological invariants for tame subextensions",
        "Detecting entanglement in any measurement using quantum networks",
        "Deciphering the dual chemotaxis strategy of bacteria in porous media",
        "A \"Black Hole Star\" Reveals the Remarkable Gas-Enshrouded Hearts of the\n  Little Red Dots",
        "On a Conjecture of Yui and Zagier II",
        "Solid-state dewetting of axisymmetric thin film on axisymmetric\n  curved-surface substrates: modeling and simulation",
        "Crosstalk analysis in single hole-spin qubits within highly anisotropic\n  g-tensors",
        "A Unified View of Optimal Kernel Hypothesis Testing",
        "Universal Quantum Computation with the $S_3$ Quantum Double: A\n  Pedagogical Exposition",
        "Embodying Newtonian Mechanics",
        "A Deep-Unfolding-Optimized Coordinate-Descent Data-Detector ASIC for\n  mmWave Massive MIMO",
        "An Empirically-parametrized Spatio-Temporal Extended-SIR Model for\n  Combined Dilution and Vaccination Mitigation for Rabies Outbreaks in Wild\n  Jackals",
        "The Type Ia Supernova and AGB-Regulated Interstellar Medium of Massive\n  Galaxies",
        "Revision of the linear stability paradox for known bounded shear flows",
        "Numerical analysis of variational-hemivariational inequalities with\n  applications in contact mechanics",
        "Matrix weighted inequalities for fractional type integrals associated to\n  operators with new classes of weights"
      ],
      "abstract":[
        "Using the replica method, we compute analytically the average largest\neigenvalue of diluted covariance matrices of the form $\\mathbf{J} =\n\\mathbf{X}^T \\mathbf{X}$, where $\\mathbf{X}$ is a $N\\times M$ sparse data\nmatrix, in the limit of large $N,M$ with fixed ratio. We allow for random\nnon-zero weights, provided they lead to an isolated largest eigenvalue. By\nformulating the problem as the optimisation of a quadratic Hamiltonian\nconstrained to the $N$-sphere at low temperatures, we derive a set of recursive\ndistributional equations for auxiliary probability density functions, which can\nbe efficiently solved using a population dynamics algorithm. The average\nlargest eigenvalue is identified with a Lagrange parameter that governs the\nconvergence of the algorithm. We find excellent agreement between our\nanalytical results and numerical results obtained from direct diagonalisation.",
        "Quantum oscillations in magnetization or resistivity are a defining feature\nof metals subject to an external magnetic field. The phenomenon is generally\nnot expected in insulators without a Fermi surface. The observations of quantum\noscillations in Kondo insulating materials have provided a rare counterexample\nand attracted much theoretical interest. However, the magnetic oscillations in\ncorrelated insulators remain poorly understood. Here we report the observations\nof resistivity quantum oscillations in an excitonic insulator realized in\nCoulomb-coupled electron-hole double layers with gate-tunability that allows\nthe phenomenon to be explored in a more controllable fashion than in bulk\nmaterials. When the cyclotron energy of the electrons or holes is tuned to be\ncomparable to or larger than the exciton binding energy, recurring transitions\nbetween excitonic insulators and electron-hole decoupled quantum Hall states\nare observed. Compressibility measurements show an oscillatory exciton binding\nenergy as a function of magnetic field and electron-hole pair density. Coulomb\ndrag measurements further reveal the formation of excitons with finite angular\nmomentum. Our results are qualitatively captured by mean-field theory\ncalculations. The study demonstrates a new platform for studying quantum\noscillations in correlated insulators.",
        "This paper establishes a new comparison principle for the minimum eigenvalue\nof a sum of independent random positive-semidefinite matrices. The principle\nstates that the minimum eigenvalue of the matrix sum is controlled by the\nminimum eigenvalue of a Gaussian random matrix that inherits its statistics\nfrom the summands. This methodology is powerful because of the vast arsenal of\ntools for treating Gaussian random matrices. As applications, the paper\npresents short, conceptual proofs of some old and new results in\nhigh-dimensional statistics. It also settles a long-standing open question in\ncomputational linear algebra about the injectivity properties of very sparse\nrandom matrices.",
        "Motivated by a recent experiment that realizes nearest-neighbor dipolar\ncouplings in an optical lattice [C. Lagoin, $\\textit{et al.}$, Nature\n$\\textbf{609}$, 485 (2022)], we study a one-dimensional version of the\ntwo-component extended Bose-Hubbard model via the density-matrix\nrenormalization group method. By using the nearest-neighbor and on-site\ninteraction parameters from the experiment, we start by mapping the quantum\nphase diagram in the hopping parameters $t_{A}\\mbox{-}t_{B}$ plane with boson\ndensities $\\rho_{A}=\\rho_{B}=1\/2$. In addition to the density wave phase\nreported in the experiment, we find several regimes of superfluidity when one\nor two hopping parameters are large enough, and interestingly there is a\nsupercounterfluid phase at moderate and comparable hopping parameters. The\nuniversality classes of these phase transitions are analyzed from the\ncorrelation functions, excitation gaps, and entanglement entropy. In\nparticular, a Berezinskii-Kosterlitz-Thouless type is recognized several\ngapped-to-gapless transitions. In addition, we also study the quantum phase\ntransitions when varying $\\rho_{B}$ from 0 to 1 while keeping $\\rho_A = 1\/2$.\nWe identify a supersolid phase in a wide range of $1\/2<\\rho_B<1$. Our work\npaves the way for realizing exotic many-body phases in cold atom experiments\nupon proper tuning of experimental parameters.",
        "National Forest Inventories (NFIs) provide statistically reliable information\non forest resources at national and other large spatial scales. As forest\nmanagement and conservation needs become increasingly complex, NFIs are being\ncalled upon to provide forest parameter estimates at spatial scales smaller\nthan current design-based estimation procedures can provide. This is\nparticularly true when estimates are desired by species or species groups. Here\nwe propose a multivariate spatial model for small area estimation of\nspecies-specific forest inventory parameters. The hierarchical Bayesian\nmodeling framework accounts for key complexities in species-specific forest\ninventory data, such as zero-inflation, correlations among species, and\nresidual spatial autocorrelation. Importantly, by fitting the model directly to\nthe individual plot-level data, the framework enables estimates of\nspecies-level forest parameters, with associated uncertainty, across any\nuser-defined small area of interest. A simulation study revealed minimal bias\nand higher accuracy of the proposed model-based approach compared to the\ndesign-based estimator and a non-parametric k-nearest neighbor (kNN) estimator.\nWe applied the model to estimate species-specific county-level aboveground\nbiomass for the 20 most abundant tree species in the southern United States\nusing Forest Inventory and Analysis (FIA) data. Biomass estimates from the\nproposed model had high correlations with design-based estimates and kNN\nestimates. Importantly, the proposed model provided large gains in precision\nacross all 20 species. On average across species, 91.5% of county-level biomass\nestimates had higher precision compared to the design-based estimates. The\nproposed framework improves the ability of NFI data users to generate\nspecies-level forest parameter estimates with reasonable precision at\nmanagement-relevant spatial scales.",
        "We employ Bayesian optimization combined with three-dimensional\nparticle-in-cell simulations to identify the optimal laser and plasma\nparameters that, for a given laser pulse energy, maximize the cut-off energy of\nan electron beam accelerated via laser wakefield acceleration. A Gaussian laser\ndriver with a matched spot size and amplitude is assumed, interacting with both\na uniform-density plasma and a preformed plasma channel of matched radius. To\ninterpret the simulation results quantitatively, we derive novel analytical\nexpressions for predicting the maximum electron energy and acceleration length,\ntaking into account the diffraction and energy depletion of the laser pulse.\nAdditionally, we discuss the potential scalability of the optimal parameters\nfor high-energy lasers.",
        "The use of quantum stochastic models is widespread in dynamical reduction,\nsimulation of open systems, feedback control and adaptive estimation. In many\napplications only part of the information contained in the filter's state is\nactually needed to reconstruct the target observable quantities; thus, filters\nof smaller dimensions could be in principle implemented to perform the same\ntask.In this work, we propose a systematic method to find, when possible,\nreduced-order quantum filters that are capable of exactly reproducing the\nevolution of expectation values of interest. In contrast with existing\nreduction techniques, the reduced model we obtain is exact and in the form of a\nBelavkin filtering equation, ensuring physical interpretability.This is\nattained by leveraging tools from the theory of both minimal realization and\nnon-commutative conditional expectations. The proposed procedure is tested on\nprototypical examples, laying the groundwork for applications in quantum\ntrajectory simulation and quantum feedback control.",
        "We perform a systematic study of flavor-changing neutral currents (FCNCs) in\nthe 331 model with right-handed neutrinos (331RHNs), analyzing constraints on\nthe $Z^\\prime$ boson mass from $K$-, $D$-, $B_d$-, and $B_s$-meson\noscillations. By explicitly incorporating scalar sector dynamics and quark\nrotation ambiguities ($V_L^{u,d}$), we demonstrate that $Z^\\prime$ mass limits\ndepend critically on the parametrization of Cabibbo-Kobayashi-Maskawa (CKM)\nmatrix factors. Three scenarios are explored: (i) $V_L^u =\nV_\\text{CKM}^\\dagger$ (FCNCs restricted to $D$-mesons), (ii) $V_L^d =\nV_\\text{CKM}$ (dominant $B_s$ constraints), and (iii) a hybrid mixing pattern.\nStrikingly, scenario (i) reduces the $Z^\\prime$ mass bound to $M_{Z^\\prime}\n\\gtrsim 600\\;\\text{GeV}$-two orders of magnitude below literature values-by\nleveraging large experimental uncertainties in $D$-$\\bar{D}$ oscillations.\nConversely, scenario (ii) requires $M_{Z^\\prime} \\gtrsim 165\\;\\text{TeV}$ due\nto stringent $B_s$ data. We further establish the alignment limit\n$\\cos(\\phi+\\varphi) = 0$ for the SM-like Higgs, showing its viability depends\non $V_L^{u,d}$ configurations, with $B_s$ systems enforcing\n$|\\cos(\\phi+\\varphi)| < 0.01$ in down-sector FCNC scenarios. Our analysis\nreveals that strategic choices of quark mixing matrices can suppress FCNC\nvisibility, reconciling the 331 framework with flavor data without ultra-heavy\n$Z^\\prime$ bosons. This work provides the first unified treatment of SM-like\nHiggs- and $Z^\\prime$-mediated FCNCs in 331 models, identifying viable\nparameter spaces for collider phenomenology.",
        "This article considers the automatic selection problem of the relevant\nexplanatory variables in a right-censored model on a massive database. We\npropose and study four aggregated censored adaptive LASSO estimators\nconstructed by dividing the observations in such a way as to keep the\nconsistency of the estimator of the survival curve. We show that these\nestimators have the same theoretical oracle properties as the one built on the\nfull database. Moreover, by Monte Carlo simulations we obtain that their\ncalculation time is smaller than that of the full database. The simulations\nconfirm also the theoretical properties. For optimal tuning parameter\nselection, we propose a BIC-type criterion.",
        "In this paper we consider high-frequency acoustic transmission problems with\njumping coefficients modelled by Helmholtz equations. The solution then is\nhighly oscillatory and, in addition, may be localized in a very small vicinity\nof interfaces (whispering gallery modes). For the reliable numerical\napproximation a) the PDE is tranformed in a classical single trace integral\nequation on the interfaces and b) a spectral Galerkin boundary element method\nis employed for its solution. We show that the resulting integral equation is\nwell posed and analyze the convergence of the boundary element method for the\nparticular case of concentric circular interfaces. We prove a condition on the\nnumber of degrees of freedom for quasi-optimal convergence. Numerical\nexperiments confirm the efficiency of our method and the sharpness of the\ntheoretical estimates.",
        "We propose incorporating multi-qubit nonunitary operations in Variational\nQuantum Thermalizers (VQTs). VQTs are hybrid quantum-classical algorithms that\ngenerate the thermal (Gibbs) state of a given Hamiltonian, with applications in\nquantum algorithms and simulations. However, current algorithms struggle at\nintermediate temperatures, where the target state is nonpure but exhibits\nentanglement. We devise multi-qubit nonunitary operations that harness weak\nsymmetries and thereby improve the performance of the algorithm. Utilizing\ndissipation engineering, we create these nonunitary multi-qubit operations\nwithout the need for measurements or additional qubits. To train the ansatz, we\ndevelop and benchmark novel methods for entropy estimation of quantum states,\nexpanding the toolbox for quantum state characterization. We demonstrate that\nour approach can prepare thermal states of paradigmatic spin models at all\ntemperatures. Our work thus creates new opportunities for simulating open\nquantum many-body systems.",
        "High-dimensional broadband quantum memory significantly expands quantum\ninformation processing capabilities, but the memory efficiency becomes\ninsufficient when extended to high dimensions. We demonstrate an efficient\nquantum memorize for hyper-dimensional photons encoded with orbital angular\nmomentum (OAM) and spin angular momentum (SAM). OAM information is encoded from\n-5 to +5, combined with spin angular momentum encoding, enabling up to 22\ndimensions. To ensure high memory efficiency, an artificial intelligent\nalgorithm, a modified Differential Evolution (DE) algorithm using Chebyshev\nsampling, is developed to obtain a perfect signal-control waveform matching.\nMemory efficiency is experimentally achieved 92% for single-mode Gaussian\nsignal, 91% for information dimension of 6 and 80% for dimensional number to\n22. The fidelity is achieved up to 99% for single-mode Gaussian signal, 96% for\nOAM information and 97% for SAM one, which is far beyond no-cloning limitation.\nOur results demonstrate superior performance and potential applications in\nhigh-dimensional quantum information processing. This achievement provides a\ncrucial foundation for future quantum communication and quantum computing.",
        "Many active galaxies harbor powerful relativistic jets, however, the detailed\nmechanisms of their formation and acceleration remain poorly understood. To\ninvestigate the area of jet acceleration and collimation with the highest\navailable angular resolution, we study the innermost region of the bipolar jet\nin the nearby low-ionization nuclear emission-line region (LINER) galaxy NGC\n1052. We combined observations of NGC 1052 taken with VLBA, GMVA, and EHT over\none week in the spring of 2017. For the first time, NGC 1052 was detected with\nthe EHT, providing a size of the central region in-between both jet bases of\n250 RS (Schwarzschild radii) perpendicular to the jet axes. This size estimate\nsupports previous studies of the jets expansion profile which suggest two\nbreaks of the profile at around 300 RS and 10000 RS distances to the core.\nFurthermore, we estimated the magnetic field to be 1.25 Gauss at a distance of\n22 {\\mu}as from the central engine by fitting a synchrotron-self absorption\nspectrum to the innermost emission feature, which shows a spectral turn-over at\nabout 130 GHz. Assuming a purely poloidal magnetic field, this implies an upper\nlimit on the magnetic field strength at the event horizon of 26000 Gauss, which\nis consistent with previous measurements. The complex, low-brightness,\ndouble-sided jet structure in NGC 1052 makes it a challenge to detect the\nsource at millimeter (mm) wavelengths. However, our first EHT observations have\ndemonstrated that detection is possible up to at least 230 GHz. This study\noffers a glimpse through the dense surrounding torus and into the innermost\ncentral region, where the jets are formed. This has enabled us to finally\nresolve this region and provide improved constraints on its expansion and\nmagnetic field strength.",
        "Let H be a finite dimensional Hopf algebra over a field K. In this paper, we\nstudy when an H-extension becomes a tame H-extension by calculating\nHopfological homology and Hopf-cyclic homology. In the (derived) category of\nH'-comodules for a Hopf algebra H', we take Hopf subalgebra H of H' and a\ncertain order A of H. We see the behavior of Hopfological homology for a tame\nA-subextension S\/R in terms of the surjectivity of trace map and of cyclic\nmodules, which induce Hopf-cyclic homology, for Hopf-Galois extensions with H\nin terms of relative Hopf modules.",
        "Entanglement is a key resource to demonstrate quantum advantage over\nclassical strategies. Entanglement in quantum states is one of the most\nwell-explored areas in quantum physics. However, a rigorous approach to\nunderstanding and detecting entanglement in composite quantum measurements is\nlacking. In this work, we focus on composite quantum measurements and classify\nthem into two classes: entangled and separable measurements. As done for\nquantum states, we define analogously a notion of witness that can be used to\ndetect entanglement in composite quantum measurements. Here, one does not need\nto trust the measurement to witness its entanglement but must trust the quantum\nstates. We then further extend this approach to show that any entangled\nmeasurement provides an advantage in network quantum steering without inputs,\nalso known as swap steering. Consequently, this provides a way to witness\nentanglement in any quantum measurement in a one-sided device-independent way.\nFinally, we consider the star network scenario and show that any rank-one\nprojective entangled quantum measurement gives a quantum advantage. Thus, one\ncan detect the entanglement in any rank-one projective measurement in a\ndevice-independent way.",
        "Chemotaxis of bacterial swimmers that move in a run-and-turn pattern is well\nstudied in uniform bulk fluid. It is primarily based on modulating the run time\nin dependence on the swimming direction with respect to the source of\nchemoattractant (run time bias). Here, we provide evidence that the\nlophotrichously flagellated soil bacterium Pseudomonas putida may also perform\nchemotaxis in porous media where the free path length is severely restricted.\nBesides the classical run time bias, we identify a second chemotactic strategy:\nthe change in swimming direction upon a turn event is adjusted, so that the\ndirection of the next run phase is biased towards the source of chemoattractant\n(turn angle bias). Agent based simulations, based on the experimentally\nobserved statistical properties of the swimming pattern, indicate that turn\nangle bias is the predominant chemotaxis strategy of bacteria in porous\nenvironments.",
        "The physical processes that led to the formation of billion solar mass black\nholes within the first 700 million years of cosmic time remain a puzzle.\nSeveral theoretical scenarios have been proposed to seed and rapidly grow black\nholes, but direct observations of these mechanisms remain elusive. Here we\npresent a source 660 million years after the Big Bang that displays singular\nproperties: among the largest Hydrogen Balmer breaks reported at any redshift,\nbroad multi-peaked H$\\beta$ emission, and Balmer line absorption in multiple\ntransitions. We model this source as a \"black hole star\" (BH*) where the Balmer\nbreak and absorption features are a result of extremely dense, turbulent gas\nforming a dust-free \"atmosphere\" around a supermassive black hole. This source\nmay provide evidence of an early black hole embedded in dense gas -- a\ntheoretical configuration proposed to rapidly grow black holes via\nsuper-Eddington accretion. Radiation from the BH* appears to dominate almost\nall observed light, leaving limited room for contribution from its host galaxy.\nWe demonstrate that the recently discovered \"Little Red Dots\" (LRDs) with\nperplexing spectral energy distributions can be explained as BH*s embedded in\nrelatively brighter host galaxies. This source provides evidence that black\nhole masses in the LRDs may be over-estimated by orders of magnitude -- the BH*\nis effectively dust-free contrary to the steep dust corrections applied while\nmodeling LRDs, and the physics that gives rise to the complex line shapes and\nluminosities may deviate from assumptions underlying standard scaling\nrelations.",
        "Yui and Zagier made some fascinating conjectures on the factorization on the\nnorm of the difference of Weber class invariants $ f(\\mathfrak a_1) -\nf(\\mathfrak a_2)$ based on their calculation in \\cite{YZ}. Here $\\mathfrak a_i$\nbelong two diferent ideal classes of discrimants $D_i$ in imagainary quadratic\nfields $\\mathbb{Q}(\\sqrt{D_i})$. In \\cite{LY}, we proved these conjectures and\ntheir generalizations when $(D_1, D_2) =1$ using the so-called big CM value\nformula of Borcherds lifting. In this sequel, we prove the conjectures when\n$\\mathbb{Q}(\\sqrt{D_1}) =\\mathbb{Q}(\\sqrt{D_2})$ using the so-called small CM\nvalue formula. In addition, we give a precise factorization formula for the\nresultant of two different Weber class invariant polynomials for distinct\norders.",
        "In this work, we consider the solid-state dewetting of an axisymmetric thin\nfilm on a curved-surface substrate, with the assumption that the substrate\nmorphology is also axisymmetric. Under the assumptions of axisymmetry, the\nsurface evolution problem on a curved-surface substrate can be reduced to a\ncurve evolution problem on a static curved substrate. Based on the\nthermodynamic variation of the anisotropic surface energy, we thoroughly derive\na sharp-interface model that is governed by anisotropic surface diffusion,\nalong with appropriate boundary conditions. The continuum system satisfies the\nlaws of energy decay and volume conservation, which motivates the design of a\nstructure-preserving numerical algorithm for simulating the mathematical model.\nBy introducing a symmetrized surface energy matrix, we derive a novel\nsymmetrized variational formulation. Then, by carefully discretizing the\nboundary terms of the variational formulation, we establish an unconditionally\nenergy-stable parametric finite element approximation of the axisymmetric\nsystem. By applying an ingenious correction method, we further develop another\nstructure-preserving method that can preserve both the energy stability and\nvolume conservation properties. Finally, we present extensive numerical\nexamples to demonstrate the convergence and structure-preserving properties of\nour proposed numerical scheme. Additionally, several interesting phenomena are\nexplored, including the migration of 'small' particles on a curved-surface\nsubstrate generated by curves with positive or negative curvature, pinch-off\nevents, and edge retraction.",
        "Spin qubits based on valence band hole states are highly promising for\nquantum information processing due to their strong spin-orbit coupling and\nultrafast operation speed. As these systems scale up, achieving high-fidelity\nsingle-qubit operations becomes essential. However, mitigating crosstalk\neffects from neighboring qubits in larger arrays, particularly for anisotropic\nqubits with strong spin-orbit coupling, presents a significant challenge. We\ninvestigate the impact of crosstalk on qubit fidelities during single-qubit\noperations and derive an analytical equation that serves as a synchronization\ncondition to eliminate crosstalk in anisotropic media. Our analysis proposes\noptimized driving field conditions that can robustly synchronize Rabi\noscillations and minimize crosstalk, showing a strong dependence on qubit\nanisotropy and the orientation of the external magnetic field. Taking\nexperimental data into our analysis, we identify a set of parameter values that\nenable nearly crosstalk-free single-qubit gates, thereby paving the way for\nscalable quantum computing architectures.",
        "This paper provides a unifying view of optimal kernel hypothesis testing\nacross the MMD two-sample, HSIC independence, and KSD goodness-of-fit\nframeworks. Minimax optimal separation rates in the kernel and $L^2$ metrics\nare presented, with two adaptive kernel selection methods (kernel pooling and\naggregation), and under various testing constraints: computational efficiency,\ndifferential privacy, and robustness to data corruption. Intuition behind the\nderivation of the power results is provided in a unified way accross the three\nframeworks, and open problems are highlighted.",
        "Non-Abelian topological order (TO) enables topologically protected quantum\ncomputation with its anyonic quasiparticles. Recently, TO with $S_3$ gauge\nsymmetry was identified as a sweet spot -- simple enough to emerge from\nfinite-depth adaptive circuits yet powerful enough to support a universal\ntopological gate-set. In these notes, we review how anyon braiding and\nmeasurement in $S_3$ TO are primitives for topological quantum computation and\nwe explicitly demonstrate universality. These topological operations are made\nconcrete in the $S_3$ quantum double lattice model, aided by the introduction\nof a generalized ribbon operator. This provides a roadmap for near-term quantum\nplatforms.",
        "Wellness and mindfulness act as buzzwords these days, often seen as separate\nfrom physics. Yet we know they are important, and everything is related to\nphysics! In this article, we will consider a few simple classroom activities\nthat can both help students internalize the basic physics of forces and motion\nand also help facilitate well-being in our classes.",
        "We present a 22 nm FD-SOI (fully depleted silicon-on-insulator)\napplication-specific integrated circuit (ASIC) implementation of a novel\nsoft-output Gram-domain block coordinate descent (GBCD) data detector for\nmassive multi-user (MU) multiple-input multiple-output (MIMO) systems. The ASIC\nsimultaneously addresses the high throughput requirements for millimeter wave\n(mmWave) communication, stringent area and power budget per subcarrier in an\northogonal frequency-division multiplexing (OFDM) system, and error-rate\nperformance challenges posed by realistic mmWave channels. The proposed GBCD\nalgorithm utilizes a posterior mean estimate (PME) denoiser and is optimized\nusing deep unfolding, which results in superior error-rate performance even in\nscenarios with highly correlated channels or where the number of user equipment\n(UE) data streams is comparable to the number of basestation (BS) antennas. The\nfabricated GBCD ASIC supports up to 16 UEs transmitting QPSK to 256-QAM symbols\nto a 128-antenna BS, and achieves a peak throughput of 7.1 Gbps at 367 mW. The\ncore area is only 0.97 mm$^2$ thanks to a reconfigurable array of processing\nelements that enables extensive resource sharing. Measurement results\ndemonstrate that the proposed GBCD data-detector ASIC achieves best-in-class\nthroughput and area efficiency.",
        "The transmission of zoonotic diseases between animals and humans poses an\nincreasing threat. Rabies is a prominent example with various instances\nglobally, facilitated by a surplus of meso-predators (commonly, facultative\nsynanthropic species e.g., golden jackals [Canis aureus, hereafter jackals])\nthanks to the abundance of anthropogenic resources leading to dense populations\nclose to human establishments. To mitigate rabies outbreaks and prevent human\ninfections, authorities target the jackal which is the main rabies vector in\nmany regions, through the dissemination of oral vaccines in known jackals'\nactivity centers, as well as opportunistic culling to reduce population\ndensity. Because dilution (i.e., culling) is not selective towards sick or\nun-vaccinated individuals, these two complementary epizootic intervention\npolicies (EIPs) can interfere with each other. Nonetheless, there is only\nlimited examination of the interactive effectiveness of these EIPs and their\npotential influence on rabies epizootic spread dynamics, highlighting the need\nto understand these measures and the spread of rabies in wild jackals. In this\nstudy, we introduce a novel spatio-temporal extended-SIR\n(susceptible-infected-recovered) model with a graph-based spatial framework for\nevaluating mitigation efficiency. We implement the model in a case study using\na jackal population in northern Israel, and using spatial and movement data\ncollected by Advanced Tracking and Localization of Animals in real-life Systems\n(ATLAS) telemetry. An agent-based simulation approach allows us to explore\nvarious biologically-realistic scenarios, and assess the impact of different\nEIPs configurations. Our model suggests that under biologically-realistic\nunderlying assumptions and scenarios, the effectiveness of both EIPs is not\ninfluenced much by the jackal population size but is sensitive to their\ndispersal between activity centers.",
        "Observations and theory suggest that Type Ia supernovae (SNIa) heating and\nmass loss from asymptotic giant branch (AGB) stars play a crucial role in the\ninterstellar medium (ISM) of massive galaxies. We perform 3D hydrodynamic\nsimulations of the central few kiloparsecs of massive galaxies, including\nradiative cooling and mass and energy injection from AGB winds and SNIa\n(resolving each SNIa remnant, a few $\\times10~\\mathrm{pc}$ in size), excluding\nblack hole feedback. We study systems with different initial core thermodynamic\nprofiles, focusing on NGC 1399. Our simulations reproduce its observed density\nand entropy profiles well. Over $100~\\mathrm{Myr}$, two steady-state profiles\nemerge, depending on the inner circumgalactic medium (CGM) pressure and the\nratio of Ia heating to cooling: (i) if SNIa heating is less than cooling, a\ncooling flow develops; (ii) if SNIa heating is comparable to or exceeds\ncooling, SNIa heating drives a slow subsonic outflow of AGB ejecta, with black\nhole accretion at small radii. This outflow, pressure-confined by the CGM,\nadapts the ISM to the CGM properties: a low entropy CGM results in a dense, low\nentropy ISM with higher black hole accretion, while a high entropy CGM leads to\na less dense, high entropy ISM with lower accretion. This suggests that the\nAGB-SNIa regulated ISM connects CGM and galaxy scales, potentially influencing\nblack hole feedback in massive halos. Approximate methods of modeling Ia\nheating, such as clustered SNIa and smoothly distributed heating, produce\nunrealistic ISM profiles over $100~\\mathrm{Myr}$, highlighting the importance\nof resolving SNIa in simulations.",
        "The well-known paradox of linear stability for the some bounded shear flows\nis not solved up to now and is bypassed on the basis of the non-linear\nmechanisms consideration. We prove that it is arising only due to an idealized\nassumption of an exact space periodicity for the small hydrodynamic\nperturbations. When finite non-zero viscosity is taken into account only\nquasi-periodic boundary conditions must be used. The conditions of linear\ninstability to the Hagen-Poiseuille flow and to the plane Couette flow are\nobtained.",
        "Variational-hemivariational inequalities are an important mathematical\nframework for nonsmooth problems. The framework can be used to study\napplication problems from physical sciences and engineering that involve\nnon-smooth and even set-valued relations, monotone or non-monotone, among\nphysical quantities. Since no analytic solution formulas are expected for\nvariational-hemivariational inequalities from applications, numerical methods\nare needed to solve the problems. This paper focuses on numerical analysis of\nvariational-hemivariational inequalities, reporting new results as well as\nsurveying some recent published results in the area. A general convergence\nresult is presented for Galerkin solutions of the inequalities under minimal\nsolution regularity conditions available from the well-posedness theory, and\nC\\'{e}a's inequalities are derived for error estimation of numerical solutions.\nThe finite element method and the virtual element method are taken as examples\nof numerical methods, optimal order error estimates for the linear element\nsolutions are derived when the methods are applied to solve three\nrepresentative contact problems under certain solution regularity assumptions.\nNumerical results are presented to show the performance of both the finite\nelement method and the virtual element method, including numerical convergence\norders of the numerical solutions that match the theoretical predictions.",
        "Let $e^{-tL}$ be a analytic semigroup generated by $-L$, where $L$ is a\nnon-negative self-adjoint operator on $L^2(\\mathbb{R}^d)$. Assume that the\nkernels of $e^{-tL}$, denoted by $p_t(x,y)$, only satisfy the upper bound: for\nall $N>0$, there are constants $c,C>0$ such that \\begin{align}\\label{upper\nbound}\n|p_t(x,y)|\\leq\\frac{C}{t^{d\/2}}e^{-\\frac{|x-y|^2}{ct}}\\Big(1+\\frac{\\sqrt{t}}{\\rho(x)}+\n\\frac{\\sqrt{t}}{\\rho(y)}\\Big)^{-N} \\end{align} holds for all\n$x,y\\in\\mathbb{R}^d$ and $t>0$. We first establish the quantitative matrix\nweighted inequalities for fractional type integrals associated to $L$ with new\nclasses of matrix weights, which are nontrivial extension of the results\nestablished by Li, Rahm and Wick [23]. Next, we give new two-weight bump\nconditions with Young functions satisfying wider conditions for fractional type\nintegrals associated to $L$, which cover the result obtained by Cruz-Uribe,\nIsralowitz and Moen [6]. We point out that the new classes of matrix weights\nand bump conditions are larger and weaker than the classical ones given in [17]\nand [6], respectively. As applications, our results can be applied to settings\nof magnetic Schr\\\"{o}dinger operator, Laguerre operators, etc."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models",
    "start_abstract":"Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
      ],
      "abstract":[
        "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Quasi-periodic oscillations of GHz-band polarization in a black hole",
        "Principles and Metrics of Extreme Learning Machines Using a Highly\n  Nonlinear Fiber",
        "Molecular Mechanism Enabling Linearity and Symmetry in Neuromorphic\n  Elements",
        "VideoMerge: Towards Training-free Long Video Generation",
        "A redescription mining framework for post-hoc explaining and relating\n  deep learning models",
        "Quantum Chebyshev Probabilistic Models for Fragmentation Functions",
        "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding",
        "Learning Control of Neural Sound Effects Synthesis from Physically\n  Inspired Models",
        "Predicting Steady-State Behavior in Complex Networks with Graph Neural\n  Networks",
        "LanP: Rethinking the Impact of Language Priors in Large Vision-Language\n  Models",
        "The Vlasov Bivector: A Parameter-Free Approach to Vlasov Kinematics",
        "Beyond the Lungs: Extending the Field of View in Chest CT with Latent\n  Diffusion Models",
        "Pulmonary Tuberculosis Edge Diagnosis System Based on MindSpore\n  Framework: Low-cost and High-precision Implementation with Ascend 310 Chip",
        "Generalized Decision Focused Learning under Imprecise\n  Uncertainty--Theoretical Study",
        "How does Radiation Reaction Affect Relativistic Magnetized Shocks\n  Emission",
        "PISN 2018ibb: radioactive emission of [O III] lines",
        "Uniform estimates for elliptic equations with Carath\\'eodory\n  nonlinearities at the interior and on the boundary",
        "VAQUUM: Are Vague Quantifiers Grounded in Visual Data?",
        "An artificially intelligent magnetic resonance spectroscopy\n  quantification method: Comparison between QNet and LCModel on the cloud\n  computing platform CloudBrain-MRS",
        "Assessing the Impact of the Quality of Textual Data on Feature\n  Representation and Machine Learning Models",
        "Can Cross Encoders Produce Useful Sentence Embeddings?",
        "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand\n  Rendering",
        "Certifying Pareto-Optimality in Multi-Objective Maximum Satisfiability",
        "Measurements of extreme first passage times in photon transport",
        "Cost Preserving Dependent Rounding for Allocation Problems",
        "A Survey of Direct Preference Optimization",
        "LU Decomposition and Generalized Autoone-Takagi Decomposition of Dual\n  Matrices and their Applications",
        "Orthogonal Alignment of Galaxy Group Angular Momentum with Cosmic\n  Filament Spines: An Observational Study",
        "Landau-Khalatnikov-Fradkin Transformations in Quantum Electrodynamics:\n  For Perturbation Theory and Dynamical Mass Generation"
      ],
      "abstract":[
        "Relativistic jets from accreting black holes (BHs) radiate non-thermal\nemission which is highly variable in different time scales. Magnetic fields\nanchored to a rotating BH or accretion disc accelerate and collimate jets of\nthe BH systems. Previous studies on black holes of different mass scales,\nincluding supermassive and stellar-mass black holes, only report flux\nquasi-periodic oscillations in radio, optical, X-ray and gamma-ray bands. No\nquasi-periodic variations in polarization have yet been detected in any black\nhole systems. Here, we report the first detection of GHz radio polarization\noscillations in GRS 1915+105, which harbors a spinning stellar-mass BH with a\nrelativistic jet. Our observations show that during the increasing phase of\nradio emission, linear polarization and flux exhibit similar oscillation\nperiods of $\\sim 17$ and $33$ seconds, and their variation patterns\nanti-correlate with each other. These rare, short-period oscillations in both\npolarization and flux would be important to understand instabilities and\nspecial dynamics in magnetized jets.",
        "Optical computing offers potential for ultra high-speed and low latency\ncomputation by leveraging the intrinsic properties of light. Here, we explore\nthe use of highly nonlinear optical fibers (HNLFs) as platforms for optical\ncomputing based on the concept of Extreme Learning Machines. Task-independent\nevaluations are introduced to the field for the first time and focus on the\nfundamental metrics of effective dimensionality and consistency, which we\nexperimentally characterize for different nonlinear and dispersive conditions.\nWe show that input power and fiber characteristics significantly influence the\ndimensionality of the computational system, with longer fibers and higher\ndispersion producing up to 100 principal components (PCs) at input power levels\nof 30 mW, where the PC correspond to the linearly independent dimensions of the\nsystem. The spectral distribution of the PC's eigenvectors reveals that the\nhigh-dimensional dynamics facilitating computing through dimensionality\nexpansion are located within 40~nm of the pump wavelength at 1560~nm, providing\ngeneral insight for computing with nonlinear Schr\\\"odinger equation systems.\nTask-dependent results demonstrate the effectiveness of HNLFs in classifying\nMNIST dataset images. Using input data compression through PC analysis, we\ninject MNIST images of various input dimensionality into the system and study\nthe impact of input power upon classification accuracy. At optimized power\nlevels we achieve a classification test accuracy of 88\\%, significantly\nsurpassing the baseline of 83.7\\% from linear systems. Noteworthy, we find that\nbest performance is not obtained at maximal input power, i.e. maximal system\ndimensionality, but at more than one order of magnitude lower. The same is\nconfirmed regarding the MNIST image's compression, where accuracy is\nsubstantially improved when strongly compressing the image to less than 50 PCs.",
        "For over a decade, linear and symmetric weight updates have remained the\nelusive holy grail in neuromorphic computing. Here, we unveil a kinetically\ncontrolled molecular mechanism driving a near-ideal neuromorphic element,\ncapable of precisely modulating conductance linearly across 16,500 analog\nlevels spanning four orders of magnitude. Our findings, supported by\nexperimental data and mathematical modelling, demonstrate how nonlinear\nprocesses such as nucleation can be orchestrated within small perturbation\nregimes to achieve linearity. This establishes a groundwork for routinely\nrealizing these long-sought neuromorphic features across a broad range of\nmaterial systems.",
        "Long video generation remains a challenging and compelling topic in computer\nvision. Diffusion based models, among the various approaches to video\ngeneration, have achieved state of the art quality with their iterative\ndenoising procedures. However, the intrinsic complexity of the video domain\nrenders the training of such diffusion models exceedingly expensive in terms of\nboth data curation and computational resources. Moreover, these models\ntypically operate on a fixed noise tensor that represents the video, resulting\nin predetermined spatial and temporal dimensions. Although several high quality\nopen-source pretrained video diffusion models, jointly trained on images and\nvideos of varying lengths and resolutions, are available, it is generally not\nrecommended to specify a video length at inference that was not included in the\ntraining set. Consequently, these models are not readily adaptable to the\ndirect generation of longer videos by merely increasing the specified video\nlength. In addition to feasibility challenges, long-video generation also\nencounters quality issues. The domain of long videos is inherently more complex\nthan that of short videos: extended durations introduce greater variability and\nnecessitate long-range temporal consistency, thereby increasing the overall\ndifficulty of the task. We propose VideoMerge, a training-free method that can\nbe seamlessly adapted to merge short videos generated by pretrained\ntext-to-video diffusion model. Our approach preserves the model's original\nexpressiveness and consistency while allowing for extended duration and dynamic\nvariation as specified by the user. By leveraging the strengths of pretrained\nmodels, our method addresses challenges related to smoothness, consistency, and\ndynamic content through orthogonal strategies that operate collaboratively to\nachieve superior quality.",
        "Deep learning models (DLMs) achieve increasingly high performance both on\nstructured and unstructured data. They significantly extended applicability of\nmachine learning to various domains. Their success in making predictions,\ndetecting patterns and generating new data made significant impact on science\nand industry. Despite these accomplishments, DLMs are difficult to explain\nbecause of their enormous size. In this work, we propose a novel framework for\npost-hoc explaining and relating DLMs using redescriptions. The framework\nallows cohort analysis of arbitrary DLMs by identifying statistically\nsignificant redescriptions of neuron activations. It allows coupling neurons to\na set of target labels or sets of descriptive attributes, relating layers\nwithin a single DLM or associating different DLMs. The proposed framework is\nindependent of the artificial neural network architecture and can work with\nmore complex target labels (e.g. multi-label or multi-target scenario).\nAdditionally, it can emulate both pedagogical and decompositional approach to\nrule extraction. The aforementioned properties of the proposed framework can\nincrease explainability and interpretability of arbitrary DLMs by providing\ndifferent information compared to existing explainable-AI approaches.",
        "We propose a quantum protocol for efficiently learning and sampling\nmultivariate probability distributions that commonly appear in high-energy\nphysics. Our approach introduces a bivariate probabilistic model based on\ngeneralized Chebyshev polynomials, which is (pre-)trained as an explicit\ncircuit-based model for two correlated variables, and sampled efficiently with\nthe use of quantum Chebyshev transforms. As a key application, we study the\nfragmentation functions~(FFs) of charged pions and kaons from single-inclusive\nhadron production in electron-positron annihilation. We learn the joint\ndistribution for the momentum fraction $z$ and energy scale $Q$ in several\nfragmentation processes. Using the trained model, we infer the correlations\nbetween $z$ and $Q$ from the entanglement of the probabilistic model, noting\nthat the developed energy-momentum correlations improve model performance.\nFurthermore, utilizing the generalization capabilities of the quantum Chebyshev\nmodel and extended register architecture, we perform a fine-grid multivariate\nsampling relevant for FF dataset augmentation. Our results highlight the\ngrowing potential of quantum generative modeling for addressing problems in\nscientific discovery and advancing data analysis in high-energy physics.",
        "Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.",
        "Sound effects model design commonly uses digital signal processing techniques\nwith full control ability, but it is difficult to achieve realism within a\nlimited number of parameters. Recently, neural sound effects synthesis methods\nhave emerged as a promising approach for generating high-quality and realistic\nsounds, but the process of synthesizing the desired sound poses difficulties in\nterms of control. This paper presents a real-time neural synthesis model guided\nby a physically inspired model, enabling the generation of high-quality sounds\nwhile inheriting the control interface of the physically inspired model. We\nshowcase the superior performance of our model in terms of sound quality and\ncontrol.",
        "In complex systems, information propagation can be defined as diffused or\ndelocalized, weakly localized, and strongly localized. This study investigates\nthe application of graph neural network models to learn the behavior of a\nlinear dynamical system on networks. A graph convolution and attention-based\nneural network framework has been developed to identify the steady-state\nbehavior of the linear dynamical system. We reveal that our trained model\ndistinguishes the different states with high accuracy. Furthermore, we have\nevaluated model performance with real-world data. In addition, to understand\nthe explainability of our model, we provide an analytical derivation for the\nforward and backward propagation of our framework.",
        "Large Vision-Language Models (LVLMs) have shown impressive performance in\nvarious tasks. However, LVLMs suffer from hallucination, which hinders their\nadoption in the real world. Existing studies emphasized that the strong\nlanguage priors of LVLMs can overpower visual information, causing\nhallucinations. However, the positive role of language priors is the key to a\npowerful LVLM. If the language priors are too weak, LVLMs will struggle to\nleverage rich parameter knowledge and instruction understanding abilities to\ncomplete tasks in challenging visual scenarios where visual information alone\nis insufficient. Therefore, we propose a benchmark called LanP to rethink the\nimpact of Language Priors in LVLMs. It is designed to investigate how strong\nlanguage priors are in current LVLMs. LanP consists of 170 images and 340\ncorresponding well-designed questions. Extensive experiments on 25 popular\nLVLMs reveal that many LVLMs' language priors are not strong enough to\neffectively aid question answering when objects are partially hidden. Many\nmodels, including GPT-4 Turbo, exhibit an accuracy below 0.5 in such a\nscenario.",
        "Plasma kinetics, for both flat and curved spacetime, is conventionally\nperformed on the mass shell, a 7--dimensional time-phase space with a Vlasov\nvector field, also known as the Liouville vector field. The choice of this\ntime-phase space encodes the parameterisation of the underling 2nd order\nordinary differential equations. By replacing the Vlasov vector on time-phase\nspace with a bivector on an 8--dimensional sub-bundle of the tangent bundle, we\ncreate a parameterisation free version of Vlasov theory. This has a number of\nadvantages, which include working for lightlike and ultra-relativistic\nparticles, non metric connections, and metric-free and premetric theories. It\nalso works for theories where no time-phase space can exist for topological\ntopological reasons. An example of this is when we wish to consider all\ngeodesics, including spacelike geodesics.\n  We extend the particle density function to a 6--form on the subbundle of the\ntangent space, and define the transport equations, which correspond to the\nVlasov equation. We then show how to define the corresponding 3--current on\nspacetime. We discuss the stress-energy tensor needed for the Einstein-Vlasov\nsystem.\n  This theory can be generalised to create parameterisation invariant Vlasov\ntheories for many 2nd order theories, on arbitrary manifolds. The relationship\nto sprays and semi-sprays is given and examples from Finsler geometry are also\ngiven.",
        "The interconnection between the human lungs and other organs, such as the\nliver and kidneys, is crucial for understanding the underlying risks and\neffects of lung diseases and improving patient care. However, most research\nchest CT imaging is focused solely on the lungs due to considerations of cost\nand radiation dose. This restricted field of view (FOV) in the acquired images\nposes challenges to comprehensive analysis and hinders the ability to gain\ninsights into the impact of lung diseases on other organs. To address this, we\npropose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel\napproach to capture the inter-organ relationships from CT images and extend the\nFOV of chest CT images. Our approach first trains a variational autoencoder\n(VAE) to encode 2D axial CT slices individually, then stacks the latent\nrepresentations of the VAE to form a 3D context for training a latent diffusion\nmodel. Once trained, our approach extends the FOV of CT images in the\nz-direction by generating new axial slices in a zero-shot manner. We evaluated\nour approach on the National Lung Screening Trial (NLST) dataset, and results\nsuggest that it effectively extends the FOV to include the liver and kidneys,\nwhich are not completely covered in the original NLST data acquisition.\nQuantitative results on a held-out whole-body dataset demonstrate that the\ngenerated slices exhibit high fidelity with acquired data, achieving an SSIM of\n0.81.",
        "Pulmonary Tuberculosis (PTB) remains a major challenge for global health,\nespecially in areas with poor medical resources, where access to specialized\nmedical knowledge and diagnostic tools is limited. This paper presents an\nauxiliary diagnosis system for pulmonary tuberculosis based on Huawei MindSpore\nframework and Ascend310 edge computing chip. Using MobileNetV3 architecture and\nSoftmax cross entropy loss function with momentum optimizer. The system\noperates with FP16 hybrid accuracy on the Orange pie AIPro (Atlas 200 DK) edge\ndevice and performs well. In the test set containing 4148 chest images, the\nmodel accuracy reached 99.1\\% (AUC = 0.99), and the equipment cost was\ncontrolled within \\$150, providing affordable AI-assisted diagnosis scheme for\nprimary care.",
        "Decision Focused Learning has emerged as a critical paradigm for integrating\nmachine learning with downstream optimisation. Despite its promise, existing\nmethodologies predominantly rely on probabilistic models and focus narrowly on\ntask objectives, overlooking the nuanced challenges posed by epistemic\nuncertainty, non-probabilistic modelling approaches, and the integration of\nuncertainty into optimisation constraints. This paper bridges these gaps by\nintroducing innovative frameworks: (i) a non-probabilistic lens for epistemic\nuncertainty representation, leveraging intervals (the least informative\nuncertainty model), Contamination (hybrid model), and probability boxes (the\nmost informative uncertainty model); (ii) methodologies to incorporate\nuncertainty into constraints, expanding Decision-Focused Learning's utility in\nconstrained environments; (iii) the adoption of Imprecise Decision Theory for\nambiguity-rich decision-making contexts; and (iv) strategies for addressing\nsparse data challenges. Empirical evaluations on benchmark optimisation\nproblems demonstrate the efficacy of these approaches in improving decision\nquality and robustness and dealing with said gaps.",
        "Relativistic magnetized shocks, through the Synchrotron Maser Instability\n(SMI) mechanism, represent a promising framework for generating coherent\nradiations, potentially accounting for the enigmatic Fast Radio Bursts\n(FRBs)-cosmic radio transients with extreme luminosity. This study investigates\nhow the radiation reaction (RR) effect, induced by high-energy photon emissions\nduring SMI, significantly modifies particle dynamics and emission properties in\nmagnetized shocks. Through comprehensive Particle-In-Cell (PIC) simulations, we\ndemonstrate that RR effects fundamentally alter coherent cyclotron motion at\nshock fronts, producing distinct observational signatures: spectral broadening,\npeak frequency upshift, and enhanced radiation intensity. Our findings suggest\nthat RR-mediated magnetized shocks could provide a natural explanation for the\nbimodal energy distribution observed in repeating FRB 121102 and the positive\ncorrelation of luminosity-bandwidth between repeating and one-off FRBs in\nCHIME\/FRB catalog. These results support the magnetized shock as a viable\nsource of FRBs.",
        "Supernova 2018ibb of the PISN category related to the dynamical instability\nof oxygen core in a supermassive star induced by pair-creation shows at the\nnebular stage strong [\\oiii] emission lines of an uncertain origin. I propose a\nsimple model that demonstrates a possibility of [O III] lines emission from the\nsupernova oxygen matter ionized and heated by the $^{56}$Co radioactive decay.\nThe reason is pinpointed by which the [O III] line luminosity among supernovae\nof PISN category can vary in a broad range.",
        "We establish an explicit uniform a priori estimate for weak solutions to\nslightly subcritical elliptic problems with nonlinearities simultaneously at\nthe interior and on the boundary. Our explicit $L^{\\infty}(\\Omega )$ a priori\nestimates are in terms of powers of their $H^{1}(\\Omega )$ norms. To prove our\nresult, we combine a De Giorgi-Nash-Moser's iteration scheme together with\nelliptic regularity and the Gagliardo-Nirenberg's interpolation inequality.",
        "Vague quantifiers such as \"a few\" and \"many\" are influenced by many\ncontextual factors, including how many objects are present in a given context.\nIn this work, we evaluate the extent to which vision-and-language models (VLMs)\nare compatible with humans when producing or judging the appropriateness of\nvague quantifiers in visual contexts. We release a novel dataset, VAQUUM,\ncontaining 20300 human ratings on quantified statements across a total of 1089\nimages. Using this dataset, we compare human judgments and VLM predictions\nusing three different evaluation methods. Our findings show that VLMs, like\nhumans, are influenced by object counts in vague quantifier use. However, we\nfind significant inconsistencies across models in different evaluation\nsettings, suggesting that judging and producing vague quantifiers rely on two\ndifferent processes.",
        "Objctives: This work aimed to statistically compare the metabolite\nquantification of human brain magnetic resonance spectroscopy (MRS) between the\ndeep learning method QNet and the classical method LCModel through an\neasy-to-use intelligent cloud computing platform CloudBrain-MRS. Materials and\nMethods: In this retrospective study, two 3 T MRI scanners Philips Ingenia and\nAchieva collected 61 and 46 in vivo 1H magnetic resonance (MR) spectra of\nhealthy participants, respectively, from the brain region of pregenual anterior\ncingulate cortex from September to October 2021. The analyses of Bland-Altman,\nPearson correlation and reasonability were performed to assess the degree of\nagreement, linear correlation and reasonability between the two quantification\nmethods. Results: Fifteen healthy volunteers (12 females and 3 males, age\nrange: 21-35 years, mean age\/standard deviation = 27.4\/3.9 years) were\nrecruited. The analyses of Bland-Altman, Pearson correlation and reasonability\nshowed high to good consistency and very strong to moderate correlation between\nthe two methods for quantification of total N-acetylaspartate (tNAA), total\ncholine (tCho), and inositol (Ins) (relative half interval of limits of\nagreement = 3.04%, 9.3%, and 18.5%, respectively; Pearson correlation\ncoefficient r = 0.775, 0.927, and 0.469, respectively). In addition,\nquantification results of QNet are more likely to be closer to the previous\nreported average values than those of LCModel. Conclusion: There were high or\ngood degrees of consistency between the quantification results of QNet and\nLCModel for tNAA, tCho, and Ins, and QNet generally has more reasonable\nquantification than LCModel.",
        "Background: Data collected in controlled settings typically results in\nhigh-quality datasets. However, in real-world applications, the quality of data\ncollection is often compromised. It is well established that the quality of a\ndataset significantly impacts the performance of machine learning models.\n  Methods: A rudimentary error rate metric was developed to evaluate textual\ndataset quality at the token level. Mixtral Large Language Model (LLM) was used\nto quantify and correct errors in low quality datasets. The study analyzed two\nhealthcare datasets: the high-quality MIMIC-III public hospital dataset and a\nlower-quality private dataset from Australian aged care homes. Errors were\nsystematically introduced into MIMIC at varying rates, while the ACH dataset\nquality was improved using the LLM.\n  Results: For the sampled 35,774 and 6,336 patients from the MIMIC and ACH\ndatasets respectively, we used Mixtral to introduce errors in MIMIC and correct\nerrors in ACH. Mixtral correctly detected errors in 63% of progress notes, with\n17% containing a single token misclassified due to medical terminology. LLMs\ndemonstrated potential for improving progress note quality by addressing\nvarious errors. Under varying error rates, feature representation performance\nwas tolerant to lower error rates (<10%) but declined significantly at higher\nrates.\n  Conclusions: The study revealed that models performed relatively well on\ndatasets with lower error rates (<10%), but their performance declined\nsignificantly as error rates increased (>=10%). Therefore, it is crucial to\nevaluate the quality of a dataset before utilizing it for machine learning\ntasks. For datasets with higher error rates, implementing corrective measures\nis essential to ensure the reliability and effectiveness of machine learning\nmodels.",
        "Cross encoders (CEs) are trained with sentence pairs to detect relatedness.\nAs CEs require sentence pairs at inference, the prevailing view is that they\ncan only be used as re-rankers in information retrieval pipelines. Dual\nencoders (DEs) are instead used to embed sentences, where sentence pairs are\nencoded by two separate encoders with shared weights at training, and a loss\nfunction that ensures the pair's embeddings lie close in vector space if the\nsentences are related. DEs however, require much larger datasets to train, and\nare less accurate than CEs. We report a curious finding that embeddings from\nearlier layers of CEs can in fact be used within an information retrieval\npipeline. We show how to exploit CEs to distill a lighter-weight DE, with a\n5.15x speedup in inference time.",
        "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance.",
        "Due to the wide employment of automated reasoning in the analysis and\nconstruction of correct systems, the results reported by automated reasoning\nengines must be trustworthy. For Boolean satisfiability (SAT) solvers - and\nmore recently SAT-based maximum satisfiability (MaxSAT) solvers -\ntrustworthiness is obtained by integrating proof logging into solvers, making\nsolvers capable of emitting machine-verifiable proofs to certify correctness of\nthe reasoning steps performed. In this work, we enable for the first time proof\nlogging based on the VeriPB proof format for multi-objective MaxSAT (MO-MaxSAT)\noptimization techniques. Although VeriPB does not offer direct support for\nmulti-objective problems, we detail how preorders in VeriPB can be used to\nprovide certificates for MO-MaxSAT algorithms computing a representative\nsolution for each element in the non-dominated set of the search space under\nPareto-optimality, without extending the VeriPB format or the proof checker. By\nimplementing VeriPB proof logging into a state-of-the-art multi-objective\nMaxSAT solver, we show empirically that proof logging can be made scalable for\nMO-MaxSAT with reasonable overhead.",
        "Photon transport through turbid media has typically been modeled through\ndiffusion or telegraph equations. These models describe behavior of the\naverage, or typical, photon with remarkable accuracy, however, we show here\nthat they fail to capture the Extreme First Passage Times (EFPTs) of photon\ntransport. By sending ultra-fast bursts of photons through a scattering medium\nand timing the arrival of the first passage photon, we measure the distribution\nof these EFPTs of photons in a random environment. Our measured EFPTs differ\nfrom those predicted by both the diffusion approximation and telegraph\nequation. Instead, we observe the EFPT as the time expected for light to travel\nthrough an index-averaged medium. These results reveal flaws in both models and\ninvite a re-examining of their underlying assumptions.",
        "We present a dependent randomized rounding scheme, which rounds fractional\nsolutions to integral solutions satisfying certain hard constraints on the\noutput while preserving Chernoff-like concentration properties. In contrast to\nprevious dependent rounding schemes, our algorithm guarantees that the cost of\nthe rounded integral solution does not exceed that of the fractional solution.\nOur algorithm works for a class of assignment problems with restrictions\nsimilar to those of prior works.\n  In a non-trivial combination of our general result with a classical approach\nfrom Shmoys and Tardos [Math. Programm.'93] and more recent linear programming\ntechniques developed for the restricted assignment variant by Bansal,\nSviridenko [STOC'06] and Davies, Rothvoss, Zhang [SODA'20], we derive a O(log\nn)-approximation algorithm for the Budgeted Santa Claus Problem. In this new\nvariant, the goal is to allocate resources with different values to players,\nmaximizing the minimum value a player receives, and satisfying a budget\nconstraint on player-resource allocation costs.",
        "Large Language Models (LLMs) have demonstrated unprecedented generative\ncapabilities, yet their alignment with human values remains critical for\nensuring helpful and harmless deployments. While Reinforcement Learning from\nHuman Feedback (RLHF) has emerged as a powerful paradigm for aligning LLMs with\nhuman preferences, its reliance on complex reward modeling introduces inherent\ntrade-offs in computational efficiency and training stability. In this context,\nDirect Preference Optimization (DPO) has recently gained prominence as a\nstreamlined alternative that directly optimizes LLMs using human preferences,\nthereby circumventing the need for explicit reward modeling. Owing to its\ntheoretical elegance and computational efficiency, DPO has rapidly attracted\nsubstantial research efforts exploring its various implementations and\napplications. However, this field currently lacks systematic organization and\ncomparative analysis. In this survey, we conduct a comprehensive overview of\nDPO and introduce a novel taxonomy, categorizing previous works into four key\ndimensions: data strategy, learning framework, constraint mechanism, and model\nproperty. We further present a rigorous empirical analysis of DPO variants\nacross standardized benchmarks. Additionally, we discuss real-world\napplications, open challenges, and future directions for DPO. This work\ndelivers both a conceptual framework for understanding DPO and practical\nguidance for practitioners, aiming to advance robust and generalizable\nalignment paradigms. All collected resources are available and will be\ncontinuously updated at\nhttps:\/\/github.com\/liushunyu\/awesome-direct-preference-optimization.",
        "This paper uses matrix transformations to provide the Autoone-Takagi\ndecomposition of dual complex symmetric matrices and extends it to dual\nquaternion $\\eta$-Hermitian matrices. The LU decomposition of dual matrices is\ngiven using the general solution of the Sylvester equation, and its equivalence\nto the existence of rank-k decomposition and dual Moore-Penrose generalized\ninverse (DMPGI) is proved. Similar methods are then used to provide the\nCholesky decomposition of dual real symmetric positive definite matrices. Both\nof our decompositions are driven by applications in numerical linear algebra.",
        "We investigate the alignment between the angular momenta of galaxy groups and\nthe spines of their associated cosmic filaments. Our results demonstrate a\nsignificant tendency for these two orientations to be perpendicular, indicating\nthat the rotation of a galaxy group does not originate from the spin of cosmic\nfilaments. Instead, it is driven by the orbital angular momentum contributed by\nmember galaxies as they accrete along the direction of the filament spines.\nMoreover, the strength of this perpendicular alignment signal varies with the\nrichness of the galaxy groups, with the most pronounced alignment observed\namong the wealthiest groups. This pronounced alignment is largely due to the\nmore coherent spatial distribution of member galaxies in richer groups relative\nto the filament spines. Our study provides valuable insights into the\nmechanisms of angular momentum acquisition in galaxy groups from an\nobservational standpoint.",
        "We carry out a comprehensive analysis of the Landau-Khalatnikov-Fradkin\ntransformations for a charged fermion propagator at the two-loop level in\nquantum electrodynamics (QED). Starting with an arbitrary covariant gauge $\\xi$\nand space-time dimension $d$, we provide its explicit expressions in three and\nfour-dimensional QED. We begin with the tree-level fermion propagator in the\nLandau gauge and gauge-transform it to obtain an analytical expression for an\nall order result in an arbitrary covariant gauge. We expand it out to two-loops\nboth for the massless and massive propagators in three and four space-time\ndimensions. In addition to comparing with all earlier results in the literature\nwherever possible, we also study constraints of multiplicative renormalizabilty\nof our results in four-dimensional QED which are logarithmically divergent.\nFinally, we analyze representative solutions of the fermion propagator which\ncorrespond to dynamical chiral symmetry breaking and mass generation in QED. We\nstudy the gauge dependence of these emergent solutions, that of the Euclidean\npole mass and the chiral fermion condensate."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology",
    "start_abstract":"Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
      ],
      "abstract":[
        "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Search for medium effects using jet axis decorrelation in inclusive jets\n  from PbPb collisions at $\\sqrt{s_\\text{NN}}$ = 5.02 TeV",
        "Colorful Helly via induced matchings",
        "Superlubric Motion of Wave-like Domain Walls in Sliding Ferroelectrics",
        "Extended string-net models with all anyons at finite temperature",
        "Holographic inflation and holographic dark energy from entropy of the\n  anti-de Sitter black hole",
        "A framework for Tate modules of abelian varieties under isogeny",
        "Monolayer transition metal dichalcogenides under finite-pulse polarized\n  radiation",
        "Constrained Fuel and Time Optimal 6DOF Powered Descent Guidance Using\n  Indirect Optimization",
        "More resourceful states improve quantum channel discrimination",
        "Fingerprint Matrix Concept for Detecting, Localizing and Characterizing\n  Targets in Complex Media",
        "Observing Hot Holographic Quark Star With Gravitational Waves",
        "Predicting the depth of the most recent common ancestor of a random\n  sample of $k$ species: the impact of phylogenetic tree shape",
        "Generating Networks to Target Assortativity via Archimedean Copula\n  Graphons",
        "Can Dark Stars account for the star formation efficiency excess at very\n  high redshifts?",
        "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays",
        "Sampling the full hierarchical population posterior distribution in\n  gravitational-wave astronomy",
        "An Interior-Point Algorithm for Continuous Nonlinearly Constrained\n  Optimization with Noisy Function and Derivative Evaluations",
        "Dynamic Photometric Variability in Three Young Brown Dwarfs in Taurus:\n  Detection of Optical Flares with TESS data",
        "Multilingual Performance of a Multimodal Artificial Intelligence System\n  on Multisubject Physics Concept Inventories",
        "New Approaches to the Monotonicity Inequality for Linear Stochastic PDEs",
        "Betti numbers of normal edge rings (II)",
        "Pricing Quanto and Composite Contracts with Local-Correlation Models",
        "One-loop matching for leading-twist generalised\n  transverse-momentum-dependent distributions",
        "$L^{2}-$ Well-posedness and Bounded Controllability of KdV-B equation",
        "A visual representation of the properties of pre- and post- selected\n  entangled systems",
        "Spin-dependent dark matter scattering in quasi-two-dimensional magnets",
        "A pseudo-dynamic phase-field model for brittle fracture",
        "Coboundaries of 3-IETs",
        "Enhanced shot noise in graphene quantum point contacts with\n  electrostatic reconstruction"
      ],
      "abstract":[
        "The jet axis decorrelation in inclusive jets is studied using lead-lead\n(PbPb) collisions at a center-of-mass energy per nucleon pair of 5.02 TeV. The\njet axis decorrelation is defined as the angular difference between two\ndefinitions of the jet axis. It is obtained by applying two recombination\nschemes on all the constituents of a given jet reconstructed by the anti-\\kt\nsequential algorithm with a distance parameter of $R$ = 0.4. The data set,\ncorresponding to an integrated luminosity of 0.66 nb$^{-1}$, was collected in\n2018 with the CMS detector at the CERN LHC. The jet axis decorrelations are\nexamined across collision centrality selections and intervals of jet transverse\nmomentum. A centrality dependent evolution of the measured distributions is\nobserved, with a progressive narrowing seen in more central events. This\nnarrowing could result from medium-induced modification of the internal jet\nstructure or reflect color charge effects in energy loss. This new measurement\nprobes jet substructure in previously unexplored kinematic domains and show\ngreat promise for providing new insights on the color charge dependence of\nenergy loss to jet-quenching models.",
        "We establish a theorem regarding the maximum size of an {\\it{induced}}\nmatching in the bipartite complement of the incidence graph of a set system\n$(X,\\mathcal{F})$. We show that this quantity plus one provides an upper bound\non the colorful Helly number of this set system, i.e. the minimum positive\ninteger $N$ for which the following statement holds: if finite subfamilies\n$\\mathcal{F}_1,\\ldots, \\mathcal{F}_{N} \\subset \\mathcal{F}$ are such that\n$\\cap_{F \\in \\mathcal{F}_{i}} F = 0$ for every $i=1,\\ldots,N$, then there\nexists $F_i \\in \\mathcal{F}_i$ such that $F_1 \\cap \\ldots \\cap F_{N} =\n\\emptyset$. We will also discuss some natural refinements of this result and\napplications.",
        "Sliding ferroelectrics constructed from stacked nonpolar monolayers enable\nout-of-plane polarization in two dimensions with exceptional properties,\nincluding ultrafast switching speeds and fatigue-free behavior. However, the\nwidely accepted switching mechanism, which posits synchronized long-distance\nin-plane translation of entire atomic layers driven by an out-of-plane electric\nfield, has shown inconsistencies with experimental observations. We demonstrate\nthat this spinodal decomposition-like homogeneous switching process violates\nNeumann's principle and is unlikely to occur due to symmetry constraint.\nInstead, symmetry-breaking domain walls (DWs) and the tensorial nature of Born\neffective charges are critical for polarization reversal, underscoring the\nquantum nature of sliding ferroelectrics. Using the Bernal-stacked $h$-BN\nbilayer as a model system, we discover that the coherent propagation of wide,\nwave-like domain walls is the key mechanism for ferroelectric switching. This\nmechanism fundamentally differs from the layer-by-layer switching associated\nwith narrow domain walls, which has been established for over sixty years in\nperovskite ferroelectrics. Moreover, these wave-like DWs exhibit superlubric\ndynamics, achieving ultrahigh velocities of approximately 4000 m\/s at room\ntemperature and displaying an anomalous cooling-promoted switching speed. The\nunexpected emergence of DW superlubricity in sliding ferroelectrics presents\nnew avenues for enhancing key performance metrics and offers exciting\nopportunities for applications in cryogenic environments.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "Based on the entropy of anti-de Sitter black hole, a new holographic dark\nenergy model has been proposed. When the Hubble horizon and particle horizon\nare chosen as the IR cutoff, the late-time accelerated expansion of universe is\nrealized. In this paper, we consider the Hubble horizon as the IR cutoff to\ninvestigate holographic inflation and slow-roll inflation in this model. We\nfind that slow-roll inflation with the chaotic potential $V_{0}\\phi^{n}$ is\nfavored by Planck results for some special cases, such as $n=1\/3$ and $n=1\/2$,\nwhile holographic inflation is not supported by Planck results. Then, we\nanalyze the reheating temperature and the number of reheating e-folds in this\nmodel, and we find that the results favor the cases $n=1\/3$ and $n=1\/2$.\nFinally, we use the dynamical analysis method, statefinder diagnostic pairs,\nand the Hubble diagram to analyze this model. Our results indicate that when\n$b^{2}$ takes a small value, this model cannot be distinguished from the\nstandard $\\Lambda$CDM model and can serve as an alternative to it.",
        "We explain the linear algebraic framework provided by Tate modules of\nisogenous abelian varieties in a category-theoretic way.",
        "Recent advances in time-resolved angle-resolved photoemission spectroscopy\nhave enabled access to ultrafast electron states and their spin dynamics in\nsolids. Atomically thin transition metal dichalcogenides are paradigmatic\ntwo-dimensional materials where electron momentum and spin degrees of freedom\nare coupled, being suitable candidates for time-resolved spectroscopy studies.\nIn this work, we present a thorough study of the electron dynamics when these\nmaterials are subject to an intense finite-pulse driving radiation. We extend\nthe scope of the conventional Floquet engineering and rely of the so-called\n$t-t^{\\prime}$ formalism to deal with driving fields described with two\ndistinct time scales, namely the envelope amplitude timescale and the time\nperiod of the external field. The interplay between the finite-pulse timescales\nand the intrinsic properties of the electrons gives rise to transient valley\npolarization and dynamical modifications of band structures, revealed by the\ntime-dependent circular dichroism of the sample.",
        "Powered descent guidance (PDG) problems subject to six-degrees-of-freedom\n(6DOF) dynamics allow for enforcement of practical attitude constraints.\nHowever, numerical solutions to 6DOF PDG problems are challenging due to fast\nrotational dynamics coupled with translational dynamics, and the presence of\nhighly nonlinear state\/control path inequality constraints. In this work,\nconstrained fuel- and time-optimal 6DOF PDG problems are solved leveraging a\nregularized indirect method, subject to inequality constraints on the thrust\nmagnitude, thruster gimbal angle, rocket tilt angle, glideslope angle, and\nangular velocity magnitude. To overcome the challenges associated with solving\nthe resulting multipoint boundary-value problems (MPBVPs), the state-only path\ninequality constraints (SOPICs) are enforced through an interior penalty\nfunction method, which embeds the resulting MPBVPs into a multi-parameter\nsmooth neighboring families of two-point BVPs. Extremal solutions are obtained\nusing an indirect multiple-shooting solution method with numerical\ncontinuation. Moreover, an empirical relation is derived for the\ndirectly-adjoined Lagrange multipliers associated with SOPICs. The fuel- and\ntime-optimal trajectories are compared against solutions of DIDO -- a capable\npseudospectral-based software for solving practical constrained optimal control\nproblems.",
        "One of the key issues in quantum discrimination problems is understanding the\nextent of the advantages in discrimination performance when using resource\nstates compared to resourceless states. We show that in any resource theory of\nstates, which may not be convex, the extent to which the maximum average\nsuccess probability can be improved in quantum channel discrimination problems\nwithout using auxiliary systems can be precisely quantified by the robustness\nmeasure. Furthermore, we demonstrate that the robustness measure can also\nquantify the improvement in channel discrimination problems that use auxiliary\nsystems. Using these findings, resources can be fully characterized to achieve\nhigher success probabilities than any state without the given resource in\nchannel discrimination problems.",
        "As waves propagate through a complex medium, they undergo multiple scattering\nevents. This phenomenon is detrimental to imaging, as it causes a full blurring\nof the image beyond a transport mean free path. Here, we show how to detect,\nlocalize, and characterize any scattering target through the reflection matrix\nof the complex medium in which this target is embedded and thus hidden from\ndirect view. More precisely, we introduce a fingerprint operator that contains\nthe specific signature of the target with respect to its environment. Applied\nto the recorded reflection matrix, this operator provides a likelihood index of\nthe target in any given state, despite the scattering fog induced by the\nsurrounding environment. This state can be the target position for localization\npurposes, its shape for characterization, or any other parameter that\ninfluences the target response. Our concept is versatile and broadly applicable\nto different type of waves for which multi-element technology allows a\nreflection matrix to be measured. We demonstrate this here explicitly by\nperforming different proof-of-concept experiments with ultrasound on targets\nburied inside a strongly scattering granular suspension, on lesion markers for\nclinical applications, and on the architecture of muscle tissue.",
        "We extract the equation of state for hot quark matter from a holographic\n$2+1$ flavor QCD model, which could form the core of a stable compact star. By\nadding a thin hadron shell, a new type of hybrid star is constructed. With the\ntemperature serving as a parameter, the EoS varies and we obtain stable stars\nwith the maximum mass of around 23 to 30 solar masses, and the compactness\naround $0.1$. The I-Love-Q-C relations are further discussed, and compared with\nthe neutron star cases. These compact stars are candidates for black hole\nmimickers, which could be observed by gravitational waves and distinguished by\nproperties like nonzero tidal Love number and electromagnetic signals.",
        "We consider the following question: how close to the ancestral root of a\nphylogenetic tree is the most recent common ancestor of $k$ species randomly\nsampled from the tips of the tree? For trees having shapes predicted by the\nYule-Harding model, it is known that the most recent common ancestor is likely\nto be close to (or equal to) the root of the full tree, even as $n$ becomes\nlarge (for $k$ fixed). However, this result does not extend to models of tree\nshape that more closely describe phylogenies encountered in evolutionary\nbiology. We investigate the impact of tree shape (via the Aldous\n$\\beta-$splitting model) to predict the number of edges that separate the most\nrecent common ancestor of a random sample of $k$ tip species and the root of\nthe parent tree they are sampled from. Both exact and asymptotic results are\npresented. We also briefly consider a variation of the process in which a\nrandom number of tip species are sampled.",
        "We develop an approach to generate random graphs to a target level of\nassortativity by using copula structures in graphons. Unlike existing random\ngraph generators, we do not use rewiring or binning approaches to generate the\ndesired random graph. Instead, we connect Archimedean bivariate copulas to\ngraphons in order to produce flexible models that can generate random graphs to\ntarget assortativity. We propose three models that use the copula distribution\nfunction, copula density function and their mixed tensor product to produce\nnetworks. We express the assortativity coefficient in terms of homomorphism\ndensities. Establishing this relationship forges a connection between the\nparameter of the copula and the frequency of subgraphs in the generated\nnetwork. Therefore, our method attains a desired the subgraph distribution as\nwell as the target assortativity. We establish the homomorphism densities and\nassortativity coefficient for each of the models. Numerical examples\ndemonstrate the ability of the proposed models to produce graphs with different\nlevels of assortativity.",
        "The James Webb Space Telescope (JWST) has recently conducted observations of\nmassive galaxies at high redshifts, revealing a notable anomaly in their star\nformation efficiency (SFE). Motivated by the recent identification of three\n$\\sim 10^{6}M_\\odot$ dark star candidates, we investigate whether dark stars\ncan be the origin of the SFE excess. It turns out that the excess can be\nreproduced by a group of dark stars with $M \\gtrsim 10^{3}\\, \\rm M_{\\odot}$,\nbecause of their domination in generating primary UV radiation in high-redshift\ngalaxies. The genesis of these dark stars is attributed to the capture of\nWeakly Interacting Massive Particles (WIMPs) within a mass range of tens of GeV\nto a few TeV. However, if the top-heavy initial mass function of dark stars\nholds up to $\\sim 10^{5}M_\\odot$, the relic black holes stemming from their\ncollapse would be too abundant to be consistent with the current observations\nof Massive Compact Halo Objects (MACHOs). We thus suggest that just a small\nfraction of SFE excess may be contributed by the very massive dark stars and\nthe majority likely originated from other reasons such as the Population III\nstars in view of their rather similar UV radiation efficiencies.",
        "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by up to two orders of\nmagnitude.",
        "We present a full sampling of the hierarchical population posterior\ndistribution of merging black holes using current gravitational-wave data. We\ndirectly tackle the the most relevant intrinsic parameter space made of the\nbinary parameters (masses, spin magnitudes, spin directions, redshift) of all\nthe events entering the GWTC-3 LIGO\/Virgo\/KAGRA catalog, as well as the\nhyperparameters of the underlying population of sources. This results in a\nparameter space of about 500 dimensions, in contrast with current\ninvestigations where the targeted dimensionality is drastically reduced by\nmarginalizing over all single-event parameters. In particular, we have direct\naccess to (i) population parameters, (ii) population-informed single-event\nparameters, and (iii) correlations between these two sets of parameters. Our\nimplementation relies on modern probabilistic programming languages and\nHamiltonian Monte Carlo, with a continuous interpolation of single-event\nposterior probabilities. Sampling the full hierarchical problem is feasible, as\ndemonstrated here, and advantageous as it removes some (but not all) of the\nMonte Carlo integrations that enter the likelihood together with the related\nvariances.",
        "An algorithm based on the interior-point methodology for solving continuous\nnonlinearly constrained optimization problems is proposed, analyzed, and\ntested. The distinguishing feature of the algorithm is that it presumes that\nonly noisy values of the objective and constraint functions and their\nfirst-order derivatives are available. The algorithm is based on a combination\nof a previously proposed interior-point algorithm that allows inexact\nsubproblem solutions and recently proposed algorithms for solving bound- and\nequality-constrained optimization problems with only noisy function and\nderivative values. It is shown that the new interior-point algorithm drives a\nstationarity measure below a threshold that depends on bounds on the noise in\nthe function and derivative values. The results of numerical experiments show\nthat the algorithm is effective across a wide range of problems.",
        "We present $I$-band time-series photometric variability studies of three\nknown nearby ($\\sim$ 140 pc) and young ( $\\sim$ 1 Myr) brown dwarfs (BD) in the\nTaurus star-forming region in the Perseus Molecular Cloud. From 10 nights of\nobservations over a time span of 10 years, with a typical run of 3 to 6 hours\neach night, we estimated that the BDs show unstable short-scale periodicity\nfrom 1.5 to 4.8 hours. Using the long-term photometry from the Transiting\nExoplanet Survey Satellite (TESS), we have conducted a time-resolved\nvariability analysis of CFHT-BD-Tau 3 and CFHT-BD-Tau 4, revealing orbital\nperiods of $\\sim$ 0.96 days and $\\sim$ 3 days respectively, consistent with\nearlier studies. We also found two superflares in TESS sector 43 data for\nCFHT-BD-Tau 4 and estimated the flare energies as $7.09\\times10^{35}$ erg and\n$3.75\\times10^{36}$ erg. A magnetic field of $\\sim3.39 ~kG$ is required to\ngenerate such flare energies on this BD. We performed spot modelling analysis\non CFHT-BD-Tau 3 and CFHT-BD-Tau 4 to address the variability detected in the\ndata using the package BASSMAN. Spectral energy distribution and infrared\ncolours of the sources suggest that they have a sufficient amount of\ncircumstellar material around them.",
        "We investigate the multilingual and multimodal performance of a large\nlanguage model-based artificial intelligence (AI) system, GPT-4o, on a diverse\nset of physics concept inventories spanning multiple languages and subject\nareas. The inventories taken from the PhysPort website cover the classical\nphysics topics of mechanics, electromagnetism, optics, and thermodynamics as\nwell as relativity, quantum mechanics, astronomy, mathematics, and laboratory\nskills. Unlike previous text-only studies, we uploaded the inventories as\nimages mirroring what a student would see on paper, assessing the system's\nmultimodal functionality. The AI is prompted in English and autonomously\nchooses the language of its response - either remaining in the nominal language\nof the test, switching entirely to English, or mixing languages - revealing\nadaptive behavior dependent on linguistic complexity and data availability. Our\nresults indicate some variation in performance across subject areas, with\nlaboratory skills standing out as the area of poorest performance. Furthermore,\nthe AI's performance on questions that require visual interpretation of images\nis worse than on purely text-based questions. Questions that are difficult for\nthe AI tend to be that way invariably of the inventory language. We also find\nlarge variations in performance across languages, with some appearing to\nbenefit substantially from language switching, a phenomenon similar to\ncode-switching ofhuman speakers. Overall, comparing the obtained AI results to\nthe existing literature, we find that the AI system outperforms average\nundergraduate students post-instruction in all subject areas but laboratory\nskills.",
        "The Monotonicity inequality is an important tool in the understanding of\nexistence and uniqueness of strong solutions for Stochastic PDEs. In this\narticle, we discuss three approaches to establish this deterministic inequality\nexplicitly.",
        "We compute the Betti numbers of the edge rings of multi-path graphs using the\ninduced-subgraph method introduced in \\cite{WL1}. Here, a multi-path graph\nrefers to a simple graph consisting of two vertices and multiple paths\nconnecting them, which generalizes the complete bipartite graph $K_{2,d}$.\nSpecial cases include the graph $G_{r,d}$ introduced in \\cite{GHK}, the graph\n$G_{r,s,d}$ introduced in \\cite{NN}, and the graph $B_{\\underline{\\ell},h}$\nintroduced in \\cite{LZ}.",
        "Pricing composite and quanto contracts requires a joint model of both the\nunderlying asset and the exchange rate. In this contribution, we explore the\npotential of local-correlation models to address the challenges of calibrating\nsynthetic quanto forward contracts and composite options quoted in the market.\nSpecifically, we design on-line calibration procedures for generic local and\nstochastic volatility models. The paper concludes with a numerical study\nassessing the calibration performance of these methodologies and comparing them\nto simpler approximations of the correlation structure.",
        "We present the one-loop matching coefficients necessary to match all of the\nleading-twist generalised transverse-momentum-dependent distributions (GTMDs)\nonto generalised parton distributions (GPDs). Matching functions are extracted\nby computing the first radiative corrections to partonic bilocal correlators\nwith staple-like Wilson lines, as appropriate for high-energy collisions. These\ncorrelators are characterised by a transverse displacement and skewed\nkinematics of external states. Using the proton helicity basis, they are\nparametrised in terms of GTMDs, which are subsequently related to leading-twist\nGPDs. Our results provide new insights into the complex dynamics of GTMDs\ngenerated by radiative corrections. In particular, we show that time-reversal\neven and odd contributions to GTMDs in the so-called ERBL region mix both under\nmatching and evolution. Finally, we present a selection of numerical results\nand comment on the quantitative behaviour of GTMDs.",
        "In this paper, the initial boundary value problem of the Korteweg-de Vries\nBurger equation on the negative half-plane is analyzed. Initially, the\nwell-posedness on $H^s(\\R^-)$ for $s\\geq 0$ of the IBVP is established to\nconcentrate on the $L^2(\\R^-)$ controllability problem when the controls are in\nthe Dirichlet and Newmann conditions at $x=0$.",
        "We introduce a visual representation for generating entangled-based quantum\neffects under pre- and post- selected states that allows us to reveal\nequivalence between seemingly different quantum effects. We show how to realize\nentangled quantum systems of an arbitrary number of qubits from a single or\npre-specified number of physical particles. We then show that a variation of\nthe quantum Cheshire cat experiment and Hardy's paradox are equivalent and\npropose a class of experiments that generalizes both experiments. We show that\nthe weak values of the products of projection operators allow us to get the\nweak value of each projection operator, implying that the weak value of the\nproduct of projection operators includes the entire information about the weak\nvalues in the system. In nature, interactions can only be acted between a pair\nof particles. We show how to realize quantum systems of multiwise interacted\nqubits, i.e., interactions that come in groups of n>2 qubits. In this way, we\nare able to propose unique quantum systems that consist of interacted groups of\nentangled states. The proposed framework opens the door toward a new way to\nexplore quantum systems of entangled particles and quantum phenomena that\nemerge from such a general setting.",
        "We study the prospects of detecting dark matter coupled to the spin of the\nelectron, such that it may scatter and excite magnons - collective excitations\nof electronic spins. We show that materials exhibiting long-range magnetic\norder where the spins are coupled only along a plane may act as directional\ndark matter detectors. These quasi-2D materials possess anisotropic dispersion\nrelations and structure functions which induce a sidereal modulation in the\nexcitation rate. We calculate the expected signal rate for some candidate\n(anti)ferromagnets, demonstrating a possible route to the direct detection of\nspin-dependent dark matter in the keV to MeV mass range.",
        "The enforcement of global energy conservation in phase-field fracture\nsimulations has been an open problem for the last 25 years. Specifically, the\noccurrence of unstable fracture is accompanied by a loss in total potential\nenergy, which suggests a violation of the energy conservation law. This\nphenomenon can occur even with purely quasi-static, displacement-driven loading\nconditions, where finite crack growth arises from an infinitesimal increase in\nload. While such behavior is typically seen in crack nucleation, it may also\noccur in other situations. Initial efforts to enforce energy conservation\ninvolved backtracking schemes based on global minimization, however in recent\nyears it has become clearer that unstable fracture, being an inherently dynamic\nphenomenon, cannot be adequately resolved within a purely quasi-static\nframework. Despite this, it remains uncertain whether transitioning to a fully\ndynamic framework would sufficiently address the issue. In this work, we\npropose a pseudo-dynamic framework designed to enforce energy balance without\nrelying on global minimization. This approach incorporates dynamic effects\nheuristically into an otherwise quasi-static model, allowing us to bypass\nsolving the full dynamic linear momentum equation. It offers the flexibility to\nsimulate crack evolution along a spectrum, ranging from full energy\nconservation at one extreme to maximal energy loss at the other. Using data\nfrom recent experiments, we demonstrate that our framework can closely\nreplicate experimental load-displacement curves, achieving results that are\nunattainable with classical phase-field models.",
        "In this note, we investigate the coboundaries of interval exchange\ntransformations of 3 intervals (3-IETs). More precisely, we show that a\ndifferentiable function with absolutely continuous derivative with bounded\nvariation, whose integral and integral of its derivative is 0, is a coboundary\nfor typical 3-IET if and only if the values at the endpoints of the domain are\nzero. We also show the existence of rare counterexamples for both cases of\npossible values at the endpoints of the interval. We obtain our result by\nstudying the properties of associated skew products.",
        "Shot noise measurements in quantum point contacts are a powerful tool to\ninvestigate charge transport in the integer and fractional quantum Hall regime,\nin particular to unveil the charge, quantum statistics and tunneling dynamics\nof edge excitations. In this letter, we describe shot noise measurements in a\ngraphene quantum point contact in the quantum Hall regime. At large magnetic\nfield, the competition between confinement and electronic interactions gives\nrise to a quantum dot located at the saddle point of the quantum point contact.\nWe show that the presence of this quantum dot leads to a $50-100~\\%$ increase\nin the shot noise, which we attribute to correlated charge tunneling. Our\nresults highlight the role played by the electrostatic environment in those\ngraphene devices."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's",
    "start_abstract":"Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment .",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
      ],
      "abstract":[
        "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
        "Invariant measure for the process viewed from the particle for 2D random\n  walks in Dirichlet environment",
        "Observation-Based Iterative Map for Solar Cycles. II. The Gnevyshev-Ohl\n  Rule and its Generation Mechanism",
        "Adaptive Variational Inference in Probabilistic Graphical Models: Beyond\n  Bethe, Tree-Reweighted, and Convex Free Energies",
        "On a class of high dimensional linear regression methods with debiasing\n  and thresholding",
        "Dynamic Refinement of Pressure Decomposition in Navier-Stokes Equations",
        "Homological data on the periodic structure of self-maps on wedge sums",
        "Making Them a Malicious Database: Exploiting Query Code to Jailbreak\n  Aligned Large Language Models",
        "Robust Multimodal Learning via Cross-Modal Proxy Tokens",
        "In-Context Meta LoRA Generation",
        "TinySense: A Lighter Weight and More Power-efficient Avionics System for\n  Flying Insect-scale Robots",
        "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven\n  Surface Normal-aware Tracking and Mapping",
        "Comprehensive Analog Signal Processing Platform Enabled with Acoustic\n  Charge Transport in Two-dimensional Materials",
        "A Comparative Performance Analysis of Classification and Segmentation\n  Models on Bangladeshi Pothole Dataset",
        "ODPG: Outfitting Diffusion with Pose Guided Condition",
        "Fourier State Tomography of Polarization-Encoded Qubits",
        "Regulating Ai In Financial Services: Legal Frameworks And Compliance\n  Challenges",
        "Understanding entropy production via a thermal zero-player game",
        "Stochastic Schr\\\"{o}dinger equation for homodyne measurements of\n  strongly correlated systems",
        "Q-Sets, \\Delta-Sets, and L-Spaces",
        "Network Embedding Exploration Tool (NEExT)",
        "Two-component jet model for the afterglow emission of GRB 201216C and\n  GRB 221009A and implications for jet structure of very-high-energy gamma-ray\n  bursts",
        "Anomalous Dimension of a General Effective Gauge Theory I: Bosonic\n  Sector",
        "How Strategic Agents Respond: Comparing Analytical Models with\n  LLM-Generated Responses in Strategic Classification",
        "The order of appearance of the product of the first and second Lucas\n  numbers",
        "Securing DRAM at Scale: ARFM-Driven Row Hammer Defense with Unveiling\n  the Threat of Short tRC Patterns",
        "Assouad dimension of the Takagi function",
        "Mitigating Omitted Variable Bias in Empirical Software Engineering",
        "A multiwavelength light curve analysis of the classical nova V392 Per:\n  Optical contribution from an irradiated accretion disk during the nova wind\n  phase"
      ],
      "abstract":[
        "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs.",
        "In this paper, we consider random walks in Dirichlet random environment\n(RWDE) on $\\mathbb{Z}^2$. We prove that, if the RWDE is recurrent (which is\nstrongly conjectured when the weights are symmetric), then there does not exist\nany invariant measure for the process viewed from the particle which is\nabsolutely continuous with respect to the static law of the environment.\nBesides, if the walk is directional transient and under condition\n$\\mathbf{(T')}$, we prove that there exists such an invariant probability\nmeasure if the trapping parameter verifies $\\kappa > 1$ or after acceleration\nof the process by a local function of the environment. This gives strong credit\nto a conjectural classification of cases of existence or non-existence of the\ninvariant measure for two dimensional RWDE. The proof is based on a new\nidentity, stated on general finite graphs, which is inspired by the\nrepresentation of the $\\star$-VRJP, a non-reversible generalization of the\nVertex reinforced Jump Process, in terms of random Schr\\\"odinger operators. In\nthe case of RWDE on 1D graph, the previous identity entails also a discrete\nanalogue of the Matsumoto-Yor property for Brownian motion.",
        "The Gnevyshev-Ohl (G-O) rule, also known as the even-odd effect, is an\nimportant observational phenomenon in solar cycles, suggesting that cycles with\neven indices tend to be followed by stronger cycles. The rule is considered to\nbe related to the solar dynamo, which drives the evolution of the Sun's\nlarge-scale magnetic field. However, observational studies of the G-O rule have\nrevealed inconsistencies, particularly regarding long-term variations and the\nunderlying physical mechanisms. In this study, we use an iterative map derived\nwithin the framework of the Babcock-Leighton (BL) dynamo to analyze the G-O\nrule. We investigate comprehensive and definitive forms of the G-O rule using\nboth a sufficiently large number of solar cycles and a limited number of solar\ncycles. Our findings indicate a higher probability for an arbitrary cycle to be\nfollowed by a stronger cycle instead of weaker, regardless of even or odd. Over\ntime spans comparable to historical observations, cycles exhibit periods that\nfollow both the G-O rule and the reversed G-O rule, without a statistically\nsignificant preference, consistent with the observed variability of the G-O\nrule. The occurrence of the reversed G-O rule is random, rather than periodic.\nThe G-O rule emerges as a result of the nonlinearity and stochasticity inherent\nin the BL mechanism. These results advance our understanding of the solar cycle\nand pave the way for improved solar dynamo modeling.",
        "Variational inference in probabilistic graphical models aims to approximate\nfundamental quantities such as marginal distributions and the partition\nfunction. Popular approaches are the Bethe approximation, tree-reweighted, and\nother types of convex free energies. These approximations are efficient but can\nfail if the model is complex and highly interactive. In this work, we analyze\ntwo classes of approximations that include the above methods as special cases:\nfirst, if the model parameters are changed; and second, if the entropy\napproximation is changed. We discuss benefits and drawbacks of either approach,\nand deduce from this analysis how a free energy approximation should ideally be\nconstructed. Based on our observations, we propose approximations that\nautomatically adapt to a given model and demonstrate their effectiveness for a\nrange of difficult problems.",
        "In this paper, we introduce a unified framework, inspired by classical\nregularization theory, for designing and analyzing a broad class of linear\nregression approaches. Our framework encompasses traditional methods like least\nsquares regression and Ridge regression, as well as innovative techniques,\nincluding seven novel regression methods such as Landweber and Showalter\nregressions. Within this framework, we further propose a class of debiased and\nthresholded regression methods to promote feature selection, particularly in\nterms of sparsity. These methods may offer advantages over conventional\nregression techniques, including Lasso, due to their ease of computation via a\nclosed-form expression. Theoretically, we establish consistency results and\nGaussian approximation theorems for this new class of regularization methods.\nExtensive numerical simulations further demonstrate that the debiased and\nthresholded counterparts of linear regression methods exhibit favorable finite\nsample performance and may be preferable in certain settings.",
        "In this work, the local decomposition of pressure in the Navier-Stokes\nequations is dynamically refined to prove that a relevant critical energy of a\nsuitable Leray-type solution inside a backward paraboloid, regardless of its\naperture is controlled near the vertex by a critical behavior confined to a\nneighborhood of the paraboloid's boundary. This neighborhood excludes the\ninterior near the vertex and remains separated from the temporal profile of the\nvertex, except at the vertex itself.",
        "In this article, we study the periodic points for continuous self-maps on the\nwedge sum of topological manifolds, exhibiting a particular combinatorial\nstructure. We compute explicitly the Lefschetz numbers, the Dold coefficients\nand consider its set of algebraic periods. Moreover, we study the special case\nof maps on the wedge sum of tori, and show some of the homological obstructions\npresent in defining these maps.",
        "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is\navailable at https:\/\/github.com\/horizonsinzqs\/QueryAttack.",
        "Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality. To efficiently learn the approximation for\nthe missing modality via CMPTs with minimal computational overhead, we employ\nlow-rank adapters in frozen unimodal encoders and jointly optimize an alignment\nloss with a task-specific loss. Extensive experiments on five multimodal\ndatasets show that our method outperforms state-of-the-art baselines across\nvarious missing rates while achieving competitive results in complete-modality\nsettings. Overall, our method offers a flexible and efficient solution for\nrobust multimodal learning. The code and pretrained models will be released on\nGitHub.",
        "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
        "In this paper, we introduce advances in the sensor suite of an autonomous\nflying insect robot (FIR) weighing less than a gram. FIRs, because of their\nsmall weight and size, offer unparalleled advantages in terms of material cost\nand scalability. However, their size introduces considerable control\nchallenges, notably high-speed dynamics, restricted power, and limited payload\ncapacity. While there have been advancements in developing lightweight sensors,\noften drawing inspiration from biological systems, no sub-gram aircraft has\nbeen able to attain sustained hover without relying on feedback from external\nsensing such as a motion capture system. The lightest vehicle capable of\nsustained hovering -- the first level of ``sensor autonomy'' -- is the much\nlarger 28 g Crazyflie. Previous work reported a reduction in size of that\nvehicle's avionics suite to 187 mg and 21 mW. Here, we report a further\nreduction in mass and power to only 78.4 mg and 15 mW. We replaced the laser\nrangefinder with a lighter and more efficient pressure sensor, and built a\nsmaller optic flow sensor around a global-shutter imaging chip. A Kalman Filter\n(KF) fuses these measurements to estimate the state variables that are needed\nto control hover: pitch angle, translational velocity, and altitude. Our system\nachieved performance comparable to that of the Crazyflie's estimator while in\nflight, with root mean squared errors of 1.573 deg, 0.186 m\/s, and 0.136 m,\nrespectively, relative to motion capture.",
        "Simultaneous Localization and Mapping (SLAM) is essential for precise\nsurgical interventions and robotic tasks in minimally invasive procedures.\nWhile recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM\nwith high-quality novel view synthesis and fast rendering, these systems\nstruggle with accurate depth and surface reconstruction due to multi-view\ninconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between\nthe reconstructed frames. In this work, we present Endo-2DTAM, a real-time\nendoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these\nchallenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which\nconsists of tracking, mapping, and bundle adjustment modules for geometrically\naccurate reconstruction. Our robust tracking module combines point-to-point and\npoint-to-plane distance metrics, while the mapping module utilizes normal\nconsistency and depth distortion to enhance surface reconstruction quality. We\nalso introduce a pose-consistent strategy for efficient and geometrically\ncoherent keyframe sampling. Extensive experiments on public endoscopic datasets\ndemonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth\nreconstruction of surgical scenes while maintaining computationally efficient\ntracking, high-quality visual appearance, and real-time rendering. Our code\nwill be released at github.com\/lastbasket\/Endo-2DTAM.",
        "Two-dimensional Acoustic Charge Transport (2D-ACT) devices, which integrate\ntwo dimensional semiconductor field-effect transistor (FET) with high-frequency\nsurface acoustic wave (SAW) device provide a potential compact platform for the\nprocessing of analog signals in a wireless, non-contact, low-loss and real-time\nway. It is expected to be used in long-distance space communication and\nsensing. However, current investigations into 2D-ACT devices are still limited\nto the observation of DC acoustoelectric currents, and have yet to achieve\nreal-time electronic signal processing capabilities. In this paper, we have\ndesigned a hybrid acoustoelectric platform composed of two-dimensional\nsemiconductor FET and SAW device. The platform is capable of processing DC\nsignals, exhibiting ambipolar transport behavior. The sub-wavelength channel\nlength of the FET within the platform allows for the real-time observation of\ncarrier distribution at a microscopic scale in conjunction with the SAW\npotential, and facilitating the reproduction and intensity regulation of AC\nsignals. By adjusting the relative phase and intensity ratio of two\ncounter-propagating SAWs, the platform also enables the addition and\nsubtraction of AC signals.",
        "The study involves a comprehensive performance analysis of popular\nclassification and segmentation models, applied over a Bangladeshi pothole\ndataset, being developed by the authors of this research. This custom dataset\nof 824 samples, collected from the streets of Dhaka and Bogura performs\ncompetitively against the existing industrial and custom datasets utilized in\nthe present literature. The dataset was further augmented four-fold for\nsegmentation and ten-fold for classification evaluation. We tested nine\nclassification models (CCT, CNN, INN, Swin Transformer, ConvMixer, VGG16,\nResNet50, DenseNet201, and Xception) and four segmentation models (U-Net,\nResU-Net, U-Net++, and Attention-Unet) over both the datasets. Among the\nclassification models, lightweight models namely CCT, CNN, INN, Swin\nTransformer, and ConvMixer were emphasized due to their low computational\nrequirements and faster prediction times. The lightweight models performed\nrespectfully, oftentimes equating to the performance of heavyweight models. In\naddition, augmentation was found to enhance the performance of all the tested\nmodels. The experimental results exhibit that, our dataset performs on par or\noutperforms the similar classification models utilized in the existing\nliterature, reaching accuracy and f1-scores over 99%. The dataset also\nperformed on par with the existing datasets for segmentation, achieving model\nDice Similarity Coefficient up to 67.54% and IoU scores up to 59.39%.",
        "Virtual Try-On (VTON) technology allows users to visualize how clothes would\nlook on them without physically trying them on, gaining traction with the rise\nof digitalization and online shopping. Traditional VTON methods, often using\nGenerative Adversarial Networks (GANs) and Diffusion models, face challenges in\nachieving high realism and handling dynamic poses. This paper introduces\nOutfitting Diffusion with Pose Guided Condition (ODPG), a novel approach that\nleverages a latent diffusion model with multiple conditioning inputs during the\ndenoising process. By transforming garment, pose, and appearance images into\nlatent features and integrating these features in a UNet-based denoising model,\nODPG achieves non-explicit synthesis of garments on dynamically posed human\nimages. Our experiments on the FashionTryOn and a subset of the DeepFashion\ndataset demonstrate that ODPG generates realistic VTON images with fine-grained\ntexture details across various poses, utilizing an end-to-end architecture\nwithout the need for explicit garment warping processes. Future work will focus\non generating VTON outputs in video format and on applying our attention\nmechanism, as detailed in the Method section, to other domains with limited\ndata.",
        "Quantum state tomography is a central technique for the characterization and\nverification of quantum systems. Standard tomography is widely used for\nlow-dimensional systems, but for larger systems, it becomes impractical due to\nthe exponential scaling of experimental complexity with the number of qubits.\nHere, we present an experimental realization of Fourier-transform quantum state\ntomography for polarization-encoded photonic states. We validate the technique\nusing weak coherent states and entangled photon pairs generated by a quantum\ndot and spontaneous parametric down-conversion source in the telecom\nwavelength. The reconstructed density matrices show excellent agreement with\nthose obtained through conventional projective tomography, with calculated\nmetrics such as fidelity and concurrence matching within error bars, confirming\nthe reliability and accuracy of the technique. Fourier state tomography employs\nonly a single rotating waveplate per qubit, thereby avoiding repeated\nadjustments across multiple waveplates and ensuring that the number of physical\nmeasurement settings scales linearly with the number of qubits, despite the\nexponential growth of the underlying state space. This reduction in optical\nconfigurations simplifies experimental overhead, making Fourier state\ntomography a practical alternative for multi-qubit characterization.",
        "This article examines the evolving landscape of artificial intelligence (AI)\nregulation in financial services, detailing the legal frameworks and compliance\nchallenges posed by rapid technological adoption. By reviewing current\nlegislation, industry guidelines, and real-world use cases, it highlights how\nAI-driven processes, from fraud detection to algorithmic trading, offer\nefficiency gains yet introduce significant risks, including algorithmic bias,\ndata privacy breaches, and lack of transparency in automated decision-making.\nThe study compares regulatory approaches across major jurisdictions such as the\nEuropean Union, United States, and United Kingdom, identifying both universal\nconcerns, like the need for explainability and robust data protection, and\nregion-specific compliance requirements that impact the implementation of\nhigh-risk AI applications. Additionally, it underscores emerging areas of\nfocus, such as liability for AI-driven errors, systemic risks posed by\ninterlinked AI systems, and the ethical considerations of technology-driven\nfinancial exclusion. The findings reveal gaps in existing rules and emphasize\nthe necessity for adaptive, technology-neutral policies capable of fostering\ninnovation while safeguarding consumer rights and market integrity. The article\nconcludes by proposing a principled regulatory model that balances flexibility\nwith enforceable standards, advocating closer collaboration between\npolicymakers, financial institutions, and AI developers to ensure a secure,\nfair, and forward-looking framework for AI in finance.",
        "A new thermal bath scheme for Ising-Conway Entropy Game (ICEg) is introduced.\nNew game moves in sampling the given temperature is achieved via Monte Carlo\ndynamics of both Metropolis and Glauber as a stochastic game. This kind of\napproach makes the game an ideal tool for demonstrating thermal dependency of\nentropy production in a novel way. Using this new approach, Ising-Conway\nEntropy game's rate of entropy production depending on different temperatures\nare explored. Thermalized game is shown to be physically interesting and\nplausible test bed for studying complex dynamical systems in classical\nstatistical mechanics, that is conceptually simple, pedagogically accessible,\nyet realistic.",
        "We derive a stochastic Schr\\\"{o}dinger equation that describes the homodyne\nmeasurement record of a strongly interacting atomic system. We derive this\nequation for a general system, where we use the rotating wave approximation in\nthe linear atom-light interaction part, and the resulting equation is expressed\nin terms of the atomic operators only. Weak measurements are theoretically\ndescribed in terms of positive operator-valued measures. Among different weak\nmeasurement schemes, several earlier references studied the Gaussian quantum\ncontinuous measurement in detail. Here we consider a homodyne measurement\nsetup. We then demonstrate that the derived equation for this setup in the\nappropriate limit is the same as the one obtained while performing a Gaussian\nquantum continuous measurement.",
        "The question whether there is a Lindelof Q-set space or Lindelof $\\Delta$-set\nspace is considered. We show that J. Moore's ZFC $L$-space is not a Q-set space\nin ZFC and, assuming all Aronszajn trees are special, it is not a $\\Delta$-set\nspace.",
        "Many real-world and artificial systems and processes can be represented as\ngraphs. Some examples of such systems include social networks, financial\ntransactions, supply chains, and molecular structures. In many of these cases,\none needs to consider a collection of graphs, rather than a single network.\nThis could be a collection of distinct but related graphs, such as different\nprotein structures or graphs resulting from dynamic processes on the same\nnetwork. Examples of the latter include the evolution of social networks,\ncommunity-induced graphs, or ego-nets around various nodes. A significant\nchallenge commonly encountered is the absence of ground-truth labels for graphs\nor nodes, necessitating the use of unsupervised techniques to analyze such\nsystems. Moreover, even when ground-truth labels are available, many existing\ngraph machine learning methods depend on complex deep learning models,\ncomplicating model explainability and interpretability. To address some of\nthese challenges, we have introduced NEExT (Network Embedding Exploration Tool)\nfor embedding collections of graphs via user-defined node features. The\nadvantages of the framework are twofold: (i) the ability to easily define your\nown interpretable node-based features in view of the task at hand, and (ii)\nfast embedding of graphs provided by the Vectorizers library. In this paper, we\ndemonstrate the usefulness of NEExT on collections of synthetic and real-world\ngraphs. For supervised tasks, we demonstrate that performance in graph\nclassification tasks could be achieved similarly to other state-of-the-art\ntechniques while maintaining model interpretability. Furthermore, our framework\ncan also be used to generate high-quality embeddings in an unsupervised way,\nwhere target variables are not available.",
        "In recent years, afterglow emission in the very-high-energy (VHE) band above\n100 GeV have been clearly detected for at least five gamma-ray bursts (GRBs\n180720B, 190114C, 190829A, 201216C and 221009A). For some of these VHE GRBs, we\npreviously proposed a two-component jet model, consisting of two uniform jets\nwith narrow and wide opening angles to explain their multiwavelength afterglows\nincluding VHE gamma rays. In this paper, we show that the VHE emission from\nGRBs 201216C and 221009A can be also explained by our two-component jet model.\nWe find that the collimation-corrected kinetic energy of the five VHE GRBs have\ntypical values of 5\\times10^{49} erg and 5\\times10^{50} erg for the narrow and\nwide jets, respectively. We discuss the similarities and differences among the\nVHE GRBs, and the implications for the structure of their jets. In particular,\nthe narrow jet of GRB 221009A has a smaller opening angle, which can explain\nwhy its isotropic-equivalent energy is unusually large.",
        "We classify the physical operators of the most general bosonic effective\ngauge theory up to dimension six using on-shell methods. Based on this\nclassification, we compute the complete one-loop anomalous dimension employing\nboth on-shell unitarity-based and geometric techniques. Our analysis fully\naccounts for the mixing of operators with different dimensions. The results\nbroadly apply to any Effective Field Theory with arbitrary gauge symmetry and\nbosonic degrees of freedom. To illustrate their utility, we perform a complete\ncross-check of results on the renormalization of the Standard Model Effective\nField Theory (SMEFT), $O(n)$ scalar theory, and the SMEFT extended with an\naxion-like particle. Additionally, we present new results for axion-like\nparticles with CP-violating interactions.",
        "When machine learning (ML) algorithms are used to automate human-related\ndecisions, human agents may gain knowledge of the decision policy and behave\nstrategically to obtain desirable outcomes. Strategic Classification (SC) has\nbeen proposed to address the interplay between agents and decision-makers.\nPrior work on SC has relied on assumptions that agents are perfectly or\napproximately rational, responding to decision policies by maximizing their\nutilities. Verifying these assumptions is challenging due to the difficulty of\ncollecting real-world agent responses. Meanwhile, the growing adoption of large\nlanguage models (LLMs) makes it increasingly likely that human agents in SC\nsettings will seek advice from these tools. We propose using strategic advice\ngenerated by LLMs to simulate human agent responses in SC. Specifically, we\nexamine five critical SC scenarios -- hiring, loan applications, school\nadmissions, personal income, and public assistance programs -- and simulate how\nhuman agents with diverse profiles seek advice from LLMs. We then compare the\nresulting agent responses with the best responses generated by existing\ntheoretical models. Our findings reveal that: (i) LLMs and theoretical models\ngenerally lead to agent score or qualification changes in the same direction\nacross most settings, with both achieving similar levels of fairness; (ii)\nstate-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide\nhelpful suggestions, though these suggestions typically do not result in\nmaximal score or qualification improvements; and (iii) LLMs tend to produce\nmore diverse agent responses, often favoring more balanced effort allocation\nstrategies. These results suggest that theoretical models align with LLMs to\nsome extent and that leveraging LLMs to simulate more realistic agent responses\noffers a promising approach to designing trustworthy ML systems.",
        "Let $\\left(U_n\\right)_{n\\geq0}$ and $\\left(V_n\\right)_{n\\geq0}$ be the first\nand second Lucas sequences, respectively. Let $m$ be a positive integer. Then\nthe order of appearance of $m$ in the first Lucas sequence is defined as the\nsmallest positive integer $k$ such that $m$ divides $U_k$ and denoted by\n$\\tau(m)$. In this paper, we give explicit formulae for the terms $\\tau(U_m\nV_n)$, $\\tau(U_m U_n)$, $\\tau(V_m V_n)$ and $\\tau(U_nU_{n+p}U_{n+2p})$, where\n$p\\geq3$ is a prime number.",
        "To address the issue of powerful row hammer (RH) attacks, our study involved\nan extensive analysis of the prevalent attack patterns in the field. We\ndiscovered a strong correlation between the timing and density of the\nactive-to-active command period, ${tRC}$, and the likelihood of RH attacks. In\nthis paper, we introduce MARC, an innovative ARFM-driven RH mitigation IP that\nsignificantly reinforces existing RH mitigation IPs. MARC dynamically adjusts\nthe frequency of RFM in response to the severity of the RH attack environment,\noffering a tailored security solution that not only detects the threats but\nalso adapts to varying threat levels. MARC's detection mechanism has\ndemonstrated remarkable efficiency, identifying over 99\\% of attack patterns.\nMoreover, MARC is designed as a compact hardware module, facilitating tight\nintegration either on the memory controller-side or DRAM-side within the memory\nsystem. It only occupies a negligible hardware area of 3363~\\textit{$\\mu m^2$}.\nBy activating ARFM based on MARC's detection, the additional energy overhead is\nalso negligible in normal workloads. We conduct experiments to compare the\nhighest row count throughout the patterns, defined as max exposure, between the\nvanilla RH mitigation IPs and the MARC-enhanced versions of the same IPs,\nfocusing on both DRAM-side and memory controller-side. On the DRAM-side, MARC +\nprobabilistic scheme and MARC + counter-based tracking scheme achieve\n8.1$\\times$ and 1.5$\\times$ improvement in max exposure ratio compared to the\nvanilla IPs, respectively. On the memory controller-side, the MARC + PARA and\nMARC + Graphene achieve 50$\\times$ and 5.7$\\times$ improvement in max exposure\nratio compared to the vanilla IPs, respectively. MARC ensures optimal security\nwithout sacrificing system performance, making MARC a pioneering solution in\nthe realm of RH attack mitigation.",
        "For any integer $b\\geq2$ and real series $\\{c_n\\}$ such that\n$\\sum_{n=0}^\\infty|c_n|<\\infty$, the generalized Takagi function $f_{{\\mathbf\nc},b}(x)$ is defined by $$\n  f_{{\\mathbf c},b}(x):=\\sum_{n=0}^\\infty c_n\\phi(b^n x), \\quad x\\in [0,1], $$\nwhere $\\phi(x)=dist(x,\\mathbb{Z})$ is the distance from $x$ to the nearest\ninteger. The collection of functions with the form are called the Takagi class.\nIn this paper, we show that in the case that $\\varlimsup_{n \\to \\infty} b^n\n|c_n|<\\infty$, the Assouad dimension of the graph ${\\mathcal G} f_{{\\mathbf\nc},b}=\\{(x,f_{{\\mathbf c},b}(x)):x\\in[0,1]\\}$ for the generalized Takagi\nfunction $f_{{\\mathbf c},b}(x)$ is equal to one, that is, $$ \\dim_A {\\mathcal\nG} f_{{\\mathbf c},b}=1. $$ In particular, for each $0<a<1$ and integer $b \\geq\n2$, we define Takagi function $T_{a,b}$ as followed, $$\n  T_{a,b}(x):=\\sum_{n=0}^\\infty a^n \\phi(b^n x), \\quad x\\in [0,1]. $$ Then $\n  \\dim_A {\\mathcal G} T_{a,b}=1 $ if and only if $0<a \\leq 1\/b$.",
        "Omitted variable bias occurs when a statistical model leaves out variables\nthat are relevant determinants of the effects under study. This results in the\nmodel attributing the missing variables' effect to some of the included\nvariables -- hence over- or under-estimating the latter's true effect. Omitted\nvariable bias presents a significant threat to the validity of empirical\nresearch, particularly in non-experimental studies such as those prevalent in\nempirical software engineering.\n  This paper illustrates the impact of omitted variable bias on two case\nstudies in the software engineering domain, and uses them to present methods to\ninvestigate the possible presence of omitted variable bias, to estimate its\nimpact, and to mitigate its drawbacks. The analysis techniques we present are\nbased on causal structural models of the variables of interest, which provide a\npractical, intuitive summary of the key relations among variables.\n  This paper demonstrates a sequence of analysis steps that inform the design\nand execution of any empirical study in software engineering. An important\nobservation is that it pays off to invest effort investigating omitted variable\nbias before actually executing an empirical study, because this effort can lead\nto a more solid study design, and to a significant reduction in its threats to\nvalidity.",
        "The classical nova V392 Per 2018 is characterized by a very fast optical\ndecline, long binary orbital period of 3.23 days, detection of GeV gamma rays,\nand almost identical decay trends of $B$, $V$, and $I_{\\rm C}$ light curves.\nThe last feature is unique because most novae develop strong emission lines in\nthe nebular phase and these lines contribute especially to the $B$ and $V$\nbands and make large differences between the $BV$ and $I_{\\rm C}$ light curves.\nThis unique feature can be understood if the optical flux is dominated by\ncontinuum until the late phase of the nova outburst. Such continuum radiation\nis emitted by a bright accretion disk irradiated by a hydrogen burning white\ndwarf (WD) and viscous heating disk with high mass-accretion rate after the\nhydrogen burning ended. We present a comprehensive nova outburst model that\nreproduces all of these light curves. We determined the WD mass to be $M_{\\rm\nWD}=1.35$ - $1.37 ~M_\\odot$ and the distance modulus in the $V$ band to be\n$(m-M)_V=14.6 \\pm 0.2$; the distance is $d= 3.45\\pm 0.5$ kpc for the reddening\nof $E(B-V)=0.62$."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"FuXi-S2S: An accurate machine learning model for global subseasonal forecasts",
    "start_abstract":"Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "Analysis methods for numerical weather prediction"
      ],
      "abstract":[
        "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "Efficient First-Principles Framework for Overdamped Phonon Dynamics and\n  Anharmonic Electron-Phonon Coupling in Superionic Materials",
        "Euclid Quick Data Release (Q1) -- Data release overview",
        "On the conjecture of non-inner automorphisms of finite $p$-groups with a\n  non-trivial abelian direct factor",
        "D-HAT: a Diatom-inspired structure for a Helmet concept Against Trauma",
        "In Vivo Study of Bone Growth Around Additively Manufactured Implants\n  with Ti-6Al-4V and Bioactive Glass Powder Composites",
        "On the Isomorphism Problem of Cayley Graphs of Graph Products",
        "Revisiting gluon density from the BK equation with kinematical\n  constraint and large x terms",
        "CIBER 4th flight fluctuation analysis: Pseudo-power spectrum formalism,\n  improved source masking and validation on mocks",
        "Study of SARS-CoV-2 Spike Protein by Surface Enhanced Raman Spectroscopy\n  and Transmission Electron Microscopy",
        "State transfer of Grover walks on unitary and quadratic unitary Cayley\n  graphs over finite commutative rings",
        "An Automated Bandwidth Division for the LHCb Upgrade Trigger",
        "Vision-Aided Channel Prediction Based on Image Segmentation at Street\n  Intersection Scenarios",
        "Partially hyperbolic symplectomorphism with C^1 bundles",
        "Principles for Open Data Curation: A Case Study with the New York City\n  311 Service Request Data",
        "Can Yang-Baxter imply Lie algebra?",
        "Comparison of stochastic BGK and FP methods for the simulation of\n  non-equilibrium multi-species molecular gas flows",
        "Exact multiblack hole spacetimes in Einstein-ModMax theory",
        "Temperature-Dependent Calibration Procedures for the Silicon\n  Photomultiplier Readout of the Cosmic Ray Veto Detector for the Mu2e\n  Experiment",
        "Determining the Density of the Sun with Neutrinos",
        "Noise equals endogenous control",
        "Nonabelian Yang-Mills-Higgs and Plateau's problem in codimension three",
        "Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for\n  Agents",
        "Transverse expansion of the metric at null hypersurfaces II. Existence\n  results and application to Killing horizons",
        "Visual tests using several safe confidence intervals",
        "Connecting SPDE to SGMs",
        "Universal point spread function engineering for 3D optical information\n  processing",
        "A Unifying Framework for Complex-Valued Eigenfunctions via The Cartan\n  Embedding",
        "On conservative algebras of 2-dimensional Algebras",
        "Bootstrap Nonparametric Inference under Data Integration"
      ],
      "abstract":[
        "Relying on the anharmonic special displacement method, we introduce an ab\ninitio quasistatic polymorphous framework to describe local disorder,\nanharmonicity, and electron-phonon coupling in superionic conductors. Using the\nexample of cubic Cu2Se, we show that positional polymorphism yields extremely\noverdamped anharmonic vibrations while preserving transverse acoustic phonons,\nconsistent with experiments. We also demonstrate well-defined electronic band\nstructures with large band gap openings due to polymorphism of 1.0 eV and\ncalculate anharmonic electron-phonon renormalization, yielding band gap\nnarrowing with increasing temperature in agreement with previous measurements.\nOur approach opens the way for efficient ab initio electronic structure\ncalculations in superionic crystals to elucidate their compelling high\nfigure-of-merit.",
        "The first Euclid Quick Data Release, Q1, comprises 63.1 sq deg of the Euclid\nDeep Fields (EDFs) to nominal wide-survey depth. It encompasses visible and\nnear-infrared space-based imaging and spectroscopic data, ground-based\nphotometry in the u, g, r, i and z bands, as well as corresponding masks.\nOverall, Q1 contains about 30 million objects in three areas near the ecliptic\npoles around the EDF-North and EDF-South, as well as the EDF-Fornax field in\nthe constellation of the same name. The purpose of this data release -- and its\nassociated technical papers -- is twofold. First, it is meant to inform the\ncommunity of the enormous potential of the Euclid survey data, to describe what\nis contained in these data, and to help prepare expectations for the\nforthcoming first major data release DR1. Second, it enables a wide range of\ninitial scientific projects with wide-survey Euclid data, ranging from the\nearly Universe to the Solar System. The Q1 data were processed with early\nversions of the processing pipelines, which already demonstrate good\nperformance, with numerous improvements in implementation compared to\npre-launch development. In this paper, we describe the sky areas released in\nQ1, the observations, a top-level view of the data processing of Euclid and\nassociated external data, the Q1 photometric masks, and how to access the data.\nWe also give an overview of initial scientific results obtained using the Q1\ndata set by Euclid Consortium scientists, and conclude with important caveats\nwhen using the data. As a complementary product, Q1 also contains observations\nof a star-forming area in Lynd's Dark Nebula 1641 in the Orion~A Cloud,\nobserved for technical purposes during Euclid's performance-verification phase.\nThis is a unique target, of a type not commonly found in Euclid's nominal sky\nsurvey.",
        "Let $p$ be a prime number. A longstanding conjecture asserts that every\nfinite non-abelian $p$-group has a non-inner automorphism of order $p$. In this\npaper, we prove that the conjecture is true when a finite non-abelian $p$-group\n$G$ has a non-trivial abelian direct factor. Moreover, we prove that the\nnon-inner automorphism is central and fixes $\\Phi(G)$ elementwise. As a\nconsequence, we prove that every group which is not purely non-abelian has a\nnon-inner central automorphism of order $p$ which fixes $\\Phi(G)$ elementwise.",
        "The primary objective of helmet design continues to be the prevention of\ntraumatic brain injuries. Yet, achieving an optimal user experience, including\naspects such as fit, thermal comfort, breathability, waterproofing, and\nreusability, is increasingly significant. Thus, designing helmets with\nmultifunctional performance represents the latest technological frontier for\nsafety devices. This study draws inspiration from the morphology of\nCoscinodiscus species diatoms to develop a biomimetic material replicating\ntheir cellular structure and multifunctionality. Unlike its biological\ncounterpart, the synthetic material is engineered as the inner liner for multi\nimpact helmets, suited for urban sports and micro mobility applications. The\narchitecture of the material is modeled using computer aided design tools, and\nits energy absorption capabilities are analyzed through finite element modeling\nand quasi static compression tests on 3D printed elastomeric samples.\nPerformance optimization is achieved through a parametric approach. The results\ndemonstrate that the material exhibits energy absorption comparable to cellular\nmaterials like honeycombs, while offering lightweight properties,\nbreathability, and resistance to atmospheric agents. This biomimetic design\nmarks a significant advancement in high performance safety equipment.",
        "Osseointegration is crucial to the success of biomedical implants. Additive\nmanufacturing of implants offers a high degree of design freedom, enabling\nprecise control over implant geometry and material composition. Bioactive glass\n(BG) can substantially enhance bone binding and bioactivity; however, limited\nresearch has been conducted on its incorporation into additively manufactured\nimplants. The performance of BG varies depending on the incorporation method,\nand the spatial and temporal evolution of its integration remains unclear. In\nthis study, we synthesized Ti-6Al-4V\/58S BG composites by using the selective\nlaser melting method and systematically compared the effects of BG coating and\ndoping in additively manufactured implants. In vivo histological results from\nanimal tests were statistically analyzed and discussed in terms of\nosseointegration over 4- and 12-week periods. Bone-to-implant contact (BIC) and\nbone density (BD) were used as quantitative metrics to evaluate interactions\nbetween the implants and surrounding bone. Our findings indicate that both\nBG-doped and BG-coated implants accelerated bone ingrowth during the early\nstages of healing. BG-coated implants demonstrated a greater improvement than\ndid pure 3D-printed Ti-6Al-4V implants. However, the effects of BG became\nnonsignificant during the later healing stage (12 weeks). This study provides a\nfoundation for systematically investigating BG incorporation methods in\n3D-printed biomedical implants and their effect on osseointegration.",
        "We investigate Cayley graphs of graph products by showing that graph products\nwith vertex groups that have isomorphic Cayley graphs yield isomorphic Cayley\ngraphs.",
        "We perform analysis of the small x non-linear evolution equation formulated\nin momentum space supplemented by higher order terms. The equation is defined\nin wide range of transverse momentum and longitudinal momentum fraction\nextending previous studies performed in \\cite{Kutak:2003bd,Kutak:2004ym}. The\nlinear part of the equation is motivated by the renormalization group improved\nsmall x approach which accounts for resummation of higher orders, and includes\ncollinear splitting function and kinematical constraint. The solution to the\nequation is then used to perform the fit to Deep Inelastic Scattering reduced\ncross section data.",
        "Precise, unbiased measurements of extragalactic background anisotropies\nrequire careful treatment of systematic effects in fluctuation-based,\nbroad-band intensity mapping measurements. In this paper we detail improvements\nin methodology for the Cosmic Infrared Background ExpeRiment (CIBER),\nconcentrating on flat field errors and source masking errors. In order to\nbypass the use of field differences, which mitigate flat field errors but\nreduce sensitivity, we characterize and correct for the flat field on\npseudo-power spectra, which includes both additive and multiplicative biases.\nTo more effectively mask point sources at 1.1 $\\mu$m and 1.8 $\\mu$m, we develop\na technique for predicting masking catalogs that utilizes optical and NIR\nphotometry through random forest regression. This allows us to mask over two\nVega magnitudes deeper than the completeness limits of 2MASS alone, with errors\nin the shot noise power remaining below $<10\\%$ at all masking depths\nconsidered. Through detailed simulations of CIBER observations, we validate our\nformalism and demonstrate unbiased recovery of the sky fluctuations on\nrealistic mocks. We demonstrate that residual flat field errors comprise\n$<20\\%$ of the final CIBER power spectrum uncertainty with this methodology.",
        "The spike protein (SP) of SARS-CoV-2 is the major molecular target for making\ndiagnostic tests, vaccines, and therapeutic development. We used a combination\nof transmission electron microscopy (TEM) and surface enhanced Raman microscopy\n(SERS) to study its structure. Using SERS on an aluminum substrate, we were\nable to detect a characteristic spectrum of SP mostly due to vibration of three\naromatic amino acids producing Raman shifts at 466 cm-1, 524 cm-1, 773 cm-1,\n831 cm-1, 1048 cm-1, 1308 cm-1, 1457 cm-1, and 1610 cm-1. Transmission Electron\nMicroscopy (TEM) of the SP showed periodic 2D-lattice orientation. The findings\nfrom this study have translational values for developing surface-enhanced Raman\nspectroscopy (SERS) based detectors for screening and testing SARS-CoV-2\nsignatures in diagnostic settings and contamination tracking.",
        "This paper focuses on periodicity and perfect state transfer of Grover walks\non two well-known families of Cayley graphs, namely, the unitary Cayley graphs\nand the quadratic unitary Cayley graphs. Let $R$ be a finite commutative ring.\nThe unitary Cayley graph $G_R$ has vertex set $R$, where two vertices $u$ and\n$v$ are adjacent if $u-v$ is a unit in $R$. We provide a necessary and\nsufficient condition for the periodicity of the Cayley graph $G_R$. We also\ncompletely determine the rings $R$ for which $G_R$ exhibits perfect state\ntransfer. The quadratic unitary Cayley graph $\\mathcal{G}_R$ has vertex set\n$R$, where two vertices $u$ and $v$ are adjacent if $u-v$ or $v-u$ is a square\nof some units in $R$. It is well known that any finite commutative ring $R$ can\nbe expressed as $R_1\\times\\cdots\\times R_s$, where each $R_i$ is a local ring\nwith maximal ideal $M_i$ for $i\\in\\{1,...,s\\}$. We characterize periodicity and\nperfect state transfer on $\\mathcal{G}_R$ under the condition that\n$|R_i|\/|M_i|\\equiv 1 \\pmod 4$ for $i\\in\\{1,...,s\\}$. Also, we characterize\nperiodicity and perfect state transfer on $\\mathcal{G}_R$, where $R$ can be\nexpressed as $R_0\\times\\cdots\\times R_s$ such that $|R_0|\/|M_0|\\equiv3\\pmod 4$,\nand $|R_i|\/|M_i|\\equiv1\\pmod4$ for $i\\in\\{1,..., s\\}$, where $R_i$ is a local\nring with maximal ideal $M_i$ for $i\\in\\{0,...,s\\}$.",
        "The upgraded Large Hadron Collider beauty (LHCb) experiment is the first\ndetector based at a hadron collider using a fully software based trigger. The\nfirst `High Level Trigger' stage (HLT1) reduces the event rate from 30 MHz to\napproximately 1 MHz based on reconstruction criteria from the tracking system\nand consists of O(100) trigger selections implemented on GPUs. These selections\nare further refined following the full offline-quality reconstruction at the\nsecond stage (HLT2) prior to saving for analysis. An automated bandwidth\ndivision has been performed to equitably divide this 1 MHz output rate between\nthe signals of interest to the LHCb physics program. This was achieved by\noptimising a set of trigger selections that maximise efficiency for signals of\ninterest to LHCb while keeping the total HLT1 readout capped to a maximum. The\nbandwidth division tool has been used to determine the optimal selection for 35\nselection algorithms over 80 characteristic physics channels.",
        "Intelligent vehicular communication with vehicle road collaboration\ncapability is a key technology enabled by 6G, and the integration of various\nvisual sensors on vehicles and infrastructures plays a crucial role. Moreover,\naccurate channel prediction is foundational to realizing intelligent vehicular\ncommunication. Traditional methods are still limited by the inability to\nbalance accuracy and operability based on substantial spectrum resource\nconsumption and highly refined description of environment. Therefore,\nleveraging out-of-band information introduced by visual sensors provides a new\nsolution and is increasingly applied across various communication tasks. In\nthis paper, we propose a computer vision (CV)-based prediction model for\nvehicular communications, realizing accurate channel characterization\nprediction including path loss, Rice K-factor and delay spread based on image\nsegmentation. First, we conduct extensive vehicle-to-infrastructure measurement\ncampaigns, collecting channel and visual data from various street intersection\nscenarios. The image-channel dataset is generated after a series of data\npost-processing steps. Image data consists of individual segmentation of target\nuser using YOLOv8 network. Subsequently, established dataset is used to train\nand test prediction network ResNet-32, where segmented images serve as input of\nnetwork, and various channel characteristics are treated as labels or target\noutputs of network. Finally, self-validation and cross-validation experiments\nare performed. The results indicate that models trained with segmented images\nachieve high prediction accuracy and remarkable generalization performance\nacross different streets and target users. The model proposed in this paper\noffers novel solutions for achieving intelligent channel\n  prediction in vehicular communications.",
        "We prove dynamical coherence for partial hyperbolic symplectomorphism in\ndimension 4 whose stable and unstable bundles are C^1.",
        "In the early 21st century, the open data movement began to transform\nsocieties and governments by promoting transparency, innovation, and public\nengagement. The City of New York (NYC) has been at the forefront of this\nmovement since the enactment of the Open Data Law in 2012, creating the NYC\nOpen Data portal. The portal currently hosts 2,700 datasets, serving as a\ncrucial resource for research across various domains, including health, urban\ndevelopment, and transportation. However, the effective use of open data relies\nheavily on data quality and usability, challenges that remain insufficiently\naddressed in the literature. This paper examines these challenges via a case\nstudy of the NYC 311 Service Request dataset, identifying key issues in data\nvalidity, consistency, and curation efficiency. We propose a set of data\ncuration principles, tailored for government-released open data, to address\nthese challenges. Our findings highlight the importance of harmonized field\ndefinitions, streamlined storage, and automated quality checks, offering\npractical guidelines for improving the reliability and utility of open\ndatasets.",
        "Quantum knot invariants (like colored HOMFLY-PT or Kauffman polynomials) are\na distinguished class of non-perturbative topological invariants. Any known way\nto construct them (via Chern-Simons theory or quantum R-matrix) starts with a\nfinite simple Lie algebra. Another set of knot invariants - of finite type - is\nrelated to quantum invariants via a perturbative expansion. However can all\nfinite type invariants be obtained in this way? Investigating this problem, P.\nVogel discovered a way to polynomially parameterize the expansion coefficients\nwith three parameters so that, at different specific values, this reproduces\nthe answers for all simple Lie (super)algebras. Then it is easy to construct a\npolynomial $P_{alg}$ that vanishes for all simple Lie algebras, and the\ncorresponding Vassiliev invariant would thus be absent from the perturbative\nexpansion.\n  We review these Vogel claims pointing out at least two interesting\nimplications of his construction. First, we discuss whether\ninfinite-dimensional Lie algebras might enlarge Chern-Simons theory. Second,\nVogel's construction implies an alternative axiomatization of simple Lie\nalgebras - when we start from knot invariants and arrive at Lie algebras and\ntheir classification, which is opposite to conventional logic that we mentioned\nat the beginning.",
        "Due to limited possibilities of experimental investigations for\nnon-equilibrium gas flows, numerical results are of highest interest. Although\nthe well-established Direct Simulation Monte Carlo (DSMC) method achieves\nhighly accurate solutions, the computational requirements increase excessively\nfor lower Knudsen regimes. Computationally more efficient simulations can be\nachieved with stochastic continuum-based methods using either the\nBhatnagar-Gross-Krook (BGK) or the Fokker-Planck (FP) approximations where,\ninstead of particle collisions, particle relaxation processes are considered.\nThis paper explains the implementation of different stochastic BGK and FP\nmethods in the open-source particle code PICLas for multi-species molecular gas\nflows. For verification, the results of different test cases are compared.",
        "Exact solutions describing multiple, electrically charged black holes (BHs)\nin a model of nonlinear electrodynamics (NLE) minimally coupled to Einstein's\ngravity are presented. The NLE model is ModMax theory, that has attracted much\nattention due to its duality and conformal invariance, features shared with\nstandard (linear) electrodynamics. In the nonextremal case, the solution has\nconical singularities, similarly to the multi Reissner-Nordstr\\\"om solution in\nEinstein-Maxwell theory. In the extremal case the solution is regular on and\noutside the event horizon; it is isometric to the Majumdar-Papapetrou solution,\nalthough the individual BHs have a nonunitary charge to mass ratio, due to\nscreening effects. Using the ModMax electromagnetic duality invariance,\nmagnetically charged and dyonic generalizations are also obtained. Finally, we\nconstruct multi-BH solutions with a positive cosmological constant.",
        "The cosmic ray veto detector for the Mu2e experiment consists of\nscintillation bars embedded with wavelength-shifting fibers and read out by\nsilicon photomultipliers (SiPMs). In this manuscript the calibration procedures\nof the SiPMs are described including corrections for the temperature dependence\nof their light yield. These corrections are needed as the SiPMs are not kept at\na constant temperature due to the complexity and cost of implementing a cooling\nsystem on such a large detector. Rather, it was decided to monitor the\ntemperature to allow the appropriate corrections to be made. The SiPM\ntemperature dependence has been measured in a dedicated experiment and the\ncalibration procedures were validated with data from production detectors\nawaiting installation at Fermilab.",
        "The discovery of solar neutrinos confirmed that the inner workings of the Sun\ngenerally match our theoretical understanding of the fusion process. Solar\nneutrinos have also played a role in discovering that neutrinos have mass and\nthat they oscillate. We combine the latest solar neutrino data along with other\noscillation data from reactors to determine the Sun's density profile. We\nderive constraints given the current data and show the anticipated improvements\nwith more reactor neutrino data from JUNO constraining the true oscillation\nparameters and more solar neutrino data from DUNE which should provide a\ncrucial measurement of $hep$ neutrinos.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "We investigate the asymptotic behavior of the\n$\\mathrm{SU}(2)$-Yang-Mills-Higgs energy $E(\\Phi,A)=\\int_M|d_A\\Phi|^2+|F_A|^2$\nin the large mass limit, proving convergence to the codimension-three area\nfunctional in the sense of De Giorgi's $\\Gamma$-convergence. More precisely,\nfor a compact manifold with boundary $M$ and any family of pairs\n$\\Phi_m\\in\\Omega^0(M;\\mathfrak{su}(2))$ and $A_m\\in\n\\Omega^1(M;\\mathfrak{su}(2))$ indexed by a mass parameter $m\\to\\infty$,\nsatisfying $$E(\\Phi_m,A_m)\\leq\nCm\\quad\\text{and}\\quad\\lim_{m\\to\\infty}\\frac{1}{m}\\int_M(m-|\\Phi_m|)^2=0,$$ we\nprove that the $(n-3)$-currents dual to $\\frac{1}{2\\pi\nm}\\mathrm{tr}(d_{A_m}\\Phi_m\\wedge F_{A_m})$ converge subsequentially to a\nrelative integral $(n-3)$-cycle $T$ of mass \\begin{equation}\n  \\mathbb{M}(T)\\leq \\liminf_{m\\to\\infty}\\frac{1}{4\\pi m}E(\\Phi_m,A_m),\n\\end{equation} and show conversely that any integral $(n-3)$-current $T$ with\n$[T]=0\\in H_{n-3}(M,\\partial M;\\mathbb{Z})$ admits such an approximation, with\nequality in the above inequality. In the special case of pairs $(\\Phi_m,A_m)$\nsatisfying the generalized monopole equation $*d_{A_m}\\Phi_m=F_{A_m}\\wedge\n\\Theta$ for a calibration form $\\Theta\\in \\Omega^{n-3}(M)$, we deduce that the\nlimit $\\nu=\\lim_{m\\to\\infty}\\frac{1}{2\\pi m}|d_{A_m}\\Phi_m|^2$ of the Dirichlet\nenergy measures satisfies $\\nu\\leq |T|$, with equality if and only if $T$ is\ncalibrated by $\\Theta$, giving evidence for predictions of Donaldson-Segal in\nthe settings of $G_2$-manifolds and Calabi-Yau $3$-folds.",
        "Modern science emerged from reasoning over repeatedly-observed planetary\nmotions. We present Gravity-Bench-v1, an environment-based benchmark that\nchallenges AI agents on tasks that parallel this historical development.\nGravity-Bench-v1 evaluates agents on the discovery of physics concealed within\na dynamic environment, using rigorous gravitational dynamics simulations.\nGravity-Bench includes out-of-distribution cases, i.e. with physics that\ndeviates from the real world, to evaluate true scientific generalization\ncapabilities. Agents must plan to collect data within an experimental budget\nand must perform a dynamic form of data analysis and reasoning to solve tasks\nefficiently. Our benchmark admits an open-ended space of solutions. PhD-level\nsolutions for each task are provided, to calibrate AI performance against human\nexpertise. Technically at an upper-undergraduate level, our benchmark proves\nchallenging to baseline AI agents. Gravity-Bench-v1 and planned extensions\nshould help map out AI progress towards scientific discovery capabilities.",
        "This paper finishes the series of two papers that we started with\n[arXiv:2405.05377], where we analyzed the transverse expansion of the metric at\na general null hypersurface. While [arXiv:2405.05377] focused on uniqueness\nresults, here we show existence of ambient manifolds given the full asymptotic\nexpansion at the null hypersurface. When such expansion fulfills a set of\n\"constraint equations\" we prove that the ambient manifold solves the Einstein\nequations to infinite order at the hypersurface. Our approach does not make any\nassumptions regarding the dimension or topology of the null hypersurface and is\nentirely covariant. Furthermore, when the hypersurface exhibits a product\ntopology we find the minimum amount of data on a cross-section that ensures the\nexistence of an ambient space solving the Einstein equations to infinite order\non the hypersurface. As an application we recall the notion of abstract Killing\nhorizon data (AKH) introduced in [arXiv:2405.05377], namely the minimal data\nneeded to define a non-degenerate Killing horizon from a detached viewpoint,\nand we prove that every AKH of arbitrary dimension and topology gives rise to\nan ambient space solving the {\\Lambda}-vacuum equations to infinite order and\nwith the given data as Killing horizon. Our result also includes the\npossibility of the Killing vector having zeroes at the horizon.",
        "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
        "This paper investigates a Stochastic Partial Differential Equation (SPDE)\nderived from the Fokker-Planck equation associated with Score-based Generative\nModels. We modify the standard Fokker-Planck equation to better represent\npractical SGMs and introduce noise to mitigate potential discretization issues.\nThe primary goal is to prove the existence and uniqueness of solutions for this\nSPDE. This aspect requires careful consideration due to the time-dependent\noperator and unbounded domain. To overcome these hurdles, we employ a\nvariational approach and introduce a novel space inspired by Ornstein-Uhlenbeck\noperators. By demonstrating that this space and its subspace satisfy the\nnecessary assumptions, they establish the existence of a solution for the given\nSPDE.",
        "Point spread function (PSF) engineering has been pivotal in the remarkable\nprogress made in high-resolution imaging in the last decades. However, the\ndiversity in PSF structures attainable through existing engineering methods is\nlimited. Here, we report universal PSF engineering, demonstrating a method to\nsynthesize an arbitrary set of spatially varying 3D PSFs between the input and\noutput volumes of a spatially incoherent diffractive processor composed of\ncascaded transmissive surfaces. We rigorously analyze the PSF engineering\ncapabilities of such diffractive processors within the diffraction limit of\nlight and provide numerical demonstrations of unique imaging capabilities, such\nas snapshot 3D multispectral imaging without involving any spectral filters,\naxial scanning or digital reconstruction steps, which is enabled by the spatial\nand spectral engineering of 3D PSFs. Our framework and analysis would be\nimportant for future advancements in computational imaging, sensing and\ndiffractive processing of 3D optical information.",
        "In this work we find a unifying scheme for the known explicit complex-valued\neigenfunctions on the classical compact Riemannian symmetric spaces. For this\nwe employ the well-known Cartan embedding for those spaces. This also leads to\nthe construction of new eigenfunctions on the quaternionic Grassmannians.",
        "In 1990 Kantor introduced the conservative algebra $\\mathcal{W}(n)$ of all\nalgebras (i.e. bilinear maps) on the $n$-dimensional vector space. In case $n\n>1$ the algebra $\\mathcal{W}(n)$ does not belong to well known classes of\nalgebras (such as associative, Lie, Jordan, Leibniz algebras). We describe\n$\\frac{1}{2}$derivations, local (resp. $2$-local) $\\frac{1}{2}$-derivations and\nbiderivations of $\\mathcal{W}(2)$. We also study similar problems for the\nalgebra $\\mathcal{W}_2$ of all commutative algebras on the two-dimensional\nvector space and the algebra $\\mathcal{S}_2$ of all commutative algebras with\ntrace zero multiplication on the two-dimensional space.",
        "We propose multiplier bootstrap procedures for nonparametric inference and\nuncertainty quantification of the target mean function, based on a novel\nframework of integrating target and source data. We begin with the relatively\neasier covariate shift scenario with equal target and source mean functions and\npropose estimation and inferential procedures through a straightforward\ncombination of all target and source datasets. We next consider the more\ngeneral and flexible distribution shift scenario with arbitrary target and\nsource mean functions, and propose a two-step inferential procedure. First, we\nestimate the target-to-source differences based on separate portions of the\ntarget and source data. Second, the remaining source data are adjusted by these\ndifferences and combined with the remaining target data to perform the\nmultiplier bootstrap procedure. Our method enables local and global inference\non the target mean function without using asymptotic distributions. To justify\nour approach, we derive an optimal convergence rate for the nonparametric\nestimator and establish bootstrap consistency to estimate the asymptotic\ndistribution of the nonparametric estimator. The proof of global bootstrap\nconsistency involves a central limit theorem for quadratic forms with dependent\nvariables under a conditional probability measure. Our method applies to\narbitrary source and target datasets, provided that the data sizes meet a\nspecific quantitative relationship. Simulation studies and real data analysis\nare provided to examine the performance of our approach."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Analysis methods for numerical weather prediction",
    "start_abstract":"Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
      ],
      "abstract":[
        "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Heavy-tailed random vectros: theory and applications",
        "QuESat: Satellite-Assisted Quantum Internet for Global-Scale\n  Entanglement Distribution",
        "Cheap Permutation Testing",
        "Efficient stochastic simulation of piecewise-deterministic Markov\n  processes and its application to the Morris-Lecar model of neural dynamics",
        "acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI\n  Models on Edge Devices",
        "PolyhedronNet: Representation Learning for Polyhedra with\n  Surface-attributed Graph",
        "Splitting CEGM Amplitudes",
        "Using Covid-19 Response Policy to Estimate Open Water Swim Drafting\n  Effects in Triathlon",
        "Is Bellman Equation Enough for Learning Control?",
        "GPDFlow: Generative Multivariate Threshold Exceedance Modeling via\n  Normalizing Flows",
        "Non-polynomial conserved quantities for ODE systems and its application\n  to the long-time behavior of solutions to cubic NLS systems",
        "Representation in large language models",
        "LongReason: A Synthetic Long-Context Reasoning Benchmark via Context\n  Expansion",
        "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge",
        "Positive self-commutators of positive operators",
        "Learning-based visibility prediction for terahertz communications in 6G\n  networks",
        "Memristor-Based Meta-Learning for Fast mmWave Beam Prediction in\n  Non-Stationary Environments",
        "DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person\n  Retrieval",
        "Do computer vision foundation models learn the low-level characteristics\n  of the human visual system?",
        "Deterministic or probabilistic? The psychology of LLMs as random number\n  generators",
        "A solution to Haagerup's problem and positive Hahn-Banach separation\n  theorems in operator algebras",
        "LLM-Pack: Intuitive Grocery Handling for Logistics Applications",
        "The Quest for Visual Understanding: A Journey Through the Evolution of\n  Visual Question Answering",
        "Interview with Hyman Bass",
        "Semicomplete multipartite weakly distance-regular digraphs",
        "Large Capacity Data Hiding in Binary Image black and white mixed regions",
        "Self-Supervised Prompt Optimization",
        "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
        "Deep Reinforcement Learning with Hybrid Intrinsic Reward Model"
      ],
      "abstract":[
        "In this paper we introduce and study several multivariate, heavy-tailed\ndistribution classes, and we explore their closure properties and their\napplications. We consider the class of multivariate, positively decreasing\ndistributions, and its intersection with other multivariate distribution\nclasses.",
        "Entanglement distribution across remote distances is critical for many\nquantum applications. Currently, the de facto approach for remote entanglement\ndistribution relies on optical fiber for on-the-ground entanglement\ndistribution. However, the fiber-based approach is incapable of global-scale\nentanglement distribution due to intrinsic limitations. This paper investigates\na new hybrid ground-satellite quantum network architecture (QuESat) for\nglobal-scale entanglement distribution, integrating an on-the-ground fiber\nnetwork with a global-scale passive optical network built with low-Earth-orbit\nsatellites. The satellite network provides dynamic construction of photon\nlightpaths based on near-vacuum beam guides constructed via adjustable arrays\nof lenses, forwarding photons from one ground station to another with very high\nefficiency over long distances compared to using fiber. To assess the\nfeasibility and effectiveness of QuESat for global communication, we formulate\nlightpath provisioning and entanglement distribution problems, considering the\norbital dynamics of satellites and the time-varying entanglement demands from\nground users. A two-stage algorithm is developed to dynamically configure the\nbeam guides and distribute entanglements, respectively. The algorithm combines\nrandomized and deterministic rounding for lightpath provisioning to enable\nglobal connectivity, with optimal entanglement swapping for distributing\nentanglements to meet users' demands. By developing a ground-satellite quantum\nnetwork simulator, QuESat achieves multi-fold improvements compared to repeater\nnetworks.",
        "Permutation tests are a popular choice for distinguishing distributions and\ntesting independence, due to their exact, finite-sample control of false\npositives and their minimax optimality when paired with U-statistics. However,\nstandard permutation tests are also expensive, requiring a test statistic to be\ncomputed hundreds or thousands of times to detect a separation between\ndistributions. In this work, we offer a simple approach to accelerate testing:\ngroup your datapoints into bins and permute only those bins. For U and\nV-statistics, we prove that these cheap permutation tests have two remarkable\nproperties. First, by storing appropriate sufficient statistics, a cheap test\ncan be run in time comparable to evaluating a single test statistic. Second,\ncheap permutation power closely approximates standard permutation power. As a\nresult, cheap tests inherit the exact false positive control and minimax\noptimality of standard permutation tests while running in a fraction of the\ntime. We complement these findings with improved power guarantees for standard\npermutation testing and experiments demonstrating the benefits of cheap\npermutations over standard maximum mean discrepancy (MMD), Hilbert-Schmidt\nindependence criterion (HSIC), random Fourier feature, Wilcoxon-Mann-Whitney,\ncross-MMD, and cross-HSIC tests.",
        "Piecewise-deterministic Markov processes combine continuous in time dynamics\nwith jump events, the rates of which generally depend on the continuous\nvariables and thus are not constants. This leads to a problem in a Monte-Carlo\nsimulation of such a system, where, at each step, one must find the time\ninstant of the next event. The latter is determined by an integral equation and\nusually is rather slow in numerical implementation. We suggest a reformulation\nof the next event problem as an ordinary differential equation where the\nindependent variable is not the time but the cumulative rate. This\nreformulation is similar to the H\\'enon approach to efficiently constructing\nthe Poincar\\'e map in deterministic dynamics. The problem is then reduced to a\nstandard numerical task of solving a system of ordinary differential equations\nwith given initial conditions on a prescribed interval. We illustrate the\nmethod with a stochastic Morris-Lecar model of neuron spiking with\nstochasticity in the opening and closing of voltage-gated ion channels.",
        "1. Passive acoustic monitoring (PAM) coupled with artificial intelligence\n(AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM\nsystems require manual data offloading and impose substantial demands on\nstorage and computing infrastructure. The combination of on-device AI-based\nprocessing and network connectivity enables local data analysis and\ntransmission of only relevant information, greatly reducing storage needs.\nHowever, programming these devices for robust operation is challenging,\nrequiring expertise in embedded systems and software engineering. Despite the\nincrease in AI-based models for bioacoustics, their full potential remains\nunrealized without accessible tools to deploy them on custom hardware and\ntailor device behaviour to specific monitoring goals. 2. To address this\nchallenge, we develop acoupi, an open-source Python framework that simplifies\nthe creation and deployment of smart bioacoustic devices. acoupi integrates\naudio recording, AI-based data processing, data management, and real-time\nwireless messaging into a unified and configurable framework. By modularising\nkey elements of the bioacoustic monitoring workflow, acoupi allows users to\neasily customise, extend, or select specific components to fit their unique\nmonitoring needs. 3. We demonstrate the flexibility of acoupi by integrating\ntwo bioacoustic classifiers: BirdNET, for the classification of bird species,\nand BatDetect2, for the classification of UK bat species. We test the\nreliability of acoupi over a month-long deployment of two acoupi-powered\ndevices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such\nas the Raspberry Pi and can be customised for various applications. acoupi\nstandardised framework and simplified tools facilitate the adoption of\nAI-powered PAM systems for researchers and conservationists. acoupi is on\nGitHub at https:\/\/github.com\/acoupi\/acoupi.",
        "Ubiquitous geometric objects can be precisely and efficiently represented as\npolyhedra. The transformation of a polyhedron into a vector, known as polyhedra\nrepresentation learning, is crucial for manipulating these shapes with\nmathematical and statistical tools for tasks like classification, clustering,\nand generation. Recent years have witnessed significant strides in this domain,\nyet most efforts focus on the vertex sequence of a polyhedron, neglecting the\ncomplex surface modeling crucial in real-world polyhedral objects. This study\nproposes \\textbf{PolyhedronNet}, a general framework tailored for learning\nrepresentations of 3D polyhedral objects. We propose the concept of the\nsurface-attributed graph to seamlessly model the vertices, edges, faces, and\ntheir geometric interrelationships within a polyhedron. To effectively learn\nthe representation of the entire surface-attributed graph, we first propose to\nbreak it down into local rigid representations to effectively learn each local\nregion's relative positions against the remaining regions without geometric\ninformation loss. Subsequently, we propose PolyhedronGNN to hierarchically\naggregate the local rigid representation via intra-face and inter-face\ngeometric message passing modules, to obtain a global representation that\nminimizes information loss while maintaining rotation and translation\ninvariance. Our experimental evaluations on four distinct datasets,\nencompassing both classification and retrieval tasks, substantiate\nPolyhedronNet's efficacy in capturing comprehensive and informative\nrepresentations of 3D polyhedral objects. Code and data are available at\n{https:\/\/github.com\/dyu62\/3D_polyhedron}.",
        "The CEGM formalism offers a general framework for scattering amplitudes,\nwhich rests on Grassmannians, moduli spaces and tropical geometry. The physical\nimplications of this generalization are still to be understood. Conventional\nwisdom says that key features of scattering amplitudes, like factorization at\ntheir poles into lower-point amplitudes, are associated to their singularities.\nThe factorization behavior of CEGM amplitudes at their poles is interesting but\ncomplicated. Recent developments have revealed important properties of standard\nparticle and string scattering amplitudes from factorizations, known as splits,\nthat happen away from poles. In this paper we introduce a kinematic subspace on\nwhich the CEGM amplitude splits into very simple rational functions. These\nfunctions, called simplex amplitudes, arise from stringy integrals for the\nmultivariate beta function, and also from restricting the biadjoint scalar\namplitude in quantum field theory to certain kinematic loci. Using split\nkinematics we also discover a specific class of zeros of the CEGM amplitude.\nOur construction rests on viewing positive moduli space as a product of\nsimplices, and it suggests a novel approach for deriving scattering amplitudes\nfrom tropical determinantal varieties.",
        "This study investigates the causal effects of open-water swim drafting by\nleveraging a natural experiment induced by staggered race starts during the\nCOVID-19 pandemic. Before 2020, athletes started in groups, enabling drafting\nbenefits, while pandemic-related restrictions significantly reduced these\nopportunities. Using agglomerative hierarchical clustering of swim-out times, I\nanalyze optimal drafting positions and estimate their impact on Swim-Out\nperformance. Our empirical findings reveal that swim drafting benefits were\nstatistically insignificant in 2020 but persisted post-pandemic at slightly\nreduced levels. I find that drafting becomes advantageous only from the third\ntrailing position onward, with earlier positions primarily serving to minimize\nfatigue. To mitigate endogeneity, I employ athlete and event fixed effects. The\nseemingly inverse decaying nature of drafting benefits partially addresses some\nconcerns of simultaneous reverse causality and omitted variable bias. This\nstudy provides the first largescale causal estimate of drafting effects in\nreal-world triathlon race settings.",
        "The Bellman equation and its continuous-time counterpart, the\nHamilton-Jacobi-Bellman (HJB) equation, serve as necessary conditions for\noptimality in reinforcement learning and optimal control. While the value\nfunction is known to be the unique solution to the Bellman equation in tabular\nsettings, we demonstrate that this uniqueness fails to hold in continuous state\nspaces. Specifically, for linear dynamical systems, we prove the Bellman\nequation admits at least $\\binom{2n}{n}$ solutions, where $n$ is the state\ndimension. Crucially, only one of these solutions yields both an optimal policy\nand a stable closed-loop system. We then demonstrate a common failure mode in\nvalue-based methods: convergence to unstable solutions due to the exponential\nimbalance between admissible and inadmissible solutions. Finally, we introduce\na positive-definite neural architecture that guarantees convergence to the\nstable solution by construction to address this issue.",
        "The multivariate generalized Pareto distribution (mGPD) is a common method\nfor modeling extreme threshold exceedance probabilities in environmental and\nfinancial risk management. Despite its broad applicability, mGPD faces\nchallenges due to the infinite possible parametrizations of its dependence\nfunction, with only a few parametric models available in practice. To address\nthis limitation, we introduce GPDFlow, an innovative mGPD model that leverages\nnormalizing flows to flexibly represent the dependence structure. Unlike\ntraditional parametric mGPD approaches, GPDFlow does not impose explicit\nparametric assumptions on dependence, resulting in greater flexibility and\nenhanced performance. Additionally, GPDFlow allows direct inference of marginal\nparameters, providing insights into marginal tail behavior. We derive tail\ndependence coefficients for GPDFlow, including a bivariate formulation, a\n$d$-dimensional extension, and an alternative measure for partial exceedance\ndependence. A general relationship between the bivariate tail dependence\ncoefficient and the generative samples from normalizing flows is discussed.\nThrough simulations and a practical application analyzing the risk among five\nmajor US banks, we demonstrate that GPDFlow significantly improves modeling\naccuracy and flexibility compared to traditional parametric methods.",
        "In this paper, we investigate the asymptotic behavior of small solutions to\nthe initial value problem for a system of cubic nonlinear Schrodinger equations\n(NLS) in one spatial dimension. We identify a new class of NLS systems for\nwhich the global boundedness and asymptotics of small solutions can be\nestablished, even in the absence of any effective conserved quantity. The key\nto this analysis lies in utilizing conserved quantities for the reduced\nordinary differential equation (ODE) systems derived from the original NLS\nsystems. In a previous study, the first author investigated conserved\nquantities expressed as quartic polynomials. In contrast, the conserved\nquantities considered in the present paper are of a different type and are not\nnecessarily polynomial.",
        "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
        "Large language models (LLMs) have demonstrated remarkable progress in\nunderstanding long-context inputs. However, benchmarks for evaluating the\nlong-context reasoning abilities of LLMs fall behind the pace. Existing\nbenchmarks often focus on a narrow range of tasks or those that do not demand\ncomplex reasoning. To address this gap and enable a more comprehensive\nevaluation of the long-context reasoning capabilities of current LLMs, we\npropose a new synthetic benchmark, LongReason, which is constructed by\nsynthesizing long-context reasoning questions from a varied set of\nshort-context reasoning questions through context expansion. LongReason\nconsists of 794 multiple-choice reasoning questions with diverse reasoning\npatterns across three task categories: reading comprehension, logical\ninference, and mathematical word problems. We evaluate 21 LLMs on LongReason,\nrevealing that most models experience significant performance drops as context\nlength increases. Our further analysis shows that even state-of-the-art LLMs\nstill have significant room for improvement in providing robust reasoning\nacross different tasks. We have open-sourced LongReason under\nhttps:\/\/huggingface.co\/datasets\/lz1bytedance\/LongReason to support the\ncomprehensive evaluation of LLMs' long-context reasoning capabilities.",
        "This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge\nevaluation regime, where the adversary controls both the candidate and\nevaluator model. The backdoored evaluator victimizes benign users by unfairly\nassigning inflated scores to adversary. A trivial single token backdoor\npoisoning 1% of the evaluator training data triples the adversary's score with\nrespect to their legitimate score. We systematically categorize levels of data\naccess corresponding to three real-world settings, (1) web poisoning, (2)\nmalicious annotator, and (3) weight poisoning. These regimes reflect a weak to\nstrong escalation of data access that highly correlates with attack severity.\nUnder the weakest assumptions - web poisoning (1), the adversary still induces\na 20% score inflation. Likewise, in the (3) weight poisoning regime, the\nstronger assumptions enable the adversary to inflate their scores from 1.5\/5 to\n4.9\/5. The backdoor threat generalizes across different evaluator\narchitectures, trigger designs, evaluation tasks, and poisoning rates. By\npoisoning 10% of the evaluator training data, we control toxicity judges\n(Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and\ndocument reranker judges in RAG to rank the poisoned document first 97% of the\ntime. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and\ntechnology, where social implications of mislead model selection and evaluation\nconstrain the available defensive tools. Amidst these challenges, model merging\nemerges as a principled tool to offset the backdoor, reducing ASR to near 0%\nwhilst maintaining SOTA performance. Model merging's low computational cost and\nconvenient integration into the current LLM Judge training pipeline position it\nas a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.",
        "We consider a positive operator $A$ on a Hilbert lattice such that its\nself-commutator $C = A^* A - A A^*$ is positive. If $A$ is also idempotent,\nthen it is an orthogonal projection, and so $C = 0$. Similarly, if $A$ is power\ncompact, then $C = 0$ as well. We prove that every positive compact central\noperator on a separable infinite-dimensional Hilbert lattice $\\mathcal H$ is a\nself-commutator of a positive operator. We also show that every positive\ncentral operator on $\\mathcal H$ is a sum of two positive self-commutators of\npositive operators.",
        "Terahertz communications are envisioned as a key enabler for 6G networks. The\nabundant spectrum available in such ultra high frequencies has the potential to\nincrease network capacity to huge data rates. However, they are extremely\naffected by blockages, to the point of disrupting ongoing communications. In\nthis paper, we elaborate on the relevance of predicting visibility between\nusers and access points (APs) to improve the performance of THz-based networks\nby minimizing blockages, that is, maximizing network availability, while at the\nsame time keeping a low reconfiguration overhead. We propose a novel approach\nto address this problem, by combining a neural network (NN) for predicting\nfuture user-AP visibility probability, with a probability threshold for AP\nreselection to avoid unnecessary reconfigurations. Our experimental results\ndemonstrate that current state-of-the-art handover mechanisms based on received\nsignal strength are not adequate for THz communications, since they are\nill-suited to handle hard blockages. Our proposed NN-based solution\nsignificantly outperforms them, demonstrating the interest of our strategy as a\nresearch line.",
        "Traditional machine learning techniques have achieved great success in\nimproving data-rate performance and reducing latency in millimeter wave\n(mmWave) communications. However, these methods still face two key challenges:\n(i) their reliance on large-scale paired data for model training and tuning\nwhich limits performance gains and makes beam predictions outdated, especially\nin multi-user mmWave systems with large antenna arrays, and (ii) meta-learning\n(ML)-based beamforming solutions are prone to overfitting when trained on a\nlimited number of tasks. To address these issues, we propose a memristorbased\nmeta-learning (M-ML) framework for predicting mmWave beam in real time. The\nM-ML framework generates optimal initialization parameters during the training\nphase, providing a strong starting point for adapting to unknown environments\nduring the testing phase. By leveraging memory to store key data, M-ML ensures\nthe predicted beamforming vectors are wellsuited to episodically dynamic\nchannel distributions, even when testing and training environments do not\nalign. Simulation results show that our approach delivers high prediction\naccuracy in new environments, without relying on large datasets. Moreover, MML\nenhances the model's generalization ability and adaptability.",
        "Text-based person retrieval (TPR) has gained significant attention as a\nfine-grained and challenging task that closely aligns with practical\napplications. Tailoring CLIP to person domain is now a emerging research topic\ndue to the abundant knowledge of vision-language pretraining, but challenges\nstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is\ncomputationally expensive and prone to overfitting.(ii) Existing\nparameter-efficient transfer learning (PETL) for TPR lacks of fine-grained\nfeature extraction. To address these issues, we propose Domain-Aware\nMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and\nPETL to enhance fine-grained feature representations while maintaining\nefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to\nMLP layers in both vision and language branches, where different experts\nspecialize in distinct aspects of person knowledge to handle features more\nfinely. To promote the router to exploit domain information effectively and\nalleviate the routing imbalance, Domain-Aware Router is then developed by\nbuilding a novel gating function and injecting learnable domain-aware prompts.\nExtensive experiments show that our DM-Adapter achieves state-of-the-art\nperformance, outperforming previous methods by a significant margin.",
        "Computer vision foundation models, such as DINO or OpenCLIP, are trained in a\nself-supervised manner on large image datasets. Analogously, substantial\nevidence suggests that the human visual system (HVS) is influenced by the\nstatistical distribution of colors and patterns in the natural world,\ncharacteristics also present in the training data of foundation models. The\nquestion we address in this paper is whether foundation models trained on\nnatural images mimic some of the low-level characteristics of the human visual\nsystem, such as contrast detection, contrast masking, and contrast constancy.\nSpecifically, we designed a protocol comprising nine test types to evaluate the\nimage encoders of 45 foundation and generative models. Our results indicate\nthat some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some of\nthe characteristics of human vision, but other models show little resemblance.\nFoundation models tend to show smaller sensitivity to low contrast and rather\nirregular responses to contrast across frequencies. The foundation models show\nthe best agreement with human data in terms of contrast masking. Our findings\nsuggest that human vision and computer vision may take both similar and\ndifferent paths when learning to interpret images of the real world. Overall,\nwhile differences remain, foundation models trained on vision tasks start to\nalign with low-level human vision, with DINOv2 showing the closest resemblance.",
        "Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases.",
        "We affirmatively resolve a question posed by Uffe Haagerup in 1975 on the\npositive version of the bipolar theorem on the dual spaces of C$^*$-algebras.\nAs a direct consequence, we obtain a complete set of four positive Hahn-Banach\nseparation theorems on von Neumann algebras, their preduals, C$^*$-algebras,\nand their duals.",
        "Robotics and automation are increasingly influential in logistics but remain\nlargely confined to traditional warehouses. In grocery retail, advancements\nsuch as cashier-less supermarkets exist, yet customers still manually pick and\npack groceries. While there has been a substantial focus in robotics on the bin\npicking problem, the task of packing objects and groceries has remained largely\nuntouched. However, packing grocery items in the right order is crucial for\npreventing product damage, e.g., heavy objects should not be placed on top of\nfragile ones. However, the exact criteria for the right packing order are hard\nto define, in particular given the huge variety of objects typically found in\nstores. In this paper, we introduce LLM-Pack, a novel approach for grocery\npacking. LLM-Pack leverages language and vision foundation models for\nidentifying groceries and generating a packing sequence that mimics human\npacking strategy. LLM-Pack does not require dedicated training to handle new\ngrocery items and its modularity allows easy upgrades of the underlying\nfoundation models. We extensively evaluate our approach to demonstrate its\nperformance. We will make the source code of LLMPack publicly available upon\nthe publication of this manuscript.",
        "Visual Question Answering (VQA) is an interdisciplinary field that bridges\nthe gap between computer vision (CV) and natural language processing(NLP),\nenabling Artificial Intelligence(AI) systems to answer questions about images.\nSince its inception in 2015, VQA has rapidly evolved, driven by advances in\ndeep learning, attention mechanisms, and transformer-based models. This survey\ntraces the journey of VQA from its early days, through major breakthroughs,\nsuch as attention mechanisms, compositional reasoning, and the rise of\nvision-language pre-training methods. We highlight key models, datasets, and\ntechniques that shaped the development of VQA systems, emphasizing the pivotal\nrole of transformer architectures and multimodal pre-training in driving recent\nprogress. Additionally, we explore specialized applications of VQA in domains\nlike healthcare and discuss ongoing challenges, such as dataset bias, model\ninterpretability, and the need for common-sense reasoning. Lastly, we discuss\nthe emerging trends in large multimodal language models and the integration of\nexternal knowledge, offering insights into the future directions of VQA. This\npaper aims to provide a comprehensive overview of the evolution of VQA,\nhighlighting both its current state and potential advancements.",
        "Interview with Hyman Bass, whose mathematical life has spanned seven decades.",
        "A digraph is semicomplete multipartite if its underlying graph is a complete\nmultipartite graph. As a special case of semicomplete multipartite digraphs,\nJ{\\o}rgensen et al. \\cite{JG14} initiated the study of doubly regular team\ntournaments. As a natural extension, we introduce doubly regular team\nsemicomplete multipartite digraphs and show that such digraphs fall into three\ntypes. Furthermore, we give a characterization of all semicomplete multipartite\ncommutative weakly distance-regular digraphs.",
        "Information hiding technology utilizes the insensitivity of human sensory\norgans to redundant data, hiding confidential information in the redundant data\nof these public digital media, and then transmitting it. The carrier media\nafter hiding secret information only displays its own characteristics, which\ncan ensure the transmission of confidential information without being detected,\nthereby greatly improving the security of the information. In theory, any\ndigital media including image, video, audio, and text can serve as a host\ncarrier. Among them, hiding information in binary images poses great\nchallenges. As we know, any information hiding method involves modifying the\ndata of the host carrier. The more information hidden, the more data of the\nhost carrier are modified. In this paper, we propose information hiding in the\nblack-and-white mixed region of binary images, which can greatly reduce visual\ndistortion. In addition, we propose an efficient encoding to achieve\nhigh-capacity information hiding while ensuring image semantics. By selecting\nbinary images of different themes, we conduct experiments. The experimental\nresults prove the feasibility of our technique and verify the expected\nperformance. Since the candidate units for information hiding are selected from\nequally sized blocks that the image is divided into, and the hiding and\nextraction of information are based on a shared encoding table, the\ncomputational cost is very low, making it suitable for real-time information\nhiding applications.",
        "Well-designed prompts are crucial for enhancing Large language models' (LLMs)\nreasoning capabilities while aligning their outputs with task requirements\nacross diverse domains. However, manually designed prompts require expertise\nand iterative experimentation. While existing prompt optimization methods aim\nto automate this process, they rely heavily on external references such as\nground truth or by humans, limiting their applicability in real-world scenarios\nwhere such data is unavailable or costly to obtain. To address this, we propose\nSelf-Supervised Prompt Optimization (SPO), a cost-efficient framework that\ndiscovers effective prompts for both closed and open-ended tasks without\nrequiring external reference. Motivated by the observations that prompt quality\nmanifests directly in LLM outputs and LLMs can effectively assess adherence to\ntask requirements, we derive evaluation and optimization signals purely from\noutput comparisons. Specifically, SPO selects superior prompts through pairwise\noutput comparisons evaluated by an LLM evaluator, followed by an LLM optimizer\nthat aligns outputs with task requirements. Extensive experiments demonstrate\nthat SPO outperforms state-of-the-art prompt optimization methods, achieving\ncomparable or superior results with significantly lower costs (e.g., 1.1% to\n5.6% of existing methods) and fewer samples (e.g., three samples). The code is\navailable at https:\/\/github.com\/geekan\/MetaGPT\/blob\/main\/examples\/spo",
        "Autoregressive sequence models, such as Transformer-based vision-language\naction (VLA) policies, can be tremendously effective for capturing complex and\ngeneralizable robotic behaviors. However, such models require us to choose a\ntokenization of our continuous action signals, which determines how the\ndiscrete symbols predicted by the model map to continuous robot actions. We\nfind that current approaches for robot action tokenization, based on simple\nper-dimension, per-timestep binning schemes, typically perform poorly when\nlearning dexterous skills from high-frequency robot data. To address this\nchallenge, we propose a new compression-based tokenization scheme for robot\nactions, based on the discrete cosine transform. Our tokenization approach,\nFrequency-space Action Sequence Tokenization (FAST), enables us to train\nautoregressive VLAs for highly dexterous and high-frequency tasks where\nstandard discretization methods fail completely. Based on FAST, we release\nFAST+, a universal robot action tokenizer, trained on 1M real robot action\ntrajectories. It can be used as a black-box tokenizer for a wide range of robot\naction sequences, with diverse action spaces and control frequencies. Finally,\nwe show that, when combined with the pi0 VLA, our method can scale to training\non 10k hours of robot data and match the performance of diffusion VLAs, while\nreducing training time by up to 5x.",
        "Intrinsic reward shaping has emerged as a prevalent approach to solving\nhard-exploration and sparse-rewards environments in reinforcement learning\n(RL). While single intrinsic rewards, such as curiosity-driven or novelty-based\nmethods, have shown effectiveness, they often limit the diversity and\nefficiency of exploration. Moreover, the potential and principle of combining\nmultiple intrinsic rewards remains insufficiently explored. To address this\ngap, we introduce HIRE (Hybrid Intrinsic REward), a flexible and elegant\nframework for creating hybrid intrinsic rewards through deliberate fusion\nstrategies. With HIRE, we conduct a systematic analysis of the application of\nhybrid intrinsic rewards in both general and unsupervised RL across multiple\nbenchmarks. Extensive experiments demonstrate that HIRE can significantly\nenhance exploration efficiency and diversity, as well as skill acquisition in\ncomplex and dynamic settings."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"A review of artificial intelligence in prostate cancer detection on imaging",
    "start_abstract":"A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
      ],
      "abstract":[
        "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Chaos in a Nonlinear Wavefunction Model: An Alternative to Born's\n  Probability Hypothesis",
        "Relative Reality",
        "A Study on the Line of Sight to Galaxies Detected at Gamma-ray Energies",
        "Planet formation and long-term stability in a very eccentric stellar\n  binary",
        "Fractional kinetic modelling of the adsorption and desorption process\n  from experimental SPR curves",
        "Noise equals endogenous control",
        "Coupling and Acceleration of Externally Injected Electron Beams in\n  Laser-Driven Plasma Wakefields",
        "$C^1$ Robust Rigidity for Bi-critical Circle Maps",
        "A quantum walk inspired model for distributed computing on arbitrary\n  graphs",
        "The hardcore brokers: Core-periphery structure and political\n  representation in Denmark's corporate elite network",
        "On $L_p$ Brunn-Minkowski type inequalities for a general class of\n  functionals",
        "A class of moving boundary problems with an exponential source term",
        "Scotogenic Froggatt-Nielsen and the Versatility of Soft Symmetry\n  Breaking",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Discretionary vs nondiscretionary in fiscal mechanism. Non-automatic\n  fiscal stabilisers vs automatic fiscal stabilisers",
        "Decomposition results for multiplicative actions and applications",
        "Anomalous Chern-Simons orbital magnetoelectric coupling of\n  three-dimensional Chern insulators: gauge-discontinuity formalism and\n  adiabatic pumping",
        "Controlling the spontaneous emission of trapped ions",
        "Investigating the shadows of new regular black holes with a Minkowski\n  core: Effects of spherical accretion and core type differences",
        "Imaging thick objects with deep-sub-angstrom resolution and\n  deep-sub-picometer precision",
        "Measurement of $\\rm ^{6}H$ ground state energy in an electron scattering\n  experiment at MAMI-A1",
        "The collisionless hydrodynamics: On the nonexistence of collisionless\n  shocks",
        "NeuralFoil: An Airfoil Aerodynamics Analysis Tool Using Physics-Informed\n  Machine Learning",
        "The $M_{*}-M_{\\rm BH}$ Relation Evolution from z $\\sim$ 6 to the Present\n  Epoch",
        "Global Geometry within an SPDE Well-Posedness Problem",
        "Some Kummer extensions over maximal cyclotomic fields, a finiteness\n  theorem of Ribet and TKND-AVKF fields",
        "FOSS solution for Molecular Dynamics Simulation Automation and\n  Collaboration with MDSGAT",
        "A helical magnetic field in quasar NRAO150 revealed by Faraday rotation",
        "Light scalar beyond the Higgs mixing limit"
      ],
      "abstract":[
        "In a prior paper, the author described an instability in a nonlinear\nwavefunction model. Proposed in connection with the Measurement Problem, the\nmodel contained an external potential creating a ``classical'' instability.\nHowever, it is interesting to ask whether such models possess an intrinsic\nrandomness -- even ``chaos\" -- independent of external potentials. In this\nwork, I investigate the criterion analytically and simulate from a small (``3\nqubit\") model, demonstrating that the Lyapunov exponent -- a standard measure\nof ``chaos\" -- is positive. I also extend the instability criterion to models\nin the continuum. These results suggest that the boundary between classical and\nwavefunction physics may also constitute the threshold of chaos, and present an\nalternative to Max Born's ad hoc probability hypothesis: random outcomes in\nexperiments result not from ``wave-particle duality\" or ``the existence of the\nquantum,\" but from sensitive dependence on initial conditions, as is common in\nthe other sciences.",
        "The ``Hard Problem\" of consciousness refers to a long-standing enigma about\nhow qualia emerge from physical processes in the brain. Building on insights\nfrom the development of non-Euclidean geometry, this paper seeks to present a\nstructured and logically coherent theory of qualia to address this problem. The\nproposed theory starts with a definition on what it means for an entity to be\nnon-physical. A postulate about awareness is posed and utilized to rigorously\nprove that qualia are non-physical and thoughts are qualia. Then the paper\nintroduces a key concept: relative reality, meaning that perceptions of reality\nare relative to the observer and time. The concept is analyzed through a\nmathematical model grounded in Hilbert space theory. The model also sheds new\nlight on cognitive science and physics. In particular, the Schr\\\"{o}dinger\nequation can be derived easily through this model. Moreover, this model shows\nthat eigenstates also exist for classical energy-conserving systems. Analyses\non the G. P. Thomson experiment and the classical harmonic oscillator are made\nto illustrate this finding. The insight gained sheds new light on the\nBohr-Einstein debate concerning the interpretation of quantum mechanics. At\nlast, the paper proposes a postulate about qualia force and demonstrates that\nit constitutes a fundamental part of absolute reality, much like the four\nfundamental forces in nature.",
        "The large-scale Universal structure comprises strands of dark matter and\ngalaxies with large under-dense volumes known as voids. We measure the fraction\nof the line of sight that intersects voids for active galactic nuclei (AGN)\ndetected by Fermi Large Area Telescope (LAT) and quasars from the Sloan Digital\nSky Survey (SDSS). This ``voidiness'' fraction is a rudimentary proxy for the\ndensity along the line of sight to the galaxies. The voidiness of SDSS-observed\nquasars (QSOs) is distinctly different from randomly distributed source\npopulations, with a median p-value of $4.6\\times10^{-5}$ and $\\ll\n1\\times10^{-7}$, when compared with 500 simulated populations with randomly\nsimulated locations but matching redshifts in the $0.1\\leq z<0.4$ and $0.4\\leq\nz < 0.7$ intervals, respectively. A similar comparison of the voidiness for\nLAT-detected AGN shows median p-values greater than 0.05 in each redshift\ninterval. When comparing the SDSS QSO population to the LAT-detected AGN, we\nmitigate potential bias from a relationship between redshift and voidiness by\ncomparing the LAT-detected AGN to a ``redshift-matched'' set of SDSS QSOs. The\nLAT-detected AGN between a redshift of 0.4 and 0.7 show higher voidiness\ncompared to the redshift-matched SDSS QSO populations, with a median p-value of\n2.3$\\times10^{-5}$, (a $4.1\\sigma$ deviation). No deviation is found when\ncomparing the same populations between redshifts of 0.1 and 0.4 (p>0.05). We do\nnot study possible causes of this voidiness difference. It might relate to\npropagation effects from lower magnetic or radiative background fields within\nvoids or to an environment more favorable for gamma-ray production for AGN near\nvoids.",
        "Planets orbiting one of the two stars in a binary are vulnerable to\ngravitational perturbations from the other star. Particularly, highly eccentric\ncompanion stars risk disrupting planetary orbits, such as in the extreme system\nTOI 4633 where close encounters between the companion and a gas giant planet in\nthe habitable zone make it one of the most fragile systems discovered so far.\nHere, we report that TOI 4633's planet likely survived these encounters\nthroughout the system's age by orbiting retrograde relative to the binary,\nstabilised by the Coriolis force. Using direct $N$-body simulations, we show it\notherwise tends to collide with the binary stars or becomes free-floating after\ngetting ejected. A retrograde planetary orbit has profound implications for TOI\n4633's formation and evolution, suggesting an extraordinary history where its\neccentric companion was likely randomly captured after planet formation in a\nsingle-star system. Alternatively, if stars and planet are born in situ from\nthe same gas clump, we show the planet must have formed at sub-snow-line\ndistances, contrary to the conventional core-accretion model. Our study\nhighlights the importance of considering the long-term stability ($\\gtrsim\\rm\nGyr$) of planets in eccentric binaries and demonstrates that the mere existence\nin such dynamically hostile environments places strong constraints on their\norbital configuration and formation.",
        "The application of surface plasmon resonance (SPR) has transformed the field\nof study of interactions between a ligand immobilized on the surface of a\nsensor chip, designated as $L_S$, and an analyte in solution, referred to as\n$A$. This technique enables the real-time measurement of interactions with high\nsensitivity. The dynamics of adsorption-desorption process, $A+L_S \\rightarrow\nAL_S$, can be expressed mathematically as a set of coupled integer-order\ndifferential equations. However, this approach has limited ability to acoount\nfor temperature distribution, diffusion and transport effects involved in the\nreaction process. The fractional kinetic model provides a methodology for\nincorporating non-local effects into the problem. In this study, the proposed\nmodel was applied to analyze data to the interaction between Immobilized Baru\nProtein (IBP) and Congo Red dye (CR) at concentrations ranging from $7.5$ to\n$97.5$ $\\mu M$, at pH $7.4$ and $16^o$ C. The variation in the kinetic\nconstants was studied, and it was demonstrated that the integer-order model is\nunable to adequately represent the experimental data. This work has shown that\nthe fractional-order model is capable of capturing the complexity of the\nadsorption-desorption process involved in the SPR data.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "The multi-stage method of laser wakefield acceleration (LWFA) presents a\npromising approach for developing stable, full-optical, high-energy electron\naccelerators. By segmenting the acceleration process into several booster\nstages, each powered by independent laser drivers, this technique effectively\nmitigates challenges such as electron dephasing, pump depletion, and laser\ndiffraction. A critical aspect of multi-stage LWFA is the nonlinear interaction\nbetween the injected electron beam and the laser-driven wakefields in the\nbooster stage. This study investigates the injection and acceleration of\nexternal electron beams within wakefields in the booster stage using\nmulti-dimensional Particle-In-Cell (PIC) simulations. We provide both\nqualitative and quantitative descriptions of the observed physical processes.\nKey parameters influencing charge coupling process and the resultant beam\nquality have been identified. Furthermore, we have examined how off-axis\ninjection relative to the driver laser influences the acceleration process and\nbeam quality. Our findings provide valuable insights for advancing and\noptimizing multi-stage plasma-based accelerators.",
        "We prove that two topologically conjugate bi-critical circle maps whose\nsignatures are the same, and whose renormalizations converge together\nexponentially fast in the $C^2$-topology, are $C^1$ conjugate.",
        "A discrete time quantum walk is known to be the single-particle sector of a\nquantum cellular automaton. For a long time, these models have interested the\ncommunity for their nice properties such as locality or translation invariance.\nThis work introduces a model of distributed computation for arbitrary graphs\ninspired by quantum cellular automata. As a by-product, we show how this model\ncan reproduce the dynamic of a quantum walk on graphs. In this context, we\ninvestigate the communication cost for two interaction schemes. Finally, we\nexplain how this particular quantum walk can be applied to solve the search\nproblem and present numerical results on different types of topologies.",
        "Who represents the corporate elite in democratic governance? Prior studies\nfind a tightly integrated \"inner circle\" network representing the corporate\nelite politically across varieties of capitalism, yet they all rely on data\nfrom a highly select sample of leaders from only the largest corporations. We\ncast a wider net. Analyzing new data on all members of corporate boards in the\nDanish economy (200k directors in 120k boards), we locate 1500 directors that\noperate as brokers between local corporate networks. We measure their network\ncoreness using k-core detection and find a highly connected core of 275\ndirectors, half of which are affiliated with smaller firms or subsidiaries.\nAnalyses show a strong positive association between director coreness and the\nlikelihood of joining one of the 650 government committees epitomizing\nDenmark's social-corporatist model of governance (net of firm and director\ncharacteristics). The political network premium is largest for directors of\nsmaller firms or subsidiaries, indicating that network coreness is a key driver\nof business political representation, especially for directors without claims\nto market power or weight in formal interest organizations.",
        "In this work, the $L_p$ version (for $p> 1$) of the dimensional\nBrunn-Minkowski inequality for the standard Gaussian measure $\\gamma_n(\\cdot)$\non $\\mathbb{R}^n$ is shown. More precisely, we prove that for any $0$-symmetric\nconvex sets with nonempty interior, any $p>1$, and every $\\lambda \\in (0,1)$,\n\\[ \\gamma_n\\bigl((1-\\lambda)\\cdot K+_p \\lambda \\cdot L\\bigr)^{p\/n} \\geqslant\n(1-\\lambda ) \\gamma_n(K)^{p\/n} + \\lambda \\gamma_n(L)^{p\/n}, \\] with equality,\nfor some $\\lambda \\in (0,1)$ and $p>1$, if and only if $K=L$. This result,\nrecently established without the equality conditions by Hosle, Kolesnikov and\nLivshyts, by using a different and functional approach, turns out to be the\n$L_p$ extension of a celebrated result for the Minkowski sum (that is, for\n$p=1$) by Eskenazis and Moschidis (2021) on a problem by Gardner and Zvavitch\n(2010).\n  Moreover, an $L_p$ Brunn-Minkowski type inequality is obtained for the\nclassical Wills functional $\\mathcal{W}(\\cdot)$ of convex bodies.\n  These results are derived as a consequence of a more general approach, which\nprovides us with other remarkable examples of functionals satisfying $L_p$\nBrunn-Minkowski type inequalities, such as different absolutely continuous\nmeasures with radially decreasing densities.",
        "This work investigates a class of moving boundary problems related to a\nnonlinear evolution equation featuring an exponential source term. We establish\na connection to Stefan-type problems, for different boundary conditions at the\nfixed face, through the application of a reciprocal transformation alongside\nthe Cole-Hopf transformation. For specific cases, we derive explicit similarity\nsolutions in parametric form. This innovative approach enhances our\nunderstanding of the underlying dynamics and offers valuable insights into the\nbehavior of these systems.",
        "Preserving the unique role of the one Higgs doublet of the standard model, it\nis proposed that quark and lepton mass patterns, often ascribed to the\nFroggatt-Nielsen mechanism using nonrenormalizable higher-dimensional terms,\nmay be enforced in a renormalizable theory of just one Higgs doublet by the\nscotogenic mechanism with soft symmetry breaking in the dark sector. A revised\nversion of the original $A_4$ model of charged leptons and neutrinos is\ndiscussed.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "The goal of the present study is to increase the intelligibility of\nmacroeconomic phenomena triggered by governmental intervention in economy by\nmeans of fiscal policies. During cyclical movements, fiscal policy can play an\nimportant role in order to help stabilise the economy. But discretionary policy\nusually implies implementation lags and is not automatically reversed when\neconomic conditions change. In contrast, automatic fiscal stabilisers (SFA)\nensure a prompter, and self-correcting fiscal response. The present study aims\nto tackle the topic of discretionary vs nondiscretionary characteristic of\nfiscal stabilisers (SF). In this context, the scope of the research undertaking\nis to launch a scientific debate over the definitions of the concepts of\nnon-automatic fiscal stabilisers (SfnA) and SFAs. We describe how we can\nquantify the discretionary and non-discretionary character of the fiscal\npolicy, by the analysis of the structure of the conventional budget balance\n(SBc), budget balance associated with the current GDP. In the final part of\nthis article, we propose a quantitative equilibrium model for establishing the\nmathematical prerequisites for an SF to become automatic. Likewise, on the\nbasis of the proposed mathematical model we have performed a qualitative\nanalysis of the influence factors.",
        "Motivated by partition regularity problems of homogeneous quadratic\nequations, we prove multiple recurrence and convergence results for\nmultiplicative measure preserving actions with iterates given by rational\nsequences involving polynomials that factor into products of linear forms in\ntwo variables. We focus mainly on actions that are finitely generated, and the\nkey tool in our analysis is a decomposition result for any bounded measurable\nfunction into a sum of two components, one that mimics concentration properties\nof pretentious multiplicative functions and another that mimics vanishing\nproperties of aperiodic multiplicative functions. Crucial to part of our\narguments are some new seminorms that are defined by a mixture of addition and\nmultiplication of the iterates of the action, and we prove an inverse theorem\nthat explicitly characterizes the factor of the system on which these seminorms\nvanish.",
        "Chern-Simons orbital magnetoelectric (OME) coupling is usually the hallmark\nof nontrivial band topology in three-dimensional (3D) crystalline insulators.\nHowever, if a 3D insulator exhibits nonzero Chern number within any\ntwo-dimensional plane of the Brillouin zone, then traditionally the\nChern-Simons coupling becomes ill defined for such 3D Chern insulators due to\ntopological obstructions. In this work, by employing a ``gauge-discontinuity\"\nformalism, we resolve this long-standing issue and rigorously derive a\nquantized layer-resolved OME response in 3D Chern insulators. We demonstrate\nthat the difference of the layer-resolved OME coupling between adjacent layers\nis universally quantized in unit of $-C e^2\/h$, where $C$ is the Chern number.\nThis quantization arises from an anomalous contribution to the Chern-Simons OME\ncoupling, which is closely associated with the Berry curvature of the occupied\nbands and the hybrid Wannier centers along the direction of the Chern vector\n$(0,0, C)$. Furthermore, we demonstrate that the anomalous Chern-Simons\ncoupling can be transported by an exact integer quantum from one unit cell to\nits neighboring cell through an adiabatic cyclic pumping process, accompanied\nby a quantized displacement of Wannier center along the direction of the Chern\nvector. Our work provides a rigorous theoretical framework for understanding\nmagnetoelectric response in 3D Chern insulators and opens avenues for designing\ntopological quantum phenomena in layered systems.",
        "We propose an experimental setup for manipulating the spontaneous emission of\ntrapped ions, based on a spatial light modulator. Anticipated novelties include\nthe potential to entangle more than two ions through a single photon detection\nevent and control the visibility for spatially distinguishable emitters. The\nsetup can be adapted to most of the existing ion traps commonly used in quantum\ntechnology.",
        "We investigate the shadows and optical appearances of a new type of regular\nblack holes (BHs) with a Minkowski core under different spherical accretion.\nThese BHs are constructed by modifying the Newtonian potential based on the\nminimum observable length in the Generalized Uncertainty Principle (GUP). They\ncorrespond one-to-one with the traditional regular BHs with a de-Sitter (dS)\ncore (such as Bardeen\/Hayward BHs), characterized by quantum gravity effect\nparameter ($\\alpha_0$) and spacetime deformation factor ($n$). We find that the\ncharacteristic parameters give rise to some novel observable features. For\nthese new BHs, the shadow radius, photon sphere radius, and total observed\nintensity increase with the increase of $\\alpha_0$ but decrease with the\nincrease of $n$. Under different spherical accretion, the shadow and photon\nsphere radius are identical, but the total observed intensity under the static\nspherical accretion is greater than that under the infalling spherical\naccretion. In addition, we find that these regular BHs with different cores\nshow differences in shadows and optical appearances, especially under the\nstatic spherical accretion. Compared with Bardeen BH, the new BH has a smaller\ntotal observed intensity, dimmer maximum luminosity, and smaller shadow and\nphoton sphere radius. The larger $\\alpha_0$ leads to more significant\ndifferences, and a similar trend is also seen in the comparison with Hayward\nBH. Under the infalling spherical accretion, these regular BHs with different\ncores only have slight differences in total observed intensity, shadow and\nphoton sphere radius, which become more obvious when $\\alpha_0$ is relatively\nlarge. It suggests that the unique spacetime features of these regular BHs with\ndifferent cores can be distinguished through astronomical observations.",
        "Size effects are ubiquitous in the structural, mechanical, and physical\nproperties of materials, making it highly desirable to study the intrinsic\nproperties of thick objects through high-resolution structural analysis in\ntransmission electron microscopy. Although deep-sub-angstrom resolution has\nbeen achieved with multislice electron ptychography, the sample thickness is\ntypically very limited. By combining energy filtering and extended\nlocal-orbital ptychography (eLOP) that retrieves varying aberrations during\nelectron scanning, here we report ptychographic reconstructions for silicon as\nthick as 85 nm, approximately three times larger than usual thickness threshold\nfor conventional multislice electron ptychography. The elimination of\naberration variations contributes to accurate reconstructions with an\ninformation limit of 18 pm and atomic position precision of 0.39 pm. Accurate\nptychographic reconstructions for thick objects can facilitate the discovery or\ninterpretation of intrinsic structural and physical phenomena in solids, which\nis of great significance in physics, chemistry, materials science, and\nsemiconductor device engineering.",
        "For the first time the neutron-rich hydrogen isotope $\\rm ^{6}H$ was produced\nin an electron scattering experiment in the reaction $\\rm\n^{7}Li(e,~e'p\\pi^{+})^{6}H$ using the spectrometer facility of the A1\nCollaboration at the Mainz Microtron accelerator. By measuring the triple\ncoincidence between the scattered electron, the produced proton, and $\\pi^{+}$,\nthe missing mass spectrum of $\\rm ^{6}H$ was obtained. A clear peak above\n$^3$H+n+n+n energy threshold was seen resulting in a ground state energy of\n$\\rm ^{6}H$ at $2.3\\pm0.5({\\rm stat.})\\pm0.4({\\rm syst.})$ MeV with a width of\n$1.9\\pm1.0({\\rm stat.})\\pm0.4({\\rm syst.})$ MeV. This work challenges the\nunderstandings of multi-nucleon interactions and presents a new method to study\nlight neutron-rich nuclei with electron scattering experiments.",
        "Collisionless shocks, essential for astrophysics, perhaps do not exist as\nstatistically stationary solutions. If so, any quantitative statement about a\ncollisionless shock should be qualified by the age of the shock.\n  A theoretical description of the upstream of the 1+1 dimensional\nelectrostatic collisionless shock is developed -- collisionless hydrodynamics.\nPeculiarities of collisionless hydrodynamics prevent a shock formation when a\npiston is driven into cold plasma. An exact self-similar solution is found\ninstead; the spatial extent of the solution grows linearly in time.\n  Direct numerical simulations of plasma kinetics in 1+1 dimensions confirm the\nhydrodynamic result -- a statistically steady collisionless shock doesn't\nexist. Instead, at each fixed time, there is a continuous succession in space\nof marginally stable velocity distribution functions. The spatial support of\nthis continuous succession grows linearly in time.",
        "NeuralFoil is an open-source Python-based tool for rapid aerodynamics\nanalysis of airfoils, similar in purpose to XFoil. Speedups ranging from 8x to\n1,000x over XFoil are demonstrated, after controlling for equivalent accuracy.\nNeuralFoil computes both global and local quantities (lift, drag, velocity\ndistribution, etc.) over a broad input space, including: an 18-dimensional\nspace of airfoil shapes, possibly including control deflections; a 360 degree\nrange of angles of attack; Reynolds numbers from $10^2$ to $10^{10}$; subsonic\nflows up to the transonic drag rise; and with varying turbulence parameters.\nResults match those of XFoil closely: the mean relative error of drag is 0.37%\non simple cases, and remains as low as 2.0% on a test dataset with numerous\npost-stall and transitional cases. NeuralFoil facilitates gradient-based design\noptimization, due to its $C^\\infty$-continuous solutions,\nautomatic-differentiation-compatibility, and bounded computational cost without\nnon-convergence issues.\n  NeuralFoil is a hybrid of physics-informed machine learning techniques and\nanalytical models. Here, physics information includes symmetries that are\nstructurally embedded into the model architecture, feature engineering using\ndomain knowledge, and guaranteed extrapolation to known limit cases. This work\nalso introduces a new approach for surrogate model uncertainty quantification\nthat enables robust design optimization.\n  This work discusses the methodology and performance of NeuralFoil with\nseveral case studies, including a practical airfoil design optimization study\nincluding both aerodynamic and non-aerodynamic constraints. Here, NeuralFoil\noptimization is able to produce airfoils nearly identical in performance and\nshape to expert-designed airfoils within seconds; these\ncomputationally-optimized airfoils provide a useful starting point for further\nexpert refinement.",
        "The ratio between the stellar mass of a galaxy, $M_{*}$, and that of its\ncentral supermassive black hole (SMBH), $M_\\bullet$, the ``Magorrian''\nrelationship, traces their coevolution. JWST observations have suggested\nsignificant evolution in $M_\\bullet\/M_{*}$ relative to local scaling\nrelationships both in low-mass galaxies and in quasars at z $\\ge$ 4. We test\nthis possibility by (1) determining the preferred $M_\\bullet\/M_{*}$ scaling\nrelation among those proposed locally; and (2) providing uniform host galaxy\nstellar mass estimates. These steps reduce the prominence of the reported\nevolution. We then apply Monte Carlo simulations to account for observational\nbiases. We still find a significant increase over the local scaling relation in\n$M_\\bullet\/M_{*}$ for z $\\ge$ 4 SMBHs in very low-mass galaxies\n($\\log(M_*\/M_{\\odot})<10$). However, similarly high values of $M_\\bullet\/M_{*}$\nare also found in low mass galaxies at $z \\sim$ 0.5 to 3 that may be common at\ncosmic noon. Nonetheless, galaxies with similar behavior are rare locally and\nnot accounted for in the local scaling relations. In contrast, z $\\sim$ 6\nquasars can have $M_\\bullet\/M_{*}$ well above the local relation value, but\nthey can be explained as extreme cases still within the scaling relation for\ntheir higher mass host galaxies. Black holes in some of them and in the\nlow-mass systems may be undergoing very high accretion episodes that result in\nhigh $M_\\bullet\/M_{*}$ but that will be followed by quiescent periods when\ngrowth of the host drives the systems toward more typical $M_\\bullet\/M_{*}$\nvalues.",
        "On a closed Riemannian manifold, we construct a family of intrinsic Gaussian\nnoises indexed by a regularity parameter $\\alpha\\geq0$ to study the\nwell-posedness of the Parabolic Anderson model. We show that with rough initial\nconditions, the equation is well-posed assuming non-positive curvature with a\ncondition on $\\alpha$ similar to that of Riesz kernel-correlated noise in\nEuclidean space. The argument was made in direct mode, showing that it is\npossible to bypass Fourier analysis, which was used in all previous work with\nrough initial conditions. Non-positive curvature was used to overcome a new\ndifficulty introduced by non-uniqueness of geodesics in this setting, which\nrequired exploration of global geometry. The well-posedness argument also\nproduces exponentially growing in time upper bounds for the moments. Using the\ngood structure of our noise, we obtain new exponentially growing in time second\nmoment lower bounds for our solutions with bounded initial condition.",
        "It is a theorem of Ribet that an abelian variety defined over a number field\n$K$ has only finitely many torsion points with values in the maximal cyclotomic\nextension field $K^{\\mathrm{cyc}}$ of $K$. Recently, R\\\"ossler and Szamuely\ngeneralized Ribet's theorem in terms of the \\'etale cohomology with\n$\\mathbb{Q}\/\\mathbb{Z}$-coefficients of a smooth proper variety. In this paper,\nwe show that the same finiteness holds even after replacing $K^{\\mathrm{cyc}}$\nwith the field obtained by adjoining to $K$ all roots of all elements of a\ncertain subset of $K$. Furthermore, we give some new examples of TKND-AVKF\nfields; the notion of TKND-AVKF is introduced by Hoshi, Mochizuki and\nTsujimura, and TKND-AVKF fields are expected as one of suitable base fields for\nanabelian geometry.",
        "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
        "Active Galactic Nuclei (AGN) are some of the most luminous and extreme\nenvironments in the Universe. The central engines of AGN, believed to be\nsuper-massive black-holes, are fed by accretion discs threaded by magnetic\nfields within a dense magneto-ionic medium. We report our findings from\npolarimetric Very-long-baseline Interferometry (VLBI) observations of quasar\nNRAO150 taken in October 2022 using a combined network of the Very Long\nBaseline Array (VLBA) and Effelsberg 100-m Radio Telescope. These observations\nare the first co-temporal multi-frequency polarimetric VLBI observations of\nNRAO150 at frequencies above 15GHz. We use the new VLBI polarization\ncalibration procedure, GPCAL, with polarization observations of frequencies of\n12GHz, 15GHz, 24GHz, and 43GHz of NRAO150. From these observations, we measure\nFaraday rotation. Using our measurement of Faraday rotation, we also derive the\nintrinsic electric vector position angle (EVPA0) for the source. As a\ncomplementary measurement we determine the behavior of polarization as a\nfunction of observed frequency. The polarization from NRAO150 only comes from\nthe core region, with a peak polarization intensity occurring at 24GHz. Across\nthe core region of NRAO150 we see clear gradients in Faraday rotation and EVPA0\nvalues that are aligned with the direction of the jet curving around the core\nregion. We find that for the majority of the polarized region the polarization\nfraction is greater at higher frequencies, with intrinsic polarization\nfractions in the core 3%. The Faraday rotation gradients and circular patterns\nin EVPA0 are strong evidence for a helical\/toroidal magnetic field, and the\npresence of low intrinsic polarization fractions indicate that the polarized\nemission and hence the helical\/toroidal magnetic field, occur within the\ninnermost jet.",
        "We explore the possibility that the interactions of a light scalar singlet,\nwhich mixes with the Standard Model~(SM) Higgs, also receive other UV\ncontributions of comparable size. We focus, in particular, on the flavor\naligned limit, where couplings of the light scalar to the SM are almost flavor\ndiagonal, but not necessarily proportional to the Higgs Yukawa couplings. The\nphenomenology of such a general flavor aligned light scalar differs from both\nthe Higgs-mixed scalar, as well as from a general axion-like particle. We\nexplore this for light scalar masses below a few hundred MeV, such that they\ncan be produced in kaon decays, and in decays of $\\eta$ and $\\eta'$ mesons, and\nthe transitions described using chiral perturbation theory. We then derive\nconstraints on the light scalar interactions, assuming that light scalar decays\nare either just into photons or are invisible. We also discuss several UV\nexamples of such light scalar models: a two-Higgs doublet model extended by a\nlight scalar, a light dilaton from the dark sector, and a SM extended by heavy\nvector-like quarks and a light scalar. For the latter we also performed\nmatching onto low energy theory at one-loop."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning",
    "start_abstract":"Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "A review of artificial intelligence in prostate cancer detection on imaging"
      ],
      "abstract":[
        "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A binary PSO based ensemble under-sampling model for rebalancing\n  imbalanced training data",
        "Enhanced Atom-by-Atom Assembly of Defect-Free Two-Dimensional\n  Mixed-Species Atomic Arrays",
        "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks\n  at ACM ICAIF FinRL Contest 2023-2024",
        "Soybean pod and seed counting in both outdoor fields and indoor\n  laboratories using unions of deep neural networks",
        "In the Picture: Medical Imaging Datasets, Artifacts, and their Living\n  Review",
        "Variations on hypergeometric functions",
        "Power-law banded random matrix ensemble as a model for quantum many-body\n  Hamiltonians",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Deviations from the Porter-Thomas distribution due to non-statistical\n  $\\gamma$ decay below the $^{150}$Nd neutron separation threshold",
        "Enhanced Continual Learning of Vision-Language Models with Model Fusion",
        "Language Representation Favored Zero-Shot Cross-Domain Cognitive\n  Diagnosis",
        "Subjective and Objective Quality Assessment of Non-Uniformly Distorted\n  Omnidirectional Images",
        "Divide-and-Conquer: Tree-structured Strategy with Answer Distribution\n  Estimator for Goal-Oriented Visual Dialogue",
        "Dammann Metasurface Route to Overcoming the Uniformity Defects in\n  Two-Dimensional Beam Multipliers",
        "Parsings of Stationary Processes, Stopping Times and the Fundamental\n  Pointwise Convergence Theorems of Ergodic Theory",
        "Hiding in Plain Sight: RIS-Aided Target Obfuscation in ISAC",
        "Degradation-based Energy Management for Microgrids in the Presence of\n  Energy Storage Elements",
        "UMC: Unified Resilient Controller for Legged Robots with Joint\n  Malfunctions",
        "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
        "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
        "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for\n  Enabling Fair LLM-Based Recommender Systems",
        "Distributionally Robust Model Predictive Control with Mixture of\n  Gaussian Processes",
        "Micromagnetic formalism for magnetic multipoles",
        "Advancing vision-language models in front-end development via data\n  synthesis",
        "Neural Chaos: A Spectral Stochastic Neural Operator",
        "Concept camera for the next-generation mm-wave cosmological surveys",
        "\\'Etude statistique du facteur premier m\\'edian, 2 : lois locales",
        "The Derrida-Retaux model on a geometric Galton-Watson tree",
        "Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety\n  Perception"
      ],
      "abstract":[
        "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
        "Defect-free single atom array in optical tweezers is a promising platform for\nscalable quantum computing, quantum simulation, and quantum metrology.\nExtending single-species array to mixed-species one promise to offer new\npossibilities. In our recent proof of principle realization of defect-free\ntwo-dimensional assembly of mixed-species $^{85}$Rb ($^{87}$Rb) atom arrays [C.\nSheng et\nal.\\href{https:\/\/journals.aps.org\/prl\/abstract\/10.1103\/PhysRevLett.128.083202}{{\\color{blue}\nPhys. Rev. Lett. 128, 083202(2022)}}], the filling fractions were limited by\nthe imperfect transfer of atoms and the occurrence of logjams during the atom\nrearrangement. In order to scale up the size of defect-free mixed-species atom\narray, we scale up the tweezer array and improve the atom transfer, and upgrade\nthe heuristic heteronuclear algorithm so as to facilitate multiple\nrearrangement cycles. Consequently, we successfully create defect-free atom\narrays with 120 mixed-species single atoms. The corresponding filling fraction\nand defect-free probability are improved to be 98.6(1)\\% and 14(2)\\%,\nrespectively. It is anticipated that the enhanced algorithm can be extended to\nother combinations of atomic species, and this mixed-species atom array is\nreadily for studies of many-body physics, quantum error correction, and quantum\nmetrology.",
        "Reinforcement learning has demonstrated great potential for performing\nfinancial tasks. However, it faces two major challenges: policy instability and\nsampling bottlenecks. In this paper, we revisit ensemble methods with massively\nparallel simulations on graphics processing units (GPUs), significantly\nenhancing the computational efficiency and robustness of trained models in\nvolatile financial markets. Our approach leverages the parallel processing\ncapability of GPUs to significantly improve the sampling speed for training\nensemble models. The ensemble models combine the strengths of component agents\nto improve the robustness of financial decision-making strategies. We conduct\nexperiments in both stock and cryptocurrency trading tasks to evaluate the\neffectiveness of our approach. Massively parallel simulation on a single GPU\nimproves the sampling speed by up to $1,746\\times$ using $2,048$ parallel\nenvironments compared to a single environment. The ensemble models have high\ncumulative returns and outperform some individual agents, reducing maximum\ndrawdown by up to $4.17\\%$ and improving the Sharpe ratio by up to $0.21$.\n  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and\n2024.",
        "Automatic counting soybean pods and seeds in outdoor fields allows for rapid\nyield estimation before harvesting, while indoor laboratory counting offers\ngreater accuracy. Both methods can significantly accelerate the breeding\nprocess. However, it remains challenging for accurately counting pods and seeds\nin outdoor fields, and there are still no accurate enough tools for counting\npods and seeds in laboratories. In this study, we developed efficient deep\nlearning models for counting soybean pods and seeds in both outdoor fields and\nindoor laboratories. For outdoor fields, annotating not only visible seeds but\nalso occluded seeds makes YOLO have the ability to estimate the number of\nsoybean seeds that are occluded. Moreover, we enhanced YOLO architecture by\nintegrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques\n(YOLO-DA), to improve model robustness and generalization across soybean images\ntaken in outdoor fields. Testing on soybean images from the outdoor field, we\nachieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for\nseed counting. For the indoor setting, we utilized Mask-RCNN supplemented with\na Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on\nsynthetic training images generated from a small set of labeled data. This\napproach resulted in near-perfect accuracy, with an MAE of 1.07 for pod\ncounting and 1.33 for seed counting across actual laboratory images from two\ndistinct studies.",
        "Datasets play a critical role in medical imaging research, yet issues such as\nlabel quality, shortcuts, and metadata are often overlooked. This lack of\nattention may harm the generalizability of algorithms and, consequently,\nnegatively impact patient outcomes. While existing medical imaging literature\nreviews mostly focus on machine learning (ML) methods, with only a few focusing\non datasets for specific applications, these reviews remain static -- they are\npublished once and not updated thereafter. This fails to account for emerging\nevidence, such as biases, shortcuts, and additional annotations that other\nresearchers may contribute after the dataset is published. We refer to these\nnewly discovered findings of datasets as research artifacts. To address this\ngap, we propose a living review that continuously tracks public datasets and\ntheir associated research artifacts across multiple medical imaging\napplications. Our approach includes a framework for the living review to\nmonitor data documentation artifacts, and an SQL database to visualize the\ncitation relationships between research artifact and dataset. Lastly, we\ndiscuss key considerations for creating medical imaging datasets, review best\npractices for data annotation, discuss the significance of shortcuts and\ndemographic diversity, and emphasize the importance of managing datasets\nthroughout their entire lifecycle. Our demo is publicly available at\nhttp:\/\/130.226.140.142.",
        "We prove new integral formulas for generalized hypergeometric functions and\ntheir confuent variants. We apply them, via stationary phase formula, to study\nWKB expansions of solutions: for large argument in the confuent case and for\nlarge parameter in the general case. We also study variations of hypergeometric\nfunctions for small perturbations of hypergeometric equations, i.e., in\nexpansions of solutions in powers of a small parameter. Next, we present a new\nproof of a theorem due to Wasow about equivalence of the Airy equation with its\nperturbation; in particular, we explain that this result does not deal with the\nWKB solutions and the Stokes phenomenon. Finally, we study hypergeometric\nequations, one of second order and another of third order, which are related\nwith two generating functions for MZVs, one $\\Delta_2 (\\lambda )$ for $\\zeta(2,\n\\ldots , 2)$'s and another $\\Delta_3 (\\lambda )$ for $\\zeta(3, \\ldots , 3)$'s;\nin particular, we correct a statement from [ZZ3] that the function\n$\\Delta_3(\\lambda)$ admits a regular WKB expansion.",
        "Hamiltonians of one-dimensional, disordered single-particle systems with\nlong-range hopping terms can naturally be modeled by power-law banded random\nmatrices. In this picture, the phase diagram of a power-law banded random\nmatrix ensemble show ergodic, weakly ergodic, multifractal, and localized\nphases. Motivated by recent developments on ergodicity breaking and\nlocalization in interacting quantum many-body systems, we explore many-body\ninterpretations of the power-law banded random matrix ensemble. We discuss a\nnumber of ways to label the basis vectors with many-body configurations, and\ncompare the physical properties of the resulting Hamiltonians. We characterize\nthe scaling of the many-body eigenstate entanglement entropy with system size\nfor the different labeling schemes and in each of the phases. Using a scaling\nanalysis on the full sets of eigenstates, we subsequently provide a\nquantitative picture of the boundary between the different types of scaling\nbehavior that we observe for the spectral-bulk and spectral-edge eigenstates.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "We introduce a new method for the study of fluctuations of partial transition\nwidths based on nuclear resonance fluorescence experiments with\nquasimonochromatic linearly-polarized photon beams below particle separation\nthresholds. It is based on the average branching of decays of $J=1$ states of\nan even-even nucleus to the $2^+_1$ state in comparison to the ground state.\nBetween 5 and 7 MeV, an almost constant average branching ratio of 0.490(16) is\nobserved for the nuclide $^{150}$Nd. Assuming $\\chi^2$-distributed partial\ntransition widths, this average branching ratio is related to a degree of\nfreedom of $\\nu = 1.93(12)$, rejecting the validity of the Porter-Thomas\ndistribution, requiring $\\nu=1$. The observed deviation can be explained by\nnon-statistical effects in the $\\gamma$-decay behavior with contributions in\nthe range of 9.4(10)% up to 94(10)%.",
        "Vision-Language Models (VLMs) represent a breakthrough in artificial\nintelligence by integrating visual and textual modalities to achieve impressive\nzero-shot capabilities. However, VLMs are susceptible to catastrophic\nforgetting when sequentially fine-tuned on multiple downstream tasks. Existing\ncontinual learning methods for VLMs often rely heavily on additional reference\ndatasets, compromise zero-shot performance, or are limited to\nparameter-efficient fine-tuning scenarios. In this paper, we propose Continual\nDecoupling-Unifying (ConDU), a novel approach, by introducing model fusion into\ncontinual learning for VLMs. ConDU maintains a unified model along with task\ntriggers and prototype sets, employing an iterative process of decoupling\ntask-specific models for previous tasks and unifying them with the model for\nthe newly learned task. Additionally, we introduce an inference strategy for\nzero-shot scenarios by aggregating predictions from multiple decoupled\ntask-specific models. Extensive experiments across various settings show that\nConDU achieves up to a 2\\% improvement in average performance across all seen\ntasks compared to state-of-the-art baselines, while also enhancing zero-shot\ncapabilities relative to the original VLM.",
        "Cognitive diagnosis aims to infer students' mastery levels based on their\nhistorical response logs. However, existing cognitive diagnosis models (CDMs),\nwhich rely on ID embeddings, often have to train specific models on specific\ndomains. This limitation may hinder their directly practical application in\nvarious target domains, such as different subjects (e.g., Math, English and\nPhysics) or different education platforms (e.g., ASSISTments, Junyi Academy and\nKhan Academy). To address this issue, this paper proposes the language\nrepresentation favored zero-shot cross-domain cognitive diagnosis (LRCD).\nSpecifically, LRCD first analyzes the behavior patterns of students, exercises\nand concepts in different domains, and then describes the profiles of students,\nexercises and concepts using textual descriptions. Via recent advanced\ntext-embedding modules, these profiles can be transformed to vectors in the\nunified language space. Moreover, to address the discrepancy between the\nlanguage space and the cognitive diagnosis space, we propose language-cognitive\nmappers in LRCD to learn the mapping from the former to the latter. Then, these\nprofiles can be easily and efficiently integrated and trained with existing\nCDMs. Extensive experiments show that training LRCD on real-world datasets can\nachieve commendable zero-shot performance across different target domains, and\nin some cases, it can even achieve competitive performance with some classic\nCDMs trained on the full response data on target domains. Notably, we\nsurprisingly find that LRCD can also provide interesting insights into the\ndifferences between various subjects (such as humanities and sciences) and\nsources (such as primary and secondary education).",
        "Omnidirectional image quality assessment (OIQA) has been one of the hot\ntopics in IQA with the continuous development of VR techniques, and achieved\nmuch success in the past few years. However, most studies devote themselves to\nthe uniform distortion issue, i.e., all regions of an omnidirectional image are\nperturbed by the ``same amount'' of noise, while ignoring the non-uniform\ndistortion issue, i.e., partial regions undergo ``different amount'' of\nperturbation with the other regions in the same omnidirectional image.\nAdditionally, nearly all OIQA models are verified on the platforms containing a\nlimited number of samples, which largely increases the over-fitting risk and\ntherefore impedes the development of OIQA. To alleviate these issues, we\nelaborately explore this topic from both subjective and objective perspectives.\nSpecifically, we construct a large OIQA database containing 10,320\nnon-uniformly distorted omnidirectional images, each of which is generated by\nconsidering quality impairments on one or two camera len(s). Then we\nmeticulously conduct psychophysical experiments and delve into the influence of\nboth holistic and individual factors (i.e., distortion range and viewing\ncondition) on omnidirectional image quality. Furthermore, we propose a\nperception-guided OIQA model for non-uniform distortion by adaptively\nsimulating users' viewing behavior. Experimental results demonstrate that the\nproposed model outperforms state-of-the-art methods. The source code is\navailable at https:\/\/github.com\/RJL2000\/OIQAND.",
        "Goal-oriented visual dialogue involves multi-round interaction between\nartificial agents, which has been of remarkable attention due to its wide\napplications. Given a visual scene, this task occurs when a Questioner asks an\naction-oriented question and an Answerer responds with the intent of letting\nthe Questioner know the correct action to take. The quality of questions\naffects the accuracy and efficiency of the target search progress. However,\nexisting methods lack a clear strategy to guide the generation of questions,\nresulting in the randomness in the search process and inconvergent results. We\npropose a Tree-Structured Strategy with Answer Distribution Estimator (TSADE)\nwhich guides the question generation by excluding half of the current candidate\nobjects in each round. The above process is implemented by maximizing a binary\nreward inspired by the ``divide-and-conquer'' paradigm. We further design a\ncandidate-minimization reward which encourages the model to narrow down the\nscope of candidate objects toward the end of the dialogue. We experimentally\ndemonstrate that our method can enable the agents to achieve high task-oriented\naccuracy with fewer repeating questions and rounds compared to traditional\nergodic question generation approaches. Qualitative results further show that\nTSADE facilitates agents to generate higher-quality questions.",
        "Dammann gratings - beam-shaping optical elements acting as beam multipliers\nwith equal-power beams - are a key element in three-dimensional imaging based\non structured light and beam combiners for high-power laser applications.\nHowever, two-dimensional Dammann grating structures suffer from a significant\nreduction of the uniformity among the diffraction orders. Here, we report\nDammann metasurfaces based on the geometric phase as the structure realization\nfor the target phase profile, which outperform the capabilities of Dammann\ngratings by overcoming the uniformity defects in their two-dimensional\ndiffraction patterns. We showed that two-dimensional Dammann metasurfaces\nexhibit high uniformity and diffraction efficiency, in contrast to Dammann\ngratings, by overcoming the uniformity defects via a robust and highly precise\nphase imprint. Moreover, Dammann metasurfaces outperform their grating\ncounterparts by exhibiting a polarization-independent response and a broadband\noperation. This study reveals that by providing physics-driven solutions,\nmetasurfaces can outperform the capabilities of their bulk optics counterparts\nwhile facilitating virtually flat, ultrathin, and lightweight optics.",
        "The idea of a parsing of a stationary process according to a collection of\nwords is introduced, and the basic framework required for the asymptotic\nanalysis of these parsings is presented. We demonstrate how the pointwise\nergodic theorem and the Shannon-McMillan-Breiman theorem can be deduced from\ntheir respective weaker convergence in probability versions combined with our\nobservations regarding parsings, where the parsings are done according to\ncollections that originate in stopping times tailored for that purpose.",
        "Integrated sensing and communication (ISAC) has been identified as a\npromising technology for the sixth generation (6G) of communication networks.\nTarget privacy in ISAC is essential to ensure that only legitimate sensors can\ndetect the target while keeping it hidden from malicious ones. In this paper,\nwe consider a downlink reconfigurable intelligent surface (RIS)-assisted ISAC\nsystem capable of protecting a sensing region against an adversarial detector.\nThe RIS consists of both reflecting and sensing elements, adaptively changing\nthe element assignment based on system needs. To achieve this, we minimize the\nmaximum sensing signal-to-interference-plus-noise-ratio (SINR) at the\nadversarial detector within sample points in the sensing region, by optimizing\nthe transmit beamformer at the base station, the RIS phase shift matrix, the\nreceived beamformer at the RIS, and the division between reflecting and\nabsorptive elements at the RIS, where the latter function as sensing elements.\nAt the same time, the system is designed to maintain a minimum sensing SINR at\neach monitored location, as well as minimum communication SINR for each user.\nTo solve this challenging optimization problem, we develop an alternating\noptimization approach combined with a successive convex approximation based\nmethod tailored for each subproblem. Our results show that the proposed\napproach achieves a 25 dB reduction in the maximum sensing SINR at the\nadversarial detector compared to scenarios without sensing area protection.\nAlso, the optimal RIS element assignment can further improve sensing protection\nby 3 dB over RISs with fixed element configuration.",
        "Integration of Inverter-based Resources (IBRs) such as solar-powered plants\nwhich lack the intrinsic characteristics such as the inertial response of the\ntraditional synchronous-generator (SG) based sources presents a new challenge\nin the form of analyzing the grid stability under their presence. For example,\nsolar power is available for approximately from 9 AM-5 PM. However, the result\nof the rise in power consumption after 6 PM and the reverting back to the\nnon-renewable source of power generation during that period puts immense stress\non the grid, testing the ramp limitations of the SGs. Failure to meet the\nrequired power demand due to SG ramp limitations leads to failure of the power\ngrid and other catastrophes. Numerous mitigation techniques exist in order to\naddress the ramping issues with adding the energy storage elements (ESE) to the\ngrid being one. ESEs have higher ramping capabilities compared to the\ntraditional SGs. Also, the ESEs can store the energy and supply it to the grid\nwhen required making them extremely responsive to high ramp situations.\nHowever, the rate of degradation of the ESEs is faster than the SGs. This\nraises an important issue of addressing the degradation of the ESEs while\nmeeting the required power demand objectives and constraints. This work\nproposes a battery degradation-aware model predictive energy management\nstrategy and it is tested via a numerical simulation on multiple physical\nsystems such as Shipboard Power Systems (SPS). Moreover, the risk arising due\nto the fault in the IBR is also studied by means of a numerical simulation.\nOverall, the goal of this study is to make the existing power grid more robust,\nresilient, and risk-free from component degradation and eventual failures.",
        "Adaptation to unpredictable damages is crucial for autonomous legged robots,\nyet existing methods based on multi-policy or meta-learning frameworks face\nchallenges like limited generalization and complex maintenance. To address this\nissue, we first analyze and summarize eight types of damage scenarios,\nincluding sensor failures and joint malfunctions. Then, we propose a novel,\nmodel-free, two-stage training framework, Unified Malfunction Controller (UMC),\nincorporating a masking mechanism to enhance damage resilience. Specifically,\nthe model is initially trained with normal environments to ensure robust\nperformance under standard conditions. In the second stage, we use masks to\nprevent the legged robot from relying on malfunctioning limbs, enabling\nadaptive gait and movement adjustments upon malfunction. Experimental results\ndemonstrate that our approach improves the task completion capability by an\naverage of 36% for the transformer and 39% for the MLP across three locomotion\ntasks. The source code and trained models will be made available to the public.",
        "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM.",
        "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
        "We propose FACTER, a fairness-aware framework for LLM-based recommendation\nsystems that integrates conformal prediction with dynamic prompt engineering.\nBy introducing an adaptive semantic variance threshold and a\nviolation-triggered mechanism, FACTER automatically tightens fairness\nconstraints whenever biased patterns emerge. We further develop an adversarial\nprompt generator that leverages historical violations to reduce repeated\ndemographic biases without retraining the LLM. Empirical results on MovieLens\nand Amazon show that FACTER substantially reduces fairness violations (up to\n95.5%) while maintaining strong recommendation accuracy, revealing semantic\nvariance as a potent proxy of bias.",
        "Despite the success of Gaussian process based Model Predictive Control (MPC)\nin robotic control, its applicability scope is greatly hindered by multimodal\ndisturbances that are prevalent in real-world settings. Here we propose a novel\nMixture of Gaussian Processes based Distributionally Robust MPC (MoGP-DR-MPC)\nframework for linear time invariant systems subject to potentially multimodal\nstate-dependent disturbances. This framework utilizes MoGP to automatically\ndetermine the number of modes from disturbance data. Using the mean and\nvariance information provided by each mode-specific predictive distribution, it\nconstructs a data-driven state-dependent ambiguity set, which allows for\nflexible and fine-grained disturbance modeling. Based on this ambiguity set, we\nimpose Distributionally Robust Conditional Value-at Risk (DR-CVaR) constraints\nto effectively achieve distributional robustness against errors in the\npredictive distributions. To address the computational challenge posed by these\nconstraints in the resulting MPC problem, we equivalently reformulate the\nDR-CVaR constraints into tractable second-order cone constraints. Furthermore,\nwe provide theoretical guarantees on the recursive feasibility and stability of\nthe proposed framework. The enhanced control performance of MoGP-DR-MPC is\nvalidated through both numerical experiments and simulations on a quadrotor\nsystem, demonstrating notable reductions in closed-loop cost by 17% and 4%\nrespectively compared against Gaussian process based MPC.",
        "Cluster magnetic multipoles are order parameters that describe the symmetry\nof spin arrangements in magnetic materials. High-order multipoles are\nparticularly important in non-collinear antiferromagnets, where they determine\nkey physical properties such as the anomalous Hall effect and the\nmagneto-optical Kerr effect. Although non-uniform multipole states have been\nobserved at the micrometer scale, their mesoscopic properties remain largely\nunexplored. In this work, we develop a general micromagnetic formalism for\ncluster magnetic multipoles, enabling the study of the spin dynamics in\nnon-uniform multipole systems. As an example, we apply this formalism to\ninvestigate magnetic-octupole domain-wall dynamics in the non-collinear\nantiferromagnet Mn3Sn. Our results capture key features of domain-wall motion,\nincluding deformation and the emergence of effective inertial mass. This study\nprovides a general framework for studying the dynamics of high-order cluster\nmagnetic multipoles, which is essential for determining the mesoscopic physical\nproperties of non-collinear magnetic materials.",
        "Modern front-end (FE) development, especially when leveraging the unique\nfeatures of frameworks like React and Vue, presents distinctive challenges.\nThese include managing modular architectures, ensuring synchronization between\ndata and visual outputs for declarative rendering, and adapting reusable\ncomponents to various scenarios. Such complexities make it particularly\ndifficult for state-of-the-art large vision-language models (VLMs) to generate\naccurate and functional code directly from design images. To address these\nchallenges, we propose a reflective agentic workflow that synthesizes\nhigh-quality image-text data to capture the diverse characteristics of FE\ndevelopment. This workflow automates the extraction of\nself-contained\\footnote{A \\textbf{self-contained} code snippet is one that\nencapsulates all necessary logic, styling, and dependencies, ensuring it\nfunctions independently without requiring external imports or context.} code\nsnippets from real-world projects, renders the corresponding visual outputs,\nand generates detailed descriptions that link design elements to functional\ncode. To further expand the scope and utility of the synthesis, we introduce\nthree data synthesis strategies: Evolution-based synthesis, which enables\nscalable and diverse dataset expansion; Waterfall-Model-based synthesis, which\ngenerates logically coherent code derived from system requirements; and\nAdditive Development synthesis, which iteratively increases the complexity of\nhuman-authored components. We build a large vision-language model, Flame,\ntrained on the synthesized datasets and demonstrate its effectiveness in\ngenerating React code via the $\\text{pass}@k$ metric. Our results suggest that\na code VLM trained to interpret images before code generation may achieve\nbetter performance.",
        "Building surrogate models with uncertainty quantification capabilities is\nessential for many engineering applications where randomness, such as\nvariability in material properties, is unavoidable. Polynomial Chaos Expansion\n(PCE) is widely recognized as a to-go method for constructing stochastic\nsolutions in both intrusive and non-intrusive ways. Its application becomes\nchallenging, however, with complex or high-dimensional processes, as achieving\naccuracy requires higher-order polynomials, which can increase computational\ndemands and or the risk of overfitting. Furthermore, PCE requires specialized\ntreatments to manage random variables that are not independent, and these\ntreatments may be problem-dependent or may fail with increasing complexity. In\nthis work, we adopt the spectral expansion formalism used in PCE; however, we\nreplace the classical polynomial basis functions with neural network (NN) basis\nfunctions to leverage their expressivity. To achieve this, we propose an\nalgorithm that identifies NN-parameterized basis functions in a purely\ndata-driven manner, without any prior assumptions about the joint distribution\nof the random variables involved, whether independent or dependent. The\nproposed algorithm identifies each NN-parameterized basis function\nsequentially, ensuring they are orthogonal with respect to the data\ndistribution. The basis functions are constructed directly on the joint\nstochastic variables without requiring a tensor product structure. This\napproach may offer greater flexibility for complex stochastic models, while\nsimplifying implementation compared to the tensor product structures typically\nused in PCE to handle random vectors. We demonstrate the effectiveness of the\nproposed scheme through several numerical examples of varying complexity and\nprovide comparisons with classical PCE.",
        "Past millimeter-wave galaxy surveys have probed the brightest starburst\ngalaxies only and suffered heavily from confusion. The interpretation of\nexisting surveys has also been hindered by the lack of reliable redshift\nindicators for measuring distances for the entire sample. Thanks to recent\nadvances in mm-wave detector technologies we can now overcome these\nlimitations, and conduct the first truly volumetric surveys of star-forming\ngalaxies at mm-wavelengths down to the L* luminosities of typical galaxies,\nwith ~1000 redshift slices spanning most of the Cosmic star-forming volume (z ~\n1--12) with nearly uniform mass and luminosity selection. We describe an\ninstrument concept capable of delivering such surveys with the technologies\navailable today, which can be built and operated on a ground-based mm-wave\nfacility in the near future. Such spectrometer cameras can resolve and redshift\nidentify up to to 25,000 star-forming galaxies per year even when operated on a\n10-m class telescope. On a larger aperture it can do the same faster or probe\neven deeper. We propose a loose, open-source collaboration to design, build,\nand operate one or several such cameras through the shared contributions of\nleading experts and telescopes from around the globe.",
        "We estimate the local laws of the distribution of the middle prime factor of\nan integer, defined according to multiplicity or not. An asymptotic estimate\nwith effective remainder is provided for a wide range of values. In particular\nthis enables to precisely describe the phase transition occurring in the\nrelevant distribution.",
        "We consider a generalized Derrida-Retaux model on a Galton-Watson tree with a\ngeometric offspring distribution. For a class of recursive systems, including\nthe Derrida-Retaux model with either a geometric or exponential initial\ndistribution, we characterize the critical curve using an involution-type\nequation and prove that the free energy satisfies the Derrida-Retaux\nconjecture.",
        "Recent advancements in large language models (LLMs) have expanded their role\nin robotic task planning. However, while LLMs have been explored for generating\nfeasible task sequences, their ability to ensure safe task execution remains\nunderdeveloped. Existing methods struggle with structured risk perception,\nmaking them inadequate for safety-critical applications where low-latency\nhazard adaptation is required. To address this limitation, we propose a\nGraphormer-enhanced risk-aware task planning framework that combines LLM-based\ndecision-making with structured safety modeling. Our approach constructs a\ndynamic spatio-semantic safety graph, capturing spatial and contextual risk\nfactors to enable online hazard detection and adaptive task refinement. Unlike\nexisting methods that rely on predefined safety constraints, our framework\nintroduces a context-aware risk perception module that continuously refines\nsafety predictions based on real-time task execution. This enables a more\nflexible and scalable approach to robotic planning, allowing for adaptive\nsafety compliance beyond static rules. To validate our framework, we conduct\nexperiments in the AI2-THOR environment. The experiments results validates\nimprovements in risk detection accuracy, rising safety notice, and task\nadaptability of our framework in continuous environments compared to static\nrule-based and LLM-only baselines. Our project is available at\nhttps:\/\/github.com\/hwj20\/GGTP"
      ]
    }
  },
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Medical diffusion on a budget: textual inversion for medical image generation",
    "start_abstract":"Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
      ],
      "abstract":[
        "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Synthesis of omnidirectional path loss model based on directional model\n  and multi-elliptical geometry",
        "Asymmetric Orbifolds, Rank Reduction and Heterotic Islands",
        "Emergence of Order in Chemically Active Droplets: Temporal Dynamics and\n  Collective Behavior",
        "EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for\n  Visual Spatial Tasks",
        "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
        "URL Inspection Tasks: Helping Users Detect Phishing Links in Emails",
        "In Pursuit of Predictive Models of Human Preferences Toward AI Teammates",
        "Topologically protected synchronization in networks",
        "Halfspace Representations of Path Polytopes of Trees",
        "Will AI replace Software Engineers? Do not hold your breath",
        "Effective enhancement of the electron-phonon coupling driven by\n  nonperturbative electronic density fluctuations",
        "PCAP-Backdoor: Backdoor Poisoning Generator for Network Traffic in\n  CPS\/IoT Environments",
        "M-LLM Based Video Frame Selection for Efficient Video Understanding",
        "Which2comm: An Efficient Collaborative Perception Framework for 3D\n  Object Detection",
        "Variational Combinatorial Sequential Monte Carlo for Bayesian\n  Phylogenetics in Hyperbolic Space",
        "Conformal defects and RG flows in ABJM",
        "Output-Feedback Full-State Targeting Model Predictive Control for\n  Station-Keeping on Near-Rectilinear Halo Orbits",
        "Digital Phenotyping for Adolescent Mental Health: A Feasibility Study\n  Employing Machine Learning to Predict Mental Health Risk From Active and\n  Passive Smartphone Data",
        "Panopticon: Advancing Any-Sensor Foundation Models for Earth Observation",
        "Unsupervised Domain Adaptation with Dynamic Clustering and Contrastive\n  Refinement for Gait Recognition",
        "StrNim: a variant of Nim played on strings",
        "Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes",
        "GHOST 2.0: generative high-fidelity one shot transfer of heads",
        "Predicting the cryogenic performance of superconducting detectors by\n  their visual properties",
        "Dispersive and Strichartz estimates for Dirac equation in a cosmic\n  string spacetime",
        "DFT+DMFT study on pressure-induced valence instability of CeCoSi",
        "Quantum computation via Floquet-tailored Rydberg interactions",
        "Computational Discovery of Chiasmus in Ancient Religious Text",
        "Temporal Logic Guided Safe Navigation for Autonomous Vehicles"
      ],
      "abstract":[
        "Millimeter wave (mmWave) technology offers high throughput but has a limited\nradio range, necessitating the use of directional antennas or beamforming\nsystems such as massive MIMO. Path loss (PL) models using narrow-beam antennas\nare known as directional models, while those using omnidirectional antennas are\nreferred to as omnidirectional models. To standardize the analysis,\nomnidirectional PL models for mmWave ranges have been introduced, including TR\n38.901 by 3GPP, which is based on measurements from directional antennas.\nHowever, synthesizing these measurements can be complex and time-consuming.\nThis study proposes a numerical approach to derive an omnidirectional model\nfrom directional data using multi-elliptical geometry. We assessed the\neffectiveness of this method against existing PL models for mmWaves that are\navailable in the literature.",
        "We consider toroidal asymmetric orbifolds of the heterotic string preserving\nall 16 supercharges, developing a general formalism to study components of the\nmoduli space characterized by rank reduction of the gauge group. In particular\nwe construct six- and four-dimensional heterotic islands with no massless\nmoduli other than the dilaton. The formalism involves the Leech lattice, its\nautomorphisms and their corresponding invariant and normal, or coinvariant,\nsublattices.",
        "Collective behaviors such as swarming, chemical signaling, and clustering are\nfundamental to biological microorganisms, enabling hierarchical colony\nformation, coordinated motion, and enhanced nutrient accessibility crucial for\ntheir survival. Over the past few decades, extensive research has been\ndedicated to unraveling the mechanisms underlying these diverse collective\npatterns through experimental model systems. Among these, active droplets have\nemerged as valuable synthetic analogs, effectively replicating key biological\nattributes and serving as ideal platforms for investigating collective\nphenomena. This research explores the collective behavior of\n4-Cyano-4-pentyl-biphenyl (5CB) oil droplets across varying P\\'eclet ($Pe$)\nnumbers. At high $Pe$, droplets exhibit a pusher mode of propulsion and form\ndynamic chain-like patterns. Decreasing $Pe$ enhances repulsive interactions\namong droplets, resulting in the inhibition of clustering. In the low $Pe$\nregime, their repulsive interactions predominated by chemical field lead to the\nemergence of an ordered structure. Furthermore, we illustrate how active\ndroplets efficiently navigate within a soft structured environment. These\nfindings contribute to our comprehension of self-organized phenomena in active\nmatter systems and provide insights for designing strategies for controlled\nlocomotion in intricate fluidic environments.",
        "While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon.",
        "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
        "The most widespread type of phishing attack involves email messages with\nlinks pointing to malicious content. Despite user training and the use of\ndetection techniques, these attacks are still highly effective. Recent studies\nshow that it is user inattentiveness, rather than lack of education, that is\none of the key factors in successful phishing attacks. To this end, we develop\na novel phishing defense mechanism based on URL inspection tasks: small\nchallenges (loosely inspired by CAPTCHAs) that, to be solved, require users to\ninteract with, and understand, the basic URL structure. We implemented and\nevaluated three tasks that act as ``barriers'' to visiting the website: (1)\ncorrect click-selection from a list of URLs, (2) mouse-based highlighting of\nthe domain-name URL component, and (3) re-typing the domain-name. These tasks\nfollow best practices in security interfaces and warning design.\n  We assessed the efficacy of these tasks through an extensive on-line user\nstudy with 2,673 participants from three different cultures, native languages,\nand alphabets. Results show that these tasks significantly decrease the rate of\nsuccessful phishing attempts, compared to the baseline case. Results also\nshowed the highest efficacy for difficult URLs, such as typo-squats, with which\nparticipants struggled the most. This highlights the importance of (1) slowing\ndown users while focusing their attention and (2) helping them understand the\nURL structure (especially, the domain-name component thereof) and matching it\nto their intent.",
        "We seek measurable properties of AI agents that make them better or worse\nteammates from the subjective perspective of human collaborators. Our\nexperiments use the cooperative card game Hanabi -- a common benchmark for\nAI-teaming research. We first evaluate AI agents on a set of objective metrics\nbased on task performance, information theory, and game theory, which are\nmeasurable without human interaction. Next, we evaluate subjective human\npreferences toward AI teammates in a large-scale (N=241) human-AI teaming\nexperiment. Finally, we correlate the AI-only objective metrics with the human\nsubjective preferences. Our results refute common assumptions from prior\nliterature on reinforcement learning, revealing new correlations between AI\nbehaviors and human preferences. We find that the final game score a human-AI\nteam achieves is less predictive of human preferences than esoteric measures of\nAI action diversity, strategic dominance, and ability to team with other AI. In\nthe future, these correlations may help shape reward functions for training\nhuman-collaborative AI.",
        "In a graph, we say that two nodes are topologically equivalent if their sets\nof first neighbors, excluding the two nodes, coincide. We prove that\nnonlinearly coupled oscillators located on a group of topologically equivalent\nnodes can get easily synchronized when the group forms a fully connected\nsubgraph (or combinations of these), regardless of the status of all the other\noscillators. More generally, any change occurring in the inner part of the\nremainder of the graph will not alter the synchronization status of the group.\nTypically, the group can synchronize when $k^{(\\mathrm{OUT})}\\leq\nk^{(\\mathrm{IN})}$, $k^{(\\mathrm{IN})}$ and $k^{(\\mathrm{OUT})}$ being the\ncommon internal and outgoing degree of each node in the group, respectively.\nSimulations confirm our analysis and suggest that groups of topologically\nequivalent nodes play the role of independent pacemakers.",
        "Given a tree $T$, its path polytope is the convex hull of the edge indicator\nvectors for the paths between any two distinct leaves in $T$. These polytopes\narise naturally in polyhedral geometry and applications, such as phylogenetics,\ntropical geometry, and algebraic statistics. We provide a minimal halfspace\nrepresentation of these polytopes. The construction is made inductively using\ntoric fiber products.",
        "Artificial Intelligence (AI) technology such as Large Language Models (LLMs)\nhave become extremely popular in creating code. This has led to the conjecture\nthat future software jobs will be exclusively conducted by LLMs, and the\nsoftware industry will cease to exist. But software engineering is much more\nthan producing code -- notably, \\emph{maintaining} large software and keeping\nit reliable is a major part of software engineering, which LLMs are not yet\ncapable of.",
        "We present a dynamical mean-field study of the nonperturbative electronic\nmechanisms, which may lead to significant enhancements of the electron-phonon\ncoupling in correlated electron systems. Analyzing the effects of electronic\ncorrelations on the lowest-order electron-phonon processes, we show that in the\nproximity of the Mott metal-to-insulator transition of the doped square lattice\nHubbard model, where the isothermal charge response becomes particularly large\nat small momenta, the coupling of electrons to the lattice is strongly\nincreased. This, in turn, induces significant corrections to both the\nelectronic self-energy and phonon-mediated pairing interaction, indicating the\npossible onset of a strong interplay between lattice and electronic degrees of\nfreedom even for small values of the bare electron-phonon coupling.",
        "The rapid expansion of connected devices has made them prime targets for\ncyberattacks. To address these threats, deep learning-based, data-driven\nintrusion detection systems (IDS) have emerged as powerful tools for detecting\nand mitigating such attacks. These IDSs analyze network traffic to identify\nunusual patterns and anomalies that may indicate potential security breaches.\nHowever, prior research has shown that deep learning models are vulnerable to\nbackdoor attacks, where attackers inject triggers into the model to manipulate\nits behavior and cause misclassifications of network traffic. In this paper, we\nexplore the susceptibility of deep learning-based IDS systems to backdoor\nattacks in the context of network traffic analysis. We introduce\n\\texttt{PCAP-Backdoor}, a novel technique that facilitates backdoor poisoning\nattacks on PCAP datasets. Our experiments on real-world Cyber-Physical Systems\n(CPS) and Internet of Things (IoT) network traffic datasets demonstrate that\nattackers can effectively backdoor a model by poisoning as little as 1\\% or\nless of the entire training dataset. Moreover, we show that an attacker can\nintroduce a trigger into benign traffic during model training yet cause the\nbackdoored model to misclassify malicious traffic when the trigger is present.\nFinally, we highlight the difficulty of detecting this trigger-based backdoor,\neven when using existing backdoor defense techniques.",
        "Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising\nresults in video reasoning. Popular Multi-Modal Large Language Model (M-LLM)\nframeworks usually apply naive uniform sampling to reduce the number of video\nframes that are fed into an M-LLM, particularly for long context videos.\nHowever, it could lose crucial context in certain periods of a video, so that\nthe downstream M-LLM may not have sufficient visual information to answer a\nquestion. To attack this pain point, we propose a light-weight M-LLM -based\nframe selection method that adaptively select frames that are more relevant to\nusers' queries. In order to train the proposed frame selector, we introduce two\nsupervision signals (i) Spatial signal, where single frame importance score by\nprompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by\nprompting Large Language Model (LLM) using the captions of all frame\ncandidates. The selected frames are then digested by a frozen downstream video\nM-LLM for visual reasoning and question answering. Empirical results show that\nthe proposed M-LLM video frame selector improves the performances various\ndownstream video Large Language Model (video-LLM) across medium (ActivityNet,\nNExT-QA) and long (EgoSchema, LongVideoBench) context video question answering\nbenchmarks.",
        "Collaborative perception allows real-time inter-agent information exchange\nand thus offers invaluable opportunities to enhance the perception capabilities\nof individual agents. However, limited communication bandwidth in practical\nscenarios restricts the inter-agent data transmission volume, consequently\nresulting in performance declines in collaborative perception systems. This\nimplies a trade-off between perception performance and communication cost. To\naddress this issue, we propose Which2comm, a novel multi-agent 3D object\ndetection framework leveraging object-level sparse features. By integrating\nsemantic information of objects into 3D object detection boxes, we introduce\nsemantic detection boxes (SemDBs). Innovatively transmitting these\ninformation-rich object-level sparse features among agents not only\nsignificantly reduces the demanding communication volume, but also improves 3D\nobject detection performance. Specifically, a fully sparse network is\nconstructed to extract SemDBs from individual agents; a temporal fusion\napproach with a relative temporal encoding mechanism is utilized to obtain the\ncomprehensive spatiotemporal features. Extensive experiments on the V2XSet and\nOPV2V datasets demonstrate that Which2comm consistently outperforms other\nstate-of-the-art methods on both perception performance and communication cost,\nexhibiting better robustness to real-world latency. These results present that\nfor multi-agent collaborative 3D object detection, transmitting only\nobject-level sparse features is sufficient to achieve high-precision and robust\nperformance.",
        "Hyperbolic space naturally encodes hierarchical structures such as\nphylogenies (binary trees), where inward-bending geodesics reflect paths\nthrough least common ancestors, and the exponential growth of neighborhoods\nmirrors the super-exponential scaling of topologies. This scaling challenge\nlimits the efficiency of Euclidean-based approximate inference methods.\nMotivated by the geometric connections between trees and hyperbolic space, we\ndevelop novel hyperbolic extensions of two sequential search algorithms:\nCombinatorial and Nested Combinatorial Sequential Monte Carlo (\\textsc{Csmc}\nand \\textsc{Ncsmc}). Our approach introduces consistent and unbiased\nestimators, along with variational inference methods (\\textsc{H-Vcsmc} and\n\\textsc{H-Vncsmc}), which outperform their Euclidean counterparts. Empirical\nresults demonstrate improved speed, scalability and performance in\nhigh-dimensional phylogenetic inference tasks.",
        "Defects play a central role in many contexts, from condensed matter to\nquantum gravity. The situations in which the bulk theory is conformal and the\ndefect inherits part of this symmetry -- the so-called defect conformal field\ntheories (dCFTs) -- have recently received a lot of attention, also thanks to\nnew powerful methods to tackle them, like supersymmetric localization,\nintegrability or the bootstrap. A dCFT may be deformed by turning on marginally\nrelevant operators, which trigger RG flows connecting different fixed points. A\nnatural arena where this phenomenon can be explored are 3-dimensional\nChern-Simons theories coupled to matter. These are in fact known to display a\nplethora of Wilson loops that can be used to define 1-dimensional dCFTs living\non their contours. Here we discuss a few examples from an intricate web of RG\nflows connecting the dCFTs defined on the BPS and non-BPS Wilson loops of ABJM\ntheory. We compute the anomalous dimensions of the deforming operators,\nestablish g-theorems along the flows, and also discuss the role played by\ncohomological anomalies and framing.",
        "We develop a model predictive control (MPC) policy for station-keeping (SK)\non a Near-Rectilinear Halo Orbit (NRHO). The proposed policy achieves\nfull-state tracking of a reference NRHO via a two-maneuver control horizon\nplaced one revolution apart. Our method abides by the typical mission\nrequirement that at most one maneuver is used for SK during each NRHO\nrevolution. Simultaneously, the policy has sufficient controllability for\nfull-state tracking, making it immune to phase deviation issues in the\nalong-track direction of the reference NRHO, a common drawback of existing SK\nmethods with a single maneuver per revolution. We report numerical simulations\nwith a navigation filter to demonstrate the MPC's performance with output\nfeedback. Our approach successfully maintains the spacecraft's motion in the\nvicinity of the reference in both space and phase, with tighter tracking than\nstate-of-the-art SK methods and comparable delta-V performance.",
        "Background: Adolescents are particularly vulnerable to mental disorders, with\nover 75% of cases manifesting before the age of 25. Research indicates that\nonly 18 to 34% of young people experiencing high levels of depression or\nanxiety symptoms seek support. Digital tools leveraging smartphones offer\nscalable and early intervention opportunities. Objective: Using a novel machine\nlearning framework, this study evaluated the feasibility of integrating active\nand passive smartphone data to predict mental disorders in non-clinical\nadolescents. Specifically, we investigated the utility of the Mindcraft app in\npredicting risks for internalising and externalising disorders, eating\ndisorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean\nage 16.1 years) were recruited from three London schools. Participants\ncompleted the Strengths and Difficulties Questionnaire, the Eating Disorders-15\nQuestionnaire, Sleep Condition Indicator Questionnaire and indicated the\npresence\/absence of suicidal ideation. They used the Mindcraft app for 14 days,\ncontributing active data via self-reports and passive data from smartphone\nsensors. A contrastive pretraining phase was applied to enhance user-specific\nfeature stability, followed by supervised fine-tuning. The model evaluation\nemployed leave-one-subject-out cross-validation using balanced accuracy as the\nprimary metric. Results: The integration of active and passive data achieved\nsuperior performance compared to individual data sources, with mean balanced\naccuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal\nideation and 0.70 for eating disorders. The contrastive learning framework\nstabilised daily behavioural representations, enhancing predictive robustness.\nThis study demonstrates the potential of integrating active and passive\nsmartphone data with advanced machine-learning techniques for predicting mental\nhealth risks.",
        "Earth observation (EO) data features diverse sensing platforms with varying\nspectral bands, spatial resolutions, and sensing modalities. While most prior\nwork has constrained inputs to fixed sensors, a new class of any-sensor\nfoundation models able to process arbitrary sensors has recently emerged.\nContributing to this line of work, we propose Panopticon, an any-sensor\nfoundation model built on the DINOv2 framework. We extend DINOv2 by (1)\ntreating images of the same geolocation across sensors as natural\naugmentations, (2) subsampling channels to diversify spectral input, and (3)\nadding a cross attention over channels as a flexible patch embedding mechanism.\nBy encoding the wavelength and modes of optical and synthetic aperture radar\nsensors, respectively, Panopticon can effectively process any combination of\narbitrary channels. In extensive evaluations, we achieve state-of-the-art\nperformance on GEO-Bench, especially on the widely-used Sentinel-1 and\nSentinel-2 sensors, while out-competing other any-sensor models, as well as\ndomain adapted fixed-sensor models on unique sensor configurations. Panopticon\nenables immediate generalization to both existing and future satellite\nplatforms, advancing sensor-agnostic EO.",
        "Gait recognition is an emerging identification technology that distinguishes\nindividuals at long distances by analyzing individual walking patterns.\nTraditional techniques rely heavily on large-scale labeled datasets, which\nincurs high costs and significant labeling challenges. Recently, researchers\nhave explored unsupervised gait recognition with clustering-based unsupervised\ndomain adaptation methods and achieved notable success. However, these methods\ndirectly use pseudo-label generated by clustering and neglect pseudolabel noise\ncaused by domain differences, which affects the effect of the model training\nprocess. To mitigate these issues, we proposed a novel model called GaitDCCR,\nwhich aims to reduce the influence of noisy pseudo labels on clustering and\nmodel training. Our approach can be divided into two main stages: clustering\nand training stage. In the clustering stage, we propose Dynamic Cluster\nParameters (DCP) and Dynamic Weight Centroids (DWC) to improve the efficiency\nof clustering and obtain reliable cluster centroids. In the training stage, we\nemploy the classical teacher-student structure and propose Confidence-based\nPseudo-label Refinement (CPR) and Contrastive Teacher Module (CTM) to encourage\nnoisy samples to converge towards clusters containing their true identities.\nExtensive experiments on public gait datasets have demonstrated that our simple\nand effective method significantly enhances the performance of unsupervised\ngait recognition, laying the foundation for its application in the\nreal-world.The code is available at https:\/\/github.com\/YanSun-github\/GaitDCCR",
        "We propose a variant of Nim, named StrNim. Whereas a position in Nim is a\ntuple of non-negative integers, that in StrNim is a string, a sequence of\ncharacters. In every turn, each player shrinks the string, by removing a\nsubstring repeating the same character. As a first study on this new game, we\npresent some sufficient conditions for the positions to be P-positions.",
        "We introduce abstract rendering, a method for computing a set of images by\nrendering a scene from a continuously varying range of camera positions. The\nresulting abstract image-which encodes an infinite collection of possible\nrenderings-is represented using constraints on the image matrix, enabling\nrigorous uncertainty propagation through the rendering process. This capability\nis particularly valuable for the formal verification of vision-based autonomous\nsystems and other safety-critical applications. Our approach operates on\nGaussian splat scenes, an emerging representation in computer vision and\nrobotics. We leverage efficient piecewise linear bound propagation to abstract\nfundamental rendering operations, while addressing key challenges that arise in\nmatrix inversion and depth sorting-two operations not directly amenable to\nstandard approximations. To handle these, we develop novel linear relational\nabstractions that maintain precision while ensuring computational efficiency.\nThese abstractions not only power our abstract rendering algorithm but also\nprovide broadly applicable tools for other rendering problems. Our\nimplementation, AbstractSplat, is optimized for scalability, handling up to\n750k Gaussians while allowing users to balance memory and runtime through tile\nand batch-based computation. Compared to the only existing abstract image\nmethod for mesh-based scenes, AbstractSplat achieves 2-14x speedups while\npreserving precision. Our results demonstrate that continuous camera motion,\nrotations, and scene variations can be rigorously analyzed at scale, making\nabstract rendering a powerful tool for uncertainty-aware vision applications.",
        "While the task of face swapping has recently gained attention in the research\ncommunity, a related problem of head swapping remains largely unexplored. In\naddition to skin color transfer, head swap poses extra challenges, such as the\nneed to preserve structural information of the whole head during synthesis and\ninpaint gaps between swapped head and background. In this paper, we address\nthese concerns with GHOST 2.0, which consists of two problem-specific modules.\nFirst, we introduce enhanced Aligner model for head reenactment, which\npreserves identity information at multiple scales and is robust to extreme pose\nvariations. Secondly, we use a Blender module that seamlessly integrates the\nreenacted head into the target background by transferring skin color and\ninpainting mismatched regions. Both modules outperform the baselines on the\ncorresponding tasks, allowing to achieve state of the art results in head\nswapping. We also tackle complex cases, such as large difference in hair styles\nof source and target. Code is available at\nhttps:\/\/github.com\/ai-forever\/ghost-2.0",
        "The testing and quality assurance of cryogenic superconducting detectors is a\ntime- and labor-intensive process. As experiments deploy increasingly larger\narrays of detectors, new methods are needed for performing this testing\nquickly. Here, we propose a process for flagging under-performing detector\nwafers before they are ever tested cryogenically. Detectors are imaged under an\noptical microscope, and computer vision techniques are used to analyze the\nimages, searching for visual defects and other predictors of poor performance.\nPipeline performance is verified via a suite of images with simulated defects,\nyielding a detection accuracy of 98.6%. Lastly, results from running the\npipeline on prototype microwave kinetic inductance detectors from the planned\nSPT-3G+ experiment are presented.",
        "In this work we study the Dirac equation on the cosmic string background,\nwhich models a one--dimensional topological defect in the spacetime. We first\ndefine the Dirac operator in this setting, classifying all of its selfadjoint\nextensions, and we give an explicit kernel for the propagator. Secondly, we\nprove dispersive estimates for the flow, with and without weights. Finally, we\nprove Strichartz estimates for the flow in a sharp restricted set of indices,\nwhich are different from the classical Euclidean ones.",
        "Rare-earth compounds RCoSi exhibit unique properties, with distinct\nstructural behaviors depending on whether R is a light, middle or heavy\nrare-earth element. Among them, CeCoSi undergoes a structural phase transition\nunder high pressure, with the phase transition pressure increasing as\ntemperature rises. Some experimental studies suggest that the transition is\nclosely related to the behavior of Ce-4f electrons. In this work, we\nsystematically studied the evolution of the electronic structure of CeCoSi with\ntemperature and pressure. First, we used the DFT+DMFT to calculate the\nenergy-volume curve of CeCoSi, which was in good agreement with the\nexperimental results and far superior to the DFT method. Next, we studied the\nelectronic structure of CeCoSi under different pressures and temperatures using\nDFT+DMFT. Our results show that CeCoSi is a Kondo metal with hybridization of\nCe-4f and Co-3d. As pressure increases, the renormalization factor Z of\nCe-4f5\/2 increases, the occupancy number of Ce-4f electrons decreases, and\nCeCoSi transitions to a mixed-valence state at ~5.5 GPa in 100 K. The pressure\nof the quantum phase transition PQ is slightly higher than the experimentally\nobserved structural phase transition pressure PS, and the PQ increases with\nincreasing temperature, which is consistent with the behavior of PS in\nexperiment. In addition, the hybridization strength of Ce-4f in the\nmixed-valence state is significantly greater than in the Kondo metal state. Our\nresults suggest that the valence instability of Ce-4f is the cause of the\nstructural phase transition. As pressure increases, Ce-4f electrons delocalize\nand CeCoSi transitions to mixed-valence state. This valence instability may\ncause redistribution of electron density, thus inducing a structural phase\ntransition. Our work reveals the cause of the structural phase transition of\nCeCoSi under high pressure.",
        "Rydberg atoms stand out as a highly promising platform for realizing quantum\ncomputation with significant advantages in constructing high-fidelity quantum\ngates. Floquet frequency modulation (FFM), in Rydberg-atom systems, provides a\nunique platform for achieving precise quantum control and uncovering exotic\nphysical phenomena, paving the way for innovative methodologies in quantum\ndynamics research. This work introduces a method to realize controlled\narbitrary phase gates in Rydberg atoms by manipulating system dynamics using\nFFM. Notably, this method eliminates the need for laser addressing of\nindividual atoms, significantly enhancing convenience for future practical\napplications. Furthermore, this approach can be integrated with soft quantum\ncontrol strategies to enhance the fidelity and robustness of the resultant\ncontrolled-phase gates. Finally, as an example, this methodology is applied in\nGrover-Long algorithm to search target items with zero failure rate,\ndemonstrating its substantial significance for future quantum information\nprocessing applications. This work leveraging Rydberg atoms and Floquet\nfrequency modulation may herald a new era of scalable and reliable quantum\ncomputing.",
        "Chiasmus, a debated literary device in Biblical texts, has captivated mystics\nwhile sparking ongoing scholarly discussion. In this paper, we introduce the\nfirst computational approach to systematically detect chiasmus within Biblical\npassages. Our method leverages neural embeddings to capture lexical and\nsemantic patterns associated with chiasmus, applied at multiple levels of\ntextual granularity (half-verses, verses). We also involve expert annotators to\nreview a subset of the detected patterns. Despite its computational efficiency,\nour method achieves robust results, with high inter-annotator agreement and\nsystem precision@k of 0.80 at the verse level and 0.60 at the half-verse level.\nWe further provide a qualitative analysis of the distribution of detected\nchiasmi, along with selected examples that highlight the effectiveness of our\napproach.",
        "Safety verification for autonomous vehicles (AVs) and ground robots is\ncrucial for ensuring reliable operation given their uncertain environments.\nFormal language tools provide a robust and sound method to verify safety rules\nfor such complex cyber-physical systems. In this paper, we propose a hybrid\napproach that combines the strengths of formal verification languages like\nLinear Temporal Logic (LTL) and Signal Temporal Logic (STL) to generate safe\ntrajectories and optimal control inputs for autonomous vehicle navigation. We\nimplement a symbolic path planning approach using LTL to generate a formally\nsafe reference trajectory. A mixed integer linear programming (MILP) solver is\nthen used on this reference trajectory to solve for the control inputs while\nsatisfying the state, control and safety constraints described by STL. We test\nour proposed solution on two environments and compare the results with popular\npath planning algorithms. In contrast to conventional path planning algorithms,\nour formally safe solution excels in handling complex specification scenarios\nwhile ensuring both safety and comparable computation times."
      ]
    }
  },
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey",
    "start_abstract":"Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Medical diffusion on a budget: textual inversion for medical image generation"
      ],
      "abstract":[
        "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Bayesian optimisation of poloidal field coil positions in tokamaks",
        "Quantum metric non-linear Hall effect in an antiferromagnetic\n  topological insulator thin-film EuSn2As2",
        "Monotone conservative strategies in data assimilation",
        "Swimming mode determines how well mesoscale swimmers shield their odor\n  in turbulence",
        "On the Picard numbers of moduli spaces of one-dimensional sheaves on\n  surfaces",
        "Regular evolution algebras are closed under subalgebras",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "The unification of gravity and the spin-1 field",
        "Void spin distribution as a powerful probe of $\\sigma_{8}$",
        "Decay of mass for a semilinear heat equation with mixed local-nonlocal\n  operators",
        "PUATE: Semiparametric Efficient Average Treatment Effect Estimation from\n  Treated (Positive) and Unlabeled Units",
        "The TES-based Cryogenic AntiCoincidence Detector of ATHENA X-IFU:\n  Validation of the thermal end-to-end simulator towards the updated\n  Demonstration Model (DM 1.1)",
        "OGBoost: A Python Package for Ordinal Gradient Boosting",
        "Regulation of Algorithmic Collusion, Refined: Testing Pessimistic\n  Calibrated Regret",
        "Spectral synthesis for exponentials in weighted $L^2$-spaces",
        "Bridging the Data Gap in AI Reliability Research and Establishing\n  DR-AIR, a Comprehensive Data Repository for AI Reliability",
        "Towards an AI co-scientist",
        "Fiducial Inference for Random-Effects Calibration Models: Advancing\n  Reliable Quantification in Environmental Analytical Chemistry",
        "Wavefront shaping enhanced nano-optomechanics down to the quantum\n  precision limit",
        "On uniqueness of functions in the extended Selberg class with moving\n  targets",
        "On refactorization problems and rational Lax matrices of quadrirational\n  Yang-Baxter maps",
        "Effects of valley splitting on resonant-tunneling readout of spin qubits",
        "Proposal of the KOTO II experiment",
        "A unified approach to hypergeometric class functions",
        "Hardy-Hilbert type inequalities on homogeneous groups-An introduction\n  and generalization to the kernel case",
        "Exploring the mechanism of phase transitions between the hexagonal\n  close-packed and the cuboidal structures",
        "$K$-theoretic pullbacks for Lagrangians on derived critical loci",
        "$^{56}$Ni production in long-lived binary neutron star merger remnants",
        "The EXO-UV program: lastest advances of experimental studies to\n  investigate the biological impact of UV radiation on exoplanets"
      ],
      "abstract":[
        "The tokamak is a world-leading concept for producing sustainable energy via\nmagnetically-confined nuclear fusion. Identifying where to position the magnets\nwithin a tokamak, specifically the poloidal field (PF) coils, is a design\nproblem which requires balancing a number of competing economic, physical, and\nengineering objectives and constraints. In this paper, we show that\nmulti-objective Bayesian optimisation (BO), an iterative optimisation technique\nutilising probabilistic machine learning models, can effectively explore this\ncomplex design space and return several optimal PF coil sets. These solutions\nspan the Pareto front, a subset of the objective space that optimally satisfies\nthe specified objective functions. We outline an easy-to-use BO framework and\ndemonstrate that it outperforms alternative optimisation techniques while using\nsignificantly fewer computational resources. Our results show that BO is a\npromising technique for fusion design problems that rely on computationally\ndemanding high-fidelity simulations.",
        "The quantum geometric structure of electrons introduces fundamental insights\ninto understanding quantum effects in materials. One notable manifestation is\nthe non-linear Hall effect (NLHE), which has drawn considerable interest for\nits potential to overcome the intrinsic limitations of semiconductor diodes at\nlow input power and high frequency. In this study, we investigate NLHE stemming\nfrom the real part of the quantum geometric tensor, specifically the quantum\nmetric, in an antiferromagnetic topological material, EuSn2As2, using density\nfunctional theory. Our calculations predict a remarkable NLHE arising from a\nsymmetry-protected, single Type-II surface Dirac cone in the\neven-numbered-layer two-dimensional slab thin-film, yielding a non-linear Hall\nconductivity exceeding 20 mA\/V2-an order of magnitude larger than previously\nreported. This single Dirac band dispersion represents the simplest model for\ngenerating NLHE, positioning the EuSn2As2 thin-film as a hydrogen atom for NLHE\nsystems. Additionally, we observe NLHE from band-edge states near the Fermi\nlevel. Our findings also reveal that 30% phosphorus (P) doping can double the\nnon-linear Hall conductivity. With its substantial and tunable NLHE, EuSn2As2\nthin-films present promising applications in antiferromagnetic spintronics and\nrectification devices.",
        "This paper studies whether numerically preserving monotonic properties can\noffer modelling advantages in data assimilation, particularly when the signal\nor data is a realization of a stochastic partial differential equation (SPDE)\nor partial differential equation (PDE) with a monotonic property. We\ninvestigate the combination of stochastic Strong Stability Preserving (SSP)\ntime-stepping, nonlinear solving strategies and data assimilation. Experimental\nresults indicate that a particle filter whose ensemble members are solved\nmonotonically can increase forecast skill when the reference data (not\nnecessarily observations) also has a monotone property. Additionally, more\nadvanced techniques used to avoid the degeneracy of the filter\n(tempering-jittering) are shown to be compatible with a conservative monotone\napproach.",
        "Marine organisms manipulate their surrounding flow through their swimming\ndynamics, which affects the transport of their own odor cues. We demonstrate by\ndirect numerical simulations how a group of mesoscale swimmers immersed in a\nturbulent flow alters the shape of the odor plume they release in the water.\nOdor mixing is enhanced by increased velocity fluctuations and a\nswimmer-induced flow circulation which widen the odor plume at close range\nwhile speeding up dilution of the chemical trace. Beyond a short-range increase\nin the likelihood of being detected, swimming considerably reduces detections\nwith effects that can persist at distances of the order of ten times the size\nof the group or more. We find that puller-like swimmers are more effective at\nolfactory shielding than pusher-like swimmers. We trace this difference back to\nthe dynamics at the swimmer location, which tends to trap odor at the source\nfor pushers and to dilute it for pullers. Olfactory shielding is robust to\nchanges in the conditions, and is more pronounced for weak turbulent Reynolds\nnumbers and large swimmer Reynolds numbers. Our results suggest that olfactory\nshielding may play a role in the emergence of different swimming modalities by\nmarine organisms.",
        "Motivated by asymptotic phenomena of moduli spaces of higher rank stable\nsheaves on algebraic surfaces, we study the Picard number of the moduli space\nof one-dimensional stable sheaves supported in a sufficiently positive divisor\nclass on a surface. We give an asymptotic lower bound of the Picard number in\ngeneral. In some special cases, we show that this lower bound is attained based\non the geometry of moduli spaces of stable pairs and relative Hilbert schemes\nof points. Additionally, we discuss several related questions and provide\nexamples where the asymptotic irreducibility of the moduli space fails,\nhighlighting a notable distinction from the higher rank case.",
        "The main goal of this note is to show that subalgebras of regular evolution\nalgebras are themselves evolution algebras. This allows us to assume, without\nloss of generality, that every subalgebra in the regular setting has a basis\nconsisting of vectors with disjoint supports. Finally, we use this result to\ncharacterise the existence of codimension-one subalgebras in regular evolution\nalgebras.",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "Unifying the massive spin-1 field with gravity requires the implementation of\na regular vector field that satisfies the spin-1 Proca equation and is a\nfundamental part of the spacetime metric. That vector field is one of the pair\nof vectors in the line element field (\\textbf{X},-\\textbf{X}), which is\nparamount to the existence of all Lorentzian metrics and Modified General\nRelativity (MGR). Symmetrization of the spin-1 Klein-Gordon equation in a\ncurved Lorentzian spacetime introduces the Lie derivative of the metric along\nthe flow of one of the regular vectors in the line element field. The Proca\nequation in curved spacetime can then be described geometrically in terms of\nthe line element vector, the Lie derivative of the Lorentzian metric, and the\nRicci tensor, which unifies gravity and the spin-1 field. Related issues\nconcerning charge conservation and the Lorenz constraint, singularities in a\nspherically symmetric curved spacetime, and geometrical implications of MGR to\nquantum theory are discussed. A geometrical unification of gravity with quantum\nfield theory is presented.",
        "We present a numerical proof of the concept that the void spin distributions\ncan in principle provide a tight constraint on the amplitude of matter density\nfluctuation on the scale of $8\\,h^{-1}{\\rm Mpc}$ ($\\sigma_{8}$) without being\nseverely deteriorated by the degeneracies of $\\sigma_{8}$ with cold dark matter\ndensity parameter multiplied by the dimensionless Hubble parameter square\n($\\Omega_{\\rm cdm}h^{2}$), total neutrino mass ($M_{\\nu}$) and dark energy\nequation of state ($w$). Applying the Void-Finder algorithm to a total of $15$\nAbacusSummit $N$-body simulations of $15$ different cosmological models, we\nidentify the giant voids to measure their spins, defined as the magnitudes of\nrescaled specific angular momenta of void halos. The $15$ cosmologies include\nthe Planck $\\Lambda$CDM and $14$ non-Planck models, each of which differs among\none another only in one of $\\{\\sigma_{8},\\ \\Omega_{\\rm cdm}h^{2},\\ M_{\\nu},\\\nw\\}$. The probability density distribution of void spins is determined for each\nmodel and found to be well approximated by the generalized Gamma distribution\nwith two characteristic parameters, $k$ and $\\theta$. It turns out that the\nbest-fit values of $k$ and $\\theta$ exhibit very sensitive dependences only on\n$\\sigma_{8}$, being almost insensitive to $\\Omega_{\\rm cdm}h^{2}$, $M_{\\nu}$,\n$w$. This exclusive $\\sigma_{8}$-dependence of the void spin distributions is\nconfirmed to be robust against the variation of the mass and number cuts of\nvoid halos. We also test an observational feasibility of estimating the void\nspins from real data on the galaxy redshifts.",
        "In this paper, we are concerned with the Cauchy problem for the\nreaction-diffusion equation $\\partial_t u+t^\\beta\\mathcal{L} u= - h(t)u^p$\nposed on $\\mathbb{R}^N$, driven by the mixed local-nonlocal operator\n$\\mathcal{L}=-\\Delta+(-\\Delta)^{\\alpha\/2}$, $\\alpha\\in(0,2)$, and supplemented\nwith a nonnegative integrable initial data, where $p>1$, $\\beta\\geq 0$, and\n$h:(0,\\infty)\\to(0,\\infty)$ is a locally integrable function. We study the\nlarge time behavior of non-negative solutions and show that the nonlinear term\ndetermines the large time asymptotic for $p\\leq 1+{\\alpha}\/{N(\\beta+1)},$ while\nthe classical\/anomalous diffusion effects win if $p>1+{\\alpha}\/{N(\\beta+1)}$.",
        "The estimation of average treatment effects (ATEs), defined as the difference\nin expected outcomes between treatment and control groups, is a central topic\nin causal inference. This study develops semiparametric efficient estimators\nfor ATE estimation in a setting where only a treatment group and an unknown\ngroup-comprising units for which it is unclear whether they received the\ntreatment or control-are observable. This scenario represents a variant of\nlearning from positive and unlabeled data (PU learning) and can be regarded as\na special case of ATE estimation with missing data. For this setting, we derive\nsemiparametric efficiency bounds, which provide lower bounds on the asymptotic\nvariance of regular estimators. We then propose semiparametric efficient ATE\nestimators whose asymptotic variance aligns with these efficiency bounds. Our\nfindings contribute to causal inference with missing data and weakly supervised\nlearning.",
        "The Cryogenic AntiCoincidence Detector (CryoAC) is a key element of the X-ray\nIntegral Field Unit (X-IFU) on board the future ATHENA X-ray observatory. It is\na TES-based detector designed to reduce the particle background of the\ninstrument, thereby increasing its sensitivity. The detector design is driven\nby an end-to-end simulator which includes the electro-thermal modelling of the\ndetector and the dynamics of its readout chain. Here, we present the\nmeasurements carried out on the last CryoAC single pixel prototype, namely\nDM127, in order to evaluate the critical thermal parameters of the detector and\nconsequently to tune and validate the CryoAC end-to-end simulator.",
        "This paper introduces OGBoost, a scikit-learn-compatible Python package for\nordinal regression using gradient boosting. Ordinal variables (e.g., rating\nscales, quality assessments) lie between nominal and continuous data,\nnecessitating specialized methods that reflect their inherent ordering. Built\non a coordinate-descent approach for optimization and the latent-variable\nframework for ordinal regression, OGBoost performs joint optimization of a\nlatent continuous regression function (functional gradient descent) and a\nthreshold vector that converts the latent continuous value into discrete class\nprobabilities (classical gradient descent). In addition to the stanadard\nmethods for scikit-learn classifiers, the GradientBoostingOrdinal class\nimplements a \"decision_function\" that returns the (scalar) value of the latent\nfunction for each observation, which can be used as a high-resolution\nalternative to class labels for comparing and ranking observations. The class\nhas the option to use cross-validation for early stopping rather than a single\nholdout validation set, a more robust approach for small and\/or imbalanced\ndatasets. Furthermore, users can select base learners with different underlying\nalgorithms and\/or hyperparameters for use throughout the boosting iterations,\nresulting in a `heterogeneous' ensemble approach that can be used as a more\nefficient alternative to hyperparameter tuning (e.g. via grid search). We\nillustrate the capabilities of OGBoost through examples, using the wine quality\ndataset from the UCI respository. The package is available on PyPI and can be\ninstalled via \"pip install ogboost\".",
        "We study the regulation of algorithmic (non-)collusion amongst sellers in\ndynamic imperfect price competition by auditing their data as introduced by\nHartline et al. [2024].\n  We develop an auditing method that tests whether a seller's pessimistic\ncalibrated regret is low. The pessimistic calibrated regret is the highest\ncalibrated regret of outcomes compatible with the observed data. This method\nrelaxes the previous requirement that a pricing algorithm must use\nfully-supported price distributions to be auditable. This method is at least as\npermissive as any auditing method that has a high probability of failing\nalgorithmic outcomes with non-vanishing calibrated regret. Additionally, we\nstrengthen the justification for using vanishing calibrated regret, versus\nvanishing best-in-hindsight regret, as the non-collusion definition, by showing\nthat even without any side information, the pricing algorithms that only\nsatisfy weaker vanishing best-in-hindsight regret allow an opponent to\nmanipulate them into posting supra-competitive prices. This manipulation cannot\nbe excluded with a non-collusion definition of vanishing best-in-hindsight\nregret.\n  We motivate and interpret the approach of auditing algorithms from their data\nas suggesting a per se rule. However, we demonstrate that it is possible for\nalgorithms to pass the audit by pretending to have higher costs than they\nactually do. For such scenarios, the rule of reason can be applied to bound the\nrange of costs to those that are reasonable for the domain.",
        "We prove that for a some natural class of weights $\\K$ and any weighted space\n$$L^2(w) = \\left\\{ f : (-\\pi,\\pi) \\to \\CC \\colon\\int_{-\\pi}^{\\pi} {{|f(t)|^2}\n\\over {w(t)}} dt < \\infty \\right\\},$$ where $w \\in \\K$, there exists a complete\nand minimal system $\\{e^{i\\lambda_nt}\\}_{n\\in \\Z}$ of exponentials, which does\nnot admit spectral synthesis.",
        "Artificial intelligence (AI) technology and systems have been advancing\nrapidly. However, ensuring the reliability of these systems is crucial for\nfostering public confidence in their use. This necessitates the modeling and\nanalysis of reliability data specific to AI systems. A major challenge in AI\nreliability research, particularly for those in academia, is the lack of\nreadily available AI reliability data. To address this gap, this paper focuses\non conducting a comprehensive review of available AI reliability data and\nestablishing DR-AIR: a data repository for AI reliability. Specifically, we\nintroduce key measurements and data types for assessing AI reliability, along\nwith the methodologies used to collect these data. We also provide a detailed\ndescription of the currently available datasets with illustrative examples.\nFurthermore, we outline the setup of the DR-AIR repository and demonstrate its\npractical applications. This repository provides easy access to datasets\nspecifically curated for AI reliability research. We believe these efforts will\nsignificantly benefit the AI research community by facilitating access to\nvaluable reliability data and promoting collaboration across various academic\ndomains within AI. We conclude our paper with a call to action, encouraging the\nresearch community to contribute and share AI reliability data to further\nadvance this critical field of study.",
        "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.",
        "This article addresses calibration challenges in analytical chemistry by\nemploying a random-effects calibration curve model and its generalizations to\ncapture variability in analyte concentrations. The model is motivated by\nspecific issues in analytical chemistry, where measurement errors remain\nconstant at low concentrations but increase proportionally as concentrations\nrise. To account for this, the model permits the parameters of the calibration\ncurve, which relate instrument responses to true concentrations, to vary across\ndifferent laboratories, thereby reflecting real-world variability in\nmeasurement processes. Traditional large-sample interval estimation methods are\ninadequate for small samples, leading to the use of an alternative approach,\nnamely the fiducial approach. The calibration curve that accurately captures\nthe heteroscedastic nature of the data, results in more reliable estimates\nacross diverse laboratory conditions. It turns out that the fiducial approach,\nwhen used to construct a confidence interval for an unknown concentration,\nproduces a slightly wider width while achieving the desired coverage\nprobability. Applications considered include the determination of the presence\nof an analyte and the interval estimation of an unknown true analyte\nconcentration. The proposed method is demonstrated for both simulated and real\ninterlaboratory data, including examples involving copper and cadmium in\ndistilled water.",
        "We introduce wavefront shaping as a tool for optimizing the sensitivity in\nnano-optomechanical measurement schemes. We perform multimode output analysis\nof an optomechanical system consisting of a focused laser beam coupled to the\ntransverse motion of a tapered cantilever, and demonstrate that wavefront\nshaping enables a 350-fold enhancement of the measurement signal-to-noise\n(+25.5 dB) compared to standard split-detection, close to the quantum precision\nlimit. Our results open new perspectives in terms of sensitivity and control of\nthe optomechanical interaction.",
        "We study the question of when two functions L_1,L_2 in the extended Selberg\nclass are identical in terms of the zeros of L_i-h(i=1,2). Here, the\nmeromorphic function h is called moving target. With the assumption on the\ngrowth order of h, we prove that L_1\\equiv L_2 if L_1-h and L_2-h have the same\nzeros counting multiplicities. Moreover, we also construct some examples to\nshow that the assumption is necessary. Compared with the known methods in the\nliterature of this area, we developed a new strategy which is based on the\ntranscendental directions first proposed in the study of distribution of Julia\nset in complex dynamical system. This may be of independent interest.",
        "We present rational Lax representations for one-component parametric\nquadrirational Yang-Baxter maps in both the abelian and non-abelian settings.\nWe show that from the Lax matrices of a general class of non-abelian involutive\nYang-Baxter maps ($\\mathcal{K}$-list), by considering the symmetries of the\n$\\mathcal{K}$-list maps, we obtain compatible refactorization problems with\nrational Lax matrices for other classes of non-abelian involutive Yang-Baxter\nmaps ($\\Lambda$, $\\mathcal{H}$ and $\\mathcal{F}$ lists). In the abelian\nsetting, this procedure generates rational Lax representations for the abelian\nYang-Baxter maps of the $F$ and $H$ lists. Additionally, we provide examples of\nnon-involutive (abelian and non-abelian) multi-parametric Yang-Baxter maps,\nalong with their Lax representations, which lie outside the preceding lists.",
        "The effect of valley splitting on the readout of qubit states is\ntheoretically investigated in a three-quantum-dot (QD) system. A single unit of\nthe three-QD system consists of qubit-QDs and a channel-QD that is connected to\na conventional transistor. The nonlinear source--drain current characteristics\nunder resonant-tunneling effects are used to distinguish different qubit\nstates. Using nonequilibrium Green functions, the current formula for the\nthree-QD system is derived when each QD has two valley energy levels. Two\nvalley states in each QD are considered to be affected by variations in the\nfabrication process. We found that when valley splitting is smaller than Zeeman\nsplitting, the current nonlinearity can improve the readout, provided that the\nnonuniformity of the valley energy levels is small. Conversely, when the valley\nsplitting is larger than the Zeeman splitting, the nonuniformity degraded the\nreadout. In both cases, we showed that there are regions where the measurement\ntime $t_{\\rm meas}$ is much less than the decoherence time $t_{\\rm dec}$ such\nthat $t_{\\rm dec}\/t_{\\rm meas}>100$. This suggests that less than 1\\%\nmeasurement error is anticipated, which opens up the possibility for\nimplementing surface codes even in the presence of valley splitting.",
        "The KOTO II experiment is proposed to measure the branching ratio of the\ndecay $K_L\\to\\pi^0\\nu\\bar{\\nu}$ at J-PARC. With a beamline to extract\nlong-lived neutral kaons at 5 degrees from a production target, the single\nevent sensitivity of the decay is $8.5\\times 10^{-13}$, which is much smaller\nthan the Standard Model prediction $3\\times 10^{-11}$. This allows searches for\nnew physics beyond the Standard Model and the first discovery of the decay with\na significance exceeding $5\\sigma$. As the only experiment proposed in the\nworld dedicated to rare kaon decays, KOTO II will be indispensable in the quest\nfor a complete understanding of flavor dynamics in the quark sector. Moreover,\nby combining efforts from the kaon community worldwide, we plan to develop the\nKOTO II detector further and expand the physics reach of the experiment to\ninclude measurements of the branching ratio of the $K_L\\to\\pi^0\\ell^+\\ell^-$\ndecays, studies of other $K_L$ decays, and searches for dark photons, axions,\nand axion-like particles. KOTO II will therefore obtain a comprehensive\nunderstanding of $K_L$ decays, providing further constraints on new physics\nscenarios with existing $K^+$ results.",
        "Hypergeometric class equations are given by second\n  order differential operators in one variable whose coefficient at the second\nderivative\n  is a polynomial of degree $\\leq2$, at the first derivative of degree\n  $\\leq1$ and the free term is a number. Their solutions, called hypergeometric\nclass functions, include the Gauss hypergeometric function and its various\nlimiting cases. The paper presents a unified approach to these functions. The\nmain structure behind this approach is a family of complex 4-dimensional Lie\nalgebras, originally due to Willard Miller. Hypergeometric class functions can\nbe interpreted as eigenfunctions of the quadratic Casimir operator in a\nrepresentation of Miller's Lie algebra given by differential operators in three\ncomplex variables. One obtains a unified treatment of various properties of\nhypergeometric class functions such as recurrence relations, discrete\nsymmetries, power series expansions, integral representations, generating\nfunctions and orthogonality of polynomial solutions.",
        "There is a lot of information available concerning Hardy-Hilbert type\ninequalities in one or more dimensions. In this paper we introduce the\ndevelopment of such inequalities on homogeneous groups. Moreover, we point out\na unification of several of the Hardy-Hilbert type inequalities in the\nclassical case to a general kernel case. Finally, we generalize these results\nto the homogeneous group case.",
        "By introducing appropriate lattice parameters for a bi-lattice smoothly\nconnecting the hexagonal close-packed (hcp) with the cuboidal structures,\nnamely the body-centered (bcc) and the face centered cubic (fcc) lattices, we\nwere able to map out the minimum energy path for a Burgers-Bain type of phase\ntransition. We demonstrate that for three different models applied, i.e. the\nkissing hard-sphere model, the Lennard-Jones potential, and density functional\ntheory for metallic lithium, the direct transition path is always from hcp to\nfcc with a separate path leading from fcc to bcc. This solves, at least for the\nmodels considered here, a long-standing controversy of whether or not fcc acts\nas an intermediate phase in martensitic type of phase transitions.",
        "Given a regular function $\\phi$ on a smooth stack, and a $(-1)$-shifted\nLagrangian $M$ on the derived critical locus of $\\phi$, under fairly general\nhypotheses, we construct a pullback map from the Grothendieck group of coherent\nmatrix factorizations of $\\phi$ to that of coherent sheaves on $M$. This map\nsatisfies a functoriality property with respect to the composition of\nLagrangian correspondences, as well as the usual bivariance and base-change\nproperties.\n  We provide three applications of the construction, one in the definition of\nquantum $K$-theory of critical loci (Landau-Ginzburg models), paving the way to\ngeneralize works of Okounkov school from Nakajima quiver varieties to quivers\nwith potentials, one in establishing a degeneration formula for $K$-theoretic\nDonaldson-Thomas theory of local Calabi-Yau 4-folds, the other in confirming a\n$K$-theoretic version of Joyce-Safronov conjecture.",
        "We investigate the nucleosynthesis and kilonova emission based on\nnumerical-relativity binary neutron star merger simulations that incorporate a\ntwo-moment neutrino-transport scheme. Unlike in previous works with simpler\nneutrino treatments, a massive, fast (up to $v=0.3c$), proton-rich\nneutrino-driven wind develops in the post-merger phase of the simulations as\nlong as the merger remnant does not collapse to a black hole. We evolve the\nejecta for 100 days after the merger using 2D ray-by-ray\nradiation-hydrodynamics simulations coupled in-situ to a complete nuclear\nnetwork. The most abundant nucleosynthesis products are He, $^{56}$Ni, and\n$^{56}$Co. We find a total yield of $\\sim 10^{-3} M_\\odot$ of $^{56}$Ni for all\nmergers that produce massive neutron star remnants, independently of the mass\nratio and equation of state. After a few days, the decay of $^{56}$Ni and later\n$^{56}$Co becomes the primary source of heating in the matter expanding above\nthe remnant. As a result, the kilonova light curve flattens on timescales of\ndays for polar observation angles. The observation of this effect could serve\nas smoking gun for the presence of a long-lived neutron star remnant in future\nkilonova observations.",
        "The EXO-UV program is an international, interdisciplinary collaboration\nbetween astrophysicists and biologists aimed at expanding the characterization\nof ultraviolet radiation (UVR) environments on exoplanets. This approach\ncombines astrophysical studies with biological experiments to better understand\nthe potential impacts of UVR on exoplanetary surfaces. UVR is particularly\nrelevant because it reaches the surface of planets and can influence their\nhabitability. The specific wavelengths within the UVR spectrum depend on the\nplanet's atmospheric composition and the spectral energy distribution of its\nhost star. Additionally, high UVR fluxes emitted during flares and superflares\nare of particular interest due to the limited information available regarding\ntheir biological impact. The EXO-UV program has successfully led to the first\nexperimental study examining the biological effects of high UVR fluences, such\nas those produced by flares and superflares. Future experimental studies aim to\ninvestigate the biological effects of repetitive flares. In this paper, we\nreview the latest results from our EXO-UV program."
      ]
    }
  }
]