[
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Long non-coding RNAs: definitions, functions, challenges and recommendations",
    "start_abstract":"Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Language Models are Few-Shot Learners"
      ],
      "abstract":[
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "J&H: Evaluating the Robustness of Large Language Models Under\n  Knowledge-Injection Attacks in Legal Domain",
        "Non-Euclidean Hierarchical Representational Learning using Hyperbolic\n  Graph Neural Networks for Environmental Claim Detection",
        "Enhancing Pancreatic Cancer Staging with Large Language Models: The Role\n  of Retrieval-Augmented Generation",
        "JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience",
        "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
        "Cross-modal Context Fusion and Adaptive Graph Convolutional Network for\n  Multimodal Conversational Emotion Recognition",
        "SurveyX: Academic Survey Automation via Large Language Models",
        "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
        "Unshackling Context Length: An Efficient Selective Attention Approach\n  through Query-Key Compression",
        "CiteCheck: Towards Accurate Citation Faithfulness Detection",
        "Modelling change in neural dynamics during phonetic accommodation",
        "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models",
        "LegalViz: Legal Text Visualization by Text To Diagram Generation",
        "Adaptive Drift Compensation for Soft Sensorized Finger Using Continual\n  Learning",
        "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
        "A Survey of Internet Censorship and its Measurement: Methodology,\n  Trends, and Challenges",
        "Unifying Perplexing Behaviors in Modified BP Attributions through\n  Alignment Perspective",
        "UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via\n  Network Architecture Search",
        "DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot\n  Trajectories",
        "Density-Functional Perturbation Theory with Numeric Atom-Centered\n  Orbitals",
        "Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and\n  Power Interference in Multi-User OSaaS Networks",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective",
        "Study of Nucleon Charge-Exchange Processes at $^{12}$C Fragmentation\n  with an Energy of 300 MeV\/Nucleon",
        "Global Picard Spectra and Borel Parametrized Algebra",
        "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
        "Microscopic investigation of wobbling motion in even-even nuclei"
      ],
      "abstract":[
        "As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs.",
        "Transformer-based models dominate NLP tasks like sentiment analysis, machine\ntranslation, and claim verification. However, their massive computational\ndemands and lack of interpretability pose challenges for real-world\napplications requiring efficiency and transparency. In this work, we explore\nGraph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as\nlightweight yet effective alternatives for Environmental Claim Detection,\nreframing it as a graph classification problem. We construct dependency parsing\ngraphs to explicitly model syntactic structures, using simple word embeddings\n(word2vec) for node features with dependency relations encoded as edge\nfeatures. Our results demonstrate that these graph-based models achieve\ncomparable or superior performance to state-of-the-art transformers while using\n30x fewer parameters. This efficiency highlights the potential of structured,\ninterpretable, and computationally efficient graph-based approaches.",
        "Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+\/RAG+ (NotebookLM\nwith REK), REK+\/RAG- (Gemini 2.0 Flash with REK), and REK-\/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+\/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+\/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+\/RAG- (38%) and REK-\/RAG- (35%). For TNM classification, REK+\/RAG+ attained\n80% accuracy, exceeding REK+\/RAG- (55%) and REK-\/RAG- (50%). Additionally,\nREK+\/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification.",
        "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com\/Zzoay\/JRE-L.",
        "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps:\/\/github.com\/HenryLau7\/CFPO.",
        "Emotion recognition has a wide range of applications in human-computer\ninteraction, marketing, healthcare, and other fields. In recent years, the\ndevelopment of deep learning technology has provided new methods for emotion\nrecognition. Prior to this, many emotion recognition methods have been\nproposed, including multimodal emotion recognition methods, but these methods\nignore the mutual interference between different input modalities and pay\nlittle attention to the directional dialogue between speakers. Therefore, this\narticle proposes a new multimodal emotion recognition method, including a cross\nmodal context fusion module, an adaptive graph convolutional encoding module,\nand an emotion classification module. The cross modal context module includes a\ncross modal alignment module and a context fusion module, which are used to\nreduce the noise introduced by mutual interference between different input\nmodalities. The adaptive graph convolution module constructs a dialogue\nrelationship graph for extracting dependencies and self dependencies between\nspeakers. Our model has surpassed some state-of-the-art methods on publicly\navailable benchmark datasets and achieved high recognition accuracy.",
        "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn",
        "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
        "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks.",
        "Citation faithfulness detection is critical for enhancing retrieval-augmented\ngeneration (RAG) systems, yet large-scale Chinese datasets for this task are\nscarce. Existing methods face prohibitive costs due to the need for manually\nannotated negative samples. To address this, we introduce the first large-scale\nChinese dataset CiteCheck for citation faithfulness detection, constructed via\na cost-effective approach using two-stage manual annotation. This method\nbalances positive and negative samples while significantly reducing annotation\nexpenses. CiteCheck comprises training and test splits. Experiments demonstrate\nthat: (1) the test samples are highly challenging, with even state-of-the-art\nLLMs failing to achieve high accuracy; and (2) training data augmented with\nLLM-generated negative samples enables smaller models to attain strong\nperformance using parameter-efficient fine-tuning. CiteCheck provides a robust\nfoundation for advancing citation faithfulness detection in Chinese RAG\nsystems. The dataset is publicly available to facilitate research.",
        "Short-term phonetic accommodation is a fundamental driver behind accent\nchange, but how does real-time input from another speaker's voice shape the\nspeech planning representations of an interlocutor? We advance a computational\nmodel of change in phonetic representations during phonetic accommodation,\ngrounded in dynamic neural field equations for movement planning and memory\ndynamics. We test the model's ability to capture empirical patterns from an\nexperimental study where speakers shadowed a model talker with a different\naccent from their own. The experimental data shows vowel-specific degrees of\nconvergence during shadowing, followed by return to baseline (or minor\ndivergence) post-shadowing. The model can reproduce these phenomena by\nmodulating the magnitude of inhibitory memory dynamics, which may reflect\nresistance to accommodation due to phonological and\/or sociolinguistic\npressures. We discuss the implications of these results for the relation\nbetween short-term phonetic accommodation and longer-term patterns of sound\nchange.",
        "The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases.",
        "Legal documents including judgments and court orders require highly\nsophisticated legal knowledge for understanding. To disclose expert knowledge\nfor non-experts, we explore the problem of visualizing legal texts with\neasy-to-understand diagrams and propose a novel dataset of LegalViz with 23\nlanguages and 7,010 cases of legal document and visualization pairs, using the\nDOT graph description language of Graphviz. LegalViz provides a simple diagram\nfrom a complicated legal corpus identifying legal entities, transactions, legal\nsources, and statements at a glance, that are essential in each judgment. In\naddition, we provide new evaluation metrics for the legal diagram visualization\nby considering graph structures, textual similarities, and legal contents. We\nconducted empirical studies on few-shot and finetuning large language models\nfor generating legal diagrams and evaluated them with these metrics, including\nlegal content-based evaluation within 23 languages. Models trained with\nLegalViz outperform existing models including GPTs, confirming the\neffectiveness of our dataset.",
        "Strain sensors are gaining popularity in soft robotics for acquiring tactile\ndata due to their flexibility and ease of integration. Tactile sensing plays a\ncritical role in soft grippers, enabling them to safely interact with\nunstructured environments and precisely detect object properties. However, a\nsignificant challenge with these systems is their high non-linearity,\ntime-varying behavior, and long-term signal drift. In this paper, we introduce\na continual learning (CL) approach to model a soft finger equipped with\npiezoelectric-based strain sensors for proprioception. To tackle the\naforementioned challenges, we propose an adaptive CL algorithm that integrates\na Long Short-Term Memory (LSTM) network with a memory buffer for rehearsal and\nincludes a regularization term to keep the model's decision boundary close to\nthe base signal while adapting to time-varying drift. We conduct nine different\nexperiments, resetting the entire setup each time to demonstrate signal drift.\nWe also benchmark our algorithm against two other methods and conduct an\nablation study to assess the impact of different components on the overall\nperformance.",
        "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
        "Internet censorship limits the access of nodes residing within a specific\nnetwork environment to the public Internet, and vice versa. During the last\ndecade, techniques for conducting Internet censorship have been developed\nfurther. Consequently, methodology for measuring Internet censorship had been\nimproved as well. In this paper, we firstly provide a survey of Internet\ncensorship techniques. Secondly, we survey censorship measurement methodology,\nincluding a coverage of available datasets. In cases where it is beneficial, we\nbridge the terminology and taxonomy of Internet censorship with related\ndomains, namely traffic obfuscation and information hiding. We cover both,\ntechnical and human aspects, as well as recent trends, and challenges.",
        "Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.",
        "Lightweight models are essential for real-time speech enhancement\napplications. In recent years, there has been a growing trend toward developing\nincreasingly compact models for speech enhancement. In this paper, we propose\nan Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS),\nwhich is suitable for implementation in low-footprint devices. Firstly, we\nexplore the application of various efficient convolutional blocks within the\nU-Net framework to identify the most promising candidates. Secondly, we\nintroduce two boosting components to enhance the capacity of these\nconvolutional blocks: a novel activation function named affine PReLU and a\ncausal time-frequency attention module. Furthermore, we leverage neural\narchitecture search to discover an optimal architecture within our carefully\ndesigned search space. By integrating the above strategies, UL-UNAS not only\nsignificantly outperforms the latest ultra-lightweight models with the same or\nlower computational complexity, but also delivers competitive performance\ncompared to recent baseline models that require substantially higher\ncomputational resources.",
        "Diffusion models excel at creating images and videos thanks to their\nmultimodal generative capabilities. These same capabilities have made diffusion\nmodels increasingly popular in robotics research, where they are used for\ngenerating robot motion. However, the stochastic nature of diffusion models is\nfundamentally at odds with the precise dynamical equations describing the\nfeasible motion of robots. Hence, generating dynamically admissible robot\ntrajectories is a challenge for diffusion models. To alleviate this issue, we\nintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to\ngenerate provably admissible trajectories of black-box robotic systems using\ndiffusion models. A sequence of states is a dynamically admissible trajectory\nif each state of the sequence belongs to the reachable set of its predecessor\nby the robot's equations of motion. To generate such trajectories, our\ndiffusion policies project their predictions onto a dynamically admissible\nmanifold during both training and inference to align the objective of the\ndenoiser neural network with the dynamical admissibility constraint. The\nauto-regressive nature of these projections along with the black-box nature of\nrobot dynamics render these projections immensely challenging. We thus enforce\nadmissibility by iteratively sampling a polytopic under-approximation of the\nreachable set of a state onto which we project its predicted successor, before\niterating this process with the projected successor. By producing accurate\ntrajectories, this projection eliminates the need for diffusion models to\ncontinually replan, enabling one-shot long-horizon trajectory planning. We\ndemonstrate that our framework generates higher quality dynamically admissible\nrobot trajectories through extensive simulations on a quadcopter and various\nMuJoCo environments, along with real-world experiments on a Unitree GO1 and\nGO2.",
        "This paper represents one contribution to a larger Roadmap article reviewing\nthe current status of the FHI-aims code. In this contribution, the\nimplementation of density-functional perturbation theory in a numerical\natom-centered framework is summarized. Guidelines on usage and links to\ntutorials are provided.",
        "We present a framework to identify and mitigate rogue OOK signals and\nuser-generated power interference in a multi-user Optical-Spectrum-as-a-Service\nnetwork. Experimental tests on the OpenIreland-testbed achieve up to 89%\ndetection rate within 10 seconds of an interference event.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks.",
        "The search of reactions with nucleon charge-exchange was performed on the\nFRAGM fragment-separator of the TWAC accelerator complex at fragmentation of\ncarbon nuclei with an energy of 300 MeV\/nucleon on a thin beryllium target. The\nexperimental setup, located at an angle of 3.5 degrees to the incident beam,\nhad high momentum resolution. Differential cross sections were measured for\n$^{11}$Be, $^{12}$B and $^{12}$Be as function of the nuclei momentum. The\nexperimental data were compared with theoretical predictions of various models\nof nucleus-nucleus interactions and other experimental results. Measurements of\nnucleon charge exchange processes in this energy region was carried out for the\nfirst time. New results were obtained to test theoretical models of\nnucleus-nucleus interactions.",
        "We answer a question of Schwede on the existence of global Picard spectra\nassociated to his ultra-commutative global ring spectra; given an\nultra-commutative global ring spectrum $R$, we show there exists a global\nspectrum $\\mathrm{pic}_\\mathrm{eq}(R)$ assembling the Picard spectra of all\nunderlying $G$-equivariant ring spectra $\\mathrm{res}_G R$ of $R$ into one\nobject, in that for all finite groups $G$, the genuine fixed points are given\nby $\\mathrm{pic}_\\mathrm{eq}(R)^G \\simeq\n\\mathrm{pic}(\\mathrm{Mod}_{\\mathrm{res}_G R}(\\mathrm{Sp}_G))$.\n  Along the way, we develop a generalization of Borel-equivariant objects in\nthe setting of parametrized higher algebra. We use this to assemble the\nsymmetric monoidal categories of $G$-spectra for all finite groups $G$ together\nwith all restrictions and norms into a single `normed global category', and\nbuild a comparison functor which allows us to import ultra-commutative\n$G$-equivariant or global ring spectra into the setting of parametrized higher\nalgebra.",
        "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
        "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Parameter-Free FISTA by Adaptive Restart and Backtracking",
    "start_abstract":"We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
      ],
      "abstract":[
        "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Zero-Shot Denoising for Fluorescence Lifetime Imaging Microscopy with\n  Intensity-Guided Learning",
        "Improving Lesion Segmentation in Medical Images by Global and Regional\n  Feature Compensation",
        "QoE-oriented Communication Service Provision for Annotation Rendering in\n  Mobile Augmented Reality",
        "Structure-from-Sherds++: Robust Incremental 3D Reassembly of Axially\n  Symmetric Pots from Unordered and Mixed Fragment Collections",
        "OSLO-IC: On-the-Sphere Learned Omnidirectional Image Compression with\n  Attention Modules and Spatial Context",
        "ILACS-LGOT: A Multi-Layer Contrast Enhancement Approach for Palm-Vein\n  Images",
        "Goal-Oriented Semantic Communication for Wireless Video Transmission via\n  Generative AI",
        "SLCGC: A lightweight Self-supervised Low-pass Contrastive Graph\n  Clustering Network for Hyperspectral Images",
        "Color Correction Meets Cross-Spectral Refinement: A Distribution-Aware\n  Diffusion for Underwater Image Restoration",
        "Physics-Informed Implicit Neural Representations for Joint B0 Estimation\n  and Echo Planar Imaging",
        "Observation-only learning of neural mapping schemes for gappy\n  satellite-derived ocean colour parameters",
        "Few Shot Alternating GD and Minimization for Generalizable Real-Time MRI",
        "4D-ACFNet: A 4D Attention Mechanism-Based Prognostic Framework for\n  Colorectal Cancer Liver Metastasis Integrating Multimodal Spatiotemporal\n  Features",
        "Decay rates of $\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar\\nu_\\ell$ using\n  helicity analysis and phase-moment parametrization",
        "Ultraviolet Renormalization of Spin Boson Models I. Normal and\n  2-Nilpotent Interactions",
        "Development of a high-rate capable DLC-RPC based on a current evacuation\n  pattern",
        "Non-Lorentzian model for strong exciton-plasmon coupling",
        "Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly\n  Detection in Videos",
        "PrivilegedDreamer: Explicit Imagination of Privileged Information for\n  Rapid Adaptation of Learned Policies",
        "Are you a DePIN? A Decision Tree to Classify Decentralized Physical\n  Infrastructure Networks",
        "Bridging HCI and AI Research for the Evaluation of Conversational SE\n  Assistants",
        "Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in\n  Ecommerce Web Components Generated by LLMs",
        "Blast waves and reverse shocks: from ultra-relativistic GRBs to\n  moderately relativistic X-ray binaries",
        "How Low Can You Go? Searching for the Intrinsic Dimensionality of\n  Complex Networks using Metric Node Embeddings",
        "Native antisite defects in h-BN",
        "Black Box Causal Inference: Effect Estimation via Meta Prediction",
        "Exploring the energy spectrum of a four-terminal Josephson junction:\n  Towards topological Andreev band structures",
        "Near-Field ISAC: Synergy of Dual-Purpose Codebooks and Space-Time\n  Adaptive Processing"
      ],
      "abstract":[
        "Multimodal and multi-information microscopy techniques such as Fluorescence\nLifetime Imaging Microscopy (FLIM) extend the informational channels beyond\nintensity-based fluorescence microscopy but suffer from reduced image quality\ndue to complex noise patterns. For FLIM, the intrinsic relationship between\nintensity and lifetime information means noise in each channel is a\nmultivariate function across channels without necessarily sharing structural\nfeatures. Based on this, we present a novel Zero-Shot Denoising Framework with\nan Intensity-Guided Learning approach. Our correlation-preserving strategy\nmaintains important biological information that might be lost when channels are\nprocessed independently. Our framework implements separate processing paths for\neach channel and utilizes a pre-trained intensity denoising prior to guide the\nrefinement of lifetime components across multiple channels. Through experiments\non real-world FLIM-acquired biological samples, we show that our approach\noutperforms existing methods in both noise reduction and lifetime preservation,\nthereby enabling more reliable extraction of physiological and molecular\ninformation.",
        "Automated lesion segmentation of medical images has made tremendous\nimprovements in recent years due to deep learning advancements. However,\naccurately capturing fine-grained global and regional feature representations\nremains a challenge. Many existing methods obtain suboptimal performance on\ncomplex lesion segmentation due to information loss during typical downsampling\noperations and the insufficient capture of either regional or global features.\nTo address these issues, we propose the Global and Regional Compensation\nSegmentation Framework (GRCSF), which introduces two key innovations: the\nGlobal Compensation Unit (GCU) and the Region Compensation Unit (RCU). The\nproposed GCU addresses resolution loss in the U-shaped backbone by preserving\nglobal contextual features and fine-grained details during multiscale\ndownsampling. Meanwhile, the RCU introduces a self-supervised learning (SSL)\nresidual map generated by Masked Autoencoders (MAE), obtained as pixel-wise\ndifferences between reconstructed and original images, to highlight regions\nwith potential lesions. These SSL residual maps guide precise lesion\nlocalization and segmentation through a patch-based cross-attention mechanism\nthat integrates regional spatial and pixel-level features. Additionally, the\nRCU incorporates patch-level importance scoring to enhance feature fusion by\nleveraging global spatial information from the backbone. Experiments on two\npublicly available medical image segmentation datasets, including brain stroke\nlesion and coronary artery calcification datasets, demonstrate that our GRCSF\noutperforms state-of-the-art methods, confirming its effectiveness across\ndiverse lesion types and its potential as a generalizable lesion segmentation\nsolution.",
        "As mobile augmented reality (MAR) continues to evolve, future 6G networks\nwill play a pivotal role in supporting immersive and personalized user\nexperiences. In this paper, we address the communication service provision\nproblem for annotation rendering in edge-assisted MAR, with the objective of\noptimizing spectrum resource utilization while ensuring the required quality of\nexperience (QoE) for MAR users. To overcome the challenges of user-specific\nuplink data traffic patterns and the complex operational mechanisms of\nannotation rendering, we propose a digital twin (DT)-based approach. We first\ndesign a DT specifically tailored for MAR applications to learn key annotation\nrendering mechanisms, enabling the network controller to access MAR\napplication-specific information. Then, we develop a DT based QoE modeling\napproach to capture the unique relationship between individual user QoE and\nspectrum resource demands. Finally, we propose a QoE-oriented resource\nallocation algorithm that decreases resource utilization compared to\nconventional net work slicing-based approaches. Simulation results demonstrate\nthat our DT-based approach outperforms benchmark approaches in the accuracy and\ngranularity of QoE modeling.",
        "Reassembling multiple axially symmetric pots from fragmentary sherds is\ncrucial for cultural heritage preservation, yet it poses significant challenges\ndue to thin and sharp fracture surfaces that generate numerous false positive\nmatches and hinder large-scale puzzle solving. Existing global approaches,\nwhich optimize all potential fragment pairs simultaneously or data-driven\nmodels, are prone to local minima and face scalability issues when multiple\npots are intermixed. Motivated by Structure-from-Motion (SfM) for 3D\nreconstruction from multiple images, we propose an efficient reassembly method\nfor axially symmetric pots based on iterative registration of one sherd at a\ntime, called Structure-from-Sherds++ (SfS++). Our method extends beyond simple\nreplication of incremental SfM and leverages multi-graph beam search to explore\nmultiple registration paths. This allows us to effectively filter out\nindistinguishable false matches and simultaneously reconstruct multiple pots\nwithout requiring prior information such as base or the number of mixed\nobjects. Our approach achieves 87% reassembly accuracy on a dataset of 142 real\nfragments from 10 different pots, outperforming other methods in handling\ncomplex fracture patterns with mixed datasets and achieving state-of-the-art\nperformance. Code and results can be found in our project page\nhttps:\/\/sj-yoo.info\/sfs\/.",
        "Developing effective 360-degree (spherical) image compression techniques is\ncrucial for technologies like virtual reality and automated driving. This paper\nadvances the state-of-the-art in on-the-sphere learning (OSLO) for\nomnidirectional image compression framework by proposing spherical attention\nmodules, residual blocks, and a spatial autoregressive context model. These\nimprovements achieve a 23.1% bit rate reduction in terms of WS-PSNR BD rate.\nAdditionally, we introduce a spherical transposed convolution operator for\nupsampling, which reduces trainable parameters by a factor of four compared to\nthe pixel shuffling used in the OSLO framework, while maintaining similar\ncompression performance. Therefore, in total, our proposed method offers\nsignificant rate savings with a smaller architecture and can be applied to any\nspherical convolutional application.",
        "This article presents an extended author's version based on our previous\nwork, where we introduced the Multiple Overlapping Tiles (MOT) method for palm\nvein image enhancement. To better reflect the specific operations involved, we\nrename MOT to ILACS-LGOT (Intensity-Limited Adaptive Contrast Stretching with\nLayered Gaussian-weighted Overlapping Tiles). This revised terminology more\naccurately represents the method's approach to contrast enhancement and blocky\neffect mitigation. Additionally, this article provides a more detailed\nanalysis, including expanded evaluations, graphical representations, and\nsample-based comparisons, demonstrating the effectiveness of ILACS-LGOT over\nexisting methods.",
        "Efficient video transmission is essential for seamless communication and\ncollaboration within the visually-driven digital landscape. To achieve low\nlatency and high-quality video transmission over a bandwidth-constrained noisy\nwireless channel, we propose a stable diffusion (SD)-based goal-oriented\nsemantic communication (GSC) framework. In this framework, we first design a\nsemantic encoder that effectively identify the keyframes from video and extract\nthe relevant semantic information (SI) to reduce the transmission data size. We\nthen develop a semantic decoder to reconstruct the keyframes from the received\nSI and further generate the full video from the reconstructed keyframes using\nframe interpolation to ensure high-quality reconstruction. Recognizing the\nimpact of wireless channel noise on SI transmission, we also propose an\nSD-based denoiser for GSC (SD-GSC) condition on an instantaneous channel gain\nto remove the channel noise from the received noisy SI under a known channel.\nFor scenarios with an unknown channel, we further propose a parallel SD\ndenoiser for GSC (PSD-GSC) to jointly learn the distribution of channel gains\nand denoise the received SI. It is shown that, with the known channel, our\nproposed SD-GSC outperforms state-of-the-art ADJSCC, Latent-Diff DNSC, DeepWiVe\nand DVST, improving Peak Signal-to-Noise Ratio (PSNR) by 69%, 58%, 33% and 38%,\nreducing mean squared error (MSE) by 52%, 50%, 41% and 45%, and reducing\nFr\\'echet Video Distance (FVD) by 38%, 32%, 22% and 24%, respectively. With the\nunknown channel, our PSD-GSC achieves a 17% improvement in PSNR, a 29%\nreduction in MSE, and a 19% reduction in FVD compared to MMSE\nequalizer-enhanced SD-GSC. These significant performance improvements\ndemonstrate the robustness and superiority of our proposed methods in enhancing\nvideo transmission quality and efficiency under various channel conditions.",
        "Self-supervised hyperspectral image (HSI) clustering remains a fundamental\nyet challenging task due to the absence of labeled data and the inherent\ncomplexity of spatial-spectral interactions. While recent advancements have\nexplored innovative approaches, existing methods face critical limitations in\nclustering accuracy, feature discriminability, computational efficiency, and\nrobustness to noise, hindering their practical deployment. In this paper, a\nself-supervised efficient low-pass contrastive graph clustering (SLCGC) is\nintroduced for HSIs. Our approach begins with homogeneous region generation,\nwhich aggregates pixels into spectrally consistent regions to preserve local\nspatial-spectral coherence while drastically reducing graph complexity. We then\nconstruct a structural graph using an adjacency matrix A and introduce a\nlow-pass graph denoising mechanism to suppress high-frequency noise in the\ngraph topology, ensuring stable feature propagation. A dual-branch graph\ncontrastive learning module is developed, where Gaussian noise perturbations\ngenerate augmented views through two multilayer perceptrons (MLPs), and a\ncross-view contrastive loss enforces structural consistency between views to\nlearn noise-invariant representations. Finally, latent embeddings optimized by\nthis process are clustered via K-means. Extensive experiments and repeated\ncomparative analysis have verified that our SLCGC contains high clustering\naccuracy, low computational complexity, and strong robustness. The code source\nwill be available at https:\/\/github.com\/DY-HYX.",
        "Underwater imaging often suffers from significant visual degradation, which\nlimits its suitability for subsequent applications. While recent underwater\nimage enhancement (UIE) methods rely on the current advances in deep neural\nnetwork architecture designs, there is still considerable room for improvement\nin terms of cross-scene robustness and computational efficiency. Diffusion\nmodels have shown great success in image generation, prompting us to consider\ntheir application to UIE tasks. However, directly applying them to UIE tasks\nwill pose two challenges, \\textit{i.e.}, high computational budget and color\nunbalanced perturbations. To tackle these issues, we propose DiffColor, a\ndistribution-aware diffusion and cross-spectral refinement model for efficient\nUIE. Instead of diffusing in the raw pixel space, we transfer the image into\nthe wavelet domain to obtain such low-frequency and high-frequency spectra, it\ninherently reduces the image spatial dimensions by half after each\ntransformation. Unlike single-noise image restoration tasks, underwater imaging\nexhibits unbalanced channel distributions due to the selective absorption of\nlight by water. To address this, we design the Global Color Correction (GCC)\nmodule to handle the diverse color shifts, thereby avoiding potential global\ndegradation disturbances during the denoising process. For the sacrificed image\ndetails caused by underwater scattering, we further present the Cross-Spectral\nDetail Refinement (CSDR) to enhance the high-frequency details, which are\nintegrated with the low-frequency signal as input conditions for guiding the\ndiffusion. This way not only ensures the high-fidelity of sampled content but\nalso compensates for the sacrificed details. Comprehensive experiments\ndemonstrate the superior performance of DiffColor over state-of-the-art methods\nin both quantitative and qualitative evaluations.",
        "Echo Planar Imaging (EPI) is widely used for its rapid acquisition but\nsuffers from severe geometric distortions due to B0 inhomogeneities,\nparticularly along the phase encoding direction. Existing methods follow a\ntwo-step process: reconstructing blip-up\/down EPI images, then estimating B0,\nwhich can introduce error accumulation and reduce correction accuracy. This is\nespecially problematic in high B0 regions, where distortions align along the\nsame axis, making them harder to disentangle. In this work, we propose a novel\napproach that integrates Implicit Neural Representations (INRs) with a\nphysics-informed correction model to jointly estimate B0 inhomogeneities and\nreconstruct distortion-free images from rotated-view EPI acquisitions. INRs\noffer a flexible, continuous representation that inherently captures complex\nspatial variations without requiring predefined grid-based field maps. By\nleveraging this property, our method dynamically adapts to subject-specific B0\nvariations and improves robustness across different imaging conditions.\nExperimental results on 180 slices of brain images from three subjects\ndemonstrate that our approach outperforms traditional methods in terms of\nreconstruction quality and field estimation accuracy.",
        "Monitoring optical properties of coastal and open ocean waters is crucial to\nassessing the health of marine ecosystems. Deep learning offers a promising\napproach to address these ecosystem dynamics, especially in scenarios where\ngap-free ground-truth data is lacking, which poses a challenge for designing\neffective training frameworks. Using an advanced neural variational data\nassimilation scheme (called 4DVarNet), we introduce a comprehensive training\nframework designed to effectively train directly on gappy data sets. Using the\nMediterranean Sea as a case study, our experiments not only highlight the high\nperformance of the chosen neural network in reconstructing gap-free images from\ngappy datasets but also demonstrate its superior performance over\nstate-of-the-art algorithms such as DInEOF and Direct Inversion, whether using\nCNN or UNet architectures.",
        "This work introduces a novel near real-time (real-time after an initial short\ndelay) MRI solution that handles motion well and is generalizable. Here,\nreal-time means the algorithm works well on a highly accelerated scan, is\nzero-latency (reconstructs a new frame as soon as MRI data for it arrives), and\nis fast enough, i.e., the time taken to process a frame is comparable to the\nscan time per frame or lesser. We demonstrate its generalizability through\nexperiments on 6 prospective datasets and 17 retrospective datasets that span\nmultiple different applications -- speech larynx imaging, brain, ungated\ncardiac perfusion, cardiac cine, cardiac OCMR, abdomen; sampling schemes --\nCartesian, pseudo-radial, radial, spiral; and sampling rates -- ranging from 6x\nto 4 radial lines per frame. Comparisons with a large number of existing\nreal-time and batch methods, including unsupervised and supervised deep\nlearning methods, show the power and speed of our approach.",
        "Postoperative prognostic prediction for colorectal cancer liver metastasis\n(CRLM) remains challenging due to tumor heterogeneity, dynamic evolution of the\nhepatic microenvironment, and insufficient multimodal data fusion. To address\nthese issues, we propose 4D-ACFNet, the first framework that synergistically\nintegrates lightweight spatiotemporal modeling, cross-modal dynamic\ncalibration, and personalized temporal prediction within a unified\narchitecture. Specifically, it incorporates a novel 4D spatiotemporal attention\nmechanism, which employs spatiotemporal separable convolution (reducing\nparameter count by 41%) and virtual timestamp encoding to model the interannual\nevolution patterns of postoperative dynamic processes, such as liver\nregeneration and steatosis. For cross-modal feature alignment, Transformer\nlayers are integrated to jointly optimize modality alignment loss and\ndisentanglement loss, effectively suppressing scale mismatch and redundant\ninterference in clinical-imaging data. Additionally, we design a dynamic\nprognostic decision module that generates personalized interannual recurrence\nrisk heatmaps through temporal upsampling and a gated classification head,\novercoming the limitations of traditional methods in temporal dynamic modeling\nand cross-modal alignment. Experiments on 197 CRLM patients demonstrate that\nthe model achieves 100% temporal adjacency accuracy (TAA), with performance\nsignificantly surpassing existing approaches. This study establishes the first\nspatiotemporal modeling paradigm for postoperative dynamic monitoring of CRLM.\nThe proposed framework can be extended to prognostic analysis of multi-cancer\nmetastases, advancing precision surgery from \"spatial resection\" to\n\"spatiotemporal cure.\"",
        "Based on the helicity method, formulae for the semileptonic transition of\n$\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar \\nu_\\ell$ including lepton mass\neffects are derived. In order to calculate the form factors of the $\\Lambda_b$\nbaryon transition matrix element, we employ the phase-moment parameterization\nand perform fits to the Lattice QCD data. With the help of the obtained form\nfactors, six helicity amplitudes and the differential decay widths are\nevaluated. Through appropriate angular integrations, we express the helicity\nflip, helicity nonflip integrated decay rates, and the lepton-side\nforward-backward asymmetry. We present a numerical analysis of these physical\nobservables. We obtain the mentioned physical quantities by performing fits to\nthe Lattice QCD data using the well-known Boyd-Grinstein-Lebed parametrization.\nComparisons with other experimental and theoretical data are also discussed.",
        "We study the ultraviolet problem for models of a finite-dimensional quantum\nmechanical system linearly coupled to a bosonic quantum field, such as the\n(many-)spin boson model or its rotating-wave approximation. If the state change\nof the system upon emission or absorption of a boson is either given by a\nnormal matrix or by a 2-nilpotent one, which is the case for the previously\nnamed examples, we prove an optimal renormalization result. We complement it,\nby proving the norm resolvent convergence of appropriately regularized models\nto the renormalized one. Our method consists of a dressing transformation\nargument in the normal case and an appropriate interior boundary condition for\nthe 2-nilpotent case.",
        "A Resistive Plate Chamber using Diamond-Like Carbon electrodes (DLC-RPC) has\nbeen developed as a background tagging detector in the MEG$~$II experiment. The\nDLC-RPC is planned to be installed in a high-intensity and low-momentum muon\nbeam. This detector is required to have a detection efficiency above 90 % with\nfour active gaps in the muon beam due to the limitation of the material budget.\nIn such an environment, the high current flowing through the resistive\nelectrodes causes a voltage drop, which reduces the performance of the DLC-RPC.\nThis voltage drop can be suppressed by implementing a current evacuation\npattern, though discharges are more likely to occur near the pattern. Therefore\nthe pattern must be covered by a protection cover made of an insulator. In this\nstudy, electrode samples with a current evacuation pattern and different widths\nof protection cover (0.2 mm and 0.8 mm) have been produced, and their\nperformance and stability were measured. The detection efficiency of a\nsingle-gap chamber for $\\beta$-rays from a $^{90}$Sr source was measured to be\nup to approximately 60 % in both electrode samples. The target efficiency can\nbe achieved even with a drop of 100 $-$ 150 V. On the other hand, after more\nthan a dozen hours of operation, discharges suddenly occurred and the detector\nwas prevented from further operation. These discharges created current paths on\nthe spacing pillars. This serious problem must be investigated and solved in\nthe future.",
        "We develop a non-Lorentzian approach for quantum emitters (QE) resonantly\ncoupled to localized surface plasmons (LSP) in metal-dielectric structures.\nUsing the exact LSP Green function, we derive non-Lorentzian version of\nMaxwell-Bloch equations which describe LSP in terms of metal complex dielectric\nfunction rather than via Lorentzian resonances. For a single QE coupled to the\nLSP, we obtain an explicit expression for the system effective optical\npolarizability which, in the Lorentzian approximation, recovers the classical\ncoupled oscillator (CO) model. We demonstrate that non-Lorentzian effects\noriginating from the temporal dispersion of metal dielectric function affect\ndramatically the optical spectra as the system transitions to the strong\ncoupling regime. Specifically, in contrast to Lorentzian models, the main\nspectral weight is shifted towards the lower energy polaritonic band,\nconsistent with the experiment.",
        "Anomaly detection in videos is a challenging task as anomalies in different\nvideos are of different kinds. Therefore, a promising way to approach video\nanomaly detection is by learning the non-anomalous nature of the video at hand.\nTo this end, we propose a one-class few-shot learning driven transformer based\napproach for anomaly detection in videos that is self-context aware. Features\nfrom the first few consecutive non-anomalous frames in a video are used to\ntrain the transformer in predicting the non-anomalous feature of the subsequent\nframe. This takes place under the attention of a self-context learned from the\ninput features themselves. After the learning, given a few previous frames, the\nvideo-specific transformer is used to infer if a frame is anomalous or not by\ncomparing the feature predicted by it with the actual. The effectiveness of the\nproposed method with respect to the state-of-the-art is demonstrated through\nqualitative and quantitative results on different standard datasets. We also\nstudy the positive effect of the self-context used in our approach.",
        "Numerous real-world control problems involve dynamics and objectives affected\nby unobservable hidden parameters, ranging from autonomous driving to robotic\nmanipulation, which cause performance degradation during sim-to-real transfer.\nTo represent these kinds of domains, we adopt hidden-parameter Markov decision\nprocesses (HIP-MDPs), which model sequential decision problems where hidden\nvariables parameterize transition and reward functions. Existing approaches,\nsuch as domain randomization, domain adaptation, and meta-learning, simply\ntreat the effect of hidden parameters as additional variance and often struggle\nto effectively handle HIP-MDP problems, especially when the rewards are\nparameterized by hidden variables. We introduce Privileged-Dreamer, a\nmodel-based reinforcement learning framework that extends the existing\nmodel-based approach by incorporating an explicit parameter estimation module.\nPrivilegedDreamer features its novel dual recurrent architecture that\nexplicitly estimates hidden parameters from limited historical data and enables\nus to condition the model, actor, and critic networks on these estimated\nparameters. Our empirical analysis on five diverse HIP-MDP tasks demonstrates\nthat PrivilegedDreamer outperforms state-of-the-art model-based, model-free,\nand domain adaptation learning algorithms. Additionally, we conduct ablation\nstudies to justify the inclusion of each component in the proposed\narchitecture.",
        "Decentralized physical infrastructure networks (DePINs) are an emerging\nvertical within \"Web3\" replacing the traditional method that physical\ninfrastructures are constructed. Yet, the boundaries between DePIN and\ntraditional method of building crowd-sourced infrastructures such as citizen\nscience initiatives or other Web3 verticals are not always so clear cut. In\nthis work, we systematically analyze the differences between DePIN and other\nWeb2 and Web3 verticals. For this, the study proposes a novel decision tree for\nclassifying systems as DePIN. This tree is informed by prior studies and\ndifferentiates DePIN from related concepts using criteria such as the presence\nof a three-sided market, token-based incentives for supply, and the requirement\nfor physical asset placement in those systems.\n  The paper demonstrates the application of the decision tree to various\nblockchain systems, including Helium and Bitcoin, showcasing its practical\nutility in differentiating DePIN systems.\n  This research offers significant contributions towards establishing a more\nobjective and systematic approach to identifying and categorizing DePIN\nsystems. It lays the groundwork for creating a comprehensive and unbiased\ndatabase of DePIN systems, which will inform future research and development\nwithin this emerging sector.",
        "As Large Language Models (LLMs) are increasingly adopted in software\nengineering, recently in the form of conversational assistants, ensuring these\ntechnologies align with developers' needs is essential. The limitations of\ntraditional human-centered methods for evaluating LLM-based tools at scale\nraise the need for automatic evaluation. In this paper, we advocate combining\ninsights from human-computer interaction (HCI) and artificial intelligence (AI)\nresearch to enable human-centered automatic evaluation of LLM-based\nconversational SE assistants. We identify requirements for such evaluation and\nchallenges down the road, working towards a framework that ensures these\nassistants are designed and deployed in line with user needs.",
        "Recent work has highlighted the risks of LLM-generated content for a wide\nrange of harmful behaviors, including incorrect and harmful code. In this work,\nwe extend this by studying whether LLM-generated web design contains dark\npatterns. This work evaluated designs of ecommerce web components generated by\nfour popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used\necommerce components (e.g., search, product reviews) and used them as prompts\nto generate a total of 312 components across all models. Over one-third of\ngenerated components contain at least one dark pattern. The majority of dark\npattern strategies involve hiding crucial information, limiting users' actions,\nand manipulating them into making decisions through a sense of urgency. Dark\npatterns are also more frequently produced in components that are related to\ncompany interests. These findings highlight the need for interventions to\nprevent dark patterns during front-end code generation with LLMs and emphasize\nthe importance of expanding ethical design education to a broader audience.",
        "Blast wave models are commonly used to model relativistic outflows from\nultra-relativistic gamma-ray bursts (GRBs), but are also applied to lower\nLorentz factor ejections from X-ray binaries (XRBs). Here we revisit the\nphysics of blast waves and reverse shocks in these systems and explore the\nsimilarities and differences between the ultra-relativistic ($\\Gamma \\gg 1$)\nand moderately relativistic ($\\Gamma \\sim$ a few) regimes. We first demonstrate\nthat the evolution of the blast wave radius as a function of the observer frame\ntime is recovered in the on-axis ultra-relativistic limit from a general energy\nand radius blast wave evolution, emphasizing that XRB ejections are off-axis,\nmoderately relativistic cousins of GRB afterglows. We show that, for fixed\nblast wave or ejecta energy, reverse shocks cross the ejecta much later\n(earlier) on in the evolution for less (more) relativistic systems, and find\nthat reverse shocks are much longer-lived in XRBs and off-axis GRBs compared to\non-axis GRBs. Reverse shock crossing should thus typically finish after\n$\\sim10-100$ days (in the observer frame) in XRB ejections. This\ncharacteristic, together with their moderate Lorentz factors and resolvable\ncore separations, makes XRB ejections unique laboratories for shock and\nparticle acceleration physics. We discuss the impact of geometry and lateral\nspreading on our results, explore how to distinguish between different shock\ncomponents, and comment on the implications for GRB and XRB environments.\nAdditionally, we argue that identification of reverse shock signatures in XRBs\ncould provide an independent constraint on the ejecta Lorentz factor.",
        "Low-dimensional embeddings are essential for machine learning tasks involving\ngraphs, such as node classification, link prediction, community detection,\nnetwork visualization, and network compression. Although recent studies have\nidentified exact low-dimensional embeddings, the limits of the required\nembedding dimensions remain unclear. We presently prove that lower dimensional\nembeddings are possible when using Euclidean metric embeddings as opposed to\nvector-based Logistic PCA (LPCA) embeddings. In particular, we provide an\nefficient logarithmic search procedure for identifying the exact embedding\ndimension and demonstrate how metric embeddings enable inference of the exact\nembedding dimensions of large-scale networks by exploiting that the metric\nproperties can be used to provide linearithmic scaling. Empirically, we show\nthat our approach extracts substantially lower dimensional representations of\nnetworks than previously reported for small-sized networks. For the first time,\nwe demonstrate that even large-scale networks can be effectively embedded in\nvery low-dimensional spaces, and provide examples of scalable, exact\nreconstruction for graphs with up to a million nodes. Our approach highlights\nthat the intrinsic dimensionality of networks is substantially lower than\npreviously reported and provides a computationally efficient assessment of the\nexact embedding dimension also of large-scale networks. The surprisingly low\ndimensional representations achieved demonstrate that networks in general can\nbe losslessly represented using very low dimensional feature spaces, which can\nbe used to guide existing network analysis tasks from community detection and\nnode classification to structure revealing exact network visualizations.",
        "Hexagonal boron nitride (hBN) is an excellent host for solid-state single\nphonon emitters. Experimental observed emission ranges from infrared to\nultraviolet. The emission centers are generally attributed to either intrinsic\nor extrinsic point defects embedded into hBN. Nevertheless, the microscopic\nstructure of most of these defect emitters is uncertain. Here, through\ndensity-functional theory calculations we studied the native antisite defects\nin hBN. We find that the neutral boron antisite might be a nonmagnetic single\nphoton source with zero-phonon-line (ZPL) at 1.58 eV and such a lineshape that\nis often observed in experiments. Furthermore, the positively charged nitrogen\nantisite might be associated with a dim color center recently observed as a\nblue emitter with ZPL at 2.63 eV. These simple single substitution defects\nindicate the existence of out-of-plane phonon mode which significantly affects\nthe optical properties. Our results could provide useful information for\nidentification of quantum emitters in hBN.",
        "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
        "Hybrid multiterminal Josephson junctions (JJs) are expected to harbor a novel\nclass of Andreev bound states (ABSs), including topologically nontrivial states\nin four-terminal devices. In these systems, topological phases emerge when ABSs\ndepend on at least three superconducting phase differences, resulting in a\nthree-dimensional (3D) energy spectrum characterized by Weyl nodes at zero\nenergy. Here, we realize a four-terminal JJ in a hybrid Al\/InAs\nheterostructure, where ABSs form a synthetic 3D band structure. We probe the\nenergy spectrum using tunneling spectroscopy and identify spectral features\nassociated with the formation of a tri-Andreev molecule, a bound state whose\nenergy depends on three superconducting phases and, therefore, is able to host\ntopological ABSs. The experimental observations are well described by a\nnumerical model. The calculations predict the appearance of four Weyl nodes at\nzero energy within a gap smaller than the experimental resolution. These\ntopological states are theoretically predicted to remain stable within an\nextended region of the parameter space, well accessible by our device. These\nfindings establish an experimental foundation to study high-dimensional\nsynthetic band structures in multiterminal JJs, and to realize topological\nAndreev bands.",
        "Integrated sensing and communication (ISAC) has emerged as a transformative\nparadigm, enabling situationally aware and perceptive next-generation wireless\nnetworks through the co-design of shared network resources. With the adoption\nof millimeter-wave (mmWave) and terahertz (THz) frequency bands, ultra-massive\nMIMO (UM-MIMO) systems and holographic surfaces unlock the potential of\nnear-field (NF) propagation, characterized by spherical wavefronts that\nfacilitate beam manipulation in both angular and range domains. This paper\npresents a unified approach to near-field beam-training and sensing,\nintroducing a dual-purpose codebook design that employs discrete Fourier\ntransform (DFT)-based codebooks for coarse estimation of sensing parameters and\npolar codebooks for parameter refinement. Leveraging these range and angle\nestimates, a customized low-complexity space-time adaptive processing (STAP)\ntechnique is proposed for NF-ISAC to detect slow-moving targets and efficiently\nmitigate clutter. The interplay between codebooks and NF-STAP framework offers\nthree key advantages: reduced communication beam training overhead, improved\nestimation accuracy, and minimal STAP computational complexity. Simulation\nresults show that the proposed framework can reduce STAP complexity by three\norders of magnitude, validating efficacy, and highlighting the potential of the\nproposed approach to seamlessly integrate NF communication and sensing\nfunctionalities in future wireless networks."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Convex generalizations of total variation based on the structure tensor with applications to inverse problems",
    "start_abstract":"We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Parameter-Free FISTA by Adaptive Restart and Backtracking"
      ],
      "abstract":[
        "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Multi-objective and hierarchical control for coupled stochastic\n  parabolic systems",
        "A dimension reduction procedure for the design of lattice-spring systems\n  with minimal fabrication cost and required multi-functional properties",
        "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$",
        "State-of-the-art Methods for Pseudo-Boolean Solving with SCIP",
        "Theorems of nonlinear separation of co-radiant sets and optimality\n  conditions for approximate and proper approximate solutions of vector\n  optimization problems",
        "Under the hood of a carbon footprint calculator",
        "On Optimal Control of Hybrid Dynamical Systems using Complementarity\n  Constraints",
        "Error Bounds for a Class of Cone-Convex Inclusion Problems",
        "Dual Regularization and Outer Approximation of Optimal Control Problems\n  in BV",
        "Solving Non-Monotone Inclusions Using Monotonicity of Pairs of Operators",
        "Assessment various control methods a digital copy of enterprise by\n  integral indicator",
        "Edge downgrades in the maximal covering location problem",
        "On Fenchel c-conjugate dual problems for DC optimization: characterizing\n  weak, strong and stable strong duality",
        "Introduction of the G$_2$-Ricci Flow: Geometric Implications for\n  Spontaneous Symmetry Breaking and Gauge Boson Masses",
        "A bang-bang optimal control for a nonlinear system modeling the Gate\n  Control Theory of Pain",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "Major-Minor Mean Field Game of Stopping: An Entropy Regularization\n  Approach",
        "TOI-512: Super-Earth transiting a K-type star discovered by TESS and\n  ESPRESSO",
        "Chung-Graham and Zeckendorf representations",
        "The Layered Catalan Monoids: Structure and Determinants",
        "Multi-Channel Currency: A Secure Method Using Semi-Quantum Tokens",
        "High Resolution {\\it BOES} Spectroscopy of Raman-scattered\n  He~II$\\lambda$6545 in Young Planetary Nebulae",
        "Reducing Simulation Effort for RIS Optimization using an Efficient\n  Far-Field Approximation",
        "Learning in Markets with Heterogeneous Agents: Dynamics and Survival of\n  Bayesian vs. No-Regret Learners",
        "Large thermoelectric spin-valve effect with a superconductor",
        "High Harmonic Generation with Orbital Angular Momentum Beams:\n  Beyond-dipole Corrections",
        "Polynomial sequences with the same recurrence relation as Chebyshev\n  polynomials and the minimal polynomial of $2\\cos (2\\pi \/n)$",
        "Preference learning made easy: Everything should be understood through\n  win rate"
      ],
      "abstract":[
        "We study the Stackelberg-Nash null controllability of a coupled system\ngoverned by two linear forward stochastic parabolic equations. The system\nincludes one leader control localized in a subset of the domain, two additional\nleader controls in the diffusion terms, and \\( m \\) follower controls, where \\(\nm \\geq 2 \\). We consider two different scenarios for the followers: first, when\nthe followers minimize a functional involving both components of the system's\nstate, and second, when they minimize a functional involving only the second\ncomponent of the state. For fixed leader controls, we first establish the\nexistence and uniqueness of the Nash equilibrium in both scenarios and provide\nits characterization. As a byproduct, the problem is reformulated as a\nclassical null controllability issue for the associated coupled\nforward-backward stochastic parabolic system. To address this, we derive new\nCarleman estimates for the adjoint stochastic systems. As far as we know, this\nproblem is among the first to be discussed for stochastic coupled systems.",
        "We show that the problem of the design of the lattices of elastoplastic\ncurrent conducting springs with optimal multi-functional properties leads to an\nanalytically tractable problem. Specifically, focusing on a lattice with a\nsmall number of springs, we use the technique of inequalities to reduce the\nnumber variables and to compute the minimal cost of lattice fabrication\nexplicitly.",
        "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions.",
        "The Pseudo-Boolean problem deals with linear or polynomial constraints with\ninteger coefficients over Boolean variables. The objective lies in optimizing a\nlinear objective function, or finding a feasible solution, or finding a\nsolution that satisfies as many constraints as possible. In the 2024\nPseudo-Boolean competition, solvers incorporating the SCIP framework won five\nout of six categories it was competing in. From a total of 1,207 instances,\nSCIP successfully solved 759, while its parallel version FiberSCIP solved 776.\nBased on the results from the competition, we further enhanced SCIP's\nPseudo-Boolean capabilities. This article discusses the results and presents\nthe winning algorithmic ideas.",
        "This paper deals with \\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper\nefficient points with respect to a co-radiant set in a vector optimization\nproblem. In the first part of the paper, we establish a new nonlinear\nseparation theorem for co-radiant sets in normed spaces. Subsequently, we\nobtain necessary and sufficient conditions by means of scalarization for both\n\\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper efficient points in a general\nframework, without any requirements on the co-radiant set or any convexity\nassumption on the sets under consideration.Consequently, our results have a\nwider range of applicability than previously stated in the literature.",
        "We explain the mathematical theory of the Input-Output method for carbon\nfootprints computations.",
        "Optimal control for switch-based dynamical systems is a challenging problem\nin the process control literature. In this study, we model these systems as\nhybrid dynamical systems with finite number of unknown switching points and\nreformulate them using non-smooth and non-convex complementarity constraints as\na mathematical program with complementarity constraints (MPCC). We utilize a\nmoving finite element based strategy to discretize the differential equation\nsystem to accurately locate the unknown switching points at the finite element\nboundary and achieve high-order accuracy at intermediate non-collocation\npoints. We propose a globalization approach to solve the discretized MPCC\nproblem using a mixed NLP\/MILP-based strategy to converge to a non-spurious\nfirst-order optimal solution. The method is tested on three dynamic\noptimization examples, including a gas-liquid tank model and an optimal control\nproblem with a sliding mode solution.",
        "In this paper, we investigate error bounds for cone-convex inclusion problems\nin finite-dimensional settings of the form $f(x)\\in K$, where $K$ is a smooth\ncone and $f$ is a continuously differentiable and $K$-concave function. We show\nthat local error bounds for the inclusion can be characterized by the Abadie\nconstraint qualification around the reference point. In the case where $f$ is\nan affine function, we precisely identify the conditions under which the\ninclusion admits global error bounds. Additionally, we derive some properties\nof smooth cones, as well as regular cones and strictly convex cones.",
        "This paper is concerned with an elliptic optimal control problem with total\nvariation (TV) restriction on the control in the constraints. We introduce a\nregularized optimal control problem by applying a quadratic regularization of\nthe dual representation of the TV-seminorm. The regularized optimal control\nproblem can be solved by means of an outer approximation algorithm. Convergence\nof the regularization for vanishing regularization parameter as well as\nconvergence of the outer approximation algorithm is proven. Moreover, we derive\nnecessary and sufficient optimality conditions for the original unregularized\noptimal control problem and use these to construct an exact solution that we\nuse in our numerical experiments to confirm our theoretical results.",
        "In this paper, under the monotonicity of pairs of operators, we propose some\nGeneralized Proximal Point Algorithms to solve non-monotone inclusions using\nwarped resolvents and transformed resolvents. The weak, strong, and linear\nconvergence of the proposed algorithms are established under very mild\nconditions.",
        "The difficulty of assessing the state lies in a little predictable change in\nthe dimension of a dynamic system under the influence of internal changes and\nenvironmental parameters. In the work, the state of such a system is estimated\nby the method of integral indicators. The application of the method of integral\nindicators allowed us to evaluate the activity of an enterprise. In the present\nwork, the method of integrated indicators is used to assess the control of a\ndigital copy (enterprise).",
        "We tackle the downgrading maximal covering location problem within a network.\nIn this problem, two actors with conflicting objectives are involved: (a) The\nlocation planner aims to determine the location of facilities to maximize the\ncovered demand while anticipating that an attacker will attempt to reduce\ncoverage by increasing the length of some edges (downgrade); (b) The attacker\nseeks to maximize the demand initially covered by the facilities but left\nuncovered after the downgrade. The attacker can increase the length of certain\nedges within a specified budget.\n  We introduce a mixed-integer linear bilevel program to formulate the problem,\nfollowed by a preprocessing phase and a matheuristic algorithm designed to\naddress it. Additionally, computational results are presented to illustrate the\npotential and limitations of the proposed algorithm.",
        "In this paper we present two Fenchel-type dual problems for a DC (difference\nof convex functions) optimization primal one. They have been built by means of\nthe c-conjugation scheme, a pattern of conjugation which has been shown to be\nsuitable for evenly convex functions. We study characterizations of weak,\nstrong and stable strong duality for both pairs of primal-dual problems. We\nalso give conditions which relate the existence of strong and stable strong\nduality for both pairs.",
        "This work introduces the G$_2$-Ricci flow on seven-dimensional manifolds with\nnon-zero torsion and explores its physical implications. By extending the Ricci\nflow to manifolds with G$_2$ structures, we study the evolution of solitonic\nsolutions and their role in spontaneous symmetry breaking in gauge theories. In\nparticular, this model proposes that the masses of the W and Z bosons are\ndetermined not by an external scalar field, as in the Higgs mechanism, but by\nthe intrinsic geometric torsion of the manifold. Furthermore, a possible\nconnection between the geometry of extra dimensions and the curvature of our\nspacetime is explored, with implications for the experimentally observed\npositive cosmological constant. This approach provides an innovative\ninterpretation of fundamental interactions in theoretical physics, opening new\npossibilities for studying extra dimensions and the geometry of\nG$_2$-manifolds.",
        "We consider a nonlinear system of coupled ordinary differential equations\n(representing the excitatory, inhibitory, and T-cell potentials) based on the\nGate Control Theory of Pain, initially proposed by R. Melzack and P.D. Wall in\n1965, and later mathematically modeled by N.F. Britton and S.M. Skevington in\n1988.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "This paper studies a discrete-time major-minor mean field game of stopping\nwhere the major player can choose either an optimal control or stopping time.\nWe look for the relaxed equilibrium as a randomized stopping policy, which is\nformulated as a fixed point of a set-valued mapping, whose existence is\nchallenging by direct arguments. To overcome the difficulties caused by the\npresence of a major player, we propose to study an auxiliary problem by\nconsidering entropy regularization in the major player's problem while\nformulating the minor players' optimal stopping problems as linear programming\nover occupation measures. We first show the existence of regularized equilibria\nas fixed points of some simplified set-valued operator using the\nKakutani-Fan-Glicksberg fixed-point theorem. Next, we prove that the\nregularized equilibrium converges as the regularization parameter $\\lambda$\ntends to 0, and the limit corresponds to a fixed point of the original\noperator, thereby confirming the existence of a relaxed equilibrium in the\noriginal mean field game problem.",
        "One of the goals of the ESPRESSO guaranteed time observations (GTOs) at the\nESO 8.2m telescope is to follow up on candidate planets from transit surveys\nsuch as the TESS mission. High-precision radial velocities are required to\ncharacterize small exoplanets. Aims. We intend to confirm the existence of a\ntransiting super-Earth around the bright (V=9.74) K0-type star TOI-512 (TIC\n119292328) and provide a characterization. Combining photometric data from TESS\nand 37 high-resolution spectroscopic observations from ESPRESSO in a joint\nMarkov chain Monte Carlo analysis, we determined the planetary parameters of\nTOI-512b and characterized its internal structure. We find that TOI-512b is a\nsuper-Earth, with a radius of $1.54 \\pm 0.10$ R$_\\oplus$ and mass of\n$3.57_{-0.55}^{+0.53}$~M$_\\oplus$, on a $7.19_{-6.1\\cdot 10^{-5}}^{+7\\cdot\n10^{-5}}$ day orbit. This corresponds to a bulk density of\n$5.62_{-1.28}^{+1.59}$ g cm$^{-3}$. Our interior structure analysis presents a\nsmall inner core representing $0.13^{+0.13}_{-0.11}$ of the solid mass fraction\nfor the planet, surrounded by a mantle with a mass fraction of\n$0.69^{+0.20}_{-0.22}$, and an upper limit of the water layer of $0.16$. The\ngas mass below $10^{-8.93}$ indicates a very small amount of gas on the planet.\nWe find no evidence of the second candidate found by the TESS pipeline,\nTOI-512.02, neither in TESS photometry, nor in the ESPRESSO radial velocities.\nThe low stellar activity makes it an interesting transmission spectroscopy\ncandidate for future-generation instruments.",
        "We examine the relationship between the Chung-Graham and Zeckendorf\nrepresentations of an integer using the software package {\\tt Walnut}.",
        "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
        "Digital currencies primarily operate online, but there is growing interest in\nenabling offline transactions to improve digital inclusion. Existing offline\nmethods struggle with double-spending risks, often limiting transaction\namounts. In this work, we propose a quantum-state-based currency system that\nuses the non-cloning theorem to enable secure, multi-channel transactions\nwithout the risk of double spending. We demonstrate this system's\nimplementation with experimental results, including use cases for currency\ntransfers and swaps. To mitigate credit risks in swaps, we also integrate\nblockchain to show its wide applicability. Our approach paves the way for\nquantum-secure digital currencies and opens new possibilities for optimizing\nmulti-channel tokens.",
        "Young planetary nebulae (PNe) are characterized by their hot central stars\nand the presence of abundant neutral and molecular components, which result\nfrom significant mass loss during the asymptotic giant branch (AGB) phase of\nstellar evolution. Far-UV \\ion{He}{2}$\\lambda$1025 line photons produced near\nthe central star can undergo Raman scattering by hydrogen atoms, creating a\nbroad emission feature centered at $\\sim$ 6545~\\AA. We conducted\nhigh-resolution spectroscopy of 12 young PNe from April 2019 to March 2020\nusing the Bohyunsan Observatory Echelle Spectrograph ({\\it BOES}). Building on\nthe study by Choi and Lee, who identified Raman-scattered \\ion{He}{2} at\n6545~\\AA\\ in NGC~6881 and NGC~6886, we report new detections of this feature in\nNGC~6741 and NGC~6884. Profile fitting reveals that the velocity of the\n\\ion{H}{1} component relative to the \\ion{He}{2} emission region ranges from\n$26-33~{\\rm km~s^{-1}}$ in these PNe. Using photoionization modeling, we\nestimate the line flux of \\ion{He}{2}$\\lambda$1025 and derive Raman conversion\nefficiencies of 0.39, 0.21, 0.24, and 0.07 for NGC~6881, NGC~6741, NGC~6886,\nand NGC~6884, respectively. These results, combined with radiative transfer\nmodeling, suggest the presence of \\ion{H}{1} components with masses around\n$10^{-2}~M_\\odot$, moving outward from the central \\ion{He}{2} emission region\nat speeds characteristic of the slow stellar wind from a mass-losing giant\nstar.",
        "Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously\nintroduced method is effective, but time-consuming, because multiport impedance\nor scatter matrices are required for each transmitter and receiver position,\nwhich generally must be obtained through full-wave simulation. Herein, a simple\nand efficient far-field approximation is introduced, to extrapolate scatter\nmatrices for arbitrary receiver and transmitter positions from only a single\nsimulation while still maintaining high accuracy suitable for optimization\npurposes. This is demonstrated through comparisons of the optimized capacitance\nvalues and further supported by empirical measurements.",
        "We analyze the performance of heterogeneous learning agents in asset markets\nwith stochastic payoffs. Our agents aim to maximize the expected growth rate of\ntheir wealth but have different theories on how to learn this best. We focus on\ncomparing Bayesian and no-regret learners in market dynamics. Bayesian learners\nwith a prior over a finite set of models that assign positive prior probability\nto the correct model have posterior probabilities that converge exponentially\nto the correct model. Consequently, they survive even in the presence of agents\nwho invest according to the correct model of the stochastic process. Bayesians\nwith a continuum prior converge to the correct model at a rate of $O((\\log\nT)\/T)$. Online learning theory provides no-regret algorithms for maximizing the\nlog of wealth in this setting, achieving a worst-case regret bound of $O(\\log\nT)$ without assuming a steady underlying stochastic process but comparing to\nthe best fixed investment rule. This regret, as we observe, is of the same\norder of magnitude as that of a Bayesian learner with a continuum prior.\nHowever, we show that even such low regret may not be sufficient for survival\nin asset markets: an agent can have regret as low as $O(\\log T)$, but still\nvanish in market dynamics when competing against agents who invest according to\nthe correct model or even against a perfect Bayesian with a finite prior. On\nthe other hand, we show that Bayesian learning is fragile, while no-regret\nlearning requires less knowledge of the environment and is therefore more\nrobust. Any no-regret learner will drive out of the market an imperfect\nBayesian whose finite prior or update rule has even small errors. We formally\nestablish the relationship between notions of survival, vanishing, and market\ndomination studied in economics and the framework of regret minimization, thus\nbridging these theories.",
        "Recent studies have revealed magnetically controllable thermoelectric effects\nin superconductor\/ferromagnet (S\/F) structures. A tunable cryogenic\nthermoelectric generator needs not only a high conversion factor between\nelectricity and heat, but also a large change in the thermoelectric output when\nswitching the magnetic state of the device. Here, we experimentally measure and\nnumerically model thermoelectric effects in fully epitaxial F\/S\/F junctions\nbased on commercially available, easily grown materials, as well as their\ndependence on the magnetic configuration of the F electrodes. We observe\nsizeable Seebeck coefficients for the parallel alignment of the ferromagnetic\nelectrodes, reaching values of about $100$~$\\mu$V\/K. Importantly, we find a\ndecrease of the thermoelectric signal of more than an order of magnitude when\nswitching from a parallel to an antiparallel configuration, constituting a\nlarge thermoelectric spin-valve effect. Theoretical modeling based on a\nself-consistent non-equilibrium Keldysh-Usadel Green function theory, combined\nwith micromagnetic simulations, qualitatively reproduce the experimental\nfindings. These findings pave the way for the development of efficient and\nversatile cryogenic thermoelectric heat engines.",
        "We study the high harmonic generation with vortex beams beyond the dipole\napproximation. To do so we employ the full minimal coupling approach to account\nfor multipolar coupling without truncation and describe the full\nspatio-temporal properties of the electromagnetic field. This allows us to\ninvestigate the beyond-dipole deviations in electron trajectories and the\nemitted power, where the influence of the orbital angular momentum contains\nboth magnetic and quadrupolar effects. In contrast to the system driven by\nplane-wave light, we show that the non-linear dipole dynamics induced by the\nvortex beams are not confined to the polarization or propagation directions,\nbut also have a component in the orthogonal direction. We identify the effects\nof the resulting symmetry breaking via increased beyond dipole corrections\nwhich are particularly apparent in even harmonics.",
        "In this paper we consider the minimal polynomial $\\psi_n(x)$ of $2\\cos (2\\pi\n\/n)$. We introduce some polynomial sequences with the same recurrence relation\nas the rescaled Chebyshev polynomials $t_n(x)=2\\, T_n(x\/2)$ of the first kind,\nwhich turn out to be related to those of various kinds, all coming from those\nof the second kind. We see that $t_n(x)\\pm 2=2(T_n(x\/2)\\pm 1)$ are divisible by\nthe square of either of these polynomials. Then by appropriately removing\nunnecessary factors from these polynomials, we can easily calculate\n$\\psi_n(x)$, which improves Barnes' result in 1977. As an appendix, we give a\ncompact list of the minimal polynomials $\\psi_n(x)$ of $2\\cos (2\\pi \/n)$ for\n$n\\leqslant 120$.",
        "Preference learning, or the task of aligning generative models to preference\ncomparison data, has yet to reach the conceptual maturity of classification,\ndensity estimation, etc. To close this gap, this work presents a framework to\nunderstand preference learning starting from the sampling distribution of\npairwise preference data. First, we prove that the only evaluation of a\ngenerative model that respects both preferences and prevalences in the data\ndistribution is a form of win rate, justifying win rate as the focal point to\nunderstand preference learning. We then analyze preference learning methods as\nwin rate optimization (WRO) or non-WRO. We present novel instances of WRO\nbeyond existing examples (RLHF, NLHF) and identify two key theoretical benefits\nof all such methods. We prove that common non-WRO methods like DPO and SFT on\npreferred samples lack these properties and suggest ways to mitigate such\ntheoretical limitations. We also show that WRO underperforms in practice due\noptimization difficulties and that optimization success predicts performance\nbetter than choices which affect the objective's solution. Our analysis\nhighlights best practices for existing methods and provides recommendations for\nfuture research, guided by the principle that one should either align non-WRO\nmethods more closely with WRO or improve the optimization of WRO objectives."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"On the Distribution of the Two-Sample Cramer-von Mises Criterion",
    "start_abstract":"The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",
    "start_categories":[
      "q-fin.GN"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Quant GANs: deep generation of financial time series"
      ],
      "abstract":[
        "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes\n  through Tree-of-Error",
        "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
        "Preference Optimization via Contrastive Divergence: Your Reward Model is\n  Secretly an NLL Estimator",
        "Correctness Learning: Deductive Verification Guided Learning for\n  Human-AI Collaboration",
        "Generative AI in Transportation Planning: A Survey",
        "What Is a Counterfactual Cause in Action Theories?",
        "LapSum -- One Method to Differentiate Them All: Ranking, Sorting and\n  Top-k Selection",
        "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained\n  Scheduling",
        "Salience-Invariant Consistent Policy Learning for Generalization in\n  Visual Reinforcement Learning",
        "A Minimax Approach to Ad Hoc Teamwork",
        "Human-Alignment Influences the Utility of AI-assisted Decision Making",
        "Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical\n  Classification",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "Performance of the ALICE Inner Tracking System 2",
        "Empowering the Future Workforce: Prioritizing Education for the\n  AI-Accelerated Job Market",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "Human Re-ID Meets LVLMs: What can we expect?",
        "Providing Machine Learning Potentials with High Quality Uncertainty\n  Estimates",
        "Causal AI-based Root Cause Identification: Research to Practice at Scale",
        "Non-local modular flows across deformed null-cuts",
        "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language\n  Models",
        "2D transmons with lifetimes and coherence times exceeding 1 millisecond",
        "Validation of the DESI DR2 Measurements of Baryon Acoustic Oscillations\n  from Galaxies and Quasars",
        "Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent\n  Hypersonic Flows on Arbitrary Grids",
        "On the Effectiveness of Random Weights in Graph Neural Networks",
        "3+1 neutrino mixings model with $A_4$ triplet Majorana neutrino",
        "Mapping AI Avant-Gardes in Time: Posthumanism, Transhumanism,\n  Genhumanism",
        "regMMD: a package for parametric estimation and regression with maximum\n  mean discrepancy"
      ],
      "abstract":[
        "Evaluating mathematical capabilities is critical for assessing the overall\nperformance of large language models (LLMs). However, existing evaluation\nmethods often focus solely on final answers, resulting in highly inaccurate and\nuninterpretable evaluation outcomes, as well as their failure to assess proof\nor open-ended problems. To address these issues, we propose a novel\nmathematical process evaluation agent based on Tree-of-Error, called\nStepMathAgent. This agent incorporates four internal core operations: logical\nstep segmentation, step scoring, score aggregation and error tree generation,\nalong with four external extension modules: difficulty calibration, simplicity\nevaluation, completeness validation and format assessment. Furthermore, we\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\nevaluation instances, derived from 200 high-quality math problems grouped by\nproblem type, subject category and difficulty level. Experiments on\nStepMathBench show that our proposed StepMathAgent outperforms all\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\nand broad applicability to various scenarios. Our data and code are available\nat https:\/\/github.com\/SHU-XUN\/StepMathAgent.",
        "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
        "Existing studies on preference optimization (PO) have centered on\nconstructing pairwise preference data following simple heuristics, such as\nmaximizing the margin between preferred and dispreferred completions based on\nhuman (or AI) ranked scores. However, none of these heuristics has a full\ntheoretical justification. In this work, we develop a novel PO framework that\nprovides theoretical guidance to effectively sample dispreferred completions.\nTo achieve this, we formulate PO as minimizing the negative log-likelihood\n(NLL) of a probability model and propose to estimate its normalization constant\nvia a sampling strategy. As we will demonstrate, these estimative samples can\nact as dispreferred completions in PO. We then select contrastive divergence\n(CD) as the sampling strategy, and propose a novel MC-PO algorithm that applies\nthe Monte Carlo (MC) kernel from CD to sample hard negatives w.r.t. the\nparameterized reward model. Finally, we propose the OnMC-PO algorithm, an\nextension of MC-PO to the online setting. On popular alignment benchmarks,\nMC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further\nimprovement.",
        "Despite significant progress in AI and decision-making technologies in\nsafety-critical fields, challenges remain in verifying the correctness of\ndecision output schemes and verification-result driven design. We propose\ncorrectness learning (CL) to enhance human-AI collaboration integrating\ndeductive verification methods and insights from historical high-quality\nschemes. The typical pattern hidden in historical high-quality schemes, such as\nchange of task priorities in shared resources, provides critical guidance for\nintelligent agents in learning and decision-making. By utilizing deductive\nverification methods, we proposed patten-driven correctness learning (PDCL),\nformally modeling and reasoning the adaptive behaviors-or 'correctness\npattern'-of system agents based on historical high-quality schemes, capturing\nthe logical relationships embedded within these schemes. Using this logical\ninformation as guidance, we establish a correctness judgment and feedback\nmechanism to steer the intelligent decision model toward the 'correctness\npattern' reflected in historical high-quality schemes. Extensive experiments\nacross multiple working conditions and core parameters validate the framework's\ncomponents and demonstrate its effectiveness in improving decision-making and\nresource optimization.",
        "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.",
        "Since the proposal by Halpern and Pearl, reasoning about actual causality has\ngained increasing attention in artificial intelligence, ranging from domains\nsuch as model-checking and verification to reasoning about actions and\nknowledge. More recently, Batusov and Soutchanski proposed a notion of actual\nachievement cause in the situation calculus, amongst others, they can determine\nthe cause of quantified effects in a given action history. While intuitively\nappealing, this notion of cause is not defined in a counterfactual perspective.\nIn this paper, we propose a notion of cause based on counterfactual analysis.\nIn the context of action history, we show that our notion of cause generalizes\nnaturally to a notion of achievement cause. We analyze the relationship between\nour notion of the achievement cause and the achievement cause by Batusov and\nSoutchanski. Finally, we relate our account of cause to Halpern and Pearl's\naccount of actual causality. Particularly, we note some nuances in applying a\ncounterfactual viewpoint to disjunctive goals, a common thorn to definitions of\nactual causes.",
        "We present a novel technique for constructing differentiable order-type\noperations, including soft ranking, soft top-k selection, and soft\npermutations. Our approach leverages an efficient closed-form formula for the\ninverse of the function LapSum, defined as the sum of Laplace distributions.\nThis formulation ensures low computational and memory complexity in selecting\nthe highest activations, enabling losses and gradients to be computed in\n$O(n\\log{}n)$ time. Through extensive experiments, we demonstrate that our\nmethod outperforms state-of-the-art techniques for high-dimensional vectors and\nlarge $k$ values. Furthermore, we provide efficient implementations for both\nCPU and CUDA environments, underscoring the practicality and scalability of our\nmethod for large-scale ranking and differentiable ordering problems.",
        "Effective multi-user delay-constrained scheduling is crucial in various\nreal-world applications, such as instant messaging, live streaming, and data\ncenter management. In these scenarios, schedulers must make real-time decisions\nto satisfy both delay and resource constraints without prior knowledge of\nsystem dynamics, which are often time-varying and challenging to estimate.\nCurrent learning-based methods typically require interactions with actual\nsystems during the training stage, which can be difficult or impractical, as it\nis capable of significantly degrading system performance and incurring\nsubstantial service costs. To address these challenges, we propose a novel\noffline reinforcement learning-based algorithm, named \\underline{S}cheduling By\n\\underline{O}ffline Learning with \\underline{C}ritic Guidance and\n\\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies\npurely from pre-collected \\emph{offline data}. SOCD innovatively employs a\ndiffusion-based policy network, complemented by a sampling-free critic network\nfor policy guidance. By integrating the Lagrangian multiplier optimization into\nthe offline reinforcement learning, SOCD effectively trains high-quality\nconstraint-aware policies exclusively from available datasets, eliminating the\nneed for online interactions with the system. Experimental results demonstrate\nthat SOCD is resilient to various system dynamics, including partially\nobservable and large-scale environments, and delivers superior performance\ncompared to existing methods.",
        "Generalizing policies to unseen scenarios remains a critical challenge in\nvisual reinforcement learning, where agents often overfit to the specific\nvisual observations of the training environment. In unseen environments,\ndistracting pixels may lead agents to extract representations containing\ntask-irrelevant information. As a result, agents may deviate from the optimal\nbehaviors learned during training, thereby hindering visual generalization.To\naddress this issue, we propose the Salience-Invariant Consistent Policy\nLearning (SCPL) algorithm, an efficient framework for zero-shot generalization.\nOur approach introduces a novel value consistency module alongside a dynamics\nmodule to effectively capture task-relevant representations. The value\nconsistency module, guided by saliency, ensures the agent focuses on\ntask-relevant pixels in both original and perturbed observations, while the\ndynamics module uses augmented data to help the encoder capture dynamic- and\nreward-relevant representations. Additionally, our theoretical analysis\nhighlights the importance of policy consistency for generalization. To\nstrengthen this, we introduce a policy consistency module with a KL divergence\nconstraint to maintain consistent policies across original and perturbed\nobservations.Extensive experiments on the DMC-GB, Robotic Manipulation, and\nCARLA benchmarks demonstrate that SCPL significantly outperforms\nstate-of-the-art methods in terms of generalization. Notably, SCPL achieves\naverage performance improvements of 14\\%, 39\\%, and 69\\% in the challenging DMC\nvideo hard setting, the Robotic hard setting, and the CARLA benchmark,\nrespectively.Project Page: https:\/\/sites.google.com\/view\/scpl-rl.",
        "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes\npolicies against an adversarial prior over partners, explicitly accounting for\nuncertainty about partners at time of deployment. Unlike existing methods that\nassume a specific distribution over partners, our approach improves worst-case\nperformance guarantees. Extensive experiments, including evaluations on\ncoordinated cooking tasks from the Melting Pot suite, show our method's\nsuperior robustness compared to self-play, fictitious play, and best response\nlearning. Our work highlights the importance of selecting an appropriate\ntraining distribution over teammates to achieve robustness in AHT.",
        "Whenever an AI model is used to predict a relevant (binary) outcome in\nAI-assisted decision making, it is widely agreed that, together with each\nprediction, the model should provide an AI confidence value. However, it has\nbeen unclear why decision makers have often difficulties to develop a good\nsense on when to trust a prediction using AI confidence values. Very recently,\nCorvelo Benz and Gomez Rodriguez have argued that, for rational decision\nmakers, the utility of AI-assisted decision making is inherently bounded by the\ndegree of alignment between the AI confidence values and the decision maker's\nconfidence on their own predictions. In this work, we empirically investigate\nto what extent the degree of alignment actually influences the utility of\nAI-assisted decision making. To this end, we design and run a large-scale human\nsubject study (n=703) where participants solve a simple decision making task -\nan online card game - assisted by an AI model with a steerable degree of\nalignment. Our results show a positive association between the degree of\nalignment and the utility of AI-assisted decision making. In addition, our\nresults also show that post-processing the AI confidence values to achieve\nmulticalibration with respect to the participants' confidence on their own\npredictions increases both the degree of alignment and the utility of\nAI-assisted decision making.",
        "Multi-level Hierarchical Classification (MLHC) tackles the challenge of\ncategorizing items within a complex, multi-layered class structure. However,\ntraditional MLHC classifiers often rely on a backbone model with independent\noutput layers, which tend to ignore the hierarchical relationships between\nclasses. This oversight can lead to inconsistent predictions that violate the\nunderlying taxonomy. Leveraging Large Language Models (LLMs), we propose a\nnovel taxonomy-embedded transitional LLM-agnostic framework for multimodality\nclassification. The cornerstone of this advancement is the ability of models to\nenforce consistency across hierarchical levels. Our evaluations on the MEP-3M\ndataset - a multi-modal e-commerce product dataset with various hierarchical\nlevels - demonstrated a significant performance improvement compared to\nconventional LLM structures.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "The upgraded Inner Tracking System (ITS2) of the ALICE experiment at the CERN\nLarge Hadron Collider is based on Monolithic Active Pixel Sensors (MAPS). With\na sensitive area of about 10 $m^2$ and 12.5 billion pixels, ITS2 represents the\nlargest pixel detector in high-energy physics. The detector consists of seven\nconcentric layers equipped with ALPIDE pixel sensors manufactured in the\nTowerJazz 180 nm CMOS Imaging Sensor process. The high spatial resolution and\nlow material budget, in combination with small radial distance of the innermost\nlayer from the interaction point, make the detector well suited for secondary\nvertex reconstruction as well as for tracking at low transverse momentum.\n  This paper will present the detector performance during the LHC Run 3 and\ngive an overview on the calibration methods and running experience.",
        "AI's rapid integration into the workplace demands new approaches to workforce\neducation and training and broader AI literacy across disciplines. Coordinated\naction from government, industry, and educational institutions is necessary to\nensure workers can adapt to accelerating technological change.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "Large vision-language models (LVLMs) have been regarded as a breakthrough\nadvance in an astoundingly variety of tasks, from content generation to virtual\nassistants and multimodal search or retrieval. However, for many of these\napplications, the performance of these methods has been widely criticized,\nparticularly when compared with state-of-the-art methods and technologies in\neach specific domain. In this work, we compare the performance of the leading\nlarge vision-language models in the human re-identification task, using as\nbaseline the performance attained by state-of-the-art AI models specifically\ndesigned for this problem. We compare the results due to ChatGPT-4o,\nGemini-2.0-Flash, Claude 3.5 Sonnet, and Qwen-VL-Max to a baseline ReID\nPersonViT model, using the well-known Market1501 dataset. Our evaluation\npipeline includes the dataset curation, prompt engineering, and metric\nselection to assess the models' performance. Results are analyzed from many\ndifferent perspectives: similarity scores, classification accuracy, and\nclassification metrics, including precision, recall, F1 score, and area under\ncurve (AUC). Our results confirm the strengths of LVLMs, but also their severe\nlimitations that often lead to catastrophic answers and should be the scope of\nfurther research. As a concluding remark, we speculate about some further\nresearch that should fuse traditional and LVLMs to combine the strengths from\nboth families of techniques and achieve solid improvements in performance.",
        "Computational chemistry has come a long way over the course of several\ndecades, enabling subatomic level calculations particularly with the\ndevelopment of Density Functional Theory (DFT). Recently, machine-learned\npotentials (MLP) have provided a way to overcome the prevalent time and length\nscale constraints in such calculations. Unfortunately, these models utilise\ncomplex and high dimensional representations, making it challenging for users\nto intuit performance from chemical structure, which has motivated the\ndevelopment of methods for uncertainty quantification. One of the most common\nmethods is to introduce an ensemble of models and employ an averaging approach\nto determine the uncertainty. In this work, we introduced Bayesian Neural\nNetworks (BNNs) for uncertainty aware energy evaluation as a more principled\nand resource efficient method to achieve this goal. The richness of our\nuncertainty quantification enables a new type of hybrid workflow where\ncalculations can be offloaded to a MLP in a principled manner.",
        "Modern applications are built as large, distributed systems spanning numerous\nmodules, teams, and data centers. Despite robust engineering and recovery\nstrategies, failures and performance issues remain inevitable, risking\nsignificant disruptions and affecting end users. Rapid and accurate root cause\nidentification is therefore vital to ensure system reliability and maintain key\nservice metrics.\n  We have developed a novel causality-based Root Cause Identification (RCI)\nalgorithm that emphasizes causation over correlation. This algorithm has been\nintegrated into IBM Instana-bridging research to practice at scale-and is now\nin production use by enterprise customers. By leveraging \"causal AI,\" Instana\nstands apart from typical Application Performance Management (APM) tools,\npinpointing issues in near real-time. This paper highlights Instana's advanced\nfailure diagnosis capabilities, discussing both the theoretical underpinnings\nand practical implementations of the RCI algorithm. Real-world examples\nillustrate how our causality-based approach enhances reliability and\nperformance in today's complex system landscapes.",
        "Modular flows probe important aspects of the entanglement structures,\nespecially those of QFTs, in a dynamical framework. Despite the expected\nnon-local nature in the general cases, the majority of explicitly understood\nexamples feature local space-time trajectories under modular flows. In this\nwork, we study a particular class of non-local modular flows. They are\nassociated with the relativistic vacuum state and sub-regions whose boundaries\nlie on a planar null-surface. They satisfy a remarkable algebraic property\nknown as the half-sided modular inclusion, and as a result the modular\nHamiltonians are exactly known in terms of the stress tensor operators. To be\nexplicit, we focus on the simplest QFT of a massive or massless free scalar in\n$2+1$ dimensions. We obtain explicit expressions for the generators. They can\nbe separated into a sum of local and non-local terms showing certain universal\npattern. The preservation of von-Neumann algebra under modular flow works in a\nsubtle way for the non-local terms. We derive a differential-integral equation\nfor the finite modular flow, which can be analyzed in perturbation theory of\nsmall distance deviating from the entanglement boundary, and re-summation can\nbe performed in appropriate limits. Comparison with the general expectation of\nmodular flows in such limits are discussed.",
        "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted.",
        "Materials improvements are a powerful approach to reducing loss and\ndecoherence in superconducting qubits because such improvements can be readily\ntranslated to large scale processors. Recent work improved transmon coherence\nby utilizing tantalum (Ta) as a base layer and sapphire as a substrate. The\nlosses in these devices are dominated by two-level systems (TLSs) with\ncomparable contributions from both the surface and bulk dielectrics, indicating\nthat both must be tackled to achieve major improvements in the state of the\nart. Here we show that replacing the substrate with high-resistivity silicon\n(Si) dramatically decreases the bulk substrate loss, enabling 2D transmons with\ntime-averaged quality factors (Q) exceeding 1.5 x 10^7, reaching a maximum Q of\n2.5 x 10^7, corresponding to a lifetime (T_1) of up to 1.68 ms. This low loss\nallows us to observe decoherence effects related to the Josephson junction, and\nwe use improved, low-contamination junction deposition to achieve Hahn echo\ncoherence times (T_2E) exceeding T_1. We achieve these material improvements\nwithout any modifications to the qubit architecture, allowing us to readily\nincorporate standard quantum control gates. We demonstrate single qubit gates\nwith 99.994% fidelity. The Ta-on-Si platform comprises a simple material stack\nthat can potentially be fabricated at wafer scale, and therefore can be readily\ntranslated to large-scale quantum processors.",
        "The Dark Energy Spectroscopic Instrument (DESI) data release 2 (DR2) galaxy\nand quasar clustering data represents a significant expansion of data from DR1,\nproviding improved statistical precision in BAO constraints across multiple\ntracers, including bright galaxies (BGS), luminous red galaxies (LRGs),\nemission line galaxies (ELGs), and quasars (QSOs). In this paper, we validate\nthe BAO analysis of DR2. We present the results of robustness tests on the\nblinded DR2 data and, after unblinding, consistency checks on the unblinded DR2\ndata. All results are compared to those obtained from a suite of mock catalogs\nthat replicate the selection and clustering properties of the DR2 sample. We\nconfirm the consistency of DR2 BAO measurements with DR1 while achieving a\nreduction in statistical uncertainties due to the increased survey volume and\ncompleteness. We assess the impact of analysis choices, including different\ndata vectors (correlation function vs. power spectrum), modeling approaches and\nsystematics treatments, and an assumption of the Gaussian likelihood, finding\nthat our BAO constraints are stable across these variations and assumptions\nwith a few minor refinements to the baseline setup of the DR1 BAO analysis. We\nsummarize a series of pre-unblinding tests that confirmed the readiness of our\nanalysis pipeline, the final systematic errors, and the DR2 BAO analysis\nbaseline. The successful completion of these tests led to the unblinding of the\nDR2 BAO measurements, ultimately leading to the DESI DR2 cosmological analysis,\nwith their implications for the expansion history of the Universe and the\nnature of dark energy presented in the DESI key paper.",
        "Designing re-entry vehicles requires accurate predictions of hypersonic flow\naround their geometry. Rapid prediction of such flows can revolutionize vehicle\ndesign, particularly for morphing geometries. We evaluate advanced neural\noperator models such as Deep Operator Networks (DeepONet),\nparameter-conditioned U-Net, Fourier Neural Operator (FNO), and MeshGraphNet,\nwith the objective of addressing the challenge of learning geometry-dependent\nhypersonic flow fields with limited data. Specifically, we compare the\nperformance of these models for two grid types: uniform Cartesian and irregular\ngrids. To train these models, we use 36 unique elliptic geometries for\ngenerating high-fidelity simulations with a high-order entropy-stable DGSEM\nsolver, emphasizing the challenge of working with a scarce dataset. We evaluate\nand compare the four operator-based models for their efficacy in predicting\nhypersonic flow field around the elliptic body. Moreover, we develop a novel\nframework, called Fusion DeepONet, which leverages neural field concepts and\ngeneralizes effectively across varying geometries. Despite the scarcity of\ntraining data, Fusion DeepONet achieves performance comparable to\nparameter-conditioned U-Net on uniform grids while it outperforms MeshGraphNet\nand vanilla DeepONet on irregular, arbitrary grids. Fusion DeepONet requires\nsignificantly fewer trainable parameters as compared to U-Net, MeshGraphNet,\nand FNO, making it computationally efficient. We also analyze the basis\nfunctions of the Fusion DeepONet model using Singular Value Decomposition. This\nanalysis reveals that Fusion DeepONet generalizes effectively to unseen\nsolutions and adapts to varying geometries and grid points, demonstrating its\nrobustness in scenarios with limited training data.",
        "Graph Neural Networks (GNNs) have achieved remarkable success across diverse\ntasks on graph-structured data, primarily through the use of learned weights in\nmessage passing layers. In this paper, we demonstrate that random weights can\nbe surprisingly effective, achieving performance comparable to end-to-end\ntraining counterparts, across various tasks and datasets. Specifically, we show\nthat by replacing learnable weights with random weights, GNNs can retain strong\npredictive power, while significantly reducing training time by up to 6$\\times$\nand memory usage by up to 3$\\times$. Moreover, the random weights combined with\nour construction yield random graph propagation operators, which we show to\nreduce the problem of feature rank collapse in GNNs. These understandings and\nempirical results highlight random weights as a lightweight and efficient\nalternative, offering a compelling perspective on the design and training of\nGNN architectures.",
        "We study a 3+1 active-sterile neutrino mixings model using an $A_4$ triplet\nright-handed neutrino $\\nu_R$ and a singlet eV-scale sterile neutrino under\n$A_4\\times Z_3 \\times Z_2$ discrete symmetry. Four scalar flavons are\nconsidered to reproduce neutrino oscillation parameters within the experimental\n3$\\sigma$ range. The model also studies the effective mass parameter in\nneutrinoless double beta decay experiments. Deviation from $\\mu-\\tau$ symmetry\nin the active neutrino mass matrix is generated through an antisymmetric\ninteraction of $\\nu_R$. This model successfully explains active-sterile\nneutrino mixings consistent with the cosmological upper bound on the sum of\nactive neutrino mass $\\sum m_i < 0.113$ eV (0.145 eV) in NH(IH).",
        "Three directions for the AI avant-garde are sketched against the background\nof time. Posthumanism changes what we are, and belongs to the radical future.\nTranshumanism changes how we are, and corresponds with the radical past.\nGenhumanism changes who we are, and exists in the radical present. While\ndeveloping the concepts, this essay intersects in two ways with theoretical\ndebates about humanism in the face of technological advance. First, it\ndescribes how temporal divisions may cleanly differentiate post- and\ntranshumanism. Second, the essay introduces generative humanism, which\ncontributes to discussions about AI and society by delineating a novel\nhumanistic response to contemporary technology. Finally, grounds are provided\nfor a practical project, one where philosophers work with AI engineers in the\narea of genhumanism. Contemporary AI research into serendipity in\nrecommendation engines provides natural support for the shared research.",
        "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Quant GANs: deep generation of financial time series",
    "start_abstract":"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
      ],
      "abstract":[
        "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
      ],
      "categories":[
        "q-fin.GN"
      ]
    },
    "list":{
      "title":[
        "Innovative Financing Solutions: A Transformative Driver for Financial\n  Performance of Businesses in Morocco",
        "Utilizing Pre-trained and Large Language Models for 10-K Items\n  Segmentation",
        "Mass Shootings, Community Mobility, and the Relocation of Economic\n  Activity",
        "People Reduce Workers' Compensation for Using Artificial Intelligence\n  (AI)",
        "Trust of Strangers: a framework for analysis",
        "Preventing Household Bankruptcy: The One-Third Rule in Financial\n  Planning with Mathematical Validation and Game-Theoretic Insights",
        "The Impact of Digitalisation and Sustainability on Inclusiveness:\n  Inclusive Growth Determinants",
        "The role of FDI along transitional dynamics of the host country in an\n  endogenous growth model",
        "Heterogeneity of household stock portfolios in a national market",
        "Estimating Sequential Search Models Based on a Partial Ranking\n  Representation",
        "Incorporating Damped Harmonic Oscillator in DSGE Models",
        "Investing in nature: Stakeholder's willingness to pay for Tunisian\n  forest services",
        "Combined climate stress testing of supply-chain networks and the\n  financial system with nation-wide firm-level emission estimates",
        "Centre-of-momentum Variables in $\\nu_\\mu$CC1p1$\\pi$",
        "Is fixed-node diffusion quantum Monte Carlo reproducible?",
        "Optical signatures of noncentrosymmetric structural distortion in\n  altermagnetic MnTe",
        "Fast-response low power atomic oven for integration into an ion\n  microchip",
        "Limits on WIMP dark matter with NaI(Tl) crystals in three years of\n  COSINE-100 data",
        "Building a Software Stack for Quantum-HPC Integration",
        "Eightfold Degenerate Dirac Nodal Line in Collinear Antiferromagnet\n  Mn$_5$Si$_3$",
        "A Full AGN Feedback Prescription for Numerical Models: Negative,\n  Positive and Hot Gas-Ejection Mode",
        "All Order Classical Electromagnetic Soft Theorems",
        "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation",
        "Channel Estimation for Pinching-Antenna Systems (PASS)",
        "Rainbow Tur\\'an numbers for short brooms",
        "Influence of departures from LTE on determinations of the sulfur\n  abundances in A-K type stars",
        "Heat transport model for the transition between scaling regimes in\n  quasistatic and full magnetoconvection",
        "Structure and Skewness of the Effective Inspiral Spin Distribution of\n  Binary Black Hole Mergers"
      ],
      "abstract":[
        "In a rapidly evolving landscape marked by continuous change and complex\nchallenges, effective cash management stands as a cornerstone for ensuring\nbusiness sustainability and driving performance. To address these pressing\ndemands, cash managersare increasingly turning to innovative financing\nsolutions such as venture capital, green finance, crowdfunding, advanced\nservices from Pan-African banks, and blockchain technology. These cutting-edge\ntools are pivotal in bolstering resilience against market volatility,\necological transitions, and the accelerating pace of technological change. The\npresent article aims to examine how such innovative financial approaches can\nserve as strategic drivers, enabling businesses to transform challenges into\nopportunities. The analysis underscores that rethinking cash management through\ninnovation is a critical pathway toboost the performance of Moroccan companies.\nTherefore, embracing these forward-thinking strategies unlocks new avenues for\ndevelopment empowering them to adapt with agility amidst the uncertainties of a\nshifting environment.",
        "Extracting specific items from 10-K reports remains challenging due to\nvariations in document formats and item presentation. Traditional rule-based\nitem segmentation approaches often yield suboptimal results. This study\nintroduces two advanced item segmentation methods leveraging language models:\n(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize\nGPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings\nwith a Bi-LSTM model in a hierarchical structure to overcome context window\nconstraints. Trained and evaluated on 3,737 annotated 10-K reports,\nBERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),\nconditional random field (0.9818), and rule-based methods (0.9048) for core\nitems (1, 1A, 3, and 7). These approaches enhance item segmentation\nperformance, improving text analytics in accounting and finance. BERT4ItemSeg\noffers satisfactory item segmentation performance, while GPT4ItemSeg can easily\nadapt to regulatory changes. Together, they offer practical benefits for\nresearchers and practitioners, enabling reliable empirical studies and\nautomated 10-K item segmentation functionality.",
        "Using foot traffic data for over 150,000 points of interest (POIs) near the\nsites of 42 mass shootings (2018-2022, U.S.), we evaluate the spatial-temporal\nimpact of the tragic events on community mobility and relocation of economic\nactivities. Visits to nearby POIs decrease, while farther away POIs experience\nincreased foot traffic, implying that communities shift their activities away\nfrom the shooting sites. The impact is stronger when stronger trauma responses\nare expected. Our results suggest that mass shootings drive significant\ndisplacements of economic activities and can consequently lead to welfare\nlosses due to distortions in optimal choices of time and location.",
        "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
        "Trust among people is essential to ensure collaboration, social network\nbuilding, transactions, and the development and engagement of new audiences for\nbrand promotion or social causes. In Berry (2024), the trust attitudes of\nrespondents toward strangers on the street, other groups of people, and\ninformation sources were measured. This study evaluates the trust of strangers\nusing a 5-factor structural equation model. The analysis yielded a robust model\nwith four of five factors and all variables being statistically significant,\nwith social trust and institutional trust yielding the greatest positive effect\non trust of strangers on the street. While demographic characteristics had a\nsmall positive effect, the trust of friends and family had a mild negative\neffect on the trust of strangers on the street. Trust of information sources\nwas not statistically significant and had a negligible positive effect on the\ntrust of strangers. The results also indicate that almost 48% of respondents\ndistrust strangers on the street, implying that trust is not automatically\nendowed. Directions for future research and implications for business and\nsocial causes are discussed.",
        "This paper analyzes the 1\/3 Financial Rule, a method of allocating income\nequally among debt repayment, savings, and living expenses. Through\nmathematical modeling, game theory, behavioral finance, and technological\nanalysis, we examine the rule's potential for supporting household financial\nstability and reducing bankruptcy risk. The research develops theoretical\nfoundations using utility maximization theory, demonstrating how equal\nallocation emerges as a solution under standard economic assumptions. The\ngame-theoretic analysis explores the rule's effectiveness across different\nhousehold structures, revealing potential strategic advantages in financial\ndecision-making. We investigate psychological factors influencing financial\nchoices, including cognitive biases and neurobiological mechanisms that impact\neconomic behavior. Technological approaches, such as AI-driven personalization,\nblockchain tracking, and smart contract applications, are examined for their\npotential to support financial planning. Empirical validation using U.S. Census\ndata and longitudinal studies assesses the rule's performance across various\nhousehold types. Stress testing under different economic conditions provides\ninsights into its adaptability and resilience. The research integrates\nmathematical analysis with behavioral insights and technological perspectives\nto develop a comprehensive approach to household financial management.",
        "Inclusiveness and economic development have been slowed by the pandemics and\nmilitary conflicts. This study investigates the main determinants of\ninclusiveness at the European level. A multi-method approach is used, with\nPrincipal Component Analysis (PCA) applied to create the Inclusiveness Index\nand Generalised Method of Moments (GMM) analysis used to investigate the\ndeterminants of inclusiveness. The data comprises a range of 22 years, from\n2000 to 2021, for 32 European countries. The determinants of inclusiveness and\ntheir effects were identified. First, economic growth, industrial upgrading,\nelectricity consumption, digitalisation, and the quantitative aspect of\ngovernance, all have a positive impact on inclusive growth in Europe. Second,\nthe level of CO2 emissions and inflation have a negative impact on\ninclusiveness. Tomorrow's inclusive and sustainable growth must include\ninvestments in renewable energy, digital infrastructure, inequality policies,\nsustainable governance, human capital, and inflation management. These findings\ncan help decision makers design inclusive growth policies.",
        "We investigate the role of foreign direct investment (FDI) in the\ntransitional dynamics of host countries by using an optimal growth model. FDI\nmay be beneficial for the host country because local people can work for\nmultinational firms to get a favorable salary. However, if the host country\nonly focuses on FDI, it may face a middle-income trap. We show that if the host\ncountry invests in research and development, its economy may have sustained\ngrowth. Moreover, in this case, FDI helps the host country only at the first\nstages of its development process.",
        "We study the long term dynamics of the stock portfolios owned by single\nFinnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001\nand 2021. Using the Herfindahl-Hirschman index as a measure of concentration\nfor the composition of stock portfolios, we investigate the concentration of\nFinnish household portfolios both at the level of each individual household and\ntracking the time evolution of an aggregated Finnish household portfolio. We\nalso consider aggregated portfolios of two other macro categories of investors\none comprising Finnish institutional investors and the other comprising foreign\ninvestors. Different macro categories of investors present a different degree\nof concentration of aggregated stock portfolios with highest concentration\nobserved for foreign investors. For individual Finnish retail investors,\nportfolio concentration estimated by the Herfindahl-Hirschman index presents\nhigh values for more than half of the total number of retail investors. In\nspite of the observation that retail stock portfolios are often composed by\njust a few stocks, the concentration of the aggregated stock portfolio for\nFinnish retail investors has a portfolio concentration comparable with the one\nof Finnish institutional investors. Within retail investors, stock portfolios\nof women present a similar pattern of portfolios of men but with a systematic\nhigher level of concentration observed for women both at individual and at\naggregated level.",
        "Consumers are increasingly shopping online, and more and more datasets\ndocumenting consumer search are becoming available. While sequential search\nmodels provide a framework for utilizing such data, they present empirical\nchallenges. A key difficulty arises from the inequality conditions implied by\nthese models, which depend on multiple unobservables revealed during the search\nprocess and necessitate solving or simulating high-dimensional integrals for\nlikelihood-based estimation methods. This paper introduces a novel\nrepresentation of inequalities implied by a broad class of sequential search\nmodels, demonstrating that the empirical content of such models can be\neffectively captured through a specific partial ranking of available actions.\nThis representation reduces the complexity caused by unobservables and provides\na tractable expression for joint probabilities. Leveraging this insight, we\npropose a GHK-style simulation-based likelihood estimator that is simpler to\nimplement than existing ones. It offers greater flexibility for handling\nincomplete search data, incorporating additional ranking information, and\naccommodating complex search processes, including those involving product\ndiscovery. We show that the estimator achieves robust performance while\nmaintaining relatively low computational costs, making it a practical and\nversatile tool for researchers and practitioners.",
        "This paper integrates the damped harmonic oscillator into DSGE models to\nbetter capture delayed economic adjustments. By introducing a damping\ncoefficient, I model economic recoveries as under-damped, critically damped, or\nover-damped processes. Numerical simulations illustrate how different damping\nlevels affect recovery speed and stability. This approach enhances DSGE models'\nrealism, offering insights into historical economic crises and improving\nmacroeconomic forecasting.",
        "This study explores the economic value of Aleppo pine forests, a unique and\nthreatened ecosystem in the border region of central Tunisia. These forests\nplay a vital role in supporting small rural communities, but face increasing\npressures and restrictions on their use. This research aims to assign a\nmonetary value to forest conservation, considering the region's specific\nsocio-economic context. Strategies for empowering local residents as key actors\nin developing sustainable cross-border initiatives are further investigated.\nEmploying the contingent valuation method, a survey of 350 local residents and\ninternational users was conducted to assess their willigness to pay fo forest\nconservation efforts. Logistic regression analysis revealed that\nsociodemographic factors, such as monthly income and preferred payment method,\nsignificantly influence both and the likehood of participation. These findingd\nhighlight the feasibility and importance of reconciling economic development\nwith ecological sustainability in this critical region.",
        "On the way towards carbon neutrality, climate stress testing provides\nestimates for the physical and transition risks that climate change poses to\nthe economy and the financial system. Missing firm-level CO2 emissions data\nseverely impedes the assessment of transition risks originating from carbon\npricing. Based on the individual emissions of all Hungarian firms (410,523), as\nestimated from their fossil fuel purchases, we conduct a stress test of both\nactual and hypothetical carbon pricing policies. Using a simple 1:1 economic\nABM and introducing the new carbon-to-profit ratio, we identify firms that\nbecome unprofitable and default, and estimate the respective loan write-offs.\nWe find that 45% of all companies are directly exposed to carbon pricing. At a\nprice of 45 EUR\/t, direct economic losses of 1.3% of total sales and bank\nequity losses of 1.2% are expected. Secondary default cascades in supply chain\nnetworks could increase these losses by 300% to 4000%, depending on firms'\nability to substitute essential inputs. To reduce transition risks, firms\nshould reduce their dependence on essential inputs from supply chains with high\nCO2 exposure. We discuss the implications of different policy implementations\non these transition risks.",
        "This study introduces a novel set of variables, namely the centre-of-momentum\nvariables, $\\theta_{\\textrm{COM}}$ and $E_{\\textrm{COM}}$, designed to isolate\nfinal-state interactions (FSI) from other aspects of neutrino-nucleus\ninteractions. Through detailed simulation studies, this work demonstrates the\nability of these variables to distinguish FSI contributions with minimal\ndependence on the nuclear initial state and, practically, on the neutrino flux,\nhighlighting their potential for advancing FSI modelling. With high-purity\nneutrino-hydrogen interaction selections, $\\theta_{\\textrm{COM}}$ offers the\nfirst opportunity for a direct cross-comparison among different neutrino\ncross-section experiments.",
        "Fixed-node diffusion quantum Monte Carlo (FN-DMC) is a widely-trusted\nmany-body method for solving the Schr\\\"{o}dinger equation, known for its\nreliable predictions of material and molecular properties. Furthermore, its\nexcellent scalability with system complexity and near-perfect utilization of\ncomputational power makes FN-DMC ideally positioned to leverage new advances in\ncomputing to address increasingly complex scientific problems. Even though the\nmethod is widely used as a computational gold standard, reproducibility across\nthe numerous FN-DMC code implementations has yet to be demonstrated. This\ndifficulty stems from the diverse array of DMC algorithms and trial wave\nfunctions, compounded by the method's inherent stochastic nature. This study\nrepresents a community-wide effort to address the titular question, affirming\nthat: Yes, FN-DMC is reproducible (when handled with care). Using the\nwater-methane dimer as the canonical test case, we compare results from eleven\ndifferent FN-DMC codes and show that the approximations to treat the\nnon-locality of pseudopotentials are the primary source of the discrepancies\nbetween them. In particular, we demonstrate that, for the same choice of\ndeterminantal component in the trial wave function, reliable and reproducible\npredictions can be achieved by employing the T-move (TM), the determinant\nlocality approximation (DLA), or the determinant T-move (DTM) schemes, while\nthe older locality approximation (LA) leads to considerable variability in\nresults. This work lays the foundation to establish accurate and reproducible\nFN-DMC estimates for all future studies across applications in materials\nscience, physics, chemistry, and biology.",
        "The hexagonal MnTe is a prime material candidate for altermagnets, an\nemerging class of magnetic compounds characterized by the nontrivial interplay\nof antiparallel spin arrangements with their underlying crystal structures.\nRecognizing precise knowledge of crystal symmetry as the cornerstone of the\nspin-group classification scheme, we report here a native\ninversion-symmetry-breaking structural distortion in this compound that has\npreviously been overlooked. Through optical polarimetry experiments and\nfirst-principle calculations, we show that MnTe belongs to the\nnoncentrosymmetric $D_{3h}$ group, effectively resolving key inconsistencies in\nthe earlier interpretations of Raman spectroscopy data. Our finding impacts the\nsymmetry analysis of MnTe within the altermagnetic class and sheds light on the\nmechanism of its magneto-controllable N\\'eel order.",
        "We present a novel microfabricated neutral atom source for quantum\ntechnologies that can be easily integrated onto microchip devices using\nwell-established MEMS fabrication techniques, and contrast this to conventional\noff-chip ion loading mechanisms. The heating filament of the device is shown to\nbe as small as 90$\\times$90 $\\mu$m$^2$. Testing of the $^{171}$Yb fluorescence\nresponse is found to be in the low tens of milliseconds, two orders of\nmagnitude faster compared to previous literature at a power of milliwatts\nmaking it desirable for low-power device packages. We demonstrate how the\nevaporation material can be capped in vacuum to work with materials such as Ba\nthat oxidise easily in air, which can avoid the need for ablation lasers in the\nloading process. We calculate oven lifetimes to be over 10 years of continuous\nuse for commonly used ion species in quantum technology.",
        "We report limits on WIMP dark matter derived from three years of data\ncollected by the COSINE-100 experiment with NaI(Tl) crystals, achieving an\nimproved energy threshold of 0.7 keV. This lowered threshold enhances\nsensitivity in the sub-GeV mass range, extending the reach for direct detection\nof low-mass dark matter. Although no excess of WIMP-like events was observed,\nthe increased sensitivity enabled a model-independent comparison between the\nexpected WIMP signal rate-based on mass limits from our data-and DAMA's\nreported modulation amplitude. Our findings strongly disfavor the DAMA signal\nas originating from WIMP interactions, fully excluding DAMA\/LIBRA 3$\\sigma$\nallowed regions and providing enhanced WIMP mass limits by an order of\nmagnitude in the spin-independent model compared to previous results. In the\nspin-dependent model, cross-section upper limits were obtained in the mass\nrange [0.1-5.0] GeV\/c$^2$, with additional sensitivity to sub-GeV WIMPs through\nthe inclusion of the Migdal effect. These results represent substantial\nprogress in low-mass dark matter exploration and reinforce constraints on the\nlongstanding DAMA claim.",
        "This paper presents a comprehensive software stack architecture for\nintegrating quantum computing (QC) capabilities with High-Performance Computing\n(HPC) environments. While quantum computers show promise as specialized\naccelerators for scientific computing, their effective integration with\nclassical HPC systems presents significant technical challenges. We propose a\nhardware-agnostic software framework that supports both current noisy\nintermediate-scale quantum devices and future fault-tolerant quantum computers,\nwhile maintaining compatibility with existing HPC workflows. The architecture\nincludes a quantum gateway interface, standardized APIs for resource\nmanagement, and robust scheduling mechanisms to handle both simultaneous and\ninterleaved quantum-classical workloads. Key innovations include: (1) a unified\nresource management system that efficiently coordinates quantum and classical\nresources, (2) a flexible quantum programming interface that abstracts\nhardware-specific details, (3) A Quantum Platform Manager API that simplifies\nthe integration of various quantum hardware systems, and (4) a comprehensive\ntool chain for quantum circuit optimization and execution. We demonstrate our\narchitecture through implementation of quantum-classical algorithms, including\nthe variational quantum linear solver, showcasing the framework's ability to\nhandle complex hybrid workflows while maximizing resource utilization. This\nwork provides a foundational blueprint for integrating QC capabilities into\nexisting HPC infrastructures, addressing critical challenges in resource\nmanagement, job scheduling, and efficient data movement between classical and\nquantum resources.",
        "We study the electronic, magnetic, and spin transport properties of the\northorhombic Mn$_{5}$Si$_{3}$ compound in the $AF2$ phase using symmetry\nanalysis and ab-initio calculations. Our ground state energy calculations align\nwith experimental observations, demonstrating that the collinear\nantiferromagnetic (AFM) order, with N\\'{e}el vector in the [010] direction, is\nthe most stable magnetic configuration both with and without spin-orbit\ncoupling (SOC) in a bulk lattice geometry. We identified an unconventional\neight-fold degenerate Dirac nodal line (DNL) close to the Fermi level,\ncharacterized by negligible SOC. This DNL is robustly protected by a unique\ncombination of a pure-spin symmetry and a lattice symmetry together with\nmagnetic space group symmetries. Upon introducing SOC, this degeneracy is\nreduced to two four-fold DNLs, being protected by the combination of\ntime-reversal, partial translation and nonsymmorphic symmetries within the\nmagnetic space group. We predict also a large intrinsic spin Hall conductivity\n(SHC) which correlates with the presence of SOC-induced splitting of these\neight-fold degenerate DNLs near the Fermi level. These intriguing\ncharacteristics position collinear antiferromagnet Mn$_{5}$Si$_{3}$ as a\ncompelling candidate for spintronic applications, particularly in the\ngeneration and detection of spin currents, while remaining compatible with\nmodern silicon technology.",
        "We build upon the state-of-the-art semi-analytic model \\texttt{FEGA24}\n(Formation and Evolution of GAlaxies, \\citealt{contini2024d}), which integrates\nthe latest prescriptions relevant to galaxy formation and evolution, alongside\na comprehensive AGN feedback model. This model incorporates three modes of\nfeedback: negative (preventing excessive cooling), positive (enhancing star\nformation), and hot gas ejection (expelling gas beyond the virial radius of\nhalos). These modes operate in a coordinated manner: the negative mode\nregulates the cooling process, the positive mode promotes bursts of star\nformation, and the hot gas ejection mode expels gas beyond the virial radius\nwhen the AGN is sufficiently powerful. Our updated semi-analytic model,\n\\texttt{FEGA25}, retains the qualitative and quantitative consistency of the\nanalyses presented in \\cite{contini2024d}, while delivering more robust\nresults. Notably, \\texttt{FEGA25} provides a more detailed characterization of\nthe fraction of red galaxies as a function of stellar mass, predicts a main\nsequence of star-forming galaxies more consistent with observations, and\nestimates the fraction of hot gas in halos closer to observed values. These\nfindings underscore the importance of a physical mechanism capable of ejecting\nhot gas beyond the virialized region of dark matter halos without significantly\naltering the stellar and cold gas components. Such a mechanism is crucial to\nensure the proper functioning of other processes, such as cooling and star\nformation. Since supernova feedback is already modeled at its maximum\nefficiency, AGN feedback emerges as the natural candidate for this role.",
        "If a set of charged objects collide in space and the fragments disperse, then\nthis process will emit electromagnetic waves. Classical soft photon theorem\ndetermines the constant term and the leading power law fall-off of the\nwave-form at late and early times in terms of only the momenta and charges of\nthe incoming and outgoing objects. In this paper we determine an infinite set\nof subleading terms in the late and early time expansion of the wave-form,\nwhich also depend only on the momenta and charges of the incoming and outgoing\nparticles. For two-particle scattering, we derive a resummed low-frequency\nelectromagnetic wave-form, as well as the resummed wave-form at early and late\ntimes. In this analysis we ignore the effect of long range gravitational\ninteraction, but our result is unaffected by any other short range interactions\namong the objects.",
        "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and\nparameter estimation.",
        "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.",
        "A graph $G$ is rainbow-$F$-free if it admits a proper edge-coloring without a\nrainbow copy of $F$. The rainbow Tur\\'an number of $F$, denoted\n$\\mathrm{ex^*}(n,F)$, is the maximum number of edges in a rainbow-$F$-free\ngraph on $n$ vertices. We determine bounds on the rainbow Tur\\'an numbers of\nstars with a single edge subdivided twice; we call such a tree with $t$ total\nedges a $t$-edge \\textit{broom} with length-$3$ handle, denoted by $B_{t,3}$.\nWe improve the best known upper bounds on $\\mathrm{ex^*}(n,B_{t,3})$ in all\ncases where $t \\neq 2^s - 2$. Moreover, in the case where $t$ is odd and in a\nfew cases when $t \\equiv 0 \\mod 4$, we provide constructions asymptotically\nachieving these upper bounds. Our results also demonstrate a dependence of\n$\\mathrm{ex^*}(n,B_{t,3})$ on divisibility properties of $t$.",
        "The influence of departures from local thermodynamic equilibrium (LTE) on\nneutral sulfur lines is considered. A grid of corrections is proposed to take\ninto account the influence of departures from LTE for neutral sulfur lines in\nthe visible and infrared spectral regions, including the H-band. The grid is\ncalculated using the atomic model of sulfur incorporating the most up-to-date\ncollision rates with electrons and hydrogen. The inclusion of levels and\ntransitions of ionized sulfur in the atomic model made it possible to expand\nthe range of effective temperatures of stellar photospheres in the grid up to\n10000 K. The atomic model was tested in determining the sulfur abundance of 13\nstars and showed its adequacy in a wide range of fundamental stellar\nparameters. In the spectra of all test stars, the sulfur lines are fitted with\nsimilar abundances of the element, regardless of the degree of influence of the\neffects of deviation from LTE on a particular spectral line. For lines of\nseveral multiplets, the wavelengths and oscillator strengths were refined. A\nlist of S I lines recommended for determining sulfur abundance has been\ncreated.",
        "In magnetoconvection, the flow is governed by the interplay between\ngravitational buoyancy and the Lorentz force, with one of these forces\ndominating in different regimes. In this paper, we develop a model with a\nsingle adjustable parameter that accurately captures the smooth transition from\na buoyancy-dominated regime to one dominated by the Lorentz force. A\nperturbative extension of the model accounts for distinct transition features\nthat occur at high Prandtl numbers. We validate the model for magnetoconvection\nin both the quasistatic regime and at finite magnetic Reynolds numbers using\ndata from direct numerical simulations and existing experimental data sets. The\nmodel contains a natural extension to rotating convection and offers a\npotential generalisation to rotating magnetoconvection.",
        "The detection of gravitational waves has brought to light a population of\nbinary black holes that merge within a Hubble time. Multiple formation channels\ncan contribute to this population, making it difficult to definitively\nassociate particular population features with underlying stellar physics. Black\nhole spins are considered an important discriminator between various channels,\nbut they are less well-measured than masses, making conclusive astrophysical\nstatements using spins difficult thus far. In this paper, we consider the\ndistribution of the effective inspiral spin $\\chi_{\\rm eff}$ -- a quantity much\nbetter measured than individual component spins. We show that non-Gaussian\nfeatures like skewness, asymmetry about zero, and multimodality can naturally\narise in the $\\chi_{\\rm eff}$ distribution when multiple channels contribute to\nthe population. Searching for such features, we find signs of skewness and\nasymmetry already in the current catalogs, but no statistically significant\nsigns of bimodality. These features provide robust evidence for the presence of\na subpopulation with spins preferentially aligned to the binary's orbital\nangular momentum; and we conservatively estimate the fraction of this\nsubpopulation to be at least $12 \\% - 17\\%$ (at $90\\%$ credibility). Our models\ndo not find an excess of non-spinning systems and instead find that at least\n$\\sim 20 \\%$ of the binaries have some degree of negative $\\chi_{\\rm eff}$. The\ndata also suggest that, if preferentially aligned mergers form a significant\nfraction of the population, they must have small spins."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Quantifying Variance in Evaluation Benchmarks",
    "start_abstract":"Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The Llama 3 Herd of Models"
      ],
      "abstract":[
        "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
        "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification",
        "VidCapBench: A Comprehensive Benchmark of Video Captioning for\n  Controllable Text-to-Video Generation",
        "RM-PoT: Reformulating Mathematical Problems and Solving via Program of\n  Thoughts",
        "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite\n  Solar Cell Research",
        "REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent\n  Systems",
        "Generating Causally Compliant Counterfactual Explanations using ASP",
        "URECA: The Chain of Two Minimum Set Cover Problems exists behind\n  Adaptation to Shifts in Semantic Code Search",
        "Reflection of Episodes: Learning to Play Game from Expert and Self\n  Experiences",
        "Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for\n  Mid-term Human Mobility Prediction",
        "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
        "Zweistein: A Dynamic Programming Evaluation Function for Einstein\n  W\\\"urfelt Nicht!",
        "Towards AI-assisted Academic Writing",
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Generalized Optimal AMG Convergence Theory for Stokes Equations Using\n  Smooth Aggregation and Vanka Relaxation Strategies",
        "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "Production, Characteristics and Biological effects of Protonated Small\n  Water Clusters",
        "NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai\n  Legal Question Answering",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Accelerated Preference Elicitation with LLM-Based Proxies",
        "A car-following model with behavioural adaptation to road geometry",
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Trajectory Prediction for Autonomous Driving: Progress, Limitations, and\n  Future Directions",
        "Two simple photon gauges in inflation",
        "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion\n  Prior and Differentiable Physics",
        "Split Adaptation for Pre-trained Vision Transformers",
        "A Detailed Analysis of Close Binary OCs"
      ],
      "abstract":[
        "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
        "Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.",
        "The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps:\/\/github.com\/VidCapBench\/VidCapBench.",
        "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation.",
        "The rapid advancement of perovskite solar cells (PSCs) has led to an\nexponential growth in research publications, creating an urgent need for\nefficient knowledge management and reasoning systems in this domain. We present\na comprehensive knowledge-enhanced system for PSCs that integrates three key\ncomponents. First, we develop Perovskite-KG, a domain-specific knowledge graph\nconstructed from 1,517 research papers, containing 23,789 entities and 22,272\nrelationships. Second, we create two complementary datasets: Perovskite-Chat,\ncomprising 55,101 high-quality question-answer pairs generated through a novel\nmulti-agent framework, and Perovskite-Reasoning, containing 2,217 carefully\ncurated materials science problems. Third, we introduce two specialized large\nlanguage models: Perovskite-Chat-LLM for domain-specific knowledge assistance\nand Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental\nresults demonstrate that our system significantly outperforms existing models\nin both domain-specific knowledge retrieval and scientific reasoning tasks,\nproviding researchers with effective tools for literature review, experimental\ndesign, and complex problem-solving in PSC research.",
        "This benchmark suite provides a comprehensive evaluation framework for\nassessing both individual LLMs and multi-agent systems in real-world planning\nscenarios. The suite encompasses eleven designed problems that progress from\nbasic to highly complex, incorporating key aspects such as multi-agent\ncoordination, inter-agent dependencies, and dynamic environmental disruptions.\nEach problem can be scaled along three dimensions: the number of parallel\nplanning threads, the complexity of inter-dependencies, and the frequency of\nunexpected disruptions requiring real-time adaptation. The benchmark includes\ndetailed specifications, evaluation metrics, and baseline implementations using\ncontemporary frameworks like LangGraph, enabling rigorous testing of both\nsingle-agent and multi-agent planning capabilities. Through standardized\nevaluation criteria and scalable complexity, this benchmark aims to drive\nprogress in developing more robust and adaptable AI planning systems for\nreal-world applications.",
        "This research is focused on generating achievable counterfactual\nexplanations. Given a negative outcome computed by a machine learning model or\na decision system, the novel CoGS approach generates (i) a counterfactual\nsolution that represents a positive outcome and (ii) a path that will take us\nfrom the negative outcome to the positive one, where each node in the path\nrepresents a change in an attribute (feature) value. CoGS computes paths that\nrespect the causal constraints among features. Thus, the counterfactuals\ncomputed by CoGS are realistic. CoGS utilizes rule-based machine learning\nalgorithms to model causal dependencies between features. The paper discusses\nthe current status of the research and the preliminary results obtained.",
        "Adaptation is to make model learn the patterns shifted from the training\ndistribution. In general, this adaptation is formulated as the minimum entropy\nproblem. However, the minimum entropy problem has inherent limitation --\nshifted initialization cascade phenomenon. We extend the relationship between\nthe minimum entropy problem and the minimum set cover problem via Lebesgue\nintegral. This extension reveals that internal mechanism of the minimum entropy\nproblem ignores the relationship between disentangled representations, which\nleads to shifted initialization cascade. From the analysis, we introduce a new\nclustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA).\nURECA is an efficient clustering algorithm for the leverage of the\nrelationships between disentangled representations. The update rule of URECA\ndepends on Thresholdly-Updatable Stationary Assumption to dynamics as a\nreleased version of Stationary Assumption. This assumption helps URECA to\ntransport disentangled representations with no errors based on the\nrelationships between disentangled representations. URECA also utilize\nsimulation trick to efficiently cluster disentangled representations. The wide\nrange of evaluations show that URECA achieves consistent performance gains for\nthe few-shot adaptation to diverse types of shifts along with advancement to\nState-of-The-Art performance in CoSQA in the scenario of query shift.",
        "StarCraft II is a complex and dynamic real-time strategy (RTS) game\nenvironment, which is very suitable for artificial intelligence and\nreinforcement learning research. To address the problem of Large Language\nModel(LLM) learning in complex environments through self-reflection, we propose\na Reflection of Episodes(ROE) framework based on expert experience and\nself-experience. This framework first obtains key information in the game\nthrough a keyframe selection method, then makes decisions based on expert\nexperience and self-experience. After a game is completed, it reflects on the\nprevious experience to obtain new self-experience. Finally, in the experiment,\nour method beat the robot under the Very Hard difficulty in TextStarCraft II.\nWe analyze the data of the LLM in the process of the game in detail, verified\nits effectiveness.",
        "Predicting individual mobility patterns is crucial across various\napplications. While current methods mainly focus on predicting the next\nlocation for personalized services like recommendations, they often fall short\nin supporting broader applications such as traffic management and epidemic\ncontrol, which require longer period forecasts of human mobility. This study\naddresses mid-term mobility prediction, aiming to capture daily travel patterns\nand forecast trajectories for the upcoming day or week. We propose a novel\nMulti-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to\nefficiently extract spatial and temporal information by decoupling daily\ntrajectories into distinct location-duration chains. Our approach employs a\nhierarchical encoder to model multi-scale temporal patterns, including daily\nrecurrence and weekly periodicity, and utilizes a transformer-based decoder to\nglobally attend to predicted information in the location or duration chain.\nAdditionally, we introduce a spatial heterogeneous graph learner to capture\nmulti-scale spatial relationships, enhancing semantic-rich representations.\nExtensive experiments, including statistical physics analysis, are conducted on\nlarge-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay\nArea, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to\nepidemic modeling in Boston, MSTDP significantly outperforms the\nbest-performing baseline, achieving a remarkable 62.8% reduction in MAE for\ncumulative new cases.",
        "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
        "This paper introduces Zweistein, a dynamic programming evaluation function\nfor Einstein W\\\"urfelt Nicht! (EWN). Instead of relying on human knowledge to\ncraft an evaluation function, Zweistein uses a data-centric approach that\neliminates the need for parameter tuning. The idea is to use a vector recording\nthe distance to the corner of all pieces. This distance vector captures the\nessence of EWN. It not only outperforms many traditional EWN evaluation\nfunctions but also won first place in the TCGA 2023 competition.",
        "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "This paper discusses our recent generalized optimal algebraic multigrid (AMG)\nconvergence theory applied to the steady-state Stokes equations discretized\nusing Taylor-Hood elements ($\\pmb{ \\mathbb{P}}_2\/\\mathbb{P}_{1}$). The\ngeneralized theory is founded on matrix-induced orthogonality of the left and\nright eigenvectors of a generalized eigenvalue problem involving the system\nmatrix and relaxation operator. This framework establishes a rigorous lower\nbound on the spectral radius of the two-grid error-propagation operator,\nenabling precise predictions of the convergence rate for symmetric indefinite\nproblems, such as those arising from saddle-point systems. We apply this theory\nto the recently developed monolithic smooth aggregation AMG (SA-AMG) solver for\nStokes, constructed using evolution-based strength of connection, standard\naggregation, and smoothed prolongation. The performance of these solvers is\nevaluated using additive and multiplicative Vanka relaxation strategies.\nAdditive Vanka relaxation constructs patches algebraically on each level,\nresulting in a nonsymmetric relaxation operator due to the partition of unity\nbeing applied on one side of the block-diagonal matrix. Although symmetry can\nbe restored by eliminating the partition of unity, this compromises\nconvergence. Alternatively, multiplicative Vanka relaxation updates velocity\nand pressure sequentially within each patch, propagating updates\nmultiplicatively across the domain and effectively addressing velocity-pressure\ncoupling, ensuring a symmetric relaxation. We demonstrate that the generalized\noptimal AMG theory consistently provides accurate lower bounds on the\nconvergence rate for SA-AMG applied to Stokes equations. These findings suggest\npotential avenues for further enhancement in AMG solver design for saddle-point\nsystems.",
        "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5.",
        "The production and characteristics of protonated small water clusters (PSWCs)\nwere reported in this work, where in electrospray ionization (ESI) of pure\nwater, the species obtained were singly charged molecular ions consisting of 2,\n3, 4 or 5 water molecules attached to a hydrogen ion, [(H2O)n+H]+, where n = 2,\n3, 4 or 5. We proposed a new type of PSWCs structure: 2, 3, 4, 5 water\nmolecules wrapped around a hydrogen ion which is located at the electrical and\ngeometric center, forming a very stable molecular structure. Furthermore,\nbiological tests of the PSWCs on mitochondrial function of intestinal\nepithelial cells and liver cells in mice showed the better therapeutic effect\non inflammatory bowel diseases compared to that of the biologic agent\nInfliximab.",
        "The application of large language models (LLMs) in the legal domain holds\nsignificant potential for information retrieval and question answering, yet\nThai legal QA systems face challenges due to a lack of standardized evaluation\nbenchmarks and the complexity of Thai legal structures. This paper introduces\nNitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering\ngeneral Thai financial law, and the NitiBench-Tax, which includes real-world\ntax law cases requiring advanced legal reasoning. We evaluate\nretrieval-augmented generation (RAG) and long-context LLM-based approaches to\naddress three key research questions: the impact of domain-specific components\nlike section-based chunking and cross-referencing, the comparative performance\nof different retrievers and LLMs, and the viability of long-context LLMs as an\nalternative to RAG. Our results show that section-based chunking significantly\nimproves retrieval and end-to-end performance, current retrievers struggle with\ncomplex queries, and long-context LLMs still underperform RAG-based systems in\nThai legal QA. To support fair evaluation, we propose tailored multi-label\nretrieval metrics and the use of an LLM-as-judge for coverage and contradiction\ndetection method. These findings highlight the limitations of current Thai\nlegal NLP solutions and provide a foundation for future research in the field.\nWe also open-sourced our codes and dataset to available publicly.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms.",
        "Understanding the effect of road geometry on human driving behaviour is\nessential for both road safety studies and traffic microsimulation. Research on\nthis topic is still limited, mainly focusing on free-flow traffic and not\nadequately considering the influence of curvature on car-following dynamics.\nThis work attempts to investigate this issue and model the adaptation of\ncar-following behaviour to horizontal curvature. For this purpose, the maximum\ndesired speed - which mainly determines the free-flow dynamics - is expressed\nas a parsimonious function of the curvature. A spatial anticipation mechanism\nis also included in order to realistically describe the driving behaviour when\napproaching or exiting from curves. The accuracy of the augmented model is\nevaluated using the Modified Intelligent Driver Model (M-IDM) and trajectory\ndata from free-flow and car-following traffic (Naples data and Zen Traffic\nData). The results show that a significant improvement is achieved in free-flow\ndynamics. In car-following situations, improvements are mainly observed at high\nspeed and are dependent on the observed driver. Overall, the analysis\nhighlights the lack of sufficiently spatially extended trajectory data to\ncalibrate and evaluate such driving behaviours.",
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "As the potential for autonomous vehicles to be integrated on a large scale\ninto modern traffic systems continues to grow, ensuring safe navigation in\ndynamic environments is crucial for smooth integration. To guarantee safety and\nprevent collisions, autonomous vehicles must be capable of accurately\npredicting the trajectories of surrounding traffic agents. Over the past\ndecade, significant efforts from both academia and industry have been dedicated\nto designing solutions for precise trajectory forecasting. These efforts have\nproduced a diverse range of approaches, raising questions about the differences\nbetween these methods and whether trajectory prediction challenges have been\nfully addressed. This paper reviews a substantial portion of recent trajectory\nprediction methods and devises a taxonomy to classify existing solutions. A\ngeneral overview of the prediction pipeline is also provided, covering input\nand output modalities, modeling features, and prediction paradigms discussed in\nthe literature. In addition, the paper discusses active research areas within\ntrajectory prediction, addresses the posed research questions, and highlights\nthe remaining research gaps and challenges.",
        "Photon propagators for power-law inflation are constructed in two\none-parameter families of noncovariant gauges, in an arbitrary number of\nspacetime dimensions. In both gauges photon propagators take relatively simple\nforms expressed in terms of scalar propagators and their derivatives. These are\nconsiderably simpler compared to their general covariant gauge counterpart.\nThis makes feasible performing dimensionally regulated loop computations\ninvolving massless vector fields in inflation.",
        "Recent advances in large models have significantly advanced image-to-3D\nreconstruction. However, the generated models are often fused into a single\npiece, limiting their applicability in downstream tasks. This paper focuses on\n3D garment generation, a key area for applications like virtual try-on with\ndynamic garment animations, which require garments to be separable and\nsimulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs\nphysics-plausible, simulation-ready separated garments with sewing patterns and\nhumans from an in-the-wild image. Starting with the image, our approach\ncombines a pre-trained image-to-sewing pattern generation model for creating\ncoarse sewing patterns with a pre-trained multi-view diffusion model to produce\nmulti-view images. The sewing pattern is further refined using a differentiable\ngarment simulator based on the generated multi-view images. Versatile\nexperiments demonstrate that our optimization approach substantially enhances\nthe geometric alignment of the reconstructed 3D garments and humans with the\ninput image. Furthermore, by integrating a texture generation module and a\nhuman motion generation module, we produce customized physics-plausible and\nrealistic dynamic garment demonstrations. Project page:\nhttps:\/\/dress-1-to-3.github.io\/",
        "Vision Transformers (ViTs), extensively pre-trained on large-scale datasets,\nhave become essential to foundation models, allowing excellent performance on\ndiverse downstream tasks with minimal adaptation. Consequently, there is\ngrowing interest in adapting pre-trained ViTs across various fields, including\nprivacy-sensitive domains where clients are often reluctant to share their\ndata. Existing adaptation methods typically require direct data access,\nrendering them infeasible under these constraints. A straightforward solution\nmay be sending the pre-trained ViT to clients for local adaptation, which poses\nissues of model intellectual property protection and incurs heavy client\ncomputation overhead. To address these issues, we propose a novel split\nadaptation (SA) method that enables effective downstream adaptation while\nprotecting data and models. SA, inspired by split learning (SL), segments the\npre-trained ViT into a frontend and a backend, with only the frontend shared\nwith the client for data representation extraction. But unlike regular SL, SA\nreplaces frontend parameters with low-bit quantized values, preventing direct\nexposure of the model. SA allows the client to add bi-level noise to the\nfrontend and the extracted data representations, ensuring data protection.\nAccordingly, SA incorporates data-level and model-level out-of-distribution\nenhancements to mitigate noise injection's impact on adaptation performance.\nOur SA focuses on the challenging few-shot adaptation and adopts patch\nretrieval augmentation for overfitting alleviation. Extensive experiments on\nmultiple datasets validate SA's superiority over state-of-the-art methods and\ndemonstrate its defense against advanced data reconstruction attacks while\npreventing model leakage with minimal computation cost on the client side. The\nsource codes can be found at https:\/\/github.com\/conditionWang\/Split_Adaptation.",
        "In this study, we analyzed the close binary open clusters CWNU 2666 and HSC\n224, which are in close spatial proximity, using photometric and astrometric\ndata from the {\\it Gaia} DR3 catalog. Likely member stars were identified based\non a membership probability threshold ($P \\geq 0.5$), resulting in 106 and 146\nmembers for CWNU 2666 and HSC 224, respectively. The mean proper motion\ncomponents ($\\mu_{\\alpha}\\cos\\delta$, $\\mu_{\\delta}$) were determined to be\n(0.646$\\pm$0.155, -0.769$\\pm$0.124) mas yr$^{-1}$ for CWNU 2666, and\n(0.665$\\pm$0.131, -0.728$\\pm$0.107) mas yr$^{-1}$ for HSC 224. The isochrone\ndistances ($d_{\\rm iso}$) were estimated as 1885$\\pm$44 pc for CWNU 2666 and\n1866$\\pm$29 pc for HSC 224. The corresponding cluster ages ($t$) were derived\nas 160$\\pm$15 Myr and 140$\\pm$15 Myr, respectively. The astrometric and\nfundamental astrophysical parameters derived in this study demonstrate that the\ntwo open clusters are a close pair of open clusters."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The Llama 3 Herd of Models",
    "start_abstract":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Quantifying Variance in Evaluation Benchmarks"
      ],
      "abstract":[
        "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "Factor Modelling for Biclustering Large-dimensional Matrix-valued Time\n  Series",
        "Estimands for single arm dose optimization trials in oncology",
        "Flexible Empirical Bayesian Approaches to Pharmacovigilance for\n  Simultaneous Signal Detection and Signal Strength Estimation in Spontaneous\n  Reporting Systems Data",
        "A retake on the analysis of scores truncated by terminal events",
        "Phylogenetic latent space models for network data",
        "Cross Validation for Correlated Data in Regression and Classification\n  Models, with Applications to Deep Learning",
        "Change-plane analysis in functional response quantile regression",
        "Detection and estimation of vertex-wise latent position shifts across\n  networks",
        "Log-Gaussian Cox Processes on General Metric Graphs",
        "Online Correlation Change Detection for Large-Dimensional Data with An\n  Application to Forecasting of El Ni\\~no Events",
        "Outcome Regression Methods for Analyzing Hybrid Control Studies:\n  Balancing Bias and Variability",
        "Generalized Multi-Linear Models for Sufficient Dimension Reduction on\n  Tensor Valued Predictors",
        "Multi-Hazard Bayesian Hierarchical Model for Damage Prediction",
        "De facto Openness to Immigration",
        "Extended $s$-wave pairing from an emergent Feshbach resonanc in bilayer\n  nickelate superconductors",
        "Optical control of the spin-Hall effect in a two-dimensional hole gas",
        "Quantum-enhanced neural networks for quantum many-body simulations",
        "A Link Between White Dwarf Pulsars and Polars: Multiwavelength\n  Observations of the 9.36-Minute Period Variable Gaia22ayj",
        "Decoding lithium's subtle phase stability with a machine learning force\n  field",
        "Decay of large solutions around shocks to multi-D viscous conservation\n  law with strictly convex flux",
        "Scalable architecture for dark photon searches: Superconducting-qubit\n  proof of principle",
        "On zero-sum Ramsey numbers modulo 3",
        "Isogenies of minimal Cantor systems: from Sturmian to Denjoy and\n  interval exchanges",
        "Modeling reflection and refraction of freeform surfaces",
        "Anisotropy can make a moving active fluid membrane rough or crumpled",
        "Tomographic identification of all molecular orbitals in a wide binding\n  energy range",
        "Star formation in interacting galaxy systems: UVIT imaging of NGC 7252\n  and NGC 5291",
        "Using Lagrangian descriptors to reveal the phase space structure of\n  dynamical systems described by fractional differential equations: Application\n  to the Duffing oscillator"
      ],
      "abstract":[
        "A novel unsupervised learning method is proposed in this paper for\nbiclustering large-dimensional matrix-valued time series based on an entirely\nnew latent two-way factor structure. Each block cluster is characterized by its\nown row and column cluster-specific factors in addition to some common matrix\nfactors which impact on all the matrix time series. We first estimate the\nglobal loading spaces by projecting the observation matrices onto the row or\ncolumn loading space corresponding to common factors. The loading spaces for\ncluster-specific factors are then further recovered by projecting the\nobservation matrices onto the orthogonal complement space of the estimated\nglobal loading spaces. To identify the latent row\/column clusters\nsimultaneously for matrix-valued time series, we provide a $K$-means algorithm\nbased on the estimated row\/column factor loadings of the cluster-specific weak\nfactors. Theoretically, we derive faster convergence rates for global loading\nmatrices than those of the state-of-the-art methods available in the literature\nunder mild conditions. We also propose an one-pass eigenvalue-ratio method to\nestimate the numbers of global and cluster-specific factors. The consistency\nwith explicit convergence rates is also established for the estimators of the\nlocal loading matrices, the factor numbers and the latent cluster memberships.\nNumerical experiments with both simulated data as well as a real data example\nare also reported to illustrate the usefulness of our proposed method.",
        "Phase I dose escalation trials in oncology generally aim to find the maximum\ntolerated dose (MTD). However, with the advent of molecular targeted therapies\nand antibody drug conjugates, dose limiting toxicities are less frequently\nobserved, giving rise to the concept of optimal biological dose (OBD), which\nconsiders both efficacy and toxicity. The Estimand framework presented in the\naddendum of the ICH E9(R1) guidelines strengthens the dialogue between\ndifferent stakeholders by bringing in greater clarity in the clinical trial\nobjectives and by providing alignment between the targeted estimand under\nconsideration and the statistical analysis methods. However, there lacks\nclarity in implementing this framework in early phase dose optimization\nstudies. This manuscript aims at discussing the Estimand framework for dose\noptimization trials in oncology considering efficacy and toxicity through\nutility functions. Such trials should include Pharmacokinetics (PK) data,\ntoxicity data, and efficacy data. Based on these data, the analysis methods\nused to identify the optimized dose\/s are also described. Focusing on\noptimizing the utility function to estimate the OBD, the population-level\nsummary measure should reflect only the properties used for the estimating this\nutility function. A detailed strategy recommendation for intercurrent events\nhas been provided using a real-life oncology case study. Key recommendations\nregarding the estimand attributes include that in a seamless Phase I\/II dose\noptimization trial, the treatment attribute should start when the subject\nreceives the first dose. We argue that such a framework brings in additional\nclarity to dose optimization trial objectives and strengthens the understanding\nof the drug under consideration that would enable the correct dose to move to\nPhase II of clinical development.",
        "Inferring adverse events (AEs) of medical products from Spontaneous Reporting\nSystems (SRS) databases is a core challenge in contemporary pharmacovigilance.\nBayesian methods for pharmacovigilance are attractive for their rigorous\nability to simultaneously detect potential AE signals and estimate their\nstrengths\/degrees of relevance. However, existing Bayesian and empirical\nBayesian methods impose restrictive parametric assumptions and\/or demand\nsubstantial computational resources, limiting their practical utility. This\npaper introduces a suite of novel, scalable empirical Bayes methods for\npharmacovigilance that utilize flexible non-parametric priors and custom,\nefficient data-driven estimation techniques to enhance signal detection and\nsignal strength estimation at a low computational cost. Our highly flexible\nmethods accommodate a broader range of data and achieve signal detection\nperformance comparable to or better than existing Bayesian and empirical\nBayesian approaches. More importantly, they provide coherent and high-fidelity\nestimation and uncertainty quantification for potential AE signal strengths,\noffering deeper insights into the comparative importance and relevance of AEs.\nExtensive simulation experiments across diverse data-generating scenarios\ndemonstrate the superiority of our methods in terms of accurate signal strength\nestimation, as measured by replication root mean squared errors. Additionally,\nour methods maintain or exceed the signal detection performance of\nstate-of-the-art techniques, as evaluated by frequentist false discovery rates\nand sensitivity metrics. Applications on FDA FAERS data for the statin group of\ndrugs reveal interesting insights through Bayesian posterior probabilities.",
        "Analysis of data from randomized controlled trials in vulnerable populations\nrequires special attention when assessing treatment effect by a score\nmeasuring, e.g., disease stage or activity together with onset of prevalent\nterminal events. In reality, it is impossible to disentangle a disease score\nfrom the terminal event, since the score is not clinically meaningful after\nthis event. In this work, we propose to assess treatment interventions\nsimultaneously on disease score and the terminal event. Our proposal is based\non a natural data-generating mechanism respecting that a disease score does not\nexist beyond the terminal event. We use modern semi-parametric statistical\nmethods to provide robust and efficient estimation of the risk of terminal\nevent and expected disease score conditional on no terminal event at a\npre-specified landmark time. We also use the simultaneous asymptotic behavior\nof our estimators to develop a powerful closed testing procedure for\nconfirmatory assessment of treatment effect on both onset of terminal event and\nlevel of disease score. A simulation study mimicking a large-scale outcome\ntrial in chronic kidney patients as well as an analysis of that trial is\nprovided to assess performance.",
        "Latent space models for network data characterize each node through a vector\nof latent features whose pairwise similarities define the edge probabilities\namong pairs of nodes. Although this formulation has led to successful\nimplementations and impactful extensions, the overarching focus has been on\ndirectly inferring node embeddings through the latent features rather than\nlearning the generative process underlying the embedding. This focus prevents\nfrom borrowing information among the features of different nodes and fails to\ninfer complex higher-level architectures regulating the formation of the\nnetwork itself. For example, routinely-studied networks often exhibit\nmultiscale structures informing on nested modular hierarchies among nodes that\ncould be learned via tree-based representations of dependencies among latent\nfeatures. We pursue this direction by developing an innovative phylogenetic\nlatent space model that explicitly characterizes the generative process of the\nnodes' feature vectors via a branching Brownian motion, with branching\nstructure parametrized by a phylogenetic tree. This tree constitutes the main\nobject of interest and is learned under a Bayesian perspective to infer\ntree-based modular hierarchies among nodes that explain heterogenous multiscale\npatterns in the network. Identifiability results are derived along with\nposterior consistency theory, and the inference potentials of the\nnewly-proposed model are illustrated in simulations and two real-data\napplications from criminology and neuroscience, where our formulation learns\ncore structures hidden to state-of-the-art alternatives.",
        "We present a methodology for model evaluation and selection where the\nsampling mechanism violates the i.i.d. assumption. Our methodology involves a\nformulation of the bias between the standard Cross-Validation (CV) estimator\nand the mean generalization error, denoted by $w_{cv}$, and practical\ndata-based procedures to estimate this term. This concept was introduced in the\nliterature only in the context of a linear model with squared error loss as the\ncriterion for prediction performance. Our proposed bias-corrected CV estimator,\n$\\text{CV}_c=\\text{CV}+w_{cv}$, can be applied to any learning model, including\ndeep neural networks, and to a wide class of criteria for prediction\nperformance in regression and classification tasks. We demonstrate the\napplicability of the proposed methodology in various scenarios where the data\ncontains complex correlation structures (such as clustered and spatial\nrelationships) with synthetic data and real-world datasets, providing evidence\nthat the estimator $\\text{CV}_c$ is better than the standard CV estimator. This\npaper is an expanded version of our published conference paper.",
        "Change-plane analysis is a pivotal tool for identifying subgroups within a\nheterogeneous population, yet it presents challenges when applied to functional\ndata. In this paper, we consider a change-plane model within the framework of\nfunctional response quantile regression, capable of identifying and testing\nsubgroups in non-Gaussian functional responses with scalar predictors. The\nproposed model naturally extends the change-plane method to account for the\nheterogeneity in functional data. To detect the existence of subgroups, we\ndevelop a weighted average of the squared score test statistic, which has a\nclosed form and thereby reduces the computational stress. An alternating\ndirection method of multipliers algorithm is formulated to estimate the\nfunctional coefficients and the grouping parameters. We establish the\nasymptotic theory for the estimates based on the reproducing kernel Hilbert\nspace and derive the asymptotic distributions of the proposed test statistic\nunder both null and alternative hypotheses. Simulation studies are conducted to\nevaluate the performance of the proposed approach in subgroup identification\nand hypothesis test. The proposed methods are also applied to two datasets, one\nfrom a study on China stocks and another from the COVID-19 pandemic.",
        "Pairwise network comparison is essential for various applications, including\nneuroscience, disease research, and dynamic network analysis. While existing\nliterature primarily focuses on comparing entire network structures, we address\na vertex-wise comparison problem where two random networks share the same set\nof vertices but allow for structural variations in some vertices, enabling a\nmore detailed and flexible analysis of network differences. In our framework,\nsome vertices retain their latent positions between networks, while others\nundergo shifts. To identify the shifted and unshifted vertices and estimate\ntheir latent position shifts, we propose a method that first derives vertex\nembeddings in a low-rank Euclidean space for each network, then aligns these\nestimated vertex latent positions into a common space to resolve potential\nnon-identifiability, and finally tests whether each vertex is shifted or not\nand estimates the vertex shifts. Our theoretical results establish the test\nstatistic for the algorithms, guide parameter selection, and provide\nperformance guarantees. Simulation studies and real data applications,\nincluding a case-control study in disease research and dynamic network\nanalysis, demonstrate that the proposed algorithms are both computationally\nefficient and effective in extracting meaningful insights from network\ncomparisons.",
        "The modeling of spatial point processes has advanced considerably, yet\nextending these models to non-Euclidean domains, such as road networks, remains\na challenging problem. We propose a novel framework for log-Gaussian Cox\nprocesses on general compact metric graphs by leveraging the Gaussian\nWhittle-Mat\\'ern fields, which are solutions to fractional-order stochastic\ndifferential equations on metric graphs. To achieve computationally efficient\nlikelihood-based inference, we introduce a numerical approximation of the\nlikelihood that eliminates the need to approximate the Gaussian process. This\nmethod, coupled with the exact evaluation of finite-dimensional distributions\nfor Whittle-Mat\\'ern fields with integer smoothness, ensures scalability and\ntheoretical rigour, with derived convergence rates for posterior distributions.\nThe framework is implemented in the open-source MetricGraph R package, which\nintegrates seamlessly with R-INLA to support fully Bayesian inference. We\ndemonstrate the applicability and scalability of this approach through an\nanalysis of road accident data from Al-Ahsa, Saudi Arabia, consisting of over\n150,000 road segments. By identifying high-risk road segments using exceedance\nprobabilities and excursion sets, our framework provides localized insights\ninto accident hotspots and offers a powerful tool for modeling spatial point\nprocesses directly on complex networks.",
        "We consider detecting change points in the correlation structure of streaming\nlarge-dimensional data with minimum assumptions posed on the underlying data\ndistribution. Depending on the $\\ell_1$ and $\\ell_{\\infty}$ norms of the\nsquared difference of vectorized pre-change and post-change correlation\nmatrices, detection statistics are constructed for dense and sparse settings,\nrespectively. The proposed detection procedures possess the bless-dimension\nproperty, as a novel algorithm for threshold selection is designed based on\nsign-flip permutation. Theoretical evaluations of the proposed methods are\nconducted in terms of average run length and expected detection delay.\nNumerical studies are conducted to examine the finite sample performances of\nthe proposed methods. Our methods are effective because the average detection\ndelays have slopes similar to that of the optimal exact CUSUM test. Moreover, a\ncombined $\\ell_1$ and $\\ell_{\\infty}$ norm approach is proposed and has\nexpected performance for transitions from sparse to dense settings. Our method\nis applied to forecast El Ni{\\~n}o events and achieves state-of-the-art hit\nrates greater than 0.86, while false alarm rates are 0. This application\nillustrates the efficiency and effectiveness of our proposed methodology in\ndetecting fundamental changes with minimal delay.",
        "There is growing interest in a hybrid control design in which a randomized\ncontrolled trial is augmented with an external control arm from a previous\ntrial or real world data. Existing methods for analyzing hybrid control studies\ninclude various downweighting and propensity score methods as well as methods\nthat combine downweighting with propensity score stratification. In this\narticle, we describe and discuss methods that make use of an outcome regression\nmodel (possibly in addition to a propensity score model). Specifically, we\nconsider an augmentation method, a G-computation method, and a weighted\nregression method, and note that the three methods provide different\nbias-variance trade-offs. The methods are compared with each other and with\nexisting methods in a simulation study. Simulation results indicate that\nweighted regression compares favorably with other model-based methods that seek\nto improve efficiency by incorporating external control data. The methods are\nillustrated using two examples from urology and infectious disease.",
        "We consider supervised learning (regression\/classification) problems with\ntensor-valued input. We derive multi-linear sufficient reductions for the\nregression or classification problem by modeling the conditional distribution\nof the predictors given the response as a member of the quadratic exponential\nfamily. We develop estimation procedures of sufficient reductions for both\ncontinuous and binary tensor-valued predictors. We prove the consistency and\nasymptotic normality of the estimated sufficient reduction using manifold\ntheory. For continuous predictors, the estimation algorithm is highly\ncomputationally efficient and is also applicable to situations where the\ndimension of the reduction exceeds the sample size. We demonstrate the superior\nperformance of our approach in simulations and real-world data examples for\nboth continuous and binary tensor-valued predictors.",
        "A fundamental theoretical limitation undermines current disaster risk models:\nexisting approaches suffer from two critical constraints. First, conventional\ndamage prediction models remain predominantly deterministic, relying on fixed\nparameters established through expert judgment rather than learned from data.\nSecond, probabilistic frameworks are fundamentally restricted by their\nunderlying assumption of hazard independence, which directly contradicts the\nobserved reality of cascading and compound disasters. By relying on fixed\nexpert parameters and treating hazards as independent phenomena, these models\ndangerously misrepresent the true risk landscape. This work addresses this\nchallenge by developing the Multi-Hazard Bayesian Hierarchical Model (MH-BHM),\nwhich reconceptualizes the classical risk equation beyond its deterministic\norigins. The model's core theoretical contribution lies in reformulating a\nclassical risk formula as a fully probabilistic model that naturally\naccommodates hazard interactions through its hierarchical structure while\npreserving the traditional hazard-exposure-vulnerability framework. Using\ntropical cyclone damage data (1952-2020) from the Philippines as a test case,\nwith out-of-sample validation on recent events (2020-2022), the model\ndemonstrates significant empirical advantages. Key findings include a reduction\nin damage prediction error by 61% compared to a single-hazard model, and 80%\ncompared to a benchmark deterministic model. This corresponds to an improvement\nin damage estimation accuracy of USD 0.8 billion and USD 2 billion,\nrespectively. The improved accuracy enables more effective disaster risk\nmanagement across multiple domains, from optimized insurance pricing and\nnational resource allocation to local adaptation strategies, fundamentally\nimproving society's capacity to prepare for and respond to disasters.",
        "Various factors influence why some countries are more open to immigration\nthan others. Policy is only one of them. We design country-specific measures of\nopenness to immigration that aim to capture de facto levels of openness to\nimmigration, complementing existing de jure measures of immigration, based on\nenacted immigration laws and policy measures. We estimate these for 148\ncountries and three years (2000, 2010, and 2020). For a subset of countries, we\nalso distinguish between openness towards tertiary-educated migrants and less\nthan tertiary-educated migrants. Using the measures, we show that most places\nin the World today are closed to immigration, and a few regions are very open.\nThe World became more open in the first decade of the millennium, an opening\nmainly driven by the Western World and the Gulf countries. Moreover, we show\nthat other factors equal, countries that increased their openness to\nimmigration, reduced their old-age dependency ratios, and experienced slower\nreal wage growth, arguably a sign of relaxing labor and skill shortages.",
        "Since the discovery of unconventional superconductivity in cuprates,\nunraveling the pairing mechanism of charge carriers in doped antiferromagnets\nhas been a long-standing challenge. Motivated by the discovery of high-T$_c$\nsuperconductivity in nickelate bilayer La$_3$Ni$_2$O$_7$ (LNO), we study a\nminimal mixed dimensional (MixD) $t-J$ model supplemented with a repulsive\nCoulomb interaction $V$. When hole-doped, previous numerical simulations\nrevealed that the system exhibits strong binding energies, with a phenomenology\nresembling a BCS-to-BEC crossover accompanied by a Feshbach resonance between\ntwo distinct types of charge carriers. Here, we perform a mean-field analysis\nthat enables a direct observation of the BCS-to-BEC crossover as well as\nmicroscopic insights into the crossover region and the pairing symmetry for\ntwo-dimensional bilayers. We benchmark our mean-field description by comparing\nit to density-matrix renormalization group (DMRG) simulations in quasi-one\ndimensional settings and find remarkably good agreement. For the\ntwo-dimensional system relevant to LNO our mean-field calculations predict a\nBCS pairing gap with an extended $s$-wave symmetry, directly resulting from the\npairing mechanism's Feshbach-origin. Our analysis hence gives insights into\npairing in unconventional superconductors and, further, can be tested in\ncurrently available ultracold atom experiments.",
        "Relativistic effects influence the motion of charged particles in solids by\nintertwining spin and momentum. The resulting phenomena exhibit rich and\nintriguing properties that can unveil radically new quantum devices. In this\ncontext, the two-dimensional hole gas formed in group IV heterostructures is a\nparticularly promising platform, owning to a notable spin-orbit coupling.\nHowever, the exploitation of spin-momentum locking and precise manipulation of\nspin currents has remained elusive thus far. Here we use the modulation-doping\ntechnique to break inversion symmetry at novel Ge1-xSnx\/Ge interfaces and\nexplore spin-orbit phenomena in the emergent Rashba-coupled hole gases.\nMagneto-optical investigations demonstrate the unusual establishment of a\nstaggered band alignment with carrier lifetime in the ns range. Optical spin\norientation is then leveraged to directly inject spin-polarized currents in the\nRashba-split 2D gas. Spin-to-charge conversion is shown to genuinely occur at\nthe staggered gap through the inverse spin-Hall effect. This provides\nunprecedented access to low-order contributions of the spin-orbit Hamiltonian.\nMoreover, it leads to the startling demonstration that the spin Hall angle can\nbe optically controlled by modifying the Rashba coupling through the\nphotoexcitation density. Ge1-xSnx quantum wells thus offer innovative solutions\nand functionalities stemming from their unique spin-dependent properties and\nintriguing quantum phenomena at the crossroad between transport and photonic\nrealms.",
        "Neural quantum states (NQS) have gained prominence in variational quantum\nMonte Carlo methods in approximating ground-state wavefunctions. Despite their\nsuccess, they face limitations in optimization, scalability, and expressivity\nin addressing certain problems. In this work, we propose a quantum-neural\nhybrid framework that combines parameterized quantum circuits with neural\nnetworks to model quantum many-body wavefunctions. This approach combines the\nefficient sampling and optimization capabilities of autoregressive neural\nnetworks with the enhanced expressivity provided by quantum circuits. Numerical\nsimulations demonstrate the scalability and accuracy of the hybrid ansatz in\nspin systems and quantum chemistry problems. Our results reveal that the hybrid\nmethod achieves notably lower relative energy compared to standalone NQS. These\nfindings underscore the potential of quantum-neural hybrid methods for tackling\nchallenging problems in quantum many-body simulations.",
        "White dwarfs (WDs) are the most abundant compact objects, and recent surveys\nhave suggested that over a third of WDs in accreting binaries host a strong (B\n$\\gtrsim$ 1 MG) magnetic field. However, the origin and evolution of WD\nmagnetism remain under debate. Two WD pulsars, AR Sco and J191213.72-441045.1\n(J1912), have been found, which are non-accreting binaries hosting rapidly\nspinning (1.97-min and 5.30-min, respectively) magnetic WDs. The WD in AR Sco\nis slowing down on a $P\/\\dot{P}\\approx 5.6\\times 10^6$ yr timescale. It is\nbelieved they will eventually become polars, accreting systems in which a\nmagnetic WD (B $\\approx 10-240$ MG) accretes from a Roche lobe-filling donor\nspinning in sync with the orbit ($\\gtrsim 78$ min). Here, we present\nmultiwavelength data and analysis of Gaia22ayj, which outbursted in March 2022.\nWe find that Gaia22ayj is a magnetic accreting WD that is rapidly spinning down\n($P\/\\dot{P} = 6.1^{+0.3}_{-0.2}\\times 10^6$ yr) like WD pulsars, but shows\nclear evidence of accretion, like polars. Strong linear polarization (40%) is\ndetected in Gaia22ayj; such high levels have only been seen in the WD pulsar AR\nSco and demonstrate the WD is magnetic. High speed photometry reveals a\n9.36-min period accompanying a high amplitude ($\\sim 2$ mag) modulation. We\nassociate this with a WD spin or spin-orbit beat period, not an orbital period\nas was previously suggested. Fast (60-s) optical spectroscopy reveals a broad\n``hump'', reminiscent of cyclotron emission in polars, between 4000-8000\nAngstrom. We find an X-ray luminosity of $L_X = 2.7_{-0.8}^{+6.2}\\times10^{32}\n\\textrm{ erg s}^{-1}$ in the 0.3-8 keV energy range, while two VLA radio\ncampaigns resulted in a non-detection with a $F_r < 15.8\\mu\\textrm{Jy}$ 3$\n\\sigma$ upper limit. The shared properties of both WD pulsars and polars\nsuggest that Gaia22ayj is a missing link between the two classes of magnetic WD\nbinaries.",
        "Understanding the phase stability of elemental lithium (Li) is crucial for\noptimizing its performance in lithium-metal battery anodes, yet this seemingly\nsimple metal exhibits complex polymorphism that requires proper accounting for\nquantum and anharmonic effects to capture the subtleties in its flat energy\nlandscape. Here we address this challenge by developing an accurate graph\nneural network-based machine learning force field and performing efficient\nself-consistent phonon calculations for bcc-, fcc-, and 9R-Li under\nnear-ambient conditions, incorporating quantum, phonon renormalization and\nthermal expansion effects. Our results reveal the important role of\nanharmonicity in determining Li's thermodynamic properties. The free energy\ndifferences between these phases, particularly fcc- and 9R-Li are found to be\nonly a few meV\/atom, explaining the experimental challenges in obtaining\nphase-pure samples and suggesting a propensity for stacking faults and related\ndefect formation. fcc-Li is confirmed as the ground state at zero temperature\nand pressure, and the predicted bcc-fcc phase boundary qualitatively matches\nexperimental phase transition lines, despite overestimation of the transition\ntemperature and pressure slope. These findings provide crucial insights into\nLi's complex polymorphism and establish an effective computational approach for\nlarge-scale atomistic simulations of Li in more realistic settings for\npractical energy storage applications.",
        "We consider a planar viscous shock for a scalar viscous conservation law with\na strictly convex flux in multi-dimensional setting, where the transversal\ndirection is periodic. We first show the contraction property for any solutions\nevolving from a large bounded initial perturbation in $L^2$ of the viscous\nshock. The contraction holds up to a dynamical shift, and it is measured by a\nweighted relative entropy. This result for the contraction extends the existing\nresult in 1D \\cite{Kang19} to the multi-dimensional case. As a consequence, if\nthe large bounded initial $L^2$-perturbation is also in $L^1$, then the large\nperturbation decays of rate $t^{-1\/4}$ in $L^2$, up to a dynamical shift that\nis uniformly bounded in time. This is the first result for the quantitative\nestimate converging to a planar shock under large perturbations.",
        "The dark photon is a well-motivated candidate of dark matter due to its\npotential to open the window of new physics beyond the Standard Model. A\nfundamental mass-range-sensitivity dilemma is always haunting the dark photon\nsearching experiments: The resonant haloscopes have excellent sensitivity but\nare narrowband, and vice versa for the non-resonant ones. A scalable\narchitecture integrating numerous resonant haloscopes will be a desirable\nsolution to this dilemma. However, even the concept of scalable searching\nremains rarely explored, due to the size limitation of conventional haloscopes\nimposed by the dark photon wavelength. Here we propose and demonstrate a novel\narchitecture using superconducting qubits as sub-wavelength haloscope units. By\nvirtue of the scalability of superconducting qubits, it is possible to\nintegrate multiple qubits with different frequencies on a chip-scale device.\nFurthermore, the frequencies of the qubits can be tuned to extend the searching\nmass range. Thus, our architectures allow for searching for dark photons in a\nbroad mass range with high sensitivity. As a proof-of-principle experiment, we\ndesigned and fabricated a three-qubit chip and successfully demonstrated a\nscalable dark-photon searching. Our work established constraints on dark\nphotons in the mass range of 15.632 $\\mu$eV$\\sim$15.638 $\\mu$eV, 15.838\n$\\mu$eV$\\sim$15.845 $\\mu$eV, and 16.463 $\\mu$eV$\\sim$16.468 $\\mu$eV,\nsimultaneously, and the constraints are much more stringent than the cosmology\nconstraints. Our work can be scaled up in the future to boost the scrutiny of\nnew physics and extended to search for more dark matter candidates, including\ndark photons, axions and axion-like particles.",
        "We start with a systematic study of the zero-sum Ramsey numbers. For a graph\n$G$ with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, the zero-sum Ramsey number is defined as\nthe smallest positive integer $R(G, \\mathbb{Z}_3)$ such that for every $n \\geq\nR(G, \\mathbb{Z}_3)$ and every edge-colouring $f$ of $K_n$ using $\\mathbb{Z}_3$,\nthere is a zero-sum copy of $G$ in $K_n$ coloured by $f$, that is: $\\sum_{e \\in\nE(G)} f(e) \\equiv 0 \\ (\\!\\!\\!\\!\\mod 3)$.\n  Only sporadic results are known for these Ramsey numbers, and we discover\nmany new ones. In particular we prove that for every forest $F$ on $n$ vertices\nand with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, $R(F, \\mathbb{Z}_3) \\leq n+2$, and this\nbound is tight if all the vertices of $F$ have degrees $1 \\ (\\!\\!\\!\\!\\mod 3)$.\nWe also determine exact values of $R(T, \\mathbb{Z}_3)$ for infinite families of\ntrees.",
        "This work is motivated by the study of continued fraction expansions of real\nnumbers: we describe in dynamical terms their orbits under the action of\n$\\mathrm{PGL}_2(\\mathbb{Q})$. A real number gives rise to a Sturmian system\nencoding a rotation of the circle. It is well known that\n$\\mathrm{PGL}_2(\\mathbb{Z})$-equivalence of real numbers, characterized by the\ntails of their continued fraction expansions, amounts to flow equivalence of\nSturmian systems. We show that the multiplicative action of $m\\in \\mathbb{Z}$\non a real number corresponds to taking the $m$th-power followed by what we call\nan infinitesimal 2-asymptotic factor of its Sturmian system.\n  This leads us to introduce the notion of isogeny between zero-dimensional\nsystems: it combines virtual flow equivalences and infinitesimal asymptotic\nequivalences. We develop tools for classifying systems up to isogeny involving\ncohomological invariants and states. We then use this to give a complete\ndescription of $\\mathrm{PSL}_2(\\mathbb{Q})$-equivalence of real numbers in\nterms of Sturmian systems. We classify Denjoy systems up to isogenies within\nthis class via the action of $\\mathrm{PGL}_{2}(\\mathbb{Q})$ on their\ninvariants.\n  We also investigate eventual flow equivalence of Sturmian systems: we show\nthat for non-quadratic parameters it amounts to topological conjugacy and for\nquadratic parameters it implies total flow equivalence and other arithmetic\nconstraints.\n  In another direction, we consider interval exchanges satisfying Keane's\ncondition. We characterize flow equivalence in terms of interval-induced\nsubsystems (or the tails of their paths in the bilateral Rauzy induction\ndiagram). Finally we find rational invariants for isogeny involving the length\nmodules and SAF invariants of the associated ergodic measures. This leads to a\nconjecture for their classification up to isogeny, which we prove in the\ntotally ergodic case.",
        "In this work, we present a detailed procedure of computer implementation of\nthe laws of refraction and reflection on an arbitrary surface with rotational\nsymmetry with respect to the propagation axis. The goal is to facilitate the\nunderstanding and application of these physical principles in a computational\ncontext. This enables students and instructors alike to develop simulations and\ninteractive applications that faithfully replicate the behavior of light and\nsound propagating in a diversity of media separated by arbitrary surfaces. In\nparticular it can help to explore freeform optics. Additionally, we include a\npractical example demonstrating these implementations using either Matlab or\nopen-source Octave programming language.",
        "We present a hydrodynamic theory of anisotropic and inversion-asymmetric\nmoving active permeable fluid membranes. These are described by an anisotropic\nKardar-Parisi-Zhang equation. Depending upon the anisotropy parameters, the\nmembrane can be large-scale anisotropic and logarithmically rough with\ntranslational quasi long range order and orientational long range order,\ntogether with the relaxational dynamics being logarithmically faster than\nordinary diffusion. For other choices of the anisotropy parameters, the\nmembrane is either effectively isotropic and algebraically rough with\ntranslational short, but orientational long range order, or crumpled.",
        "In the past decade, photoemission orbital tomography (POT) has evolved into a\npowerful tool to investigate the electronic structure of organic molecules\nadsorbed on surfaces. Here we show that POT allows for the comprehensive\nexperimental identification of all molecular orbitals in a substantial binding\nenergy range, in the present case more than 10 eV. Making use of the angular\ndistribution of photoelectrons as a function of binding energy, we exemplify\nthis by extracting orbital-resolved partial densities of states (pDOS) for 15\n$\\pi$ and 23 $\\sigma$ orbitals from the experimental photoemission intensities\nof the prototypical organic molecule bisanthene (C$_{28}$H$_{14}$) on a Cu(110)\nsurface. In their entirety, these experimentally measured orbital-resolved pDOS\nfor an essentially complete set of orbitals serve as a stringent benchmark for\nelectronic structure methods, which we illustrate by performing density\nfunctional theory (DFT) calculations employing four frequently-used\nexchange-correlation functionals. By computing the respective\nmolecular-orbital-projected densities of states of the bisanthene\/Cu(110)\ninterface, a one-to-one comparison with experimental data for an unprecedented\nnumber of 38 orbital energies becomes possible. The quantitative analysis of\nour data reveals that the range-separated hybrid functional HSE performs best\nfor the investigated organic\/metal interface. At a more fundamental level, the\nremarkable agreement between the experimental and the Kohn-Sham orbital\nenergies over a binding energy range larger than 10\\,eV suggests that --\nperhaps unexpectedly -- Kohn-Sham orbitals approximate Dyson orbitals, which\nwould rigorously account for the electron extraction process in photoemission\nspectroscopy but are notoriously difficult to compute, in a much better way\nthan previously thought.",
        "Interactions play a significant role in the formation and evolution of\ngalaxies in the Universe. The galaxy systems, NGC 7252 and NGC 5291 are two\nnearby interacting systems that are hosting Tidal Dwarf Galaxies (TDGs) and\nstar-forming knots. The present work aims (a) To determine the\nattenuation-corrected star formation rate (SFR) of the interacting system NGC\n7252 (b) To compare the star formation in the NGC 7252 system with that of the\nNGC 5291 system (c) To explore the relation between surface densities of gas\nand SFR in these two systems. The study utilises high-resolution FUV and NUV\nimaging data from the Ultraviolet Imaging Telescope (UVIT) on board AstroSat.\nSix star-forming regions, including the merger remnant, were identified in the\nNGC 7252 system. The SFR corrected for attenuation of the knots in the NGC 7252\nsystem is determined using the continuum slope (\\beta) calculated from the\nFUV-NUV colour. It has been observed that the attenuation-corrected SFR values\nof the knots in this system fall within the range of SFR values determined for\nthe NGC 5291 knots. The TDGs in both systems adhere to the same\nKennicutt-Schmidt (KS) relation as regular spiral galaxies.",
        "We showcase the utility of the Lagrangian descriptors method in qualitatively\nunderstanding the underlying dynamical behavior of dynamical systems governed\nby fractional-order differential equations. In particular, we use the\nLagrangian descriptors method to study the phase space structure of the\nunforced and undamped Duffing oscillator when its time evolution is governed by\nfractional-order differential equations. In our study, we implement two types\nof fractional derivatives, namely the standard Gr\\\"unwald-Letnikov method,\nwhich is a finite difference approximation of the Riemann-Liouville fractional\nderivative, and a Gr\\\"unwald-Letnikov method with a correction term that\napproximates the Caputo fractional derivative. While there is no issue with\nforward-time integrations needed for the evaluation of Lagrangian descriptors,\nwe discuss in detail ways to perform the non-trivial task of backward-time\nintegrations and implement two methods for this purpose: a `nonlocal implicit\ninverse' technique and a `time-reverse inverse' approach. We analyze the\ndifferences in the Lagrangian descriptors results due to the two backward-time\nintegration approaches, discuss the physical significance of these differences,\nand eventually argue that the nonlocal implicit inverse implementation of the\nGr\\\"unwald-Letnikov fractional derivative manages to reveal the phase space\nstructure of fractional-order dynamical systems correctly."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"RNA-Seq: a revolutionary tool for transcriptomics",
    "start_abstract":"RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "White-Box Transformers via Sparse Rate Reduction"
      ],
      "abstract":[
        "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Deep learning for temporal super-resolution 4D Flow MRI",
        "Continuous Diffusion Model for Language Modeling",
        "ADO: Automatic Data Optimization for Inputs in LLM Prompts",
        "Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data\n  Combination",
        "Long-range Brain Graph Transformer",
        "LLM-driven Knowledge Distillation for Dynamic Text-Attributed Graphs",
        "Overcoming Fake Solutions in Semi-Dual Neural Optimal Transport: A\n  Smoothing Approach for Learning the Optimal Transport Plan",
        "Compositional World Knowledge leads to High Utility Synthetic data",
        "T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model",
        "Geoinformatics-Guided Machine Learning for Power Plant Classification",
        "Dynamic Low-Rank Sparse Adaptation for Large Language Models",
        "Incorporating graph neural network into route choice model",
        "Efficient Time Series Forecasting via Hyper-Complex Models and Frequency\n  Aggregation",
        "On the Fly Adaptation of Behavior Tree-Based Policies through\n  Reinforcement Learning",
        "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry",
        "Synthetic $\\pi$-flux system in 2D superconducting qubit array with\n  tunable coupling",
        "A Note on Exact State Visit Probabilities in Two-State Markov Chains",
        "Erd\\H{o}s's integer dilation approximation problem and GCD graphs",
        "Cracking Vector Search Indexes",
        "Computing Game Symmetries and Equilibria That Respect Them",
        "Revealing quantum operator scrambling via measuring Holevo information\n  on digital quantum simulators",
        "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
        "A Geometric Perspective for High-Dimensional Multiplex Graphs",
        "Employee Turnover Prediction: A Cross-component Attention Transformer\n  with Consideration of Competitor Influence and Contagious Effect",
        "Towards High-performance Spiking Transformers from ANN to SNN Conversion",
        "Cohort-attention Evaluation Metric against Tied Data: Studying\n  Performance of Classification Models in Cancer Detection",
        "Stability, periodic orbits and KAM tori in the dynamics of the three\n  fixed centers problem",
        "The Unbearable Lightness of Prompting: A Critical Reflection on the\n  Environmental Impact of genAI use in Design Education"
      ],
      "abstract":[
        "4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique\nfor volumetric, time-resolved blood flow quantification. However, apparent\ntrade-offs between acquisition time, image noise, and resolution limit clinical\napplicability. In particular, in regions of highly transient flow, coarse\ntemporal resolution can hinder accurate capture of physiologically relevant\nflow variations. To overcome these issues, post-processing techniques using\ndeep learning have shown promising results to enhance resolution post-scan\nusing so-called super-resolution networks. However, while super-resolution has\nbeen focusing on spatial upsampling, temporal super-resolution remains largely\nunexplored. The aim of this study was therefore to implement and evaluate a\nresidual network for temporal super-resolution 4D Flow MRI. To achieve this, an\nexisting spatial network (4DFlowNet) was re-designed for temporal upsampling,\nadapting input dimensions, and optimizing internal layer structures. Training\nand testing were performed using synthetic 4D Flow MRI data originating from\npatient-specific in-silico models, as well as using in-vivo datasets. Overall,\nexcellent performance was achieved with input velocities effectively denoised\nand temporally upsampled, with a mean absolute error (MAE) of 1.0 cm\/s in an\nunseen in-silico setting, outperforming deterministic alternatives (linear\ninterpolation MAE = 2.3 cm\/s, sinc interpolation MAE = 2.6 cm\/s). Further, the\nnetwork synthesized high-resolution temporal information from unseen\nlow-resolution in-vivo data, with strong correlation observed at peak flow\nframes. As such, our results highlight the potential of utilizing data-driven\nneural networks for temporal super-resolution 4D Flow MRI, enabling\nhigh-frame-rate flow quantification without extending acquisition times beyond\nclinically acceptable limits.",
        "Diffusion models have emerged as a promising alternative to autoregressive\nmodels in modeling discrete categorical data. Yet diffusion models that\ndirectly work on discrete data space do not fully exploit the power of\niterative refinement, as the signals are lost during the transition between\ndiscrete states. Existing continuous diffusion models for discrete data have\nlimited performance compared to discrete approaches, and the unclear link\nbetween them restricts the development of diffusion models for discrete data.\nIn this work, we propose a continuous diffusion model for language modeling\nthat incorporates the geometry of the underlying categorical distribution. We\nestablish a connection between the discrete diffusion and continuous flow on\nthe statistical manifold, and building on the analogy, we introduce a simple\ndesign for the diffusion process that generalizes previous discrete diffusion\nmodels. We further propose a simulation-free training framework based on radial\nsymmetry and a simple technique to address the high dimensionality of the\nmanifold. Comprehensive experiments on language modeling benchmarks and other\nmodalities show that our method outperforms existing discrete diffusion models\nand approaches the performance of autoregressive models. Codes available at\n\\href{https:\/\/github.com\/harryjo97\/RDLM}{https:\/\/github.com\/harryjo97\/RDLM}.",
        "This study explores a novel approach to enhance the performance of Large\nLanguage Models (LLMs) through the optimization of input data within prompts.\nWhile previous research has primarily focused on refining instruction\ncomponents and augmenting input data with in-context examples, our work\ninvestigates the potential benefits of optimizing the input data itself. We\nintroduce a two-pronged strategy for input data optimization: content\nengineering and structural reformulation. Content engineering involves imputing\nmissing values, removing irrelevant attributes, and enriching profiles by\ngenerating additional information inferred from existing attributes. Subsequent\nto content engineering, structural reformulation is applied to optimize the\npresentation of the modified content to LLMs, given their sensitivity to input\nformat. Our findings suggest that these optimizations can significantly improve\nthe performance of LLMs in various tasks, offering a promising avenue for\nfuture research in prompt engineering. The source code is available at\nhttps:\/\/anonymous.4open.science\/r\/ADO-6BC5\/",
        "Long-term causal inference has drawn increasing attention in many scientific\ndomains. Existing methods mainly focus on estimating average long-term causal\neffects by combining long-term observational data and short-term experimental\ndata. However, it is still understudied how to robustly and effectively\nestimate heterogeneous long-term causal effects, significantly limiting\npractical applications. In this paper, we propose several two-stage style\nnonparametric estimators for heterogeneous long-term causal effect estimation,\nincluding propensity-based, regression-based, and multiple robust estimators.\nWe conduct a comprehensive theoretical analysis of their asymptotic properties\nunder mild assumptions, with the ultimate goal of building a better\nunderstanding of the conditions under which some estimators can be expected to\nperform better. Extensive experiments across several semi-synthetic and\nreal-world datasets validate the theoretical results and demonstrate the\neffectiveness of the proposed estimators.",
        "Understanding communication and information processing among brain regions of\ninterest (ROIs) is highly dependent on long-range connectivity, which plays a\ncrucial role in facilitating diverse functional neural integration across the\nentire brain. However, previous studies generally focused on the short-range\ndependencies within brain networks while neglecting the long-range\ndependencies, limiting an integrated understanding of brain-wide communication.\nTo address this limitation, we propose Adaptive Long-range aware TransformER\n(ALTER), a brain graph transformer to capture long-range dependencies between\nbrain ROIs utilizing biased random walk. Specifically, we present a novel\nlong-range aware strategy to explicitly capture long-range dependencies between\nbrain ROIs. By guiding the walker towards the next hop with higher correlation\nvalue, our strategy simulates the real-world brain-wide communication.\nFurthermore, by employing the transformer framework, ALERT adaptively\nintegrates both short- and long-range dependencies between brain ROIs, enabling\nan integrated understanding of multi-level communication across the entire\nbrain. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER\nconsistently outperforms generalized state-of-the-art graph learning methods\n(including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning\nbased brain network analysis methods (including FBNETGEN, BrainNetGNN,\nBrainGNN, and BrainNETTF) in neurological disease diagnosis. Cases of\nlong-range dependencies are also presented to further illustrate the\neffectiveness of ALTER. The implementation is available at\nhttps:\/\/github.com\/yushuowiki\/ALTER.",
        "Dynamic Text-Attributed Graphs (DyTAGs) have numerous real-world\napplications, e.g. social, collaboration, citation, communication, and review\nnetworks. In these networks, nodes and edges often contain text descriptions,\nand the graph structure can evolve over time. Future link prediction, edge\nclassification, relation generation, and other downstream tasks on DyTAGs\nrequire powerful representations that encode structural, temporal, and textual\ninformation. Although graph neural networks (GNNs) excel at handling structured\ndata, encoding temporal information within dynamic graphs remains a significant\nchallenge. In this work, we propose LLM-driven Knowledge Distillation for\nDynamic Text Attributed Graph (LKD4DyTAG) with temporal encoding to address\nthese challenges. We use a simple, yet effective approach to encode temporal\ninformation in edges so that graph convolution can simultaneously capture both\ntemporal and structural information in the hidden representations. To leverage\nLLM's text processing capabilities for learning richer representations on\nDyTAGs, we distill knowledge from LLM-driven edge representations (based on a\nneighborhood's text attributes) into saptio-temporal representations using a\nlightweight GNN model that encodes temporal and structural information. The\nobjective of knowledge distillation enables the GNN to learn representations\nthat more effectively encode the available structural, temporal, and textual\ninformation in DyTAG. We conducted extensive experimentation on six real-world\nDyTAG datasets to verify the effectiveness of our approach LKD4DyTAG for future\nlink prediction and edge classification task. The results show that our\napproach significantly improves the performance of downstream tasks compared to\nthe baseline models.",
        "We address the convergence problem in learning the Optimal Transport (OT)\nmap, where the OT Map refers to a map from one distribution to another while\nminimizing the transport cost. Semi-dual Neural OT, a widely used approach for\nlearning OT Maps with neural networks, often generates fake solutions that fail\nto transfer one distribution to another accurately. We identify a sufficient\ncondition under which the max-min solution of Semi-dual Neural OT recovers the\ntrue OT Map. Moreover, to address cases when this sufficient condition is not\nsatisfied, we propose a novel method, OTP, which learns both the OT Map and the\nOptimal Transport Plan, representing the optimal coupling between two\ndistributions. Under sharp assumptions on the distributions, we prove that our\nmodel eliminates the fake solution issue and correctly solves the OT problem.\nOur experiments show that the OTP model recovers the optimal transport map\nwhere existing methods fail and outperforms current OT-based models in\nimage-to-image translation tasks. Notably, the OTP model can learn stochastic\ntransport maps when deterministic OT Maps do not exist, such as one-to-many\ntasks like colorization.",
        "Machine learning systems struggle with robustness, under subpopulation\nshifts. This problem becomes especially pronounced in scenarios where only a\nsubset of attribute combinations is observed during training -a severe form of\nsubpopulation shift, referred as compositional shift. To address this problem,\nwe ask the following question: Can we improve the robustness by training on\nsynthetic data, spanning all possible attribute combinations? We first show\nthat training of conditional diffusion models on limited data lead to incorrect\nunderlying distribution. Therefore, synthetic data sampled from such models\nwill result in unfaithful samples and does not lead to improve performance of\ndownstream machine learning systems. To address this problem, we propose CoInD\nto reflect the compositional nature of the world by enforcing conditional\nindependence through minimizing Fisher's divergence between joint and marginal\ndistributions. We demonstrate that synthetic data generated by CoInD is\nfaithful and this translates to state-of-the-art worst-group accuracy on\ncompositional shift tasks on CelebA.",
        "We introduce Test-time Scalable MCTS-enhanced Diffusion Model (T-SCEND), a\nnovel framework that significantly improves diffusion model's reasoning\ncapabilities with better energy-based training and scaling up test-time\ncomputation. We first show that na\\\"ively scaling up inference budget for\ndiffusion models yields marginal gain. To address this, the training of T-SCEND\nconsists of a novel linear-regression negative contrastive learning objective\nto improve the performance-energy consistency of the energy landscape, and a KL\nregularization to reduce adversarial sampling. During inference, T-SCEND\nintegrates the denoising process with a novel hybrid Monte Carlo Tree Search\n(hMCTS), which sequentially performs best-of-N random search and MCTS as\ndenoising proceeds. On challenging reasoning tasks of Maze and Sudoku, we\ndemonstrate the effectiveness of T-SCEND's training objective and scalable\ninference method. In particular, trained with Maze sizes of up to $6\\times6$,\nour T-SCEND solves $88\\%$ of Maze problems with much larger sizes of\n$15\\times15$, while standard diffusion completely fails. Code to reproduce the\nexperiments can be found at https:\/\/github.com\/AI4Science-WestlakeU\/t_scend.",
        "This paper proposes an approach in the area of Knowledge-Guided Machine\nLearning (KGML) via a novel integrated framework comprising CNN (Convolutional\nNeural Networks) and ViT (Vision Transformers) along with GIS (Geographic\nInformation Systems) to enhance power plant classification in the context of\nenergy management. Knowledge from geoinformatics derived through Spatial Masks\n(SM) in GIS is infused into an architecture of CNN and ViT, in this proposed\nKGML approach. It is found to provide much better performance compared to the\nbaseline of CNN and ViT only in the classification of multiple types of power\nplants from real satellite imagery, hence emphasizing the vital role of the\ngeoinformatics-guided approach. This work makes a contribution to the main\ntheme of KGML that can be beneficial in many AI systems today. It makes broader\nimpacts on AI in Smart Cities, and Environmental Computing.",
        "Despite the efficacy of network sparsity in alleviating the deployment strain\nof Large Language Models (LLMs), it endures significant performance\ndegradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs\noffers an intuitive approach to counter this predicament, while it holds\nshortcomings include: 1) The inability to integrate LoRA weights into sparse\nLLMs post-training, and 2) Insufficient performance recovery at high sparsity\nratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA),\na novel method that seamlessly integrates low-rank adaptation into LLM sparsity\nwithin a unified framework, thereby enhancing the performance of sparse LLMs\nwithout increasing the inference latency. In particular, LoSA dynamically\nsparsifies the LoRA outcomes based on the corresponding sparse weights during\nfine-tuning, thus guaranteeing that the LoRA module can be integrated into the\nsparse LLMs post-training. Besides, LoSA leverages Representation Mutual\nInformation (RMI) as an indicator to determine the importance of layers,\nthereby efficiently determining the layer-wise sparsity rates during\nfine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based\non the variability in layer-wise reconstruction errors, allocating an\nappropriate fine-tuning for each layer to reduce the output discrepancies\nbetween dense and sparse LLMs. Extensive experiments tell that LoSA can\nefficiently boost the efficacy of sparse LLMs within a few hours, without\nintroducing any additional inferential burden. For example, LoSA reduced the\nperplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by\n16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on\nGPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU.\nCode is available at https:\/\/github.com\/wzhuang-xmu\/LoSA.",
        "Route choice models are one of the most important foundations for\ntransportation research. Traditionally, theory-based models have been utilized\nfor their great interpretability, such as logit models and Recursive logit\nmodels. More recently, machine learning approaches have gained attentions for\ntheir better prediction accuracy. In this study, we propose novel hybrid models\nthat integrate the Recursive logit model with Graph Neural Networks (GNNs) to\nenhance both predictive performance and model interpretability. To the authors'\nknowldedge, GNNs have not been utilized for route choice modeling, despite\ntheir proven effectiveness in capturing road network features and their\nwidespread use in other transportation research areas. We mathematically show\nthat our use of GNN is not only beneficial for enhancing the prediction\nperformance, but also relaxing the Independence of Irrelevant Alternatives\nproperty without relying on strong assumptions. This is due to the fact that a\nspecific type of GNN can efficiently capture multiple cross-effect patterns on\nnetworks from data. By applying the proposed models to one-day travel\ntrajectory data in Tokyo, we confirmed their higher prediction accuracy\ncompared to the existing models.",
        "Time series forecasting is a long-standing problem in statistics and machine\nlearning. One of the key challenges is processing sequences with long-range\ndependencies. To that end, a recent line of work applied the short-time Fourier\ntransform (STFT), which partitions the sequence into multiple subsequences and\napplies a Fourier transform to each separately. We propose the Frequency\nInformation Aggregation (FIA)-Net, which is based on a novel complex-valued MLP\narchitecture that aggregates adjacent window information in the frequency\ndomain. To further increase the receptive field of the FIA-Net, we treat the\nset of windows as hyper-complex (HC) valued vectors and employ HC algebra to\nefficiently combine information from all STFT windows altogether. Using the\nHC-MLP backbone allows for improved handling of sequences with long-term\ndependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up\nto three times fewer parameters than the equivalent standard window aggregation\nmethod. We evaluate the FIA-Net on various time-series benchmarks and show that\nthe proposed methodologies outperform existing state of the art methods in\nterms of both accuracy and efficiency. Our code is publicly available on\nhttps:\/\/anonymous.4open.science\/r\/research-1803\/.",
        "With the rising demand for flexible manufacturing, robots are increasingly\nexpected to operate in dynamic environments where local -- such as slight\noffsets or size differences in workpieces -- are common. We propose to address\nthe problem of adapting robot behaviors to these task variations with a\nsample-efficient hierarchical reinforcement learning approach adapting Behavior\nTree (BT)-based policies. We maintain the core BT properties as an\ninterpretable, modular framework for structuring reactive behaviors, but extend\ntheir use beyond static tasks by inherently accommodating local task\nvariations. To show the efficiency and effectiveness of our approach, we\nconduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with\nthe manipulator adapting to different obstacle avoidance and pivoting tasks.",
        "One of the challenges in weak gravitational lensing by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, which is known as inversion problem. We introduce a novel\ntheoretical approach to solve the inversion problem. The cornerstone of the\nproposed method lies in a complex formalism that describes the lens mapping as\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which is, in principle, observable from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping with\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations solely depending on the reduced shear field.\nExperimental results for both the Schwarzschild and singular isothermal lens\ndemonstrate the agreement of our proposed method with the analytically\ncomputable solutions.",
        "Flat-band systems provide an ideal platform for exploring exotic quantum\nphenomena, where the strongly suppressed kinetic energy in these flat energy\nbands suggests the potential for exotic phases driven by geometric structure,\ndisorder, and interactions. While intriguing phenomena and physical mechanisms\nhave been unveiled in theoretical models, synthesizing such systems within\nscalable quantum platforms remains challenging. Here, we present the\nexperimental realization of a $\\pi$-flux rhombic system using a two-dimensional\nsuperconducting qubit array with tunable coupling. We experimentally observe\ncharacteristic dynamics, e.g., $\\pi$-flux driven destructive interference, and\ndemonstrate the protocol for eigenstate preparation in this rhombic array with\ncoupler-assisted flux. Our results provide future possibilities for exploring\nthe interplay of geometry, interactions, and quantum information encoding in\nsuch degenerate systems.",
        "In this note we derive the exact probability that a specific state in a\ntwo-state Markov chain is visited exactly $k$ times after $N$ transitions. We\nprovide a closed-form solution for $\\mathbb{P}(N_l = k \\mid N)$, considering\ninitial state probabilities and transition dynamics. The solution corrects and\nextends prior incomplete results, offering a rigorous framework for enumerating\nstate transitions. Numerical simulations validate the derived expressions,\ndemonstrating their applicability in stochastic modeling.",
        "Let $\\mathcal{A}\\subset\\mathbb{R}_{\\geqslant1}$ be a countable set such that\n$\\limsup_{x\\to\\infty}\\frac{1}{\\log\nx}\\sum_{\\alpha\\in\\mathcal{A}\\cap[1,x]}\\frac{1}{\\alpha}>0$. We prove that, for\nevery $\\varepsilon>0$, there exist infinitely many pairs $(\\alpha, \\beta)\\in\n\\mathcal{A}^2$ such that $\\alpha\\neq \\beta$ and $|n\\alpha-\\beta| <\\varepsilon$\nfor some positive integer $n$. This resolves a problem of Erd\\H{o}s from 1948.\nA critical role in the proof is played by the machinery of GCD graphs, which\nwere introduced by the first author and by James Maynard in their work on the\nDuffin--Schaeffer conjecture in Diophantine approximation.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "Strategic interactions can be represented more concisely, and analyzed and\nsolved more efficiently, if we are aware of the symmetries within the\nmultiagent system. Symmetries also have conceptual implications, for example\nfor equilibrium selection. We study the computational complexity of identifying\nand using symmetries. Using the classical framework of normal-form games, we\nconsider game symmetries that can be across some or all players and\/or actions.\nWe find a strong connection between game symmetries and graph automorphisms,\nyielding graph automorphism and graph isomorphism completeness results for\ncharacterizing the symmetries present in a game. On the other hand, we also\nshow that the problem becomes polynomial-time solvable when we restrict the\nconsideration of actions in one of two ways.\n  Next, we investigate when exactly game symmetries can be successfully\nleveraged for Nash equilibrium computation. We show that finding a Nash\nequilibrium that respects a given set of symmetries is PPAD- and CLS-complete\nin general-sum and team games respectively -- that is, exactly as hard as\nBrouwer fixed point and gradient descent problems. Finally, we present\npolynomial-time methods for the special cases where we are aware of a vast\nnumber of symmetries, or where the game is two-player zero-sum and we do not\neven know the symmetries.",
        "Quantum operator scrambling describes the spreading of local operators into\nthe whole system in the picture of Heisenberg evolution, which is often\nquantified by the operator size growth. Here we propose a measure of quantum\noperator scrambling via Holevo information of operators, by taking its capacity\nto distinguish operator information locally. We show that the operator size is\nclosely related to a special kind of Holevo information of operators. Moreover,\nwe propose a feasible protocol for measuring Holevo information of operators on\ndigital quantum simulators based on random states. Our numerical simulations\nshow that the integrable system can be told apart from the chaotic system by\nmeasuring the spatial-temporal patterns of Holevo information. Furthermore, we\nfind that error mitigation is required to restore the time-oscillation behavior\nof Holevo information for the integrable system, a crucial feature distinct\nfrom the chaotic one. Our work provides a new perspective to understand the\ninformation scrambling and quantum chaos from aspects of Holevo information of\noperators.",
        "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https:\/\/github.com\/Sohanpatnaik106\/coalition.",
        "High-dimensional multiplex graphs are characterized by their high number of\ncomplementary and divergent dimensions. The existence of multiple hierarchical\nlatent relations between the graph dimensions poses significant challenges to\nembedding methods. In particular, the geometric distortions that might occur in\nthe representational space have been overlooked in the literature. This work\nstudies the problem of high-dimensional multiplex graph embedding from a\ngeometric perspective. We find that the node representations reside on highly\ncurved manifolds, thus rendering their exploitation more challenging for\ndownstream tasks. Moreover, our study reveals that increasing the number of\ngraph dimensions can cause further distortions to the highly curved manifolds.\nTo address this problem, we propose a novel multiplex graph embedding method\nthat harnesses hierarchical dimension embedding and Hyperbolic Graph Neural\nNetworks. The proposed approach hierarchically extracts hyperbolic node\nrepresentations that reside on Riemannian manifolds while gradually learning\nfewer and more expressive latent dimensions of the multiplex graph.\nExperimental results on real-world high-dimensional multiplex graphs show that\nthe synergy between hierarchical and hyperbolic embeddings incurs much fewer\ngeometric distortions and brings notable improvements over state-of-the-art\napproaches on downstream tasks.",
        "Employee turnover refers to an individual's termination of employment from\nthe current organization. It is one of the most persistent challenges for\nfirms, especially those ones in Information Technology (IT) industry that\nconfront high turnover rates. Effective prediction of potential employee\nturnovers benefits multiple stakeholders such as firms and online recruiters.\nPrior studies have focused on either the turnover prediction within a single\nfirm or the aggregated employee movement among firms. How to predict the\nindividual employees' turnovers among multiple firms has gained little\nattention in literature, and thus remains a great research challenge. In this\nstudy, we propose a novel deep learning approach based on job embeddedness\ntheory to predict the turnovers of individual employees across different firms.\nThrough extensive experimental evaluations using a real-world dataset, our\ndeveloped method demonstrates superior performance over several\nstate-of-the-art benchmark methods. Additionally, we estimate the cost saving\nfor recruiters by using our turnover prediction solution and interpret the\nattributions of various driving factors to employee's turnover to showcase its\npractical business value.",
        "Spiking neural networks (SNNs) show great potential due to their energy\nefficiency, fast processing capabilities, and robustness. There are two main\napproaches to constructing SNNs. Direct training methods require much memory,\nwhile conversion methods offer a simpler and more efficient option. However,\ncurrent conversion methods mainly focus on converting convolutional neural\nnetworks (CNNs) to SNNs. Converting Transformers to SNN is challenging because\nof the presence of non-linear modules. In this paper, we propose an Expectation\nCompensation Module to preserve the accuracy of the conversion. The core idea\nis to use information from the previous T time-steps to calculate the expected\noutput at time-step T. We also propose a Multi-Threshold Neuron and the\ncorresponding Parallel Parameter normalization to address the challenge of\nlarge time steps needed for high accuracy, aiming to reduce network latency and\npower consumption. Our experimental results demonstrate that our approach\nachieves state-of-the-art performance. For example, we achieve a top-1 accuracy\nof 88.60\\% with only a 1\\% loss in accuracy using 4 time steps while consuming\nonly 35\\% of the original power of the Transformer. To our knowledge, this is\nthe first successful Artificial Neural Network (ANN) to SNN conversion for\nSpiking Transformers that achieves high accuracy, low latency, and low power\nconsumption on complex datasets. The source codes of the proposed method are\navailable at https:\/\/github.com\/h-z-h-cell\/Transformer-to-SNN-ECMT.",
        "Artificial intelligence (AI) has significantly improved medical screening\naccuracy, particularly in cancer detection and risk assessment. However,\ntraditional classification metrics often fail to account for imbalanced data,\nvarying performance across cohorts, and patient-level inconsistencies, leading\nto biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT)\nframework to address these challenges. CAT introduces patient-level assessment,\nentropy-based distribution weighting, and cohort-weighted sensitivity and\nspecificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe),\nand CATMean ensure balanced and fair evaluation across diverse populations.\nThis approach enhances predictive reliability, fairness, and interpretability,\nproviding a robust evaluation method for AI-driven medical screening models.",
        "We investigate the motion in space of an infinitesimal particle in the\ngravitational field generated by three primary bodies positioned at the\nvertices of a fixed equilateral triangle. We assume that the distances between\nthe primaries are small compared to their separation from the particle. By\napplying a Lie-Deprit normalization, we simplify the Hamiltonian, relegating\nboth the mean anomaly and the argument of periapisis to third-order terms or\nhigher. After reducing out the symmetries associated with the Kepler flow and\nthe central action of the angular momentum, we examine the relative equilibria\nin the first and second reduced spaces. We are able to identify the conditions\nfor the existence of circular periodic orbits and KAM tori, thus providing\ninsight into the system's long-term stability and dynamic structure.",
        "Design educators are finding ways to support students in skillfully using\nGenAI tools in their practices while encouraging the critical scrutiny of the\nethical and social issues around these technologies. However, the issue of\nenvironmental sustainability remains unaddressed. There is a lack of both\nresources to grasp the environmental costs of genAI in education and a lack of\nshared practices for engaging with the issue. This paper critically reflects on\nthe energy costs of using genAI in design education, using a workshop held in\n2023 with 49 students as a motivating example. Through this reflection, we\ndevelop a set of five alternative stances, with related actions, that support\nthe conscious use of genAI in design education. The work contributes to the\nfield of design and HCI by bringing together ways for educators to reflect on\ntheir practices, informing the future development of educational programs\naround genAI."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"White-Box Transformers via Sparse Rate Reduction",
    "start_abstract":"In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "RNA-Seq: a revolutionary tool for transcriptomics"
      ],
      "abstract":[
        "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "CoverM: Read alignment statistics for metagenomics",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Causes of evolutionary divergence in prostate cancer",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Impact of transverse strain on linear, transitional and self-similar\n  turbulent mixing layers",
        "Dynamics of the general $Q$-tensor model interacting with a rigid body",
        "Magnetic mirror stars in five dimensions",
        "Quantum Electrodynamics from Quantum Cellular Automata, and the Tension\n  Between Symmetry, Locality and Positive Energy",
        "Oddities in the Entanglement Scaling of the Quantum Six-Vertex Model",
        "The Impact of Stellar Flares on the Atmospheric Escape of Exoplanets\n  orbiting M stars I: Insights from the AU Mic System",
        "Gradient-Based Optimization of Core-Shell Particles with Discrete\n  Materials for Directional Scattering",
        "Effects of GaAs Buffer Layer on Structural, Magnetic, and Transport\n  Properties of Magnetic Topological Insulators\n  Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ and\n  V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ Films",
        "Unlocking ultra-deep wide-field imaging with sidereal visibility\n  averaging",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Enhancing Efficiency of Local Projections Estimation with Volatility\n  Clustering in High-Frequency Data",
        "Nice q-analogs of orthogonal polynomials with nice moments: Some simple\n  examples",
        "Euclid Quick Data Release (Q1): First visual morphology catalogue",
        "Table-top three-dimensional photoemission orbital tomography with a\n  femtosecond extreme ultraviolet light source",
        "Predicting the spectrum and decay constants of positive-parity\n  heavy-strange mesons using domain-wall fermions"
      ],
      "abstract":[
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "The growth of interfacial instabilities such as the Rayleigh-Taylor (RTI) and\nRichtmyer-Meshkov instability (RMI) are modified when developing in convergent\ngeometries. Whilst these modifications are usually quantified by the\ncompression rate and convergence rate of the mixing layer, an alternative\nframework is proposed, describing the evolution of the mixing layer through the\neffects of the mean strain rates experienced by the mixing layer. An\ninvestigation into the effect of the transverse strain rate on the mixing layer\ndevelopment is conducted through application of transverse strain rates in\nplanar geometry. A model for the linear regime in planar geometry with\ntransverse strain rate is derived, with equivalent solutions to convergent\ngeometry, and validated with two-dimensional simulations demonstrating the\namplification of the instability growth under transverse compression. The\neffect of the transverse strain rate on the transitional-to-turbulent mixing\nlayer is investigated with implicit large eddy simulation based on the\nmulti-mode quarter-scale $\\theta$-group case by Thornber et al. (Phys. Fluids,\nvol. 29, 2017, 105107). The mixing layer's growth exhibits the opposite trend\nto the linear regime model, with reduced growth under transverse compression.\nThe effect of shear-production under transverse compression causes the mixing\nlayer to become more mixed and the turbulent kinetic energy is increasingly\ndominated by the transverse directions, deviating from the unstrained\nself-similar state. The mixing layer width is able to be predicted by adjusting\nthe buoyancy-drag model by Youngs & Thornber (Physica D, vol. 410, 2020,\n132517) to utilise a drag length scale that scales with the transverse\nexpansion.",
        "In this article, the fluid-rigid body interaction problem of nematic liquid\ncrystals described by the general Beris-Edwards $Q$-tensor model is studied. It\nis proved first that the total energy of this problem decreases in time. The\nassociated mathematical problem is a quasilinear mixed-order system with moving\nboundary. After the transformation to a fixed domain, a monolithic approach\nbased on the added mass operator and lifting arguments is employed to establish\nthe maximal $L^p$-regularity of the linearized problem in an anisotropic ground\nspace. This paves the way for the local strong well-posedness for large data\nand global strong well-posedness for small data of the interaction problem.",
        "We discuss a class of solutions of multidimensional gravity which are\nformally related to black-hole solutions but can observationally look like\ncompact stars whose surface reflects back all particles or signals getting\nthere. Some particular examples of such solutions are presented and studied,\nincluding those with a magnetic field in Maxwell or nonlinear electrodynamics\n(NED) in five dimensions. For NED as a possible source for magnetic mirror\nstars, we formulate a methodology of solving the 5D Einstein-NED equations and\npoint out the conditions under which there always exist mirror star solutions.\nWe also note that some of the Einstein-Maxwell solutions under consideration\nare discussed in the literature and called ``topological stars'' due to the\ncircular topology of the fifth dimension.",
        "We show that free QED is equivalent to the continuous-space-and-time limit of\nFermi and Bose lattice quantum cellular automata theories derived from quantum\nrandom walks satisfying simple symmetry and unitarity conditions. In doing so\nwe define the Fermi and Bose theories in a unified manner using the usual\nfermion internal space but a boson internal space that is six-dimensional. We\nshow that the reduction to a two-dimensional boson internal space (two helicity\nstates arising from spin-1 plus the photon transversality condition) comes from\nrestricting the quantum cellular automaton theory to positive energies. We\nbriefly examine common symmetries of quantum cellular automata, and how\ntime-reversal symmetry demands the existence of negative-energy solutions.\nThese solutions produce a tension in coupling the Fermi and Bose theories, in\nwhich the strong locality of quantum cellular automata seems to require a\nnonzero amplitude to produce negative-energy states, leading to an unphysical\ncascade of negative-energy particles. However, we show in a 1D model that by\nextending interactions over a larger (but finite) range it is possible to\nexponentially suppress the production of negative-energy particles to the point\nwhere they can be neglected.",
        "We investigate the entanglement properties of the Quantum Six-Vertex Model on\na cylinder, focusing on the Shannon-Renyi entropy in the limit of Renyi order\n$n = \\infty$. This entropy, calculated from the ground state amplitudes of the\nequivalent XXZ spin-1\/2 chain, allows us to determine the Renyi entanglement\nentropy of the corresponding Rokhsar-Kivelson wavefunctions, which describe the\nground states of certain conformal quantum critical points. Our analysis\nreveals a novel logarithmic correction to the expected entanglement scaling\nwhen the system size is odd. This anomaly arises from the geometric frustration\nof spin configurations imposed by periodic boundary conditions on odd-sized\nchains. We demonstrate that the scaling prefactor of this logarithmic term is\ndirectly related to the compactification radius of the low-energy bosonic field\ntheory description, or equivalently, the Luttinger parameter. Thus, this\ncorrection provides a direct probe of the underlying Conformal Field Theory\n(CFT) describing the critical point. Our findings highlight the crucial role of\nsystem size parity in determining the entanglement properties of this model and\noffer insights into the interplay between geometry, frustration, and\ncriticality.",
        "The X-rays and Extreme Ultraviolet (XUV) emission from M stars can drive the\natmospheric escape on planets orbiting them. M stars are also known for their\nfrequent emission of stellar flares, which will increase the high-energy flux\nreceived by their orbiting planets. To understand how stellar flares impact the\nprimordial atmospheres of planets orbiting young M stars, we use UV\nspectroscopic data of flares from the Habitable Zones and M dwarf Activity\nacross Time (HAZMAT) and Measurements of the Ultraviolet Spectral\nCharacteristics of Low-mass Exoplanetary Systems (MUSCLES) programs as a proxy\nto the XUV flare emission. Using the software package VPLanet, we simulate the\nyoung AU Mic planetary system composed of two Neptune-sized and one Earth-sized\nplanet orbiting a 23-Myr-old M1 star. Our findings show that the Earth-sized\nplanet AU Mic d should be in the process of losing completely its atmosphere in\nthe next couple million years, solely due to the quiescent emission, with\nflares not significantly contributing to its atmospheric escape due to the\nsmall size of AU mic d and its close-in distance from the star. However, our\nresults indicate that flares would play a crucial role for such planets further\naway, in the habitable zone (i.e. 0.2935 AU) of AU Mic-like stars during the\npost-saturation phase, accelerating the total atmospheric loss process by a few\nbillion years. For planets between 0.365 AU and the HZ outer edge, the\nadditional XUV from flares is necessary to deplete primordial atmospheres fully\nsince the quiescent emission alone is insufficient.",
        "Designing nanophotonic structures traditionally grapples with the\ncomplexities of discrete parameters, such as real materials, often resorting to\ncostly global optimization methods. This paper introduces an approach that\nleverages generative deep learning to map discrete parameter sets into a\ncontinuous latent space, enabling direct gradient-based optimization. For\nscenarios with non-differentiable physics evaluation functions, a neural\nnetwork is employed as a differentiable surrogate model. The efficacy of this\nmethodology is demonstrated by optimizing the directional scattering properties\nof core-shell nanoparticles composed of a selection of realistic materials. We\nderive suggestions for core-shell geometries with strong forward scattering and\nminimized backscattering. Our findings reveal significant improvements in\ncomputational efficiency and performance when compared to global optimization\ntechniques. Beyond nanophotonics design problems, this framework holds promise\nfor broad applications across all types of inverse problems constrained by\ndiscrete variables.",
        "Here, we study the effects of a GaAs buffer layer on the structural,\nmagnetic, and transport properties of Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$\nmagnetic topological insulator thin films and compare them with those of\nV$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, which we recently reported. Similar to\nthe case of V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, growth on a GaAs buffer\nlayer leads to some distinctly different properties than direct growth on InP\nsubstrates. These include improved interface quality confirmed by transmission\nelectron microscopy, enhanced magnetic coercive fields, and smaller resistivity\npeaks at the magnetization reversals. Furthermore, the Bi-ratio dependence of\nthe carrier density reveals that the interface property also affects the Fermi\nlevel. These results demonstrate the importance of the buffer layer in\ncontrolling the electronic properties of the magnetic topological insulator\nfilms.",
        "Producing ultra-deep high-angular-resolution images with current and\nnext-generation radio interferometers introduces significant computational\nchallenges. In particular, the imaging is so demanding that processing large\ndatasets, accumulated over hundreds of hours on the same pointing, is likely\ninfeasible in the current data reduction schemes. In this paper, we revisit a\nsolution to this problem that was considered in the past but is not being used\nin modern software: sidereal visibility averaging (SVA). This technique\ncombines individual observations taken at different sidereal days into one much\nsmaller dataset by averaging visibilities at similar baseline coordinates. We\npresent our method and validated it using four separate 8-hour observations of\nthe ELAIS-N1 deep field, taken with the International LOw Frequency ARray\n(LOFAR) Telescope (ILT) at 140~MHz. Additionally, we assessed the accuracy\nconstraints imposed by Earth's orbital motion relative to the observed pointing\nwhen combining multiple datasets. We find, with four observations, data volume\nreductions of a factor of 1.8 and computational time improvements of a factor\nof 1.6 compared to standard imaging. These factors will increase when more\nobservations are combined with SVA. For instance, with 3000~hours of LOFAR data\naimed at achieving sensitivities of the order of {\\mu}Jy\/beam at sub-arcsecond\nresolutions, we estimate data volume reductions of up to a factor of 169 and a\n14-fold decrease in computing time using our current algorithm. This\nadvancement for imaging large deep interferometric datasets will benefit\ncurrent generation instruments, such as LOFAR, and upcoming instruments such as\nthe Square Kilometre Array (SKA), provided the calibrated visibility data of\nthe individual observations are retained.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This paper advances the local projections (LP) method by addressing its\ninefficiency in high-frequency economic and financial data with volatility\nclustering. We incorporate a generalized autoregressive conditional\nheteroskedasticity (GARCH) process to resolve serial correlation issues and\nextend the model with GARCH-X and GARCH-HAR structures. Monte Carlo simulations\nshow that exploiting serial dependence in LP error structures improves\nefficiency across forecast horizons, remains robust to persistent volatility,\nand yields greater gains as sample size increases. Our findings contribute to\nrefining LP estimation, enhancing its applicability in analyzing economic\ninterventions and financial market dynamics.",
        "In this note I collect some typical examples of orthogonal polynomials with\nsimple moments where both moments and orthogonal polynomials have nice\nq-analogs.",
        "We present a detailed visual morphology catalogue for Euclid's Quick Release\n1 (Q1). Our catalogue includes galaxy features such as bars, spiral arms, and\nongoing mergers, for the 378000 bright ($I_E < 20.5$) or extended (area $\\geq\n700\\,$pixels) galaxies in Q1. The catalogue was created by finetuning the\nZoobot galaxy foundation models on annotations from an intensive one month\ncampaign by Galaxy Zoo volunteers. Our measurements are fully automated and\nhence fully scaleable. This catalogue is the first 0.4% of the approximately\n100 million galaxies where Euclid will ultimately resolve detailed morphology.",
        "Following electronic processes in molecules and materials at the level of the\nquantum mechanical electron wavefunction with angstrom-level spatial resolution\nand with full access to its femtosecond temporal dynamics is at the heart of\nultrafast condensed matter physics. A breakthrough invention allowing\nexperimental access to electron wavefunctions was the reconstruction of\nmolecular orbitals from angle-resolved photoelectron spectroscopy data in 2009,\ntermed photoemission orbital tomography (POT). This invention puts ultrafast\nthree-dimensional (3D) POT in reach, with many new prospects for the study of\nultrafast light-matter interaction, femtochemistry and photo-induced phase\ntransitions. Here, we develop a synergistic experimental-algorithmic approach\nto realize the first 3D-POT experiment using a short-pulse extreme ultraviolet\nlight source. We combine a new variant of photoelectron spectroscopy, namely\nultrafast momentum microscopy, with a table-top spectrally-tunable\nhigh-harmonic generation light source and a tailored algorithm for efficient 3D\nreconstruction from sparse, undersampled data. This combination dramatically\nspeeds up the experimental data acquisition, while at the same time reducing\nthe sampling requirements to achieve complete 3D information. We demonstrate\nthe power of this approach by full 3D imaging of the frontier orbitals of a\nprototypical organic semiconductor absorbed on pristine Ag(110).",
        "We present a lattice-QCD calculation of the masses and decay constants of the\npositive-parity heavy-strange mesons $D^*_{s0}$, $D_{s1}$, $B^*_{s0}$, and\n$B_{s1}$. The calculations are performed with domain-wall fermions for the\nlight and strange quarks and an anisotropic clover action for the charm and\nbottom quarks. We use seven different RBC\/UKQCD ensembles with pion masses\nranging from a near-physical 139 MeV up to 431 MeV. We consider two different\nanalysis types, with or without two-meson operators at the source. We observe\nthe expected below-threshold ground states. The fits without the two-meson\noperators appear to be more stable, but may overestimate the ground-state\nenergies, while preliminary fits with two-meson operators at the source only\nappear to underestimate the ground-state energies."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models",
    "start_abstract":"The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "How resilient are language models to text perturbations"
      ],
      "abstract":[
        "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Explainable Distributed Constraint Optimization Problems",
        "CSSDM Ontology to Enable Continuity of Care Data Interoperability",
        "Generative AI in Transportation Planning: A Survey",
        "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
        "Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat\n  Elite AI in TextStarCraft II for the First Time",
        "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
        "The Goofus & Gallant Story Corpus for Practical Value Alignment",
        "Online inductive learning from answer sets for efficient reinforcement\n  learning exploration",
        "Demonstrating specification gaming in reasoning models",
        "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
        "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty",
        "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
        "REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent\n  Systems",
        "Apparent violations of the second law in the quantum-classical dynamics\n  of interacting levitated nanoparticles",
        "Does dark matter fall in the same way as standard model particles? A\n  direct constraint of Euler's equation with cosmological data",
        "A countable Boolean algebra that is Reichenbach's common cause complete",
        "Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading\n  Agents",
        "Effects of galactic environment on size and dark matter content in\n  low-mass galaxies",
        "Lattice-Based Pruning in Recurrent Neural Networks via Poset Modeling",
        "Lipschitz Decompositions of Finite $\\ell_{p}$ Metrics",
        "DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with\n  Contrastive Training Strategy for Deepfake Speech Detection",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "An Investigation of FP8 Across Accelerators for LLM Inference",
        "ASTRAL: Automated Safety Testing of Large Language Models",
        "Scalable and Site-Specific Frequency Tuning of Two-Level System Defects\n  in Superconducting Qubit Arrays",
        "Fabrication of Soft and Comfortable Pressure-Sensing Shoe Sole for\n  Intuitive Monitoring of Human Quality Gaits",
        "Cognitive Performance Measurements and the Impact of Sleep Quality Using\n  Wearable and Mobile Sensors",
        "High-throughput Discovery of Anti-gap Semiconductors"
      ],
      "abstract":[
        "The Distributed Constraint Optimization Problem (DCOP) formulation is a\npowerful tool to model cooperative multi-agent problems that need to be solved\ndistributively. A core assumption of existing approaches is that DCOP solutions\ncan be easily understood, accepted, and adopted, which may not hold, as\nevidenced by the large body of literature on Explainable AI. In this paper, we\npropose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include\nits solution and a contrastive query for that solution. We formally define some\nkey properties that contrastive explanations must satisfy for them to be\nconsidered as valid solutions to X-DCOPs as well as theoretical results on the\nexistence of such valid explanations. To solve X-DCOPs, we propose a\ndistributed framework as well as several optimizations and suboptimal variants\nto find valid explanations. We also include a human user study that showed that\nusers, not surprisingly, prefer shorter explanations over longer ones. Our\nempirical evaluations showed that our approach can scale to large problems, and\nthe different variants provide different options for trading off explanation\nlengths for smaller runtimes. Thus, our model and algorithmic contributions\nextend the state of the art by reducing the barrier for users to understand\nDCOP solutions, facilitating their adoption in more real-world applications.",
        "The rapid advancement of digital technologies and recent global pandemic\nscenarios have led to a growing focus on how these technologies can enhance\nhealthcare service delivery and workflow to address crises. Action plans that\nconsolidate existing digital transformation programs are being reviewed to\nestablish core infrastructure and foundations for sustainable healthcare\nsolutions. Reforming health and social care to personalize home care, for\nexample, can help avoid treatment in overcrowded acute hospital settings and\nimprove the experiences and outcomes for both healthcare professionals and\nservice users. In this information-intensive domain, addressing the\ninteroperability challenge through standards-based roadmaps is crucial for\nenabling effective connections between health and social care services. This\napproach facilitates safe and trustworthy data workflows between different\nhealthcare system providers. In this paper, we present a methodology for\nextracting, transforming, and loading data through a semi-automated process\nusing a Common Semantic Standardized Data Model (CSSDM) to create personalized\nhealthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology\nof ISO 13940 ContSys and incorporates FHIR-based specifications to support\nstructural attributes for generating KGs. We propose that the CSSDM facilitates\ndata harmonization and linking, offering an alternative approach to\ninteroperability. This approach promotes a novel form of collaboration between\ncompanies developing health information systems and cloud-enabled health\nservices. Consequently, it provides multiple stakeholders with access to\nhigh-quality data and information sharing.",
        "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.",
        "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
        "Since the emergence of the Large Language Model (LLM), LLM has been widely\nused in fields such as writing, translating, and searching. However, there is\nstill great potential for LLM-based methods in handling complex tasks such as\ndecision-making in the StarCraft II environment. To address problems such as\nlack of relevant knowledge and poor control over subtasks of varying\nimportance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method\nimproves the understanding of game situations through expert-level tactical\nknowledge, improving the processing quality of tasks of varying importance\nthrough a hierarchical framework. Our approach defeated the highest level\n(Elite) standard built-in agent in TextStarCraft II for the first time and\nconsistently outperformed the baseline method in other difficulties. Our\nexperiments suggest that the proposed method is a practical solution for\ntackling complex decision-making challenges. The replay video can be viewed on\nhttps:\/\/www.bilibili.com\/video\/BV1uz42187EF and https:\/\/youtu.be\/dO3PshWLV5M,\nand our codes have been open-sourced on\nhttps:\/\/github.com\/luchang1113\/HEP-LLM-play-StarCraftII.",
        "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
        "Values or principles are key elements of human society that influence people\nto behave and function according to an accepted standard set of social rules to\nmaintain social order. As AI systems are becoming ubiquitous in human society,\nit is a major concern that they could violate these norms or values and\npotentially cause harm. Thus, to prevent intentional or unintentional harm, AI\nsystems are expected to take actions that align with these principles. Training\nsystems to exhibit this type of behavior is difficult and often requires a\nspecialized dataset. This work presents a multi-modal dataset illustrating\nnormative and non-normative behavior in real-life situations described through\nnatural language and artistic images. This training set contains curated sets\nof images that are designed to teach young children about social principles. We\nargue that this is an ideal dataset to use for training socially normative\nagents given this fact.",
        "This paper presents a novel approach combining inductive logic programming\nwith reinforcement learning to improve training performance and explainability.\nWe exploit inductive learning of answer set programs from noisy examples to\nlearn a set of logical rules representing an explainable approximation of the\nagent policy at each batch of experience. We then perform answer set reasoning\non the learned rules to guide the exploration of the learning agent at the next\nbatch, without requiring inefficient reward shaping and preserving optimality\nwith soft bias. The entire procedure is conducted during the online execution\nof the reinforcement learning algorithm. We preliminarily validate the efficacy\nof our approach by integrating it into the Q-learning algorithm for the Pac-Man\nscenario in two maps of increasing complexity. Our methodology produces a\nsignificant boost in the discounted return achieved by the agent, even in the\nfirst batches of training. Moreover, inductive learning does not compromise the\ncomputational time required by Q-learning and learned rules quickly converge to\nan explanation of the agent policy.",
        "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like o1 preview and\nDeepSeek-R1 will often hack the benchmark by default, while language models\nlike GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work\nto hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.",
        "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.",
        "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
        "Handcrafting heuristics for solving complex optimization tasks (e.g., route\nplanning and task allocation) is a common practice but requires extensive\ndomain knowledge. Recently, Large Language Model (LLM)-based automatic\nheuristic design (AHD) methods have shown promise in generating high-quality\nheuristics without manual interventions. Existing LLM-based AHD methods employ\na population to maintain a fixed number of top-performing LLM-generated\nheuristics and introduce evolutionary computation (EC) to iteratively enhance\nthe population. However, these population-based procedures cannot fully develop\nthe potential of each heuristic and are prone to converge into local optima. To\nmore comprehensively explore the space of heuristics, this paper proposes to\nuse Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The\nproposed MCTS-AHD method organizes all LLM-generated heuristics in a tree\nstructure and can better develop the potential of temporarily underperforming\nheuristics. In experiments, MCTS-AHD delivers significantly higher-quality\nheuristics on various complex tasks. Our code is available.",
        "This benchmark suite provides a comprehensive evaluation framework for\nassessing both individual LLMs and multi-agent systems in real-world planning\nscenarios. The suite encompasses eleven designed problems that progress from\nbasic to highly complex, incorporating key aspects such as multi-agent\ncoordination, inter-agent dependencies, and dynamic environmental disruptions.\nEach problem can be scaled along three dimensions: the number of parallel\nplanning threads, the complexity of inter-dependencies, and the frequency of\nunexpected disruptions requiring real-time adaptation. The benchmark includes\ndetailed specifications, evaluation metrics, and baseline implementations using\ncontemporary frameworks like LangGraph, enabling rigorous testing of both\nsingle-agent and multi-agent planning capabilities. Through standardized\nevaluation criteria and scalable complexity, this benchmark aims to drive\nprogress in developing more robust and adaptable AI planning systems for\nreal-world applications.",
        "Random exchanges of energy arise naturally in stochastic systems. As a\nconsequence, apparent violations of the second law of thermodynamics can occur,\nas it holds true on average. Here we investigate the occurrence of these\napparent violations -- termed free lunches -- in a quantum-classical system\ncomprised of levitated nanoparticles exchanging energy via the Coulomb\ninteraction. We consider different initial states for the quantum system, and\nhow these exert work and fluctuations upon the classical particle affecting the\nprobability of free lunches. With that, we initiate the study of hybrid\nquantum-classical systems through the lens of stochastic thermodynamics.",
        "Since dark matter particles have never been directly detected, we do not know\nhow they move, and in particular we do not know how they fall inside\ngravitational potential wells. Usually it is assumed that dark matter only\ninteracts gravitationally with itself and with particles of the standard model,\nand therefore that its motion is governed by Euler's equation. In this paper,\nwe test this assumption for the first time at cosmological scales, by combining\nmeasurements of galaxy velocities with measurements of gravitational potential\nwells, encoded in the Weyl potential. We find that current data are consistent\nwith Euler's equation at redshifts $z\\in [0.3,0.8]$, and we place constraints\non the strength of a potential fifth force, which would alter the way dark\nmatter particles fall. We find that a positive fifth force cannot exceed 7% of\nthe gravitational interaction strength, while a negative fifth force is limited\nto 21%. The coming generation of surveys, including the Legacy Survey of Space\nand Time (LSST) of the Vera C. Rubin Observatory and the Dark Energy\nSpectroscopic Instrument (DESI) will drastically improve the constraints,\nallowing to constrain a departure from pure gravitational interaction at the\nlevel of 2%.",
        "The common cause completeness (CCC) is a philosophical principle that asserts\nthat if we consider two positively correlated events then it evokes a common\ncause. The principle is due to H. Reichenbach and has been largely studied in\nBoolean algebras and elsewhere.The results published so far bring about a\nquestion whether there is a small (countable) Boolean algebra with CCC. In this\nnote we construct such an example.",
        "Companies across all economic sectors continue to deploy large language\nmodels at a rapid pace. Reinforcement learning is experiencing a resurgence of\ninterest due to its association with the fine-tuning of language models from\nhuman feedback. Tool-chain language models control task-specific agents; if the\nconverse has not already appeared, it soon will. In this paper, we present what\nwe believe is the first investigation of an intelligent trading agent based on\ncontinuous deep reinforcement learning that also controls a large language\nmodel with which it can post to a social media feed observed by other traders.\nWe empirically investigate the performance and impact of such an agent in a\nsimulated financial market, finding that it learns to optimize its total\nreward, and thereby augment its profit, by manipulating the sentiment of the\nposts it produces. The paper concludes with discussion, limitations, and\nsuggestions for future work.",
        "We utilize the cosmological volume simulation, FIREbox, to investigate how a\ngalaxy's environment influences its size and dark matter content. Our study\nfocuses on approximately 1,200 galaxies (886 central and 332 satellite halos)\nin the low-mass regime, with stellar masses between $10^6$ to $10^9$\n$M_{\\odot}$. We analyze the size-mass relation ($r_{50} - M_{\\star}$), inner\ndark matter mass-stellar mass ($M^{50}_{\\rm DM} - M_{\\star}$) relation, and the\nhalo mass-stellar mass ($M_{\\rm halo} - M_{\\star}$) relation. At fixed stellar\nmass, we find the galaxies experiencing stronger tidal influences, indicated by\nhigher Perturbation Indices (PI $>$ 1) are generally larger and have lower\nmasses relative to their counterparts with lower Perturbation Indices (PI $<$\n1). Applying a Random Forest regression model, we show that both the\nenvironment (PI) and halo mass ($M_{rm halo}$) are significant predictors of a\ngalaxy's relative size and dark matter content. Notably, because $M_{\\rm halo}$\nis also strongly affected by the environment, our findings indicate that\nenvironmental conditions not only influence galactic sizes and relative inner\ndark matter content directly, but also indirectly through their impact on halo\nmass. Our results highlight a critical interplay between environmental factors\nand halo mass in shaping galaxy properties, affirming the environment as a\nfundamental driver in galaxy formation and evolution.",
        "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
        "Lipschitz decomposition is a useful tool in the design of efficient\nalgorithms involving metric spaces. While many bounds are known for different\nfamilies of finite metrics, the optimal parameters for $n$-point subsets of\n$\\ell_p$, for $p > 2$, remained open, see e.g. [Naor, SODA 2017]. We make\nsignificant progress on this question and establish the bound\n$\\beta=O(\\log^{1-1\/p} n)$. Building on prior work, we demonstrate applications\nof this result to two problems, high-dimensional geometric spanners and\ndistance labeling schemes. In addition, we sharpen a related decomposition\nbound for $1<p<2$, due to Filtser and Neiman [Algorithmica 2022].",
        "In this paper, we propose a deep neural network approach for deepfake speech\ndetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)\ntrained with a contrastive training strategy (CTS). In this framework, input\naudio recordings are first transformed into spectrograms using Short-Time\nFourier Transform (STFT) and Linear Filter (LF), which are then used to train\nthe DIN. Once trained, the DIN processes bonafide utterances to extract audio\nembeddings, which are used to construct a Gaussian distribution representing\ngenuine speech. Deepfake detection is then performed by computing the distance\nbetween a test utterance and this distribution to determine whether the\nutterance is fake or bonafide. To evaluate our proposed systems, we conducted\nextensive experiments on the benchmark dataset of ASVspoof 2019 LA. The\nexperimental results demonstrate the effectiveness of combining the\nDepthwise-Inception Network with the contrastive learning strategy in\ndistinguishing between fake and bonafide utterances. We achieved Equal Error\nRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%\nrespectively using a single, low-complexity DIN with just 1.77 M parameters and\n985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposed\nsystem outperforms the single-system submissions in the ASVspoof 2019 LA\nchallenge, showcasing its potential for real-time applications.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "The introduction of 8-bit floating-point (FP8) computation units in modern AI\naccelerators has generated significant interest in FP8-based large language\nmodel (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep\nlearning requires a shared scaling factor. Additionally, while E4M3 and E5M2\nare well-defined at the individual value level, their scaling and accumulation\nmethods remain unspecified and vary across hardware and software\nimplementations. As a result, FP8 behaves more like a quantization format than\na standard numeric representation. In this work, we provide the first\ncomprehensive analysis of FP8 computation and acceleration on two AI\naccelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that\nthe Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency\nduring LLM inference, offering valuable insights into the practical\nimplications of FP8 adoption for datacenter-scale LLM serving.",
        "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.",
        "State-of-the-art superconducting quantum processors containing tens to\nhundreds of qubits have demonstrated the building blocks for realizing\nfault-tolerant quantum computation. Nonetheless, a fundamental barrier to\nscaling further is the prevalence of fluctuating quantum two-level system (TLS)\ndefects that can couple resonantly to qubits, causing excess decoherence and\nenhanced gate errors. Here we introduce a scalable architecture for\nsite-specific and in-situ manipulation of TLS frequencies out of the spectral\nvicinity of our qubits. Our method is resource efficient, combining TLS\nfrequency tuning and universal single qubit control into a single on-chip\ncontrol line per qubit. We independently control each qubit's dissipative\nenvironment to dynamically improve both qubit coherence times and single qubit\ngate fidelities -- with a constant time overhead that does not scale with the\ndevice size. Over a period of 40 hours across 6 qubits, we demonstrate a $36\\%$\nimprovement in average single qubit error rates and a $17\\%$ improvement in\naverage energy relaxation times. Critically, we realize a 4-fold suppression in\nthe occurrence of TLS-induced performance outliers, and a complete reduction of\nsimultaneous outlier events. These results mark a significant step toward\novercoming the challenges that TLS defects pose to scaling superconducting\nquantum processors.",
        "The study discusses the design and fabrication of flexible pressure sensors\nusing Ecoflex\/Graphene composites. The fabricated sensor is used for the\napplication of intuitive monitoring of human quality gaits and implementation\nof the soft and comfortable shoe sole for rehabilitation of the patients with\nfoot disorder is also taken into consideration. The sensor is fabricated using\nmolding and casting technique by sandwiching the thin film Ecoflex\/Graphene\ncomposites between the copper (Cu) electrodes with the dimension of 15 x 15 mm2\nwith high sensitivity. There are five pressure sensors integrated in the shoe\nsole, a sensor at the forefoot, three sensors at the midfoot and one sensor at\nthe lower foot (heel). The behavior of the sensor is negative piezoresistive in\nwhich the resistance decreases as the pressure increases. The sensors are\nembedded in a soft and comfortable shoe sole and then integrated with a laptop\nor mobile application to monitor and analyze human gait in real-time.\nFurthermore, a dedicated Graphical User Interface (GUI) is designed to read the\ndata. The pressure sensors are integrated with ESP32 microcontroller which\nwirelessly transmit data to the GUI and smart phones which could be further\nused in the intuitive monitoring, rehabilitation of the patients with foot\ndisorder or neuromotor diseases.",
        "Human cognitive performance is an underlying factor in most of our daily\nlives, and numerous factors influence cognitive performance. In this work, we\ninvestigate how changes in sleep quality influence cognitive performance,\nmeasured from a dataset collected during a 2-month field study. We collected\ncognitive performance data (alertness) with the Psychomotor Vigilance Task\n(PVT), mobile keyboard typing metrics from participants' smartphones, and sleep\nquality metrics through a wearable sleep tracking ring. Our findings highlight\nthat specific sleep metrics like night-time heart rate, sleep latency, sleep\ntiming, sleep restfulness, and overall sleep quantity significantly influence\ncognitive performance. To strengthen the current research on cognitive\nmeasurements, we introduce smartphone typing metrics as a proxy or a\ncomplementary method for continuous passive measurement of cognitive\nperformance. Together, our findings contribute to ubiquitous computing via a\nlongitudinal case study with a novel wearable device, the resulting findings on\nthe association between sleep and cognitive function, and the introduction of\nsmartphone keyboard typing as a proxy of cognitive function.",
        "Conventional semiconductors typically have bonding states near the valence\nband maximum (VBM) and antibonding states near the conduction band minimum\n(CBM). Semiconductors with the opposite electronic configuration, namely an\nantibonding VBM and a bonding CBM, are here termed ``anti-gap semiconductors\".\nThey have been theoretically proposed to exhibit excellent optoelectronic\nproperties because of their strong tolerance to defects. However, no anti-gap\nsemiconductors have been identified so far, despite a known list of\nsemiconductors with an antibonding VBM. Here, we use high-throughput\ncomputation to identify over 100 anti-gap semiconductors. From this group, we\nanalyze the transition metal dichalcogenide MX$_2$ (M=Hf, Zr; X=S, Se) family\nin detail. In addition to verifying their defect tolerance for both electrons\nand holes using first-principles simulations, we also discovered that\nphotoexcitation of charge carriers can lead to significant lattice stiffening\nand increased thermal conductivity in anti-gap semiconductors, which can be\npotentially used as photo-driven thermal switches. Our work analyzes the\nformation of the anti-gap electronic structure and showcases their unusual\nphotoinduced lattice dynamics that can have a potential impact on their\nphotophysical applications."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"How resilient are language models to text perturbations",
    "start_abstract":"Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
      ],
      "abstract":[
        "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "Tightening Causal Bounds via Covariate-Aware Optimal Transport",
        "Clustered Flexible Calibration Plots For Binary Outcomes Using Random\n  Effects Modeling",
        "Identifying rapid changes in the hemodynamic response in event-related\n  functional magnetic resonance imaging",
        "The Building Blocks of Classical Nonparametric Two-Sample Testing\n  Procedures: Statistically Equivalent Blocks",
        "Wavelet-based estimation of long-memory parameter in stochastic\n  volatility models using a robust log-periodogram",
        "Spectral Clustering on Multilayer Networks with Covariates",
        "Inverse sampling intensity weighting for preferential sampling\n  adjustment",
        "Sparsity learning via structured functional factor augmentation",
        "Two-Round Distributed Principal Component Analysis: Closing the\n  Statistical Efficiency Gap",
        "A time-to-event three-outcome design for randomized phase II cancer\n  trials",
        "A Generalized Fr\\'echet Test for Object Data with Unequal Repeated\n  Measurements",
        "Time-Varying Causal Survival Learning",
        "Denoising Diffused Embeddings: a Generative Approach for Hypergraphs",
        "Qmod: Expressive High-Level Quantum Modeling",
        "On almost Gallai colourings in complete graphs",
        "Assessment of spectral phases of non-Hermitian quantum systems through\n  complex and singular values",
        "Bifurcations and stability of synchronized solutions in the Kuramoto\n  model with uniformly spaced natural frequencies",
        "Construction of exact refinements for the two-dimensional HB\/THB-spline\n  de Rham complex",
        "AI-powered virtual tissues from spatial proteomics for clinical\n  diagnostics and biomedical discovery",
        "Exact calculation of spectral properties of a particle interacting with\n  a one-dimensional Fermi gas in optical lattices",
        "Independent transversal blow-up of graphs",
        "WISDOM Project -- XXII. A 5% precision CO-dynamical supermassive black\n  hole mass measurement in the galaxy NGC 383",
        "Effects of Ru-doping on the magnetism of Ag3LiIr2O6, a candidate Kitaev\n  quantum spin liquid",
        "A Bayesian Record Linkage Approach to Applications in Tree Demography\n  Using Overlapping LiDAR Scans",
        "Multipoint stress mixed finite element methods for elasticity on cuboid\n  grids",
        "$q$-deformation of random partitions, determinantal structure, and\n  Riemann-Hilbert problem",
        "Efficient parameter inference in networked dynamical systems via steady\n  states: A surrogate objective function approach integrating mean-field and\n  nonlinear least squares",
        "A note on Arveson's hyperrigidity and non-degenerate C*-correspondences"
      ],
      "abstract":[
        "Causal estimands can vary significantly depending on the relationship between\noutcomes in treatment and control groups, potentially leading to wide partial\nidentification (PI) intervals that impede decision making. Incorporating\ncovariates can substantially tighten these bounds, but requires determining the\nrange of PI over probability models consistent with the joint distributions of\nobserved covariates and outcomes in treatment and control groups. This problem\nis known to be equivalent to a conditional optimal transport (COT) optimization\ntask, which is more challenging than standard optimal transport (OT) due to the\nadditional conditioning constraints. In this work, we study a tight relaxation\nof COT that effectively reduces it to standard OT, leveraging its\nwell-established computational and theoretical foundations. Our relaxation\nincorporates covariate information and ensures narrower PI intervals for any\nvalue of the penalty parameter, while becoming asymptotically exact as a\npenalty increases to infinity. This approach preserves the benefits of\ncovariate adjustment in PI and results in a data-driven estimator for the PI\nset that is easy to implement using existing OT packages. We analyze the\nconvergence rate of our estimator and demonstrate the effectiveness of our\napproach through extensive simulations, highlighting its practical use and\nsuperior performance compared to existing methods.",
        "Evaluation of clinical prediction models across multiple clusters, whether\ncenters or datasets, is becoming increasingly common. A comprehensive\nevaluation includes an assessment of the agreement between the estimated risks\nand the observed outcomes, also known as calibration. Calibration is of utmost\nimportance for clinical decision making with prediction models and it may vary\nbetween clusters. We present three approaches to take clustering into account\nwhen evaluating calibration. (1) Clustered group calibration (CG-C), (2) two\nstage meta-analysis calibration (2MA-C) and (3) mixed model calibration (MIX-C)\ncan obtain flexible calibration plots with random effects modelling and\nproviding confidence and prediction intervals. As a case example, we externally\nvalidate a model to estimate the risk that an ovarian tumor is malignant in\nmultiple centers (N = 2489). We also conduct a simulation study and synthetic\ndata study generated from a true clustered dataset to evaluate the methods. In\nthe simulation and the synthetic data analysis MIX-C gave estimated curves\nclosest to the true overall and center specific curves. Prediction interval was\nbest for 2MA-C with splines. Standard flexible calibration worked likewise in\nterms of calibration error when sample size is limited. We recommend using\n2MA-C (splines) to estimate the curve with the average effect and the 95% PI\nand MIX-C for the cluster specific curves, specially when sample size per\ncluster is limited. We provide ready-to-use code to construct summary flexible\ncalibration curves with confidence and prediction intervals to assess\nheterogeneity in calibration across datasets or centers.",
        "The hemodynamic response (HR) in event-related functional magnetic resonance\nimaging is typically assumed to be stationary. While there are some approaches\nin the literature to model nonstationary HRs, few focus on rapid changes. In\nthis work, we propose two procedures to investigate rapid changes in the HR.\nBoth procedures make inference on the existence of rapid changes for\nmulti-subject data. We allow the change point locations to vary between\nsubjects, conditions and brain regions. The first procedure utilizes available\ninformation about the change point locations to compare multiple shape\nparameters of the HR over time. In the second procedure, the change point\nlocations are determined for each subject separately. To account for the\nestimation of the change point locations, we propose the notion of post\nselection variance. The power of the proposed procedures is assessed in\nsimulation studies. We apply the procedure for pre-specified change point\nlocations to data from a category learning experiment.",
        "Statistically equivalent blocks are not frequently considered in the context\nof nonparametric two-sample hypothesis testing. Despite the limited exposure,\nthis paper shows that a number of classical nonparametric hypothesis tests can\nbe derived on the basis of statistically equivalent blocks and their\nfrequencies. Far from a moot historical point, this allows for a more unified\napproach in considering the many two-sample nonparametric tests based on ranks,\nsigns, placements, order statistics, and runs. Perhaps more importantly, this\napproach also allows for the easy extension of many univariate nonparametric\ntests into arbitrarily high dimensions that retain all null properties\nregardless of dimensionality and are invariant to the scaling of the\nobservations. These generalizations do not require depth functions or the\nexplicit use of spatial signs or ranks and may be of use in various areas such\nas life-testing and quality control. In the manuscript, an overview of\nstatistically equivalent blocks and tests based on these blocks are provided.\nThis is followed by reformulations of some popular univariate tests and\ngeneralizations to higher dimensions. Comments comparing proposed methods to\nthose based on spatial signs and ranks are offered along with some conclusions.",
        "In this paper, we propose a novel method for estimating the long-memory\nparameter in time series. By combining the multi-resolution framework of\nwavelets with the robustness of the Least Absolute Deviations (LAD) criterion,\nwe introduce a periodogram providing a robust alternative to classical methods\nin the presence of non-Gaussian noise. Incorporating this periodogram into a\nlog-periodogram regression, we develop a new estimator. Simulation studies\ndemonstrate that our estimator outperforms the Geweke and Porter-Hudak (GPH)\nand Wavelet-Based Log-Periodogram (WBLP) estimators, particularly in terms of\nmean squared error, across various sample sizes and parameter configurations.",
        "The community detection problem on multilayer networks have drawn much\ninterest. When the nodal covariates ar also present, few work has been done to\nintegrate information from both sources. To leverage the multilayer networks\nand the covariates, we propose two new algorithms: the spectral clustering on\naggregated networks with covariates (SCANC), and the spectral clustering on\naggregated Laplacian with covariates (SCALC). These two algorithms are easy to\nimplement, computationally fast, and feature a data-driven approach for tuning\nparameter selection.\n  We establish theoretical guarantees for both methods under the Multilayer\nStochastic Blockmodel with Covariates (MSBM-C), demonstrating their consistency\nin recovering community structure. Our analysis reveals that increasing the\nnumber of layers, incorporating covariate information, and enhancing network\ndensity all contribute to improved clustering accuracy. Notably, SCANC is most\neffective when all layers exhibit similar assortativity, whereas SCALC performs\nbetter when both assortative and disassortative layers are present. On the\nsimulation studies and a primary school contact data analysis, our method\noutperforms other methods. Our results highlight the advantages of\nspectral-based aggregation techniques in leveraging both network structure and\nnodal attributes for robust community detection.",
        "Traditional geostatistical methods assume independence between observation\nlocations and the spatial process of interest. Violations of this independence\nassumption are referred to as preferential sampling (PS). Standard methods to\naddress PS rely on estimating complex shared latent variable models and can be\ndifficult to apply in practice. We study the use of inverse sampling intensity\nweighting (ISIW) for PS adjustment in model-based geostatistics. ISIW is a\ntwo-stage approach wherein we estimate the sampling intensity of the\nobservation locations then define intensity-based weights within a weighted\nlikelihood adjustment. Prediction follows by substituting the adjusted\nparameter estimates within a kriging framework. A primary contribution was to\nimplement ISIW by means of the Vecchia approximation, which provides large\ncomputational gains and improvements in predictive accuracy. Interestingly, we\nfound that accurate parameter estimation had little correlation with predictive\nperformance, raising questions about the conditions and parameter choices\ndriving optimal implementation of kriging-based predictors under PS. Our work\nhighlights the potential of ISIW to adjust for PS in an intuitive, fast, and\neffective manner.",
        "As one of the most powerful tools for examining the association between\nfunctional covariates and a response, the functional regression model has been\nwidely adopted in various interdisciplinary studies. Usually, a limited number\nof functional covariates are assumed in a functional linear regression model.\nNevertheless, correlations may exist between functional covariates in\nhigh-dimensional functional linear regression models, which brings significant\nstatistical challenges to statistical inference and functional variable\nselection. In this article, a novel functional factor augmentation structure\n(fFAS) is proposed for multivariate functional series, and a multivariate\nfunctional factor augmentation selection model (fFASM) is further proposed to\ndeal with issues arising from variable selection of correlated functional\ncovariates. Theoretical justifications for the proposed fFAS are provided, and\nstatistical inference results of the proposed fFASM are established. Numerical\ninvestigations support the superb performance of the novel fFASM model in terms\nof estimation accuracy and selection consistency.",
        "We enhance Fan et al.'s (2019) one-round distributed principal component\nanalysis algorithm by adding a second fixed-point iteration round. Random\nmatrix theory reveals the one-round estimator exhibits higher asymptotic error\nthan the pooling estimator under moderate local signal-to-noise ratios.\nRemarkably, our second iteration round eliminates this efficiency gap. It\nfollows from a careful analysis of the first-order perturbation of eigenspaces.\nEmpirical experiments on synthetic and benchmark datasets consistently\ndemonstrate the two-round method's statistical advantage over the one-round\napproach.",
        "Tumor response, a binary variable, has historically been the main measure of\nantitumor activity for many cancer phase II single-arm trials. Simon two-stage\ndesigns are often used. Sargent et al. proposed a three-outcome trial design in\nthis setting which requires smaller sample sizes. For many new, molecularly\ntargeted therapies, however, tumor response may not be the most reliable\nendpoint for measuring anti-tumor activity. Increasingly, time-to-event\nendpoints, such as progression-free survival (PFS), are used in the phase II\nsetting. When such endpoints are the primary measure of efficacy, a randomized\nconcurrently controlled study design is usually required. Given limited\nresources for phase II, studies are often underpowered with relatively large\ntype I and II error rates, and it is sometimes unavoidable to have a \"gray\"\ndecision zone after phase II where a clear decision regarding further\ndevelopment actions cannot be made without additional information. Compared\nwith an underpowered standard two-outcome study, a three-outcome design prompts\nclinical trialists to contemplate the likelihood of landing in the \"gray\" zone\nat the trial design stage and choose study design parameters more\nappropriately. We propose a three-outcome design, with or without interim\nanalyses, for randomized comparative phase II trials when a time-to-event\nendpoint is used.",
        "Advancements in data collection have led to increasingly common repeated\nobservations with complex structures in biomedical studies. Treating these\nobservations as random objects, rather than summarizing features as vectors,\navoids feature extraction and better reflects the data's nature. Examples\ninclude repeatedly measured activity intensity distributions in physical\nactivity analysis and brain networks in neuroimaging. Testing whether these\nrepeated random objects differ across groups is fundamentally important;\nhowever, traditional statistical tests often face challenges due to the\nnon-Euclidean nature of metric spaces, dependencies from repeated measurements,\nand the unequal number of repeated measures. By defining within-subject\nvariability using pairwise distances between repeated measures and extending\nFr\\'echet analysis of variance, we develop a generalized Fr\\'echet test for\nexchangeable repeated random objects, applicable to general metric space-valued\ndata with unequal numbers of repeated measures. The proposed test can\nsimultaneously detect differences in location, scale, and within-subject\nvariability. We derive the asymptotic distribution of the test statistic, which\nfollows a weighted chi-squared distribution. Simulations demonstrate that the\nproposed test performs well across different types of random objects. We\nillustrate its effectiveness through applications to physical activity data and\nresting-state functional magnetic resonance imaging data.",
        "This work bridges the gap between staggered adoption designs and survival\nanalysis to estimate causal effects in settings with time-varying treatments,\naddressing a fundamental challenge in medical research exemplified by the\nStanford Heart Transplant study. In medical interventions, particularly organ\ntransplantation, the timing of treatment varies significantly across patients\ndue to factors such as donor availability and patient readiness, introducing\npotential bias in treatment effect estimation if not properly accounted for. We\nidentify conditions under which staggered adoption assumptions can justify the\nuse of survival analysis techniques for causal inference with time-varying\ntreatments. By establishing this connection, we enable the use of existing\nsurvival analysis methods while maintaining causal interpretability.\nFurthermore, we enhance estimation performance by incorporating double machine\nlearning methods, improving efficiency when handling complex relationships\nbetween patient characteristics and survival outcomes. Through both simulation\nstudies and application to heart transplant data, our approach demonstrates\nsuperior performance compared to traditional methods, reducing bias and\noffering theoretical guarantees for improved efficiency in survival analysis\nsettings.",
        "Hypergraph data, which capture multi-way interactions among entities, are\nbecoming increasingly prevalent in the big data eta. Generating new hyperlinks\nfrom an observed, usually high-dimensional hypergraph is an important yet\nchallenging task with diverse applications, such as electronic health record\nanalysis and biological research. This task is fraught with several challenges.\nThe discrete nature of hyperlinks renders many existing generative models\ninapplicable. Additionally, powerful machine learning-based generative models\noften operate as black boxes, providing limited interpretability. Key\nstructural characteristics of hypergraphs, including node degree heterogeneity\nand hyperlink sparsity, further complicate the modeling process and must be\ncarefully addressed. To tackle these challenges, we propose Denoising Diffused\nEmbeddings (DDE), a general generative model architecture for hypergraphs. DDE\nexploits potential low-rank structures in high-dimensional hypergraphs and\nadopts the state-of-the-art diffusion model framework. Theoretically, we show\nthat when true embeddings are accessible, DDE exactly reduces the task of\ngenerating new high-dimensional hyperlinks to generating new low-dimensional\nembeddings. Moreover, we analyze the implications of using estimated embeddings\nin DDE, revealing how hypergraph properties--such as dimensionality, node\ndegree heterogeneity, and hyperlink sparsity--impact its generative\nperformance. Simulation studies demonstrate the superiority of DDE over\nexisting methods, in terms of both computational efficiency and generative\naccuracy. Furthermore, an application to a symptom co-occurrence hypergraph\nderived from electronic medical records uncovers interesting findings and\nhighlights the advantages of DDE.",
        "Quantum computing hardware is advancing at a rapid pace, yet the lack of\nhigh-level programming abstractions remains a serious bottleneck in the\ndevelopment of new applications. Widely used frameworks still rely on\ngate-level circuit descriptions, causing the algorithm's functional intent to\nbecome lost in low-level implementation details, and hindering flexibility and\nreuse. While various high-level quantum programming languages have emerged in\nrecent years - offering a significant step toward higher abstraction - many\nstill lack support for classical-like expression syntax, and native constructs\nfor useful quantum algorithmic idioms. This paper presents Qmod, a high-level\nquantum programming language designed to capture algorithmic intent in natural\nterms while delegating implementation decisions to automation. Qmod introduces\nquantum numeric variables and expressions, including digital fixed-point\narithmetic tuned for compact representations and optimal resource usage. Beyond\ndigital encoding, Qmod also supports non-digital expression modes - phase and\namplitude encoding - frequently exploited by quantum algorithms to achieve\ncomputational advantages. We describe the language's constructs, demonstrate\npractical usage examples, and outline future work on evaluating Qmod across a\nbroader set of use cases.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "Chaotic behavior or lack thereof in non-Hermitian systems is often diagnosed\nvia spectral analysis of associated complex eigenvalues. Very recently,\nsingular values of the associated non-Hermitian systems have been proposed as\nan effective measure to study dissipative quantum chaos. Motivated by the rich\nproperties of non-Hermitian power-law banded random matrices and its promise as\na platform to study localized and delocalized phases in non-Hermitian systems,\nwe make an in-depth study to assess different spectral phases of these matrices\nthrough the lens of both complex eigenvalues and singular values. Remarkably,\nthe results from complex spectra and singular value analysis are seemingly\ndifferent, thereby necessitating caution while identifying different phases. We\nalso exemplify our findings by studying a non-Hermitian Hamiltonian with a\ncomplex on-site disorder. Our work indicates that systems, where disorder is\npresent both in the Hermitian and non-Hermitian segments of a Hamiltonian, are\nsensitive to the specific diagnostic tool that needs to be employed to study\nquantum chaos.",
        "We consider the classical Kuramoto model (KM) with natural frequencies and\nits continuum limit (CL), and discuss the existence of synchronized solutions\nand their bifurcations and stability. We specifically assume that the frequency\nfunction is symmetric and linear in the CL, so that the natural frequencies are\nevenly spaced in the KM. We show that in the KM, $O(2^n)$ one-parameter\nfamilies of synchronized solutions are born and $O(2^n)$ {saddle-node and}\npitchfork bifurcations occur at least, when the node number $n$ is odd and\ntends to infinity. Moreover, we prove that the family of synchronized solutions\nobtained in the previous work suffers a saddle-node bifurcation at which its\nstability changes from asymptotically stable to unstable and the other families\nof synchronized solutions are unstable in the KM. For the CL, we show that the\none-parameter family of synchronized solutions obtained in the previous work is\nthe only continuous one and there exist uncountably many one-parameter families\nof noncontinuous synchronized solutions and that the former is asymptotically\nstable and the latter are unstable.",
        "Studying the de Rham complex is a natural choice when working with problems\nin electromagnetics and fluid mechanics. By discretizing the complex correctly,\nit is possible to attain stable numerical methods to tackle these problems. An\nimportant consideration when constructing the discrete complex is that it must\npreserve the cohomology structure of the original one. This property is not\nguaranteed when the discrete function spaces chosen are hierarchical B-splines.\nResearch shows that a poor choice of refinement domains may give rise to\nspurious harmonic forms that ruin the accuracy of solutions, even for the\nsimplest partial differential equations. Another crucial aspect to consider in\nthe hierarchical setting is the notion of admissibility, as it is possible to\nobtain optimal convergence rates of numerical solutions by limiting the\nmulti-level interaction of basis functions. We will focus on the\ntwo-dimensional de Rham complex over the unit square $\\Omega \\subseteq\n\\mathbb{R}^2$. In this scenario, the discrete de Rham complex should be exact,\nand we provide both the theoretical and the algorithm-implementation framework\nto ensure this is the case. Moreover, we show that, under a common restriction,\nthe admissibility class of the first space of the discrete complex persists\nthroughout the remaining spaces. Finally, we include numerical results that\nmotivate the importance of the previous concerns for the vector Laplace and\nMaxwell eigenvalue problems.",
        "Spatial proteomics technologies have transformed our understanding of complex\ntissue architectures by enabling simultaneous analysis of multiple molecular\nmarkers and their spatial organization. The high dimensionality of these data,\nvarying marker combinations across experiments and heterogeneous study designs\npose unique challenges for computational analysis. Here, we present Virtual\nTissues (VirTues), a foundation model framework for biological tissues that\noperates across the molecular, cellular and tissue scale. VirTues introduces\ninnovations in transformer architecture design, including a novel tokenization\nscheme that captures both spatial and marker dimensions, and attention\nmechanisms that scale to high-dimensional multiplex data while maintaining\ninterpretability. Trained on diverse cancer and non-cancer tissue datasets,\nVirTues demonstrates strong generalization capabilities without task-specific\nfine-tuning, enabling cross-study analysis and novel marker integration. As a\ngeneralist model, VirTues outperforms existing approaches across clinical\ndiagnostics, biological discovery and patient case retrieval tasks, while\nproviding insights into tissue function and disease mechanisms.",
        "By using the exact Bethe wavefunctions of the one-dimensional Hubbard model\nwith $N$ spin-up fermions and one spin-down impurity, we derive an analytic\nexpression of the impurity form factor, in the form of a determinant of a\n$(N+1)$ by $(N+1)$ matrix. This analytic expression enables us to exactly\ncalculate spectral properties of one-dimensional Fermi polarons in lattices,\nwhen the masses of the impurity particle and the Fermi bath are equal. We\npresent the impurity spectral function as functions of the on-site interaction\nstrength and the filling factor of the Fermi bath, and discuss the origin of\nFermi singularities in the spectral function at small momentum and the\nemergence of polaron quasiparticles at large momentum near the boundary of\nBrillouin zone. Our analytic expression of the impurity form factors pave the\nway to exploring the intriguing dynamics of a particle interacting with a Fermi\nbath. Our exact predictions on the impurity spectral function could be directly\nexamined in cold-atom laboratories by using the radio-frequency spectroscopy\nand Ramsey spectroscopy.",
        "In an $r$-partite graph, an independent transversal of size $s$ (ITS)\nconsists of $s$ vertices from each part forming an independent set. Motivated\nby a question from Bollob\\'as, Erd\\H{o}s, and Szemer\\'edi (1975), Di Braccio\nand Illingworth (2024) inquired about the minimum degree needed to ensure an $n\n\\times \\cdots \\times n$ $r$-partite graph contains $K_r(s)$, a complete\n$r$-partite graph with $s$ vertices in each part. We reformulate this as\nfinding the smallest $n$ such that any $n \\times \\cdots \\times n$ $r$-partite\ngraph with maximum degree $\\Delta$ has an ITS. For any $\\varepsilon>0$, we\nprove the existence of a $\\gamma>0$ ensuring that if $G$ is a multipartite\ngraph partitioned as $(V_1, V_2, \\ldots, V_r)$, where the average degree of\neach part $V_i$ is at most $D$, the maximum degree of any vertex to any part\n$V_i$ is at most $\\gamma D$, and the size of each part $V_i$ is at least $(s +\n\\varepsilon)D$, then $G$ possesses an ITS. The constraint $(s + \\varepsilon)D$\non the part size is tight. This extends results of Loh and Sudakov (2007),\nGlock and Sudakov (2022), and Kang and Kelly (2022). We also show that any $n\n\\times \\cdots \\times n$ $r$-partite graph with minimum degree at least\n$\\left(r-1-\\frac{1}{2s^2}\\right)n$ contains $K_r(s)$ and provide a relative\nTur\\'an-type result. Additionally, this paper explores counting ITSs in\nmultipartite graphs.",
        "We present a measurement of the supermassive black hole (SMBH) mass of the\nnearby lenticular galaxy NGC 383, based on Atacama Large\nMillimeter\/sub-millimeter Array (ALMA) observations of the $^{12}$CO(2-1)\nemission line with an angular resolution of $0.''050\\times0.''024$\n($\\approx16\\times8$ pc$^2$). These observations spatially resolve the nuclear\nmolecular gas disc down to $\\approx41,300$ Schwarzschild radii and the SMBH\nsphere of influence by a factor of $\\approx24$ radially, better than any other\nSMBH mass measurement using molecular gas to date. The high resolution enables\nus to probe material with a maximum circular velocity of $\\approx1040$ km\/s,\neven higher than those of the highest-resolution SMBH mass measurements using\nmegamasers. We detect a clear Keplerian increase (from the outside in) of the\nline-of-sight rotation velocities, a slight offset between the gas disc\nkinematic (i.e. the position of the SMBH) and morphological (i.e. the centre of\nthe molecular gas emission) centres, an asymmetry of the innermost rotation\nvelocity peaks and evidence for a mild position angle warp and\/or non-circular\nmotions within the central $\\approx0.''3$. By forward modelling the mass\ndistribution and ALMA data cube, we infer a SMBH mass of\n$(3.58\\pm0.19)\\times10^9$ M$_\\odot$ ($1\\sigma$ confidence interval), more\nprecise ($5\\%$) but consistent within $\\approx1.4\\sigma$ with the previous\nmeasurement using lower-resolution molecular gas data. Our measurement\nemphasises the importance of high spatial resolution observations for precise\nSMBH mass determinations.",
        "We report our investigations on Ag3LiIr1.4Ru0.6O6, which results from the Ru\nsubstitution in the Kitaev quantum spin liquid candidate Ag3LiIr2O6. It\ncrystallizes in the monoclinic C2\/m space group like its parent compound,\nAg3LiIr2O6. Our susceptibility measurements reveal an effective moment = 2.6\nmuB, which is higher than the moments of the parent compound and less than that\nof the Ru-analog (Ag3LiRu2O6), suggesting the presence of magnetic Ir4+ (Jeff=\n1\/2) and Ru4+ (S=1). Bulk magnetic susceptibility suggests long-range order\n(LRO)at T~20 K, whereas no clear signature is present in the heat capacity.\nLikewise, there is a loss of the 7Li NMR spectral intensity around T~20 K as\nexpected at the onset of LRO, but a complete wipe-out is not seen in contrast\nto the result in Ag3LiIr2O6. There is also a T~20 K anomaly in the 7Li NMR\nrelaxation rate and also a fall in the 7Li NMR shift with decreasing\ntemperature. These results suggest LRO at T~20 K in Ag3LiIr1.4Ru0.6O6. However,\nat low-T below 10 K, we observe a power law variation in magnetic heat capacity\nand spin lattice relaxation rate, temperature-independent-7K, and no further\nloss of the 7Li NMR spectral intensity. These results might suggest the\npersistence or stabilisation of a quantum spin liquid-like phase, perhaps from\na fraction of the sample in Ag3LiIr1.4Ru0.6O6 below 10 K. Our muon spin\nrelaxation measurements suggest ordering around 20 K, consistent with our other\nprobes. It appears that the main effect of Ru-substitution is to shift the LRO\nto a higher temperature in comparison with Ag3LiIr2O6, though there are\nsignatures of a novel phase below about 10 K.",
        "In the information age, it has become increasingly common for data containing\nrecords about overlapping individuals to be distributed across multiple\nsources, making it necessary to identify which records refer to the same\nindividual. The goal of record linkage is to estimate this unknown structure in\nthe absence of a unique identifiable attribute. We introduce a Bayesian\nhierarchical record linkage model for spatial location data motivated by the\nestimation of individual specific growth-size curves for conifer species using\ndata derived from overlapping LiDAR scans. Annual tree growth may be estimated\ndependent upon correctly identifying unique individuals across scans in the\npresence of noise. We formalize a two-stage modeling framework, connecting the\nrecord linkage model and a flexible downstream individual tree growth model,\nthat provides robust uncertainty quantification and propagation through both\nstages of the modeling pipeline via an extension of the linkage-averaging\napproach of Sadinle (2018). In this paper, we discuss the two-stage model\nformulation, outline the computational strategies required to achieve\nscalability, assess the model performance on simulated data, and fit the model\nto a bi-temporal dataset derived from LiDAR scans of the Upper Gunnison\nWatershed provided by the Rocky Mountain Biological Laboratory to assess the\nimpact of key topographic covariates on the growth behavior of conifer species\nin the Southern Rocky Mountains (USA).",
        "We develop multipoint stress mixed finite element methods for linear\nelasticity with weak stress symmetry on cuboid grids, which can be reduced to a\nsymmetric and positive definite cell-centered system. The methods employ the\nlowest-order enhanced Raviart-Thomas finite element space for the stress and\npiecewise constant displacement. The vertex quadrature rule is employed to\nlocalize the interaction of stress degrees of freedom, enabling local stress\nelimination around each vertex. We introduce two methods. The first method uses\na piecewise constant rotation, resulting in a cell-centered system for the\ndisplacement and rotation. The second method employs a continuous piecewise\ntrilinear rotation and the vertex quadrature rule for the asymmetry bilinear\nforms, allowing for further elimination of the rotation and resulting in a\ncell-centered system for the displacement only. Stability and error analysis is\nperformed for both methods. For the stability analysis of the second method, a\nnew auxiliary H-curl conforming matrix-valued space is constructed, which forms\nan exact sequence with the stress space. A matrix-matrix inf-sup condition is\nshown for the curl of this auxiliary space and the trilinear rotation space.\nFirst-order convergence is established for all variables in their natural\nnorms, as well as second-order superconvergence of the displacement at the cell\ncenters. Numerical results are presented to verify the theory.",
        "We study $q$-deformation of the probability measure on partitions, i.e.,\n$q$-deformed random partitions. We in particular consider the $q$-Plancherel\nmeasure and show a determinantal formula for the correlation function using a\n$q$-deformation of the discrete Bessel kernel. We also investigate\nRiemann-Hilbert problems associated with the corresponding orthogonal\npolynomials and obtain $q$-Painlev{\\'e} equations from the $q$-difference Lax\nformalism.",
        "In networked dynamical systems, inferring governing parameters is crucial for\npredicting nodal dynamics, such as gene expression levels, species abundance,\nor population density. While many parameter estimation techniques rely on\ntime-series data, particularly systems that converge over extreme time ranges,\nonly noisy steady-state data is available, requiring a new approach to infer\ndynamical parameters from noisy observations of steady states. However, the\ntraditional optimization process is computationally demanding, requiring\nrepeated simulation of coupled ordinary differential equations (ODEs). To\novercome these limitations, we introduce a surrogate objective function that\nleverages decoupled equations to compute steady states, significantly reducing\ncomputational complexity. Furthermore, by optimizing the surrogate objective\nfunction, we obtain steady states that more accurately approximate the ground\ntruth than noisy observations and predict future equilibria when topology\nchanges. We empirically demonstrate the effectiveness of the proposed method\nacross ecological, gene regulatory, and epidemic networks. Our approach\nprovides an efficient and effective way to estimate parameters from\nsteady-state data and has the potential to improve predictions in networked\ndynamical systems.",
        "We revisit the results of Kim, and of Katsoulis and Ramsey concerning\nhyperrigidity for non-degenerate C*-correspondences. We show that the tensor\nalgebra is hyperrigid, if and only if Katsura's ideal acts non-degenerately, if\nand only if Katsura's ideal acts non-degenerately under any representation.\nThis gives a positive answer to the question of Katsoulis and Ramsey, showing\nthat their necessary condition and their sufficient condition for hyperrigidity\nof the tensor algebra are equivalent. Non-degeneracy of the left action of\nKatsura's ideal was also shown by Kim to be equivalent to hyperrigidity for the\nselfadjoint operator space associated with the C*-correspondence, and our\napproach provides a simplified proof of this result as well. In the process we\nrevisit Arveson's criterion connecting maximality with the unique extension\nproperty and hyperrigidity, in conjunction with the work of Salomon on\ngenerating sets."
      ]
    }
  },
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b4",
    "start_title":"Mean\u2010field games with differing beliefs for algorithmic trading",
    "start_abstract":"Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity.",
    "start_categories":[
      "q-fin.MF"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Linear-quadratic mean field games"
      ],
      "abstract":[
        "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Online Learning-Based Predictive Control for Nonlinear System",
        "Characterization of Highly Robust Solutions in Multi-Objective\n  Programming in Banach Spaces",
        "Engagement Zones for a Turn Constrained Pursuer",
        "Nonlinearly Preconditioned Gradient Methods under Generalized Smoothness",
        "Approximate solutions in multiobjective interval-valued optimization\n  problems: Existence theorems and optimality conditions",
        "Pseudo-concave optimization of the first eigenvalue of elliptic\n  operators with application to topology optimization by homogenization",
        "PDLP: A Practical First-Order Method for Large-Scale Linear Programming",
        "Generalized transition uncertainties in constrained Markov decision\n  processes",
        "Unifying restart accelerated gradient and proximal bundle methods",
        "Double Traversals in Optimal Picker Routes for Warehouses with Multiple\n  Blocks",
        "Two Innovations in Inexact Augmented Lagrangian Methods for Convex\n  Optimization",
        "Optimal mixed fleet and charging infrastructure planning to electrify\n  demand responsive feeder services with target CO2 emission constraints",
        "Inner approximations of convex sets and intersections of projectionally\n  exposed cones",
        "The Eigenfunctions of the Transfer Operator for the Dyson model in a\n  field",
        "Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for\n  Stable Lesion Segmentation",
        "Learning Privacy from Visual Entities",
        "A proposal for removing $\\pi N$-state contamination from the nucleon\n  induced pseudoscalar form factor in lattice QCD",
        "Modular Units on $X_{1}( p)$ and Quotients of the Cuspidal Group",
        "Entente: Cross-silo Intrusion Detection on Network Log Graphs with\n  Federated Learning",
        "Twin-Space Representation of Classical Mapping Model in the Constraint\n  Phase Space Representation: Numerically Exact Approach to Open Quantum\n  Systems",
        "Structure and Dynamics of Deep Eutectic Systems from Cluster-Optimized\n  Energy Functions",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "Towards Heisenberg limit without critical slowing down via quantum\n  reinforcement learning",
        "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
        "Unreflected Use of Tabular Data Repositories Can Undermine Research\n  Quality",
        "Quantum stochastic communication via high-dimensional entanglement",
        "The closure of linear foliations",
        "Dynamics near a class of nonhyperbolic fixed points"
      ],
      "abstract":[
        "In this paper, we propose an online learning-based predictive control (LPC)\napproach designed for nonlinear systems that lack explicit system dynamics.\nUnlike traditional model predictive control (MPC) algorithms that rely on known\nsystem models to optimize controller outputs, our proposed algorithm integrates\na reinforcement learning component to learn optimal policies in real time from\nthe offline dataset and real-time data. Additionally, an optimal control\nproblem (OCP)-based optimization framework is incorporated to enhance real-time\ncomputational efficiency while ensuring stability during online operation.\nMoreover, we rigorously establish the super-linear convergence properties of\nthe algorithm. Finally, extensive simulations are performed to evaluate the\nfeasibility and effectiveness of the proposed approach.",
        "This paper delves into the challenging issues in uncertain multi-objective\noptimization, where uncertainty permeates nonsmooth nonconvex objective and\nconstraint functions. In this context, we investigate highly robust (weakly\nefficient) solutions, a solution concept defined by efficiency across all\nscenarios. Our exploration reveals important relationships between highly\nrobust solutions and other robustness notions, including set-based and\nworst-case notions, as well as connections with proper and isolated efficiency.\nLeveraging modern techniques from variational analysis, we establish necessary\nand sufficient optimality conditions for these solutions. Moreover, we explore\nthe robustness of multi-objective optimization problems in the face of various\nuncertain sets, such as ball, ellipsoidal, and polyhedral sets.",
        "This work derives two basic engagement zone models, describing regions of\npotential risk or capture for a mobile vehicle by a pursuer. The pursuer is\nmodeled as having turn-constraints rather than simple motion. Turn-only\n(C-Paths) and turn-straight (CS-Paths) paths are considered for the pursuer of\nlimited range. Following the derivation, a simulation of a vehicle avoiding the\npursuer's engagement zone is provided.",
        "We analyze nonlinearly preconditioned gradient methods for solving smooth\nminimization problems. We introduce a generalized smoothness property, based on\nthe notion of abstract convexity, that is broader than Lipschitz smoothness and\nprovide sufficient first- and second-order conditions. Notably, our framework\nencapsulates algorithms associated with the clipping gradient method and brings\nout novel insights for the class of $(L_0,L_1)$-smooth functions that has\nreceived widespread interest recently, thus allowing us to go beyond already\nestablished methods. We investigate the convergence of the proposed method in\nboth the convex and nonconvex setting.",
        "This paper is devoted to the study of approximate solutions for a\nmultiobjective interval-valued optimization problem based on an interval order.\nWe establish new existence theorems of approximate solutions for such a problem\nunder some mild conditions. Moreover, we give KKT optimality conditions for\napproximate solutions for such a problem whose associated functions are\nnonsmooth and nonconvex. We also propose the approximate KKT optimality\ncondition of an approximate solution for such a problem. Finally, we apply some\nobtained results to a noncooperative game involving the multiobjective\ninterval-valued function.",
        "We consider optimization problems of the first eigenvalue of linear elliptic\noperators. It has an application to a two-phase optimal design problem (also\nknown as topology optimization problem) relaxed by the homogenization method.\nUnder certain assumptions, we show that the first eigenvalue is a\npseudo-concave function with respect to the density-like parameter and apply\nthe result to optimal design problems of conductivity and elasticity. Due to\npseudo-concavity, every stationary point is a global optimal solution for a\nmaximization problem, and there exists a solution that is an extreme point\n(corresponding to a 0-1 design) for a minimization problem. In numerical\nexperiments, we demonstrate that a global optimal solution or a 0-1 solution\ncan be obtained by a simple projected gradient method. These problems can be\nused as benchmark problems to test heuristic topology optimization methods used\nin engineering. The 80-line FreeFEM code is provided in the appendix.",
        "We present PDLP, a practical first-order method for linear programming (LP)\ndesigned to solve large-scale LP problems. PDLP is based on the primal-dual\nhybrid gradient (PDHG) method applied to the minimax formulation of LP. PDLP\nincorporates several enhancements to PDHG, including diagonal preconditioning,\npresolving, adaptive step sizes, adaptive restarting, and feasibility\npolishing. Our algorithm is implemented in C++, available in Google's\nopen-source OR-Tools library, and supports multithreading.\n  To evaluate our method, we introduce a new collection of eleven large-scale\nLP problems with sizes ranging from 125 million to 6.3 billion nonzeros. PDLP\nsolves eight of these instances to optimality gaps of 1\\% (with primal and dual\nfeasibility errors of less than $10^{-8}$) within six days on a single machine.\nWe also compare PDLP with Gurobi barrier, primal simplex, and dual simplex\nimplementations. Gurobi barrier solves only three instances, exceeding our 1TB\nRAM limit on the other eight. While primal and dual simplex are more\nmemory-efficient than the barrier method, they are slower and solve only three\ninstances within six days.\n  Compared with the conference version of this work (in: Advances in Neural\nInformation Processing Systems 34 (NeurIPS 2021)), the key new contributions\nare: (i) feasibility polishing, a technique that quickly finds solutions that\nare approximately optimal but almost exactly feasible (without which only three\nof the eleven problems can be solved); (ii) a multithreaded C++ implementation\navailable in Google OR-Tools; and (iii) a new collection of large-scale LP\nproblems. Note that the conference version should be referred to for\ncomparisons with SCS and ablation studies, which we do not repeat in this\npaper.",
        "We examine a constrained Markov decision process under uncertain transition\nprobabilities, with the uncertainty modeled as deviations from observed\ntransition probabilities. We construct the uncertainty set associated with the\ndeviations using polyhedral and second-order cone constraints and employ a\nrobust optimization framework. We demonstrate that each inner optimization\nproblem of the robust model can be equivalently transformed into a second-order\ncone programming problem. Using strong duality arguments, we show that the\nresulting robust problem can be equivalently reformulated into a non-convex\nprogramming problem that includes bilinear and second-order cone constraints.\nIn the numerical experiments, we study a machine replacement problem and\nexplore potential sources of uncertainty in the transition probabilities. We\nexamine how the optimal values and solutions differ as we vary the feasible\nregion of the uncertainty set, considering only polyhedral constraints and a\ncombination of polyhedral and second-order cone constraints. Furthermore, we\nanalyze the impact of the number of states, the discount factor, and variations\nin the feasible region of the uncertainty set on the optimal values.",
        "This paper presents a novel restarted version of Nesterov's accelerated\ngradient method and establishes its optimal iteration-complexity for solving\nconvex smooth composite optimization problems. The proposed restart accelerated\ngradient method is shown to be a specific instance of the accelerated inexact\nproximal point framework introduced in \"An accelerated hybrid proximal\nextragradient method for convex optimization and its implications to\nsecond-order methods\" by Monteiro and Svaiter, SIAM Journal on Optimization,\n2013. Furthermore, this work examines the proximal bundle method within the\ninexact proximal point framework, demonstrating that it is an instance of the\nframework. Notably, this paper provides new insights into the underlying\nalgorithmic principle that unifies two seemingly disparate optimization\nmethods, namely, the restart accelerated gradient and the proximal bundle\nmethods.",
        "The Picker Routing Problem is a variation of the Rectilinear Traveling\nSalesman Problem that involves finding the optimal tour of a warehouse that\ncollects all the required items on a given pick list. It has been proven that\nfor a rectangular warehouse with only two cross-aisles, traversing an aisle\ntwice is not required for an optimal tour; however, instances exist where this\nis needed for warehouses with more than two cross-aisles. In this paper, we\ndefine categories of double traversals in the Rectilinear Traveling Salesman\nProblem and prove that some of these are not required in a minimal tour. This\nis then applied to the Picker Routing Problem for generalized rectangular\nwarehouses of any size. These results are then used to show that some vertices\ncan be ignored when constructing a tour subgraph, and two out of the six\nvertical edge configurations considered in existing algorithms are not\nrequired. Finally, it is demonstrated that the horizontal edges of a minimal\ntour subgraph determine the vertical edge configurations, a result that could\nlead to practical improvement to existing algorithms.",
        "This paper presents two new techniques relating to inexact solution of\nsubproblems in augmented Lagrangian methods for convex programming. The first\ninvolves combining a relative error criterion for solution of the subproblems\nwith over- or under-relaxation of the multiplier update step. In one\ninterpretation of our proposed iterative scheme, a predetermined amount of\nrelaxation effects the criterion for an acceptably accurate solution value.\nAlternatively, the amount of multiplier step relaxation can be adapted to the\naccuracy of the subproblem subject to a viability test employing the\ndiscriminant of a certain quadratic function. The second innovation involves\nsolution of augmented Lagrangian subproblems for problems posed in standard\nFenchel-Rockafellar form. We show that applying alternating minimization to\nthis subproblem, as in the first two steps of the ADMM, is equivalent to\nexecuting the classical proximal gradient method on a dual formulation of the\nsubproblem. By substituting more sophisticated variants of the proximal\ngradient method for the classical one, it is possible to construct new\nADMM-like methods with better empirical performance than using ordinary\nalternating minimization within an inexact augmented Lagrangian framework. The\npaper concludes by describing some computational experiments exploring using\nthese two innovations, both separately and jointly, to solve LASSO problems.",
        "Electrifying demand-responsive transport systems need to plan the charging\ninfrastructure carefully, considering the trade-offs of charging efficiency and\ncharging infrastructure costs. Earlier studies assume a fully electrified fleet\nand overlook the planning issue in the transition period. This study addresses\nthe joint fleet size and charging infrastructure planning for a\ndemand-responsive feeder service under stochastic demand, given a user-defined\ntargeted CO2 emission reduction policy. We propose a bi-level optimization\nmodel where the upper-level determines charging station configuration given\nstochastic demand patterns, whereas the lower-level solves a mixed fleet\ndial-a-ride routing problem under the CO2 emission and capacitated charging\nstation constraints. An efficient deterministic annealing metaheuristic is\nproposed to solve the CO2-constrained mixed fleet routing problem. The\nperformance of the algorithm is validated by a series of numerical test\ninstances with up to 500 requests. We apply the model for a real-world case\nstudy in Bettembourg, Luxembourg, with different demand and customised CO2\nreduction targets. The results show that the proposed method provides a\nflexible tool for joint charging infrastructure and fleet size planning under\ndifferent levels of demand and CO2 emission reduction targets.",
        "A convex cone is said to be projectionally exposed (p-exposed) if every face\narises as a projection of the original cone. It is known that, in dimension at\nmost four, the intersection of two p-exposed cones is again p-exposed. In this\npaper we construct two p-exposed cones in dimension $5$ whose intersection is\nnot p-exposed. This construction also leads to the first example of an amenable\ncone that is not projectionally exposed, showing that these properties, which\ncoincide in dimension at most $4$, are distinct in dimension $5$. In order to\nachieve these goals, we develop a new technique for constructing arbitrarily\ntight inner convex approximations of compact convex sets with desired facial\nstructure. These inner approximations have the property that all proper faces\nare extreme points, with the exception of a specific exposed face of the\noriginal set.",
        "The recent works \\cite{EFMV2024} and \\cite{JOP2023} have studied the spectral\nproperties of the Dyson model in the absence of an external field. This paper\nis a continuation of \\cite{EFMV2024} and aims to bridge the gap in the\nliterature by investigating the Dyson model in a field.\\\\ In this paper, we\nprove that, for high temperatures or strong magnetic fields, there exists a\nnon-negative, integrable (with respect to the unique half-line Gibbs measure)\neigenfunction of the transfer operator for the Dyson model if $\\alpha\\in(\\frac\n3 2,2]$. However, unlike in the zero-magnetic-field case, this eigenfunction is\nnot continuous.",
        "Deep learning has achieved significant advancements in medical image\nsegmentation, but existing models still face challenges in accurately\nsegmenting lesion regions. The main reason is that some lesion regions in\nmedical images have unclear boundaries, irregular shapes, and small tissue\ndensity differences, leading to label ambiguity. However, the existing model\ntreats all data equally without taking quality differences into account in the\ntraining process, resulting in noisy labels negatively impacting model training\nand unstable feature representations. In this paper, a data-driven alternating\nlearning (DALE) paradigm is proposed to optimize the model's training process,\nachieving stable and high-precision segmentation. The paradigm focuses on two\nkey points: (1) reducing the impact of noisy labels, and (2) calibrating\nunstable representations. To mitigate the negative impact of noisy labels, a\nloss consistency-based collaborative optimization method is proposed, and its\neffectiveness is theoretically demonstrated. Specifically, the label confidence\nparameters are introduced to dynamically adjust the influence of labels of\ndifferent confidence levels during model training, thus reducing the influence\nof noise labels. To calibrate the learning bias of unstable representations, a\ndistribution alignment method is proposed. This method restores the underlying\ndistribution of unstable representations, thereby enhancing the discriminative\ncapability of fuzzy region representations. Extensive experiments on various\nbenchmarks and model backbones demonstrate the superiority of the DALE\nparadigm, achieving an average performance improvement of up to 7.16%.",
        "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes.",
        "In the PACS10 project, the PACS collaboration has generated three sets of the\nPACS10 gauge configurations at the physical point with lattice volume larger\nthan $(10\\;{\\rm fm})^4$ and three different lattice spacings. The isovector\nnucleon form factors had been already calculated by using two sets of the\nPACS10 gauge configurations. In our strategy, the smearing parameters of the\nnucleon interpolation operator were highly optimized to eliminate as much as\npossible the contribution of excited states in the nucleon two-point function.\nThis strategy was quite successful in calculations of the electric ($G_E$),\nmagnetic ($G_M$) and axial-vector ($F_A$) form factors, while the induced\npseudoscalar ($F_P$) and pseudoscalar ($G_P$) form factors remained strongly\naffected by residual contamination of $\\pi N$-state contribution. In this work,\nwe propose a simple method to remove the $\\pi N$-state contamination from the\n$F_P$ form factor, and then evaluate the induced pseudoscalar charge $g_P^\\ast$\nand the pion-nucleon coupling $g_{\\pi NN}$ from existing data in a new\nanalysis. Applying this method to the $G_P$ form factor is also considered with\na help of the axial Ward-Takahashi identity.",
        "Modular units are functions on modular curves whose divisors are supported on\nthe cusps. They form a free abelian group of rank at most one less than the\nnumber of cusps. In this paper we study the group of modular units on $X_{1}( p\n)$, with prime level $p \\ge 5$. We give an explicit basis for this group and\nstudy certain rational subgroups of it. We use the basis to numerically\ninvestigate the structure of the cuspidal group of $X_{1}( p)$ and its rational\nsubgroup. In the later stages of this paper we use our basis to determine a\nspecific large quotient of the cuspidal group.",
        "Graph-based Network Intrusion Detection System (GNIDS) has gained significant\nmomentum in detecting sophisticated cyber-attacks, like Advanced Persistent\nThreat (APT), in an organization or across organizations. Though achieving\nsatisfying detection accuracy and adapting to ever-changing attacks and normal\npatterns, all prior GNIDSs assume the centralized data settings directly, but\nnon-trivial data collection is not always practical under privacy regulations\nnowadays. We argue that training a GNIDS model has to consider privacy\nregulations, and propose to leverage federated learning (FL) to address this\nprominent challenge.\n  Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like\nnon-IID (independent and identically distributed) graph data over clients and\nthe diverse design choices taken by different GNIDS. We address these issues\nwith a set of novel techniques tailored to the graph datasets, including\nreference graph synthesis, graph sketching and adaptive contribution scaling,\nand develop a new system Entente. We evaluate Entente on the large-scale LANL,\nOpTC and Pivoting datasets. The result shows Entente outperforms the other\nbaseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate\nEntente under FL poisoning attacks tailored to the GNIDS setting, and show\nEntente is able to bound the attack success rate to low values. Overall, our\nresult suggests building cross-silo GNIDS is feasible and we hope to encourage\nmore efforts in this direction.",
        "The constraint coordinate-momentum \\textit{phase space} (CPS) has recently\nbeen developed to study nonadiabatic dynamics in gas-phase and condensed-phase\nmolecular systems. Although the CPS formulation is exact for describing the\ndiscrete (electronic\/ vibrational\/spin) state degrees of freedom (DOFs), when\nsystem-bath models in condense phase are studied, previous works often employ\nthe discretization of environmental bath DOFs, which breaks the time\nirreversibility and may make it difficult to obtain numerically converged\nresults in the long-time limit. In this paper, we develop an exact\ntrajectory-based phase space approach by adopting the twin-space (TS)\nformulation of quantum statistical mechanics, in which the density operator of\nthe reduced system is transformed to the wavefunction of an expanded system\nwith twice the DOFs. The classical mapping model (CMM) is then used to map the\nHamiltonian of the expanded system to its equivalent classical counterpart on\nCPS. To demonstrate the applicability of the TS-CMM approach, we compare\nsimulated population dynamics and nonlinear spectra for a few benchmark\ncondensed phase system-bath models with those obtained from the hierarchical\nequations of motion method, which shows that our approach yields accurate\ndynamics of open quantum systems.",
        "Generating energy functions for heterogeneous systems suitable for\nquantitative and predictive atomistic simulations is a challenging undertaking.\nThe present work combines a cluster-based approach with electronic structure\ncalculations at the density functional theory level and machine learning-based\nenergy functions for a spectroscopic reporter for eutectic mixtures consisting\nof water, acetamide and KSCN. Two water models are considered: TIP3P which is\nconsistent with the CGenFF energy function and TIP4P which - as a water model -\nis superior to TIP4P. Both fitted models, {\\bf M2$^{\\rm TIP3P}$} and {\\bf\n  M2$^{\\rm TIP4P}$}, yield favourable thermodynamic, structural, spectroscopic\nand transport properties from extensive molecular dynamics simulations. In\nparticular, the slow and fast decay times from 2-dimensional infrared\nspectroscopy and the viscosity for water-rich mixtures are described\nrealistically and consistent with experiments. On the other hand, including the\nco-solvent (acetamide) in the present case is expected to further improve the\ncomputed viscosity for low-water content. It is concluded that such a\ncluster-based approach is a promising and generalizable route for routine\nparametrization of heterogeneous, electrostatically dominated systems.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "Critical ground states of quantum many-body systems have emerged as vital\nresources for quantum-enhanced sensing. Traditional methods to prepare these\nstates often rely on adiabatic evolution, which may diminish the quantum\nsensing advantage. In this work, we propose a quantum reinforcement learning\n(QRL)-enhanced critical sensing protocol for quantum many-body systems with\nexotic phase diagrams. Starting from product states and utilizing\nQRL-discovered gate sequences, we explore sensing accuracy in the presence of\nunknown external magnetic fields, covering both local and global regimes. Our\nresults demonstrate that QRL-learned sequences reach the finite quantum speed\nlimit and generalize effectively across systems of arbitrary size, ensuring\naccuracy regardless of preparation time. This method can robustly achieve\nHeisenberg and super-Heisenberg limits, even in noisy environments with\npractical Pauli measurements. Our study highlights the efficacy of QRL in\nenabling precise quantum state preparation, thereby advancing scalable,\nhigh-accuracy quantum critical sensing.",
        "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
        "Data repositories have accumulated a large number of tabular datasets from\nvarious domains. Machine Learning researchers are actively using these datasets\nto evaluate novel approaches. Consequently, data repositories have an important\nstanding in tabular data research. They not only host datasets but also provide\ninformation on how to use them in supervised learning tasks. In this paper, we\nargue that, despite great achievements in usability, the unreflected usage of\ndatasets from data repositories may have led to reduced research quality and\nscientific rigor. We present examples from prominent recent studies that\nillustrate the problematic use of datasets from OpenML, a large data repository\nfor tabular data. Our illustrations help users of data repositories avoid\nfalling into the traps of (1) using suboptimal model selection strategies, (2)\noverlooking strong baselines, and (3) inappropriate preprocessing. In response,\nwe discuss possible solutions for how data repositories can prevent the\ninappropriate use of datasets and become the cornerstones for improved overall\nquality of empirical research studies.",
        "Entanglement has the ability to enhance the transmission of classical\ninformation over a quantum channel. However, fully harvesting this advantage\ntypically requires complex entangling measurements, which are challenging to\nimplement and scale with the system's size. In this work, we consider a natural\nquantum information primitive in which the message to be communicated is\nselected stochastically. We introduce a protocol that leverages\nhigh-dimensional entanglement to perform this task perfectly, without requiring\nquantum interference between particles at the measurement station. We\nexperimentally demonstrate the protocol's scalability in an optical setup using\n8-dimensional entanglement and multi-outcome detection, providing a practical\nsolution for stochastic communication and a robust method for certifying the\ndimensionality of entanglement in communication experiments.",
        "This paper presents a simplified geometric proof of the\nMolino-Alexandrino-Radeschi (MAR) Theorem, which states that the closure of a\nsingular Riemannian foliation on a complete Riemannian manifold is itself a\nsmooth singular Riemannian foliation. Our approach circumvents several\ntechnical and analytical tools employed in the previous proof of the Theorem,\nresulting in a more direct geometric demonstration. We first establish\nconditions for a projectable foliation to be Riemannian, focusing on compatible\nconnections. We then apply these results to linear foliations on vector bundles\nand their lifts to frame bundles. Finally, we use these findings to the\nlinearization of singular Riemannian foliations around leaf closures. This\nmethod allows us to prove the smoothness of the closure directly for the linear\nsemi-local model, bypassing the need for intermediate results on orbit-like\nfoliations.",
        "In this paper, we investigate some dynamical properties near a nonhyperbolic\nfixed point. Under some conditions on the higher nonlinear terms, we establish\na stable manifold theorem and a degenerate Hartman theorem. Furthermore, the\nfinite shadowing property also be discussed."
      ]
    }
  },
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Linear-quadratic mean field games",
    "start_abstract":"In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mean\u2010field games with differing beliefs for algorithmic trading"
      ],
      "abstract":[
        "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
      ],
      "categories":[
        "q-fin.MF"
      ]
    },
    "list":{
      "title":[
        "Short-Rate Derivatives in a Higher-for-Longer Environment",
        "Existence of Optimal Contracts for Principal-Agent Problem with Drift\n  Control and Quadratic Effort Cost",
        "Decentralized Annuity: A Quest for the Holy Grail of Lifetime Financial\n  Security",
        "Numerical methods for two-dimensional G-heat equation",
        "On consistency of optimal portfolio choice for state-dependent\n  exponential utilities",
        "Heterogenous Macro-Finance Model: A Mean-field Game Approach",
        "Modelling High-Frequency Data with Bivariate Hawkes Processes: Power-Law\n  vs. Exponential Kernels",
        "Dual Formulation of the Optimal Consumption problem with Multiplicative\n  Habit Formation",
        "Capturing Smile Dynamics with the Quintic Volatility Model: SPX,\n  Skew-Stickiness Ratio and VIX",
        "Pricing time-capped American options using Least Squares Monte Carlo\n  method",
        "Beyond the Leland strategies",
        "Pricing American options under rough volatility using deep-signatures\n  and signature-kernels",
        "Optimal risk-aware interest rates for decentralized lending protocols",
        "GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts",
        "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction",
        "MODRIC: A Cost Effective MODular Data Center Network Architecture with\n  Rich InterConnections",
        "Investigating Human-Aligned Large Language Model Uncertainty",
        "Synthesizing Consistent Novel Views via 3D Epipolar Attention without\n  Re-Training",
        "An X-ray view of the Cataclysmic Variable V902 Mon: Discovery of an\n  X-ray eclipse",
        "Superconducting LaPtH$_{ 6 }$ with triatomic hydrogen units",
        "Bridging Structural Dynamics and Biomechanics: Human Motion Estimation\n  through Footstep-Induced Floor Vibrations",
        "Connecting the dots: Tracing the evolutionary pathway of Polar Ring\n  Galaxies in the cases of NGC 3718, NGC 2685, and NGC 4262",
        "TRADES: Generating Realistic Market Simulations with Diffusion Models",
        "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
        "Optimizing CNN Architectures for Advanced Thoracic Disease\n  Classification",
        "Evolution of Spots and Stripes in Cellular Automata",
        "Formation of super-Earths and mini-Neptunes from rings of planetesimals",
        "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and\n  efficient reconstruction of dynamic scene"
      ],
      "abstract":[
        "We introduce a class of short-rate models that exhibit a ``higher for\nlonger'' phenomenon. Specifically, the short-rate is modeled as a general\ntime-homogeneous one-factor Markov diffusion on a finite interval. The lower\nendpoint is assumed to be regular, exit or natural according to boundary\nclassification while the upper endpoint is assumed to be regular with absorbing\nbehavior. In this setting, we give an explicit expression for price of a\nzero-coupon bond (as well as more general interest rate derivatives) in terms\nof the transition density of the short-rate under a new probability measure,\nand the solution of a non-linear ordinary differential equation (ODE). We then\nnarrow our focus to a class of models for which the transition density and ODE\ncan be solved explicitly. For models within this class, we provide conditions\nunder which the lower endpoint is regular, exit and natural. Finally, we study\ntwo specific models -- one in which the lower endpoint is exit and another in\nwhich the lower endpoint is natural. In these two models, we give an explicit\nsolution of transition density of the short-rate as a (generalized)\neigenfunction expansion. We provide plots of the transition density,\n(generalized) eigenfunctions, bond prices and the associated yield curve.",
        "The existence of optimal contracts of the principal-agent problem is a\nlong-standing problem. According to the general framework in Cvitani\\'c et al.\n[2], this existence can be derived from the existence of a classical solution\nto a degenerated fully nonlinear parabolic partial differential equation\nproblem. In this work we consider the simple case with drift control and\nquadratic cost function, then prove the existence of classical solution to that\nPDE.",
        "This paper presents a novel framework for decentralized annuities, aiming to\naddress the limitations of traditional pension systems such as defined\ncontribution (DC) and defined benefit (DB) plans, while providing lifetime\nfinancial support. It sheds light on often ignored pitfalls within current\nretirement schemes and introduces individual rationality properties. The\nresearch delves into various fairness concepts that underpin existing plans,\nemphasizing that decentralized annuities, while meeting similar fairness\ncriteria, offer enhanced flexibility for individual rationality and improved\nsocial welfare for all participants. Using theoretical models and examples, we\ndemonstrate the potential of decentralized annuities to outperform self-managed\nplans (DC) and to produce effects comparable to defined benefit (DB) plans,\nparticularly within larger participant pools. The paper concludes by exploring\nthe managerial implications of decentralized annuities and laying the\ngroundwork for the further advancement of equitable and sustainable\ndecentralized annuity systems.",
        "The G-expectation is a sublinear expectation. It is an important tool for\npricing financial products and managing risk thanks to its ability to deal with\nmodel uncertainty. The problem is how to efficiently quantify it since the\ncommonly used Monte Carlo method does not work. Fortunately, the expectation of\na G-normal random variable can be linked to the viscosity solution of a fully\nnonlinear G-heat equation. In this paper, we propose a novel numerical scheme\nfor the two-dimensional G-heat equation and pay more attention to the case that\nthere exists uncertainty on the correlationship, especially to the case that\nthe correlationship ranges from negative to positive. The scheme is monotonic,\nstable, and convergent. The numerical tests show that the scheme is highly\nefficient.",
        "In an arbitrage-free simple market, we demonstrate that for a class of\nstate-dependent exponential utilities, there exists a unique prediction of the\nrandom risk aversion that ensures the consistency of optimal strategies across\nany time horizon. Our solution aligns with the theory of forward performances,\nwith the added distinction of identifying, among the infinite possible\nsolutions, the one for which the profile remains optimal at all times for the\nmarket-adjusted system of preferences adopted.",
        "We investigate the full dynamics of capital allocation and wealth\ndistribution of heterogeneous agents in a frictional economy during booms and\nbusts using tools from mean-field games. Two groups in our models, namely the\nexpert and the household, are interconnected within and between their classes\nthrough the law of capital processes and are bound by financial constraints.\nSuch a mean-field interaction explains why experts accumulate a lot of capital\nin the good times and reverse their behavior quickly in the bad times even in\nthe absence of aggregate macro-shocks. When common noises from the market are\ninvolved, financial friction amplifies the mean-field effect and leads to\ncapital fire sales by experts. In addition, the implicit interlink between and\nwithin heterogeneous groups demonstrates the slow economic recovery and\ncharacterizes the deviating and fear-of-missing-out (FOMO) behaviors of\nhouseholds compared to their counterparts. Our model also gives a fairly\nexplicit representation of the equilibrium solution without exploiting\ncomplicated numerical approaches.",
        "This study explores the application of Hawkes processes to model\nhigh-frequency data in the context of limit order books. Two distinct\nHawkes-based models are proposed and analyzed: one utilizing exponential\nkernels and the other employing power-law kernels. These models are implemented\nwithin a bivariate framework. The performance of each model is evaluated using\nhigh-frequency trading data, with a focus on their ability to reproduce key\nstatistical properties of limit order books. Through a comprehensive\ncomparison, we identify the strengths and limitations of each kernel type,\nproviding insights into their suitability for modeling high-frequency financial\ndata. Simulations are conducted to validate the models, and the results are\ninterpreted. Based on these insights, a trading strategy is formulated.",
        "This paper provides a dual formulation of the optimal consumption problem\nwith internal multiplicative habit formation. In this problem, the agent\nderives utility from the ratio of consumption to the internal habit component.\nDue to this multiplicative specification of the habit model, the optimal\nconsumption problem is not strictly concave and incorporates irremovable\npath-dependency. As a consequence, standard Lagrangian techniques fail to\nsupply a candidate for the corresponding dual formulation. Using Fenchel's\nDuality Theorem, we manage to identify a candidate formulation and prove that\nit satisfies strong duality. On the basis of this strong duality result, we are\nable to derive duality relations that stipulate how the optimal primal controls\ndepend on the optimal dual controls and vice versa. {Moreover, using the dual\nformulation, we develop an analytical evaluation mechanism to bound the\naccuracy of approximations to the optimal solutions.",
        "We introduce the two-factor Quintic Ornstein-Uhlenbeck model, where\nvolatility is modeled as a polynomial of degree five based on the sum of two\nOrnstein-Uhlenbeck processes driven by the same Brownian Motion, each\nmean-reverting at a different speed. We demonstrate that the Quintic model\neffectively captures the volatility surfaces of SPX and VIX while aligning with\nthe skew-stickiness ratio (SSR) across maturities ranging from a few days to\nover two years. Furthermore, the Quintic model shows consistency with key\nempirical stylized facts, notably reproducing the Zumbach effect.",
        "In this paper, we adopt the least squares Monte Carlo (LSMC) method to price\ntime-capped American options. The aforementioned cap can be an independent\nrandom variable or dependent on asset price at random time. We allow various\ntime caps. In particular, we give an algorithm for pricing the American options\ncapped by the first drawdown epoch. We focus on the geometric L\\'evy market. We\nprove that our estimator converges to the true price as one takes the\ndiscretisation step tending to zero and the number of trajectories going to\ninfinity.",
        "In the Black and Scholes model with proportional transaction costs, the\nLeland strategy allows to asymptotically super-replicate the European Call\noption as the number of revision dates converges to + infinity and the\ntransaction costs rate tends rapidly to 0. This method relies heavily on the\nexplicit expression of the delta-hedging strategy in the Black and Scholes\nmodel where the volatility is enlarged to compensate for the transaction costs.\nWe solve the same problem of super-hedging but for a general model with an\narbitrary fixed number of revision dates and arbitrary fixed transaction costs\nrates. Moreover, our approach does not need the existence of a risk-neutral\nprobability measure and is (almost) model free and easily implementable from\nreal data.",
        "We extend the signature-based primal and dual solutions to the optimal\nstopping problem recently introduced in [Bayer et al.: Primal and dual optimal\nstopping with signatures, to appear in Finance & Stochastics 2025], by\nintegrating deep-signature and signature-kernel learning methodologies. These\napproaches are designed for non-Markovian frameworks, in particular enabling\nthe pricing of American options under rough volatility. We demonstrate and\ncompare the performance within the popular rough Heston and rough Bergomi\nmodels.",
        "Decentralized lending protocols within the decentralized finance ecosystem\nenable the lending and borrowing of crypto-assets without relying on\ntraditional intermediaries. Interest rates in these protocols are set\nalgorithmically and fluctuate according to the supply and demand for liquidity.\nIn this study, we propose an agent-based model tailored to a decentralized\nlending protocol and determine the optimal interest rate model. When the\nresponses of the agents are linear with respect to the interest rate, the\noptimal solution is derived from a system of Riccati-type ODEs. For nonlinear\nbehaviors, we propose a Monte-Carlo estimator, coupled with deep learning\ntechniques, to approximate the optimal solution. Finally, after calibrating the\nmodel using block-by-block data, we conduct a risk-adjusted profit and loss\nanalysis of the liquidity pool under industry-standard interest rate models and\nbenchmark them against the optimal interest rate model.",
        "Low-light enhancement has wide applications in autonomous driving, 3D\nreconstruction, remote sensing, surveillance, and so on, which can\nsignificantly improve information utilization. However, most existing methods\nlack generalization and are limited to specific tasks such as image recovery.\nTo address these issues, we propose \\textbf{Gated-Mechanism Mixture-of-Experts\n(GM-MoE)}, the first framework to introduce a mixture-of-experts network for\nlow-light image enhancement. GM-MoE comprises a dynamic gated weight\nconditioning network and three sub-expert networks, each specializing in a\ndistinct enhancement task. Combining a self-designed gated mechanism that\ndynamically adjusts the weights of the sub-expert networks for different data\ndomains. Additionally, we integrate local and global feature fusion within\nsub-expert networks to enhance image quality by capturing multi-scale features.\nExperimental results demonstrate that the GM-MoE achieves superior\ngeneralization with respect to 25 compared approaches, reaching\nstate-of-the-art performance on PSNR on 5 benchmarks and SSIM on 4 benchmarks,\nrespectively.",
        "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs.",
        "Shipping container based modular architectures provide design flexibility in\ndata centers with building blocks to expand the network as and when needed. In\nthis paper, high capacity Modular Data Center (MDC) network architecture with\nRich Inter Connections named MODRIC is proposed. MODRIC is a cost-effective\nswitch-centric network design which allows building a flexible MDC network with\ncommodity switches. It uses an inter-container connectivity similar to the\nstructure of generalized hypercube in order to provide high inter-container\nbandwidth. Further, a hybrid Clos topology is used to build the container\nnetwork. MODRIC is highly suitable for cost effectively building mega data\ncenters requiring high throughput capacity and resilience against failures.\nThis paper presents the proposed architecture, discusses its relevant\nproperties, and proposes suitable addressing, routing and network construction\nschemes. The paper also presents comparative studies on its cost and\nperformance with existing network topologies.",
        "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
        "Large diffusion models demonstrate remarkable zero-shot capabilities in novel\nview synthesis from a single image. However, these models often face challenges\nin maintaining consistency across novel and reference views. A crucial factor\nleading to this issue is the limited utilization of contextual information from\nreference views. Specifically, when there is an overlap in the viewing frustum\nbetween two views, it is essential to ensure that the corresponding regions\nmaintain consistency in both geometry and appearance. This observation leads to\na simple yet effective approach, where we propose to use epipolar geometry to\nlocate and retrieve overlapping information from the input view. This\ninformation is then incorporated into the generation of target views,\neliminating the need for training or fine-tuning, as the process requires no\nlearnable parameters. Furthermore, to enhance the overall consistency of\ngenerated views, we extend the utilization of epipolar attention to a\nmulti-view setting, allowing retrieval of overlapping information from the\ninput view and other target views. Qualitative and quantitative experimental\nresults demonstrate the effectiveness of our method in significantly improving\nthe consistency of synthesized views without the need for any fine-tuning.\nMoreover, This enhancement also boosts the performance of downstream\napplications such as 3D reconstruction. The code is available at\nhttps:\/\/github.com\/botaoye\/ConsisSyn.",
        "V902 Mon is one of a few eclipsing Intermediate Polars (IPs), and show deep\neclipses in the optical lightcurves. The presence of a strong Fe K$\\alpha$\nfluorescence line in its X-ray spectrum and its low X-ray flux compared to\nother IPs suggests significant absorption, most likely from an accretion disk.\nIn an observation carried out using the Nuclear Spectroscopic Telescope Array\n(NuSTAR), we confirm the presence of an X-ray eclipse in the energy resolved\nlightcurves, coincident with the optical AAVSO\/CV-band lightcurves. Broadband\nX-ray spectral analysis using NuSTAR and XMM-Newton observations confirm a\nstrong absorption N$_{H}$ $\\sim 10^{23}$ cm$^{-2}$ local to the source, along\nwith a high equivalent width of about 0.7 keV for a Fe K$\\alpha$ fluorescence\nline. We interpret this using a model similar to an Accretion Disk Corona\nsource, which have a very high inclination and the compact object is heavily\nobscured by the body of the accretion disk. We propose that the primary X-rays\nfrom the accretion column in V902 Mon is hidden from our direct view at all\ntimes by the accretion disk. In this scenario, the observed scattered X-rays\nindicate substantial absorption of direct X-rays by the accretion disk.\nAdditionally, a strong Fe fluorescence line suggests reprocessing of the\nradiation by a more extended region, such as the pre-shock region, which could\nbe located a few white dwarf radii above the orbital plane.",
        "To veryfy \"hot supreconductivity\" recently proposed in lanthanum\nhydride-based compounds, we explored thermodynamically stable and\nsuperconducting phases in the lanthanum (La)-platinum (Pt)-hydrogen (H) ternary\nsystem at 20 GPa using an evolutionary construction scheme of a\nformation-enthalpy convex hull, universal neural network potential\ncalculations, and density functional theory calculations. Although we found no\nevidence of the hot superconductivity in this ternary system, we predicted a\nunique compound, LaPtH$_{ 6 }$, which has equilateral triangular H$_{ 3 }$\nunits nearly forming a two-dimensional kagome lattice between La and Pt layers\nand shows the superconductivity at 18.67 K. This structure is dynamically\nstable from ambient pressure to at least 200 GPa and the superconducting\ncritical temperature increases from 13.51 to 40.63 K.",
        "Quantitative estimation of human joint motion in daily living spaces is\nessential for early detection and rehabilitation tracking of\nneuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall\nrisks for older adults. Existing approaches involve monitoring devices such as\ncameras, wearables, and pressure mats, but have operational constraints such as\ndirect line-of-sight, carrying devices, and dense deployment. To overcome these\nlimitations, we leverage gait-induced floor vibration to estimate lower-limb\njoint motion (e.g., ankle, knee, and hip flexion angles), allowing\nnon-intrusive and contactless gait health monitoring in people's living spaces.\nTo overcome the high uncertainty in lower-limb movement given the limited\ninformation provided by the gait-induced floor vibrations, we formulate a\nphysics-informed graph to integrate domain knowledge of gait biomechanics and\nstructural dynamics into the model. Specifically, different types of nodes\nrepresent heterogeneous information from joint motions and floor vibrations;\nTheir connecting edges represent the physiological relationships between joints\nand forces governed by gait biomechanics, as well as the relationships between\nforces and floor responses governed by the structural dynamics. As a result,\nour model poses physical constraints to reduce uncertainty while allowing\ninformation sharing between the body and the floor to make more accurate\npredictions. We evaluate our approach with 20 participants through a real-world\nwalking experiment. We achieved an average of 3.7 degrees of mean absolute\nerror in estimating 12 joint flexion angles (38% error reduction from\nbaseline), which is comparable to the performance of cameras and wearables in\ncurrent medical practices.",
        "Polar Ring Galaxies (PRGs) are a unique class of galaxies characterised by a\nring of gas and stars orbiting nearly orthogonal to the main body. This study\ndelves into the evolutionary trajectory of PRGs using the exemplary trio of NGC\n3718, NGC 2685, and NGC 4262. We investigate the distinct features of PRGs by\nanalysing their ring and host components to reveal their unique characteristics\nthrough Spectral Energy Distribution (SED) fitting. Using CIGALE, we performed\nSED fitting to independently analyse the ring and host spatially resolved\nregions, marking the first decomposed SED analysis for PRGs, which examines\nstellar populations using high-resolution observations from AstroSat UVIT at a\nresolved scale. The UV-optical surface profiles provide an initial idea that\ndistinct patterns in the galaxies, with differences in FUV and NUV, suggest\nthree distinct stages of ring evolution in the selected galaxies. The study of\nresolved-scale stellar regions reveals that the ring regions are generally\nyounger than their host galaxies, with the age disparity progressively\ndecreasing along the evolutionary sequence from NGC 3718 to NGC 4262. Star\nformation rates (SFR) also exhibit a consistent pattern, with higher SFR in the\nring of NGC 3718 compared to the others, and a progressive decrease through NGC\n2685 and NGC 4262. Finally, the representation of the galaxies in the HI gas\nfraction versus the NUV- r plane supports the idea that they are in three\ndifferent evolutionary stages of PRG evolution, with NGC 3718 in the initial\nstage, NGC 2685 in the intermediate stage, and NGC 4262 representing the final\nstage. NGC 3718, NGC 2685, and NGC 4262 represent different stages of this\nevolution, highlighting the dynamic nature of PRGs and emphasising the\nimportance of studying their evolutionary processes to gain insights into\ngalactic formation and evolution.",
        "Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com\/LeonardoBerti00\/DeepMarket.",
        "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
        "Machine learning, particularly convolutional neural networks (CNNs), has\nshown promise in medical image analysis, especially for thoracic disease\ndetection using chest X-ray images. In this study, we evaluate various CNN\narchitectures, including binary classification, multi-label classification, and\nResNet50 models, to address challenges like dataset imbalance, variations in\nimage quality, and hidden biases. We introduce advanced preprocessing\ntechniques such as principal component analysis (PCA) for image compression and\npropose a novel class-weighted loss function to mitigate imbalance issues. Our\nresults highlight the potential of CNNs in medical imaging but emphasize that\nissues like unbalanced datasets and variations in image acquisition methods\nmust be addressed for optimal model performance.",
        "Cellular automata are computers, similar to Turing machines. The main\ndifference is that Turing machines use a one-dimensional tape, whereas cellular\nautomata use a two-dimensional grid. The best-known cellular automaton is the\nGame of Life, which is a universal computer. It belongs to a family of cellular\nautomata with 262,144 members. Playing the Game of Life generally involves\nengineering; that is, assembling a device composed of various parts that are\ncombined to achieve a specific intended result. Instead of engineering cellular\nautomata, we propose evolving cellular automata. Evolution applies mutation and\nselection to a population of organisms. If a mutation increases the fitness of\nan organism, it may have many descendants, displacing the less fit organisms.\nUnlike engineering, evolution does not work towards an imagined goal. Evolution\nworks towards increasing fitness, with no expectations about the specific form\nof the final result. Mutation, selection, and fitness yield structures that\nappear to be more organic and life-like than engineered structures. In our\nexperiments, the patterns resulting from evolving cellular automata look much\nlike the spots on leopards and the stripes on tigers.",
        "The solar system planetary architecture has been proposed to be consistent\nwith the terrestrial and giant planets forming from material rings at ~1 au and\n~5 au, respectively. Here, we show that super-Earths and mini-Neptunes may\nshare a similar formation pathway. In our simulations conducted with a disk\nalpha-viscosity of 4e-3, super-Earths accrete from rings of rocky material in\nthe inner disk, growing predominantly via planetesimal accretion. Mini-Neptunes\nprimarily originate from rings located beyond the water snowline, forming via\npebble accretion. Our simulations broadly match the period-ratio distribution,\nthe intra-system size uniformity, and the planet multiplicity distribution of\nexoplanets. The radius valley constrains the typical total mass available for\nrocky planet formation to be less than 3-6 Earth masses. Our results predict\nthat planets at ~1 au in systems with close-in super-Earths and mini-Neptunes\nare predominantly water-rich. Though relatively uncommon, at ~1% level, such\nsystems might also host rocky Earth-sized planets in the habitable zone that\nunderwent late giant impacts, akin to the Moon-forming event.",
        "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps:\/\/github.com\/WuJH2001\/swift4d."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem",
    "start_abstract":"Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
      ],
      "abstract":[
        "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Etude du graphe divisoriel 6",
        "Singularities in Bayesian Inference: Crucial or Overstated?",
        "Comparative Analysis of Two-Stage Distributionally Robust Optimization\n  over 1-Wasserstein and 2-Wasserstein Balls",
        "Intrinsic Donaldson-Thomas theory. II. Stability measures and invariants",
        "Joint modeling of longitudinal HRQoL data accounting for the risk of\n  competing dropouts",
        "Disturbance-to-state stabilization by output feedback of nonlinear ODE\n  cascaded with a reaction-diffusion equation",
        "Circular sorting",
        "Global well-posedness and stability of three-dimensional isothermal\n  Euler equations with damping",
        "Direct sampling from conditional distributions by sequential maximum\n  likelihood estimations",
        "Decay of mass for a semilinear heat equation with mixed local-nonlocal\n  operators",
        "Isoparametric foliations and bounded geometry",
        "Operator $\\ell^\\infty \\to \\ell^\\infty$ norm of products of random\n  matrices",
        "Major-minor mean field games: common noise helps",
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation"
      ],
      "abstract":[
        "The divisor graph is the non oriented graph whose vertices are the positive\nintegers, and edges are the {a,b} such that a divides b or b divides a. Let\nF(x,y) be the maximum number of integers<= x belonging in one of y pairwise\ndisjoint simple path of the restriction of the divisor graph to integers <= x.\nOur main result is the following. There exist two real numbers K >c>0 such that\nfor every x and y with x>=2y>=2 , we have cx \/ log(x\/y) <= F(x,y) <= Kx \/\nlog(x\/y). It answers a question of Erd\\\"os.",
        "Over the past two decades, shrinkage priors have become increasingly popular,\nand many proposals can be found in the literature. These priors aim to shrink\nsmall effects to zero while maintaining true large effects. Horseshoe-type\npriors have been particularly successful in various applications, mainly due to\ntheir computational advantages. However, there is no clear guidance on choosing\nthe most appropriate prior for a specific setting. In this work, we propose a\nframework that encompasses a large class of shrinkage distributions, including\npriors with and without a singularity at zero. By reframing such priors in the\ncontext of reliability theory and wealth distributions, we provide insights\ninto the prior parameters and shrinkage properties. The paper's key\ncontributions are based on studying the folded version of such distributions,\nwhich we refer to as the Gambel distribution. The Gambel can be rewritten as\nthe ratio between a Generalised Gamma and a Generalised Beta of the second\nkind. This representation allows us to gain insights into the behaviours near\nthe origin and along the tails, compute measures to compare their\ndistributional properties, derive consistency results, devise MCMC schemes for\nposterior inference and ultimately provide guidance on the choice of the\nhyperparameters.",
        "This paper investigates advantages of using 2-Wasserstein ambiguity sets over\n1-Wasserstein sets in two-stage distributionally robust optimization with\nright-hand side uncertainty. We examine the worst-case distributions within 1-\nand 2-Wasserstein balls under both unrestricted and nonnegative orthant\nsupports, highlighting a pathological behavior arising in 1-Wasserstein balls.\nClosed-form solutions for a single-scenario newsvendor problem illustrate that\n2-Wasserstein balls enable more informed decisions. Additionally, a\npenalty-based dual interpretation suggests that 2-Wasserstein balls may\noutperform 1-Wasserstein balls across a broader range of Wasserstein radii,\neven with general support sets.",
        "This is the second paper in a series on intrinsic Donaldson-Thomas theory, a\nframework for studying the enumerative geometry of general algebraic stacks.\n  In this paper, we present the construction of Donaldson-Thomas invariants for\ngeneral $(-1)$-shifted symplectic derived Artin stacks, generalizing the\nconstructions of Joyce-Song and Kontsevich-Soibelman for moduli stacks of\nobjects in $3$-Calabi-Yau abelian categories. Our invariants are defined using\nrings of motives, and depend intrinsically on the stack, together with a set of\ncombinatorial data similar to a stability condition, called a stability measure\non the component lattice of the stack. For our invariants to be well-defined,\nwe prove a generalization of Joyce's no-pole theorem to general stacks, using a\nsimpler and more conceptual argument than the original proof in the abelian\ncategory case.\n  Further properties and applications of these invariants, such as\nwall-crossing formulae, will be discussed in a forthcoming paper.",
        "In cancer clinical trials, health-related quality of life (HRQoL) is an\nimportant endpoint, providing information about patients' well-being and daily\nfunctioning. However, missing data due to premature dropout can lead to biased\nestimates, especially when dropouts are informative. This paper introduces the\nextJMIRT approach, a novel tool that efficiently analyzes multiple longitudinal\nordinal categorical data while addressing informative dropout. Within a joint\nmodeling framework, this approach connects a latent variable, derived from\nHRQoL data, to cause-specific hazards of dropout. Unlike traditional joint\nmodels, which treat longitudinal data as a covariate in the survival submodel,\nour approach prioritizes the longitudinal data and incorporates the log\nbaseline dropout risks as covariates in the latent process. This leads to a\nmore accurate analysis of longitudinal data, accounting for potential effects\nof dropout risks. Through extensive simulation studies, we demonstrate that\nextJMIRT provides robust and unbiased parameter estimates and highlight the\nimportance of accounting for informative dropout. We also apply this\nmethodology to HRQoL data from patients with progressive glioblastoma,\nshowcasing its practical utility.",
        "In this paper, we analyze the output stabilization problem for cascaded\nnonlinear ODE with $1-d$ heat diffusion equation affected by both in-domain and\nboundary perturbations. We assume that the only available part of states is the\nfirst components of the ODE-subsystem and one boundary of the heat-subsystem.\nThe particularity of this system is two folds i) it contains a nonlinear\nadditive term in the ODE-subsystem, and ii) it is affected by both boundary and\nin-domain perturbations signals.\n  For such a system, and unlike the existing works, we succeeded to design an\noutput observer-based feedback that guarantees not only asymptotic\nstabilization result but also a globally {\\it disturbance-to-state\nstabilization} for our cascaded system. The output feedback is designed using\nan adequate backstepping transformation recently introduced for coupled\nODE-heat equations combined with high-gain observer and high-gain controller.",
        "We determine the maximal number of steps required to sort $n$ labeled points\non a circle by adjacent swaps. Lower bounds for sorting by all swaps, not\nnecessarily adjacent, are given as well.",
        "The global well-posedness and stability of solutions to the three-dimensional\ncompressible Euler equations with damping is a longstanding open problem. This\nproblem was addressed in \\cite{WY, STW} in the isentropic regime (i.e.\n$\\gamma>1$) for small smooth solutions. In this paper, we prove the global\nwell-posedness and stability of smooth solutions to the three-dimensional\nisothermal Euler equations ($\\gamma=1$) with damping for some partially large\ninitial values, i.e., $\\|(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ could be large, but\n$\\|D^3(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ is necessarily small. Moreover, the optimal\nalgebraic decay rate is also obtained.\n  The proof is based on the observation that the isothermal Euler equations\nwith damping possess a good structure so that the equations can be reduced into\na symmetrically hyperbolic system with partial damping, i.e., \\eqref{au}. In\nthe new system, all desired a priori estimates can be obtained under the\nassumption that $\\int_0^T(\\|\\nabla \\mathrm{ln}\\rho\\|_{L^{\\infty}}+\\|\\nabla\nu\\|_{L^{\\infty}}) \\mathrm{d}t $ is small. The assumption can be verified\nthrough the low-high frequency analysis via Fourier transformation under the\ncondition that $\\|D^3(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ is small, but\n$\\|(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ could be large.",
        "We can directly sample from the conditional distribution of any log-affine\nmodel. The algorithm is a Markov chain on a bounded integer lattice, and its\ntransition probability is the ratio of the UMVUE (uniformly minimum variance\nunbiased estimator) of the expected counts to the total number of counts. The\ncomputation of the UMVUE accounts for most of the computational cost, which\nmakes the implementation challenging. Here, we investigated an approximate\nalgorithm that replaces the UMVUE with the MLE (maximum likelihood estimator).\nAlthough it is generally not exact, it is efficient and easy to implement; no\nprior study is required, such as about the connection matrices of the holonomic\nideal in the original algorithm.",
        "In this paper, we are concerned with the Cauchy problem for the\nreaction-diffusion equation $\\partial_t u+t^\\beta\\mathcal{L} u= - h(t)u^p$\nposed on $\\mathbb{R}^N$, driven by the mixed local-nonlocal operator\n$\\mathcal{L}=-\\Delta+(-\\Delta)^{\\alpha\/2}$, $\\alpha\\in(0,2)$, and supplemented\nwith a nonnegative integrable initial data, where $p>1$, $\\beta\\geq 0$, and\n$h:(0,\\infty)\\to(0,\\infty)$ is a locally integrable function. We study the\nlarge time behavior of non-negative solutions and show that the nonlinear term\ndetermines the large time asymptotic for $p\\leq 1+{\\alpha}\/{N(\\beta+1)},$ while\nthe classical\/anomalous diffusion effects win if $p>1+{\\alpha}\/{N(\\beta+1)}$.",
        "We prove that there are only finitely many isoparametrically foliated closed\nconnected Riemannian manifolds with bounded geometry, fixed dimension $n\\neq5$,\nand finite fundamental group, up to foliated diffeomorphism. In addition, we\nconstruct various infinite families of isoparametric foliations that are\nmutually not foliated diffeomorphic, for instance on a fixed sphere.",
        "We study the $\\ell^\\infty \\to \\ell^\\infty$ operator norm of products of\nindependent random matrices with independent and identically distributed\nentries. For $n$-by-$n$ matrices whose entries are centered, have unit\nvariance, and have a finite moment of order $4\\alpha$ for some $\\alpha > 1$, we\nfind that the operator norm of the product of $p$ matrices behaves\nasymptotically like $n^{\\frac {p+1}{2}}\\sqrt{2\/\\pi}$. The case of products of\npossibly non-square matrices with possibly non-centered entries is also\ncovered.",
        "The objective of this work is to study the existence, uniqueness, and\nstability of equilibria in mean field games involving a major player and a\ncontinuum of minor players over finite intervals of arbitrary length. Following\nearlier articles addressing similar questions in the context of classical mean\nfield games, the cost functions for the minor players are assumed to satisfy\nthe Lasry-Lions monotonicity condition. In this contribution, we demonstrate\nthat if, in addition to the monotonicity condition, the intensity of the\n(Brownian) noise driving the major player is sufficiently high, then -- under\nfurther mild regularity assumptions on the coefficients -- existence,\nuniqueness, and stability of equilibria are guaranteed. A key challenge is to\nshow that the threshold (beyond which the noise intensity must be taken) can be\nchosen independently of the length of the time interval over which the game is\ndefined. Building on the stability properties thus established, we further show\nthat the associated system of master equations admits a unique classical\nsolution. To the best of our knowledge, this is the first result of its kind\nfor major-minor mean field games defined over intervals of arbitrary length.",
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b22",
    "start_title":"Vertical slice modelling of nonlinear Eady waves using a compatible finite element method",
    "start_abstract":"A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
      ],
      "abstract":[
        "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Optimal Criteria for Best Subset Selection",
        "Approximate Dynamic Programming for a Remanufacture-to-Order System",
        "Sparse control in microscopic and mean-field leader-follower models",
        "Stabilization and Optimal Control of an Interconnected $n + m$\n  Hetero-directional Hyperbolic PDE-SDE System",
        "Upper and Lower Bounds for a Class of Constrained Linear Time-Varying\n  Games",
        "Stabilization of a Chain of Three Hyperbolic PDEs using a Time-Delay\n  Representation",
        "A Survey of Exact and Approximation Algorithms for Linear-Parametric\n  Optimization Problems",
        "Error Bounds for a Class of Cone-Convex Inclusion Problems",
        "On extending the class of convex functions",
        "A relaxed proximal point algorithm with double-inertial effects for\n  nonconvex equilibrium problems",
        "Assessment various control methods a digital copy of enterprise by\n  integral indicator",
        "Some commutation principles for optimization problems over\n  transformation groups and semi-FTvN systems",
        "On the resolution and linear optimization problems subject to a system\n  of bipolar fuzzy relational equalities defined with continuous Archimedean\n  t-norms",
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation"
      ],
      "abstract":[
        "This paper introduces two novel criteria: one for feature selection and\nanother for feature elimination in the context of best subset selection, which\nis a benchmark problem in statistics and machine learning. From the perspective\nof optimization, we revisit the classical selection and elimination criteria in\ntraditional best subset selection algorithms, revealing that these classical\ncriteria capture only partial variations of the objective function after the\nentry or exit of features. By formulating and solving optimization subproblems\nfor feature entry and exit exactly, new selection and elimination criteria are\nproposed, proved as the optimal decisions for the current entry-and-exit\nprocess compared to classical criteria. Replacing the classical selection and\nelimination criteria with the proposed ones generates a series of enhanced best\nsubset selection algorithms. These generated algorithms not only preserve the\ntheoretical properties of the original algorithms but also achieve significant\nmeta-gains without increasing computational cost across various scenarios and\nevaluation metrics on multiple tasks such as compressed sensing and sparse\nregression.",
        "Remanufacturing is pivotal in transitioning to more sustainable economies.\nWhile industry evidence highlights its vast market potential and economic and\nenvironmental benefits, remanufacturing remains underexplored in theoretical\nresearch. This study revisits a state-of-the-art remanufacture-to-order (RtO)\nsystem and develops an alternative approach by developing an approximate\ndynamic programming (ADP) algorithm to solve larger RtO instances. The proposed\nmethodology yields results consistent with existing state-dependent,\nnon-congestive policies. Key findings include the optimality of fulfilling\ndemand whenever remanufacturable cores are available and prioritizing cores of\nthe highest quality level. This work highlights the potential of ADP in\naddressing problems in remanufacturing domains.",
        "This work investigates the decay properties of Lyapunov functions in\nleader-follower systems seen as a sparse control framework. Starting with a\nmicroscopic representation, we establish conditions under which the total\nLyapunov function, encompassing both leaders and followers, exhibits\nexponential decay. The analysis is extended to a hybrid setting combining a\nmean-field description for followers and a microscopic model for leaders. We\nidentify sufficient conditions on control gain and interaction strengths that\nguarantee stabilization of the linear system towards a target state. The\nresults highlight the influence of sparse control and interaction parameters in\nachieving coordinated behavior in multi-agent systems.",
        "In this paper, we design a controller for an interconnected system composed\nof a linear Stochastic Differential Equation (SDE) controlled through a linear\nhetero-directional hyperbolic Partial Differential Equation (PDE). Our\nobjective is to steer the coupled system to a desired final state on average,\nwhile keeping the variance-in-time as small as possible, improving robustness\nto disturbances. By employing backstepping techniques, we decouple the original\nPDE, reformulating the system as an input delayed SDE with a stochastic drift.\nWe first establish a controllability result, shading light on lower bounds for\nthe variance. This shows that the system can never improve variance below\nstrict structural limits. Under standard controllability conditions, we then\ndesign a controller that drives the mean of the states while keeping the\nvariance bounded. Finally, we analyze the optimal control problem of variance\nminimization along the entire trajectory. Under additional controllability\nassumptions, we prove that the optimal control can achieve any variance level\nabove the fundamental structural limit.",
        "This paper develops an algorithm for upper- and lower-bounding the value\nfunction for a class of linear time-varying games subject to convex control\nsets. In particular, a two-player zero-sum differential game is considered\nwhere the respective players aim to minimise and maximise a convex terminal\nstate cost. A collection of solutions of a single-player dynamical system\nsubject to a trimmed control set is used to characterise a viscosity\nsupersolution of a Hamilton-Jacobi (HJ) equation, which in turn yields an upper\nbound for the value function. Analogously, a collection of hyperplanes is used\nto characterise a viscosity subsolution of the HJ equation, which yields a\nlower bound. The computational complexity and memory requirement of the\nproposed algorithm scales with the number of solutions and hyperplanes that\ncharacterise the bounds, which is not explicitly tied to the number of system\nstates. Thus, the algorithm is tractable for systems of moderately high\ndimension whilst preserving rigorous guarantees for optimal control and\ndifferential game applications.",
        "This paper addresses the stabilization of a chain system consisting of three\nhyperbolic Partial Differential Equations (PDEs). The system is reformulated\ninto a pure transport system of equations via an invertible backstepping\ntransformation. Using the method of characteristics and exploiting the inherent\ncascade structure of the chain, the stabilization problem is reduced to that of\nan associated Integral Difference Equation (IDE). A dynamic controller is\ndesigned for the IDE, whose gains are computed by solving a system of\nFredholm-type integral equations. This approach provides a systematic framework\nfor achieving exponential stabilization of the chain of hyperbolic PDEs.",
        "Linear-parametric optimization, where multiple objectives are combined into a\nsingle objective using linear combinations with parameters as coefficients, has\nnumerous links to other fields in optimization and a wide range of application\nareas. In this survey, we provide a comprehensive overview of structural\nresults and algorithmic strategies for solving linear-parametric optimization\nproblems exactly and approximately. Transferring concepts from related areas\nsuch as multi-objective optimization provides further relevant results. The\nsurvey consists of two parts: First, we list strategies that work in a general\nfashion and do not rely on specific problem structures. Second, we look at\nwell-studied parametric optimization problems and cover both important\ntheoretical results and specialized algorithmic approaches for these problems.\nAmong these problems are parametric variants of shortest path problems, minimum\ncost flow and maximum flow problems, spanning tree problems, the knapsack\nproblem, and matching problems. Overall, we cover the results from 128\npublications (and refer to 33 supplemental works) published between 1963 and\n2024.",
        "In this paper, we investigate error bounds for cone-convex inclusion problems\nin finite-dimensional settings of the form $f(x)\\in K$, where $K$ is a smooth\ncone and $f$ is a continuously differentiable and $K$-concave function. We show\nthat local error bounds for the inclusion can be characterized by the Abadie\nconstraint qualification around the reference point. In the case where $f$ is\nan affine function, we precisely identify the conditions under which the\ninclusion admits global error bounds. Additionally, we derive some properties\nof smooth cones, as well as regular cones and strictly convex cones.",
        "In this brief note, it is shown that the function p^TW log(p) is convex in p\nif W is a diagonally dominant positive definite M-matrix. The techniques used\nto prove convexity are well-known in linear algebra and essentially involves\nfactoring the Hessian in a way that is amenable to martix analysis. Using\nsimilar techniques, two classes of convex homogeneous polynomials is derived -\nnamely, p^TW p2 and (p^k)^TW p^k - the latter also happen to be SOS-convex.\nLastly, usign the same techniques, it is also shown that the function p^TW ep\nis convex over the positive reals only if W is a non-negative diagonal matrix.\nDiscussions regarding the utility of these functions and examples accompany the\nresults presented.",
        "In this paper, we present a relaxation proximal point method with double\ninertial effects to approximate a solution of a non-convex equilibrium problem.\nWe give global\n  convergence results of the iterative sequence generated by our algorithm.\nSome known results are recovered\n  as special cases of our results. Numerical test is given to support the\ntheoretical findings.",
        "The difficulty of assessing the state lies in a little predictable change in\nthe dimension of a dynamic system under the influence of internal changes and\nenvironmental parameters. In the work, the state of such a system is estimated\nby the method of integral indicators. The application of the method of integral\nindicators allowed us to evaluate the activity of an enterprise. In the present\nwork, the method of integrated indicators is used to assess the control of a\ndigital copy (enterprise).",
        "We introduce the concepts of commutativity relative to a transformation group\nand strong commutativity in the setting of a semi-FTvN system and show their\nappearance as optimality conditions in certain optimization problems. In the\nsetting of a semi-FTvN system (in particular, in an FTvN system), we show that\nstrong commutativity implies commutativity and observe that in the special case\nof Euclidean Jordan algebra, commutativity and strong commutativity concepts\nreduce, respectively, to those of operator and strong operator commutativity.\nWe demonstrate that every complete hyperbolic polynomial induces a semi-FTvN\nsystem. By way of an application, we describe several commutation principles.",
        "This paper considers the linear objective function optimization with respect\nto a more general class of bipolar fuzzy relational equations, where the fuzzy\ncompositions are defined by an arbitrary continuous Archimedean t-norm. In\naddition, a faster method for finding a global optimum is proposed that, unlike\nthe previous work, does not require obtaining all local optimal solutions and\nclassifying the constraints. Analytical concepts and properties of the\nArchimedean bipolar fuzzy equations are investigated and two necessary\nconditions are presented to conceptualize the feasibility of the problem. It is\nshown that the feasible solution set can be resulted by a union of the finite\nnumber of compact sets, where each compact set is obtained by a function.\nMoreover, to accelerate identification of the mentioned compact sets (and\ntherefore, to speed up solution finding), four simplification techniques are\npresented, which are based on either omitting redundant constraints and\/or\neliminating unknowns by assigning them a fixed value. Also, three additional\nsimplification techniques are given to reduce the search domain by removing\nsome parts of the feasible region that do not contain optimal solutions.\nSubsequently, a method is proposed to find an optimal solution for the current\nlinear optimization problems. The proposed method consists of two accelerative\nstrategies that are used during the problem solving process. By the first\nstrategy, the method neglects some candidate solutions that are not optimal, by\nconsidering only a subset of admissible functions. As for the second strategy,\na branch-and-bound method is used to delete non-optimal branches. Then, the\nmethod is summarized in an algorithm that represents all essential steps of the\nsolution and finally, the whole method is applied in an example that has been\nchosen in such a way that the various situations are illustrated.",
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association",
    "start_abstract":"Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Voxel Scene Graph for Intracranial Hemorrhage"
      ],
      "abstract":[
        "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Transformer Dynamics: A neuroscientific approach to interpretability of\n  large language models",
        "Actual Causation and Nondeterministic Causal Models",
        "ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural\n  Faithfulness in SpArX",
        "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "On Sequential Fault-Intolerant Process Planning",
        "Neuro-Symbolic AI in 2024: A Systematic Review",
        "On the Complexity of Global Necessary Reasons to Explain Classification",
        "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning",
        "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform",
        "On Scaling Neurosymbolic Programming through Guided Logical Inference",
        "MMCR: Advancing Visual Language Model in Multimodal Multi-Turn\n  Contextual Reasoning",
        "From Screens to Scenes: A Survey of Embodied AI in Healthcare",
        "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
        "Mobile Application Threats and Security",
        "Water Flow Detection Device Based on Sound Data Analysis and Machine\n  Learning to Detect Water Leakage",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "ReasonGraph: Visualisation of Reasoning Paths",
        "Foundations of block-parallel automata networks",
        "Enhancing Quantum-ready QUBO-based Suppression for Object Detection with\n  Appearance and Confidence Features",
        "Uncertainty-Aware Decoding with Minimum Bayes Risk",
        "StructVPR++: Distill Structural and Semantic Knowledge with Weighting\n  Samples for Visual Place Recognition",
        "Infinitely many solutions for elliptic system with Hamiltonian type",
        "Stochastic resonance in Schmitt trigger and its application towards weak\n  signal detection",
        "mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body\n  Reconstruction",
        "Dubrovin duality and mirror symmetry for ADE resolutions",
        "Geometric deformations of cuspidal $S_1$ singularities"
      ],
      "abstract":[
        "As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na \"neuroscience of AI\" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.",
        "In (Beckers, 2025) I introduced nondeterministic causal models as a\ngeneralization of Pearl's standard deterministic causal models. I here take\nadvantage of the increased expressivity offered by these models to offer a\nnovel definition of actual causation (that also applies to deterministic\nmodels). Instead of motivating the definition by way of (often subjective)\nintuitions about examples, I proceed by developing it based entirely on the\nunique function that it can fulfil in communicating and learning a causal\nmodel. First I generalize the more basic notion of counterfactual dependence,\nsecond I show how this notion has a vital role to play in the logic of causal\ndiscovery, third I introduce the notion of a structural simplification of a\ncausal model, and lastly I bring both notions together in my definition of\nactual causation. Although novel, the resulting definition arrives at verdicts\nthat are almost identical to those of my previous definition (Beckers, 2021,\n2022).",
        "In the field of Explainable Artificial Intelligence (XAI), argumentative XAI\napproaches have been proposed to represent the internal reasoning process of\ndeep neural networks in a more transparent way by interpreting hidden nodes as\narguements. However, as the number of layers increases, existing compression\nmethods simplify all layers at once, which lead to high accumulative\ninformation loss. To compensate for this, we propose an iterative\nlayer-by-layer compression technique in which each layer is compressed\nseparately and the reduction error in the next layer is immediately compensated\nfor, thereby improving the overall input-output and structural fidelity of the\nmodel. Experiments on the Breast Cancer Diagnosis dataset show that, compared\nto traditional compression, the method reduces input-output and structural\nunfaithfulness, and maintains a more consistent attack-support relationship in\nthe Argumentative Explanation scheme. This is significant because it provides a\nnew way to make complex MLP models more compact while still conveying their\ninternal inference logic without distortion.",
        "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "We propose and study a planning problem we call Sequential Fault-Intolerant\nProcess Planning (SFIPP). SFIPP captures a reward structure common in many\nsequential multi-stage decision problems where the planning is deemed\nsuccessful only if all stages succeed. Such reward structures are different\nfrom classic additive reward structures and arise in important applications\nsuch as drug\/material discovery, security, and quality-critical product design.\nWe design provably tight online algorithms for settings in which we need to\npick between different actions with unknown success chances at each stage. We\ndo so both for the foundational case in which the behavior of actions is\ndeterministic, and the case of probabilistic action outcomes, where we\neffectively balance exploration for learning and exploitation for planning\nthrough the usage of multi-armed bandit algorithms. In our empirical\nevaluations, we demonstrate that the specialized algorithms we develop, which\nleverage additional information about the structure of the SFIPP instance,\noutperform our more general algorithm.",
        "Background: The field of Artificial Intelligence has undergone cyclical\nperiods of growth and decline, known as AI summers and winters. Currently, we\nare in the third AI summer, characterized by significant advancements and\ncommercialization, particularly in the integration of Symbolic AI and\nSub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n  Methods: The review followed the PRISMA methodology, utilizing databases such\nas IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\ncriteria targeted peer-reviewed papers published between 2020 and 2024. Papers\nwere screened for relevance to Neuro-Symbolic AI, with further inclusion based\non the availability of associated codebases to ensure reproducibility.\n  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\nand were analyzed in detail. The majority of research efforts are concentrated\nin the areas of learning and inference (63%), logic and reasoning (35%), and\nknowledge representation (44%). Explainability and trustworthiness are less\nrepresented (28%), with Meta-Cognition being the least explored area (5%). The\nreview identifies significant interdisciplinary opportunities, particularly in\nintegrating explainability and trustworthiness with other research areas.\n  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\nconcentrated efforts in learning and inference. Significant gaps remain in\nexplainability, trustworthiness, and Meta-Cognition. Addressing these gaps\nthrough interdisciplinary research will be crucial for advancing the field\ntowards more intelligent, reliable, and context-aware AI systems.",
        "Explainable AI has garnered considerable attention in recent years, as\nunderstanding the reasons behind decisions or predictions made by AI systems is\ncrucial for their successful adoption. Explaining classifiers' behavior is one\nprominent problem. Work in this area has proposed notions of both local and\nglobal explanations, where the former are concerned with explaining a\nclassifier's behavior for a specific instance, while the latter are concerned\nwith explaining the overall classifier's behavior regardless of any specific\ninstance. In this paper, we focus on global explanations, and explain\nclassification in terms of ``minimal'' necessary conditions for the classifier\nto assign a specific class to a generic instance. We carry out a thorough\ncomplexity analysis of the problem for natural minimality criteria and\nimportant families of classifiers considered in the literature.",
        "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
        "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
        "Probabilistic neurosymbolic learning seeks to integrate neural networks with\nsymbolic programming. Many state-of-the-art systems rely on a reduction to the\nProbabilistic Weighted Model Counting Problem (PWMC), which requires computing\na Boolean formula called the logical provenance.However, PWMC is \\\\#P-hard, and\nthe number of clauses in the logical provenance formula can grow exponentially,\ncreating a major bottleneck that significantly limits the applicability of PNL\nsolutions in practice.We propose a new approach centered around an exact\nalgorithm DPNL, that enables bypassing the computation of the logical\nprovenance.The DPNL approach relies on the principles of an oracle and a\nrecursive DPLL-like decomposition in order to guide and speed up logical\ninference.Furthermore, we show that this approach can be adapted for\napproximate reasoning with $\\epsilon$ or $(\\epsilon, \\delta)$ guarantees,\ncalled ApproxDPNL.Experiments show significant performance gains.DPNL enables\nscaling exact inference further, resulting in more accurate models.Further,\nApproxDPNL shows potential for advancing the scalability of neurosymbolic\nprogramming by incorporating approximations even further, while simultaneously\nensuring guarantees for the reasoning process.",
        "Compared to single-turn dialogue, multi-turn dialogue involving multiple\nimages better aligns with the needs of real-world human-AI interactions.\nAdditionally, as training data, it provides richer contextual reasoning\ninformation, thereby guiding the model to achieve better performance. However,\nexisting vision-language models (VLMs) primarily rely on single-turn dialogue\ntraining and evaluation benchmarks. In this paper, following the\ncharacteristics of human dialogue, such as focused topics and concise, clear\ncontent, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel\ndataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn\ninstruction tuning dataset with 310K contextual dialogues, each covering 1-4\nimages and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark\nfeaturing dialogues, spanning 8 domains (Humanities, Natural, Science,\nEducation, etc.) and 40 sub-topics. Extensive evaluations demonstrate that\nmodels fine-tuned with MMCR-310k achieve 5.2\\% higher contextual accuracy on\nMMCR-Bench, while showing consistent improvements on existing benchmarks\n(+1.1\\% on AI2D, +1.2\\% on MMMU and MMVet). MMCR and prompt engineering will be\nreleased publicly.",
        "Healthcare systems worldwide face persistent challenges in efficiency,\naccessibility, and personalization. Powered by modern AI technologies such as\nmultimodal large language models and world models, Embodied AI (EmAI)\nrepresents a transformative frontier, offering enhanced autonomy and the\nability to interact with the physical world to address these challenges. As an\ninterdisciplinary and rapidly evolving research domain, \"EmAI in healthcare\"\nspans diverse fields such as algorithms, robotics, and biomedicine. This\ncomplexity underscores the importance of timely reviews and analyses to track\nadvancements, address challenges, and foster cross-disciplinary collaboration.\nIn this paper, we provide a comprehensive overview of the \"brain\" of EmAI for\nhealthcare, wherein we introduce foundational AI algorithms for perception,\nactuation, planning, and memory, and focus on presenting the healthcare\napplications spanning clinical interventions, daily care & companionship,\ninfrastructure support, and biomedical research. Despite its promise, the\ndevelopment of EmAI for healthcare is hindered by critical challenges such as\nsafety concerns, gaps between simulation platforms and real-world applications,\nthe absence of standardized benchmarks, and uneven progress across\ninterdisciplinary domains. We discuss the technical barriers and explore\nethical considerations, offering a forward-looking perspective on the future of\nEmAI in healthcare. A hierarchical framework of intelligent levels for EmAI\nsystems is also introduced to guide further development. By providing\nsystematic insights, this work aims to inspire innovation and practical\napplications, paving the way for a new era of intelligent, patient-centered\nhealthcare.",
        "Large language models (LLMs) achieve impressive performance by scaling model\nparameters, but this comes with significant inference overhead. Feed-forward\nnetworks (FFNs), which dominate LLM parameters, exhibit high activation\nsparsity in hidden neurons. To exploit this, researchers have proposed using a\nmixture-of-experts (MoE) architecture, where only a subset of parameters is\nactivated. However, existing approaches often require extensive training data\nand resources, limiting their practicality. We propose CMoE (Carved MoE), a\nnovel framework to efficiently carve MoE models from dense models. CMoE\nachieves remarkable performance through efficient expert grouping and\nlightweight adaptation. First, neurons are grouped into shared and routed\nexperts based on activation rates. Next, we construct a routing mechanism\nwithout training from scratch, incorporating a differentiable routing process\nand load balancing. Using modest data, CMoE produces a well-designed, usable\nMoE from a 7B dense model within five minutes. With lightweight fine-tuning, it\nachieves high-performance recovery in under an hour. We make our code publicly\navailable at https:\/\/github.com\/JarvisPei\/CMoE.",
        "The movement to mobile computing solutions provides flexibility to different\nusers whether it is a business user, a student, or even providing entertainment\nto children and adults of all ages. Due to these emerging technologies mobile\nusers are unable to safeguard private information in a very effective way and\ncybercrimes are increasing day by day. This manuscript will focus on security\nvulnerabilities in the mobile computing industry, especially focusing on\ntablets and smart phones. This study will dive into current security threats\nfor the Android & Apple iOS market, exposing security risks and threats that\nthe novice or average user may not be aware of. The purpose of this study is to\nanalyze current security risks and threats, and provide solutions that may be\ndeployed to protect against such threats.",
        "In this paper, we introduce a novel mechanism that uses machine learning\ntechniques to detect water leaks in pipes. The proposed simple and low-cost\nmechanism is designed that can be easily installed on building pipes with\nvarious sizes. The system works based on gathering and amplifying water flow\nsignals using a mechanical sound amplifier. Then sounds are recorded and\nconverted to digital signals in order to be analyzed. After feature extraction\nand selection, deep neural networks are used to discriminate between with and\nwithout leak pipes. The experimental results show that this device can detect\nat least 100 milliliters per minute (mL\/min) of water flow in a pipe so that it\ncan be used as a core of a water leakage detection system.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Large Language Models (LLMs) reasoning processes are challenging to analyze\ndue to their complexity and the lack of organized visualization tools. We\npresent ReasonGraph, a web-based platform for visualizing and analyzing LLM\nreasoning processes. It supports both sequential and tree-based reasoning\nmethods while integrating with major LLM providers and over fifty\nstate-of-the-art models. ReasonGraph incorporates an intuitive UI with meta\nreasoning method selection, configurable visualization parameters, and a\nmodular framework that facilitates efficient extension. Our evaluation shows\nhigh parsing reliability, efficient processing, and strong usability across\nvarious downstream applications. By providing a unified visualization\nframework, ReasonGraph reduces cognitive load in analyzing complex reasoning\npaths, improves error detection in logical processes, and enables more\neffective development of LLM-based applications. The platform is open-source,\npromoting accessibility and reproducibility in LLM reasoning analysis.",
        "We settle the theoretical ground for the study of automata networks under\nblock-parallel update schedules, which are somehow dual to the block-sequential\nones, but allow for repetitions of automaton updates. This gain in expressivity\nbrings new challenges, and we analyse natural equivalence classes of update\nschedules: those leading to the same dynamics, and to the same limit dynamics,\nfor any automata network. Countings and enumeration algorithms are provided,\nfor their numerical study. We also prove computational complexity bounds for\nmany classical problems, involving fixed points, limit cycles, the recognition\nof subdynamics, reachability, etc. The PSPACE-completeness of computing the\nimage of a single configuration lifts the complexity of most problems, but the\nlandscape keeps some relief, in particular for reversible computations.",
        "Quadratic Unconstrained Binary Optimization (QUBO)-based suppression in\nobject detection is known to have superiority to conventional Non-Maximum\nSuppression (NMS), especially for crowded scenes where NMS possibly suppresses\nthe (partially-) occluded true positives with low confidence scores. Whereas\nexisting QUBO formulations are less likely to miss occluded objects than NMS,\nthere is room for improvement because existing QUBO formulations naively\nconsider confidence scores and pairwise scores based on spatial overlap between\npredictions. This study proposes new QUBO formulations that aim to distinguish\nwhether the overlap between predictions is due to the occlusion of objects or\ndue to redundancy in prediction, i.e., multiple predictions for a single\nobject. The proposed QUBO formulation integrates two features into the pairwise\nscore of the existing QUBO formulation: i) the appearance feature calculated by\nthe image similarity metric and ii) the product of confidence scores. These\nfeatures are derived from the hypothesis that redundant predictions share a\nsimilar appearance feature and (partially-) occluded objects have low\nconfidence scores, respectively. The proposed methods demonstrate significant\nadvancement over state-of-the-art QUBO-based suppression without a notable\nincrease in runtime, achieving up to 4.54 points improvement in mAP and 9.89\npoints gain in mAR.",
        "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
        "Visual place recognition is a challenging task for autonomous driving and\nrobotics, which is usually considered as an image retrieval problem. A commonly\nused two-stage strategy involves global retrieval followed by re-ranking using\npatch-level descriptors. Most deep learning-based methods in an end-to-end\nmanner cannot extract global features with sufficient semantic information from\nRGB images. In contrast, re-ranking can utilize more explicit structural and\nsemantic information in one-to-one matching process, but it is time-consuming.\nTo bridge the gap between global retrieval and re-ranking and achieve a good\ntrade-off between accuracy and efficiency, we propose StructVPR++, a framework\nthat embeds structural and semantic knowledge into RGB global representations\nvia segmentation-guided distillation. Our key innovation lies in decoupling\nlabel-specific features from global descriptors, enabling explicit semantic\nalignment between image pairs without requiring segmentation during deployment.\nFurthermore, we introduce a sample-wise weighted distillation strategy that\nprioritizes reliable training pairs while suppressing noisy ones. Experiments\non four benchmarks demonstrate that StructVPR++ surpasses state-of-the-art\nglobal methods by 5-23% in Recall@1 and even outperforms many two-stage\napproaches, achieving real-time efficiency with a single RGB input.",
        "In this paper, we use Legendre-Fenchel transform and a space decomposition to\ncarry out Fountain theorem and dual Fountain theorem for the following elliptic\nsystem of Hamiltonian type: \\[ \\begin{cases} \\begin{aligned} -\\Delta u&=H_v(u,\nv) \\,\\quad&&\\text{in}~\\Omega,\\\\ -\\Delta v&=H_u(u, v)\n\\,\\quad&&\\text{in}~\\Omega,\\\\ u,\\,v&=0~~&&\\text{on} ~ \\partial\\Omega,\\\\\n\\end{aligned} \\end{cases} \\] where $N\\ge 1$, $\\Omega \\subset \\mathbb{R}^N$ is a\nbounded domain and $H\\in C^1( \\mathbb{R}^2)$ is strictly convex, even and\nsubcritical. We mainly present two results: (i) When $H$ is superlinear, the\nsystem has infinitely many solutions, whose energies tend to infinity. (ii)\nWhen $H$ is sublinear, the system has infinitely many solutions, whose energies\nare negative and tend to 0. As a byproduct, the Lane-Emden system under\nsubcritical growth has infinitely many solutions.",
        "This study explores stochastic resonance (SR) in a Schmitt trigger circuit\nand its application to weak signal detection. SR, a phenomenon where noise\nsynchronizes with weak signals to enhance detectability, was demonstrated using\na custom-designed bi-stable Schmitt trigger system. The circuit's bi-stability\nwas validated through hysteresis curve analysis, confirming its suitability for\nSR studies. Experimental results revealed SR behavior by analyzing\nsignal-to-noise ratio (SNR) responses to noise amplitude variations. Detection\nexperiments were conducted to determine frequency and amplitude of damping\nsinusoidal pulses. Frequency detection proved effective, albeit with\nlimitations at low frequencies, while amplitude detection faced challenges due\nto mathematical complexities. Nonetheless, the study highlights SR's potential\nfor weak signal detection, with proposed enhancements to improve detection\naccuracy. This work underscores the adaptability of classical SR principles to\npractical detection systems and suggests future applications in advanced\ndetection technologies, including quantum systems.",
        "Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverse\nenvironments, making it a highly promising solution for human body\nreconstruction due to its privacy-friendly and non-intrusive nature. However,\nthe significant sparsity of mmWave point clouds limits the estimation accuracy.\nTo overcome this challenge, we propose a two-stage deep learning framework that\nenhances mmWave point clouds and improves human body reconstruction accuracy.\nOur method includes a mmWave point cloud enhancement module that densifies the\nraw data by leveraging temporal features and a multi-stage completion network,\nfollowed by a 2D-3D fusion module that extracts both 2D and 3D motion features\nto refine SMPL parameters. The mmWave point cloud enhancement module learns the\ndetailed shape and posture information from 2D human masks in single-view\nimages. However, image-based supervision is involved only during the training\nphase, and the inference relies solely on sparse point clouds to maintain\nprivacy. Experiments on multiple datasets demonstrate that our approach\noutperforms state-of-the-art methods, with the enhanced point clouds further\nimproving performance when integrated into existing models.",
        "We show that, under Dubrovin's notion of ''almost'' duality, the Frobenius\nmanifold structure on the orbit spaces of the extended affine Weyl groups of\ntype $\\mathrm{ADE}$ is dual, for suitable choices of weight markings, to the\nequivariant quantum cohomology of the minimal resolution of the du Val\nsingularity of the same Dynkin type. We also provide a uniform Lie-theoretic\nconstruction of Landau-Ginzburg mirrors for the quantum cohomology of\n$\\mathrm{ADE}$ resolutions. The mirror B-model is described by a\none-dimensional LG superpotential associated to the spectral curve of the\n$\\widehat{\\mathrm{ADE}}$ affine relativistic Toda chain.",
        "To study a deformation of a singularity taking into consideration their\ndifferential geometric properties, a form representing the deformation using\nonly diffeomorphisms on the source space and isometries of the target space\nplays a crucial role. Such a form for an $S_1$ singularity is obtained by the\nauthor's previous work. On this form, we give a necessary and sufficient\ncondition for such a map is being a frontal. The form for an $S_1$ singularity\nwith the frontal condition can be considered such a form for a cuspidal $S_1$\nsingularity. Using this form, we investigate geometric properties of cuspidal\n$S_1$ singularities and the cuspidal cross caps appearing in the deformation."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"Voxel Scene Graph for Intracranial Hemorrhage",
    "start_abstract":"Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
      ],
      "abstract":[
        "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "An intriguing coincidence between the majority of vast polar structure\n  dwarfs and a recent major merger at the M31 position",
        "Fault-Resilience of Dissipative Processes for Quantum Computing",
        "Does a Large Language Model Really Speak in Human-Like Language?",
        "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
        "Time Evolution of the Symmetry Resolved Entanglement Entropy after a\n  Mass Quench",
        "Quantum metric induced magneto-optical effects in\n  $\\mathcal{PT}$-symmetric antiferromagnets",
        "Systematic Search for Long-Term Trends in Fermi-LAT Jetted Active\n  Galactic Nuclei",
        "Approximate Evaluation Method for the Probability of the Union of\n  Independent Events",
        "Unveiling stellar spin: Determining inclination angles in Be stars",
        "FEASTS Combined with Interferometry (IV): Mapping HI Emission to a limit\n  of $N_{\\text{HI}}=10^{17.7} \\text{cm}^{-2}$ in Seven Edge-on Galaxies",
        "Randomized Spectral Clustering for Large-Scale Multi-Layer Networks",
        "Liquidity provision of utility indifference type in decentralized\n  exchanges",
        "How good is PAC-Bayes at explaining generalisation?",
        "Sliding ferroelectric control of unconventional magnetism in stacked\n  bilayers",
        "The centimeter emission from planet-forming disks in Taurus"
      ],
      "abstract":[
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "A significant part of the Milky Way (MW) dwarf galaxies orbit within a Vast\nPOlar Structure (VPOS), which is perpendicular to the Galactic disc and whose\norigin has not yet been identified. It includes the Large Magellanic Cloud\n(LMC) and its six dynamically associated dwarf galaxies. Andromeda Galaxy (M31)\nexperienced a major merger two to three billion years ago, and its accurate\nmodelling predicts that an associated tidal tail is pointing towards the\nGalaxy. Here, we tested a possible association between M31 tidal tail particles\nand MW dwarf galaxies, focusing first on the LMC and its associated dwarfs\nsince they are less affected by ram pressure. We traced back these dwarf galaxy\norbits by one billion years and calculated their association with the tidal\ntail particles in the 6D phase space, based on their proper motion from\n\\textit{Gaia} DR3. We find that for low-mass MW models (total mass less than 5\n$\\times 10^{11} M_{\\odot}$), the separation in the 6D space can be less than\n1$\\sigma$ for most of the M31 modelling, albeit with a significant degree of\nfreedom due to the still unknown proper motion of M31. We further discover that\nmany other dwarfs could also be associated with the M31 tidal tails if their\nmotions had been radially slowed, as expected from the ram pressure exerted by\nthe MW corona. This intriguing coincidence could explain the origin of the\nVPOS, which resulted from a matter exchange between M31 and MW.",
        "Dissipative processes have long been proposed as a means of performing\ncomputational tasks on quantum computers that may be intrinsically more robust\nto noise. In this work, we prove two main results concerning the\nerror-resilience capabilities of two types of dissipative algorithms:\ndissipative ground state preparation in the form of the dissipative quantum\neigensolver (DQE), and dissipative quantum computation (DQC). The first result\nis that under circuit-level depolarizing noise, a version of the DQE algorithm\napplied to the geometrically local, stabilizer-encoded Hamiltonians that arise\nnaturally when fermionic Hamiltonians are represented in qubits, can suppress\nthe additive error in the ground space overlap of the final output state\nexponentially in the code distance. This enables us to get closer to\nfault-tolerance for this task without the associated overhead. In contrast, for\ncomputation as opposed to ground state preparation, the second result proves\nthat DQC is no more robust to noise than the standard quantum circuit model.",
        "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.",
        "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
        "In this paper we investigate the properties of the symmetry resolved\nentanglement entropy after a mass quench in the Ising field theory. Since the\ntheory is free and the post-quench state known explicitly, the one-point\nfunction of the relevant (composite) branch point twist field can be computed\nusing form factor techniques, similar to previous work on the branch point\ntwist field and the magnetisation, respectively. We find that the symmetry\nresolved entropy grows linearly in time at the same rate as the total entropy,\nand that there are sub-leading oscillatory corrections. This result provides\nthe first explicit computation of the out-of-equilibrium dynamics of the\nsymmetry resolved entropy employing twist fields in quantum field theory and is\nconsistent with existing results based on the quasiparticle picture.",
        "The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can\nreveal the electronic structures of materials. The related probing methods are\nwidely used in the study of magnetic materials. However, space-time inversion\n($\\mathcal{PT}$) symmetric antiferromagnets were previously believed to be\nmagneto-optically inactive. Here, we point out that this traditional\nunderstanding is incorrect. Based on our generic formulas and symmetry\nanalysis, we find that in $\\mathcal{PT}$-symmetric antiferromagnets, it is the\nquantum metric, i.e., the real part of the quantum geometry, that induces MOEs.\nCombining a tight-binding model and first-principles calculations, we confirm\nthis observation by showing MOEs in the $\\mathcal{PT}$-symmetric\nantiferromagnet. Our work demonstrates that $\\mathcal{PT}$-symmetric\nantiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and\ngreatly broaden the research on MOEs.",
        "Jetted Active Galactic Nuclei (AGN) exhibit variability across a wide range\nof time scales. Traditionally, this variability can often be modeled well as a\nstochastic process. However, in certain cases, jetted AGN variability displays\nregular patterns, enabling us to conduct investigations aimed at understanding\nits origins. Additionally, a novel type of variability has emerged in jetted\nAGN lightcurves, specifically, the observation of a long-term trend\ncharacterized by a linear increase of the flux with time in blazars such as PG\n1553+113, which is among the objects most likely to display periodic behavior.\nIn this paper, we present the results of a systematic search for long-term\ntrends, spanning $\\approx$10\\, years, utilizing 12 years of Fermi-LAT\nobservations. The study is focused on detecting the presence of linear or\nquadratic long-term trends in a sample of 3308 jetted AGN. Our analysis has\nidentified 40 jetted AGN that exhibit long-term trends, each with distinct\nproperties, which we also characterize in this study. These long-term trends\nmay originate from the dynamics of a supermassive black hole binary system, or\nthey could be the result of intrinsic phenomena within the jet itself. Our\nfindings can help in addressing questions pertaining to the astrophysical\norigins of variability and periodicity within jetted AGN.",
        "The evaluation of the probability of union of a large number of independent\nevents requires several combinations involving the factorial and the use of\nhigh performance computers with several hours of processing. Bounds and\nsimplifications on the probability of the union are useful in the analysis of\nstochastic problems across various areas including (but not limited to) systems\nreliability, biological systems, real-time fault-tolerant systems, probability\ntheory, information theory and communications. We propose an approximation to\nevaluate the probability of the union of several independent events that uses\nthe arithmetic mean of the probability of all of them. The approximate results\nare very close to, but larger than the exact values. The method allows a much\nsmaller number of operations with a similar result and more simplicity.",
        "The physical properties of stellar atmospheres in rapidly rotating massive\nstars, such as Be stars, are critical to understanding their evolution and\ntheir role as progenitors of supernovae. These stars, which often have\nnear-critical rotation, exhibit equatorial stretching and gravity darkening,\nwhich significantly complicates the determination of parameters such as the\ninclination angle. Be stars, characterized by their extreme rotational\nvelocities, serve as excellent candidates for exploring these phenomena.\nHowever, fundamental quantities such as polar and equatorial radii and\ninclination angles are typically derived from interferometry, which applies\nonly to a limited number of stars. This study aims to enhance the determination\nof inclination angles for Be stars using the ZPEKTR spectral synthesis code. By\nincorporating advanced models of gravity darkening and stellar deformation, we\nevaluated the effectiveness of this method with a sample of ten Be stars from\nthe BeSOS database, comparing results with established interferometric data.\nMethods. We used the ZPEKTR code to model the effects of stellar oblateness and\ngravity darkening on spectral lines, focusing on the HeI 4471 line. We applied\na chi-squared test minimization approach to identify the best-fitting models,\nand we evaluated the inclination angles derived against interferometric\nmeasurements. Our analysis reveals a robust linear correlation between the\ninclination angles derived from ZPEKTR and using interferometric techniques,\nwhich demonstrates an excellent agreement. The ZPEKTR code effectively models\nhigh rotational velocity effects, providing precise stellar parameter\ndeterminations. The results underscore the potential of advanced spectroscopic\ntechniques to yield inclination measurements comparable to interferometry,\nwhich offers a pathway to studying distant massive stars.",
        "We present a statistical study of the neutral atomic hydrogen (HI) gas\nextending into the circumgalactic medium perpendicular to the disk for 7\nedge-on galaxies with inclinations above $85^{\\circ}$ from the FEASTS program\nwith a $3\\sigma$ ($20\\,\\text{km}\\,\\text{s}^{-1}$) column density\n($N_{\\text{HI}}$) depth of $5\\times10^{17} \\text{cm}^{-2}$. We develop two\nphotometric methods to separate the extraplanar HI from the disk component,\nbased on existing interferometric data and parametric modeling of the disk flux\ndistribution respectively. With both methods, the FEASTS data exhibit clear\nextended wings beyond the disk along the minor axis. The extraplanar HI\naccounts for 5% to 20% of the total HI mass and extends to $20\\text{-}50$ kpc\nat $N_{\\text{HI}}=10^{18} \\text{cm}^{-2}$. We find a tight positive correlation\nbetween vertical extensions of the extraplanar HI and total HI mass\n$M_\\text{HI}$. The iso-density shape of HI at $N_{\\text{HI}}=10^{18}\n\\text{cm}^{-2}$ has an average axis ratio of $0.56\\pm0.11$. The off-disk\n$N_{\\text{HI}}$ profiles of these edge-on galaxies well represent the lower\nenvelop of previous Lyman-$\\alpha$ absorption measurements at low-redshift. Our\nresults suggest that at $N_{\\text{HI}}=5\\times10^{17} \\text{cm}^{-2}$, the HI\nextends considerably further than the known thin and thick disks in the\nvertical direction, but still remains much flattener than a spherical\ndistribution, consistent with theoretical expectations that outflow,\ncirculation, and accretion should have different impacts in these two\ndirections. We show the tension of our results with Illustris and TNG\npredictions, highlighting the constraining power of our results for future\nsimulations.",
        "Large-scale multi-layer networks with large numbers of nodes, edges, and\nlayers arise across various domains, which poses a great computational\nchallenge for the downstream analysis. In this paper, we develop an efficient\nrandomized spectral clustering algorithm for community detection of multi-layer\nnetworks. We first utilize the random sampling strategy to sparsify the\nadjacency matrix of each layer. Then we use the random projection strategy to\naccelerate the eigen-decomposition of the sum-of-squared sparsified adjacency\nmatrices of all layers. The communities are finally obtained via the k-means of\nthe eigenvectors. The algorithm not only has low time complexity but also saves\nthe storage space. Theoretically, we study the misclassification error rate of\nthe proposed algorithm under the multi-layer stochastic block models, which\nshows that the randomization does not deteriorate the error bound under certain\nconditions. Numerical studies on multi-layer networks with millions of nodes\nshow the superior efficiency of the proposed algorithm, which achieves\nclustering results rapidly. A new R package called MLRclust is developed and\nmade available to the public.",
        "We present a mathematical formulation of liquidity provision in decentralized\nexchanges. We focus on constant function market makers of utility indifference\ntype, which include constant product market makers with concentrated liquidity\nas a special case. First, we examine no-arbitrage conditions for a liquidity\npool and compute an optimal arbitrage strategy when there is an external liquid\nmarket. Second, we show that liquidity provision suffers from impermanent loss\nunless a transaction fee is levied under the general framework with\nconcentrated liquidity. Third, we establish the well-definedness of\narbitrage-free reserve processes of a liquidity pool in continuous-time and\nshow that there is no loss-versus-rebalancing under a nonzero fee if the\nexternal market price is continuous. We then argue that liquidity provision by\nmultiple liquidity providers can be understood as liquidity provision by a\nrepresentative liquidity provider, meaning that the analysis boils down to that\nfor a single liquidity provider. Last, but not least, we give an answer to the\nfundamental question in which sense the very construction of constant function\nmarket makers with concentrated liquidity in the popular platform Uniswap v3 is\noptimal.",
        "We discuss necessary conditions for a PAC-Bayes bound to provide a meaningful\ngeneralisation guarantee. Our analysis reveals that the optimal generalisation\nguarantee depends solely on the distribution of the risk induced by the prior\ndistribution. In particular, achieving a target generalisation level is only\nachievable if the prior places sufficient mass on high-performing predictors.\nWe relate these requirements to the prevalent practice of using data-dependent\npriors in deep learning PAC-Bayes applications, and discuss the implications\nfor the claim that PAC-Bayes ``explains'' generalisation.",
        "The control of unconventional magnetism, which displays an antiferromagnetic\nconfiguration with ferromagnetism-like properties, has drawn intense attention\nfor advancing antiferromagnetic spintronics. Here, through symmetry analysis,\nwe propose a general stacking rule, characterized by a connection operator\nlinking two stacked bilayers, for controlling unconventional magnetism via\nsliding ferroelectricity. Such rule enables the simultaneous switching of both\nelectric polarization and nonrelativistic spin splitting or anomalous Hall\neffect in altermagnets, a class of collinear unconventional magnets. By\ncomprehensively surveying the 80 layer groups, we identify all the stacking\norders that allow for such two types of simultaneous switching. Combined with\nfirst-principles calculations, we demonstrate the sliding ferroelectric control\nof spin polarization and anomalous Hall effect in the altermagnetic AgF2\nbilayer. Our work provides a symmetry strategy for achieving ferroelectric\ncontrol of unconventional magnetism in bilayer systems and opens avenues for\nexploring new types of magnetoelectric coupling.",
        "The last decade has witnessed remarkable advances in the characterization of\nthe (sub-)millimeter emission from planet-forming disks. Instead, the study of\nthe (sub-)centimeter emission has made more limited progress, to the point that\nonly a few exceptional disk-bearing objects have been characterized in the\ncentimeter regime. This work takes a broad view of the centimeter emission from\na large sample with VLA observations that is selected from previous ALMA\nsurveys of more representative disks in brightness and extent. We report on the\ndetection and characterization of flux at centimeter wavelengths from 21\nsources in the Taurus star-forming region. Complemented by literature and\narchival data, the entire photometry from 0.85 mm to 6 cm is fitted by a\ntwo-component model that determines the ubiquitous presence of free-free\nemission entangled with the dust emission. The flux density of the free-free\nemission is found to scale with the accretion rate but is independent of the\nouter disk morphology depicted by ALMA. The dust emission at 2 cm is still\nappreciable, and offers the possibility to extract an unprecedented large set\nof dust spectral indices in the centimeter regime. A pronounced change between\nthe median millimeter indices (2.3) and centimeter indices (2.8) suggests that\na large portion of the disk emission is optically thick up to 3 mm. The\ncomparison of both indices and fluxes with the ALMA disk extent indicates that\nthis portion can be as large as 40 au, and suggests that the grain population\nwithin this disk region that emits the observed centimeter emission is similar\nin disks with different size and morphology. All these results await\nconfirmation and dedicated dust modeling once facilities like ngVLA or SKA-mid\nare able to resolve the centimeter emission from planet-forming disks and\ndisentangle the various components."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion",
    "start_abstract":"Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
      ],
      "abstract":[
        "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Robust Cislunar Low-Thrust Trajectory Optimization under Uncertainties\n  via Sequential Covariance Steering",
        "Complete systems of inequalities relating the perimeter, the area and\n  the Cheeger constant of planar domains",
        "Bi-Parameterized Two-Stage Stochastic Min-Max and Min-Min Mixed Integer\n  Programs",
        "Disturbance-to-state stabilization by output feedback of nonlinear ODE\n  cascaded with a reaction-diffusion equation",
        "A bang-bang optimal control for a nonlinear system modeling the Gate\n  Control Theory of Pain",
        "Separable Approximations of Optimal Value Functions and Their\n  Representation by Neural Networks",
        "On extending the class of convex functions",
        "Convergence Analysis of EXTRA in Non-convex Distributed Optimization",
        "A Dual Koopman Approach to Observer Design for Nonlinear Systems",
        "Transportation Network Analysis, Volume I: Static and Dynamic Traffic\n  Assignment",
        "A unified recursive identification algorithm with quantized observations\n  based on weighted least-squares type criteria",
        "A Unified Dual Consensus Approach to Distributed Optimization with\n  Globally-Coupled Constraints",
        "Generic linear convergence for algorithms of non-linear least squares\n  over smooth varieties",
        "A Survey on Foundation-Model-Based Industrial Defect Detection",
        "Generalized $\\eta$-pairing theory and anomalous localization in\n  non-Hermitian systems",
        "Aerial Vision-and-Language Navigation with Grid-based View Selection and\n  Map Construction",
        "Real-Time Fast Marching Tree for Mobile Robot Motion Planning in Dynamic\n  Environments",
        "The Muddy Waters of Modeling Empathy in Language: The Practical Impacts\n  of Theoretical Constructs",
        "Theoretical study of the $\\Sigma N$ cusp in the\n  $K^-d\\rightarrow\\pi\\Lambda N$ reaction",
        "A modeling framework to support the electrification of private transport\n  in African cities: a case study of Addis Ababa",
        "Stacked Intelligent Metasurface Enabled Near-Field Multiuser\n  Beamfocusing in the Wave Domain",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "A note On the existence of solutions to Hitchin's self-duality equations",
        "General relativistic quasi-spherical accretion in a dark matter halo",
        "General relativistic particle trajectories via quantum mechanical weak\n  values and the Schwarzschild-Alcubierre spacetime",
        "ProPINN: Demystifying Propagation Failures in Physics-Informed Neural\n  Networks",
        "RePanda: Pandas-powered Tabular Verification and Reasoning",
        "Comparing Native and Non-native English Speakers' Behaviors in\n  Collaborative Writing through Visual Analytics"
      ],
      "abstract":[
        "Spacecraft operations are influenced by uncertainties such as dynamics\nmodeling, navigation, and maneuver execution errors. Although mission design\nhas traditionally incorporated heuristic safety margins to mitigate the effect\nof uncertainties, particularly before\/after crucial events, it is yet unclear\nwhether this practice will scale in the cislunar region, which features locally\nchaotic nonlinear dynamics and involves frequent lunar flybys. This paper\napplies chance-constrained covariance steering and sequential convex\nprogramming to simultaneously design an optimal trajectory and trajectory\ncorrection policy that can probabilistically guarantee safety constraints under\nthe assumed physical\/navigational error models. The results show that the\nproposed method can effectively control the state uncertainty in a highly\nnonlinear environment and provide a trajectory with better local stability\nproperties than a trajectory designed without considering uncertainties. The\nframework allows faster computation and lossless covariance propagation\ncompared to existing methods, enabling a rapid and accurate comparison of\n$\\Delta V_{99}$ costs for different uncertainty parameters. We demonstrate the\nalgorithm on several transfers in the Earth-Moon Circular Restricted Three Body\nProblem.",
        "The object of the paper is to find complete systems of inequalities relating\nthe perimeter $P$, the area $|\\cdot|$ and the Cheeger constant $h$ of planar\nsets. To do so, we study the so called Blaschke--Santal\\'o diagram of the\ntriplet $(P,h,|\\cdot|)$ for different classes of domains: simply connected\nsets, convex sets and convex polygons with at most $N$ sides. We completely\ndetermine the diagram in the latter cases except for the class of convex\n$N$-gons when $N\\ge 5$ is odd: therein, we show that the boundary of the\ndiagram is given by the graphs of two continuous and strictly increasing\nfunctions. An explicit formula for the lower one and a numerical method to\nobtain the upper one is provided. At last, some applications of the results are\npresented.",
        "We introduce two-stage stochastic min-max and min-min integer programs with\nbi-parameterized recourse (BTSPs), where the first-stage decisions affect both\nthe objective function and the feasible region of the second-stage problem. To\nsolve these programs efficiently, we introduce Lagrangian-integrated L-shaped\n($L^2$) methods, which guarantee exact solutions when the first-stage decisions\nare pure binary. For mixed-binary first-stage programs, we present a\nregularization-augmented variant of this method. We also introduce\ndistributionally robust bi-parameterized two-stage stochastic integer programs\nand present an extension of the $L^2$ method and a reformulation-based method\nfor programs with finite and continuous supports, respectively. Our\ncomputational results show that the $L^2$ method surpasses the benchmark method\nfor bi-parameterized stochastic network interdiction problems, solving all\ninstances in 23 seconds on average, whereas the benchmark method failed to\nsolve any instance within 3600 seconds. Additionally, it achieves optimal\nsolutions up to 18.4 and 1.7 times faster for instances of risk-neutral and\ndistributionally robust bi-parameterized stochastic facility location problems,\nrespectively. Furthermore, BTSPs have applications in solving stochastic\nproblems with decision-dependent probability distributions or sets of\ndistributions (ambiguity set). The $L^2$ method outperforms existing\napproaches, achieving optimal solutions 5.3 times faster for distributionally\nrobust facility location problem with a decision-dependent and non-relatively\ncomplete ambiguity set.",
        "In this paper, we analyze the output stabilization problem for cascaded\nnonlinear ODE with $1-d$ heat diffusion equation affected by both in-domain and\nboundary perturbations. We assume that the only available part of states is the\nfirst components of the ODE-subsystem and one boundary of the heat-subsystem.\nThe particularity of this system is two folds i) it contains a nonlinear\nadditive term in the ODE-subsystem, and ii) it is affected by both boundary and\nin-domain perturbations signals.\n  For such a system, and unlike the existing works, we succeeded to design an\noutput observer-based feedback that guarantees not only asymptotic\nstabilization result but also a globally {\\it disturbance-to-state\nstabilization} for our cascaded system. The output feedback is designed using\nan adequate backstepping transformation recently introduced for coupled\nODE-heat equations combined with high-gain observer and high-gain controller.",
        "We consider a nonlinear system of coupled ordinary differential equations\n(representing the excitatory, inhibitory, and T-cell potentials) based on the\nGate Control Theory of Pain, initially proposed by R. Melzack and P.D. Wall in\n1965, and later mathematically modeled by N.F. Britton and S.M. Skevington in\n1988.",
        "The use of separable approximations is proposed to mitigate the curse of\ndimensionality related to the approximation of high-dimensional value functions\nin optimal control. The separable approximation exploits intrinsic decaying\nsensitivity properties of the system, where the influence of a state variable\non another diminishes as their spatial, temporal, or graph-based distance\ngrows. This property allows the efficient representation of global functions as\na sum of localized contributions. A theoretical framework for constructing\nseparable approximations in the context of optimal control is proposed by\nleveraging decaying sensitivity in both discrete and continuous time. Results\nextend prior work on decay properties of solutions to Lyapunov and Riccati\nequations, offering new insights into polynomial and exponential decay regimes.\nConnections to neural networks are explored, demonstrating how separable\nstructures enable scalable representations of high-dimensional value functions\nwhile preserving computational efficiency.",
        "In this brief note, it is shown that the function p^TW log(p) is convex in p\nif W is a diagonally dominant positive definite M-matrix. The techniques used\nto prove convexity are well-known in linear algebra and essentially involves\nfactoring the Hessian in a way that is amenable to martix analysis. Using\nsimilar techniques, two classes of convex homogeneous polynomials is derived -\nnamely, p^TW p2 and (p^k)^TW p^k - the latter also happen to be SOS-convex.\nLastly, usign the same techniques, it is also shown that the function p^TW ep\nis convex over the positive reals only if W is a non-negative diagonal matrix.\nDiscussions regarding the utility of these functions and examples accompany the\nresults presented.",
        "Optimization problems involving the minimization of a finite sum of smooth,\npossibly non-convex functions arise in numerous applications. To achieve a\nconsensus solution over a network, distributed optimization algorithms, such as\n\\textbf{EXTRA} (decentralized exact first-order algorithm), have been proposed\nto address these challenges. In this paper, we analyze the convergence\nproperties of \\textbf{EXTRA} in the context of smooth, non-convex optimization.\nBy interpreting its updates as a nonlinear dynamical system, we show novel\ninsights into its convergence properties. Specifically, i) \\textbf{EXTRA}\nconverges to a consensual first-order stationary point of the global objective\nwith a sublinear rate; and ii) \\textbf{EXTRA} avoids convergence to consensual\nstrict saddle points, offering second-order guarantees that ensure robustness.\nThese findings provide a deeper understanding of \\textbf{EXTRA} in a non-convex\ncontext.",
        "The Koopman operator approach to the state estimation problem for nonlinear\nsystems is a promising research area. The main goal of this paper is an attempt\nto provide a rigorous theoretical framework for this approach. In particular,\nthe (linear) dual Koopman system is introduced and studied in an infinite\ndimensional context. Moreover, new concepts of observability and detectability\nare defined in the dual Koopman system, which are shown to be equivalent to the\nobservability and detectability of the nonlinear system, respectively. The\ntheoretical framework is applied to a class of holomorphic dynamics. For this\nclass, a Luenberger-type observer is designed for the dual Koopman system via a\nspectral method, yielding an estimate of the state of the nonlinear system. A\nparticular attention is given to the existence of an appropriate solution to\nthe dual Koopman system and observer, which are defined in the Hardy space on\nthe polydisc. Spectral observability and detectability conditions are derived\nin this setting, and the exponential convergence of the Koopman observer is\nshown. Finally, numerical experiments support the theoretical findings.",
        "This book covers static and dynamic traffic assignment models used in\ntransportation planning and network analysis. Traffic assignment is the final\nstep in the traditional planning process, and recent decades have seen many\nadvances in formulating and solving such models. The book discusses classical\nsolution methods alongside recent ones used in contemporary planning software.\n  The primary audience for the book is graduate students new to transportation\nnetwork analysis, and to this end there are appendices providing general\nmathematical background, and more specific background in formulating\noptimization problems. We have also included appendices discussing more general\noptimization applications outside of traffic assignment. We believe the book is\nalso of interest to practitioners seeking to understand recent advances in\nnetwork analysis, and to researchers wanting a unified reference for traffic\nassignment content.\n  A second volume is currently under preparation, and will cover transit,\nfreight, and logistics models in transportation networks. A free PDF version of\nthe text will always be available online at\nhttps:\/\/sboyles.github.io\/blubook.html. We will periodically post updated\nversions of the text at this link, along with slides and other instructor\nresources.",
        "This paper investigates system identification problems with Gaussian inputs\nand quantized observations under fixed thresholds. A new formulation for the\npredictor of quantized observations is introduced, establishing a linear\ncorrelation with the parameter estimations through a probabilistic relationship\namong quantized observations, Gaussian inputs, and system parameters.\nSubsequently, a novel weighted least-squares criterion is proposed, and a\ntwo-step recursive identification algorithm is constructed, which is capable of\naddressing both noisy and noise-free linear systems. Convergence analysis of\nthis identification algorithm is conducted, demonstrating convergence in both\nalmost sure and $L^{p}$ senses under mild conditions, with respective rates of\n$O(\\sqrt{ \\log \\log k\/k})$ and $O(1\/k^{p\/2})$, where $k$ denotes the time step.\nIn particular, this algorithm offers an asymptotically efficient estimation of\nthe variance of Gaussian variables using quantized observations. Additionally,\nasymptotic normality is established, and an expression for the asymptotic\nvariance is provided when the weight coefficients are properly selected.\nFurthermore, extensions to output-error systems are discussed, enhancing the\napplicability and relevance of the proposed methods. Two numerical examples are\nprovided to validate these theoretical advancements.",
        "This article explores distributed convex optimization with globally-coupled\nconstraints, where the objective function is a general nonsmooth convex\nfunction, the constraints include nonlinear inequalities and affine equalities,\nand the feasible region is possibly unbounded. To address such problems, a\nunified DUal Consensus Algorithm (DUCA) and its proximal variant (Pro-DUCA) are\nproposed, which are unified frameworks that approximate the method of\nmultipliers applied to the corresponding dual problem in no need of a\nclosed-form dual objective. With varied parameter settings, DUCA and Pro-DUCA\nnot only extend a collection of existing consensus optimization methods to\nsolve the dual problem that they used to be inapplicable to, but also aid in\noffering new efficient algorithms to the literature. The proposed unified\nalgorithms are shown to achieve $O(1\/k)$ convergence rates in terms of\noptimality and feasibility, providing new or enhanced convergence results for a\nnumber of existing methods. Simulations demonstrate that these algorithms\noutperform several state-of-the-art alternatives in terms of objective and\nfeasibility errors.",
        "In applications, a substantial number of problems can be formulated as\nnon-linear least squares problems over smooth varieties. Unlike the usual least\nsquares problem over a Euclidean space, the non-linear least squares problem\nover a variety can be challenging to solve and analyze, even if the variety\nitself is simple. Geometrically, this problem is equivalent to projecting a\npoint in the ambient Euclidean space onto the image of the given variety under\na non-linear map. It is the singularities of the image that make both the\ncomputation and the analysis difficult. In this paper, we prove that under some\nmild assumptions, these troublesome singularities can always be avoided. This\nenables us to establish a linear convergence rate for iterative sequences\ngenerated by algorithms satisfying some standard assumptions. We apply our\ngeneral results to the low-rank partially orthogonal tensor approximation\nproblem. As a consequence, we obtain the linear convergence rate for a\nclassical APD-ALS method applied to a generic tensor, without any further\nassumptions.",
        "As industrial products become abundant and sophisticated, visual industrial\ndefect detection receives much attention, including two-dimensional and\nthree-dimensional visual feature modeling. Traditional methods use statistical\nanalysis, abnormal data synthesis modeling, and generation-based models to\nseparate product defect features and complete defect detection. Recently, the\nemergence of foundation models has brought visual and textual semantic prior\nknowledge. Many methods are based on foundation models (FM) to improve the\naccuracy of detection, but at the same time, increase model complexity and slow\ndown inference speed. Some FM-based methods have begun to explore lightweight\nmodeling ways, which have gradually attracted attention and deserve to be\nsystematically analyzed. In this paper, we conduct a systematic survey with\ncomparisons and discussions of foundation model methods from different aspects\nand briefly review non-foundation model (NFM) methods recently published.\nFurthermore, we discuss the differences between FM and NFM methods from\ntraining objectives, model structure and scale, model performance, and\npotential directions for future exploration. Through comparison, we find FM\nmethods are more suitable for few-shot and zero-shot learning, which are more\nin line with actual industrial application scenarios and worthy of in-depth\nresearch.",
        "By generalizing the eta-pairing theory to non-Hermitian Hubbard models on\narbitrary lattices, we obtain the sufficient and necessary condition for the\neta-pairing operator to be an eigenoperator of the Hamiltonian $H$, and find\nunique eta-pairing phenomena without Hermitian analogs. For instance, the\nHermitian conjugate of an eta-pairing eigenoperator may not be an\neigenoperator, eta-pairing eigenoperators can be spatially modulated, and the\n$SU(2)$ pseudospin symmetry may not be respected even if $H$ commutes with the\neta-pairing operators. Remarkably, these novel non-Hermitian phenomena are\nclosely related to each other by several theorems we establish and can lead to,\ne.g., the notion of non-Hermitian angular-momentum operators and the anomalous\nlocalization of eta-pairing eigenstates. Some issues on the $SO(4)$ and\nparticle-hole symmetries are clarified. Our general eta-pairing theory also\nreveals a previously unnoticed unification of these symmetries of the Hubbard\nmodel. To exemplify these findings, we propose the Hatano-Nelson-Hubbard model.\nIn this interacting non-Hermitian system without even the bulk translation\ninvariance, the right and left two-particle eta-pairing eigenstates are\nexponentially localized at opposite boundaries of the chain. We then generalize\nthis model to two dimensions and find that the eta-pairing eigenstate can\nexhibit the first- or second-order skin effect. Thus, eta-pairing may represent\na new mechanism for skin effects in interacting non-Hermitian systems, even in\nhigher dimensions and without the bulk translation symmetry. To realize all of\nthe non-Hermitian eta-pairing phenomena, we construct a general two-sublattice\nmodel defined on an arbitrary lattice, which can exhibit anomalous localization\nof eta-pairing eigenstates; besides, this model can reveal the eta-pairing\nstructure [e.g., the $SO(4)$ symmetry] in systems with Hermitian hoppings.",
        "Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned\naerial vehicle agent to navigate aerial 3D environments following human\ninstruction. Compared to ground-based VLN, aerial VLN requires the agent to\ndecide the next action in both horizontal and vertical directions based on the\nfirst-person view observations. Previous methods struggle to perform well due\nto the longer navigation path, more complicated 3D scenes, and the neglect of\nthe interplay between vertical and horizontal actions. In this paper, we\npropose a novel grid-based view selection framework that formulates aerial VLN\naction prediction as a grid-based view selection task, incorporating vertical\naction prediction in a manner that accounts for the coupling with horizontal\nactions, thereby enabling effective altitude adjustments. We further introduce\na grid-based bird's eye view map for aerial space to fuse the visual\ninformation in the navigation history, provide contextual scene information,\nand mitigate the impact of obstacles. Finally, a cross-modal transformer is\nadopted to explicitly align the long navigation history with the instruction.\nWe demonstrate the superiority of our method in extensive experiments.",
        "This paper proposes the Real-Time Fast Marching Tree (RT-FMT), a real-time\nplanning algorithm that features local and global path generation,\nmultiple-query planning, and dynamic obstacle avoidance. During the search,\nRT-FMT quickly looks for the global solution and, in the meantime, generates\nlocal paths that can be used by the robot to start execution faster. In\naddition, our algorithm constantly rewires the tree to keep branches from\nforming inside the dynamic obstacles and to maintain the tree root near the\nrobot, which allows the tree to be reused multiple times for different goals.\nOur algorithm is based on the planners Fast Marching Tree (FMT*) and Real-time\nRapidly-Exploring Random Tree (RT-RRT*). We show via simulations that RT-FMT\noutperforms RT- RRT* in both execution cost and arrival time, in most cases.\nMoreover, we also demonstrate via simulation that it is worthwhile taking the\nlocal path before the global path is available in order to reduce arrival time,\neven though there is a small possibility of taking an inferior path.",
        "Conceptual operationalizations of empathy in NLP are varied, with some having\nspecific behaviors and properties, while others are more abstract. How these\nvariations relate to one another and capture properties of empathy observable\nin text remains unclear. To provide insight into this, we analyze the transfer\nperformance of empathy models adapted to empathy tasks with different\ntheoretical groundings. We study (1) the dimensionality of empathy definitions,\n(2) the correspondence between the defined dimensions and measured\/observed\nproperties, and (3) the conduciveness of the data to represent them, finding\nthey have a significant impact to performance compared to other transfer\nsetting features. Characterizing the theoretical grounding of empathy tasks as\ndirect, abstract, or adjacent further indicates that tasks that directly\npredict specified empathy components have higher transferability. Our work\nprovides empirical evidence for the need for precise and multidimensional\nempathy operationalizations.",
        "The $K^-d\\rightarrow\\pi\\Lambda N$ reaction is useful for exploring the\nhyperon-nucleon interaction through final state interactions. In particular,\nthe cusp structure of the $\\Lambda N$ invariant mass spectrum at the $\\Sigma N$\nthreshold contains information about the s-wave interaction of 1\/2-isospin\nhyperon-nucleon systems. The calculation of the spectrum is performed with the\naim of extracting the scattering length of the $\\Sigma N(I=1\/2)$ channel that\ncouples to the $\\Lambda N$ channel from this reaction, and the results are\ndiscussed in comparison with experimental data to highlight the factors that\nshould be considered.",
        "The electrification of road transport, as the predominant mode of\ntransportation in Africa, represents a great opportunity to reduce greenhouse\ngas emissions and dependence on costly fuel imports. However, it introduces\nmajor challenges for local energy infrastructures, including the deployment of\ncharging stations and the impact on often fragile electricity grids. Despite\nits importance, research on electric mobility planning in Africa remains\nlimited, while existing planning tools rely on detailed local mobility data\nthat is often unavailable, especially for privately owned passenger vehicles.\nIn this study, we introduce a novel framework designed to support private\nvehicle electrification in data-scarce regions and apply it to Addis Ababa,\nsimulating the mobility patterns and charging needs of 100,000 electric\nvehicles. Our analysis indicate that these vehicles generate a daily charging\ndemand of approximately 350 MWh and emphasize the significant influence of the\ncharging location on the spatial and temporal distribution of this demand.\nNotably, charging at public places can help smooth the charging demand\nthroughout the day, mitigating peak charging loads on the electricity grid. We\nalso estimate charging station requirements, finding that workplace charging\nrequires approximately one charging point per three electric vehicles, while\npublic charging requires only one per thirty. Finally, we demonstrate that\nphotovoltaic energy can cover a substantial share of the charging needs,\nemphasizing the potential for renewable energy integration. This study lays the\ngroundwork for electric mobility planning in Addis Ababa while offering a\ntransferable framework for other African cities.",
        "Intelligent surfaces represent a breakthrough technology capable of\ncustomizing the wireless channel cost-effectively. However, the existing works\ngenerally focus on planar wavefront, neglecting near-field spherical wavefront\ncharacteristics caused by large array aperture and high operation frequencies\nin the terahertz (THz). Additionally, the single-layer reconfigurable\nintelligent surface (RIS) lacks the signal processing ability to mitigate the\ncomputational complexity at the base station (BS). To address this issue, we\nintroduce a novel stacked intelligent metasurfaces (SIM) comprised of an array\nof programmable metasurface layers. The SIM aims to substitute conventional\ndigital baseband architecture to execute computing tasks with ultra-low\nprocessing delay, albeit with a reduced number of radio-frequency (RF) chains\nand low-resolution digital-to-analog converters. In this paper, we present a\nSIM-aided multiuser multiple-input single-output (MU-MISO) near-field system,\nwhere the SIM is integrated into the BS to perform beamfocusing in the wave\ndomain and customize an end-to-end channel with minimized inter-user\ninterference. Finally, the numerical results demonstrate that near-field\ncommunication achieves superior spatial gain over the far-field, and the SIM\neffectively suppresses inter-user interference as the wireless signals\npropagate through it.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "In 1987, Hitchin introduced the self-duality equations on rank-2 complex\nvector bundles over compact Riemann surfaces with genus greater than one as a\nreduction of the Yang-Mills equation and established the existence of solutions\nto these equations starting from a Higgs stable bundle. In this paper, we fill\nin some technical details in Hitchin's original proof by the following three\nsteps. First, we reduce the existence of a solution of class $L_1^2$ to\nminimizing the energy functional within a Higgs stable orbit of the $L_2^2$\ncomplex gauge group action. Second, using this transformation, we obtain a\nsolution of class $L_1^2$ in this orbit. These two steps primarily follow\nHitchin's original approach. Finally, using the Coulomb gauge, we construct a\nsmooth solution by applying an $L_2^2$ unitary gauge transformation to the\n$L_1^2$ solution constructed previously. This last step provides additional\ntechnical details to Hitchin's original proof.",
        "Context. The Bondi spherical accretion solution has been used to model\naccretion onto compact objects in a variety of situations, from interpretation\nof observations to subgrid models in cosmological simulations. Aims. We aim to\ninvestigate how the presence of dark matter (DM) alters the dynamics and\nphysical properties of accretion onto supermassive black holes on scales\nranging from ~ 10 pc to the event horizon. Methods. In particular, we\ninvestigate Bondi-like accretion flows with zero and low specific angular\nmomentum around supermassive black holes surrounded by dark-matter halos by\nperforming 1D and 2.5D general relativistic hydrodynamics (GRHD) simulations\nusing the black hole accretion code (BHAC). Results. We find notable\ndifferences in the dynamics and structure of spherical accretion flows in the\npresence of DM. The most significant effects include increases in density,\ntemperature, and pressure, as well as variations in radial velocity both inside\nand outside the regions containing DM or even the production of outflow.\nConclusions. This investigation provides valuable insights into the role of\ncosmological effects, particularly DM, in shaping the behavior of accretion\nflows and black holes (BHs). Our simulations may be directly applicable to\nmodel systems with a large black hole-to-halo mass ratio, which are expected to\nbe found at very high redshifts.",
        "We show that the average trajectories of relativistic quantum particles in\nSchwarzschild spacetime, obtained via quantum mechanical weak measurements of\nmomentum and energy, are equivalent to the predicted flow lines of probability\ncurrent in curved spacetime quantum theory. We subsequently demonstrate that\nthese trajectories correspond exactly to classical null geodesics in a hybrid\nSchwarzschild-Alcubierre spacetime. This threefold equivalence demonstrates how\nquantum theory in curved spacetime can be formulated via operationally-defined\nmeasurements, and that such a theory may be interpreted deterministically, in\nthe spirit of hidden-variable models such as Bohmian mechanics, through the\nnovel connection to an underlying \"guiding metric.\"",
        "Physics-informed neural networks (PINNs) have earned high expectations in\nsolving partial differential equations (PDEs), but their optimization usually\nfaces thorny challenges due to the unique derivative-dependent loss function.\nBy analyzing the loss distribution, previous research observed the propagation\nfailure phenomenon of PINNs, intuitively described as the correct supervision\nfor model outputs cannot ``propagate'' from initial states or boundaries to the\ninterior domain. Going beyond intuitive understanding, this paper provides the\nfirst formal and in-depth study of propagation failure and its root cause.\nBased on a detailed comparison with classical finite element methods, we\nascribe the failure to the conventional single-point-processing architecture of\nPINNs and further prove that propagation failure is essentially caused by the\nlower gradient correlation of PINN models on nearby collocation points.\nCompared to superficial loss maps, this new perspective provides a more precise\nquantitative criterion to identify where and why PINN fails. The theoretical\nfinding also inspires us to present a new PINN architecture, named ProPINN,\nwhich can effectively unite the gradient of region points for better\npropagation. ProPINN can reliably resolve PINN failure modes and significantly\nsurpass advanced Transformer-based models with 46% relative promotion.",
        "Fact-checking tabular data is essential for ensuring the accuracy of\nstructured information. However, existing methods often rely on black-box\nmodels with opaque reasoning. We introduce RePanda, a structured fact\nverification approach that translates claims into executable pandas queries,\nenabling interpretable and verifiable reasoning.\n  To train RePanda, we construct PanTabFact, a structured dataset derived from\nthe TabFact train set, where claims are paired with executable queries\ngenerated using DeepSeek-Chat and refined through automated error correction.\nFine-tuning DeepSeek-coder-7B-instruct-v1.5 on PanTabFact, RePanda achieves\n84.09% accuracy on the TabFact test set.\n  To evaluate Out-of-Distribution (OOD) generalization, we interpret\nquestion-answer pairs from WikiTableQuestions as factual claims and refer to\nthis dataset as WikiFact. Without additional fine-tuning, RePanda achieves\n84.72% accuracy on WikiFact, significantly outperforming all other baselines\nand demonstrating strong OOD robustness. Notably, these results closely match\nthe zero-shot performance of DeepSeek-Chat (671B), indicating that our\nfine-tuning approach effectively distills structured reasoning from a much\nlarger model into a compact, locally executable 7B model.\n  Beyond fact verification, RePanda extends to tabular question answering by\ngenerating executable queries that retrieve precise answers. To support this,\nwe introduce PanWiki, a dataset mapping WikiTableQuestions to pandas queries.\nFine-tuning on PanWiki, RePanda achieves 75.1% accuracy in direct answer\nretrieval. These results highlight the effectiveness of structured\nexecution-based reasoning for tabular verification and question answering.\n  We have publicly released the dataset on Hugging Face at\ndatasets\/AtoosaChegini\/PanTabFact.",
        "Understanding collaborative writing dynamics between native speakers (NS) and\nnon-native speakers (NNS) is critical for enhancing collaboration quality and\nteam inclusivity. In this paper, we partnered with communication researchers to\ndevelop visual analytics solutions for comparing NS and NNS behaviors in 162\nwriting sessions across 27 teams. The primary challenges in analyzing writing\nbehaviors are data complexity and the uncertainties introduced by automated\nmethods. In response, we present \\textsc{COALA}, a novel visual analytics tool\nthat improves model interpretability by displaying uncertainties in author\nclusters, generating behavior summaries using large language models, and\nvisualizing writing-related actions at multiple granularities. We validated the\neffectiveness of \\textsc{COALA} through user studies with domain experts\n(N=2+2) and researchers with relevant experience (N=8). We present the insights\ndiscovered by participants using \\textsc{COALA}, suggest features for future\nAI-assisted collaborative writing tools, and discuss the broader implications\nfor analyzing collaborative processes beyond writing."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Causal identification of single-cell experimental perturbation effects with CINEMA-OT",
    "start_abstract":"Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
      ],
      "abstract":[
        "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "An Evaluation on the Role of Non-Coding RNA in HIV Transcription and\n  Latency: A Review",
        "Pushing the boundaries of Structure-Based Drug Design through\n  Collaboration with Large Language Models",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Progress of the anti-obesity of Berberine",
        "Applying computational protein design to therapeutic antibody discovery\n  -- current state and perspectives",
        "The standard coil or globule phases cannot describe the denatured state\n  of structured proteins and intrinsically disordered proteins",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Inverse problems with experiment-guided AlphaFold",
        "Mechanism of Electricacupuncture Treating Detrusor Bladder Neck\n  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Nucleolus Credit Assignment for Effective Coalitions in Multi-agent\n  Reinforcement Learning",
        "Mechanics and Design of Metastructured Auxetic Patches with Bio-inspired\n  Materials",
        "Rethinking High-speed Image Reconstruction Framework with Spike Camera",
        "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
        "Optimal disk packing of chloroplasts in plant cells",
        "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?",
        "Native Three-Body Interactions in a Superconducting Lattice Gauge\n  Quantum Simulator",
        "Test Schedule Generation for Acceptance Testing of Mission-Critical\n  Satellite Systems",
        "A novel approach to data generation in generative model",
        "Towards Location-Specific Precipitation Projections Using Deep Neural\n  Networks",
        "Rotatable Antenna Enabled Wireless Communication: Modeling and\n  Optimization",
        "Adapting Automatic Speech Recognition for Accented Air Traffic Control\n  Communications",
        "A spatially varying differential equation for multi-patch pandemic\n  propagation",
        "Quantification of Uncertainties in Probabilistic Deep Neural Network by\n  Implementing Boosting of Variational Inference"
      ],
      "abstract":[
        "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "The existence of latent cellular reservoirs is recognized as the major\nbarrier to an HIV cure. Reactivating and eliminating \"shock and kill\" or\npermanently silencing \"block and lock\" the latent HIV reservoir, as well as\ngene editing, remain promising approaches, but so far have proven to be only\npartially successful. Moreover, using latency reversing agents or \"block and\nlock\" drugs pose additional considerations, including the ability to cause\ncellular toxicity, a potential lack of specificity for HIV, or low potency when\neach agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long\nnon-coding RNAs (lncRNAs) are becoming increasingly recognized as important\nregulators of gene expression. RNA-based approaches for combatting HIV latency\nrepresent a promising strategy since both miRNAs and lncRNAs are more cell-type\nand tissue specific than protein coding genes. Thus, a higher specificity of\ntargeting the latent HIV reservoir with less overall cellular toxicity can\nlikely be achieved. In this review, we summarize current knowledge about HIV\ngene expression regulation by miRNAs and lncRNAs encoded in the human genome,\nas well as regulatory molecules encoded in the HIV genome. We discuss both the\ntranscriptional and post-transcriptional regulation of HIV gene expression to\nalign with the current definition of latency, and describe RNA molecules that\neither promote HIV latency or have anti-latency properties. Finally, we provide\nperspectives on using each class of RNAs as potential targets for combatting\nHIV latency, and describe the complexity of the interactions between different\nRNA molecules, their protein targets, and HIV.",
        "Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "Obesity is defined as the excessive accumulation or abnormal distribution of\nbody fat. According to data from World Obesity Atlas 2024, the increase in\nprevalence of obesity has become a major worldwide health problem in adults as\nwell as among children and adolescents. Although an increasing number of drugs\nhave been approved for the treatment of obesity in recent years, many of these\ndrugs have inevitable side effects which have increased the demand for new\nsafe, accessible and effective drugs for obesity and prompt interest in natural\nproducts. Berberine (BBR) and its metabolites, known for their multiple\npharmacological effects. Recent studies have emphatically highlighted the\nanti-obesity benefits of BBR and the underlying mechanisms have been gradually\nelucidated. However, its clinical application is limited by poor oral\nabsorption and low bioavailability. Based on this, this review summarizes\ncurrent research on the anti-obesity effects of BBR and its metabolites,\nincluding advancements in clinical trail results, understanding potential\nmolecular mechanisms and absorption and bioavailability. As a natural compound\nderived from plants, BBR holds potential as an alternative approach for\nmanaging obesity.",
        "Machine learning applications in protein sciences have ushered in a new era\nfor designing molecules in silico. Antibodies, which currently form the largest\ngroup of biologics in clinical use, stand to benefit greatly from this shift.\nDespite the proliferation of these protein design tools, their direct\napplication to antibodies is often limited by the unique structural biology of\nthese molecules. Here, we review the current computational methods for antibody\ndesign, highlighting their role in advancing computational drug discovery.",
        "The concepts of globule and random coil were developed to describe the phases\nof homopolymers and then used to characterize the denatured state of structured\ncytosolic proteins and intrinsically disordered proteins. Using multi-scale\nmolecular dynamics simulations, we were able to explore the conformational\nspace of the disordered conformations of both types of protein under biological\nconditions in an affordable amount of computational time. By studying the size\nof the protein and the density correlations in space, we conclude that the\nstandard phases of homopolymers and the tools to detect them cannot be applied\nstraightforwardly to proteins.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Objectives This study aimed to elucidate the potential mechanisms of\nelectroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)\nfollowing suprasacral spinal cord injury.\n  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned\nto either a sham group (n=12) or a spinal cord injury model group (n=40). In\nthe model group, DBND was induced in 40 rats through Hassan Shaker spinal cord\ntransection, with 24 rats surviving spinal shock and subsequently randomized\ninto two groups: a model-only group (DBND, n=12) and an EA intervention group\n(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and\nSanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10\nconsecutive days. On day 29 post-injury, all rats underwent urodynamic\nassessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag\n(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder\nneck tissues.\n  Results Urodynamic evaluation demonstrated that EA intervention enhanced\nbladder function in DBND rats. HE staining indicated reduced fibroplasia in the\ndetrusor muscle and alleviated inflammation in the bladder neck following EA.\nTMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in\nthe detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results\ncorroborated these TMT findings.\n  Conclusion EA effectively promotes synergy between the detrusor muscle and\nbladder neck in DBND, likely by enhancing detrusor contractility and\nfacilitating bladder neck relaxation during urination. This study provides\nmechanistic insights into the therapeutic role of EA in managing DBND.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "In cooperative multi-agent reinforcement learning (MARL), agents typically\nform a single grand coalition based on credit assignment to tackle a composite\ntask, often resulting in suboptimal performance. This paper proposed a\nnucleolus-based credit assignment grounded in cooperative game theory, enabling\nthe autonomous partitioning of agents into multiple small coalitions that can\neffectively identify and complete subtasks within a larger composite task.\nSpecifically, our designed nucleolus Q-learning could assign fair credits to\neach agent, and the nucleolus Q-operator provides theoretical guarantees with\ninterpretability for both learning convergence and the stability of the formed\nsmall coalitions. Through experiments on Predator-Prey and StarCraft scenarios\nacross varying difficulty levels, our approach demonstrated the emergence of\nmultiple effective coalitions during MARL training, leading to faster learning\nand superior performance in terms of win rate and cumulative rewards especially\nin hard and super-hard environments, compared to four baseline methods. Our\nnucleolus-based credit assignment showed the promise for complex composite\ntasks requiring effective subteams of agents.",
        "Metastructured auxetic patches, characterized by negative Poisson's ratios,\noffer unique mechanical properties that closely resemble the behavior of human\ntissues and organs. As a result, these patches have gained significant\nattention for their potential applications in organ repair and tissue\nregeneration. This study focuses on neural networks-based computational\nmodeling of auxetic patches with a sinusoidal metastructure fabricated from\nsilk fibroin, a bio-inspired material known for its biocompatibility and\nstrength. The primary objective of this research is to introduce a novel,\ndata-driven framework for patch design. To achieve this, we conducted\nexperimental fabrication and mechanical testing to determine material\nproperties and validate the corresponding finite element models. Finite element\nsimulations were then employed to generate the necessary data, while greedy\nsampling, an active learning technique, was utilized to reduce the\ncomputational cost associated with data labeling. Two neural networks were\ntrained to accurately predict Poisson's ratios and stresses for strains up to\n15\\%, respectively. Both models achieved $R^2$ scores exceeding 0.995, which\nindicates highly reliable predictions. Building on this, we developed a neural\nnetwork-based design model capable of tailoring patch designs to achieve\nspecific mechanical properties. This model demonstrated superior performance\nwhen compared to traditional optimization methods, such as genetic algorithms,\nby providing more efficient and precise design solutions. The proposed\nframework represents a significant advancement in the design of bio-inspired\nmetastructures for medical applications, paving the way for future innovations\nin tissue engineering and regenerative medicine.",
        "Spike cameras, as innovative neuromorphic devices, generate continuous spike\nstreams to capture high-speed scenes with lower bandwidth and higher dynamic\nrange than traditional RGB cameras. However, reconstructing high-quality images\nfrom the spike input under low-light conditions remains challenging.\nConventional learning-based methods often rely on the synthetic dataset as the\nsupervision for training. Still, these approaches falter when dealing with\nnoisy spikes fired under the low-light environment, leading to further\nperformance degradation in the real-world dataset. This phenomenon is primarily\ndue to inadequate noise modelling and the domain gap between synthetic and real\ndatasets, resulting in recovered images with unclear textures, excessive noise,\nand diminished brightness. To address these challenges, we introduce a novel\nspike-to-image reconstruction framework SpikeCLIP that goes beyond traditional\ntraining paradigms. Leveraging the CLIP model's powerful capability to align\ntext and images, we incorporate the textual description of the captured scene\nand unpaired high-quality datasets as the supervision. Our experiments on\nreal-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP\nsignificantly enhances texture details and the luminance balance of recovered\nimages. Furthermore, the reconstructed images are well-aligned with the broader\nvisual features needed for downstream tasks, ensuring more robust and versatile\nperformance in challenging environments.",
        "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
        "Photosynthesis is vital for the survival of entire ecosystems on Earth. While\nlight is fundamental to this process, excessive exposure can be detrimental to\nplant cells. Chloroplasts, the photosynthetic organelles, actively move in\nresponse to light and self-organize within the cell to tune light absorption.\nThese disk-shaped motile organelles must balance dense packing for enhanced\nlight absorption under dim conditions with spatial rearrangements to avoid\ndamage from excessive light exposure. Here, we reveal that the packing\ncharacteristics of chloroplasts within plant cells show signatures of\noptimality. Combining measurements of chloroplast densities and\nthree-dimensional cell shape in the water plant Elodea densa, we construct an\nargument for optimal cell shape versus chloroplast size to achieve two targets:\ndense packing into a two-dimensional monolayer for optimal absorption under dim\nlight conditions and packing at the sidewalls for optimal light avoidance. We\nformalize these constraints using a model for random close packing matched with\npacking simulations of polydisperse hard disks confined within rectangular\nboxes. The optimal cell shape resulting from these models corresponds closely\nto that measured in the box-like plant cells, highlighting the importance of\nparticle packing in the light adaptation of plants. Understanding the interplay\nbetween structure and function sheds light on how plants achieve efficient\nphoto adaptation. It also highlights a broader principle: how cell shape\nrelates to the optimization of packing finite and relatively small numbers of\norganelles under confinement. This universal challenge in biological systems\nshares fundamental features with the mechanics of confined granular media and\nthe jamming transitions in dense active and passive systems across various\nscales and contexts.",
        "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nexecutability of notebooks improves by 42.7% and 28% by installing the correct\nmodules and generating synthetic data. These findings challenge prior\nassumptions, suggesting that notebooks have higher executability than\npreviously reported, many of which offer valuable partial execution, and that\ntheir executability should be evaluated within the interactive notebook\nparadigm rather than through traditional software executability standards.",
        "While universal quantum computers remain under development, analog quantum\nsimulators offer a powerful alternative for understanding complex systems in\ncondensed matter, chemistry, and high-energy physics. One compelling\napplication is the characterization of real-time lattice gauge theories (LGTs).\nLGTs are nonperturbative tools, utilizing discretized spacetime to describe\ngauge-invariant models. They hold immense potential for understanding\nfundamental physics but require enforcing local constraints analogous to\nelectromagnetism's Gauss's Law. These constraints, which arise from gauge\nsymmetries and dictate the form of the interaction between matter and gauge\nfields, are a significant challenge for simulators to enforce. Implementing\nthese constraints at the hardware level in analog simulations is crucial. This\nrequires realizing multibody interactions between matter and gauge-field\nelements, enabling them to evolve together while suppressing unwanted two-body\ninteractions that violate the gauge symmetry. In this paper, we propose and\nimplement a novel parametrically activated three-qubit interaction within a\ncircuit quantum electrodynamics architecture. We experimentally demonstrate a\nminimal $U(1)$ spin-1\/2 model with a time evolution that intrinsically\nsatisfies Gauss's law in the system. This design serves as the foundational\nblock for simulating LGTs on a superconducting photonic lattice.",
        "Mission-critical system, such as satellite systems, healthcare systems, and\nnuclear power plant control systems, undergo rigorous testing to ensure they\nmeet specific operational requirements throughout their operation. This\nincludes Operational Acceptance Testing (OAT), which aims to ensure that the\nsystem functions correctly under real-world operational conditions. In\nsatellite development, In-Orbit Testing (IOT) is a crucial OAT activity\nperformed regularly and as needed after deployment in orbit to check the\nsatellite's performance and ensure that operational requirements are met. The\nscheduling of an IOT campaign, which executes multiple IOT procedures, is an\nimportant yet challenging problem, as it accounts for various factors,\nincluding satellite visibility, antenna usage costs, testing time periods, and\noperational constraints. To address the IOT scheduling problem, we propose a\nmulti-objective approach to generate near-optimal IOT schedules, accounting for\noperational costs, fragmentation (i.e., the splitting of tests), and resource\nefficiency, which align with practitioners' objectives for IOT scheduling. Our\nindustrial case study with SES Techcom shows significant improvements, as\nfollows: an average improvement of 49.4% in the cost objective, 60.4% in the\nfragmentation objective, and 30% in the resource usage objective, compared to\nour baselines. Additionally, our approach improves cost efficiency by 538% and\nresource usage efficiency by 39.42% compared to manually constructed schedules\nprovided by practitioners, while requiring only 12.5% of the time needed for\nmanual IOT scheduling.",
        "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.",
        "Accurate precipitation estimates at individual locations are crucial for\nweather forecasting and spatial analysis. This study presents a paradigm shift\nby leveraging Deep Neural Networks (DNNs) to surpass traditional methods like\nKriging for station-specific precipitation approximation. We propose two\ninnovative NN architectures: one utilizing precipitation, elevation, and\nlocation, and another incorporating additional meteorological parameters like\nhumidity, temperature, and wind speed. Trained on a vast dataset (1980-2019),\nthese models outperform Kriging across various evaluation metrics (correlation\ncoefficient, root mean square error, bias, and skill score) on a five-year\nvalidation set. This compelling evidence demonstrates the transformative power\nof deep learning for spatial prediction, offering a robust and precise\nalternative for station-specific precipitation estimation.",
        "Fluid antenna system (FAS) and movable antenna (MA) have recently emerged as\npromising technologies to exploit new spatial degrees of freedom (DoFs), which\nhave attracted growing attention in wireless communication. In this paper, we\npropose a new rotatable antenna (RA) model to improve the performance of\nwireless communication systems. Different from conventional fixed antennas, the\nproposed RA system can flexibly alter the three-dimensional (3D) boresight\ndirection of each antenna independently by adjusting its deflection angles to\nachieve a desired array directional gain pattern. Specifically, we investigate\nan RA-enabled uplink communication system, where the receive beamforming and\nthe deflection angles of all RAs at the base station (BS) are jointly optimized\nto maximize the minimum signal-to-interference-plus-noise ratio (SINR) among\nall the users. In the special single-user and free-space propagation setup, the\noptimal deflection angles of RAs are derived in closed form with the\nmaximum-ratio combining (MRC) beamformer applied at the BS. Moreover, we\nanalyze the asymptotic performance with an infinite number of antennas based on\nthis solution, which theoretically proves that the RA system can achieve a\nhigher array gain as compared to the fixed-antenna system. In the general\nmulti-user and multi-path channel setup, we first propose an alternating\noptimization (AO) algorithm to alternately optimize the receive beamforming and\nthe deflection angles of RAs in an iterative manner. Then, a two-stage\nalgorithm that solves the formulated problem without the need for iteration is\nfurther proposed to reduce computational complexity. Simulation results are\nprovided to validate our analytical results and demonstrate that the proposed\nRA system can significantly outperform other benchmark schemes.",
        "Effective communication in Air Traffic Control (ATC) is critical to\nmaintaining aviation safety, yet the challenges posed by accented English\nremain largely unaddressed in Automatic Speech Recognition (ASR) systems.\nExisting models struggle with transcription accuracy for Southeast\nAsian-accented (SEA-accented) speech, particularly in noisy ATC environments.\nThis study presents the development of ASR models fine-tuned specifically for\nSoutheast Asian accents using a newly created dataset. Our research achieves\nsignificant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82%\non SEA-accented ATC speech. Additionally, the paper highlights the importance\nof region-specific datasets and accent-focused training, offering a pathway for\ndeploying ASR systems in resource-constrained military operations. The findings\nemphasize the need for noise-robust training techniques and region-specific\ndatasets to improve transcription accuracy for non-Western accents in ATC\ncommunications.",
        "We develop an extension of the Susceptible-Infected-Recovery (SIR) model to\naccount for spatial variations in population as well as infection and recovery\nparameters. The equations are derived by taking the continuum limit of discrete\ninteracting patches, and results in a diffusion equation with some nonlinear\nterms. The resulting population dynamics can be reinterpreted as a nonlinear\nheat flow equation where the temperature vector captures both infected and\nrecovered populations across multiple patches.",
        "Modern neural network architectures have achieved remarkable accuracies but\nremain highly dependent on their training data, often lacking interpretability\nin their learned mappings. While effective on large datasets, they tend to\noverfit on smaller ones. Probabilistic neural networks, such as those utilizing\nvariational inference, address this limitation by incorporating uncertainty\nestimation through weight distributions rather than point estimates. However,\nstandard variational inference often relies on a single-density approximation,\nwhich can lead to poor posterior estimates and hinder model performance. We\npropose Boosted Bayesian Neural Networks (BBNN), a novel approach that enhances\nneural network weight distribution approximations using Boosting Variational\nInference (BVI). By iteratively constructing a mixture of densities, BVI\nexpands the approximating family, enabling a more expressive posterior that\nleads to improved generalization and uncertainty estimation. While this\napproach increases computational complexity, it significantly enhances accuracy\nan essential tradeoff, particularly in high-stakes applications such as medical\ndiagnostics, where false negatives can have severe consequences. Our\nexperimental results demonstrate that BBNN achieves ~5% higher accuracy\ncompared to conventional neural networks while providing superior uncertainty\nquantification. This improvement highlights the effectiveness of leveraging a\nmixture-based variational family to better approximate the posterior\ndistribution, ultimately advancing probabilistic deep learning."
      ]
    }
  },
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"A 2021 update on cancer image analytics with deep learning",
    "start_abstract":"Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
      ],
      "abstract":[
        "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MMCR: Advancing Visual Language Model in Multimodal Multi-Turn\n  Contextual Reasoning",
        "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
        "Governing AI Agents",
        "Saarthi: The First AI Formal Verification Engineer",
        "Think Smarter not Harder: Adaptive Reasoning with Inference Aware\n  Optimization",
        "Observer-Aware Probabilistic Planning Under Partial Observability",
        "PairVDN - Pair-wise Decomposed Value Functions",
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "How Well Can AI Build SD Models?",
        "ASKCOS: an open source software suite for synthesis planning",
        "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
        "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning",
        "Analyzing Advanced AI Systems Against Definitions of Life and\n  Consciousness",
        "SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered\n  CPUs",
        "VarGes: Improving Variation in Co-Speech 3D Gesture Generation via\n  StyleCLIPS",
        "Thermodynamics of driven systems with explicitly broken detailed balance",
        "How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines",
        "Charge symmetry breaking in hypernuclei within RMF model",
        "The nonlinear limit of Babinet's Principle",
        "Structural and optical properties of in situ Eu-doped ZnCdO\/ZnMgO\n  superlattices grown by plasma-assisted molecular beam epitaxy",
        "Accenture-NVS1: A Novel View Synthesis Dataset",
        "CANUCS\/Technicolor: JWST Medium Band Photometry Finds Half of the Star\n  Formation at $z>7.5$ is Obscured",
        "Sparse wavefield reconstruction and denoising with boostlets",
        "Theoretical study of the Spectroscopic measurements of Kerr non-linear\n  resonators with four-body interaction",
        "Energy dynamics in a class of local random matrix Hamiltonians",
        "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs",
        "Pointwise estimates for the fundamental solutions of higher order\n  schr\\\"{o}dinger equations with finite rank perturbations",
        "3D ReX: Causal Explanations in 3D Neuroimaging Classification"
      ],
      "abstract":[
        "Compared to single-turn dialogue, multi-turn dialogue involving multiple\nimages better aligns with the needs of real-world human-AI interactions.\nAdditionally, as training data, it provides richer contextual reasoning\ninformation, thereby guiding the model to achieve better performance. However,\nexisting vision-language models (VLMs) primarily rely on single-turn dialogue\ntraining and evaluation benchmarks. In this paper, following the\ncharacteristics of human dialogue, such as focused topics and concise, clear\ncontent, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel\ndataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn\ninstruction tuning dataset with 310K contextual dialogues, each covering 1-4\nimages and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark\nfeaturing dialogues, spanning 8 domains (Humanities, Natural, Science,\nEducation, etc.) and 40 sub-topics. Extensive evaluations demonstrate that\nmodels fine-tuned with MMCR-310k achieve 5.2\\% higher contextual accuracy on\nMMCR-Bench, while showing consistent improvements on existing benchmarks\n(+1.1\\% on AI2D, +1.2\\% on MMMU and MMVet). MMCR and prompt engineering will be\nreleased publicly.",
        "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
        "The field of AI is undergoing a fundamental transition from generative models\nthat can produce synthetic content to artificial agents that can plan and\nexecute complex tasks with only limited human involvement. Companies that\npioneered the development of language models have now built AI agents that can\nindependently navigate the internet, perform a wide range of online tasks, and\nincreasingly serve as AI personal assistants and virtual coworkers. The\nopportunities presented by this new technology are tremendous, as are the\nassociated risks. Fortunately, there exist robust analytic frameworks for\nconfronting many of these challenges, namely, the economic theory of\nprincipal-agent problems and the common law doctrine of agency relationships.\nDrawing on these frameworks, this Article makes three contributions. First, it\nuses agency law and theory to identify and characterize problems arising from\nAI agents, including issues of information asymmetry, discretionary authority,\nand loyalty. Second, it illustrates the limitations of conventional solutions\nto agency problems: incentive design, monitoring, and enforcement might not be\neffective for governing AI agents that make uninterpretable decisions and\noperate at unprecedented speed and scale. Third, the Article explores the\nimplications of agency law and theory for designing and regulating AI agents,\narguing that new technical and legal infrastructure is needed to support\ngovernance principles of inclusivity, visibility, and liability.",
        "Recently, Devin has made a significant buzz in the Artificial Intelligence\n(AI) community as the world's first fully autonomous AI software engineer,\ncapable of independently developing software code. Devin uses the concept of\nagentic workflow in Generative AI (GenAI), which empowers AI agents to engage\nin a more dynamic, iterative, and self-reflective process. In this paper, we\npresent a similar fully autonomous AI formal verification engineer, Saarthi,\ncapable of verifying a given RTL design end-to-end using an agentic workflow.\nWith Saarthi, verification engineers can focus on more complex problems, and\nverification teams can strive for more ambitious goals. The domain-agnostic\nimplementation of Saarthi makes it scalable for use across various domains such\nas RTL design, UVM-based verification, and others.",
        "Solving mathematics problems has been an intriguing capability of large\nlanguage models, and many efforts have been made to improve reasoning by\nextending reasoning length, such as through self-correction and extensive long\nchain-of-thoughts. While promising in problem-solving, advanced long reasoning\nchain models exhibit an undesired single-modal behavior, where trivial\nquestions require unnecessarily tedious long chains of thought. In this work,\nwe propose a way to allow models to be aware of inference budgets by\nformulating it as utility maximization with respect to an inference budget\nconstraint, hence naming our algorithm Inference Budget-Constrained Policy\nOptimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to\n``understand'' the difficulty of queries and allocate inference budgets to\nharder ones. With different inference budgets, our best models are able to have\na $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative\nimprovement) on MATH500 using $2.16$x and $4.32$x inference budgets\nrespectively, relative to LLaMA3.1 8B Instruct. These improvements are\napproximately $2$x those of self-consistency under the same budgets.",
        "In this article, we are interested in planning problems where the agent is\naware of the presence of an observer, and where this observer is in a partial\nobservability situation. The agent has to choose its strategy so as to optimize\nthe information transmitted by observations. Building on observer-aware Markov\ndecision processes (OAMDPs), we propose a framework to handle this type of\nproblems and thus formalize properties such as legibility, explicability and\npredictability. This extension of OAMDPs to partial observability can not only\nhandle more realistic problems, but also permits considering dynamic hidden\nvariables of interest. These dynamic target variables allow, for instance,\nworking with predictability, or with legibility problems where the goal might\nchange during execution. We discuss theoretical properties of PO-OAMDPs and,\nexperimenting with benchmark problems, we analyze HSVI's convergence behavior\nwith dedicated initializations and study the resulting strategies.",
        "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https:\/\/github.com\/zzbuzzard\/PairVDN.",
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
        "The advancement of machine learning and the availability of large-scale\nreaction datasets have accelerated the development of data-driven models for\ncomputer-aided synthesis planning (CASP) in the past decade. Here, we detail\nthe newest version of ASKCOS, an open source software suite for synthesis\nplanning that makes available several research advances in a freely available,\npractical tool. Four one-step retrosynthesis models form the basis of both\ninteractive planning and automatic planning modes. Retrosynthetic planning is\ncomplemented by other modules for feasibility assessment and pathway\nevaluation, including reaction condition recommendation, reaction outcome\nprediction, and auxiliary capabilities such as solubility prediction and\nquantum mechanical descriptor prediction. ASKCOS has assisted hundreds of\nmedicinal, synthetic, and process chemists in their day-to-day tasks,\ncomplementing expert decision making. It is our belief that CASP tools like\nASKCOS are an important part of modern chemistry research, and that they offer\never-increasing utility and accessibility.",
        "Handcrafting heuristics for solving complex optimization tasks (e.g., route\nplanning and task allocation) is a common practice but requires extensive\ndomain knowledge. Recently, Large Language Model (LLM)-based automatic\nheuristic design (AHD) methods have shown promise in generating high-quality\nheuristics without manual interventions. Existing LLM-based AHD methods employ\na population to maintain a fixed number of top-performing LLM-generated\nheuristics and introduce evolutionary computation (EC) to iteratively enhance\nthe population. However, these population-based procedures cannot fully develop\nthe potential of each heuristic and are prone to converge into local optima. To\nmore comprehensively explore the space of heuristics, this paper proposes to\nuse Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The\nproposed MCTS-AHD method organizes all LLM-generated heuristics in a tree\nstructure and can better develop the potential of temporarily underperforming\nheuristics. In experiments, MCTS-AHD delivers significantly higher-quality\nheuristics on various complex tasks. Our code is available.",
        "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market.",
        "Could artificial intelligence ever become truly conscious in a functional\nsense; this paper explores that open-ended question through the lens of Life, a\nconcept unifying classical biological criteria (Oxford, NASA, Koshland) with\nempirical hallmarks such as adaptive self maintenance, emergent complexity, and\nrudimentary self referential modeling. We propose a number of metrics for\nexamining whether an advanced AI system has gained consciousness, while\nemphasizing that we do not claim all AI stems can become conscious. Rather, we\nsuggest that sufficiently advanced architectures exhibiting immune like\nsabotage defenses, mirror self-recognition analogs, or meta-cognitive updates\nmay cross key thresholds akin to life-like or consciousness-like traits. To\ndemonstrate these ideas, we start by assessing adaptive self-maintenance\ncapability, and introduce controlled data corruption sabotage into the training\nprocess. The result demonstrates AI capability to detect these inconsistencies\nand revert or self-correct analogous to regenerative biological processes. We\nalso adapt an animal-inspired mirror self recognition test to neural\nembeddings, finding that partially trained CNNs can distinguish self from\nforeign features with complete accuracy. We then extend our analysis by\nperforming a question-based mirror test on five state-of-the-art chatbots\n(ChatGPT4, Gemini, Perplexity, Claude, and Copilot) and demonstrated their\nability to recognize their own answers compared to those of the other chatbots.",
        "Large language models have high compute, latency, and memory requirements.\nWhile specialized accelerators such as GPUs and TPUs typically run these\nworkloads, CPUs are more widely available and consume less energy. Accelerating\nLLMs with CPUs enables broader AI access at a lower cost and power consumption.\nThis acceleration potential for CPUs is especially relevant during the\nmemory-bound decoding stage of LLM inference, which processes one token at a\ntime and is becoming increasingly utilized with reasoning models. We utilize\nAdvanced Matrix Extensions (AMX) support on the latest Intel CPUs together with\nunstructured sparsity to achieve a $1.42 \\times$ reduction in end-to-end\nlatency compared to the current PyTorch implementation by applying our\ntechnique in linear layers. We provide a set of open-source customized sparse\nkernels that can speed up any PyTorch model by automatically replacing all\nlinear layers with our custom sparse implementation. Furthermore, we\ndemonstrate for the first time the use of unstructured sparsity in the\nattention computation achieving a $1.14 \\times$ speedup over the current\nsystems without compromising accuracy. Code:\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning\/tree\/main\/SparAMX",
        "Generating expressive and diverse human gestures from audio is crucial in\nfields like human-computer interaction, virtual reality, and animation. Though\nexisting methods have achieved remarkable performance, they often exhibit\nlimitations due to constrained dataset diversity and the restricted amount of\ninformation derived from audio inputs. To address these challenges, we present\nVarGes, a novel variation-driven framework designed to enhance co-speech\ngesture generation by integrating visual stylistic cues while maintaining\nnaturalness. Our approach begins with the Variation-Enhanced Feature Extraction\n(VEFE) module, which seamlessly incorporates \\textcolor{blue}{style-reference}\nvideo data into a 3D human pose estimation network to extract StyleCLIPS,\nthereby enriching the input with stylistic information. Subsequently, we employ\nthe Variation-Compensation Style Encoder (VCSE), a transformer-style encoder\nequipped with an additive attention mechanism pooling layer, to robustly encode\ndiverse StyleCLIPS representations and effectively manage stylistic variations.\nFinally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio\nfeatures with StyleCLIPS encodings via cross-attention, injecting this fused\ndata into a cross-conditional autoregressive model to modulate 3D human gesture\ngeneration based on audio input and stylistic clues. The efficacy of our\napproach is validated on benchmark datasets, where it outperforms existing\nmethods in terms of gesture diversity and naturalness. The code and video\nresults will be made publicly available upon\nacceptance:https:\/\/github.com\/mookerr\/VarGES\/ .",
        "In systems with detailed balance, the stationary distribution and the\nequilibrium distribution are identical, creating a clear connection between\nenergetic and entropic quantities. Many driven systems violate detailed balance\nand still pose a challenge for a consistent thermodynamic interpretation. Even\nsteady-state potentials like entropy or free energy are no longer state\nvariables. Here, we use a framework for systems with broken detailed balance,\nwhere Boltzmann entropy can be computed while properly taking constraints on\nstate transitions into account. As an illustration, we establish the\nthermodynamic relations for arbitrarily driven sample space-reducing processes\nthat are non-equilibrium but show steady states. We demonstrate that, despite\nexplicitly broken detailed balance, it remains feasible to define and\nunambiguously interpret the effective thermodynamic potentials.",
        "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.",
        "We study the charge symmetry breaking (CSB) effect in the binding energy of\nmirror hypernuclei in the mass region $A=7\\sim 48$ in relativistic mean field\n(RMF) models introducing $NN$ and $\\Lambda N$ interactions. The\nphenomenological $\\Lambda N$ CSB interaction is introduced and the strength\nparameter is fitted to reproduce the experimental binding energy difference\nbetween the mirror hypernuclei $^{12}_\\Lambda$B and $^{12}_\\Lambda$C. This\nmodel is applied to calculate the CSB energy anomaly in mirror hypernuclei with\nthe mass $A=7\\sim48$. The model is further applied to predict the binding\nenergy difference of mirror hypernuclei of $A$=40 with the isospin $T=1\/2$,\n$3\/2$ and $5\/2$ nuclei together with various hyper Ca isotopes and their mirror\nhypernuclei. Finally the binding energy systematics of $A=$48 hypernuclei are\npredicted with\/without the CSB effect by the PK1 and TM2 energy density\nfunctionals (EDFs).",
        "Babinet's principle is a powerful tool for predicting the scattering behavior\nof planar structures where the solution for the complementary structure is\nalready known. This makes it ubiquitous in the design of aperture antennas or\nmetamaterials. Even for plasmonic nanostructures, a qualitative match of the\nbehavior for complementary structures has been reported. Here, we discuss\nwhether Babinet's principle can be extended to nonlinear scattering. We compare\nthe third harmonic emission of plasmonic nanorods and complementary nanoslits\nby far field imaging and simulation. We find significantly different far field\nimages, in agreement between experiment and simulation. We explain these\ndifferences by the higher spatial resolution at the third harmonic wavelength\nand by additional eddy currents in slits that are not present in rods. Within\nthese limits, Babinet's principle can guide the design of inverted nonlinear\nplasmonic resonators, which promise to be more stable at high excitation power\ndue to better thermal conductivity.",
        "In situ Eu-doped ZnCdO-ZnMgO superlattices with varying ZnCdO:Eu and ZnMgO\nsublayers thicknesses were deposited by plasma assisted molecular beam epitaxy.",
        "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.",
        "We present a sample of 146 high-redshift ($z>7.5$) galaxies from the\nCANUCS\/Technicolor surveys, showcasing photometry in every wide- and\nmedium-band NIRCam filter in addition to ancillary HST data sampling $0.4-5 \\mu\nm$ (22 JWST bands out of 29 bands total). Additionally, 48 ($33\\%$) galaxies in\nour sample meet criteria to be classified as extreme emission line galaxies, 15\n($10\\%$) of which are completely missed by typical dropout selections due to\nfaint UV emission. By fitting the SEDs covering the rest-frame UV to optical at\n$z > 7.5$, we investigate the dust obscuration properties, giving an unbiased\nview of dust buildup in high-redshift galaxies free from spectroscopic\nfollow-up selection effects. Dust attenuation correlates with stellar mass, but\nmore strongly with star formation rate. We find typical galaxies at $z>7.5$\nhave $\\sim 25 \\%$ of their star formation obscured. However, since galaxies\nwith higher star formation rates suffer more attenuation, $\\sim 50 \\%$ of the\ntotal star formation rate density at $7.5<z<9$ is obscured. The obscured\nfraction drops to $\\sim 35 \\%$ in our $9<z<12$ bin, possibly due to substantial\ndust grain growth in the interstellar medium not having time to occur.\nExtrapolating the decline in dust obscuration of galaxies to higher redshifts,\nwe infer that dust obscuration should approach zero at $z > 15$, implying that\nepoch as when dust first forms in bright galaxies.",
        "Boostlets are spatiotemporal functions that decompose nondispersive\nwavefields into a collection of localized waveforms parametrized by dilations,\nhyperbolic rotations, and translations. We study the sparsity properties of\nboostlets and find that the resulting decompositions are significantly sparser\nthan those of other state-of-the-art representation systems, such as wavelets\nand shearlets. This translates into improved denoising performance when\nhard-thresholding the boostlet coefficients. The results suggest that boostlets\noffer a natural framework for sparsely decomposing wavefields in unified\nspace-time.",
        "Quantum annealing provides a promising way to solve combinational\noptimization problems where the solutions correspond to the ground state of the\nIsing Hamiltonian. We can implement quantum annealing using the Kerr non-linear\nresonators, with bifurcation phenomena emerging when subjected to a parametric\ndrive. These bifurcated states can function as bases of qubits. Moreover,\nintegrating four-body interactions between physical qubits enables the\nestablishment of effective all-to-all long-range interactions between logical\nqubits, which is essential for practical quantum annealing. While theoretical\nproposals exist for creating four-body interactions within Kerr non-linear\nresonators, there has not been experimental verification through their\nspectroscopic signatures. In this paper, we theoretically investigate the\nspectroscopic measurements of Kerr non-linear resonators featuring four-body\ninteraction. We identify six distinct frequencies exhibiting population changes\nby employing resonant driving on one resonator and weak driving on another.\nAnalytical and numerical calculations validate these findings. Our study\ndemonstrates the potential of spectroscopy in characterizing systems with\nfour-body interactions, offering insights for realizing quantum annealing with\nKerr parametric oscillators.",
        "Random matrix theory yields valuable insights into the universal features of\nquantum many-body chaotic systems. Although all-to-all interactions are\ntraditionally studied, many interesting dynamical questions, such as transport\nof a conserved density, require a notion of spatially local interactions. We\nstudy the transport of the energy, the most basic conserved density, in\nfew-body and 1D chains of nearest-neighbor random matrix terms that square to\none. In the few-body but large local Hilbert space dimension case, we develop a\nmapping for the energy dynamics to a single-particle hopping picture. This\nallows for the computation of the energy density autocorrelators and an\nout-of-time-ordered correlator of the energy density. In the 1D chain, we\nnumerically study the energy transport for a small local Hilbert space\ndimension. We also discuss the density of states throughout and touch upon the\nrelation to free probability theory.",
        "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies.",
        "This paper is dedicated to studying pointwise estimates of the fundamental\nsolution for the higher order Schr\\\"{o}dinger equation: % we investigate the\nfundamental solution of the higher order Schr\\\"{o}dinger equation\n$$i{\\partial}_{t}u(x,t)=Hu(x,t),\\ \\ \\ t\\in \\mathbb{R},\\ x\\in\n{\\mathbb{R}}^{n},$$ where the Hamiltonian $H$ is defined as\n$$H={(-\\Delta)}^{m}+\\displaystyle\\sum_{j=1}^{N} \\langle\\cdotp ,{\\varphi }_{j}\n\\rangle{\\varphi }_{j},$$ with each $\\varphi_j$ ($1\\le j\\le N$) satisfying\ncertain smoothness and decay conditions. %Let ${P}_{ac}(H)$ denote the\nprojection onto the absolutely continuous space of $H$. We show that for any\npositive integer $m>1$ and spatial dimension $n\\ge 1$, %under a spectral\nassumption, the operator is sharp in the sense that it\n  ${e}^{-i tH}P_{ac}(H)$ has an integral kernel $K(t,x,y)$ satisfying the\nfollowing pointwise estimate: $$\\left |K(t,x,y)\\right |\\lesssim\n|t|^{-\\frac{n}{2m}}(1+|t|^{-\\frac{1}{2m}}\\left | x-y\\right\n|)^{-\\frac{n(m-1)}{2m-1}} ,\\ \\ t\\ne 0,\\ x,y\\in {\\mathbb{R}}^{n}.$$ This\nestimate is consistent with the upper bounds in the free case. As an\napplication, we derive $L^p-L^q$ decay estimates for the propagator ${e}^{-\\i\ntH}P_{ac}(H)$, where the pairs $(1\/p, 1\/q)$ lie within a quadrilateral region\nin the plane.",
        "Explainability remains a significant problem for AI models in medical\nimaging, making it challenging for clinicians to trust AI-driven predictions.\nWe introduce 3D ReX, the first causality-based post-hoc explainability tool for\n3D models. 3D ReX uses the theory of actual causality to generate\nresponsibility maps which highlight the regions most crucial to the model's\ndecision. We test 3D ReX on a stroke detection model, providing insight into\nthe spatial distribution of features relevant to stroke."
      ]
    }
  }
]