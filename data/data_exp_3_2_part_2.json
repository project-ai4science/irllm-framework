[
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Variable Imaging Projection Cloud Scattering Tomography",
    "start_abstract":"Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Distributed Sky Imaging Radiometry and Tomography"
      ],
      "abstract":[
        "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
      ],
      "categories":[
        "astro-ph.EP"
      ]
    },
    "list":{
      "title":[
        "Prediction and observation of a stellar occultation by Haumea's\n  satellite Namaka",
        "Polar circumtriple planets and disks around misaligned hierarchical\n  triple stars",
        "Smuggling unnoticed: Towards a 2D view of water and dust delivery to the\n  inner regions of protoplanetary discs",
        "The Interplay between Dust Dynamics and Turbulence Induced by the\n  Vertical Shear Instability",
        "Three-dimensional transport of solids in a protoplanetary disk\n  containing a growing giant planet",
        "On the road to the radius valley: distinguishing between gas dwarfs and\n  water worlds with young transiting exoplanets",
        "The coexistence of the streaming instability and the vertical shear\n  instability in protoplanetary disks: Scale-dependence of dust diffusion",
        "An Energy-Angular Momentum Phase Function for Rubble Pile Asteroids",
        "Activity of comet 7P\/Pons-Winnecke during the 2021 apparition",
        "Revisiting the multi-planetary system of the nearby star HD 20794:\n  Confirmation of a low-mass planet in the habitable zone of a nearby G-dwarf",
        "Precise Parameters from Bayesian SED Fitting Indicate Thermally-Driven\n  Mass Loss Likely Driver of Radius Valley",
        "TOI-6324b: An Earth-Mass Ultra-Short-Period Planet Transiting a Nearby M\n  Dwarf",
        "Stellar occultation observations of (38628) Huya and its satellite: a\n  detailed look into the system",
        "Signless Laplacian State Transfer on Vertex Complemented Coronae",
        "Testing and Combining Transient Spectral Classification Tools on\n  4MOST-like Blended Spectra",
        "Highly Entangled Magnetodielectric and Magnetostriction effects, and\n  Spin-Phonon coupling in the Antiferromagnetic Ni$_2$ScSbO$_6$",
        "Sky localization of gravitational waves from eccentric binaries",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "J-braid groups are torus necklace groups",
        "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
        "Revealing higher-order neural representations with generative artificial\n  intelligence",
        "Are compact open-charm tetraquarks consistent with recent lattice\n  results?",
        "Partial Condition Numbers for Double Saddle Point Problems",
        "ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish",
        "Regularity for free boundary surfaces minimizing degenerate area\n  functionals",
        "A parameterization method for quasi-periodic systems with noise:\n  computation of random invariant tori",
        "Really perverse periodic solutions of the planar N-body problem",
        "Topological superconductivity in hourglass Dirac chain metals (Ti,\n  Hf)IrGe"
      ],
      "abstract":[
        "Stellar occultations are an ideal way to characterize the physical and\norbital properties of trans-Neptunian binary systems. In this research note, we\ndetail the prediction and observation of a stellar occultation observed with\nNASA's IRTF on March 16$^{\\mathrm{th}}$, 2025 (UT), with drop-outs from both\nthe dwarf planet Haumea and its smaller satellite Namaka. This occultation\nplaces a lower limit of 83 $\\pm$ 2 km on Namaka's diameter. We also discuss the\npossibility that this detection could help to constrain the orbit of Namaka,\nmeasure Haumea's gravitational harmonics, and provide a path to measuring the\ninternal structure of Haumea.",
        "Observations of hierarchical triple star systems show that misalignments are\ncommon both between the angular momentum vector of the inner binary and the\nouter companion orbit, and between the outer binary orbit and a circumtriple\ngas disk. With analytic methods and n-body simulations we explore the dynamics\nof circumtriple orbits around a misaligned hierarchical triple star.\nCircumtriple test particle orbits nodally precess either about the outer binary\nangular momentum vector (circulating orbits) or about a stationary inclination\nthat depends upon the binary properties (librating orbits). For a coplanar (or\nretrograde coplanar) triple star, the apsidal precession rate is maximal and\nthe critical orbital radius outside of which all orbits are circulating is\nminimal. Polar alignment of a circumtriple gas disk requires nodal libration\nand therefore it can be more likely if there is a large misalignment between\nthe inner and outer binary orbits. There are two values of the mutual\nmisalignment, i_c and 180-i_c, for which the apsidal precession rate of the\ntriple star is zero and polar alignment is possible at all orbital radii. For a\ncircular inner binary orbit i_c=55, and it changes with eccentricity of the\ninner binary while being insensitive to other triple star parameters.",
        "Infrared spectroscopy, e.g., with JWST, provides a glimpse into the chemical\ninventory of the innermost region of protoplanetary discs, where terrestrial\nplanets eventually form. The chemical make-up of regions inside snowlines is\nconnected to the material drifting from the outer regions, which can be modeled\nwith dust evolution models. However, infrared observations are limited by the\nhigh dust extinction in the inner disc, and only probes the abundances of\ngaseous species in the disc surface layers. As a result, the bulk mass of\ndelivered volatiles is not directly relatable to what is measured through\ninfrared spectra. In this paper, we investigate how the delivery of dust and\nice after prolonged pebble drift affects the observable reservoir of water\nvapor in the inner disc. We develop a 1+1D approach based on dust evolution\nmodels to determine the delivery and distribution of vapor compared to the\nheight of the $\\tau = 1$ surface in the dust continuum. We find that the\nobservable column density of water vapor at wavelengths probed by JWST spans\nmany orders of magnitude over time, exhibiting different radial profiles\ndepending on dust properties, drift rate, and local processing. In the presence\nof a traffic-jam effect inside the snowline, the observable vapor reservoir\nappears constant in time despite the ongoing delivery by pebble drift, such\nthat water is effectively smuggled unnoticed. Differences in measured column\ndensities then originate not only from variations in bulk vapor content, but\nalso from differences in the properties and distribution of dust particles.",
        "The interaction between gas and dust in protoplanetary disks (PPDs) plays a\ncrucial role in setting the stage of planet formation. In particular, the\nstreaming instability (SI) is well recognized as the mechanism for planetesimal\nformation out of this interaction. The outer region of PPDs is likely subject\nto the vertical shear instability (VSI), representing a major source of disk\nturbulence characterized by vertical corrugation that leads to strong dust\nstirring. In the meantime, the VSI turbulence in 3D generates vortices through\nthe Rossby wave instability (RWI), which can trap dust and thereby promote dust\nconcentration. In this study, we use the multifluid dust module in Athena++ to\nconduct 2D axisymmetric global simulations of PPDs with mesh refinement and 3D\nglobal simulations with modest resolution. In 2D, the VSI corrugation mode is\nweakened by dust back-reaction, while the SI can still survive regardless of\ninitial conditions. Dust clumping occurs and is seeded by VSI-induced zonal\nflows. In 3D, dust can settle even more with increased dusty buoyancy,\nsuppressing the VSI corrugation mode. Meanwhile, dust back-reaction enhances\ndust concentration in RWI vortices, though higher resolution is needed to\nassess dust clumping.",
        "We present the results of combined hydrodynamic and particle tracking\npost-processing modeling to study the transport of small dust in a\nprotoplanetary disk containing an embedded embryo in 3D. We use a suite of\nFARGO3D hydrodynamic simulations of disks containing a planetary embryo varying\nin mass up to 300 $M_\\oplus$ on a fixed orbit in both high and low viscosity\ndisks. We then simulate solid particles through the disk as a post-processing\nstep using a Monte Carlo integration, allowing us to track the trajectories of\nindividual particles as they travel throughout the disk. We find that gas\nadvection onto the planet can carry small, well-coupled solids across the gap\nopened in the disk by the embedded planet for planetary masses above the pebble\nisolation mass. This mixing between the inner and outer disk can occur in both\ndirections, with solids in the inner disk mixing to the outer disk as well.\nAdditionally, in low viscosity disks, multiple pile-ups in the outer disk may\npreserve isotopic heterogeneities, possibly providing an outermost tertiary\nisotopic reservoir. Throughout Jupiter's growth, the extent of mixing between\nisotopic reservoirs varied depending on dust size, gas turbulence, and the\nJovian embryo mass.",
        "The detection of young transiting exoplanets represents a new frontier in our\nunderstanding of planet formation and evolution. For the population of observed\nclose-in sub-Neptunes, two proposed formation pathways can reproduce their\nobserved masses and radii at $\\sim$Gyr ages: the \"gas dwarf\" hypothesis and the\n\"water world\" hypothesis. We show that a sub-Neptune's size at early ages\n$\\lesssim 100$ Myrs is strongly dependent on the bulk mean molecular weight\nwithin its envelope. As a result, gas dwarfs and water worlds should diverge in\nsize at early ages since the mean molecular weight of gas dwarf envelopes is\npredicted to be smaller than that of water worlds. We construct population\nmodels under both scenarios that reproduce Kepler demographics in the age range\n$\\sim1-10$ Gyrs. We find tentative evidence that the gas dwarf model is more\nconsistent with the small population of young exoplanets $< 40$ Myrs from TESS.\nWe show that planet radius is relatively insensitive to planet mass for young,\npuffy sub-Neptunes, meaning that well-characterised masses are not necessarily\nrequired to exploit the effects of mean molecular weight at the population\nlevel. We confirm the predicted difference in planet size between the models is\nalso true under mixed-envelope scenarios, in which envelopes consist of\nmixtures of hydrogen and steam. We highlight that transit surveys of young\nexoplanets should target the youngest observable stellar clusters to exploit\nthe effects of mean molecular weight.",
        "The vertical shear instability and the streaming instability are two robust\nsources of turbulence in protoplanetary disks. The former has been found to\ninduce anisotropic turbulence that is stronger in the vertical than in the\nradial dimension and to be overall stronger compared to the largely isotropic\nturbulence caused by the streaming instability. In this study, we shed light on\nthe dust diffusion by the vertical shear instability and the streaming\ninstability separately and together, and in particular on the direction- and\nscale-dependence of the diffusion. To this end, we employ two-dimensional\nglobal models of the two instabilities either in isolation or in combination.\nThe vertical shear instability in isolation diffuses dust more strongly in the\nvertical direction than the streaming instability in isolation, resulting in a\nwave-shaped dust layer in our two-dimensional simulations. Compared with this\nlarge-scale diffusion, though, our study highlights that the vertical shear\ninstability causes substantially weaker or even negligible small-scale\ndiffusion. We validate this result using previously published three-dimensional\nsimulations. In particular when simulating centimetre-sized dust, the\nundulating dust layer becomes internally razor-thin. In contrast, the diffusion\nowing to the streaming instability exhibits only a marginal scale-dependence,\nwith the dust layer possessing a Gaussian shape. In models including both\ninstabilities, the undulating mid-plane layer is broadened to a width set by\nthe intrinsic diffusion level caused by the streaming instability.",
        "This work analyzes the energetics of asteroid rubble piles in order to\nunderstand what asteroid morphologies should naturally arise from their\nformation and evolution process. In doing this, a phase diagram is developed\nthat maps out the range of final minimum energy states that a collapsing\ngravitational aggregate can achieve as a function of total angular momentum and\nmass distribution. This is developed assuming properties associated with rubble\npile asteroids, and can provide insight into the formation and subsequent\nevolution of contact binaries and orbital binaries in the solar system as an\noutcome of catastrophic disruptions. The system angular momentum is used as an\nindependent parameter, combined with resulting minimum energy configurations as\na simple function of mass morphology of the final system. The configuration of\nsystems with an energy boosted above the minimum energy state are also\nconsidered. This paper considers an ideal case, but outlines general results\nthat can be continued for more precise models of distributed granular media\nmodeled using continuum models or using discrete element models.",
        "Comet 7P\/Pons-Winnecke was observed from the Calar Alto Observatory (Spain)\nfor four months during the 2021 inbound apparition. Broad-band visible images\nwere taken between 1.71 and 1.25 AU pre-perihelion, while long-slit\nspectrophotometric data were taken at $\\sim$ 1.25 AU pre-perihelion. This\ndataset has been complemented with three $r$-Sloan images observed from Zwicky\nTransient Facility (ZTF) to model the physical properties and loss rate of the\ndust with a forward Monte Carlo dust tail code. The model fits the observed\nisophotes well for most observations. The peak dust production rate was\nmeasured at 83 kg s$^{-1}$, 15 days after perihelion. The particle terminal\nspeed ranges from 3 m s$^{-1}$ for 0.1 m particles to 23 m s$^{-1}$ for 5\n$\\mu$m particles. Regarding the gas production from spectra, CN and C$_2$ show\nasymmetric emission between the sunward and antisunward directions beyond the\ndata uncertainties and error propagation, while a clear asymmetry for C$_3$\ncannot be definitively claimed. Average production rates for CN, C$_2$, and\nC$_3$ near 2021 perihelion are 1.15 $\\times 10^{24}$, 2.32$\\times 10^{24}$, and\n1.69$\\times 10^{23}$ s$^{-1}$, respectively. The dust-to-gas mass ratio value\nis estimated to be around 2, suggesting a dust-rich composition. Based on the\ngas composition and the $Af\\rho$ value, we classify 7P\/Pons-Winnecke as having\na typical composition for Jupiter Family comets, with some C$_3$ depletion.\nGiven the limited previous knowledge, our work contributes to expanding the\nunderstanding of the activity and characteristics of 7P\/Pons-Winnecke.",
        "Close-by Earth analogs and super-Earths are of primary importance because\nthey will be preferential targets for the next generation of direct imaging\ninstruments. Bright and close-by G-to-M type stars are preferential targets in\nradial velocity surveys to find Earth analogs. We present an analysis of the RV\ndata of the star HD 20794, a target whose planetary system has been extensively\ndebated in the literature. The broad time span of the observations makes it\npossible to find planets with signal semi-amplitudes below 1 m\/s in the\nhabitable zone. We monitored the system with ESPRESSO. We joined ESPRESSO data\nwith the HARPS data, including archival data and new measurements from a recent\nprogram. We applied the post-processing pipeline YARARA to HARPS data to\ncorrect systematics, improve the quality of RV measurements, and mitigate the\nimpact of stellar activity. Results. We confirm the presence of three planets,\nwith periods of 18.3142 +\/- 0.0022 d, 89.68 +\/- 0.10 d, and 647.6 +\/- 2.6 d,\nalong with masses of 2.15 +\/- 0.17 MEarth, 2.98 +\/- 0.29 MEarth, and 5.82 +\/-\n0.57 MEarth respectively. For the outer planet, we find an eccentricity of 0.45\n+\/- 0.10, whereas the inner planets are compatible with circular orbits. The\nlatter is likely to be a rocky planet in the habitable zone of HD 20794. From\nthe analysis of activity indicators, we find evidence of a magnetic cycle with\na period around 3000 d, along with evidence pointing to a rotation period\naround 39 d. We have determined the presence of a system of three planets\norbiting the solar-type star HD 20794. This star is bright (V=4.34 mag) and\nclose (d = 6.04 pc), and HD 20794 d resides in the stellar habitable zone,\nmaking this system a high-priority target for future atmospheric\ncharacterization with direct imaging facilities.",
        "Several planet formation models have been proposed to explain the gap in the\npopulation of planets between $1.8$ $R_\\oplus$ to $2.0$ $R_\\oplus$ known as the\nRadius Valley. To apply these models to confirmed exoplanets, accurate and\nprecise host star and planet parameters are required to ensure the observed\nmeasurements correctly match model predictions. Previous studies have\nemphasized the need for a larger, more precise sample to further confirm\ndominant formation processes. By enhancing standard SED (Spectral Energy\nDistribution) fitting using Bayesian methods we derived highly accurate and\nprecise host star and planet parameters. Specifically, we achieved median\nfractional uncertainties for stellar and planet radii of 2.4% and 3.4%,\nrespectively. We then produced the largest, most precise sample to date of 1923\nplanets when compared to previous studies. This full sample, as well as a\nsampled filtered for host stellar masses between $0.8$ and $1.2$ $M_\\odot$, are\nthen used to derive the slope and position of the radius valley as a function\nof orbital period, insolation flux and stellar mass to compare them to\npredictive models and previous observational results. Our results are\nconsistent with thermally-driven mass loss with a planet radius vs. orbital\nperiod slope of $-0.142$ $\\pm0.006$ for the full sample leaning toward\ncore-powered mass loss. The planet radius vs. insolation flux slope of $0.136$\n$\\pm0.014$ for the filtered sample leaned toward photoevaporation. Also, the\nslope as a function of stellar mass for both samples appear more consistent\nwith thermally driven processes when compared to models and previous studies.",
        "We report the confirmation of TOI-6324 b, an Earth-sized (1.059 $\\pm$ 0.041\nR$_\\oplus$) ultra-short-period (USP) planet orbiting a nearby ($\\sim$20 pc) M\ndwarf. Using the newly commissioned Keck Planet Finder (KPF) spectrograph, we\nhave measured the mass of TOI-6324 b 1.17 $\\pm$ 0.22 M$_\\oplus$. Because of its\nextremely short orbit of just $\\sim$6.7 hours, TOI-6324 b is intensely\nirradiated by its M dwarf host, and is expected to be stripped of any thick,\nH\/He envelope. We were able to constrain its interior composition and found an\niron core mass fraction (CMF = 27$\\pm$37%) consistent with that of Earth\n($\\sim$33%) and other confirmed USPs. TOI-6324 b is the closest to Earth-sized\nUSP confirmed to date. TOI-6324 b is a promising target for JWST phase curve\nand secondary eclipse observations (Emission Spectroscopy Metric = 25) which\nmay reveal its surface mineralogy, day-night temperature contrast, and possible\ntidal deformation. From 7 sectors of TESS data, we report a tentative detection\nof the optical phase curve variation with an amplitude of 42$\\pm$28 ppm.",
        "The physical and orbital parameters of Trans-Neptunian Objects (TNOs) provide\nvaluable information about the Solar System's formation and evolution. In\nparticular, the characterization of binaries provides insights into the\nformation mechanisms that may be playing a role at such large distances from\nthe Sun. Studies show two distinct populations, and (38628) Huya occupies an\nintermediate position between the unequal-size binaries and those with\ncomponents of roughly equal sizes. In this work, we predicted and observed\nthree stellar occultation events by Huya. Huya and its satellite - S\/2012\n(38628) 1 - were detected during occultations in March 2021 and again in June\n2023. Additionally, an attempt to detect Huya in February 2023 resulted in an\nadditional single-chord detection of the secondary. A spherical body with a\nminimum diameter of D = 165 km can explain the three single-chord observations\nand provide a lower limit for the satellite size. The astrometry of Huya's\nsystem, as derived from the occultations and supplemented by observations from\nthe Hubble Space Telescope and Keck Observatory, provided constraints on the\nsatellite orbit and the mass of the system. Therefore, assuming the secondary\nis in an equatorial orbit around the primary, the limb fitting was constrained\nby the satellite orbit position angle. The system density, calculated by\nsumming the most precise measurement of Huya's volume to the spherical\nsatellite average volume, is $\\rho_{1}$ = 1073 $\\pm$ 66 kg m$^{-3}$. The\ndensity that the object would have assuming a Maclaurin equilibrium shape with\na rotational period of 6.725 $\\pm$ 0.01 hours is $\\rho_{2}$ = 768 $\\pm$ 42 kg\nm$^{-3}$. This difference rules out the Maclaurin equilibrium assumption for\nthe main body shape.",
        "Given a graph $G$ with vertex set $V(G)=\\{v_1,v_2,\\ldots,v_{n_1}\\}$ and a\ngraph $H$ of order $n_2$, the vertex complemented corona, denoted by\n$G\\tilde{\\circ}{H}$, is the graph produced by copying $H$ $n_1$ times, with the\n$i$-th copy of $H$ corresponding to the vertex $v_i$, and then adding edges\nbetween any vertex in $V(G)\\setminus\\{v_{i}\\}$ and any vertex of the $i$-th\ncopy of $H$. The present article deals with quantum state transfer of vertex\ncomplemented coronae concerning signless Laplacian matrix. Our research\ninvestigates conditions in which signless Laplacian perfect state transfer\nexists or not on vertex complemented coronae. Additionally, we also provide\nsome mild conditions for the class of graphs under consideration that allow\nsignless Laplacian pretty good state transfer.",
        "With the 4-meter Multi-Object Spectroscopic Telescope (4MOST) expected to\nprovide an influx of transient spectra when it begins observations in early\n2026 we consider the potential for real-time classification of these spectra.\nWe investigate three extant spectroscopic transient classifiers: the Deep\nAutomated Supernova and Host classifier (DASH), Next Generation SuperFit (NGSF)\nand SuperNova IDentification (SNID), with a focus on comparing the efficiency\nand purity of the transient samples they produce. We discuss our method for\nsimulating realistic, 4MOST-like, host-galaxy contaminated spectra and\ndetermining quality cuts for each classifier used to ensure pure SN Ia samples\nwhile maintaining efficient classification in other transient classes. We\ninvestigate the classifiers individually and in combinations. We find that a\ncombination of DASH and NGSF can produce a SN Ia sample with a purity of 99.9%\nwhile successfully classifying 70% of SNe Ia. However, it struggles to classify\nnon-SN Ia transients. We investigate photometric cuts to transient magnitude\nand transient flux fraction, finding that both can be used to improve transient\nclassification efficiencies by 7--25% depending on the transient subclass.\nFinally, we present an example classification plan for live classification and\nthe predicted purities and efficiencies across five transient classes: Ia, Ibc,\nII, superluminous and non-supernova transients.",
        "Magnetic systems with noncentrosymmetric crystal structures are renowned for\ntheir complex magnetic ordering and diverse and fascinating physical\nproperties. In this report, we provide a comprehensive study of the chiral\nmagnetic system Ni$_2$ScSbO$_6$, which exhibits a robust incommensurate\nlong-range antiferromagnetic spin ordering at a temperature of $T_N = 62$~K, as\nrevealed by bulk magnetization, specific heat, and neutron diffraction studies.\nThis magnetic ordering triggers a series of intriguing phenomena, including\nprominent magnetodielectric coupling manifested by a dielectric peak at $T_N$,\nsignificant spin-phonon coupling resulting in strong phonon renormalization\ncharacterized by anomalous softening of various Raman modes, and a remarkable\nvolume magnetostriction effect probed by high-resolution synchrotron X-ray\ndiffraction. These phenomena are intricately interlinked, positioning the\npresent system as a rare and interesting material.",
        "We demonstrate that the orbital eccentricity in compact binary mergers can be\nused to improve their sky localization using gravitational wave observations.\nExisting algorithms that conduct the localizations are not optimized for\neccentric sources. We use a semi-Bayesian technique to carry out localizations\nof simulated sources recovered using a matched-filter search. Through these\nsimulations, we find that if a non-negligible eccentricity is obtained during\nthe detection, an eccentricity-optimized algorithm can significantly improve\nthe localization areas compared to the existing methods. We also lay out the\nfoundation for an eccentric early-warning system using the matched-filter\nsearch. The potential impact on the early-warning localization is investigated.\nWe indicate a few possible cases of improvements while accounting for\neccentricity toward any detectable eccentric neutron star binaries in the\nforthcoming observing scenarios of ground-based detectors. Improved\nlocalizations can be useful in effectually utilizing the capabilities of the\nfollow-up facilities.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "We construct a family of links we call torus necklaces for which the link\ngroups are precisely the braid groups of generalised $J$-reflection groups.\nMoreover, this correspondence exhibits the meridians of the aforementioned link\ngroups as braid reflections. In particular, this construction generalises to\nall irreducible rank two complex reflection groups a well-known correspondence\nbetween some rank two complex braid groups and some torus knot groups. In\naddition, as abstract groups, we show that the family of link groups associated\nto Seifert links coincides with the family of circular groups. This shows that\nevery time a link group has a non-trivial center, it is a Garside group.",
        "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
        "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
        "We argue that the hypothesis that positive-parity charm meson resonances\nexhibit a compact tetraquark structure has some clear tension with recent\nlattice results for the $S$-wave $\\pi D$ system for an SU(3) flavor symmetric\nsetting. In particular, we show that such a diquark--anti-diquark tetraquark\nscenario would call for the presence of a state in the flavor\n$[{\\mathbf{\\overline{15}}}]$ representation, not seen in the lattice analysis.\nMoreover, we show that analogous lattice data in the axial-vector channel are\neven more sensitive to the internal structure of these very interesting states.",
        "This paper presents a unified framework for investigating the partial\ncondition number (CN) of the solution of double saddle point problems (DSPPs)\nand provides closed-form expressions for it. This unified framework encompasses\nthe well-known partial normwise CN (NCN), partial mixed CN (MCN) and partial\ncomponentwise CN (CCN) as special cases. Furthermore, we derive sharp upper\nbounds for the partial NCN, MCN and CCN, which are computationally efficient\nand free of expensive Kronecker products. By applying perturbations that\npreserve the structure of the block matrices of the DSPPs, we analyze the\nstructured partial NCN, MCN and CCN when the block matrices exhibit linear\nstructures. By leveraging the relationship between DSPP and equality\nconstrained indefinite least squares (EILS) problems, we recover the partial\nCNs for the EILS problem. Numerical results confirm the sharpness of the\nderived upper bounds and demonstrate their effectiveness in estimating the\npartial CNs.",
        "Data-driven benchmarks have led to significant progress in key scientific\nmodeling domains including weather and structural biology. Here, we introduce\nthe Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on\nthe problem of predicting cellular-resolution neural activity throughout an\nentire vertebrate brain. The benchmark is based on a novel dataset containing\n4d light-sheet microscopy recordings of over 70,000 neurons in a larval\nzebrafish brain, along with motion stabilized and voxel-level cell\nsegmentations of these data that facilitate development of a variety of\nforecasting methods. Initial results from a selection of time series and\nvolumetric video modeling approaches achieve better performance than naive\nbaseline methods, but also show room for further improvement. The specific\nbrain used in the activity recording is also undergoing synaptic-level\nanatomical mapping, which will enable future integration of detailed structural\ninformation into forecasting methods.",
        "We establish an epsilon-regularity theorem at points in the free boundary of\nalmost-minimizers of the energy\n$\\mathrm{Per}_{w}(E)=\\int_{\\partial^*E}w\\,\\mathrm{d} {\\mathscr{H}}^{n-1}$,\nwhere $w$ is a weight asymptotic to $d(\\cdot,\\mathbb{R}^n\\setminus\\Omega)^a$\nnear $\\partial\\Omega$ and $a>0$.\n  This implies that the boundaries of almost-minimizers are\n$C^{1,\\gamma_0}$-surfaces that touch $\\partial \\Omega$ orthogonally, up to a\nSingular Set $\\mathrm{Sing}(\\partial E)$ whose Hausdorff dimension satisfies\nthe bound\n  $d_{\\mathscr{H}}(\\mathrm{Sing}(\\partial E)) \\leq n +a -(5+\\sqrt{8})$.",
        "This work is devoted to studying normally hyperbolic invariant manifolds\n(NHIMs) for a class of quasi-periodically forced systems subject to additional\nstochastic noise. These systems can be understood as skew-product systems. The\nexistence of NHIMs is established by developing a parameterization method in\nrandom settings and applying the Implicit Function Theorem in appropriate\nBanach spaces. Based on this, we propose a numerical algorithm to compute the\nstatistics of NHIMs and Lyapunov exponents.",
        "Examples are given of solutions of the planar N-body problem which remain the\nsame for at least two systems of masses with the same sum and same center of\nmass. The least value of N achieved up to now with this property is 474, a\nnumber which had been announced in the first author's thesis.",
        "Realizing topological superconductivity in stoichiometric materials is a key\nchallenge in condensed matter physics. Here, we report the discovery of ternary\ngermanide superconductors, $M$IrGe ($M$ = Ti, Hf), as prime candidates for\ntopological superconductivity, predicted to exhibit nonsymmorphic\nsymmetry-protected hourglass Dirac chains. Using comprehensive thermodynamic\nand muon-spin rotation\/relaxation ($\\mu$SR) measurements, we establish these\nmaterials as conventional bulk type-II superconductors with transition\ntemperatures of 2.24(5) K for TiIrGe and 5.64(4) K for HfIrGe, featuring a full\ngap and preserved time-reversal symmetry. First-principles calculations reveal\nstriking topological features in $M$IrGe, including hourglass-shaped bulk\ndispersions and a Dirac chain -- a ring of fourfold-degenerate Dirac points\nprotected by nonsymmorphic symmetry. Each Dirac point corresponds to the neck\nof the hourglass dispersion, while the Dirac chain gives rise to drumhead-like\nsurface states near the Fermi level. Additionally, nontrivial $\\mathbb{Z}_2$\ntopology leads to isolated Dirac surface states with helical spin textures that\ndisperse across the Fermi level, forming an ideal platform for\nproximity-induced topological superconductivity. The coexistence of\nconventional bulk superconductivity, symmetry-protected hourglass topology, and\nhelical spin-textured surface states establishes $M$IrGe as a rare and robust\nplatform to realize topological superconductivity, opening new avenues for\nnext-generation quantum technologies."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Physics guided deep learning for generative design of crystal materials with symmetry constraints",
    "start_abstract":"Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability.",
    "start_categories":[
      "physics.comp-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
      ],
      "abstract":[
        "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Cancermorphic Computing Toward Multilevel Machine Intelligence",
        "Range and Angle Estimation with Spiking Neural Resonators for FMCW Radar",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "A Runtime Analysis of the Multi-Valued Compact Genetic Algorithm on\n  Generalized LeadingOnes",
        "A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization\n  of Multi-Objective Runway Operations Scheduling",
        "Quantum Simplicial Neural Networks",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Adding numbers with spiking neural circuits on neuromorphic hardware",
        "An Enhancement of Cuckoo Search Algorithm for Optimal Earthquake\n  Evacuation Space Allocation in Intramuros, Manila City",
        "Evolving Form and Function: Dual-Objective Optimization in Neural\n  Symbolic Regression Networks",
        "Brain in the Dark: Design Principles for Neuromimetic Inference under\n  the Free Energy Principle",
        "A RankNet-Inspired Surrogate-Assisted Hybrid Metaheuristic for Expensive\n  Coverage Optimization",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "Sovereign Debt Default and Climate Risk",
        "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with\n  Generative Flow Networks",
        "Mathematical modelling and homogenization of thin fiber-reinforced\n  hydrogels",
        "Training Dynamics of In-Context Learning in Linear Attention",
        "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$",
        "Surface Diagrams for Frobenius Algebras and Frobenius-Schur Indicators\n  in Grothendieck-Verdier Categories",
        "Technical Note: Targeted Maximum Likelihood Estimator for an ATE\n  Standardized for New Target Population",
        "Temporal-Guided Spiking Neural Networks for Event-Based Human Action\n  Recognition",
        "SoK: A Review of Cross-Chain Bridge Hacks in 2023",
        "O-RIS-ing: Evaluating RIS-Assisted NextG Open RAN",
        "Algebraization of rigid analytic varieties and formal schemes via\n  perfect complexes",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Enhancing Large Language Model Efficiencyvia Symbolic Compression: A\n  Formal Approach Towards Interpretability",
        "Improved quasi-invariance result for the periodic Benjamin-Ono-BBM\n  equation",
        "The dynamics of meaning through time: Assessment of Large Language\n  Models"
      ],
      "abstract":[
        "Despite their potential to address crucial bottlenecks in computing\narchitectures and contribute to the pool of biological inspiration for\nengineering, pathological biological mechanisms remain absent from\ncomputational theory. We hereby introduce the concept of cancer-inspired\ncomputing as a paradigm drawing from the adaptive, resilient, and evolutionary\nstrategies of cancer, for designing computational systems capable of thriving\nin dynamic, adversarial or resource-constrained environments. Unlike known\nbioinspired approaches (e.g., evolutionary and neuromorphic architectures),\ncancer-inspired computing looks at emulating the uniqueness of cancer cells\nsurvival tactics, such as somatic mutation, metastasis, angiogenesis and immune\nevasion, as parallels to desirable features in computing architectures, for\nexample decentralized propagation and resource optimization, to impact areas\nlike fault tolerance and cybersecurity. While the chaotic growth of cancer is\ncurrently viewed as uncontrollable in biology, randomness-based algorithms are\nalready being successfully demonstrated in enhancing the capabilities of other\ncomputing architectures, for example chaos computing integration. This vision\nfocuses on the concepts of multilevel intelligence and context-driven mutation,\nand their potential to simultaneously overcome plasticity-limited neuromorphic\napproaches and the randomness of chaotic approaches. The introduction of this\nconcept aims to generate interdisciplinary discussion to explore the potential\nof cancer-inspired mechanisms toward powerful and resilient artificial systems.",
        "Automotive radar systems face the challenge of managing high sampling rates\nand large data bandwidth while complying with stringent real-time and energy\nefficiency requirements. The growing complexity of autonomous vehicles further\nintensifies these requirements. Neuromorphic computing offers promising\nsolutions because of its inherent energy efficiency and parallel processing\ncapacity. This research presents a novel spiking neuron model for signal\nprocessing of frequency-modulated continuous wave (FMCW) radars that\noutperforms the state-of-the-art spectrum analysis algorithms in latency and\ndata bandwidth. These spiking neural resonators are based on the\nresonate-and-fire neuron model and optimized to dynamically process raw radar\ndata while simultaneously emitting an output in the form of spikes. We designed\nthe first neuromorphic neural network consisting of these spiking neural\nresonators that estimates range and angle from FMCW radar data. We evaluated\nthe range-angle maps on simulated datasets covering multiple scenarios and\ncompared the results with a state-of-the-art pipeline for radar processing. The\nproposed neuron model significantly reduces the processing latency compared to\ntraditional frequency analysis algorithms, such as the Fourier transformation\n(FT), which needs to sample and store entire data frames before processing. The\nevaluations demonstrate that these spiking neural resonators achieve\nstate-of-the-art detection accuracy while emitting spikes simultaneously to\nprocessing and transmitting only 0.02 % of the data compared to a float-32 FT.\nThe results showcase the potential for neuromorphic signal processing for FMCW\nradar systems and pave the way for designing neuromorphic radar sensors.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "In the literature on runtime analyses of estimation of distribution\nalgorithms (EDAs), researchers have recently explored univariate EDAs for\nmulti-valued decision variables. Particularly, Jedidia et al. gave the first\nruntime analysis of the multi-valued UMDA on the r-valued LeadingOnes\n(r-LeadingOnes) functions and Adak et al. gave the first runtime analysis of\nthe multi-valued cGA (r-cGA) on the r-valued OneMax function. We utilize their\nframework to conduct an analysis of the multi-valued cGA on the r-valued\nLeadingOnes function. Even for the binary case, a runtime analysis of the\nclassical cGA on LeadingOnes was not yet available. In this work, we show that\nthe runtime of the r-cGA on r-LeadingOnes is O(n^2r^2 log^3 n log^2 r) with\nhigh probability.",
        "This dissertation addresses the growing challenge of air traffic flow\nmanagement by proposing a simulation-based optimization (SbO) approach for\nmulti-objective runway operations scheduling. The goal is to optimize airport\ncapacity utilization while minimizing delays, fuel consumption, and\nenvironmental impacts. Given the NP-Hard complexity of the problem, traditional\nanalytical methods often rely on oversimplifications and fail to account for\nreal-world uncertainties, limiting their practical applicability. The proposed\nSbO framework integrates a discrete-event simulation model to handle stochastic\nconditions and a hybrid Tabu-Scatter Search algorithm to identify\nPareto-optimal solutions, explicitly incorporating uncertainty and fairness\namong aircraft as key objectives. Computational experiments using real-world\ndata from a major U.S. airport demonstrate the approach's effectiveness and\ntractability, outperforming traditional methods such as First-Come-First-Served\n(FCFS) and deterministic approaches while maintaining schedule fairness. The\nalgorithm's ability to generate trade-off solutions between competing\nobjectives makes it a promising decision support tool for air traffic\ncontrollers managing complex runway operations.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms.",
        "The Cuckoo Search Algorithm (CSA), while effective in solving complex\noptimization problems, faces limitations in random population initialization\nand reliance on fixed parameters. Random initialization of the population often\nresults in clustered solutions, resulting in uneven exploration of the search\nspace and hindering effective global optimization. Furthermore, the use of\nfixed values for discovery rate and step size creates a trade-off between\nsolution accuracy and convergence speed. To address these limitations, an\nEnhanced Cuckoo Search Algorithm (ECSA) is proposed. This algorithm utilizes\nthe Sobol Sequence to generate a more uniformly distributed initial population\nand incorporates Cosine Annealing with Warm Restarts to dynamically adjust the\nparameters. The performance of the algorithms was evaluated on 13 benchmark\nfunctions (7 unimodal, 6 multimodal). Statistical analyses were conducted to\ndetermine the significance and consistency of the results. The ECSA outperforms\nthe CSA in 11 out of 13 benchmark functions with a mean fitness improvement of\n30% across all functions, achieving 35% for unimodal functions and 24% for\nmultimodal functions. The enhanced algorithm demonstrated increased convergence\nefficiency, indicating its superiority to the CSA in solving a variety of\noptimization problems. The ECSA is subsequently applied to optimize earthquake\nevacuation space allocation in Intramuros, Manila.",
        "Data increasingly abounds, but distilling their underlying relationships down\nto something interpretable remains challenging. One approach is genetic\nprogramming, which `symbolically regresses' a data set down into an equation.\n  However, symbolic regression (SR) faces the issue of requiring training from\nscratch for each new dataset. To generalize across all datasets, deep learning\ntechniques have been applied to SR.\n  These networks, however, are only able to be trained using a symbolic\nobjective: NN-generated and target equations are symbolically compared. But\nthis does not consider the predictive power of these equations, which could be\nmeasured by a behavioral objective that compares the generated equation's\npredictions to actual data.\n  Here we introduce a method that combines gradient descent and evolutionary\ncomputation to yield neural networks that minimize the symbolic and behavioral\nerrors of the equations they generate from data.\n  As a result, these evolved networks are shown to generate more symbolically\nand behaviorally accurate equations than those generated by networks trained by\nstate-of-the-art gradient based neural symbolic regression methods.\n  We hope this method suggests that evolutionary algorithms, combined with\ngradient descent, can improve SR results by yielding equations with more\naccurate form and function.",
        "Deep learning has revolutionised artificial intelligence (AI) by enabling\nautomatic feature extraction and function approximation from raw data. However,\nit faces challenges such as a lack of out-of-distribution generalisation,\ncatastrophic forgetting and poor interpretability. In contrast, biological\nneural networks, such as those in the human brain, do not suffer from these\nissues, inspiring AI researchers to explore neuromimetic deep learning, which\naims to replicate brain mechanisms within AI models. A foundational theory for\nthis approach is the Free Energy Principle (FEP), which despite its potential,\nis often considered too complex to understand and implement in AI as it\nrequires an interdisciplinary understanding across a variety of fields. This\npaper seeks to demystify the FEP and provide a comprehensive framework for\ndesigning neuromimetic models with human-like perception capabilities. We\npresent a roadmap for implementing these models and a Pytorch code repository\nfor applying FEP in a predictive coding network.",
        "Coverage optimization generally involves deploying a set of facilities to\nbest satisfy the demands of specified points, with broad applications in fields\nsuch as location science and sensor networks. Recent applications reveal that\nthe subset site selection coupled with continuous angular parameter\noptimization can be formulated as Mixed-Variable Optimization Problems (MVOPs).\nMeanwhile, high-fidelity discretization and visibility analysis significantly\nincrease computational cost and complexity, evolving the MVOP into an Expensive\nMixed-Variable Optimization Problem (EMVOP). While canonical Evolutionary\nAlgorithms have yielded promising results, their reliance on numerous fitness\nevaluations is too costly for our problem. Furthermore, most surrogate-assisted\nmethods face limitations due to their reliance on regression-based models. To\naddress these issues, we propose the RankNet-Inspired Surrogate-assisted Hybrid\nMetaheuristic (RI-SHM), an extension of our previous work. RI-SHM integrates\nthree key components: (1) a RankNet-based pairwise global surrogate that\ninnovatively predicts rankings between pairs of individuals, bypassing the\nchallenges of fitness estimation in discontinuous solution space; (2) a\nsurrogate-assisted local Estimation of Distribution Algorithm that enhances\nlocal exploitation and helps escape from local optima; and (3) a fitness\ndiversity-driven switching strategy that dynamically balances exploration and\nexploitation. Experiments demonstrate that our algorithm can effectively handle\nlarge-scale coverage optimization tasks of up to 300 dimensions and more than\n1,800 targets within desirable runtime. Compared to state-of-the-art algorithms\nfor EMVOPs, RI-SHM consistently outperforms them by up to 56.5$\\%$ across all\ntested instances.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "We explore the interplay between sovereign debt default\/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
        "Vision-Language Models (VLMs) have recently shown promising advancements in\nsequential decision-making tasks through task-specific fine-tuning. However,\ncommon fine-tuning methods, such as Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),\npresent notable limitations: SFT assumes Independent and Identically\nDistributed (IID) data, while PPO focuses on maximizing cumulative rewards.\nThese limitations often restrict solution diversity and hinder generalization\nin multi-step reasoning tasks. To address these challenges, we introduce a\nnovel framework, GFlowVLM, a framework that fine-tune VLMs using Generative\nFlow Networks (GFlowNets) to promote generation of diverse solutions for\ncomplex reasoning tasks. GFlowVLM models the environment as a non-Markovian\ndecision process, allowing it to capture long-term dependencies essential for\nreal-world applications. It takes observations and task descriptions as inputs\nto prompt chain-of-thought (CoT) reasoning which subsequently guides action\nselection. We use task based rewards to fine-tune VLM with GFlowNets. This\napproach enables VLMs to outperform prior fine-tuning methods, including SFT\nand RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex\ntasks such as card games (NumberLine, BlackJack) and embodied planning tasks\n(ALFWorld), showing enhanced training efficiency, solution diversity, and\nstronger generalization capabilities across both in-distribution and\nout-of-distribution scenarios.",
        "This work considers simultaneous homogenization dimension reduction of a\nporoelastic model for thin fiber-reinforced hydrogels. The analysed medium is\ndefined as a two-component system consisting of a continuous fiber framework\nwith hydrogel inclusions arranged periodically throughout. The fibers are\nassumed to operate under quasi-stationary linear elasticity, whereas the\nhydrogel's hydromechanical behavior is represented using Biot's linear\nporoelasticity model. The asymptotic limit of the coupled system is established\nwhen the periodicity and thickness parameters are of the same order and tend to\nzero simultaneously, utilizing the re-scaling unfolding operator. It is\ndemonstrated that the limit displacement exhibits Kirchhoff-Love-type behavior\nthrough Griso's decomposition of plate displacements. Towards the end, a unique\nsolution for the macroscopic problem has been demonstrated.",
        "While attention-based models have demonstrated the remarkable ability of\nin-context learning, the theoretical understanding of how these models acquired\nthis ability through gradient descent training is still preliminary. Towards\nanswering this question, we study the gradient descent dynamics of multi-head\nlinear self-attention trained for in-context linear regression. We examine two\nparametrizations of linear self-attention: one with the key and query weights\nmerged as a single matrix (common in theoretical studies), and one with\nseparate key and query matrices (closer to practical settings). For the merged\nparametrization, we show the training dynamics has two fixed points and the\nloss trajectory exhibits a single, abrupt drop. We derive an analytical\ntime-course solution for a certain class of datasets and initialization. For\nthe separate parametrization, we show the training dynamics has exponentially\nmany fixed points and the loss exhibits saddle-to-saddle dynamics, which we\nreduce to scalar ordinary differential equations. During training, the model\nimplements principal component regression in context with the number of\nprincipal components increasing over training time. Overall, we characterize\nhow in-context learning abilities evolve during gradient descent training of\nlinear attention, revealing dynamics of abrupt acquisition versus progressive\nimprovements in models with different parametrizations.",
        "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions.",
        "Grothendieck-Verdier categories (also known as $\\ast$-autonomous categories)\ngeneralize rigid monoidal categories, with notable representation-theoretic\nexamples including categories of bimodules, modules over Hopf algebroids, and\nmodules over vertex operator algebras.\n  In this paper, we develop a surface-diagrammatic calculus for\nGrothendieck-Verdier categories, extending the string-diagrammatic calculus of\nJoyal and Street for rigid monoidal categories into a third dimension. This\nextension naturally arises from the non-invertibility of coherence data in\nGrothendieck-Verdier categories.\n  We show that key properties of Frobenius algebras in rigid monoidal\ncategories carry over to the Grothendieck-Verdier setting. Moreover, we\nintroduce higher Frobenius-Schur indicators for suitably finite $k$-linear\npivotal Grothendieck-Verdier categories and prove their invariance under\npivotal Frobenius linearly distributive equivalences.\n  The proofs are carried out using the surface-diagrammatic calculus. To\nfacilitate the verification of some of our results, we provide auxiliary files\nfor the graphical proof assistant homotopy.io.",
        "In this technical note we present a targeted maximum likelihood estimator\n(TMLE) for a previously studied target parameter that aims to transport an\naverage treatment effect (ATE) on a clinical outcome in a source population to\nwhat the ATE would have been in another target population. It is assumed that\none only observes baseline covariates in the target population, while we assume\nthat one can learn the conditional treatment effect on the outcome of interest\nin the source population. We also allow that one might observe only a subset of\nthe covariates in the target population while all covariates are measured in\nthe source population. We consider the case that the outcome is a clinical\noutcome at some future time point that is subject to missingness, or that our\noutcome of interest is a time to event that is subject to right-censoring. We\nderive the canonical gradients and present the corresponding TMLEs for these\ntwo cases.",
        "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
        "Blockchain technology has revolutionized industries by enabling secure and\ndecentralized transactions. However, the isolated nature of blockchain\necosystems hinders the seamless transfer of digital assets across different\nchains. Cross-chain bridges have emerged as vital web3 infrastructure to\naddress this challenge by facilitating interoperability between distinct\nblockchains. Cross-chain bridges remain vulnerable to various attacks despite\nsophisticated designs and security measures. The industry has experienced a\nsurge in bridge attacks, resulting in significant financial losses. The largest\nhack impacted Axie Infinity Ronin Bridge, with a loss of almost \\$600 million\nUSD. This paper analyzes recent cross-chain bridge hacks in 2022 and 2023 and\nexamines the exploited vulnerabilities. By understanding the attack nature and\nunderlying weaknesses, the paper aims to enhance bridge security and propose\npotential countermeasures. The findings contribute to developing industry-wide\nstandards for bridge security and operational resilience. Addressing the\nvulnerabilities and weaknesses exploited in recent cross-chain bridge hacks\nfosters trust and confidence in cross-chain interoperability.",
        "Reconfigurable Intelligent Surfaces (RISs) pose as a transformative\ntechnology to revolutionize the cellular architecture of Next Generation\n(NextG) Radio Access Networks (RANs). Previous studies have demonstrated the\ncapabilities of RISs in optimizing wireless propagation, achieving high\nspectral efficiency, and improving resource utilization. At the same time, the\ntransition to softwarized, disaggregated, and virtualized architectures, such\nas those being standardized by the O-RAN ALLIANCE, enables the vision of a\nreconfigurable Open RAN. In this work, we aim to integrate these technologies\nby studying how different resource allocation policies enhance the performance\nof RIS-assisted Open RANs. We perform a comparative analysis among various\nnetwork configurations and show how proper network optimization can enhance the\nperformance across the Enhanced Mobile Broadband (eMBB) and Ultra Reliable and\nLow Latency Communications (URLLC) network slices, achieving up to ~34%\nthroughput improvement. Furthermore, leveraging the capabilities of OpenRAN\nGym, we deploy an xApp on Colosseum, the world's largest wireless system\nemulator with hardware-in-the-loop, to control the Base Station (BS)'s\nscheduling policy. Experimental results demonstrate that RIS-assisted\ntopologies achieve high resource efficiency and low latency, regardless of the\nBS's scheduling policy.",
        "In this paper, we extend a theorem of To\\\"en and Vaqui\\'e to the\nnon-Archimedean and formal settings. More precisely, we prove that a smooth and\nproper rigid analytic variety is algebraizable if and only if its category of\nperfect complexes is smooth and proper. As a corollary, we deduce an analogous\nstatement for formal schemes and demonstrate that, in general, the bounded\nderived category of coherent sheaves on a formal scheme is not smooth.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "Large language models (LLMs) face significant token efficiency bottlenecks in\ncode generation and logical reasoning tasks, a challenge that directly impacts\ninference cost and model interpretability. This paper proposes a formal\nframework based on symbolic compression,integrating combinatory logic,\ninformation-theoretic optimal encoding, and context-aware inference techniques\nto achieve a step-change improvement in token efficiency while preserving\nsemantic integrity. We establish a mathematical framework within a functional\nprogramming paradigm, derive the quantitative relationship between symbolic\ndensity and model interpretability, and propose a differentiable compression\nfactor metric to evaluate encoding efficiency. Furthermore, we leverage\nparameter-efficient fine-tuning (PEFT) techniques to achieve a low-cost\napplication of the GAEL language. Experimental results show that this method\nachieves a 78.3% token compression rate in code generation tasks while\nimproving logical traceability by 62% through structural explicitness. This\nresearch provides new theoretical tools for efficient inference in LLMs and\nopens a symbolic path for modelinterpretability research.",
        "We extend recent results of Genovese-Luca-Tzvetkov (2022) regarding the\nquasi-invariance of Gaussian measures under the flow of the periodic\nBenjamin-Ono-BBM (BO-BBM) equation to the full range where BO-BBM is globally\nwell-posed. The main difficulty is due to the critical nature of the dispersion\nwhich we overcome by combining the approach of Coe-Tolomeo (2024) with an\niteration argument due to Forlano-Tolomeo (2024) to obtain long-time higher\nintegrability bounds on the transported density.",
        "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks",
    "start_abstract":"Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org .",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
      ],
      "abstract":[
        "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
      ],
      "categories":[
        "physics.comp-ph"
      ]
    },
    "list":{
      "title":[
        "CardSharp: A python library for generating MCNP6 input decks",
        "Exact Constraint of Density Functional Approximations at the\n  Semiclassical Limit",
        "Magnetic skyrmions embedded in a vortex",
        "Global physics-informed neural networks (GPINNs): from local point-wise\n  constraint to global nodal association",
        "Many-Body Coarse-Grained Molecular Dynamics with the Atomic Cluster\n  Expansion",
        "Monotone conservative strategies in data assimilation",
        "Application of the Pathline Method to the Aircraft Reactor Experiment",
        "High-Order Modulation Large MIMO Detector Based on Physics-Inspired\n  Methods",
        "An a-posteriori analysis of co-kurtosis PCA based dimensionality\n  reduction using a neural ODE solver",
        "A Batch Power Iteration Approach for the Iterative Quasi-Monte Carlo\n  Method Using a Randomized-Halton Sequence",
        "Micromagnetic Simulation and Optimization of Spin-Wave Transducers",
        "Discovering dense hydrogen solid at 1200K with deep variational free\n  energy approach",
        "DiffChip: Thermally Aware Chip Placement with Automatic Differentiation",
        "Inductive methods for counting number fields",
        "Strengthening the No-Go Theorem for QRNGs",
        "Social Influence Distorts Ratings in Online Interfaces",
        "Bounds for quasimodes with polynomially narrow bandwidth on surfaces of\n  revolution",
        "Semiclassical scar on tori in high dimension",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Some limit theorems for locally stationary Hawkes processes",
        "Energy burdens of carbon lock-in in household heating transitions",
        "Perturbations of a minimal surface with triple junctions in\n  $\\mathbb{R}^2 \\times \\mathbb{S}^1$",
        "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators",
        "Theoretical and Experimental Investigations of High-Performance\n  Sr2CoNbO6-delta Double Perovskite for IT-SOFC Cathode Applications",
        "Exploring quasar evolution with proximate molecular absorbers: Insights\n  from the kinematics of highly ionized nitrogen",
        "Any function I can actually write down is measurable, right?",
        "SyNPar: Synthetic Null Data Parallelism for High-Power False Discovery\n  Rate Control in High-Dimensional Variable Selection",
        "Strategizing with AI: Insights from a Beauty Contest Experiment"
      ],
      "abstract":[
        "A python library for the creation of MCNP6 input decks is described. The\nlibrary supports geometry generation with automatic assignment of surface\/facet\nnumbers, cell numbers, transform numbers and material numbers along with MCNP\nUniverses and FILL feature. Rectangular and Hexagonal Lattices are also\nsupported. A large material library is included. Support for a good selection\nof common sources and tallies is also provided. Cards or features which are\ncurrently not supported in the library can also be inserted as raw strings into\nthe output stream. Combining Python features like descriptively named\nvariables, functions and for loops with library functions provides an intuitive\nand parametric way to create, modify and maintain complicated geometries and\nsimulation models. The generated card deck also has human readable comments\nwhich makes it easy to read and relate back to the python source. Some support\nfor running MCNP, reading tallies and plotting is also provided.",
        "We introduce the semiclassical limit to electronic systems by taking the\nlimit $\\hbar\\rightarrow 0$ in the solution of Schr\\\"odinger equations. We show\nthat this limit is closely related to one type of strong correlation that is\nparticularly challenging from conventional multi-configurational perspective\nbut can be readily described through semiclassical analysis. Furthermore, by\nstudying the performance of density functional approximations (DFAs) in the\nsemiclassical limit, we find that mainstream DFAs have erroneous divergent\nenergy behaviors as $\\hbar \\rightarrow 0$, violating the exact constraint of\nfinite energy. Importantly, by making connection of the significantly\nunderestimated DFA energies of many strongly correlated transition-metal\ndiatomic molecules to their rather small estimated $\\hbar_{\\text{eff}}$, we\ndemonstrate the usefulness of our semiclassical analysis and its promise for\ninspiring better DFAs.",
        "Magnetic vortices and skyrmions represent two fundamental classes of\ntopological spin textures in ferromagnetic systems, distinguished by their\nunique stabilization mechanisms and degrees of freedom. Vortices, characterized\nby circular in-plane magnetization (chirality) and out-of-plane core\npolarization, naturally arise in confined geometries due to the interplay\nbetween exchange and dipolar interactions. In contrast, skyrmions typically\nrequire the Dzyaloshinskii-Moriya interaction for stabilization and exhibit\nfixed chirality-polarity relationships. Through micromagnetic simulations, we\nreveal that these seemingly distinct topological states can coexist, forming a\nnovel composite state termed the \\textit{n}-skyrmion vortex, which represents a\nskyrmion-embedded vortex state. These composite states possess quantized\ntopological charges $Q$ that follow the relation $Q_{\\text{total}} =\nQ_{\\text{vortex}} + nQ_{\\text{skyrmion}}$, where $n$ denotes the number of\nembedded skyrmions. Similar to vortices, these states exhibit independent\nchirality and polarity and are energetically degenerate.",
        "Recently, physics-informed neural networks (PINNs) and their variants have\ngained significant popularity as a scientific computing method for solving\npartial differential equations (PDEs), whereas accuracy is still its main\nshortcoming. Despite numerous development efforts, there is no literature\ndemonstrating that these methods surpass classic numerical algorithms in\nsolving the forward issue. In this paper, by analyzing the disparities between\nPINNs and traditional numerical methods based on mesh discretization, we\ninvestigate the underlying causes for the in adequate precision of PINNs and\nintroduce a novel approach named global physics-informed neural networks\n(GPINNs). Inspired by the crucial concept of global nodal association in\nconventional numerical algorithms, GPINNs leverages the prior field\ndistribution information from pre-trained PINNs to estimate the association\nweights between arbitrary nodes in space. GPINNs can not only be regarded as a\nmeshless approach but also be demonstrated, both theoretically and in practical\ncircumstances, to have the ability of second-order convergence when trained\nwith equidistant nodes. Overall, GPINNs may be seen as an ideal approach to\ninheriting the merits of scientific machine learning (SciML) and conventional\nnumerical computing, which also represent the first SciML algorithm to surpass\nstandard numerical methods in terms of accuracy.",
        "Molecular dynamics (MD) simulations provide detailed insight into\natomic-scale mechanisms but are inherently restricted to small spatio-temporal\nscales. Coarse-grained molecular dynamics (CGMD) techniques allow simulations\nof much larger systems over extended timescales. In theory, these techniques\ncan be quantitatively accurate, but common practice is to only target\nqualitatively correct behaviour of coarse-grained models. Recent advances in\napplying machine learning methodology in this setting are now being applied to\ncreate also quantitatively accurate CGMD models. We demonstrate how the Atomic\nCluster Expansion parameterization (Drautz, 2019) can be used in this task to\nconstruct highly efficient, interpretable and accurate CGMD models. We focus in\nparticular on exploring the role of many-body effects.",
        "This paper studies whether numerically preserving monotonic properties can\noffer modelling advantages in data assimilation, particularly when the signal\nor data is a realization of a stochastic partial differential equation (SPDE)\nor partial differential equation (PDE) with a monotonic property. We\ninvestigate the combination of stochastic Strong Stability Preserving (SSP)\ntime-stepping, nonlinear solving strategies and data assimilation. Experimental\nresults indicate that a particle filter whose ensemble members are solved\nmonotonically can increase forecast skill when the reference data (not\nnecessarily observations) also has a monotone property. Additionally, more\nadvanced techniques used to avoid the degeneracy of the filter\n(tempering-jittering) are shown to be compatible with a conservative monotone\napproach.",
        "In this work, a new numerical method for the transport of Delayed Neutron\nPrecursors (DNPs) is applied to the Aircraft Reactor Experiment (ARE). The\npathline method is based on the Method of Characteristics (MOC) and leverages\nthe pathlines of the liquid nuclear fuel to derive an integral form of the DNPs\nbalance equation. The method has previously been tested on the CNRS benchmark\nand in a simplified 2D geometry where turbulent diffusivity was significant\ncompared to advection. Here, the pathline method is applied to a real-world\nMolten Salt Reactor (MSR), the ARE. DNPs transport is implemented in the\nframework of the coupling between neutron transport solver\nAPOLLO3\\textregistered{} and computational fluid dynamics code TrioCFD, both\ndeveloped at the French Atomic and Energy Commission (CEA). The DNPs\nconcentration obtained with the pathline method were compared with those\npreviously computed by TrioCFD, highlighting the importance of recirculation of\nfission products. The L-7 experiment was also replicated to demonstrate the\nmethod's capability.",
        "Applying quantum annealing or current quantum-\/physics-inspired algorithms\nfor MIMO detection always abandon the direct gray-coded bit-to-symbol mapping\nin order to obtain Ising form, leading to inconsistency errors. This often\nresults in slow convergence rates and error floor, particularly with high-order\nmodulations. We propose HOPbit, a novel MIMO detector designed to address this\nissue by transforming the MIMO detection problem into a higher-order\nunconstrained binary optimization (HUBO) problem while maintaining gray-coded\nbit-to-symbol mapping. The method then employs the simulated probabilistic bits\n(p-bits) algorithm to directly solve HUBO without degradation. This innovative\nstrategy enables HOPbit to achieve rapid convergence and attain near-optimal\nmaximum-likelihood performance in most scenarios, even those involving\nhigh-order modulations. The experiments show that HOPbit surpasses ParaMax by\nseveral orders of magnitude in terms of bit error rate (BER) in the context of\n12-user massive and large MIMO systems even with computing resources. In\naddition, HOPbit achieves lower BER rates compared to other traditional\ndetectors.",
        "A low-dimensional representation of thermochemical scalars based on\ncokurtosis principal component analysis (CoK-PCA) has been shown to effectively\ncapture stiff chemical dynamics in reacting flows relative to the widely used\nprincipal component analysis (PCA). The effectiveness of the reduced manifold\nwas evaluated in a priori analyses using both linear and nonlinear\nreconstructions of thermochemical scalars from aggressively truncated principal\ncomponents (PCs). In this study, we demonstrate the efficacy of a CoK-PCA-based\nreduced manifold using a posteriori analysis. Simulations of spontaneous\nignition in a homogeneous reactor that pose a challenge in accurately capturing\nthe ignition delay time as well as the scalar profiles within the reaction zone\nare considered. The governing ordinary differential equations (ODEs) in the PC\nspace were evolved from the initial conditions using two ODE solvers. First, a\nstandard ODE solver that uses a pre-trained artificial neural network (ANN) to\nestimate the source terms and integrates the solution in time. Second, a neural\nODE solver that incorporates the time integration of PCs into the ANN training.\nThe time-evolved profiles of the PCs and reconstructed thermochemical scalars\ndemonstrate the robustness of the CoK-PCA-based low-dimensional manifold in\naccurately capturing the ignition process. Furthermore, we observed that the\nneural ODE solver minimized propagation errors across time steps and provided\nmore accurate results than the standard ODE solver. The results of this study\ndemonstrate the potential of CoK-PCA-based manifolds to be implemented in\nmassively parallel reacting flow solvers.",
        "The Iterative Quasi-Monte Carlo (iQMC) method is a recently developed hybrid\nmethod for neutron transport simulations. iQMC replaces standard quadrature\ntechniques used in deterministic linear solvers with Quasi-Monte Carlo\nsimulation for accurate and efficient solutions to the neutron transport\nequation. Previous iQMC studies utilized a fixed-seed approach wherein\nparticles were reset to the same initial position and direction of travel at\nthe start of every transport sweep. While the QMC samples offered greatly\nimproved uniformity compared to pseudo-random samples, the fixed-seed approach\nmeant that some regions of the problem were under-sampled and resulted in\nerrors similar to ray effects observed in discrete ordinates methods.\n  This work explores using randomized-Quasi Monte Carlo techniques (RQMC) to\ngenerate unique sets of QMC samples for each transport sweep and gain a\nmuch-improved sampling of the phase space. The use of RQMC introduces some\nstochastic noise to iQMC's iterative process, which was previously absent. To\ncompensate, we adopt a ``batch'' approach similar to typical Monte Carlo\nk-eigenvalue problems, where the iQMC source is converged over\n$N_\\text{inactive}$ batches, then results from $N_\\text{active}$ batches are\nrecorded and used to calculate the average and standard deviation of the\nsolution.\n  The RQMC batch method was implemented in the Monte Carlo Dynamic Code (MC\/DC)\nand is shown to be a large improvement over the fixed-seed method. The batch\nmethod was able to provide iteratively stable and more accurate solutions with\nnearly two orders of magnitude reduction in the number of particle histories\nper batch. Notably, despite introducing some stochastic noise to the solution,\nthe RQMC batch approach converges both the k-effective and mean scalar flux\nerror at the theoretical QMC convergence rate of $O(N^{-1})$.",
        "The increasing demand for higher data volume and faster transmission in\nmodern wireless telecommunication systems has elevated requirements for 5G\nhigh-band RF hardware. Spin-Wave technology offers a promising solution, but\nits adoption is hindered by significant insertion loss stemming from the low\nefficiency of magnonic transducers. This work introduces a micromagnetic\nsimulation method for directly computing the spin-wave resistance, the real\npart of spin-wave impedance, which is crucial for optimizing magnonic\ntransducers. By integrating into finite-difference micromagnetic simulations,\nthis approach extends analytical models to arbitrary transducer geometries. We\ndemonstrate its effectiveness through parameter studies on transducer design\nand waveguide properties, identifying key strategies to enhance the overall\ntransducer efficiency. Our studies show that by varying single parameters of\nthe transducer geometry or the YIG thickness, the spin-wave efficiency, the\nparameter describing the efficiency of the transfer of electromagnetic energy\nto the spin wave, can reach values up to 0.75. The developed numerical model\nallows further fine-tuning of the transducers to achieve even higher\nefficiencies.",
        "We perform deep variational free energy calculations to investigate the dense\nhydrogen system at 1200 K and high pressures. In this computational framework,\nneural networks are used to model the free energy through the proton Boltzmann\ndistribution and the electron wavefunction. By directly minimizing the free\nenergy, our results reveal the emergence of a crystalline order associated with\nthe center of mass of hydrogen molecules at approximately 180 GPa. This\ntransition from atomic liquid to a molecular solid is marked by discontinuities\nin both the pressure and thermal entropy. Additionally, we discuss the broader\nimplications and limitations of these findings in the context of recent studies\nof dense hydrogen under similar conditions.",
        "Chiplets are modular integrated circuits that can be combined to form a\nlarger system, offering flexibility and performance enhancements. However,\ntheir dense packing often leads to significant thermal management challenges,\nrequiring careful floorplanning to ensure efficient heat distribution. To\naddress thermal considerations, layout optimization algorithms concurrently\nminimize the total wirelength and the maximum temperature. However, these\nefforts employ gradient-free approaches, such as simulated annealing, which\nsuffer from poor scaling and slow convergence. In this paper, we propose\nDiffChip, a chiplet placement algorithm based on automatic differentiation\n(AD). The proposed framework relies on a differentiable thermal solver that\ncomputes the sensitivity of the temperature map with respect to the positions\nof the chiplets. Regularization strategies for peak temperature, heat sources,\nand material properties enable end-to-end differentiability, allowing for\ngradient-based optimization. We apply DiffChip to optimize a layout where the\ntotal wirelength is minimized while keeping the maximum temperature below a\ndesired threshold. By leveraging AD and physics-aware optimization, our\napproach accelerates the design process of microelectronic systems, exceeding\ntraditional trial-and-error and gradient-free methods.",
        "We give a new method for counting extensions of a number field asymptotically\nby discriminant, which we employ to prove many new cases of Malle's Conjecture\nand counterexamples to Malle's Conjecture. We consider families of extensions\nwhose Galois closure is a fixed permutation group $G$. Our method relies on\nhaving asymptotic counts for $T$-extensions for some normal subgroup $T$ of\n$G$, uniform bounds for the number of such $T$-extensions, and possibly weak\nbounds on the asymptotic number of $G\/T$-extensions. However, we do not require\nthat most $T$-extensions of a $G\/T$-extension are $G$-extensions. Our new\nresults use $T$ either abelian or $S_3^m$, though our framework is general.",
        "Quantum random numbers are essential for security against quantum algorithms.\nRandomness as a beacon is a service being provided for companies and\ngovernments to upgrade their security standards from RSA to PQC - QKD or\nPQC-RSA protocols. Both security mechanisms assume trust in the service\nprovider unless one aims for device-independent protocols. How does an entity\nensure that the beacon service has a quantum signature other than relying on\nfaith? Specifically, given a bit-stream, can a user verify a quantum signature\nin it? Researchers claim this is indecipherable and have stated a no-go theorem\nfor post-processed bit-streams. In this article, we corroborate the results of\nthe no-go theorem while discussing its nuances using two different random\nnumber generators and four test methods. These include the NIST statistical\ntest suite and machine learning algorithms that strengthen the theorem. This\nwork is relevant for companies and governments using QRNG OpenAPI to enhance\nsecurity against quantum threats.",
        "Theoretical work on sequential choice and large-scale experiments in online\nranking and voting systems has demonstrated that social influence can have a\ndrastic impact on social and technological systems. Yet, the effect of social\ninfluence on online rating systems remains understudied and the few existing\ncontributions suggest that online ratings would self-correct given enough\nusers. Here, we propose a new framework for studying the effect of social\ninfluence on online ratings. We start from the assumption that people are\ninfluenced linearly by the observed average rating, but postulate that their\npropensity to be influenced varies. When the weight people assign to the\nobserved average depends only on their own latent rating, the resulting system\nis linear, but the long-term rating may substantially deviate from the true\nmean rating. When the weight people put on the observed average depends on both\ntheir own latent rating and the observed average rating, the resulting system\nis non-linear, and may support multiple equilibria, suggesting that ratings\nmight be path-dependent and deviations dramatic. Our results highlight\npotential limitations in crowdsourced information aggregation and can inform\nthe design of more robust online rating systems.",
        "Given a compact surface of revolution with Laplace-beltrami operator\n$\\Delta$, we consider the spectral projector $P_{\\lambda,\\delta}$ on a\npolynomially narrow frequency interval $[\\lambda-\\delta,\\lambda + \\delta]$,\nwhich is associated to the self-adjoint operator $\\sqrt{-\\Delta}$. For a large\nclass of surfaces of revolution, and after excluding small disks around the\npoles, we prove that the $L^2 \\to L^{\\infty}$ norm of $P_{\\lambda,\\delta}$ is\nof order $\\lambda^{\\frac{1}{2}} \\delta^{\\frac{1}{2}}$ up to $\\delta \\geq\n\\lambda^{-\\frac{1}{32}}$. We adapt the microlocal approach introduced by Sogge\nfor the case $\\delta = 1$, by using the Quantum Completely Integrable structure\nof surfaces of revolution introduced by Colin de Verdi\\`ere. This reduces the\nanalysis to a number of estimates of explicit oscillatory integrals, for which\nwe introduce new quantitative tools.",
        "We show that the eigenfunctions of the self-adjoint elliptic $h-$differential\noperator $P_{h}(t)$ exhibits semiclassical scar phenomena on the\n$d-$dimensional torus, under the $\\sigma$-Bruno-R\\\"{u}ssmann condition, instead\nof the Diophantine one. Its equivalence is described as: for almost all\nperturbed Hamiltonian's KAM Lagrangian tori $\\Lambda_{\\omega}$, there exists a\nsemiclassical measure with positive mass on $\\Lambda_{\\omega}$. The premise is\nthat we can obatain a family of quasimodes for the $h-$differential operator\n$P_{h}(t)$ in the semiclassical limit as $h\\rightarrow0$, under the\n$\\sigma$-Bruno-R\\\"{u}ssmann condition.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "We prove a law of large numbers and functional central limit theorem for a\nclass of multivariate Hawkes processes with time-dependent reproduction rate.\nWe address the difficulties induced by the use of non-convolutive Volterra\nprocesses by recombining classical martingale methods introduced in Bacry et\nal. [3] with novel ideas proposed by Kwan et al. [19]. The asymptotic theory we\nobtain yields useful applications in financial statistics. As an illustration,\nwe derive closed-form expressions for price distortions under liquidity\nconstraints.",
        "Heating electrification presents opportunities and challenges for energy\naffordability. Without careful planning and policy, the costs of natural gas\nservice will be borne by a shrinking customer base, driving up expenses for\nthose who are left behind. This affordability issue is worsened by new fossil\nfuel investments, which risk locking communities into carbon-intensive\ninfrastructure. Here, we introduce a framework to quantify the distributional\neffects of natural gas phasedown on energy affordability, integrating detailed\nhousehold data with utility financial and planning documents. Applying our\nframework first to Massachusetts and then nationwide, we show that vulnerable\ncommunities face disproportionate affordability risks in building energy\ntransitions. Households that do not electrify may bear up to 50% higher energy\ncosts over the next decade. Targeted electrification may help to alleviate\nimmediate energy burdens, but household heating transitions will ultimately\nrequire coordinated, neighborhood-scale strategies that consider the high fixed\ncosts of legacy infrastructure.",
        "We construct stationary perturbations of a specific minimal surface with a\ncircle of triple junctions in $\\mathbb{R}^2 \\times \\mathbb{S}^1$, that satisfy\ngiven boundary data.",
        "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.",
        "Enhancing the transport of oxygen anions in the cathode while maintaining\nsurface stability is essential for improving the performance of\nintermediate-temperature solid oxide fuel cells (IT-SOFCs). This study\ninvestigates a novel cathode material candidate, Sr2CoNbO6-delta (SCNO), using\ndensity functional theory, molecular dynamics, and experimental\ncharacterization. The redox active Co cation at B-site and less reducible Nb\ncation at the B'-site together enhance both surface stability and\nelectrocatalytic performance. SCNO is observed to have a higher concentration\nof oxygen vacancies and increased oxygen diffusivity on the surface. The\nsurface stability of SCNO is further improved when simulated under compressive\nstrain due to the GDC electrolyte substrate. These findings offer new insights\ninto controlling Sr segregation in SCNO, contributing to a better understanding\nof its enhanced oxygen reduction reaction (ORR) activity and high surface\nstability. Subsequently, SCNO was synthesized to evaluate its potential as a\ncathode material in SOFCs. To assess its performance, symmetric cells with\nuniform dense thin films of varying thicknesses (40 and 80 nm) were fabricated\nusing the pulsed laser deposition technique. Electrochemical impedance\nspectroscopy and distributed relaxation time analysis indicate that bulk oxygen\nion diffusion is a limiting factor for the ORR in SCNO. The polarization\nresistance for the 40 and 80 nm dense thin film symmetric cells ranged between\n0.329 - 0.241 ohm cm2 and 1.095 - 0.438 ohm cm2, respectively, within the\ntemperature range of 773 - 973 K in an air atmosphere. The full cell\nconfiguration of NiO-GDC|GDC|SCNO demonstrated a significantly high peak power\ndensity of 0.633 W\/cm2 at 973 K. This theory-guided design and experimental\nstudy suggest that SCNO is a promising candidate for IT-SOFC cathode materials.",
        "We investigate the presence and kinematics of NV absorption proximate to high\nredshift quasars selected upon the presence of strong $H_{2}$ and HI absorption\nat the quasar redshift. Our spectroscopic observations with X-shooter at the\nVLT reveal a 70% detection rate of NV (9 of 13 quasars with 2.5 < z < 3.3),\nremarkably higher than the 10% detection rate in intervening DLA systems and\nthe 30% rate observed within a few thousand km\/s of the source in the general\nquasar population. While many NV components lie within the velocity range of\nthe neutral gas, the kinematic profiles of high-ionization species appear\ndecoupled from those of low-ionization species, with the former extending over\nmuch larger velocity ranges, particularly towards bluer velocities. We also\nobserve significant variations in the NV\/SiIV, which we attribute to varying\nionization conditions, with a velocity-dependent trend: blueshifted NV\ncomponents systematically exhibit higher ionization parameters compared to\nthose near the quasar's systemic redshift. Furthermore, the most redshifted\nsystems relative to the quasar show no evidence of NV absorption. The results\nsuggest that proximate $H_{2}$ absorption systems select critical stages of\nquasar evolution, during which the quasar remains embedded in a rich molecular\nenvironment. Redshifted systems trace infalling gas, potentially associated\nwith mergers, preceding the onset of outflows. Such outflows may reach or even\ncarry out neutral and molecular gas.This latter stage would correspond to\nproximate $H_{2}$ systems located around or blueshifted relative to the\nquasar's systemic z. Finally, the only case in our sample featuring highly\nblueshifted neutral gas shows no evidence of an association with the quasar.Our\nfindings highlight the need to account for the ionization state when defining a\nvelocity threshold to distinguish quasar-associated systems from intervening.",
        "In this expository paper aimed at a general mathematical audience, we discuss\nhow to combine certain classic theorems of set-theoretic inner model theory and\neffective descriptive set theory with work on Hilbert's tenth problem and\nuniversal Diophantine equations to produce the following surprising result:\nThere is a specific polynomial $p(x,y,z,n,k_1,\\dots,k_{70})$ of degree $7$ with\ninteger coefficients such that it is independent of $\\mathsf{ZFC}$ (and much\nstronger theories) whether the function $$f(x) = \\inf_{y \\in \\mathbb{R}}\\sup_{z\n\\in \\mathbb{R}}\\inf_{n \\in \\mathbb{N}}\\sup_{\\bar{k} \\in\n\\mathbb{N}^{70}}p(x,y,z,n,\\bar{k})$$ is Lebesgue measurable. We also give\nsimilarly defined $g(x,y)$ with the property that the statement \"$x \\mapsto\ng(x,r)$ is measurable for every $r \\in \\mathbb{R}$\" has large cardinal\nconsistency strength (and in particular implies the consistency of\n$\\mathsf{ZFC}$) and $h(m,x,y,z)$ such that $h(1,x,y,z),\\dots,h(16,x,y,z)$ can\nconsistently be the indicator functions of a Banach$\\unicode{x2013}$Tarski\nparadoxical decomposition of the sphere.\n  Finally, we discuss some situations in which measurability of analogously\ndefined functions can be concluded by inspection, which touches on\nmodel-theoretic o-minimality and the fact that sufficiently strong large\ncardinal hypotheses (such as Vop\\v{e}nka's principle and much weaker\nassumptions) imply that all 'reasonably definable' functions (including the\nabove $f(x)$, $g(x,y)$, and $h(m,x,y,z)$) are universally measurable.",
        "Balancing false discovery rate (FDR) and statistical power to ensure reliable\ndiscoveries is a key challenge in high-dimensional variable selection. Although\nseveral FDR control methods have been proposed, most involve perturbing the\noriginal data, either by concatenating knockoff variables or splitting the data\ninto two halves, both of which can lead to a loss of power. In this paper, we\nintroduce a novel approach called Synthetic Null Parallelism (SyNPar), which\ncontrols the FDR in high-dimensional variable selection while preserving the\noriginal data. SyNPar generates synthetic null data from a model fitted to the\noriginal data and modified to reflect the null hypothesis. It then applies the\nsame estimation procedure in parallel to both the original and synthetic null\ndata to estimate coefficients that indicate feature importance. By comparing\nthe coefficients estimated from the null data with those from the original\ndata, SyNPar effectively identifies false positives, functioning as a numerical\nanalog of a likelihood ratio test. We provide theoretical guarantees for FDR\ncontrol at any desired level while ensuring that the power approaches one with\nhigh probability asymptotically. SyNPar is straightforward to implement and can\nbe applied to a wide range of statistical models, including high-dimensional\nlinear regression, generalized linear models, Cox models, and Gaussian\ngraphical models. Through extensive simulations and real data applications, we\ndemonstrate that SyNPar outperforms state-of-the-art methods, including\nknockoffs and data-splitting methods, in terms of FDR control, power, and\ncomputational efficiency.",
        "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
    "start_abstract":"TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
      ],
      "abstract":[
        "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "CoverM: Read alignment statistics for metagenomics",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Causes of evolutionary divergence in prostate cancer",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Non-uniqueness of normalized NLS ground states on bounded domains with\n  homogeneous Neumann boundary conditions",
        "Drinfeld modules with maximal Galois action",
        "Toughness of double network hydrogels: the role of reduced stress\n  propagation",
        "Traffic noise assessment in urban Bulgaria using explainable machine\n  learning",
        "Quadratic BSDEs with Singular Generators and Unbounded Terminal\n  Conditions: Theory and Applications",
        "Improved Online Confidence Bounds for Multinomial Logistic Bandits",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Polarization-controlled strong light-matter interaction with templated\n  molecular aggregates",
        "Joint Power Allocation and Phase Shift Design for Stacked Intelligent\n  Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL",
        "An exact closed walks series formula for the complexity of regular\n  graphs and some related bounds",
        "Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Allostatic Control of Persistent States in Spiking Neural Networks for\n  perception and computation",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "Steady compressible Navier-Stokes-Fourier system with general\n  temperature dependent viscosities I: density estimates based on Bogovskii\n  operator"
      ],
      "abstract":[
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "We provide a general non-uniqueness result for normalized ground states of\nnonlinear Schr\\\"odinger equations with pure power nonlinearity on bounded\ndomains with homogeneous Neumann boundary conditions, defined as global\nminimizers of the associated energy functional among functions with prescribed\nmass. Precisely, for nonlinearity powers slightly smaller than the\n$L^2$-critical exponent, we prove that there always exists at least one value\nof the mass for which normalized ground states are not unique.",
        "With a fixed prime power $q>1$, define the ring of polynomials\n$A=\\mathbb{F}_q[t]$ and its fraction field $F=\\mathbb{F}_q(t)$. For each pair\n$a=(a_1,a_2) \\in A^2$ with $a_2$ nonzero, let $\\phi(a)\\colon A\\to F\\{\\tau\\}$ be\nthe Drinfeld $A$-module of rank $2$ satisfying $t\\mapsto t+a_1\\tau+a_2\\tau^2$.\nThe Galois action on the torsion of $\\phi(a)$ gives rise to a Galois\nrepresentation $\\rho_{\\phi(a)}\\colon\n\\operatorname{Gal}(F^{\\operatorname{sep}}\/F)\\to\n\\operatorname{GL}_2(\\widehat{A})$, where $\\widehat{A}$ is the profinite\ncompletion of $A$. We show that the image of $\\rho_{\\phi(a)}$ is large for\nrandom $a$. More precisely, for all $a\\in A^2$ away from a set of density $0$,\nwe prove that the index\n$[\\operatorname{GL}_2(\\widehat{A}):\\rho_{\\phi(a)}(\\operatorname{Gal}(F^{\\operatorname{sep}}\/F))]$\ndivides $q-1$ when $q>2$ and divides $4$ when $q=2$. We also show that the\nrepresentation $\\rho_{\\phi(a)}$ is surjective for a positive density set of\n$a\\in A^2$.",
        "Double network hydrogels show remarkable mechanical performance, combining\nhigh strength and fracture toughness with sufficient stiffness to bear load,\ndespite containing only a low density of cross-linked polymer molecules in\nwater. We introduce a simple mesoscale model of a double network material,\ndetailed enough to resolve the salient microphysics of local plastic bond\nbreakage, yet simple enough to address macroscopic cracking. Load sharing\nbetween the networks results in a delocalisation of stress such that the double\nnetwork inherits both the stiffness of its stiff-and-brittle sacrificial\nnetwork and the ductility of its soft-and-ductile matrix network. The\nunderlying mechanism is a reduction in the Eshelby stress propagator between\nsacrificial bonds, inhibiting the tendency for the plastic failure of one\nsacrificial bond to propagate stress to neighbouring sacrificial bonds and\ncause a follow-on cascade of breakages. The mechanism of brittle macroscopic\ncracking is thereby suppressed, giving instead ductile deformation via\ndiffusely distributed microcracking.",
        "Fine-grained noise maps are vital for epidemiological studies on traffic\nnoise. However, detailed information on traffic noise is often limited,\nespecially in Eastern Europe. Rigid linear noise land-use regressions are\ntypically employed to estimate noise levels; however, machine learning likely\noffers more accurate noise predictions. We innovated by comparing the\npredictive accuracies of supervised machine learning models to estimate traffic\nnoise levels across the five largest Bulgarian cities. In situ A-weighted\nequivalent continuous sound levels were obtained from 232 fixed-site monitors\nacross these cities. We included transport- and land-use-related predictors\nusing 50-1,000 m buffers. Extreme gradient boosting (XGB) had the highest\nten-fold cross-validated fit (R2=0.680) and the lowest root mean square error\n(RMSE=4.739), insignificantly besting the random forest-based model (R2=0.667,\nRMSE=4.895). Support vector regression (R2=0.633, RMSE=5.358), elastic net\n(R2=0.568, RMSE=5.625), and linear regression (R2=0.548, RMSE=5.569) performed\nsignificantly worse. Shapley values for the XGB showed that the length of major\nroads within 100 m buffers, footways within 50 m buffers, residential roads\nwithin 50 m buffers, and the number of buildings within 50 m buffers were\nimportant non-linear predictors. Our spatially resolved noise maps revealed\nstriking geographic noise variations and that, on average, 96.8% of the urban\npopulation experiences harmful noise levels.",
        "We investigate a class of quadratic backward stochastic differential\nequations (BSDEs) with generators singular in $ y $. First, we establish the\nexistence of solutions and a comparison theorem, thereby extending results in\nthe literature. Additionally, we analyze the stability property and the\nFeynman-Kac formula, and prove the uniqueness of viscosity solutions for the\ncorresponding singular semilinear partial differential equations (PDEs).\nFinally, we demonstrate applications in the context of robust control linked to\nstochastic differential utility and certainty equivalent based on\n$g$-expectation. In these applications, the coefficient of the quadratic term\nin the generator captures the level of ambiguity aversion and the coefficient\nof absolute risk aversion, respectively.",
        "In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over\nthe previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee & Oh, 2024). This is\nmainly achieved by establishing tighter self-concordant properties of the MNL\nloss and introducing a novel intermediary term to bound the estimation error.\nUsing this new online confidence bound, we propose a constant-time algorithm,\nOFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log\nT \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $ for sufficiently large $T$, where\n$\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$ is the\ndimension of the contexts, and $T$ is the total number of rounds. Furthermore,\nwe introduce a Maximum Likelihood Estimation (MLE)-based algorithm,\nOFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O \\Big( d \\log\n(BT) \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "We demonstrate strong light-matter interaction for a layer of templated\nmerocyanine molecules in a planar microcavity. Using a single layer of graphene\nnanoribbons as a templating layer, we obtain an aligned layer of aggregated\nmolecules. The molecular layer displays anisotropic optical properties\nresembling those of a biaxial crystal. The anisotropic excitonic component in\nthe cavity results in strongly polarization-dependent light-matter interaction\nand in increased Rabi-energies. The increased light-matter interaction is\npossibly due to reduced molecular disorder in the templated molecular layer.\nThis conclusion is supported by an analysis based on a multi-oscillator model.\nWe further use photoluminescence microspectroscopy to demonstrate that the\nlight-matter coupling is spatially homogeneous. Our study introduces molecular\ntemplating to strong light-matter studies. The reduced disorder of the system\nas a consequence of templating is highly beneficial for engineering\nlight-matter interaction.",
        "Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer\nhigh spectral efficiency (SE) through multiple distributed access points (APs).\nHowever, the large number of antennas increases power consumption. We propose\nincorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a\ncost-effective, energy-efficient solution. This paper focuses on optimizing the\njoint power allocation of APs and the phase shift of SIMs to maximize the sum\nSE. To address this complex problem, we introduce a fully distributed\nmulti-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the\nnoisy value method with a recurrent policy in multi-agent policy optimization\n(NVR-MAPPO), enhances performance by encouraging diverse exploration under\ncentralized training and decentralized execution. Simulations demonstrate that\nNVR-MAPPO significantly improves sum SE and robustness across various\nscenarios.",
        "The complexity of a graph is the number of its labeled spanning trees. In\nthis work complexity is studied in settings that admit regular graphs. An exact\nformula is established linking complexity of the complement of a regular graph\nto numbers of closed walks in the graph by way of an infinite alternating\nseries. Some consequences of this result yield infinite classes of lower and\nupper bounds on the complexity of such graphs. Applications of these\nmathematical results to biological problems on neuronal activity are described.",
        "Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a novel model for updating perceptual beliefs about the\nenvironment by extending the concept of Allostasis to the control of internal\nrepresentations. Allostasis is a fundamental regulatory mechanism observed in\nanimal physiology that orchestrates responses to maintain a dynamic equilibrium\nin bodily needs and internal states. In this paper, we focus on an application\nin numerical cognition, where a bump of activity in an attractor network is\nused as a spatial numerical representation. While existing neural networks can\nmaintain persistent states, to date, there is no unified framework for\ndynamically controlling spatial changes in neuronal activity in response to\nenvironmental changes. To address this, we couple a well known allostatic\nmicrocircuit, the Hammel model, with a ring attractor, resulting in a Spiking\nNeural Network architecture that can modulate the location of the bump as a\nfunction of some reference input. This localized activity in turn is used as a\nperceptual belief in a simulated subitization task a quick enumeration process\nwithout counting. We provide a general procedure to fine-tune the model and\ndemonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare it\nwith biological data. Finally, we analyze the dynamics of the network to\nunderstand the selectivity and specificity of different neurons to distinct\ncategories present in the input. The results of this paper, particularly the\nmechanism for moving persistent states, are not limited to numerical cognition\nbut can be applied to a wide range of tasks involving similar representations.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "The aim of this paper is to reconsider the existence theory for steady\ncompressible Navier--Stokes--Fourier system assuming more general condition of\nthe dependence of the viscosities on the temperature in the form\n$\\mu(\\vartheta)$, $\\xi(\\vartheta) \\sim (1+\\vartheta)^\\alpha$ for $0\\leq \\alpha\n\\leq 1$. This extends the known theory for $\\alpha=1$ from and improves\nsignificantly the results for $\\alpha =0$. This paper is the first of a series\nof two papers dealing with this problem and is connected with the\nBogovskii-type estimates of the sequence of densities. This leads, among\nothers, to the limitation $\\gamma >\\frac 32$ for the pressure law\n$p(\\varrho,\\vartheta) \\sim \\varrho^\\gamma + \\varrho\\vartheta$. The paper\nconsiders both the heat-flux (Robin) and Dirichlet boundary conditions for the\ntemperature as well as both the homogeneous Dirichlet and zero inflow\/outflow\nNavier boundary conditions for the velocity. Further extension for $\\gamma >1$\nonly is based on different type of pressure estimates and will be the content\nof the subsequent paper."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102",
    "start_abstract":"Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
      ],
      "abstract":[
        "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Analysis of Indic Language Capabilities in LLMs",
        "Explicit vs. Implicit: Investigating Social Bias in Large Language\n  Models through Self-Reflection",
        "Accelerating Antibiotic Discovery with Large Language Models and\n  Knowledge Graphs",
        "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter",
        "DUAL: Diversity and Uncertainty Active Learning for Text Summarization",
        "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law",
        "Learning to Search Effective Example Sequences for In-Context Learning",
        "Information Types in Product Reviews",
        "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs",
        "Challenges in Expanding Portuguese Resources: A View from Open\n  Information Extraction",
        "Elevating Legal LLM Responses: Harnessing Trainable Logical Structures\n  and Semantic Knowledge with Legal Reasoning",
        "Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection",
        "Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language\n  Models",
        "A novel Facial Recognition technique with Focusing on Masked Faces",
        "Exact Maximin Share Fairness via Adjusted Supply",
        "Network-assisted collective operations for efficient distributed quantum\n  computing",
        "A Moving Mesh Isogeometric Method Based on Harmonic Maps",
        "Quasi-two-dimensional magnetism and antiferromagnetic ground state in\n  Li$_2$FeSiO$_4$",
        "Channel deformations during elastocapillary spreading of gaseous\n  embolisms in biomimetic leaves",
        "Pretraining Generative Flow Networks with Inexpensive Rewards for\n  Molecular Graph Generation",
        "Measuring Star Formation Rates in the Milky Way from Hi-GAL 70 $\\mu$m\n  Observations",
        "Non-negative tensor factorization-based dependence map analysis for\n  local damage detection in presence of non-Gaussian noise",
        "Low-energy insulating reconstructions of Si(111)-7x7 surface with and\n  without stacking fault discovered by graph theory",
        "Cultivating Precision: Comparative Analysis of Sensor-Based Yogurt\n  Fermentation Monitoring Techniques",
        "Existence and Uniqueness of Local Solutions for a Class of Partial\n  Differential-Algebraic Equations",
        "Generalizable automated ischaemic stroke lesion segmentation with vision\n  transformers",
        "A Spatio-Temporal Dirichlet Process Mixture Model on Linear Networks for\n  Crime Data",
        "Weighted Graph Structure Learning with Attention Denoising for Node\n  Classification"
      ],
      "abstract":[
        "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies.",
        "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated bias in LLMs, prior work has predominantly focused on explicit\nbias, leaving the more nuanced implicit biases largely unexplored. This paper\npresents a systematic framework grounded in social psychology theories to\ninvestigate and compare explicit and implicit biases in LLMs. We propose a\nnovel \"self-reflection\" based evaluation framework that operates in two phases:\nfirst measuring implicit bias through simulated psychological assessment\nmethods, then evaluating explicit bias by prompting LLMs to analyze their own\ngenerated content. Through extensive experiments on state-of-the-art LLMs\nacross multiple social dimensions, we demonstrate that LLMs exhibit a\nsubstantial inconsistency between explicit and implicit biases, where explicit\nbiases manifest as mild stereotypes while implicit biases show strong\nstereotypes. Furthermore, we investigate the underlying factors contributing to\nthis explicit-implicit bias inconsistency. Our experiments examine the effects\nof training data scale, model parameters, and alignment techniques. Results\nindicate that while explicit bias diminishes with increased training data and\nmodel size, implicit bias exhibits a contrasting upward trend. Notably,\ncontemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit\nbias but show limited efficacy in mitigating implicit bias. These findings\nsuggest that while scaling up models and alignment training can address\nexplicit bias, the challenge of implicit bias requires novel approaches beyond\ncurrent methodologies.",
        "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available.",
        "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC.",
        "With the rise of large language models, neural text summarization has\nadvanced significantly in recent years. However, even state-of-the-art models\ncontinue to rely heavily on high-quality human-annotated data for training and\nevaluation. Active learning is frequently used as an effective way to collect\nsuch datasets, especially when annotation resources are scarce. Active learning\nmethods typically prioritize either uncertainty or diversity but have shown\nlimited effectiveness in summarization, often being outperformed by random\nsampling. We present Diversity and Uncertainty Active Learning (DUAL), a novel\nalgorithm that combines uncertainty and diversity to iteratively select and\nannotate samples that are both representative of the data distribution and\nchallenging for the current model. DUAL addresses the selection of noisy\nsamples in uncertainty-based methods and the limited exploration scope of\ndiversity-based methods. Through extensive experiments with different\nsummarization models and benchmark datasets, we demonstrate that DUAL\nconsistently matches or outperforms the best performing strategies. Using\nvisualizations and quantitative metrics, we provide valuable insights into the\neffectiveness and robustness of different active learning strategies, in an\nattempt to understand why these strategies haven't performed consistently in\ntext summarization. Finally, we show that DUAL strikes a good balance between\ndiversity and robustness.",
        "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.",
        "Large language models (LLMs) demonstrate impressive few-shot learning\ncapabilities, but their performance varies widely based on the sequence of\nin-context examples. Key factors influencing this include the sequence's\nlength, composition, and arrangement, as well as its relation to the specific\nquery. Existing methods often tackle these factors in isolation, overlooking\ntheir interdependencies. Moreover, the extensive search space for selecting\noptimal sequences complicates the development of a holistic approach. In this\nwork, we introduce Beam Search-based Example Sequence Constructor (BESC), a\nnovel method for learning to construct optimal example sequences. BESC\naddresses all key factors involved in sequence selection by considering them\njointly during inference, while incrementally building the sequence. This\ndesign enables the use of beam search to significantly reduce the complexity of\nthe search space. Experiments across various datasets and language models show\nnotable improvements in performance.",
        "Information in text is communicated in a way that supports a goal for its\nreader. Product reviews, for example, contain opinions, tips, product\ndescriptions, and many other types of information that provide both direct\ninsights, as well as unexpected signals for downstream applications. We devise\na typology of 24 communicative goals in sentences from the product review\ndomain, and employ a zero-shot multi-label classifier that facilitates\nlarge-scale analyses of review data. In our experiments, we find that the\ncombination of classes in the typology forecasts helpfulness and sentiment of\nreviews, while supplying explanations for these decisions. In addition, our\ntypology enables analysis of review intent, effectiveness and rhetorical\nstructure. Characterizing the types of information in reviews unlocks many\nopportunities for more effective consumption of this genre.",
        "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality. Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval quality.\nConversely, coupled methods embed KG information within models to improve\nretrieval quality, but at the expense of flexibility. In this paper, we propose\na novel flexible modular KG-RAG framework, termed FRAG, which synergizes the\nadvantages of both approaches. FRAG estimates the hop range of reasoning paths\nbased solely on the query and classify it as either simple or complex. To match\nthe complexity of the query, tailored pipelines are applied to ensure efficient\nand accurate reasoning path retrieval, thus fostering the final reasoning\nprocess. By using the query text instead of the KG to infer the structural\ninformation of reasoning paths and employing adaptable retrieval strategies,\nFRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG\ndoes not require extra LLMs fine-tuning or calls, significantly boosting\nefficiency and conserving resources. Extensive experiments show that FRAG\nachieves state-of-the-art performance with high efficiency and low resource\nconsumption.",
        "Open Information Extraction (Open IE) is the task of extracting structured\ninformation from textual documents, independent of domain. While traditional\nOpen IE methods were based on unsupervised approaches, recently, with the\nemergence of robust annotated datasets, new data-based approaches have been\ndeveloped to achieve better results. These innovations, however, have focused\nmainly on the English language due to a lack of datasets and the difficulty of\nconstructing such resources for other languages. In this work, we present a\nhigh-quality manually annotated corpus for Open Information Extraction in the\nPortuguese language, based on a rigorous methodology grounded in established\nsemantic theories. We discuss the challenges encountered in the annotation\nprocess, propose a set of structural and contextual annotation rules, and\nvalidate our corpus by evaluating the performance of state-of-the-art Open IE\nsystems. Our resource addresses the lack of datasets for Open IE in Portuguese\nand can support the development and evaluation of new methods and systems in\nthis area.",
        "Large Language Models (LLMs) have achieved impressive results across numerous\ndomains, yet they experience notable deficiencies in legal question-answering\ntasks. LLMs often generate generalized responses that lack the logical\nspecificity required for expert legal advice and are prone to hallucination,\nproviding answers that appear correct but are unreliable. Retrieval-Augmented\nGeneration (RAG) techniques offer partial solutions to address this challenge,\nbut existing approaches typically focus only on semantic similarity, neglecting\nthe logical structure essential to legal reasoning. In this paper, we propose\nthe Logical-Semantic Integration Model (LSIM), a novel supervised framework\nthat bridges semantic and logical coherence. LSIM comprises three components:\nreinforcement learning predicts a structured fact-rule chain for each question,\na trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant\ncandidate questions by integrating semantic and logical features, and\nin-context learning generates the final answer using the retrieved content. Our\nexperiments on a real-world legal QA dataset-validated through both automated\nmetrics and human evaluation-demonstrate that LSIM significantly enhances\naccuracy and reliability compared to existing methods.",
        "Recent advances on instruction fine-tuning have led to the development of\nvarious prompting techniques for large language models, such as explicit\nreasoning steps. However, the success of techniques depends on various\nparameters, such as the task, language model, and context provided. Finding an\neffective prompt is, therefore, often a trial-and-error process. Most existing\napproaches to automatic prompting aim to optimize individual techniques instead\nof compositions of techniques and their dependence on the input. To fill this\ngap, we propose an adaptive prompting approach that predicts the optimal prompt\ncomposition ad-hoc for a given input. We apply our approach to social bias\ndetection, a highly context-dependent task that requires semantic\nunderstanding. We evaluate it with three large language models on three\ndatasets, comparing compositions to individual techniques and other baselines.\nThe results underline the importance of finding an effective prompt\ncomposition. Our approach robustly ensures high detection performance, and is\nbest in several settings. Moreover, first experiments on other tasks support\nits generalizability.",
        "Graph-structured data plays a vital role in numerous domains, such as social\nnetworks, citation networks, commonsense reasoning graphs and knowledge graphs.\nWhile graph neural networks have been employed for graph processing, recent\nadvancements have explored integrating large language models for graph-based\ntasks. In this paper, we propose a novel approach named Learnable Graph Pooling\nToken (LGPT), which addresses the limitations of the scalability issues in\nnode-level projection and information loss in graph-level projection. LGPT\nenables flexible and efficient graph representation by introducing learnable\nparameters that act as tokens in large language models, balancing fine-grained\nand global graph information. Additionally, we investigate an Early Query\nFusion technique, which fuses query context before constructing the graph\nrepresentation, leading to more effective graph embeddings. Our method achieves\na 4.13\\% performance improvement on the GraphQA benchmark without training the\nlarge language model, demonstrating significant gains in handling complex\ntextual-attributed graph data.",
        "Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces.",
        "This work addresses fair allocation of indivisible items in settings wherein\nit is feasible to create copies of resources or dispose of tasks. We establish\nthat exact maximin share (MMS) fairness can be achieved via limited duplication\nof goods even under monotone valuations. We also show that, when allocating\nchores under monotone costs, MMS fairness is always feasible with limited\ndisposal of chores. Since monotone valuations do not admit any nontrivial\napproximation guarantees for MMS, our results highlight that such barriers can\nbe circumvented by post facto adjustments in the supply of the items.\n  We prove that, for division of $m$ goods among $n$ agents with monotone\nvaluations, there always exists an assignment of subsets of goods to the agents\nsuch that they receive at least their maximin shares and no single good is\nallocated to more than $3 \\log m$ agents. In addition, the sum of the sizes of\nthe assigned subsets does not exceed $m$. For identically ordered valuations,\nwe obtain an upper bound of $O(\\sqrt{\\log m})$ on the maximum assignment\nmultiplicity across goods and an $m + \\widetilde{O}\\left(\\frac{m}{\\sqrt{n}}\n\\right)$ bound for the total number of goods assigned. Further, for additive\nvaluations, we prove that there always exists an MMS assignment in which no\nsingle good is allocated to more than $2$ agents and the total number of goods\nassigned is at most $2m$.\n  For chores, we upper bound the number of chores that need to be discarded for\nensuring MMS fairness. We prove that, under monotone costs, there exists an MMS\nassignment in which at most $\\frac{m}{e}$ remain unassigned. For identically\nordered costs, we establish that MMS fairness can be achieved while keeping at\nmost $\\widetilde{O} \\left(\\frac{m}{n^{1\/4}} \\right)$ chores unassigned. We also\nprove that the obtained bounds for monotone valuations and monotone costs are\nessentially tight.",
        "We propose protocols for the distribution of collective quantum operations\nbetween remote quantum processing units (QPUs), a requirement for distributed\nquantum computing. Using only local operations and classical communication\n(LOCC), these protocols allow for collective multicontrolled and multitarget\ngates to be executed in network architectures similar to those used for\nhigh-performance computing. The types of gates that can be implemented\nfollowing this scheme are discussed. The Bell pair cost for a single\ndistributed multicontrolled gate is estimated, arriving to a single additional\nBell pair over the theoretically optimal calculation with pre-shared\nentanglement, demonstrating better scalability when compared to current\nproposals based on entanglement swapping through a network, and bounds are\ncalculated for general diagonal gates. A recipe is provided for the lumped\ndistribution of gates such as arbitrarily-sized Toffoli and multicontrolled Z,\nand $R_{zz}(\\theta)$ gates. Finally, we provide an exact implementation of a\ndistributed Grover's search algorithm using this protocol to partition the\ncircuit, with Bell pair cost growing linearly with the number of Grover\niterations and the number of partitions.",
        "Although the isogeometric analysis has shown its great potential in achieving\nhighly accurate numerical solutions of partial differential equations, its\nefficiency is the main factor making the method more competitive in practical\nsimulations. In this paper, an integration of isogeometric analysis and a\nmoving mesh method is proposed, providing a competitive approach to resolve the\nefficiency issue. Focusing on the Poisson equation, the implementation of the\nalgorithm and related numerical analysis are presented in detail, including the\nnumerical discretization of the governing equation utilizing isogeometric\nanalysis, and a mesh redistribution technique developed via harmonic maps. It\nis found that the isogeometric analysis brings attractive features in the\nrealization of moving mesh method, such as it provides an accurate expression\nfor moving direction of mesh nodes, and allows for more choices for\nconstructing monitor functions. Through a series of numerical experiments, the\neffectiveness of the proposed method is successfully validated and the\npotential of the method towards the practical application is also well\npresented with the simulation of a helium atom in Kohn--Sham density functional\ntheory.",
        "Our experimental (neutron diffraction, M\\\"ossbauer spectroscopy, magnetic\nsusceptibility, specific heat) and numerical studies on the evolution of short-\nand long-range magnetic order in $\\gamma_{\\rm II}$-Li\\(_2\\)FeSiO\\(_4\\) suggest\na quasi-two-dimensional (2D) nature of magnetism. The experimental data\nobtained on single crystals imply long-range antiferromagnetic order below\n$T_{\\rm N}= 17$~K. A broad maximum in magnetic susceptibility $\\chi$ at $T_{\\rm\nm}\\simeq 28$~K, observation of magnetic entropy changes up to 100~K and\nanisotropy in $\\chi$ are indicative of low-dimensional magnetism and suggest\nshort-range magnetic correlations up to 200~K. Neutron diffraction shows that\nlong-range antiferromagnetic order is characterised by the propagation vector\nk=(1\/2,0,1\/2). The ordered moment $\\mu = 2.50(2) \\mu_B$ \/Fe, at $T = 1.5$~K, is\nalong the crystallographic $a$-axis. This is consistent with the observed\nstatic hyperfine field of $B_{\\rm hyp}=14.8(3)$\\,T by M\\\"ossbauer spectroscopy\nwhich indicates significant orbital contributions. The temperature dependence\nof $B_{\\rm hyp}$ yields the critical exponent $\\beta=0.116(12)$ which is in the\nregime of the 2D Ising behaviour. LSDA+U studies exploiting the experimental\nspin structure suggest dominating magnetic exchange coupling within the\n$ac$-layers (i.e., $J_3\\simeq -6$~K and $J_6\\simeq-2$~K) while interlayer\ncoupling is much smaller and partly frustrated. This confirms the 2D nature of\nmagnetism and is in full agreement with the experimental findings.",
        "The nucleation and\/or spreading of bubbles in water under tension (due to\nwater evaporation) can be problematic for most plants along the ascending sap\nnetwork from root to leaves, named xylem. Due to global warming, trees facing\ndrought conditions are particularly threatened by the formation of such air\nembolisms, which spreads intermittently and hinder the flow of sap and could\nultimately result in their demise. PDMS-based biomimetic leaves simulating\nevapotranspiration have demonstrated that, in a linear configuration, the\nexistence of a slender constriction in the channel allows for the creation of\nintermittent embolism propagation (as an interaction between the elasticity of\nthe biomimetic leaf (mainly the deformable ceiling of the microchannels) and\nthe capillary forces at the air\/water interfaces)\n\\cite{Keiser2022}-\\cite{keiser2024}. Here we use analog PDMS-based biomimetic\nleaves in 1d and 2d. To better explore the embolism spreading mechanism, we add\nto the setup an additional technique, allowing to measure directly the\nmicrochannel's ceiling deformation versus time, which corresponds to the\npressure variations. We present here such a method that allows to have\nquantitative insights in the dynamics of embolism spreading. The coupling\nbetween channel deformations and the Laplace pressure threshold explains the\nobserved elastocapillary dynamics.",
        "Generative Flow Networks (GFlowNets) have recently emerged as a suitable\nframework for generating diverse and high-quality molecular structures by\nlearning from rewards treated as unnormalized distributions. Previous works in\nthis framework often restrict exploration by using predefined molecular\nfragments as building blocks, limiting the chemical space that can be accessed.\nIn this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative\nmodel leveraging individual atoms as building blocks to explore drug-like\nchemical space more comprehensively. We propose an unsupervised pre-training\napproach using drug-like molecule datasets, which teaches A-GFNs about\ninexpensive yet informative molecular descriptors such as drug-likeliness,\ntopological polar surface area, and synthetic accessibility scores. These\nproperties serve as proxy rewards, guiding A-GFNs towards regions of chemical\nspace that exhibit desirable pharmacological properties. We further implement a\ngoal-conditioned finetuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on a subset of ZINC\ndataset, and by employing robust evaluation metrics we show the effectiveness\nof our approach when compared to other relevant baseline methods for a wide\nrange of drug design tasks.",
        "Three methods for computing the total star formation rate of the Milky Way\nagree well with a reference value of $1.65\\pm0.19$ M$_\\odot$ yr$^{-1}$. They\nare then used to determine the radial dependence of the star formation rate and\nface-on map for the Milky Way. First, the method based on a model of star\nformation in Hi-GAL-defined dense clumps, adjusted for an increase in the\ngas-to-dust ratio with Galactocentric radius, predicts $1.65\\pm0.61$ M$_\\odot$\nyr$^{-1}$. Second, the method using the 70 $\\mu$m emission, commonly used in\nother galaxies, with a technique to assign distances to the extended emission,\npredicts $1.42^{+0.63}_{-0.44}$ M$_\\odot$ yr$^{-1}$. Finally, a method based on\ntheoretical predictions of star formation efficiency as a function of virial\nparameter, with masses corrected for metallicity dependence, applied to a\ncatalog of molecular clouds also predicts a value in agreement at $1.47$\nM$_\\odot$ yr$^{-1}$. The three methods predict the radial variation of the star\nformation rate, with remarkably good agreement from the CMZ out to about 20\nkpc. More differences were seen in face-on maps with a resolution of 0.5 kpc\nmade with the three approaches and in comparisons to the local (within 3 kpc)\nstar formation rate, indicating limitations of the methods when applied to\nsmaller scales. The 70 $\\mu$m star formation rate follows very closely the\nsurface density of molecular gas, corrected for a metallicity-dependent CO\nconversion factor. A molecular gas depletion time of 1 Gyr is consistent with\nthe data, as is a molecular Kennicutt-Schmidt relation with a power-law slope\nof $1.10 \\pm 0.06$.",
        "The time-frequency map (TFM) is frequently used in condition monitoring,\nnecessitating further processing to select an informative frequency band (IFB)\nor directly detect damage. However, selecting an IFB is challenging due to the\ncomplexity of spectral structures, non-Gaussian disturbances, and overlapping\nfault signatures in vibration signals. Additionally, dynamic operating\nconditions and low signal-to-noise ratio further complicate the identification\nof relevant features that indicate damage. To solve this problem, the present\nwork proposes a novel method for informative band selection and local damage\ndetection in rolling element bearings, utilizing non-negative tensor\nfactorization (NTF)-based dependence map analysis. The recently introduced\nconcept of the dependence map is leveraged, with a set of these maps being\nfactorized to separate informative components from non-informative ones.\nDependence maps provide valuable information on the auto-similarity of spectral\ncontent, while NTF, a powerful tool commonly used in image processing for\nfeature extraction, enhances this process. The combination of these methods\nallows for the extraction of IFBs, forming the basis for local damage\ndetection. The effectiveness of the proposed method has been validated using\nboth synthetic and real vibration signals corrupted with non-Gaussian\ndisturbances.",
        "The 7x7 reconstruction of Si(111) surface is one of the most fascinating\nconfiguration in nature, whose STM image has been well-understood by the famous\ndimer-adatom-stacking-fault model (DAS). However, the electronic property of\nthe DAS model is always confirmed to be metallic by first-principles\ncalculations, while some experiments detected insulating features. It is still\nchallenge to predict DAS-like reconstructions through traditional method to\nsolve such a puzzle. Here, we show that 7x7 reconstructions can be quickly\ndiscovered by graph theory as implemented in the graph-space based RG2 code for\ncrystal structure prediction. Two groups of reconstructions with (DAS-d8-T12,\nDAS-d8-T9H3-A, DAS-d8-T9H3-B and DAS-d8-T6H6) and without (AB-d10-T12,\nAB-d10-T9H3, AA-d10-T12 and AA-d10-T9H3) stacking-fault are discovered. They\npossess energetic stabilities comparable to the well-known DAS (DAS-d8-T12) and\nshow similar STM patterns, providing a plausible explanation for the\nexperimentally observed 7x7 reconstruction on the Si(111) surface. The\nfirst-principles calculations show that DAS-d8-T12, DAS-d8-T6H6, AB-d10-T12,\nand AA-d10-T12 are metallic, while DAS-d8-T9H3-A, DAS-d8-T9H3-B, AB-d10-T9H3\nand AA-d10-T9H3 are insulating phases with gaps of 0.043 eV, 0.182 eV, 0.043 eV\nand 0.059 eV, respectively. Our work demonstrates the predictability of the\nSi(111)-7x7 reconstruction and provides the structural candidates for\nunderstanding the experimentally observed metal-to-insulator transition.",
        "Fermented dairy products, including yogurt, are widely consumed for their\nnutritional and health benefits. While numerous methods exist to monitor and\nunderstand yogurt fermentation, the literature lacks an integrated evaluation\nof diverse sensing approaches within a single experimental framework. To\naddress this gap, this study systematically examines and compares multiple\nmeasurement techniques--electrical impedance, DC resistance, pH, optical\ntransparency, carbon dioxide concentration, ambient temperature, and relative\nhumidity--in tracking the yogurt fermentation process. By presenting a unified\nset of experimental results and assessing each method's observational\ncharacteristics, this work offers an encompassing reference point for\nresearchers seeking to understand the relative merits and limitations of\ndifferent sensing modalities. Rather than establishing definitive guidelines or\npractical recommendations, the findings provide a foundation for subsequent\ninvestigations into sensor-based fermentation monitoring, thereby contributing\nto a more comprehensive understanding of yogurt fermentation dynamics.",
        "In this work, we present a result on the local existence and uniqueness of\nsolutions to nonlinear Partial Differential-Algebraic Equations (PDAEs). By\napplying established theoretical results, we identify the conditions that\nguarantee the existence of a unique local solution. The analysis relies on\ntechniques from functional analysis, semi-group theory, and the theory of\ndifferential-algebraic systems. Additionally, we provide applications to\nillustrate the effectiveness of this result.",
        "Ischaemic stroke, a leading cause of death and disability, critically relies\non neuroimaging for characterising the anatomical pattern of injury.\nDiffusion-weighted imaging (DWI) provides the highest expressivity in ischemic\nstroke but poses substantial challenges for automated lesion segmentation:\nsusceptibility artefacts, morphological heterogeneity, age-related\ncomorbidities, time-dependent signal dynamics, instrumental variability, and\nlimited labelled data. Current U-Net-based models therefore underperform, a\nproblem accentuated by inadequate evaluation metrics that focus on mean\nperformance, neglecting anatomical, subpopulation, and acquisition-dependent\nvariability. Here, we present a high-performance DWI lesion segmentation tool\naddressing these challenges through optimized vision transformer-based\narchitectures, integration of 3563 annotated lesions from multi-site data, and\nalgorithmic enhancements, achieving state-of-the-art results. We further\npropose a novel evaluative framework assessing model fidelity, equity (across\ndemographics and lesion subtypes), anatomical precision, and robustness to\ninstrumental variability, promoting clinical and research utility. This work\nadvances stroke imaging by reconciling model expressivity with domain-specific\nchallenges and redefining performance benchmarks to prioritize equity and\ngeneralizability, critical for personalized medicine and mechanistic research.",
        "Analyzing crime events is crucial to understand crime dynamics and it is\nlargely helpful for constructing prevention policies. Point processes specified\non linear networks can provide a more accurate description of crime incidents\nby considering the geometry of the city. We propose a spatio-temporal Dirichlet\nprocess mixture model on a linear network to analyze crime events in Valencia,\nSpain. We propose a Bayesian hierarchical model with a Dirichlet process prior\nto automatically detect space-time clusters of the events and adopt a\nconvolution kernel estimator to account for the network structure in the city.\nFrom the fitted model, we provide crime hotspot visualizations that can inform\nsocial interventions to prevent crime incidents. Furthermore, we study the\nrelationships between the detected cluster centers and the city's amenities,\nwhich provides an intuitive explanation of criminal contagion.",
        "Node classification in graphs aims to predict the categories of unlabeled\nnodes by utilizing a small set of labeled nodes. However, weighted graphs often\ncontain noisy edges and anomalous edge weights, which can distort fine-grained\nrelationships between nodes and hinder accurate classification. We propose the\nEdge Weight-aware Graph Structure Learning (EWGSL) method, which combines\nweight learning and graph structure learning to address these issues. EWGSL\nimproves node classification by redefining attention coefficients in graph\nattention networks to incorporate node features and edge weights. It also\napplies graph structure learning to sparsify attention coefficients and uses a\nmodified InfoNCE loss function to enhance performance by adapting to denoised\ngraph weights. Extensive experimental results show that EWGSL has an average\nMicro-F1 improvement of 17.8% compared with the best baseline."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Llamafactory: Unified efficient fine-tuning of 100+ language models",
    "start_abstract":"Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
      ],
      "abstract":[
        "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "CoverM: Read alignment statistics for metagenomics",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Causes of evolutionary divergence in prostate cancer",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Radiative decays of $X(3872)$ in $D{\\bar D}^*$ molecule scenario",
        "Spontaneous in-plane anomalous Hall response observed in a ferromagnetic\n  oxide",
        "Hyperbolicity and Volume of Hyperbolic Bongles",
        "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism",
        "Solar prosumage under different pricing regimes: Interactions with the\n  transmission grid",
        "Finite-temperature bubble nucleation with shifting scale hierarchies",
        "Residually finite amenable groups that are not Hilbert-Schmidt stable",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "1-shifted Lie bialgebras and their quantizations",
        "Application of the Pontryagin Maximum Principle to the robust\n  time-optimal control of two-level quantum systems",
        "Ensemble control of n-level quantum systems with a scalar control",
        "A Differential Index Measuring Rater's Capability in Educational\n  Assessment",
        "Two-dimensional higher-order Weyl semimetals",
        "Background-field method and QCD factorization",
        "Powerful rank verification for multivariate Gaussian data with any\n  covariance structure"
      ],
      "abstract":[
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "We investigate the radiative decays of the $X(3872)$ to $\\gamma\nV~(V=\\rho^0,\\, \\omega)$ in the molecule scenario, where the $X(3872)$ is\nregarded as a pure hadronic molecule of the $D\\bar{D}^*+c.c$ in an $S$-wave\nwith the quantum numbers $J^{PC}=1^{++}$. The radiative processes were assumed\nto occur via the triangle hadronic loops, and the relevant calculations were\nconducted using an effective Lagrangian approach. It is found that the absolute\ndecay widths are model-dependent, but the relative width ratio is rather\nindependent of the model parameter. Moreover, the calculated results indicate\nthat the radiative decays of the $X(3872)$ are strongly influenced by the\nmolecular configuration characterized by the proportion of the charged and\nneutral constituents. We hope that the present calculations could be tested by\nthe experimental measurements.",
        "Recent observation of anomalous Hall effect (AHE) induced by magnetic field\nor spin magnetization lying in the Hall deflection plane has sparked interest\nin diverse mechanisms for inducing the Hall vector component perpendicular to\nthe applied magnetic field. Such off-diagonal coupling, which is strictly\nconstrained by symmetry of the system, provides new degrees of freedom for\nengineering Hall responses. However, spontaneous response as extensively\nstudied for out-of-plane AHE remains unexplored. Here we elucidate in-plane AHE\nin a typical ferromagnetic oxide SrRuO$_3$. The (111)-orientated ultrathin\nfilms with in-plane easy axes of spin magnetization exhibit spontaneous AHE at\nzero field, which is intrinsically coupled to the in-plane spin magnetization\nand controllable via its direction. Systematic measurements by varying\nazimuthal and polar field angles further reveal complex Hall responses shaped\nby higher-order terms allowed by trigonal distortion of the films. Our findings\nhighlight versatile and controllable in-plane Hall responses with out-of-plane\norbital ferromagnetism.",
        "We consider a simple but infinite class of staked links known as bongles. We\nprovide necessary and sufficient conditions for these bongles to be hyperbolic.\nThen, we prove that all balanced hyperbolic $n$-bongles have the same volume\nand the corresponding volume is an upper bound on the volume of any hyperbolic\n$n$-bongle for $n$ even. Moreover, all hyperbolic $n$-bongles have volume\nstrictly less than $5n(1.01494\\dots)$. We also include explicit volume\ncalculations for all hyperbolic 3-bongles through 6-bongles.",
        "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.",
        "Solar prosumers, residential electricity consumers equipped with photovoltaic\n(PV) systems and battery storage, are transforming electricity markets. Their\ninteractions with the transmission grid under varying tariff designs are not\nyet fully understood. We explore the influence of different pricing regimes on\nprosumer investment and dispatch decisions and their subsequent impact on the\ntransmission grid. Using an integrated modeling approach that combines two\nopen-source dispatch, investment and grid models, we simulate prosumage\nbehavior in Germany's electricity market under real-time pricing or\ntime-invariant pricing, as well as under zonal or nodal pricing. Our findings\nshow that zonal pricing favors prosumer investments, while time-invariant\npricing rather hinders it. In comparison, regional solar availability emerges\nas a larger driver for rooftop PV investments. The impact of prosumer\nstrategies on grid congestion remains limited within the scope of our\nmodel-setup, in which home batteries cannot be used for energy arbitrage.",
        "Focusing on supercooled phase transitions in models with classical scale\nsymmetry, we formulate a state-of-the art framework for computing the\nbubble-nucleation rate, accounting for the presence of various energy scales.\nIn particular, we examine the limitations of derivative expansions in\nconstructing a thermal effective field theory for bubble nucleation. We show\nthat for gauge field fluctuations, derivative expansions diverge after the\nleading two orders due to the strong variation in gauge field masses between\nthe high- and low-temperature phases. By directly computing these contributions\nusing the fluctuation determinant, we capture these effects while also\naccounting for large explicit logarithms at two loops, utilising the exact\nrenormalisation group structure of the EFT. Finally, we demonstrate how this\napproach significantly improves nucleation rate calculations compared to\nleading-order results, providing a more robust framework for predicting\ngravitational-wave signals from supercooled phase transitions in models such as\nthe SU(2)cSM.",
        "We construct the first examples of residually finite amenable groups that are\nnot Hilbert-Schmidt (HS) stable. We construct finitely generated, class 3\nnilpotent by cyclic examples and solvable linear finitely presented examples.\nThis also provides the first examples of amenable groups that are very flexibly\nHS-stable but not flexibly HS-stable and the first examples of residually\nfinite amenable groups that are not locally HS-stable. Along the way we exhibit\n(necessarily not-finitely-generated) class 2 nilpotent groups $G = A\\rtimes \\Z$\nwith $A$ abelian such that the periodic points of the dual action are dense but\nit does not admit dense periodic measures. Finally we use the\nTikuisis-White-Winter theorem to show all of the examples are not even\noperator-HS-stable; they admit operator norm almost homomorphisms that can not\nbe HS-perturbed to true homomorphisms.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "In this paper, we define (cohomologically) 1-shifted Manin triples and\n1-shifted Lie bialgebras, and study their properties. We derive many results\nthat are parallel to those found in ordinary Lie bialgebras, including the\ndouble construction and the existence of a 1-shifted $r$-matrix satisfying the\nclassical Yang-Baxter equation.\n  Turning to quantization, we first construct a canonical quantization for each\n1-shifted metric Lie algebra $\\mathfrak{g}$, producing a deformation to the\nsymmetric monoidal category of $\\mathfrak{g}$ modules over a formal variable\n$\\hbar$. This quantization is in terms of a curved differential graded algebra.\nUnder a further technical assumption, we construct quantizations of transverse\nLagrangian subalgebras of $\\mathfrak{g}$, which is a pair of DG algebras\nconnected by Koszul duality, and give rise to monoidal module categories of the\nquantized double.\n  Finally, we apply this to Manin triples arising from Lie algebras of loop\ngroups, and construct 1-shifted meromorphic $r$-matrices. The resulting\nquantizations are the cohomologically-shifted analogue of Yangians.",
        "We study the time-optimal robust control of a two-level quantum system\nsubjected to field inhomogeneities. We apply the Pontryagin Maximum Principle\nand we introduce a reduced space onto which the optimal dynamics is projected\ndown. This reduction leads to a complete analytical derivation of the optimal\nsolution in terms of elliptic functions and elliptic integrals. Necessary\noptimality conditions are then obtained for the original system. These\nconditions are verified numerically and lead to the optimal control protocol.\nVarious examples, ranging from state-to-state transfer to the generation of a\nNot gate, illustrate this study. The connection with other geometric\noptimization approaches that have been used to solve this problem is also\ndiscussed.",
        "In this paper we discuss how a general bilinear finite-dimensional closed\nquantum system with dispersed parameters can be steered between eigenstates. We\nshow that, under suitable conditions on the separation of spectral gaps and the\nboundedness of parameter dispersion, rotating wave and adiabatic approximations\ncan be employed in cascade to achieve population inversion between arbitrary\neigenstates. We propose an explicit control law and test numerically the\nsharpness of the conditions on several examples.",
        "A rater's ability to assign accurate scores can significantly impact the\noutcomes of educational assessments. However, common indices for evaluating\nrater characteristics typically focus on either their severity or their\ndiscrimination ability (i.e., skills to differentiate between students).\nAdditionally, these indices are often developed without considering the rater's\naccuracy in scoring students at different ability levels. To address the\nlimitations, this study proposes a single-value measure to assess a rater's\ncapability of assigning accurate scores to students with varying ability\nlevels. The measure is derived from the partial derivatives of each rater's\npassing rate concerning student ability. Mathematical derivations of the index\nunder generalized multi-facet models and hierarchical rater models are\nprovided. To ease the implementation of the index, this study develops\nparameter estimation using marginal likelihood and its Laplacian approximation\nwhich allows for efficient evaluation and processing of large datasets\ninvolving numerous students and raters. Simulation studies demonstrate the\naccuracy of parameter recovery using the approximate likelihood and show how\nthe capability indices vary with different levels of rater severity. An\nempirical study further tests the practical applicability of the new measure,\nwhere raters evaluate essays on four topics: \"family,\" \"school,\" \"sport,\" and\n\"work.\" Results show that raters are most capable when rating the topic of\nfamily and least capable when rating sport, with individual raters displaying\ndifferent capabilities across the various topics.",
        "We propose a theoretical scheme to realize two-dimensional higher-order Weyl\nsemimetals using a trilayer topological insulator film coupled with a d-wave\naltermagnet. Our results show that the trilayer topological insulator exhibits\ntwo-dimensional Weyl semimetal characteristics with helical edge states.\nNotably, the Weyl points are located at four high-symmetry points in the\nBrillouin zone, and the topology of symmetric subspaces governs the formation\nof these Weyl points and edge states. Upon introducing a d-wave altermagnet\noriented along the z-direction, gaps open in the helical edge states while\npreserving two Weyl points, leading to the realization of two-dimensional\nhigher-order Weyl semimetals hosting topological corner states. The nonzero\nwinding number in the subspace along the high-symmetry line serves as a\ntopological invariant characterizing these corner states, and the other\nsubspace Hamiltonian confirms the existence of the Weyl points. Finally, a\ntopological phase diagram provides a complete topological description of the\nsystem.",
        "One method for deriving a factorization for QCD processes is to use\nsuccessive integration over fields in the functional integral. In this\napproach, we separate the fields into two categories: dynamical fields with\nmomenta above a relevant cutoff, and background fields with momenta below the\ncutoff. The dynamical fields are then integrated out in the background of the\nlow-momentum background fields. This strategy works well at tree level,\nallowing us to quickly derive QCD factorization formulas at leading order.\nHowever, to extend the approach to higher loops, it is necessary to rigorously\ndefine the functional integral over dynamical fields in an arbitrary background\nfield. This framework was carefully developed for the calculation of the\neffective action in a background field at the two-loop level in the classic\npaper by Abbott [1]. Building on this work, I specify the renormalized\nbackground-field Lagrangian and define the notion of the quantum average of an\noperator in a background field, consistent with the ``separation of scales''\nscheme mentioned earlier. As examples, I discuss the evolution of the twist-2\ngluon light-ray operator and the one-loop gluon propagator in a background\nfield near the light cone.",
        "Upon observing $n$-dimensional multivariate Gaussian data, when can we infer\nthat the largest $K$ observations came from the largest $K$ means? When $K=1$\nand the covariance is isotropic, \\cite{Gutmann} argue that this inference is\njustified when the two-sided difference-of-means test comparing the largest and\nsecond largest observation rejects. Leveraging tools from selective inference,\nwe provide a generalization of their procedure that applies for both any $K$\nand any covariance structure. We show that our procedure draws the desired\ninference whenever the two-sided difference-of-means test comparing the pair of\nobservations inside and outside the top $K$ with the smallest standardized\ndifference rejects, and sometimes even when this test fails to reject. Using\nthis insight, we argue that our procedure renders existing simultaneous\ninference approaches inadmissible when $n > 2$. When the observations are\nindependent (with possibly unequal variances) or equicorrelated, our procedure\ncorresponds exactly to running the two-sided difference-of-means test comparing\nthe pair of observations inside and outside the top $K$ with the smallest\nstandardized difference."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model",
    "start_abstract":"Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Llamafactory: Unified efficient fine-tuning of 100+ language models"
      ],
      "abstract":[
        "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
        "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual\n  Verification of Machine-Generated Text",
        "SurveyX: Academic Survey Automation via Large Language Models",
        "Do as We Do, Not as You Think: the Conformity of Large Language Models",
        "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
        "Story Grammar Semantic Matching for Literary Study",
        "Controllable Emotion Generation with Emotion Vectors",
        "RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for\n  Inspirational Quote Extraction from Long Documents",
        "Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire\n  Articles Beyond the Front Page",
        "A Study of the Plausibility of Attention between RNN Encoders in Natural\n  Language Inference",
        "Benchmarking Abstractive Summarisation: A Dataset of Human-authored\n  Summaries of Norwegian News Articles",
        "HonkaiChat: Companions from Anime that feel alive!",
        "Automating Mathematical Proof Generation Using Large Language Model\n  Agents and Knowledge Graphs",
        "Computation of the Hilbert Series for the Support-Minors Modeling of the\n  MinRank Problem",
        "A glimpse into an effective world",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "Giant Kohn anomaly and chiral phonons in the charge density wave phase\n  of 1H-NbSe$_2$",
        "A study on $T$-equivalent graphs",
        "Criteria for unbiased estimation: applications to noise-agnostic sensing\n  and learnability of quantum channel",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Applied Machine Learning Methods with Long-Short Term Memory Based\n  Recurrent Neural Networks for Multivariate Temperature Prediction",
        "GenMetaLoc: Learning to Learn Environment-Aware Fingerprint Generation\n  for Sample Efficient Wireless Localization",
        "On the structure of some one-generator nilpotent braces",
        "Multi-Instance Partial-Label Learning with Margin Adjustment",
        "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility",
        "Evaluation of Large Language Models via Coupled Token Generation",
        "Hand-Object Contact Detection using Grasp Quality Metrics",
        "Processes on Wasserstein spaces and energy-minimizing particle\n  representations in fractional Sobolev spaces"
      ],
      "abstract":[
        "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
        "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext.",
        "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn",
        "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.",
        "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a state-of\nthe-art 54% success rate on the WebArena-Lite benchmark.",
        "In Natural Language Processing (NLP), semantic matching algorithms have\ntraditionally relied on the feature of word co-occurrence to measure semantic\nsimilarity. While this feature approach has proven valuable in many contexts,\nits simplistic nature limits its analytical and explanatory power when used to\nunderstand literary texts. To address these limitations, we propose a more\ntransparent approach that makes use of story structure and related elements.\nUsing a BERT language model pipeline, we label prose and epic poetry with story\nelement labels and perform semantic matching by only considering these labels\nas features. This new method, Story Grammar Semantic Matching, guides literary\nscholars to allusions and other semantic similarities across texts in a way\nthat allows for characterizing patterns and literary technique.",
        "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
        "Inspirational quotes from famous individuals are often used to convey\nthoughts in news articles, essays, and everyday conversations. In this paper,\nwe propose a novel context-based quote extraction system that aims to extract\nthe most relevant quote from a long text. We formulate this quote extraction as\nan open domain question answering problem first by employing a vector-store\nbased retriever and then applying a multi-task reader. We curate three\ncontext-based quote extraction datasets and introduce a novel multi-task\nframework RA-MTR that improves the state-of-the-art performance, achieving a\nmaximum improvement of 5.08% in BoW F1-score.",
        "I introduce a new large-scale dataset of historical wire articles from U.S.\nSouthern newspapers, spanning 1960-1975 and covering multiple wire services:\nThe Associated Press, United Press International, Newspaper Enterprise\nAssociation. Unlike prior work focusing on front-page content, this dataset\ncaptures articles across the entire newspaper, offering broader insight into\nmid-century Southern coverage. The dataset includes a version that has\nundergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its\nsuitability for quantitative text analysis. Additionally, duplicate versions of\narticles are retained to enable analysis of editorial differences in language\nand framing across newspapers. Each article is tagged by wire service,\nfacilitating comparative studies of editorial patterns across agencies. This\nresource opens new avenues for research in computational social science,\ndigital humanities, and historical linguistics, providing a detailed\nperspective on how Southern newspapers relayed national and international news\nduring a transformative period in American history. The dataset will be made\navailable upon publication or request for research purposes.",
        "Attention maps in neural models for NLP are appealing to explain the decision\nmade by a model, hopefully emphasizing words that justify the decision. While\nmany empirical studies hint that attention maps can provide such justification\nfrom the analysis of sound examples, only a few assess the plausibility of\nexplanations based on attention maps, i.e., the usefulness of attention maps\nfor humans to understand the decision. These studies furthermore focus on text\nclassification. In this paper, we report on a preliminary assessment of\nattention maps in a sentence comparison task, namely natural language\ninference. We compare the cross-attention weights between two RNN encoders with\nhuman-based and heuristic-based annotations on the eSNLI corpus. We show that\nthe heuristic reasonably correlates with human annotations and can thus\nfacilitate evaluation of plausible explanations in sentence comparison tasks.\nRaw attention weights however remain only loosely related to a plausible\nexplanation.",
        "We introduce a dataset of high-quality human-authored summaries of news\narticles in Norwegian. The dataset is intended for benchmarking the abstractive\nsummarisation capabilities of generative language models. Each document in the\ndataset is provided with three different candidate gold-standard summaries\nwritten by native Norwegian speakers, and all summaries are provided in both of\nthe written variants of Norwegian -- Bokm{\\aa}l and Nynorsk. The paper\ndescribes details on the data creation effort as well as an evaluation of\nexisting open LLMs for Norwegian on the dataset. We also provide insights from\na manual human evaluation, comparing human-authored to model-generated\nsummaries. Our results indicate that the dataset provides a challenging LLM\nbenchmark for Norwegian summarisation capabilities",
        "Modern conversational agents, including anime-themed chatbots, are frequently\nreactive and personality-driven but fail to capture the dynamic nature of human\ninteractions. We propose an event-driven dialogue framework to address these\nlimitations by embedding dynamic events in conversation prompts and fine-tuning\nmodels on character-specific data. Evaluations on GPT-4 and comparisons with\nindustry-leading baselines demonstrate that event-driven prompts significantly\nimprove conversational engagement and naturalness while reducing\nhallucinations. This paper explores the application of this approach in\ncreating lifelike chatbot interactions within the context of Honkai: Star Rail,\nshowcasing the potential for dynamic event-based systems to transform\nrole-playing and interactive dialogue.",
        "Large Language Models have demonstrated remarkable capabilities in natural\nlanguage processing tasks, including mathematical problem-solving that requires\nmulti-step logical reasoning. However, challenges persist in automating the\nidentification of key mathematical concepts, understanding their\ninterrelations, and formalizing proofs within a rigorous framework. We present\na novel framework that leverages knowledge graphs to augment LLMs to construct\nand formalize mathematical proofs. Our results demonstrate significant\nperformance improvements across multiple datasets, with using knowledge graphs,\nachieving up to a 34% success rate on the MUSTARDSAUCE dataset on o1-mini and\nconsistently outperforming baseline approaches by 2-11% across different\nmodels. We show how this approach bridges the gap between natural language\nunderstanding and formal logic proof systems and achieve elevated results for\nfoundation models over baseline.",
        "The MinRank problem is a simple linear algebra problem: given matrices with\ncoefficients in a field, find a non trivial linear combination of the matrices\nthat has a small rank. There are several algebraic modeling of the problem. The\nmain ones are: the Kipnis-Shamir modeling, the Minors modeling and the\nSupport-Minors modeling. The Minors modeling has been studied by Faug\\`ere et\nal. in 2010, where the authors provide an analysis of the complexity of\ncomputing a Gr\\\"obner basis of the modeling, through the computation of the\nexact Hilbert Series for a generic instance. For the Support-Minors modeling,\nthe first terms of the Hilbert Series are given by Bardet et al. in 2020 based\non an heuristic and experimental work. In this work, we provide a formula and a\nproof for the complete Hilbert Series of the Support Minors modeling for\ngeneric instances. This is done by adapting well known results on determinantal\nideals to an ideal generated by a particular subset of the set of all minors of\na matrix of variables. We then show that this ideal is generated by a\nparticular subset of the set of all minors of a matrix of variables. We then\nshow that this ideal is generated by standard monomials having a particular\nshape, and derive the Hilbert Series by counting the number of such standard\nmonomials. Following the work done for the Minors Modeling, we then transfer\nthe properties of this particular determinantal ideal to ideals generated by\nthe Support Minors system, by adding generic forms. This work allows to make a\nprecise comparison between the Minors and Support Minors modeling, and a\nprecise estimate of the complexity of solving MinRank instances for the\nparameters of the Mirath signature scheme that is currently at the second round\nof the NIST standardization process for Additional Digital Signature Schemes.",
        "Our contribution aims to celebrate the immeasurable contribution that Tom Kuo\nhas provided to the understanding of the structure of atomic nuclei, and also\nof the infinite nuclear matter, in terms of the fundamental principles\ngoverning the realistic nuclear potential. The authors want to testify Tom\nKuo's heritage and impact on their approach to the study of nuclear systems by\nreviewing some recent findings on the role of the two-body component of\nshell-model effective $\\beta$-decay operators. The focus is spotted on the\nso-called Pauli-blocking effect, that plays a non-negligible role in nuclei\ncharacterized by a large number of valence nucleons.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "Despite extensive investigations, many aspects of charge density waves (CDWs)\nremain elusive, especially the relative roles of electron-phonon coupling and\nFermi surface nesting as the underlying driving mechanisms responsible for the\nemergence of the CDW vector $\\bl Q_{CDW}$. It is puzzling that even though\nelectrons interact strongly with optical phonons in many correlated systems,\nthe actual mode softening is of an acoustic mode. Here we consider monolayer\n1H-NbSe$_2$ as an exemplar system, and through an accurate computation of the\nphonon self-energy, including its off-diagonal components. We provide\ncompelling evidence that the relevant mode is a longitudinal optical phonon\nthat softens by anti-crossing several intervening phonon bands. We also show\nthat $\\bl Q_{CDW}$ is fixed by the convolution of the susceptibility and\nelectron-phonon coupling, and that the softened phonons are circularly\npolarized.",
        "In his article [J. Comb. Theory Ser. B 16 (1974), 168-174], Tutte called two\ngraphs $T$-equivalent (i.e., codichromatic) if they have the same Tutte\npolynomial and showed that graphs $G$ and $G'$ are $T$-equivalent if $G'$ is\nobtained from $G$ by flipping a rotor (i.e., replacing it by its mirror) of\norder at most $5$, where a rotor of order $k$ in $G$ is an induced subgraph $R$\nhaving an automorphism $\\psi$ with a vertex orbit $\\{\\psi^i(u): i\\ge 0\\}$ of\nsize $k$ such that every vertex of $R$ is only adjacent to vertices in $R$\nunless it is in this vertex orbit. In this article, we first show the above\nresult due to Tutte can be extended to a rotor $R$ of order $k\\ge 6$ if the\nsubgraph of $G$ induced by all those edges of $G$ which are not in $R$\nsatisfies certain conditions. Also, we provide a new method for generating\ninfinitely many non-isomorphic $T$-equivalent pairs of graphs.",
        "We establish the necessary and sufficient conditions for unbiased estimation\nin multi-parameter estimation tasks. More specifically, we first consider\nquantum state estimation, where multiple parameters are encoded in a quantum\nstate, and derive two equivalent necessary and sufficient conditions for an\nunbiased estimation: one formulated in terms of the quantum Fisher information\nmatrix (QFIM) and the other based on the derivatives of the encoded state.\nFurthermore, we introduce a generalized quantum Cram\\'er-Rao bound, which\nprovides a fundamental achievable lower bound on the estimation error even when\nthe QFIM is non-invertible. To demonstrate the utility of our framework, we\nconsider phase estimation under unknown Pauli noise. We show that while\nunbiased phase estimation is infeasible with a naive scheme, employing an\nentangled probe with a noiseless ancilla enables unbiased estimation. Next, we\nextend our analysis to quantum channel estimation (equivalently, quantum\nchannel learning), where the goal is to estimate parameters characterizing an\nunknown quantum channel. We establish the necessary and sufficient condition\nfor unbiased estimation of these parameters. Notably, by interpreting unbiased\nestimation as learnability, our result applies to the fundamental learnability\nof parameters in general quantum channels. As a concrete application, we\ninvestigate the learnability of noise affecting non-Clifford gates via cycle\nbenchmarking.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "This paper gives an overview on how to develop a dense and deep neural\nnetwork for making a time series prediction. First, the history and\ncornerstones in Artificial Intelligence and Machine Learning will be presented.\nAfter a short introduction to the theory of Artificial Intelligence and Machine\nLearning, the paper will go deeper into the techniques for conducting a time\nseries prediction with different models of neural networks. For this project,\nPython's development environment Jupyter, extended with the TensorFlow package\nand deep-learning application Keras is used. The system setup and project\nframework are explained in more detail before discussing the time series\nprediction. The main part shows an applied example of time series prediction\nwith weather data. For this work, a deep recurrent neural network with Long\nShort-Term Memory cells is used to conduct the time series prediction. The\nresults and evaluation of the work show that a weather prediction with deep\nneural networks can be successful for a short time period. However, there are\nsome drawbacks and limitations with time series prediction, which will be\ndiscussed towards the end of the paper.",
        "Existing fingerprinting-based localization methods often require extensive\ndata collection and struggle to generalize to new environments. In contrast to\nprevious environment-unknown MetaLoc, we propose GenMetaLoc in this paper,\nwhich first introduces meta-learning to enable the generation of dense\nfingerprint databases from an environment-aware perspective. In the model\naspect, the learning-to-learn mechanism accelerates the fingerprint generation\nprocess by facilitating rapid adaptation to new environments with minimal data.\nAdditionally, we incorporate 3D point cloud data from the first Fresnel zone\nbetween the transmitter and receiver, which describes the obstacles\ndistribution in the environment and serves as a condition to guide the\ndiffusion model in generating more accurate fingerprints. In the data\nprocessing aspect, unlike most studies that focus solely on channel state\ninformation (CSI) amplitude or phase, we present a comprehensive processing\nthat addresses both, correcting errors from WiFi hardware limitations such as\namplitude discrepancies and frequency offsets. For the data collection\nplatform, we develop an uplink wireless localization system that leverages the\nsensing capabilities of existing commercial WiFi devices and mobile phones,\nthus reducing the need for additional deployment costs. Experimental results on\nreal datasets show that our framework outperforms baseline methods.",
        "This article provides a detailed description of some nilpotent left braces\ngenerated by one element.",
        "Multi-instance partial-label learning (MIPL) is an emerging learning\nframework where each training sample is represented as a multi-instance bag\nassociated with a candidate label set. Existing MIPL algorithms often overlook\nthe margins for attention scores and predicted probabilities, leading to\nsuboptimal generalization performance. A critical issue with these algorithms\nis that the highest prediction probability of the classifier may appear on a\nnon-candidate label. In this paper, we propose an algorithm named MIPLMA, i.e.,\nMulti-Instance Partial-Label learning with Margin Adjustment, which adjusts the\nmargins for attention scores and predicted probabilities. We introduce a\nmargin-aware attention mechanism to dynamically adjust the margins for\nattention scores and propose a margin distribution loss to constrain the\nmargins between the predicted probabilities on candidate and non-candidate\nlabel sets. Experimental results demonstrate the superior performance of MIPLMA\nover existing MIPL algorithms, as well as other well-established multi-instance\nlearning algorithms and partial-label learning algorithms.",
        "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems.",
        "State of the art large language models rely on randomization to respond to a\nprompt. As an immediate consequence, a model may respond differently to the\nsame prompt if asked multiple times. In this work, we argue that the evaluation\nand ranking of large language models should control for the randomization\nunderpinning their functioning. Our starting point is the development of a\ncausal model for coupled autoregressive generation, which allows different\nlarge language models to sample responses with the same source of randomness.\nBuilding upon our causal model, we first show that, on evaluations based on\nbenchmark datasets, coupled autoregressive generation leads to the same\nconclusions as vanilla autoregressive generation but using provably fewer\nsamples. However, we further show that, on evaluations based on (human)\npairwise comparisons, coupled and vanilla autoregressive generation can\nsurprisingly lead to different rankings when comparing more than two models,\neven with an infinite amount of samples. This suggests that the apparent\nadvantage of a model over others in existing evaluation protocols may not be\ngenuine but rather confounded by the randomness inherent to the generation\nprocess. To illustrate and complement our theoretical results, we conduct\nexperiments with several large language models from the Llama family. We find\nthat, across multiple knowledge areas from the popular MMLU benchmark dataset,\ncoupled autoregressive generation requires up to 40% fewer samples to reach the\nsame conclusions as vanilla autoregressive generation. Further, using data from\nthe LMSYS Chatbot Arena platform, we find that the win-rates derived from\npairwise comparisons by a strong large language model to prompts differ under\ncoupled and vanilla autoregressive generation.",
        "We propose a novel hand-object contact detection system based on grasp\nquality metrics extracted from object and hand poses, and evaluated its\nperformance using the DexYCB dataset. Our evaluation demonstrated the system's\nhigh accuracy (approaching 90%). Future work will focus on a real-time\nimplementation using vision-based estimation, and integrating it to a\nrobot-to-human handover system.",
        "Given a probability-measure-valued process $(\\mu_t)$, we aim to find, among\nall path-continuous stochastic processes whose one-dimensional time marginals\ncoincide almost surely with $(\\mu_t)$ (if there is any), a process that\nminimizes a given energy in expectation. Building on our recent study\n(arXiv:2502.12068), where the minimization of fractional Sobolev energy was\ninvestigated for deterministic paths on Wasserstein spaces, we now extend the\nresults to the stochastic setting to address some applications that originally\nmotivated our study. Two applications are given. We construct minimizing\nparticle representations for processes on Wasserstein spaces on $\\mathbb{R}$\nwith H\\\"{o}lder regularity, using optimal transportation. We prove the\nexistence of minimizing particle representations for solutions to stochastic\nFokker--Planck--Kolmogorov equations on $\\mathbb{R}^\\mathrm{d}$ satisfying an\nintegrability condition, using the stochastic superposition principle of\nLacker--Shkolnikov--Zhang (J. Eur. Math. Soc. 25, 3229--3288 (2023))."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"2016 Alzheimer's disease facts and figures",
    "start_abstract":"This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
      ],
      "abstract":[
        "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "HCAST: Human-Calibrated Autonomy Software Tasks",
        "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks",
        "Towards A Litmus Test for Common Sense",
        "CollabLLM: From Passive Responders to Active Collaborators",
        "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management",
        "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning",
        "ARES: Auxiliary Range Expansion for Outlier Synthesis",
        "Machine Learning in Biomechanics: Key Applications and Limitations in\n  Walking, Running, and Sports Movements",
        "Large Language Models and Mathematical Reasoning Failures",
        "OmniScience: A Domain-Specialized LLM for Scientific Reasoning and\n  Discovery",
        "Decision Information Meets Large Language Models: The Future of\n  Explainable Operations Research",
        "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
        "SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning",
        "Dirichlet's Lemma in Number Fields",
        "Forecasting Monthly Residential Natural Gas Demand Using\n  Just-In-Time-Learning Modeling",
        "Non-Hermitian Aharonov-Bohm Cage in Bosonic Bogoliubov-de Gennes Systems",
        "Einstein multiply warped products and generalized Kasner manifolds with\n  multidimensional base",
        "Probing Topological Anderson Transition in Quasiperiodic Photonic\n  Lattices via Chiral Displacement and Wavelength Tuning",
        "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
        "Subwavelength plasmonic antennas based on asymmetric\n  split-ring-resonators for high near-field enhancements",
        "Poisoning Bayesian Inference via Data Deletion and Replication",
        "Parking Space Detection in the City of Granada",
        "COFO: COdeFOrces dataset for Program Classification, Recognition and\n  Tagging",
        "Estimating treatment effects with competing intercurrent events in\n  randomized controlled trials",
        "$\\eta$, $\\eta^\\prime$ mesons from lattice QCD in fully physical\n  conditions",
        "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "An empirical formulation of accelerated molecular dynamics for\n  simulating and predicting microstructure evolution in materials"
      ],
      "abstract":[
        "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours.",
        "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability.",
        "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining\/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence.",
        "Large Language Models are typically trained with next-turn rewards, limiting\ntheir ability to optimize for long-term interaction. As a result, they often\nrespond passively to ambiguous or open-ended user requests, failing to help\nusers reach their ultimate intents and leading to inefficient conversations. To\naddress these limitations, we introduce CollabLLM, a novel and general training\nframework that enhances multiturn human-LLM collaboration. Its key innovation\nis a collaborative simulation that estimates the long-term contribution of\nresponses using Multiturn-aware Rewards. By reinforcement fine-tuning these\nrewards, CollabLLM goes beyond responding to user requests, and actively\nuncovers user intent and offers insightful suggestions-a key step towards more\nhuman-centered AI. We also devise a multiturn interaction benchmark with three\nchallenging tasks such as document creation. CollabLLM significantly\noutperforms our baselines with averages of 18.5% higher task performance and\n46.3% improved interactivity by LLM judges. Finally, we conduct a large user\nstudy with 201 judges, where CollabLLM increases user satisfaction by 17.6% and\nreduces user spent time by 10.4%.",
        "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
        "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
        "Recent successes of artificial intelligence and deep learning often depend on\nthe well-collected training dataset which is assumed to have an identical\ndistribution with the test dataset. However, this assumption, which is called\nclosed-set learning, is hard to meet in realistic scenarios for deploying deep\nlearning models. As one of the solutions to mitigate this assumption, research\non out-of-distribution (OOD) detection has been actively explored in various\ndomains. In OOD detection, we assume that we are given the data of a new class\nthat was not seen in the training phase, i.e., outlier, at the evaluation\nphase. The ultimate goal of OOD detection is to detect and classify such unseen\noutlier data as a novel \"unknown\" class. Among various research branches for\nOOD detection, generating a virtual outlier during the training phase has been\nproposed. However, conventional generation-based methodologies utilize\nin-distribution training dataset to imitate outlier instances, which limits the\nquality of the synthesized virtual outlier instance itself. In this paper, we\npropose a novel methodology for OOD detection named Auxiliary Range Expansion\nfor Outlier Synthesis, or ARES. ARES models the region for generating\nout-of-distribution instances by escaping from the given in-distribution\nregion; instead of remaining near the boundary of in-distribution region.\nVarious stages consists ARES to ultimately generate valuable OOD-like virtual\ninstances. The energy score-based discriminator is then trained to effectively\nseparate in-distribution data and outlier data. Quantitative experiments on\nbroad settings show the improvement of performance by our method, and\nqualitative results provide logical explanations of the mechanism behind it.",
        "This chapter provides an overview of recent and promising Machine Learning\napplications, i.e. pose estimation, feature estimation, event detection, data\nexploration & clustering, and automated classification, in gait (walking and\nrunning) and sports biomechanics. It explores the potential of Machine Learning\nmethods to address challenges in biomechanical workflows, highlights central\nlimitations, i.e. data and annotation availability and explainability, that\nneed to be addressed, and emphasises the importance of interdisciplinary\napproaches for fully harnessing the potential of Machine Learning in gait and\nsports biomechanics.",
        "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling.",
        "Large Language Models (LLMs) have demonstrated remarkable potential in\nadvancing scientific knowledge and addressing complex challenges. In this work,\nwe introduce OmniScience, a specialized large reasoning model for general\nscience, developed through three key components: (1) domain adaptive\npretraining on a carefully curated corpus of scientific literature, (2)\ninstruction tuning on a specialized dataset to guide the model in following\ndomain-specific tasks, and (3) reasoning-based knowledge distillation through\nfine-tuning to significantly enhance its ability to generate contextually\nrelevant and logically sound responses. We demonstrate the versatility of\nOmniScience by developing a battery agent that efficiently ranks molecules as\npotential electrolyte solvents or additives. Comprehensive evaluations reveal\nthat OmniScience is competitive with state-of-the-art large reasoning models on\nthe GPQA Diamond and domain-specific battery benchmarks, while outperforming\nall public reasoning and non-reasoning models with similar parameter counts. We\nfurther demonstrate via ablation experiments that domain adaptive pretraining\nand reasoning-based knowledge distillation are critical to attain our\nperformance levels, across benchmarks.",
        "Operations Research (OR) is vital for decision-making in many industries.\nWhile recent OR methods have seen significant improvements in automation and\nefficiency through integrating Large Language Models (LLMs), they still\nstruggle to produce meaningful explanations. This lack of clarity raises\nconcerns about transparency and trustworthiness in OR applications. To address\nthese challenges, we propose a comprehensive framework, Explainable Operations\nResearch (EOR), emphasizing actionable and understandable explanations\naccompanying optimization. The core of EOR is the concept of Decision\nInformation, which emerges from what-if analysis and focuses on evaluating the\nimpact of complex constraints (or parameters) changes on decision-making.\nSpecifically, we utilize bipartite graphs to quantify the changes in the OR\nmodel and adopt LLMs to improve the explanation capabilities. Additionally, we\nintroduce the first industrial benchmark to rigorously evaluate the\neffectiveness of explanations and analyses in OR, establishing a new standard\nfor transparency and clarity in the field.",
        "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
        "Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.",
        "Dirichlet's Lemma states that every primitive quadratic Dirichlet character\n$\\chi$ can be written in the form $\\chi(n) = (\\frac{\\Delta}n)$ for a suitable\nquadratic discriminant $\\Delta$. In this article we define a group, the\nseparant class group, that measures the extent to which Dirichlet's Lemma fails\nin general number fields $F$. As an application we will show that over fields\nwith trivial separant class groups, genus theory of quadratic extensions can be\nmade as explicit as over the rationals.",
        "Natural gas (NG) is relatively a clean source of energy, particularly\ncompared to fossil fuels, and worldwide consumption of NG has been increasing\nalmost linearly in the last two decades. A similar trend can also be seen in\nTurkey, while another similarity is the high dependence on imports for the\ncontinuous NG supply. It is crucial to accurately forecast future NG demand\n(NGD) in Turkey, especially, for import contracts; in this respect, forecasts\nof monthly NGD for the following year are of utmost importance. In the current\nstudy, the historical monthly NG consumption data between 2014 and 2024\nprovided by SOCAR, the local residential NG distribution company for two cities\nin Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD\nforecasts for a period of one year and nine months using various time series\nmodels, including SARIMA and ETS models, and a novel proposed machine learning\nmethod. The proposed method, named Just-in-Time-Learning-Gaussian Process\nRegression (JITL-GPR), uses a novel feature representation for the past NG\ndemand values; instead of using past demand values as column-wise separate\nfeatures, they are placed on a two-dimensional (2-D) grid of year-month values.\nFor each test point, a kernel function, tailored for the NGD predictions, is\nused in GPR to predict the query point. Since a model is constructed separately\nfor each test point, the proposed method is, indeed, an example of JITL. The\nJITL-GPR method is easy to use and optimize, and offers a reduction in forecast\nerrors compared to traditional time series methods and a state-of-the-art\ncombination model; therefore, it is a promising tool for NGD forecasting in\nsimilar settings.",
        "The non-Hermitian Aharonov-Bohm (AB) cage is a unique localization phenomenon\nthat confines all possible excitations. This confinement leads to fully flat\nspectra in momentum space, which are typically accompanied with the degeneracy\nwith various types. Classifying the degeneracy type is crucial for studying the\ndynamical properties of the non-Hermitian AB cage, but the methods for such\nclassification and their physical connections remain not very clear. Here, we\nconstruct a non-Hermitian AB cage in a bosonic Bogoliubov-de Gennes (BdG)\nsystem with various types of degenerate flat bands (DFBs). Using the transfer\nmatrix, we demonstrate the localization mechanism for the formation of AB cage\nand derive the minimal polynomial in mathematics for classifying the degeneracy\ntypes of DFBs, thus providing comprehensive understanding of the correspondence\namong the degeneracy type of DFBs, the minimal polynomial, and the transfer\nmatrix. With such correspondence, we propose a scheme to realize highly\ndegenerate flat bands.",
        "The purpose of this paper is to provide conditions for the existence or non\nexistence of non trivial Einstein multiply warped products, specially of\ngeneralised Kasner type; as well as to show estimates of the Einstein parameter\nthat condition the existence of such metrics.",
        "The interplay of topology and disorder in quantum dynamics has recently\nattracted significant attention across diverse platforms, including solid-state\ndevices, ultracold atoms, and photonic systems. Here, we report on a\ntopological Anderson transition caused by quasiperiodic intra-cell coupling\ndisorder in photonic Su-Schrieffer-Heeger lattices. As the quasiperiodic\nstrength is varied, the system exhibits a reentrant transition from a trivial\nphase to a topological phase and back to a trivial phase, accompanied by the\nclosing and reopening of the band gap around zero energy. Unlike the\ntraditional detection of photonic topological edge modes, we measure the mean\nchiral displacement from the transport of light in the bulk of the lattices. In\nour photonic lattices with a fixed length, the propagation dynamics is\nretrieved by varying the wavelength of light, which tunes the inter-waveguide\ncouplings.",
        "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures.",
        "As for plasmonic antenna structures that generate localized near-field\nenhancement, the most effective current implementations are based on electric\ndipole resonance modes, but this approach also imposes limitations on their\nfurther optimization. Here we introduce an ASRR structure whose ASR mode\nenables differential charge distribution across both sides of the split.\nThrough asymmetric regulation, charges at one end can become highly localized,\nthereby achieving efficient near-field enhancement. The formation of this\nstructure was initially driven by a hybrid computational framework integrating\nevolutionary optimization with residual neural networks, and subsequently\nsimplified into an ASRR prototype using the Occam's Razor principle. The ASRR\ndimer structure can achieve an electric field intensity enhancement over 6.5\ntimes larger than a traditional nanorod dimer, while maintaining a compact size\n(<1\/3 the working wavelength). The ASRR configuration also demonstrates\nsuperior Purcell factor and fluorescence enhancement. These results can find\napplications in surface-enhanced spectroscopy, nonlinear optics, and quantum\nlight-matter interactions.",
        "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
        "This paper addresses the challenge of parking space detection in urban areas,\nfocusing on the city of Granada. Utilizing aerial imagery, we develop and apply\nsemantic segmentation techniques to accurately identify parked cars, moving\ncars and roads. A significant aspect of our research is the creation of a\nproprietary dataset specific to Granada, which is instrumental in training our\nneural network model. We employ Fully Convolutional Networks, Pyramid Networks\nand Dilated Convolutions, demonstrating their effectiveness in urban semantic\nsegmentation. Our approach involves comparative analysis and optimization of\nvarious models, including Dynamic U-Net, PSPNet and DeepLabV3+, tailored for\nthe segmentation of aerial images. The study includes a thorough\nexperimentation phase, using datasets such as UDD5 and UAVid, alongside our\ncustom Granada dataset. We evaluate our models using metrics like Foreground\nAccuracy, Dice Coefficient and Jaccard Index. Our results indicate that\nDeepLabV3+ offers the most promising performance. We conclude with future\ndirections, emphasizing the need for a dedicated neural network for parked car\ndetection and the potential for application in other urban environments. This\nwork contributes to the fields of urban planning and traffic management,\nproviding insights into efficient utilization of parking spaces through\nadvanced image processing techniques.",
        "In recent years, a lot of technological advances in computer science have\naided software programmers to create innovative and real-time user-friendly\nsoftware. With the creation of the software and the urging interest of people\nto learn to write software, there is a large collection of source codes that\ncan be found on the web, also known as Big Code, which can be used as a source\nof data for driving the machine learning applications tending to solve certain\nsoftware engineering problems. In this paper, we present COFO, a dataset\nconsisting of 809 classes\/problems with a total of 369K source codes written in\nC, C++, Java, and Python programming languages, along with other metadata such\nas code tags, problem specification, and input-output specifications. COFO has\nbeen scraped from the openly available Codeforces website using a\nselenium-beautifulsoup-python based scraper. We envision that this dataset can\nbe useful for solving machine learning-based problems like program\nclassification\/recognition, tagging, predicting program properties, and code\ncomprehension.",
        "The analysis of randomized controlled trials is often complicated by\nintercurrent events--events that occur after treatment initiation and may\nimpact outcome assessment. These events may lead to patients discontinuing\ntheir assigned treatment or dropping out of the trial entirely. In an analysis\nof data from two recent immunology trials, we categorize intercurrent events\ninto two broad types: those unrelated to treatment (e.g., withdrawal from the\nstudy due to external factors like pandemics or relocation) and those related\nto treatment (e.g., adverse events or lack of efficacy). We adopt distinct\nstrategies to handle each type, aiming to target a clinically more relevant\nestimand. For treatment-related intercurrent events, they often meaningfully\ndescribe the patient's outcome, we employ a composite variable strategy, where\nwe attribute an outcome value that reflects the lack of treatment success. For\ntreatment-unrelated intercurrent events, we adopt a hypothetical strategy that\nassumes these event times are conditionally independent of the outcome, given\ntreatment and covariates, and envisions a scenario in which the intercurrent\nevents do not occur. We establish the nonparametric identification and\nsemiparametric estimation theory for the causal estimand and introduce doubly\nrobust estimators. We illustrate our methods through the re-analysis of two\nrandomized trials on baricitinib for Systemic Lupus Erythematosus. We classify\nintercurrent events, apply four estimators, and compare our approach with\ncommon ad-hoc methods, highlighting the robustness and practical implications\nof our framework.",
        "We determine masses and mixing parameters of the $\\eta$ and $M_{\\eta^\\prime}$\nmeson in lattice QCD. The calculations are carried out on a set of 13 ETMC\ngauge ensembles with $N_f=2+1+1$ (maximally) twisted-mass Clover-improved\nquarks. These ensemble cover four values of the lattice spacing\n$a=0.057\\mathrm{fm},...,0.092\\mathrm{fm}$ and pion masses from\n$140\\mathrm{MeV}$ to $360\\mathrm{MeV}$, including three ensembles at physical\nquark masses and six ensembles with $M_\\pi<200\\mathrm{MeV}$. The strange-quark\ncontribution is treated in a mixed-action approach using Osterwalder-Seiler\nfermions to avoid complications due to flavor mixing in the heavy quark sector\nand to enable the use of the one-end trick in the computation of strange\nquark-disconnected diagrams. With the strange-quark mass tuned to its physical\nvalue and several ensembles having close-to-physical light-quark mass,\nuncertainties related to the chiral extrapolations are reduced significantly\ncompared to earlier studies. Physical results are computed with fully\ncontrolled systematics from a combined chiral, continuum and infinite-volume\nextrapolation, and a full error budget is obtained from model averages over of\nvarious fit ans\\\"atze and data cuts. Our results for the masses are given by\n$M_\\eta=551(16)\\mathrm{MeV}$ and $M_{\\eta^\\prime}=972(20)\\mathrm{MeV}$,\nrespectively, where statistical and systematic errors have been added in\nquadrature. For the mixing angle and decay-constant parameters the\nFeldmann-Kroll-Stech scheme is employed to compute them from pseudoscalar\nmatrix elements in the quark-flavor basis. For the mixing angle we obtain\n$\\phi^\\mathrm{phys}=39.3(2.0)^\\circ$ and our results for the decay-constant\nparameters are given by $f_l^\\mathrm{phys}=138.6(4.4)\\mathrm{MeV}$ and\n$f_s^\\mathrm{phys}=170.7(3.3)\\mathrm{MeV}$.",
        "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "Despite its widespread use in materials science, conventional molecular\ndynamics simulations are severely constrained by timescale limitations. To\naddress this shortcoming, we propose an empirical formulation of accelerated\nmolecular dynamics method, adapted from a collective-variable-based extended\nsystem dynamics framework. While this framework is originally developed for\nefficient free energy sampling and reaction pathway determination of specific\nrare events in condensed matter, we have modified it to enable accelerated\nmolecular dynamics simulation and prediction of microstructure evolution of\nmaterials across a broad range of scenarios. In essence, the nearest neighbor\noff-centering absolute displacement (NNOAD), which quantifies the deviation of\nan atom from the geometric center of its nearest neighbors in materials, is\nintroduced. We propose that the collection of NNOADs of all atoms can serve as\na generalized reaction coordinate for various structural transitions in\nmaterials. The NNOAD of each atom, represented by its three components, is\ncoupled with three additional dynamic variables assigned to the atom. Time\nevolution of the additional dynamic variables follows Langevin equation, while\nNos\\'e-Hoover dynamics is employed to thermostat the system. Through careful\nanalysis and benchmark simulations, we established appropriate parameter ranges\nfor the equations in our method. Application of this method to several test\ncases demonstrates its effectiveness and consistency in accelerating molecular\ndynamics simulations and predicting various microstructure evolutions of\nmaterials over much longer timescale. We also provide a preliminary theoretical\nanalysis and qualitative justification of the method, offering insights into\nits underlying principles."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)",
    "start_abstract":"<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "2016 Alzheimer's disease facts and figures"
      ],
      "abstract":[
        "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Language modulates vision: Evidence from neural networks and human\n  brain-lesion models",
        "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Localization of Seizure Onset Zone based on Spatio-Temporal Independent\n  Component Analysis on fMRI",
        "A Relativistic Theory of Consciousness (shortened version)",
        "Electrophysiological Investigation of Insect Pain Threshold",
        "Mechanoreceptive A$\\beta$ primary afferents discriminate naturalistic\n  social touch inputs at a functionally relevant time scale",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "Understanding and controlling the geometry of memory organization in\n  RNNs",
        "A UDP Packet Format Establishing Adress Event Representation\n  Communication Between Remote Neuromorphic and Biological Setups",
        "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery",
        "Deviance Detection and Regularity Sensitivity in Dissociated Neuronal\n  Cultures",
        "Axion Stabilization in Modular Cosmology",
        "Reconstruction of space-dependence and nonlinearity of a reaction term\n  in a subdiffusion equation",
        "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas",
        "On the spectral gap of negatively curved covers",
        "Fluid Reconfigurable Intelligent Surfaces: Joint On-Off Selection and\n  Beamforming with Discrete Phase Shifts",
        "Forecasting Local Ionospheric Parameters Using Transformers",
        "Fourier dimension of the graph of fractional Brownian motion with $H \\ge\n  1\/2$",
        "A generalization of Zwegers' multivariable $\\mu$-function",
        "Enumeration of lattices of nullity $k$ and containing $r$ comparable\n  reducible elements",
        "Coherent manifolds",
        "Study of event and particle selection effects on elliptic flow\n  background at the isobar experiments based on AMPT model",
        "Progress of the TianQin project",
        "Dimension-free Score Matching and Time Bootstrapping for Diffusion\n  Models",
        "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence",
        "Modulating Optical Properties through Cation Substitution:\n  Composition-Property Relationships in $M^I_3$$M^{III}$P$_3$O$_9$N:Eu$^{2+}$\n  ($M^I$=Na, K; $M^{III}$=Al, Ga, In)"
      ],
      "abstract":[
        "Comparing information structures in between deep neural networks (DNNs) and\nthe human brain has become a key method for exploring their similarities and\ndifferences. Recent research has shown better alignment of vision-language DNN\nmodels, such as CLIP, with the activity of the human ventral occipitotemporal\ncortex (VOTC) than earlier vision models, supporting the idea that language\nmodulates human visual perception. However, interpreting the results from such\ncomparisons is inherently limited due to the \"black box\" nature of DNNs. To\naddress this, we combined model-brain fitness analyses with human brain lesion\ndata to examine how disrupting the communication pathway between the visual and\nlanguage systems causally affects the ability of vision-language DNNs to\nexplain the activity of the VOTC. Across four diverse datasets, CLIP\nconsistently outperformed both label-supervised (ResNet) and unsupervised\n(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,\naligning with the human language network. Analyses of the data of 33 stroke\npatients revealed that reduced white matter integrity between the VOTC and the\nlanguage region in the left angular gyrus was correlated with decreased CLIP\nperformance and increased MoCo performance, indicating a dynamic influence of\nlanguage processing on the activity of the VOTC. These findings support the\nintegration of language modulation in neurocognitive models of human vision,\nreinforcing concepts from vision-language DNN models. The sensitivity of\nmodel-brain similarity to specific brain lesions demonstrates that leveraging\nmanipulation of the human brain is a promising framework for evaluating and\ndeveloping brain-like computer models.",
        "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Localizing the seizure onset zone (SOZ) as a step of presurgical planning\nleads to higher efficiency in surgical and stimulation treatments. However, the\nclinical localization including structural, ictal, and invasive data\nacquisition and assessment is a difficult and long procedure with increasing\nchallenges in patients with complex epileptic foci. The interictal methods are\nproposed to assist in presurgical planning with simpler data acquisition and\nhigher speed. This study presents a spatiotemporal component classification for\nthe localization of epileptic foci using resting-state functional magnetic\nresonance imaging data. This method is based on spatiotemporal independent\ncomponent analysis on rsfMRI with a component-sorting procedure upon dominant\npower frequency, biophysical constraints, spatial lateralization, local\nconnectivity, temporal energy, and functional non-Gaussianity. This method\nutilized the rs-fMRI potential to reach a high spatial accuracy in localizing\nepileptic foci from interictal data while retaining the reliability of results\nfor clinical usage. Thirteen patients with temporal lobe epilepsy who underwent\nsurgical resection and had seizure-free surgical outcomes after a 12-month\nfollow-up were included in this study. All patients had presurgical structural\nMRI and rsfMRI while postsurgical MRI images were available for ten. Based on\nthe relationship between the localized foci and resection, the results were\nclassified into three groups fully concordant, partially concordant, and\ndiscordant. These groups had the resulting cluster aligned with, in the same\nlobe with, and outside the lobe of the resection area, respectively.",
        "This paper is a shortened version of the full paper that was published in the\njournal Frontiers of Psychology in May 2022. In recent decades, the scientific\nstudy of consciousness has significantly increased our understanding of this\nelusive phenomenon. Yet, despite critical development in our understanding of\nthe functional side of consciousness, we still lack a fundamental theory\nregarding its phenomenal aspect. The phenomenal aspect of consciousness is the\nfirst-person answer to what it is like question, and it has thus far proved\nrecalcitrant to direct scientific investigation. The question of how the brain,\nor any cognitive system, can create conscious experience out of neural\nrepresentations poses a great conundrum to science. Naturalistic dualists argue\nthat it is composed of a primitive, private, nonreductive element of reality.\nIllusionists, on the other hand, argue that it is merely a cognitive illusion.\nWe contend that both the dualist and illusionist positions are flawed because\nthey tacitly assume consciousness to be an absolute property that does not\ndepend on the observer. We developed a conceptual and a mathematical argument\nfor a relativistic theory of consciousness in which a system either has or does\nnot have phenomenal consciousness with respect to some observer. According to\nthe theory, Phenomenal consciousness is neither private nor delusional, just\nrelativistic. In the frame of reference of the cognitive system, it will be\nobservable (first-person perspective) and in other frame of reference it will\nnot (third-person perspective). These two cognitive frames of reference are\nboth correct, just as in the case of an observer that claims to be at rest\nwhile another will claim that the observer has constant velocity. Neither\nobserver position can be privileged, as they both describe the same underlying\nreality.",
        "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
        "Interpersonal touch is an important channel of social emotional interaction.\nHow these physical skin-to-skin touch expressions are processed in the\nperipheral nervous system is not well understood. From microneurography\nrecordings in humans, we evaluated the capacity of six subtypes of cutaneous\nmechanoreceptive afferents to differentiate human-delivered social touch\nexpressions. Leveraging statistical and classification analyses, we found that\nsingle units of multiple mechanoreceptive A$\\beta$ subtypes, especially slowly\nadapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can\nreliably differentiate social touch expressions at accuracies similar to human\nrecognition. We then identified the most informative firing patterns of SA-II\nand HFA afferents, which indicate that average durations of 3-4 s of firing\nprovide sufficient discriminative information. Those two subtypes also exhibit\nrobust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch\nexpressions due to their specific firing properties. Greater shifts in\nspike-timing, however, can change a firing pattern's envelope to resemble that\nof another expression and drastically compromise an afferent's discrimination\ncapacity. Altogether, the findings indicate that SA-II and HFA afferents\ndifferentiate the skin contact of social touch at time scales relevant for such\ninteractions, which are 1-2 orders of magnitude longer than those for\nnon-social touch.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "Training recurrent neural networks (RNNs) is a high-dimensional process that\nrequires updating numerous parameters. Therefore, it is often difficult to\npinpoint the underlying learning mechanisms. To address this challenge, we\npropose to gain mechanistic insights into the phenomenon of \\emph{abrupt\nlearning} by studying RNNs trained to perform diverse short-term memory tasks.\nIn these tasks, RNN training begins with an initial search phase. Following a\nlong period of plateau in accuracy, the values of the loss function suddenly\ndrop, indicating abrupt learning. Analyzing the neural computation performed by\nthese RNNs reveals geometric restructuring (GR) in their phase spaces prior to\nthe drop. To promote these GR events, we introduce a temporal consistency\nregularization that accelerates (bioplausible) training, facilitates attractor\nformation, and enables efficient learning in strongly connected networks. Our\nfindings offer testable predictions for neuroscientists and emphasize the need\nfor goal-agnostic secondary mechanisms to facilitate learning in biological and\nartificial networks.",
        "In the field of brain-machine interfaces, biohybrids offer an interesting new\nperspective, as in them, the technological side acts like a closed-loop\nextension or real counterpart of biological tissue, instead of the usual open\nloop approaches in tranditional BMI. To achieve a credible counterpart to\nbiological tissue, biohybrids usually employ one or several neuromorphic\ncomponents as the hardware half of the biohybrid. However, advanced\nneuromorphic circuit such as memristor crossbars usually operate best in a\ndedicated lab with corresponding support equipment. The same is true for\nbiological tissue, which makes co-locating all of the parts of a biohybrid in\nthe same lab challenging. Here, we present as solution to this co-location\nissue a simple method to connect biohybrids via the internet by a custom UDP\npacket format. We show that the characteristics achieved with our solution\n(jitter, delay, packet loss, packet reordering) on a standard internet\nconnection are compatible with various biohybrid processing paradigms, and we\npresent a short three-ways experiment as proof-of-concept. The described UDP\nformat has been employed to link biohybrids and neuromorphic circuits in four\ndifferent EC-funded projects.",
        "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell.",
        "Understanding how neural networks process complex patterns of information is\ncrucial for advancing both neuroscience and artificial intelligence. To\ninvestigate fundamental principles of neural computation, we studied\ndissociated neuronal cultures, one of the most primitive living neural\nnetworks, on high-resolution CMOS microelectrode arrays and tested whether the\ndissociated culture exhibits regularity sensitivity beyond mere\nstimulus-specific adaptation and deviance detection. In oddball electrical\nstimulation paradigms, we confirmed that the neuronal culture produced mismatch\nresponses (MMRs) with true deviance detection beyond mere adaptation. These\nMMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to\nmismatch negativity (MMN) in humans, which is known to have true deviance\ndetection properties. Crucially, we also showed sensitivity to the statistical\nregularity of stimuli, a phenomenon previously observed only in intact brains:\nthe MMRs in a predictable, periodic sequence were smaller than those in a\ncommonly used sequence in which the appearance of the deviant stimulus was\nrandom and unpredictable. These results challenge the traditional view that a\nhierarchically structured neural network is required to process complex\ntemporal patterns, suggesting instead that deviant detection and regularity\nsensitivity are inherent properties arising from the primitive neural network.\nThey also suggest new directions for the development of neuro-inspired\nartificial intelligence systems, emphasizing the importance of incorporating\nadaptive mechanisms and temporal dynamics in the design of neural networks.",
        "The $SL(2,\\mathbb{Z})$ invariant $\\alpha$-attractor models have plateau\npotentials with respect to the inflaton and axion fields. The potential in the\naxion direction is almost exactly flat during inflation, hence, the axion field\nremains nearly massless. In this paper, we develop a generalized class of such\nmodels, where the $SL(2,\\mathbb{Z})$ symmetry is preserved, but the axion\nacquires a large mass and becomes strongly stabilized during inflation, which\neliminates isocurvature perturbations in this scenario. Inflation in such\ntwo-field models occurs as in the single-field $\\alpha$-attractors and leads to\nthe same cosmological predictions.",
        "In this paper we study the simultaneous reconstruction of two coefficients in\na reaction-subdiffusion equation, namely a nonlinearity and a space dependent\nfactor. The fact that these are coupled in a multiplicative matter makes the\nreconstruction particularly challenging. Several situations of overposed data\nare considered: boundary observations over a time interval, interior\nobservations at final time, as well as a combination thereof. We devise fixed\npoint schemes and also describe application of a frozen Newton method. In the\nfinal time data case we prove convergence of the fixed point scheme as well as\nuniqueness of both coefficients. Numerical experiments illustrate performance\nof the reconstruction methods, in particular dependence on the differentiation\norder in the subdiffusion equation.",
        "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees.",
        "Given a negatively curved compact Riemannian surface $X$, we give an explicit\nestimate, valid with high probability as the degree goes to infinity, of the\nfirst non-trivial eigenvalue of the Laplacian on random Riemannian covers of\n$X$. The explicit gap is given in terms of the bottom of the spectrum of the\nuniversal cover of $X$ and the topological entropy of the geodesic flow on X.\nThis result generalizes in variable curvature a result of Magee-Naud-Puder for\nhyperbolic surfaces. We then formulate a conjecture on the optimal spectral gap\nand show that there exists covers with near optimal spectral gaps using a\nresult of Louder-Magee and techniques of strong convergence from random matrix\ntheory.",
        "This letter proposes a fluid reconfigurable intelligent surface (FRIS)\nparadigm, extending the conventional reconfigurable intelligent surface (RIS)\ntechnology to incorporate position reconfigurability of the elements. In our\nmodel, a `fluid' element is realized by a dense matrix of subelements over a\ngiven space and dynamically selecting specific elements for signal modulation\nbased on channel conditions. Specifically, we consider a FRIS-assisted\nsingle-user single-input single-output (SU-SISO) system and formulate an\noptimization problem that can jointly optimize element selection and their\ndiscrete phase shifts to maximize the achievable rate. To address this problem\nefficiently, we propose an iterative algorithm based on the cross-entropy\noptimization (CEO) framework. Simulation results reveal that FRIS achieves\nsignificant performance gains over traditional RIS.",
        "We present a novel method for forecasting key ionospheric parameters using\ntransformer-based neural networks. The model provides accurate forecasts and\nuncertainty quantification of the F2-layer peak plasma frequency (foF2), the\nF2-layer peak density height (hmF2), and total electron content (TEC) for a\ngiven geographic location. It supports a number of exogenous variables,\nincluding F10.7cm solar flux and disturbance storm time (Dst). We demonstrate\nhow transformers can be trained in a data assimilation-like fashion that use\nthese exogenous variables along with na\\\"ive predictions from climatology to\ngenerate 24-hour forecasts with non-parametric uncertainty bounds. We call this\nmethod the Local Ionospheric Forecast Transformer (LIFT). We demonstrate that\nthe trained model can generalize to new geographic locations and time periods\nnot seen during training, and we compare its performance to that of the\nInternational Reference Ionosphere (IRI).",
        "We prove that the Fourier dimension of the graph of fractional Brownian\nmotion with Hurst index greater than $1\/2$ is almost surely 1. This extends the\nresult of Fraser and Sahlsten (2018) for the Brownian motion and verifies\npartly the conjecture of Fraser, Orponen and Sahlsten (2014). We introduce a\ncombinatorial integration by parts formula to compute the moments of the\nFourier transform of the graph measure. The proof of our main result is based\non this integration by parts formula together with Fa\\`a di Bruno's formula and\nstrong local nondeterminism of fractional Brownian motion. We also show that\nthe Fourier dimension of the graph of a symmetric $\\alpha$-stable process with\n$\\alpha\\in[1,2]$ is almost surely 1.",
        "We introduce a one parameter deformation of Zwegers' multivariable\n$\\mu$-function by applying iterations of the $q$-Borel summation method, which\nis also a multivariate analogue of the generalized $\\mu$-function introduced by\nthe authors. For this deformed multivariable $\\mu$-function, we give some\nformulas, for example, forward shift formula, translation and\n$\\mathfrak{S}_{N+1}$-symmetry. Further we mention modular formulas for the\nZwegers' original multivariable $\\mu$-function.",
        "In 2002 Thakare et al.\\ counted non-isomorphic lattices on $n$ elements,\nhaving nullity up to two. In 2020 Bhavale and Waphare introduced the concept of\nRC-lattices as the class of all lattices in which all the reducible elements\nare comparable. In this paper, we enumerate all non-isomorphic RC-lattices on\n$n$ elements. For this purpose, firstly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$, and containing\n$2 \\leq r \\leq 2k$ reducible elements. Secondly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$. This work is in\nrespect of Birkhoff's open problem of enumerating all finite lattices on $n$\nelements.",
        "This paper defines coherent manifolds and discusses their properties and\ntheir application in quantum mechanics. Every coherent manifold with a large\ngroup of symmetries gives rise to a Hilbert space, the completed quantum space\nof $Z$, which contains a distinguished family of coherent states labeled by the\npoints of the manifold.\n  The second quantization map in quantum field theory is generalized to\nquantization operators on arbitrary coherent manifolds. It is shown how the\nSchr\\\"odinger equation on any such completed quantum space can be solved in\nterms of computations only involving the coherent product. In particular, this\napplies to a description of Bosonic Fock spaces as completed quantum spaces of\na class of coherent manifolds called Klauder spaces.",
        "Measurement of the Chiral Magnetic Effect (CME) has been a popular topic of\nhigh-energy nuclear physics in the last decade. The flow correlation $\\gamma$\nbetween charged hadron pairs of the same and opposite charges and their\ndifference $\\Delta \\gamma$ were measured to separate the CME-driven signal from\nthe collective flow background especially second-order elliptic $v_{2}$. The\nSTAR experiment have stepped further to the isobar experiment to compare\n$\\gamma$ and $\\Delta \\gamma$ between Ru+Ru and Zr+Zr\n~\\cite{PhysRevC.105.014901}, which were theoretically expected to produce the\nsame elliptic flow background but different CME signals. However, the measured\nflow backgrounds also differ between Ru+Ru and Zr+Zr, indicating more\nfine-tuning of RP and centrality definition necessary.\n  This analysis applied the AMPT model~\\cite{PhysRevC.72.064901} to simulate\nthe same collision system and energy as the STAR isobar experiment. Since the\nAMPT model does not include magnetic field effects, we expect comparing its\noutput between Ru+Ru and Zr+Zr collision systems can provide an insight of the\npossible bias of flow background definition, and help improve the measurement\nof CME signal in real experiments. Multiple combinations of centrality and flow\ndefinition were chosen to study how the $v_2$ and their difference would be\naffected, especially by varying the particles selection of charge versus\nneutral properties and broadening (pseudo-)rapidity regions, while STAR CME\nwork relied on charged-only particles at central rapidity.",
        "TianQin is a future space-based gravitational wave observatory targeting the\nfrequency window of $10^{-4}$ Hz $\\sim 1$ Hz. A large variety of gravitational\nwave sources are expected in this frequency band, including the merger of\nmassive black hole binaries, the inspiral of extreme\/intermediate mass ratio\nsystems, stellar-mass black hole binaries, Galactic compact binaries, and so\non. TianQin will consist of three Earth orbiting satellites on nearly identical\norbits with orbital radii of about $10^5$ km. The satellites will form a normal\ntriangle constellation whose plane is nearly perpendicular to the ecliptic\nplane. The TianQin project has been progressing smoothly following the ``0123\"\ntechnology roadmap. In step ``0\", the TianQin laser ranging station has been\nconstructed and it has successfully ranged to all the five retro-reflectors on\nthe Moon. In step ``1\", the drag-free control technology has been tested and\ndemonstrated using the TianQin-1 satellite. In step ``2\", the inter-satellite\nlaser interferometry technology will be tested using the pair of TianQin-2\nsatellites. The TianQin-2 mission has been officially approved and the\nsatellites will be launched around 2026. In step ``3\", i.e., the TianQin-3\nmission, three identical satellites will be launched around 2035 to form the\nspace-based gravitational wave detector, TianQin, and to start gravitational\nwave detection in space.",
        "Diffusion models generate samples by estimating the score function of the\ntarget distribution at various noise levels. The model is trained using samples\ndrawn from the target distribution, progressively adding noise. In this work,\nwe establish the first (nearly) dimension-free sample complexity bounds for\nlearning these score functions, achieving a double exponential improvement in\ndimension over prior results. A key aspect of our analysis is the use of a\nsingle function approximator to jointly estimate scores across noise levels, a\ncritical feature of diffusion models in practice which enables generalization\nacross timesteps. Our analysis introduces a novel martingale-based error\ndecomposition and sharp variance bounds, enabling efficient learning from\ndependent data generated by Markov processes, which may be of independent\ninterest. Building on these insights, we propose Bootstrapped Score Matching\n(BSM), a variance reduction technique that utilizes previously learned scores\nto improve accuracy at higher noise levels. These results provide crucial\ninsights into the efficiency and effectiveness of diffusion models for\ngenerative modeling.",
        "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}\/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https:\/\/github.com\/fshp971\/adv-icl.",
        "Developing phosphors with narrow photoluminescence emission peaks and high\nchromatic stability holds significant importance in light-emitting diode (LED)\ndisplay technologies, where a wide color gamut is essential to achieve the Rec.\n2020 specifications. This research focuses on the optical properties of a solid\nsolution: $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N [$M^I$=Na, K;\n$M^{III}$=Al, (Al$_{0.75}$Ga$_{0.25}$), (Al$_{0.5}$Ga$_{0.5}$),\n(Al$_{0.25}$Ga$_{0.75}$), Ga, (Ga$_{0.75}$In$_{0.25}$), (Ga$_{0.5}$In$_{0.5}$)]\nto understand how the narrow-emitting photoluminescence in\nK$_3$AlP$_3$O$_9$N:Eu$^{2+}$ can evolve during host structure cation\nsubstitution. Photoluminescence measurements at low temperature (15 K) support\nthat Eu$^{2+}$ replaces three crystallographically independent Na$^+$ sites in\nNa$_{2.97}$Eu$_{0.015}$AlP$_3$O$_9$N, similar to the parent K$^+$ phosphor, but\nsubstituting Ga$^{3+}$ and In$^{3+}$ for Al$^{3+}$ leads to a change in\nEu$^{2+}$ site preference, narrowing the full-width-at-half-maximum (fwhm) of\nthe emission peak. The chromatic stability and photoluminescence quantum yield\nare also enhanced with higher Ga$^{3+}$ content in the host but not with\nIn$^{3+}$. Thermoluminescence analysis indicates the relationship between trap\nstates and the enhanced quantum yield with Ga$^{3+}$ leads to the series' best\nperformance. The analysis of the $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N\nseries offers insight into the potential method for modulating optical\nproperties with cation substitution in the host structure."
      ]
    }
  },
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
    "start_abstract":"Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "A primer on deep learning in genomics"
      ],
      "abstract":[
        "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "CoverM: Read alignment statistics for metagenomics",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Causes of evolutionary divergence in prostate cancer",
        "Spatial locking of chimera states to frequency heterogeneity in\n  nonlocally coupled oscillators",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Wafer-scale waveguide sidewall roughness scattering loss\n  characterization by image processing",
        "The Aesthetic Imperative of Lev Landau's Geometric Reductionism in\n  Theoretical Physics",
        "BrainOOD: Out-of-distribution Generalizable Brain Network Analysis",
        "Fenchel's conjecture on NEC groups",
        "Using the STIX background detector as a proxy for GOES",
        "Exercises in Iterational Asymptotics II",
        "Tools for Unbinned Unfolding",
        "On stabilization at a soliton for generalized Korteweg--De Vries pure\n  power equation for any power $p\\in (1,5)$",
        "An improved evaluation of the electroweak contribution to $(g-2)_\\mu$",
        "The puzzle of isolated and quenched dwarf galaxies in cosmic voids",
        "Site-engineered ferromagnetism in Ca and Cr co-substituted Bismuth\n  Ferrite Nanoparticles",
        "A modified two-stage search framework for constrained multi-gradient\n  descent",
        "Ultrafast Charge Separation on the Nanoscale Induced by a Uniform Field"
      ],
      "abstract":[
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Chimera states in systems of nonlocally coupled oscillators, i.e.,\nself-organized coexistence of coherent and incoherent oscillator populations,\nhave attracted much attention. In this study, we consider the effect of\nfrequency heterogeneities on the chimera state and reveal that it induces\nspatial locking of the chimera state, i.e., the coherent and incoherent domains\nalign with lower and higher frequency regions, respectively, in a self-adaptive\nmanner. Using an extended self-consistency approach, we show that such\nspatially locked chimera states can be reproduced as steady solutions of the\nsystem in the continuum limit. Furthermore, we develop a variational argument\nto explain the mechanism leading to spatial locking. Our analysis reveals how\nheterogeneity can affect the collective dynamics of the chimera states and\noffers insights into their control and applications.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "Photonic integrated circuits (PICs) are vital for developing affordable,\nhigh-performance optoelectronic devices that can be manufactured at an\nindustrial scale, driving innovation and efficiency in various applications.\nOptical loss of modes in thin film waveguides and devices is a critical measure\nof their performance. Thin films growth, lithography, masking, and etching\nprocesses are imperfect processes that introduce significant sidewall and\ntop-surface roughness and cause dominating optical losses in waveguides and\nphotonic structures. These roughness as perturbations couple light from guided\nto far-field radiation modes, leading to scattering losses that can be\nestimated from theoretical models. Typically, with UV-based lithography\nsidewall roughness is found to be significantly larger than wafer-top surface\nroughness. Atomic force microscopy (AFM) imaging measurement gives 3D and\nhigh-resolution roughness profile but the measurement is inconvenient, costly,\nand unscalable for large-scale PICs and at wafer-scale. Here, we evaluate the\nsidewall roughness profile based on 2D high-resolution scanning electron\nmicroscope imaging. We characterized the loss on two homemade nitride and oxide\nfilms on 3-inch silicon wafers with 12 waveguide devices on each and co-related\nthe scattering loss estimated from a 2D image-based sidewall profile and\ntheoretical Payne model. The lowest loss of guided fundamental transverse\nelectric (TE$_{0}$) is found at 0.075 dB\/cm at 633 nm across 24 devices, which\nis a record at visible wavelength. Our work shows a 100% success in edge\ndetection in image processing to estimate autocorrelation function and optical\nmode loss. These demonstrations offer valuable insights into waveguide sidewall\nroughness and comparison of experimental and 2D SEM image-processing-based loss\nestimations.",
        "This paper explores the ontological and epistemological foundations of Lev\nLandau's theoretical physics through the lens of his unpublished philosophical\nnotes and scientific practice. We identify a unique form of geometric\nreductionism where physical laws emerge as inevitable consequences of symmetry\nbreaking in progressively constrained phase spaces. Landau's dismissal of\nquantum interpretation debates and his famous \"axiomatic minimalism\" in the\nCourse of Theoretical Physics are shown to stem from a deep epistemological\ncommitment to dimensional aesthetics - the belief that fundamental truths must\nmanifest through dimensional economy in mathematical representations.",
        "In neuroscience, identifying distinct patterns linked to neurological\ndisorders, such as Alzheimer's and Autism, is critical for early diagnosis and\neffective intervention. Graph Neural Networks (GNNs) have shown promising in\nanalyzing brain networks, but there are two major challenges in using GNNs: (1)\ndistribution shifts in multi-site brain network data, leading to poor\nOut-of-Distribution (OOD) generalization, and (2) limited interpretability in\nidentifying key brain regions critical to neurological disorders. Existing\ngraph OOD methods, while effective in other domains, struggle with the unique\ncharacteristics of brain networks. To bridge these gaps, we introduce BrainOOD,\na novel framework tailored for brain networks that enhances GNNs' OOD\ngeneralization and interpretability. BrainOOD framework consists of a feature\nselector and a structure extractor, which incorporates various auxiliary losses\nincluding an improved Graph Information Bottleneck (GIB) objective to recover\ncausal subgraphs. By aligning structure selection across brain networks and\nfiltering noisy features, BrainOOD offers reliable interpretations of critical\nbrain regions. Our approach outperforms 16 existing methods and improves\ngeneralization to OOD subjects by up to 8.5%. Case studies highlight the\nscientific validity of the patterns extracted, which aligns with the findings\nin known neuroscience literature. We also propose the first OOD brain network\nbenchmark, which provides a foundation for future research in this field. Our\ncode is available at https:\/\/github.com\/AngusMonroe\/BrainOOD.",
        "A classical discovery known as Fenchel's conjecture and proved in the 1950s,\nshows that every co-compact Fuchsian group $F$ has a normal subgroup of finite\nindex isomorphic to the fundamental group of a compact unbordered orientable\nsurface, or in algebraic terms, that $F$ has a normal subgroup of finite index\nthat contains no element of finite order other than the identity. In this paper\nwe initiate and make progress on an extension of Fenchel's conjecture by\nconsidering the following question: Does every planar non-Euclidean\ncrystallographic group $\\Gamma$ containing transformations that reverse\norientation have a normal subgroup of finite index isomorphic to the\nfundamental group of a compact unbordered non-orientable surface? We answer\nthis question in the affirmative in the case where the orbit space of $\\Gamma$\nis a nonorientable surface, and also in the case where this orbit space is a\nbordered orientable surface of positive genus. In the case where the genus of\nthe quotient is $0$, we have an affirmative answer in many subcases, but the\nquestion is still open for others.",
        "Context. The Spectrometer\/Telescope for Imaging X-Rays (STIX) onboard Solar\nOrbiter was designed to observe solar flares in the X-ray range of 4-150 keV,\nproviding spectral, temporal and spatial information. Besides 30 imaging\ndetectors, STIX has two additional detectors, the coarse flare locator (CFL)\nand the background (BKG) detector. Flares observed from Earth are classified\nusing their peak X-ray flux observed by the GOES satellites. Roughly half of\nall flares observed by STIX are located on the backside of the Sun. These\nflares lack a GOES-class classification.\n  Aims. In this paper, we describe the calibration of the BKG detector aperture\nsizes. Using the calibrated measurements of the BKG detector, we explore the\nrelationship between the peak flux for flares jointly observed by STIX and\nGOES. This allows us to estimate the GOES flare classes of backside flares\nusing STIX measurements.\n  Methods. We looked at the 500 largest flares observed by both STIX and GOES\nin the time range Feb. 21 to Apr. 23. Aperture size calibration is done by\ncomparing 4-10 keV counts of the BKG detector with the CFL measurements. In a\nsecond step, we correlate the calibrated STIX BKG peak flux with the GOES peak\nflux for individual flares.\n  Results. We calibrated the BKG detector aperture sizes of STIX. Further, we\nshowed that for the larger flares a close power law fit exists between the STIX\nBKG and GOES peak flux with a Pearson correlation coefficient of 0.97. This\ncorrelation provides a GOES proxy with a one sigma uncertainty of 11%. We were\nable to show that the BKG detector can reliably measure a broad range of GOES\nflare classes from roughly B5 up to at least X85 (assuming a radial distance of\n1AU), making it an interesting detector-concept for future space weather\nmissions. The largest flare observed by STIX to date is an estimated X16.5\n$\\pm$ 1.8 backside flare on the 20 Mai 2024.",
        "The nonlinear recurrences we consider here include the functions $3x(1-x)$\nand $\\cos(x)$, which possess attractive fixed points $2\/3$ and $0.739...$\n(Dottie's number). Detailed asymptotics for oscillatory convergence are found,\nstarting with a 1960 paper by Wolfgang Thron. Another function,\n$x\/(1+x\\ln(1+x))$, gives rise to a sequence with monotonic convergence to $0$\nbut requires substantial work to calculate its associated constant $C$.",
        "Machine learning has enabled differential cross section measurements that are\nnot discretized. Going beyond the traditional histogram-based paradigm, these\nunbinned unfolding methods are rapidly being integrated into experimental\nworkflows. In order to enable widespread adaptation and standardization, we\ndevelop methods, benchmarks, and software for unbinned unfolding. For\nmethodology, we demonstrate the utility of boosted decision trees for unfolding\nwith a relatively small number of high-level features. This complements\nstate-of-the-art deep learning models capable of unfolding the full phase\nspace. To benchmark unbinned unfolding methods, we develop an extension of\nexisting dataset to include acceptance effects, a necessary challenge for real\nmeasurements. Additionally, we directly compare binned and unbinned methods\nusing discretized inputs for the latter in order to control for the binning\nitself. Lastly, we have assembled two software packages for the OmniFold\nunbinned unfolding method that should serve as the starting point for any\nfuture analyses using this technique. One package is based on the widely-used\nRooUnfold framework and the other is a standalone package available through the\nPython Package Index (PyPI).",
        "We apply our idea, which previously we used in the analysis of the pure power\nNLS, consisting in spitting the virial inequality method into a large energy\ninequality combined with Kato smoothing, to the case of generalized\nKorteweg--De Vries pure power equations. We assume that a solution remains for\nall positive times very close to a soliton and then we prove an asymptotic\nstability result for $t\\to +\\infty$.",
        "A precise evaluation of the electroweak contribution to the anomalous\nmagnetic moment of the muon requires control over all aspects of the Standard\nModel, ranging from Higgs physics, over multi-loop computations for bosonic and\n(heavy-)fermion diagrams, to non-perturbative effects in the presence of light\nquarks. Currently, the dominant uncertainties arise from such hadronic effects\nin the vector-vector-axial-vector three-point function, an improved\nunderstanding of which has recently emerged in the context of hadronic\nlight-by-light scattering. Profiting from these developments as well as new\nperturbative and non-perturbative input for the charm contribution, we obtain\n$a_\\mu^\\text{EW}=154.4(4)\\times 10^{-11}$.",
        "We report, for the first time, the detection of a sample of quenched and\nisolated dwarf galaxies (with 8.9 $<$ log(M$_{\\rm \\star}$\/M$_{\\rm \\odot}$) $<$\n9.5) in the least dense regions of the cosmic web, including voids, filaments,\nand walls. These dwarfs have no neighbouring galaxy within 1.0~Mpc in projected\ndistance. Based on the full spectral fitting of their central spectra using\nSloan Digital Sky Survey data, these galaxies are gas-deprived, exhibit stellar\nmass assembly very similar to dwarfs in the central regions of galaxy clusters,\nand have experienced no significant star formation in the past 2 Gyr.\nAdditionally, analysis of r-band images from the Dark Energy Camera Legacy\nSurvey showed that these dwarf galaxies host a central Nuclear Star Cluster\n(NSC). Detecting quenched, isolated dwarf galaxies in cosmic voids indicates\nthat environmental factors are not the sole drivers of their quenching.\nInternal mechanisms, such as feedback from in-situ star formation, also\ncontributing to the NSC formation, black holes, or variations in conditions\nduring their formation, offer potential explanations for star formation\nsuppression in these galaxies. These findings highlight the need for a\nsignificant revision in our understanding of baryonic physics, particularly\nconcerning the formation and evolution of low-mass galaxies.",
        "Multiferroic perovskites that exhibit room temperature magnetization and\npolarization have immense potential in the next generation of magneto-electric\nand spintronic memory devices. In this work, the magnetic and ferroelectric\nproperties of Bismuth Ferrite, BiFeO3 (BFO) nanoparticles (NPs) were enhanced\nthrough simultaneous A and B site Ca and Cr co-substitution. Novel compositions\nof Bi0.97Ca0.03CrxFe1-xO3 (x=0, 0.01, 0.03, 0.05) were synthesized using the\nsol-gel route and annealed at 550 degrees Celcius. Rietveld Refinement of XRD\npatterns confirmed high phase purity, while SEM analysis revealed a decreasing\ntrend in average particle size with increasing dopant concentration. Hysteresis\nloops showed enhanced magnetic properties as particle size approached the spin\ncycloid wavelength (around 62 nm), disrupting the intrinsic antiferromagnetic\nordering of BFO. Moreover, the presence of exchange bias in the NPs was linked\nto the formation of core-shell structure. Temperature dependent magnetization\nstudies showed an increase in N\\'eel temperature upon Ca substitution. XPS\nanalysis confirmed that Bi0.97Ca0.03FeO3 samples exhibited the highest oxygen\nvacancy concentration, while Fe3+ remained the dominant oxidation state across\nall compositions. Ferroelectric polarization loop measurements showed enhanced\nremanent polarization in doped samples, with leakage linked to oxygen vacancies\nand extrinsic microstructural effects.",
        "\\cite{desideri2012multiple} proposed a multi-gradient descent algorithm\n(MGDA) that can improve all objectives based on seeking the minim norm point in\nthe convex hull consisting of objectives function gradients as the common\ndescent direction, which has become the cornerstone of the multi-musk learning\nand multi-objective optimization. However, the logic to seek a common descent\ndirection through finding the minim-norm point in the gradient convex hull may\nfail under constraints, no matter whether taking the constraints into\nconsideration directly or projecting it into the feasible region after finding\nthe common descent direction with no constraints. Therefore, we proposed a\ntwo-stage search framework. In the first stage, a min-max search algorithm is\nimplemented to minimize the upper bound of the directional derivatives under\nconstraints, and the weak Pareto stationary can be theoretically reached in the\nfirst stage. In the second stage, the Pareto stationary can be theoretically\nobtained through the minimization of the lower bound of the directional\nderivatives under constraints. In numerical studies, we show the effectiveness\nof our proposed framework from the calibration of the multi-regime fundamental\ndiagram model and large-scale multi-objective portfolio problem.",
        "When illuminated by white light, atoms, molecules, and materials absorb only\ncertain characteristic energy contributions based on their absorption\nproperties. Here, we show that this effect can be translated from energy to\nspace: a spatially uniform laser pulse can create strongly localized carrier\nexcitations, including excitons, and spatial charge separation on the\nsub-nanometer scale within a few femtoseconds, opening new avenues for\nnanoelectronics and bringing petahertz switching within reach. Using\nnonequilibrium Green functions simulations we demonstrate this effect by\nexciting targeted areas of small graphene nanoribbon heterostructures by\ncareful choice of the laser energy and polarization."
      ]
    }
  },
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"A primer on deep learning in genomics",
    "start_abstract":"Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
      ],
      "abstract":[
        "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
        "Off-Switching Not Guaranteed",
        "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training",
        "Perspectives for Direct Interpretability in Multi-Agent Deep\n  Reinforcement Learning",
        "PairVDN - Pair-wise Decomposed Value Functions",
        "Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging",
        "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
        "A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
        "Fighter Jet Navigation and Combat using Deep Reinforcement Learning with\n  Explainable AI",
        "Knowledge is Power: Harnessing Large Language Models for Enhanced\n  Cognitive Diagnosis",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "AI-Driven Decision Support in Oncology: Evaluating Data Readiness for\n  Skin Cancer Treatment",
        "Lessons From Red Teaming 100 Generative AI Products",
        "Nonexistence of traveling wave solutions in the fractional Rosenau-Hyman\n  equation via homotopy perturbation method",
        "FORTE: An Open-Source System for Cost-Effective and Scalable\n  Environmental Monitoring",
        "UniCoRN: Unified Commented Retrieval Network with LMMs",
        "Combinatorial construction of symplectic 6-manifolds via bifibration\n  structures",
        "MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing",
        "Make Optimization Once and for All with Fine-grained Guidance",
        "Circular-polarization-selective perfect reflection from chiral\n  superconductors",
        "Forecasting Frontier Language Model Agent Capabilities",
        "Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation\n  Research",
        "Experimental demonstration of entanglement pumping with bosonic logical\n  qubits",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "PoisonedParrot: Subtle Data Poisoning Attacks to Elicit\n  Copyright-Infringing Content from Large Language Models",
        "Primordial Black Hole Formation via Inverted Bubble Collapse",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "Reinforcement Learning for Quantum Control under Physical Constraints"
      ],
      "abstract":[
        "Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.",
        "Hadfield-Menell et al. (2017) propose the Off-Switch Game, a model of\nHuman-AI cooperation in which AI agents always defer to humans because they are\nuncertain about our preferences. I explain two reasons why AI agents might not\ndefer. First, AI agents might not value learning. Second, even if AI agents\nvalue learning, they might not be certain to learn our actual preferences.",
        "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
        "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in\nsolving complex problems in robotics or games, yet most of the trained models\nare hard to interpret. While learning intrinsically interpretable models\nremains a prominent approach, its scalability and flexibility are limited in\nhandling complex tasks or multi-agent dynamics. This paper advocates for direct\ninterpretability, generating post hoc explanations directly from trained\nmodels, as a versatile and scalable alternative, offering insights into agents'\nbehaviour, emergent phenomena, and biases without altering models'\narchitectures. We explore modern methods, including relevance backpropagation,\nknowledge edition, model steering, activation patching, sparse autoencoders and\ncircuit discovery, to highlight their applicability to single-agent,\nmulti-agent, and training process challenges. By addressing MADRL\ninterpretability, we propose directions aiming to advance active topics such as\nteam identification, swarm coordination and sample efficiency.",
        "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https:\/\/github.com\/zzbuzzard\/PairVDN.",
        "Personalized messaging plays an essential role in improving communication in\nareas such as healthcare, education, and professional engagement. This paper\nintroduces a framework that uses the Knowledge Graph (KG) to dynamically\nrephrase written communications by integrating individual and context-specific\ndata. The knowledge graph represents individuals, locations, and events as\ncritical nodes, linking entities mentioned in messages to their corresponding\ngraph nodes. The extraction of relevant information, such as preferences,\nprofessional roles, and cultural norms, is then combined with the original\nmessage and processed through a large language model (LLM) to generate\npersonalized responses. The framework demonstrates notable message acceptance\nrates in various domains: 42% in healthcare, 53% in education, and 78% in\nprofessional recruitment. By integrating entity linking, event detection, and\nlanguage modeling, this approach offers a structured and scalable solution for\ncontext-aware, audience-specific communication, facilitating advanced\napplications in diverse fields.",
        "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps:\/\/github.com\/ZitongShi\/EPEAgent",
        "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes.",
        "This paper presents the development of an Artificial Intelligence (AI) based\nfighter jet agent within a customized Pygame simulation environment, designed\nto solve multi-objective tasks via deep reinforcement learning (DRL). The jet's\nprimary objectives include efficiently navigating the environment, reaching a\ntarget, and selectively engaging or evading an enemy. A reward function\nbalances these goals while optimized hyperparameters enhance learning\nefficiency. Results show more than 80\\% task completion rate, demonstrating\neffective decision-making. To enhance transparency, the jet's action choices\nare analyzed by comparing the rewards of the actual chosen action (factual\naction) with those of alternate actions (counterfactual actions), providing\ninsights into the decision-making rationale. This study illustrates DRL's\npotential for multi-objective problem-solving with explainable AI. Project page\nis available at:\n\\href{https:\/\/github.com\/swatikar95\/Autonomous-Fighter-Jet-Navigation-and-Combat}{Project\nGitHub Link}.",
        "Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive\nstates by analyzing their performance across a series of exercises. However,\nexisting CDMs often struggle with diagnosing infrequent students and exercises\ndue to a lack of rich prior knowledge. With the advancement in large language\nmodels (LLMs), which possess extensive domain knowledge, their integration into\ncognitive diagnosis presents a promising opportunity. Despite this potential,\nintegrating LLMs with CDMs poses significant challenges. LLMs are not\nwell-suited for capturing the fine-grained collaborative interactions between\nstudents and exercises, and the disparity between the semantic space of LLMs\nand the behavioral space of CDMs hinders effective integration. To address\nthese issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD)\nframework, which is a model-agnostic framework utilizing LLMs to enhance CDMs\nand compatible with various CDM architectures. The KCD framework operates in\ntwo stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis\nstage, both students and exercises are diagnosed to achieve comprehensive and\ndetailed modeling. In the Cognitive Level Alignment stage, we bridge the gap\nbetween the CDMs' behavioral space and the LLMs' semantic space using\ncontrastive learning and mask-reconstruction approaches. Experiments on several\nreal-world datasets demonstrate the effectiveness of our proposed framework.",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "This research focuses on evaluating and enhancing data readiness for the\ndevelopment of an Artificial Intelligence (AI)-based Clinical Decision Support\nSystem (CDSS) in the context of skin cancer treatment. The study, conducted at\nthe Skin Tumor Center of the University Hospital M\\\"unster, delves into the\nessential role of data quality, availability, and extractability in\nimplementing effective AI applications in oncology. By employing a multifaceted\nmethodology, including literature review, data readiness assessment, and expert\nworkshops, the study addresses the challenges of integrating AI into clinical\ndecision-making. The research identifies crucial data points for skin cancer\ntreatment decisions, evaluates their presence and quality in various\ninformation systems, and highlights the difficulties in extracting information\nfrom unstructured data. The findings underline the significance of\nhigh-quality, accessible data for the success of AI-driven CDSS in medical\nsettings, particularly in the complex field of oncology.",
        "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider.",
        "We apply the homotopy perturbation method to construct series solutions for\nthe fractional Rosenau-Hyman (fRH) equation and study their dynamics. Unlike\nthe classical RH equation where compactons arise from truncated periodic\nsolutions, we show that spatial nonlocality prevents the existence of\ncompactons, and therefore periodic traveling waves are considered. By\nasymptotic analyses involving the Mittag-Leffler function, it is shown that the\nquadratic fRH equation exhibits bifurcation with respect to the order of the\ntemporal fractional derivative, leading to the eventual pinning of wave\npropagation. Additionally, numerical results suggest potential finite time\nblow-up in the cubic fRH. While HPM proves effective in constructing analytic\nsolutions, we identify cases of divergence, underscoring the need for further\nresearch into its convergence properties and broader applicability.",
        "Forests are an essential part of our biosphere, regulating climate, acting as\na sink for greenhouse gases, and providing numerous other ecosystem services.\nHowever, they are negatively impacted by climatic stressors such as drought or\nheat waves. In this paper, we introduce FORTE, an open-source system for\nenvironmental monitoring with the aim of understanding how forests react to\nsuch stressors. It consists of two key components: (1) a wireless sensor\nnetwork (WSN) deployed in the forest for data collection, and (2) a Data\nInfrastructure for data processing, storage, and visualization. The WSN\ncontains a Central Unit capable of transmitting data to the Data Infrastructure\nvia LTE-M and several spatially independent Satellites that collect data over\nlarge areas and transmit them wirelessly to the Central Unit. Our prototype\ndeployments show that our solution is cost-effective compared to commercial\nsolutions, energy-efficient with sensor nodes lasting for several months on a\nsingle charge, and reliable in terms of data quality. FORTE's flexible\narchitecture makes it suitable for a wide range of environmental monitoring\napplications beyond forest monitoring. The contributions of this paper are\nthree-fold. First, we describe the high-level requirements necessary for\ndeveloping an environmental monitoring system. Second, we present an\narchitecture and prototype implementation of the requirements by introducing\nour FORTE platform and demonstrating its effectiveness through multiple field\ntests. Lastly, we provide source code, documentation, and hardware design\nartifacts as part of our open-source repository.",
        "Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR \/ +18.4% BEM over RAG for commenting in CoR.",
        "A bifibration structure on a $6$-manifold is a map to either the complex\nprojective plane $\\mathbb{P}^2$ or a $\\mathbb{P}^1$-bundle over $\\mathbb{P}^1$,\nsuch that its composition with the projection to $\\mathbb{P}^1$ is a\n($6$-dimensional) Lefschetz fibration\/pencil, and its restriction to the\npreimage of a generic $\\mathbb{P}^1$-fiber is also a ($4$-dimensional)\nLefschetz fibration\/pencil. This object has been studied by Auroux, Katzarkov,\nSeidel, among others. From a pair consisting of a monodromy representation of a\nLefschetz fibration\/pencil on a $4$-manifold and a relation in a braid group,\nwhich are mutually compatible in an appropriate sense, we construct a\nbifibration structure on a closed symplectic $6$-manifold, producing the given\ncompatible pair as its monodromies. We further establish methods for computing\ntopological invariants of symplectic $6$-manifolds, including Chern numbers,\nfrom compatible pairs. Additionally, we provide an explicit example of a\ncompatible pair, conjectured to correspond to a bifibration structure derived\nfrom the degree-$2$ Veronese embedding of the $3$-dimensional complex\nprojective space. This example can be viewed as a higher-dimensional analogue\nof the lantern relation in the mapping class group of the four-punctured\nsphere. Our results not only extend the applicability of combinatorial\ntechniques to higher-dimensional symplectic geometry but also offer a unified\nframework for systematically exploring symplectic $6$-manifolds.",
        "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https:\/\/github.com\/Eureka-Maggie\/MIGE.",
        "Learning to Optimize (L2O) enhances optimization efficiency with integrated\nneural networks. L2O paradigms achieve great outcomes, e.g., refitting\noptimizer, generating unseen solutions iteratively or directly. However,\nconventional L2O methods require intricate design and rely on specific\noptimization processes, limiting scalability and generalization. Our analyses\nexplore general framework for learning optimization, called Diff-L2O, focusing\non augmenting sampled solutions from a wider view rather than local updates in\nreal optimization process only. Meanwhile, we give the related generalization\nbound, showing that the sample diversity of Diff-L2O brings better performance.\nThis bound can be simply applied to other fields, discussing diversity,\nmean-variance, and different tasks. Diff-L2O's strong compatibility is\nempirically verified with only minute-level training, comparing with other\nhour-levels.",
        "Integrating mirrors with magnetic components is crucial for constructing\nchiral optical cavities, which provide tunable platforms for\ntime-reversal-asymmetric light-matter interactions. Here, we introduce\nsingle-crystal circular-polarization-selective mirrors based on chiral\nsuperconductors, which break time-reversal symmetry by themselves eliminating\nthe need for additional components. We show that a\ncircular-polarization-selective perfect reflection (CSPR) occurs for\nstrong-coupling superconductors in the BCS-BEC crossover regime or beyond if\nthe optical Hall conductivity is significant in the unit of conductivity\nquantum per unit layer, $e^2\/ha_z$, where $a_z$ is the lattice constant along\nthe surface normal. While the optical Hall conductivity in chiral\nsuperconductors is typically tiny, we classify three routes to obtain a large\nvalue. We demonstrate the significant optical Hall conductivity and the\nresulting CSPR with two examples: (1) superconductivity in doped quantum Hall\ninsulators and (2) chiral pairing that preserves the Bogoliubov Fermi surfaces\nin the weak-pairing limit. We also discuss the application of our theory to the\nrecently discovered chiral superconducting phase in rhombohedral graphene. Our\ntheory reveals the potential of these classes of chiral superconductors as\npromising elements for building high-quality-factor terahertz chiral cavities.",
        "As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.",
        "Effective strategies for sensor data management are essential for advancing\ntransportation research, especially in the current data-driven era, due to the\nadvent of novel applications in artificial intelligence. This paper presents\ncomprehensive guidelines for managing transportation sensor data, encompassing\nboth archived static data and real-time data streams. The real-time system\narchitecture integrates various applications with data acquisition systems\n(DAQ). By deploying the in-house designed, open-source Avena software platform\nalongside the NATS messaging system as a secure communication broker, reliable\ndata exchange is ensured. While robust databases like TimescaleDB facilitate\norganized storage, visualization platforms like Grafana provide real-time\nmonitoring capabilities.\n  In contrast, static data standards address the challenges in handling\nunstructured, voluminous datasets. The standards advocate for a combination of\ncost-effective bulk cloud storage for unprocessed sensor data and relational\ndatabases for recording summarized analyses. They highlight the role of cloud\ndata transfer tools like FME for efficient migration of sensor data from local\nstorages onto the cloud. Further, integration of robust visualization tools\ninto the framework helps in deriving patterns and trends from these complex\ndatasets.\n  The proposals were applied to INDOT's real-world case studies involving the\nI-65 and I-69 Greenfield districts. For real-time data collection, Campbell\nScientific DAQ systems were used, enabling continuous generation and monitoring\nof sensor metrics. In the case of the archived I-69 database, summary data was\ncompiled in Oracle, while the unprocessed data was stored in SharePoint. The\nresults underline the effectiveness of the proposed guidelines and motivate\ntheir adoption in research projects.",
        "Entanglement is crucial for quantum networks and computation, yet maintaining\nhigh-fidelity entangled quantum states is hindered by decoherence and\nresource-intensive purification methods. Here, we experimentally demonstrate\nentanglement pumping, utilizing bosonic quantum error correction (QEC) codes as\nlong-coherence-time storage qubits. By repetitively generating entanglement\nwith short-coherence-time qubits and injecting it into QEC-protected logical\nqubits, our approach effectively preserves entanglement. Through error\ndetection to discard error states and entanglement pumping to mitigate errors\nwithin the code space, we extend the lifespan of entangled logical qubits by\nnearly 50% compared to the case without entanglement pumping. This work\nhighlights the potential of bosonic logical qubits for scalable quantum\nnetworks and introduces a novel paradigm for efficient entanglement management.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "As the capabilities of large language models (LLMs) continue to expand, their\nusage has become increasingly prevalent. However, as reflected in numerous\nongoing lawsuits regarding LLM-generated content, addressing copyright\ninfringement remains a significant challenge. In this paper, we introduce\nPoisonedParrot: the first stealthy data poisoning attack that induces an LLM to\ngenerate copyrighted content even when the model has not been directly trained\non the specific copyrighted material. PoisonedParrot integrates small fragments\nof copyrighted text into the poison samples using an off-the-shelf LLM. Despite\nits simplicity, evaluated in a wide range of experiments, PoisonedParrot is\nsurprisingly effective at priming the model to generate copyrighted content\nwith no discernible side effects. Moreover, we discover that existing defenses\nare largely ineffective against our attack. Finally, we make the first attempt\nat mitigating copyright-infringement poisoning attacks by proposing a defense:\nParrotTrap. We encourage the community to explore this emerging threat model\nfurther.",
        "We propose a novel mechanism of primordial black hole (PBH) formation through\ninverted bubble collapse. In this scenario, bubbles nucleate sparsely in an\nincomplete first-order phase transition, followed by a bulk phase transition in\nthe rest of the universe that inverts these pre-existing bubbles into false\nvacuum regions. These spherically symmetric false-vacuum bubbles subsequently\ncollapse to form PBHs. Unlike conventional PBH formation mechanisms associated\nwith domain wall collapse or bubble coalescence, our inverted bubble collapse\nmechanism naturally ensures spherical collapse. We demonstrate that, when\napplied to the singlet extension of the Standard Model, this mechanism can\nproduce highly monochromatic PBHs with masses up to ${\\cal\nO}(10^{-7}\\,\\text{-}\\,10^{-5}) M_\\odot$, which potentially explain the\nmicrolensing events observed in the OGLE and Subaru HSC data.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "Quantum optimal control is concerned with the realisation of desired dynamics\nin quantum systems, serving as a linchpin for advancing quantum technologies\nand fundamental research. Analytic approaches and standard optimisation\nalgorithms do not yield satisfactory solutions for large quantum systems, and\nespecially not for real world quantum systems which are open and noisy. We\ndevise a physics-informed Reinforcement Learning (RL) algorithm that restricts\nthe space of possible solutions. We incorporate priors about the desired time\nscales of the quantum state dynamics - as well as realistic control signal\nlimitations - as constraints to the RL algorithm. These physics-informed\nconstraints additionally improve computational scalability by facilitating\nparallel optimisation. We evaluate our method on three broadly relevant quantum\nsystems (multi-level $\\Lambda$ system, Rydberg atom and superconducting\ntransmon) and incorporate real-world complications, arising from dissipation\nand control signal perturbations. We achieve both higher fidelities - which\nexceed 0.999 across all systems - and better robustness to time-dependent\nperturbations and experimental imperfections than previous methods. Lastly, we\ndemonstrate that incorporating multi-step feedback can yield solutions robust\neven to strong perturbations."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Solar Cells for Indoor Applications: Progress and Development",
    "start_abstract":"The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications.",
    "start_categories":[
      "astro-ph.SR"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      ],
      "abstract":[
        "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "RM-PoT: Reformulating Mathematical Problems and Solving via Program of\n  Thoughts",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Beyond Local Selection: Global Cut Selection for Enhanced Mixed-Integer\n  Programming",
        "Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the\n  ARC Task",
        "Sustainable and Intelligent Public Facility Failure Management System\n  Based on Large Language Models",
        "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
        "MEPNet: Medical Entity-balanced Prompting Network for Brain CT Report\n  Generation",
        "Electronic Health Records: Towards Digital Twins in Healthcare",
        "Lessons From Red Teaming 100 Generative AI Products",
        "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained\n  Scheduling",
        "Guidelines for Applying RL and MARL in Cybersecurity Applications",
        "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
        "Limited Effectiveness of LLM-based Data Augmentation for COVID-19\n  Misinformation Stance Detection",
        "The normal growth of linear groups over formal power serieses",
        "The multi-level friendship paradox for sparse random graphs",
        "The duality resolution at $n=p=2$",
        "Optimal Control of the Navier-Stokes equations via Pressure Boundary\n  Conditions",
        "Clinically Ready Magnetic Microrobots for Targeted Therapies",
        "How can representation dimension dominate structurally pruned LLMs?",
        "Deterministic Global Optimization over trained Kolmogorov Arnold\n  Networks",
        "EOG Communication Interface for Quadriplegics: Prototype & Signal\n  Processing",
        "The Popularity Hypothesis in Software Security: A Large-Scale\n  Replication with PHP Packages",
        "Transformer Based Time-Series Forecasting for Stock",
        "Defending Against Gradient Inversion Attacks for Biomedical Images via\n  Learnable Data Perturbation",
        "Cognitive AI framework: advances in the simulation of human thought",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Negativity in Self-Admitted Technical Debt: How Sentiment Influences\n  Prioritization"
      ],
      "abstract":[
        "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "In mixed-integer programming (MIP) solvers, cutting planes are essential for\nBranch-and-Cut (B&C) algorithms as they reduce the search space and accelerate\nthe solving process. Traditional methods rely on hard-coded heuristics for cut\nplane selection but fail to leverage problem-specific structural features.\nRecent machine learning approaches use neural networks for cut selection but\nfocus narrowly on the efficiency of single-node within the B&C algorithm,\nwithout considering the broader contextual information. To address this, we\npropose Global Cut Selection (GCS), which uses a bipartite graph to represent\nthe search tree and combines graph neural networks with reinforcement learning\nto develop cut selection strategies. Unlike prior methods, GCS applies cutting\nplanes across all nodes, incorporating richer contextual information.\nExperiments show GCS significantly improves solving efficiency for synthetic\nand large-scale real-world MIPs compared to traditional and learning-based\nmethods.",
        "While LLMs have exhibited strong performance on various NLP tasks, it is\nnoteworthy that most of these tasks rely on utilizing the vast amount of\nknowledge encoded in LLMs' parameters, rather than solving new problems without\nprior knowledge. In cognitive research, the latter ability is referred to as\nfluid intelligence, which is considered to be critical for assessing human\nintelligence. Recent research on fluid intelligence assessments has highlighted\nsignificant deficiencies in LLMs' abilities. In this paper, we analyze the\nchallenges LLMs face in demonstrating fluid intelligence through controlled\nexperiments, using the most representative ARC task as an example. Our study\nrevealed three major limitations in existing LLMs: limited ability for skill\ncomposition, unfamiliarity with abstract input formats, and the intrinsic\ndeficiency of left-to-right decoding. Our data and code can be found in\nhttps:\/\/wujunjie1998.github.io\/araoc-benchmark.github.io\/.",
        "This paper presents a new Large Language Model (LLM)-based Smart Device\nManagement framework, a pioneering approach designed to address the intricate\nchallenges of managing intelligent devices within public facilities, with a\nparticular emphasis on applications to libraries. Our framework leverages\nstate-of-the-art LLMs to analyze and predict device failures, thereby enhancing\noperational efficiency and reliability. Through prototype validation in\nreal-world library settings, we demonstrate the framework's practical\napplicability and its capacity to significantly reduce budgetary constraints on\npublic facilities. The advanced and innovative nature of our model is evident\nfrom its successful implementation in prototype testing. We plan to extend the\nframework's scope to include a wider array of public facilities and to\nintegrate it with cutting-edge cybersecurity technologies, such as Internet of\nThings (IoT) security and machine learning algorithms for threat detection and\nresponse. This will result in a comprehensive and proactive maintenance system\nthat not only bolsters the security of intelligent devices but also utilizes\nmachine learning for automated analysis and real-time threat mitigation. By\nincorporating these advanced cybersecurity elements, our framework will be\nwell-positioned to tackle the dynamic challenges of modern public\ninfrastructure, ensuring robust protection against potential threats and\nenabling facilities to anticipate and prevent failures, leading to substantial\ncost savings and enhanced service quality.",
        "Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.",
        "The automatic generation of brain CT reports has gained widespread attention,\ngiven its potential to assist radiologists in diagnosing cranial diseases.\nHowever, brain CT scans involve extensive medical entities, such as diverse\nanatomy regions and lesions, exhibiting highly inconsistent spatial patterns in\n3D volumetric space. This leads to biased learning of medical entities in\nexisting methods, resulting in repetitiveness and inaccuracy in generated\nreports. To this end, we propose a Medical Entity-balanced Prompting Network\n(MEPNet), which harnesses the large language model (LLM) to fairly interpret\nvarious entities for accurate brain CT report generation. By introducing the\nvisual embedding and the learning status of medical entities as enriched clues,\nour method prompts the LLM to balance the learning of diverse entities, thereby\nenhancing reports with comprehensive findings. First, to extract visual\nembedding of entities, we propose Knowledge-driven Joint Attention to explore\nand distill entity patterns using both explicit and implicit medical knowledge.\nThen, a Learning Status Scorer is designed to evaluate the learning of entity\nvisual embeddings, resulting in unique learning status for individual entities.\nFinally, these entity visual embeddings and status are elaborately integrated\ninto multi-modal prompts, to guide the text generation of LLM. This process\nallows LLM to self-adapt the learning process for biased-fitted entities,\nthereby covering detailed findings in generated reports. We conduct experiments\non two brain CT report generation benchmarks, showing the effectiveness in\nclinical accuracy and text coherence.",
        "The pivotal shift from traditional paper-based records to sophisticated\nElectronic Health Records (EHR), enabled systematic collection and analysis of\npatient data through descriptive statistics, providing insight into patterns\nand trends across patient populations. This evolution continued toward\npredictive analytics, allowing healthcare providers to anticipate patient\noutcomes and potential complications before they occur. This progression from\nbasic digital record-keeping to sophisticated predictive modelling and digital\ntwins reflects healthcare's broader evolution toward more integrated,\npatient-centred approaches that combine data-driven insights with personalized\ncare delivery. This chapter explores the evolution and significance of\nhealthcare information systems, beginning with an examination of the\nimplementation of EHR in the UK and the USA. It provides a comprehensive\noverview of the International Classification of Diseases (ICD) system, tracing\nits development from ICD-9 to ICD-10. Central to this discussion is the\nMIMIC-III database, a landmark achievement in healthcare data sharing and\narguably the most comprehensive critical care database freely available to\nresearchers worldwide. MIMIC-III has democratized access to high-quality\nhealthcare data, enabling unprecedented opportunities for research and\nanalysis. The chapter examines its structure, clinical outcome analysis\ncapabilities, and practical applications through case studies, with a\nparticular focus on mortality and length of stay metrics, vital signs\nextraction, and ICD coding. Through detailed entity-relationship diagrams and\npractical examples, the text illustrates MIMIC's complex data structure and\ndemonstrates how different querying approaches can lead to subtly different\nresults, emphasizing the critical importance of understanding the database's\narchitecture for accurate data extraction.",
        "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider.",
        "Effective multi-user delay-constrained scheduling is crucial in various\nreal-world applications, such as instant messaging, live streaming, and data\ncenter management. In these scenarios, schedulers must make real-time decisions\nto satisfy both delay and resource constraints without prior knowledge of\nsystem dynamics, which are often time-varying and challenging to estimate.\nCurrent learning-based methods typically require interactions with actual\nsystems during the training stage, which can be difficult or impractical, as it\nis capable of significantly degrading system performance and incurring\nsubstantial service costs. To address these challenges, we propose a novel\noffline reinforcement learning-based algorithm, named \\underline{S}cheduling By\n\\underline{O}ffline Learning with \\underline{C}ritic Guidance and\n\\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies\npurely from pre-collected \\emph{offline data}. SOCD innovatively employs a\ndiffusion-based policy network, complemented by a sampling-free critic network\nfor policy guidance. By integrating the Lagrangian multiplier optimization into\nthe offline reinforcement learning, SOCD effectively trains high-quality\nconstraint-aware policies exclusively from available datasets, eliminating the\nneed for online interactions with the system. Experimental results demonstrate\nthat SOCD is resilient to various system dynamics, including partially\nobservable and large-scale environments, and delivers superior performance\ncompared to existing methods.",
        "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.",
        "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
        "Misinformation surrounding emerging outbreaks poses a serious societal\nthreat, making robust countermeasures essential. One promising approach is\nstance detection (SD), which identifies whether social media posts support or\noppose misleading claims. In this work, we finetune classifiers on COVID-19\nmisinformation SD datasets consisting of claims and corresponding tweets.\nSpecifically, we test controllable misinformation generation (CMG) using large\nlanguage models (LLMs) as a method for data augmentation. While CMG\ndemonstrates the potential for expanding training datasets, our experiments\nreveal that performance gains over traditional augmentation methods are often\nminimal and inconsistent, primarily due to built-in safeguards within LLMs. We\nrelease our code and datasets to facilitate further research on misinformation\ndetection and generation.",
        "Put $R=\\F[[t_1, \\ldots, t_d]])$. We estimate the number of normal subgroups\nof $\\mathrm{SL}_2^1(\\F[[t_1, \\ldots, t_d]])$ for $p>2$, the number of ideals in\nthe Lie algebra $\\Lie(R)$, and the number of ideals in the associative algebra\n$R$.",
        "In Hazra, den Hollander and Parvaneh (2025) we analysed the friendship\nparadox for sparse random graphs. For four classes of random graphs we\ncharacterised the empirical distribution of the friendship biases between\nvertices and their neighbours at distance $1$, proving convergence as\n$n\\to\\infty$ to a limiting distribution, with $n$ the number of vertices, and\nidentifying moments and tail exponents of the limiting distribution. In the\npresent paper we look at the multi-level friendship bias between vertices and\ntheir neighbours at distance $k \\in \\mathbb{N}$ obtained via a $k$-step\nexploration according to a backtracking or a non-backtracking random walk. We\nidentify the limit of empirical distribution of the multi-level friendship\nbiases as $n\\to\\infty$ and\/or $k\\to\\infty$. We show that for non-backtracking\nexploration the two limits commute for a large class of sparse random graphs,\nincluding those that locally converge to a rooted Galton-Watson tree. In\nparticular, we show that the same limit arises when $k$ depends on $n$, i.e.,\n$k=k_n$, provided $\\lim_{n\\to\\infty} k_n = \\infty$ under some mild conditions.\nWe exhibit cases where the two limits do not commute and show the relevance of\nthe mixing time of the exploration.",
        "Working at the prime $2$ and chromatic height $2$, we construct a finite\nresolution of the homotopy fixed points of Morava $E$-theory with respect to\nthe subgroup $\\mathbb{G}_2^1$ of the Morava stabilizer group. This is an\nupgrade of the finite resolution of the homotopy fixed points of $E$-theory\nwith respect to the subgroup $\\mathbb{S}_2^1$ constructed in work of\nGoerss-Henn-Mahowald-Rezk, Beaudry and Bobkova-Goerss.",
        "In this work we study an optimal control problem subject to the instationary\nNavier-Stokes equations, where the control enters via an inhomogeneous\nNeumann\/Do-Nothing boundary condition. Despite the Navier-Stokes equations with\nthese boundary conditions not being well-posed for large times and\/or data, we\nobtain wellposedness of the optimal control problem by choosing a proper\ntracking type term. In order to discuss the regularity of the optimal control,\nstate and adjoint state, we present new results on $L^2(I;H^2(\\Omega))$\nregularity of solutions to a Stokes problem with mixed inhomogeneous boundary\nconditions.",
        "Systemic drug administration often causes off-target effects limiting the\nefficacy of advanced therapies. Targeted drug delivery approaches increase\nlocal drug concentrations at the diseased site while minimizing systemic drug\nexposure. We present a magnetically guided microrobotic drug delivery system\ncapable of precise navigation under physiological conditions. This platform\nintegrates a clinical electromagnetic navigation system, a custom-designed\nrelease catheter, and a dissolvable capsule for accurate therapeutic delivery.\nIn vitro tests showed precise navigation in human vasculature models, and in\nvivo experiments confirmed tracking under fluoroscopy and successful navigation\nin large animal models. The microrobot balances magnetic material\nconcentration, contrast agent loading, and therapeutic drug capacity, enabling\neffective hosting of therapeutics despite the integration complexity of its\ncomponents, offering a promising solution for precise targeted drug delivery.",
        "Pruning assumes a subnetwork exists in the original deep neural network,\nwhich can achieve comparative model performance with less computation than the\noriginal. However, it is unclear how the model performance varies with the\ndifferent subnetwork extractions. In this paper, we choose the representation\ndimension (or embedding dimension, model dimension, the dimension of the\nresidual stream in the relevant literature) as the entry point to this issue.\nWe investigate the linear transformations in the LLM transformer blocks and\nconsider a specific structured pruning approach, SliceGPT, to extract the\nsubnetworks of different representation dimensions. We mechanistically analyse\nthe activation flow during the model forward passes, and find the\nrepresentation dimension dominates the linear transformations, model\npredictions, and, finally, the model performance. Explicit analytical relations\nare given to calculate the pruned model performance (perplexity and accuracy)\nwithout actual evaluation, and are empirically validated with\nLlama-3-8B-Instruct and Phi-3-mini-4k-Instruct.",
        "To address the challenge of tractability for optimizing mathematical models\nin science and engineering, surrogate models are often employed. Recently, a\nnew class of machine learning models named Kolmogorov Arnold Networks (KANs)\nhave been proposed. It was reported that KANs can approximate a given\ninput\/output relationship with a high level of accuracy, requiring\nsignificantly fewer parameters than multilayer perceptrons. Hence, we aim to\nassess the suitability of deterministic global optimization of trained KANs by\nproposing their Mixed-Integer Nonlinear Programming (MINLP) formulation. We\nconduct extensive computational experiments for different KAN architectures.\nAdditionally, we propose alternative convex hull reformulation, local support\nand redundant constraints for the formulation aimed at improving the\neffectiveness of the MINLP formulation of the KAN. KANs demonstrate high\naccuracy while requiring relatively modest computational effort to optimize\nthem, particularly for cases with less than five inputs or outputs. For cases\nwith higher inputs or outputs, carefully considering the KAN architecture\nduring training may improve its effectiveness while optimizing over a trained\nKAN. Overall, we observe that KANs offer a promising alternative as surrogate\nmodels for deterministic global optimization.",
        "Electrooculography (EOG) is an electrophysiological signal that determines\nthe human eye orientation and is therefore widely used in Human Tracking\nInterfaces (HCI). The purpose of this project is to develop a communication\nmethod for quadriplegic patients using EOG signals aimed at text and voice\ngeneration. The system consists of 3D eye movement tracking embedded using a\ncustom-built prototype to measure the eyeball's left-right and up-down\nmovements. The ESP32 board, which has a set of parameters to convert the data\ninto content displayed on LCDs and MP3 players, is used to capture and process\nthe signal. helps people by facilitating more natural and efficient symptom\nexpression. The blink system will be able to incorporate face masks and more\neye tests as it continues to develop. Even if it might work, more research and\nclinical trials are needed to evaluate the system's usefulness and ensure that\nit performs as planned in real-world scenarios. With this project, assistive\ntechnology will make significant progress and improve the lives of many who\nsuffer from severe motor impairments.",
        "There has been a long-standing hypothesis that a software's popularity is\nrelated to its security or insecurity in both research and popular discourse.\nThere are also a few empirical studies that have examined the hypothesis,\neither explicitly or implicitly. The present work continues with and\ncontributes to this research with a replication-motivated large-scale analysis\nof software written in the PHP programming language. The dataset examined\ncontains nearly four hundred thousand open source software packages written in\nPHP. According to the results based on reported security vulnerabilities, the\nhypothesis does holds; packages having been affected by vulnerabilities over\ntheir release histories are generally more popular than packages without having\nbeen affected by a single vulnerability. With this replication results, the\npaper contributes to the efforts to strengthen the empirical knowledge base in\ncyber and software security.",
        "To the naked eye, stock prices are considered chaotic, dynamic, and\nunpredictable. Indeed, it is one of the most difficult forecasting tasks that\nhundreds of millions of retail traders and professional traders around the\nworld try to do every second even before the market opens. With recent advances\nin the development of machine learning and the amount of data the market\ngenerated over years, applying machine learning techniques such as deep\nlearning neural networks is unavoidable. In this work, we modeled the task as a\nmultivariate forecasting problem, instead of a naive autoregression problem.\nThe multivariate analysis is done using the attention mechanism via applying a\nmutated version of the Transformer, \"Stockformer\", which we created.",
        "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data.",
        "The Human Cognitive Simulation Framework represents a significant advancement\nin integrating human cognitive capabilities into artificial intelligence\nsystems. By merging short-term memory (conversation context), long-term memory\n(interaction context), advanced cognitive processing, and efficient knowledge\nmanagement, it ensures contextual coherence and persistent data storage,\nenhancing personalization and continuity in human-AI interactions. The\nframework employs a unified database that synchronizes these contexts while\nincorporating logical, creative, and analog processing modules inspired by\nhuman brain hemispheric functions to perform structured tasks and complex\ninferences. Dynamic knowledge updates enable real-time integration, improving\nadaptability and fostering applications in education, behavior analysis, and\nknowledge management. Despite its potential to process vast data volumes and\nenhance user experience, challenges remain in scalability, cognitive bias\nmitigation, and ethical compliance. This framework lays the foundation for\nfuture research in continuous learning algorithms, sustainability, and\nmultimodal adaptability, positioning Cognitive AI as a transformative model in\nemerging fields.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "Self-Admitted Technical Debt, or SATD, is a self-admission of technical debt\npresent in a software system. To effectively manage SATD, developers need to\nestimate its priority and assess the effort required to fix the described\ntechnical debt. About a quarter of descriptions of SATD in software systems\nexpress some form of negativity or negative emotions when describing technical\ndebt. In this paper, we report on an experiment conducted with 59 respondents\nto study whether negativity expressed in the description of SATD\n\\textbf{actually} affects the prioritization of SATD. The respondents are a mix\nof professional developers and students, and in the experiment, we asked\nparticipants to prioritize four vignettes: two expressing negativity and two\nexpressing neutral sentiment. To ensure realism, vignettes were based on\nexisting SATD. We find that negativity causes between one-third and half of\ndevelopers to prioritize SATD, in which negativity is expressed as having more\npriority. Developers affected by negativity when prioritizing SATD are twice as\nlikely to increase their estimation of urgency and 1.5 times as likely to\nincrease their estimation of importance and effort for SATD compared to the\nlikelihood of decreasing these prioritization scores. Our findings show how\ndevelopers actively use negativity in SATD to determine how urgently a\nparticular instance of TD should be addressed. However, our study also\ndescribes a gap in the actions and belief of developers. Even if 33% to 50% use\nnegativity to prioritize SATD, 67% of developers believe that using negativity\nas a proxy for priority is unacceptable. Therefore, we would not recommend\nusing negativity as a proxy for priority. However, we also recognize that\ndevelopers might unavoidably express negativity when describing technical debt."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "start_abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement).",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Solar Cells for Indoor Applications: Progress and Development"
      ],
      "abstract":[
        "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
      ],
      "categories":[
        "astro-ph.SR"
      ]
    },
    "list":{
      "title":[
        "TESS light curves and period changes in low-mass eclipsing binary BB\n  Persei",
        "Spectroscopic study of the late B-type eclipsing binary system AR\n  Aurigae A and B: Towards clarifying the differences in atmospheric parameters\n  and chemical abundances",
        "The Nature of Classical Be Star Outbursts: A Multi-Epoch Study of the Be\n  system EPIC 202060631",
        "Pulsation Properties of Blazhko and Non-Blazhko RRab Stars",
        "Investigating Solar Wind Outflows from Open-Closed Magnetic Field\n  Structures Using Coordinated Solar Orbiter and Hinode Observations",
        "On field line slippage rates in the solar corona",
        "Preliminary results from 5 years' spectral monitoring of Antares",
        "Bursty acceleration and 3D trajectories of electrons in a solar flare",
        "The influence of chromospheric activity on line formation",
        "Localized Heating and Dynamics of the Solar Corona due to a Symbiosis of\n  Waves and Reconnection",
        "Forecasting the 8 April 2024 Total Solar Eclipse with Multiple Solar\n  Photospheric Magnetograms",
        "Solar oblateness & asphericities temporal variations: outstanding some\n  unsolved issues",
        "Tests and calibrations of stellar models with two triply eclipsing\n  triple systems",
        "The MAGPI Survey: the subtle role of environment and not-so-subtle\n  impact of generations of stars on galaxy dynamics",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Noncommutative Phantom BTZ Black Hole",
        "Spectral synthesis in multidimensional Fourier algebras",
        "Chiral Vibrational Modes in Small Molecules",
        "Stability of oscillations in the spatially extended May-Leonard model",
        "Zero modes and Dirac-(logarithmic) Sobolev-type inequalities",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Phonon anomalies within the polar charge density wave phase of\n  superconductor Mo$_3$Al$_2$C with structural chirality",
        "Tuning Quantum States at Chirality-Reversed Planar Interface in Weyl\n  Semimetals using an Interstitial Layer",
        "Using Matrix-Free Tensor-Network Optimizations to Construct a\n  Reduced-Scaling and Robust Second-Order M{\\o}ller-Plesset Theory",
        "A filtered two-step variational integrator for charged-particle dynamics\n  in a normal or strong magnetic field",
        "Distribution and Moments of a Normalized Dissimilarity Ratio for two\n  Correlated Gamma Variables",
        "Prediction for close approaches with terrestrial planets of asteroids\n  from the main belt",
        "Infrared Metaplasmonics"
      ],
      "abstract":[
        "We present a detailed analysis of the low-mass detached eclipsing binary\nsystem BB Persei, which contains two K-type stars in a circular orbit with a\nshort period of 0.4856 d. We used light curves from the Transiting Exoplanet\nSurvey Satellite, which observed BB Per in five sectors, to determine its\nphotometric properties and a precise orbital ephemeris. The solution of the\nTESS light curve in Phoebe results in a detached configuration, where the\ntemperature of the primary component was fixed to $T_1 = 5~300$ K according to\nLamost, which gives us $T_2 = 5~050 \\pm 50$ K for the secondary. The spectral\ntype of the primary component was derived as K0 and the photometric mass ratio\nwas estimated $q = 0.90$. Slow period changes on the current O-C diagram\nspanning the past 25 years indicate the presence of a third body orbiting the\neclipsing pair with an orbital period of about 22 years. The companion could be\na red dwarf of spectral type M6 - M7 with a minimal mass of about 0.1\nM$_{\\odot}$. The characteristics and temporal variation of the dark region on\nthe surface of the secondary component were estimated.",
        "AR Aur A+B is a close binary of astrophysical interest, because dissimilar\nsurface compositions are reported between similar late B-type dwarfs. A new\nspectroscopic study on this system was carried out based on the disentangled\nspectra, in order to determine their atmospheric parameters and elemental\nabundances, The effective temperature and microturbulence (determined from the\nequivalent widths of Fe II lines) turned out (11150K, 0.9km\/s) and (10650K,\n0.1km\/s) for A and B. The chemical abundances of 28 elements were derived while\ntaking into account the non-LTE effect for Z<=15 elements (Z: atomic number).\nThe following trends were elucidated for [X\/H] (abundance of X relative to the\nSun): (1) Qualitatively, [X\/H] shows a rough global tendency of increasing with\nZ, with the gradient steeper for A than for B. (2) However, considerable\ndispersion is involved for A, since prominently large peculiarities are seen in\nspecific elements reflecting the characteristics of HgMn stars (e.g., very\ndeficient N, Al, Sc, Ni; markedly overabundant P, Mn). (3) In contrast, the\nZ-dependence of [X\/H] for B tends to be nearly linear with only a small\ndispersion. These observational facts may serve as a key to understanding the\ncritical condition for the emergence of chemical anomaly.",
        "Through cross-matching the ASASSN photometric light curves and LAMOST\nspectroscopic observations, we serendipitously captured a rare major outburst\nevent from the Be star EPIC 202060631, lasting over 1000 days. Fortuitously,\nthe LAMOST ow-resolution spectra densely covered the flux rising stage with 20\nepochs, while an additional 7 low-resolution and 11 medium esolution spectra\nmonitored the subsequent decay phase. Moreover, this target was observed by\nKepler telescope in its K2 mission when before the outburst and by TESS\ntelescope while after returning to quiescence. Analyses of these datasets\nreveal pulsation behavior and mode amplitudes, indications of radial and\ntangential motions in the photosphere, and clear evidence of mass ejection and\ncircumstellar disk formation. We modeled the central star using the BRUCE04\ncode and analyzed the disk structure using HDUST. Our results suggest episodic\nmass injection events from the central star triggered the disk buildup and\ncarried imprints of the changing stellar pulsations. This study offers unique\ninsights into the connections between photospheric activities, disk evolution,\nand stellar rotation during Be star outbursts.",
        "In this study, we conduct a comparative analysis of the properties of Blazhko\nand non-Blazhko RRab stars. We identified 1054 non-Blazhko and 785 Blazhko RRab\nstars in the photometric data observed by K2 mission, which, combined with\nthose 37 stars observed in the original Kepler field, constituted our study\nsample. Using the Fourier Decomposition method, we calculated the pulsation\nparameters, including phase differences and amplitude ratios, for these RRab\nstars, revealing significant discrepancies in the pulsation parameters between\nBlazhko and non-Blazhko RRab stars. However, distinguishing between Blazhko and\nNon-Blazhko RRab stars based on Fourier parameters remains challenging due to\nthe significant overlap in their distributions. By cross-matching our sample\nwith the LRS of LAMOST DR12, we identified 147 Blazhko and 111 non-Blazhko RRab\nstars, which exhibit similar metallicity distributions. Furthermore,\ncross-matching with Gaia DR3 data yielded 766 Blazhko and 950 non-Blazhko RRab\nstars, showing differences in color indices but not in absolute magnitudes. Our\nfindings suggested the Blazhko effect is linked to pulsation parameters and\ncolors, rather than metallicities or absolute magnitude.",
        "ESA\/NASA's Solar Orbiter (SO) allows us to study the solar corona at closer\ndistances and from different perspectives, which helps us to gain significant\ninsights into the origins of the solar wind. In this work, we present the\nanalysis of solar wind outflows from two locations: a narrow open-field\ncorridor and a small, mid-latitude coronal hole. These outflows were observed\noff-limb by the Metis coronagraph onboard SO and on-disk by the Extreme\nUltraviolet Imaging Spectrometer (EIS) onboard Hinode. Magnetic field\nextrapolations suggest that the upflow regions seen in EIS were the sources of\nthe outflowing solar wind observed with Metis. We find that the plasma\nassociated with the narrow open-field corridor has higher electron densities\nand lower outflow velocities compared to the coronal hole plasma in the middle\ncorona, even though the plasma properties of the two source regions in the low\ncorona are found to be relatively similar. The speed of solar wind from the\nopen-field corridor also shows no correlation with the magnetic field expansion\nfactor, unlike the coronal hole. These pronounced differences at higher\naltitudes may arise from the dynamic nature of the low-middle corona, in which\nreconnection can readily occur and may play an important role in driving solar\nwind variability.",
        "Magnetic reconnection is one of the fundamental dynamical processes in the\nsolar corona. The method of studying reconnection in active region-scale\nmagnetic fields generally depends on non-local methods (i.e. requiring\ninformation across the magnetic field under study) of magnetic topology, such\nas separatrix skeletons and quasi-separatrix layers. The theory of General\nMagnetic Reconnection is also non-local, in that its measure of the\nreconnection rate depends on determining the maxima of integrals along field\nlines. In this work, we complement the above approaches by introducing a local\ntheory of magnetic reconnection, that is one in which information about\nreconnection at a particular location depends only on quantities at that\nlocation. The theory connects the concept of the field line slippage rate,\nrelative to ideal motion, to the underlying local geometry of the magnetic\nfield characterized in terms of the Lorentz force and field-aligned current\ndensity. It is argued that the dominant non-ideal term for the solar corona,\ndiscussed in relation to this new theory, is mathematically equivalent to the\nanomalous resistivity employed by many magnetohrdrodynamic simulations.\nHowever, the general application of the theory is adaptable to the inclusion of\nother non-ideal terms, which may arise from turbulence modelling or the\ninclusion of a generalized Ohm's law. The theory is illustrated with two\nexamples of coronal magnetic fields related to flux ropes: an analytical model\nand a nonlinear force-free extrapolation. In terms of the latter, the slippage\nrate corresponds to the reconnection which would happen if the given (static)\nforce-free equilibrium were the instantaneous form of the magnetic field\ngoverned by an Ohm's law with non-ideal terms.",
        "We present preliminary results of 5 years' monitoring of the radial velocity\nof Alpha Sco, performed at the Astronomical Observatory of the Universidad de\nLos Andes in Bogot\\'a, Colombia. The data include 580 spectra acquired on 153\nnights between March 2015 and March 2020. The aim of this study is to probe the\ndynamics of the star's atmosphere on all possible time-scales through the\nvariations in observed radial velocity. At present, our findings are consistent\nwith previous results from other observers, and the combination of older and\nnew data make it possible to assess the several periodicities. A detailed study\nof these results, including the convective motions in the photosphere, is still\nin progress.",
        "During a solar flare, electrons are accelerated to non-thermal energies as a\nresult of magnetic reconnection. These electrons then propagate upwards and\ndownwards from the energy release site along magnetic field lines and produce\nradio and X-ray emission. On 11 November 2022, an M5.1 solar flare was observed\nby the Spectrometer\/Telescope for Imaging X-rays (STIX) on board Solar Orbiter\ntogether with various ground- and space-based radio instruments. The flare was\nassociated with several fine hard X-ray (HXR) structures and a complex set of\nmetric radio bursts (type III, J, and narrowband). By studying the evolution of\nX-ray, extreme ultraviolet, and radio sources, we aim to study the trajectories\nof the flare-accelerated electrons in the lower solar atmosphere and low\ncorona. We used observations from the STIX on board Solar Orbiter to study the\nevolution of X-ray sources. Using radio imaging from the Nan\\c{c}ay Radio\nheliograph (NRH) and the Newkirk density model, we constructed 3D trajectories\nof 14 radio bursts. Imaging of the HXR fine structures shows several sources at\ndifferent times. The STIX and NRH imaging shows correlated changes in the\nlocation of the HXR and radio source at the highest frequency during the most\nintense impulsive period. Imaging and 3D trajectories of all the bursts show\nthat electrons are getting accelerated at different locations and along several\ndistinct field lines. The longitude and latitude extent of the trajectories are\n~30 arcsec and ~ 152 arcsec. We find that the electrons producing HXR and radio\nemission have similar acceleration origins. Importantly, our study supports the\nscenario that the flare acceleration process is temporally and spatially\nfragmentary, and during each of these small-scale processes, the electron beams\nare injected into a very fibrous environment and produce complex HXR and radio\nemission.",
        "One of the primary sources of stellar spectral variability is magnetic\nactivity. While our current understanding of chromospheric activity is largely\nderived from specific lines sensitive to chromospheric heating, such as the Ca\nII HK doublet, previous observational studies have shown that other spectral\nlines are also affected. To investigate the influence of activity on line\nformation in greater detail, we constructed a set of stellar models for\nhypothetical G2 dwarf stars with varying levels of activity and calculated\ntheir synthetic spectra. A comparison of these spectra revealed two spectral\nregions most significantly impacted by activity: approximately 3300-4400 A and\n5250-5500 A. By calculating the total contribution function of the lines, we\ndetermined that the emergence of a secondary chromospheric contribution to line\nformation is the primary mechanism driving these changes. Based on our\ncalculations and analysis, we compiled a list of transition lines and their\ncorresponding changes due to chromospheric activity. This list could serve as a\nvaluable tool for selecting spectral lines applicable to a wide range of\nastrophysical studies.",
        "The Sun's outer atmosphere, the corona, is maintained at mega-Kelvin\ntemperatures and fills the heliosphere with a supersonic outflowing wind. The\ndissipation of magnetic waves and direct electric currents are likely to be the\nmost significant processes for heating the corona, but a lively debate exists\non their relative roles. Here, we suggest that the two are often intrinsically\nlinked, since magnetic waves may trigger current dissipation, and impulsive\nreconnection can launch magnetic waves. We present a study of the first of\nthese processes by using a 2D physics-based numerical simulation using the\nAdaptive Mesh Refined (AMR) Versatile Advection Code (VAC). Magnetic waves such\nas fast magnetoacoustic waves are often observed to propagate in the\nlarge-scale corona and interact with local magnetic structures. The present\nnumerical simulations show how the propagation of magnetic disturbances towards\na null point or separator can lead to the accumulation of the electric\ncurrents. Lorentz forces can laterally push and vertically stretch the magnetic\nfields, forming a current sheet with a strong magnetic-field gradient. The\nmagnetic field lines then break and reconnect, and so contribute towards\ncoronal heating. Numerical results are presented that support these ideas and\nsupport the concept of a symbiosis between waves and reconnection in heating\nthe solar corona.",
        "The 8 April 2024 total solar eclipse (TSE) provides a unique opportunity to\nstudy the solar corona. This work presents our prediction of the solar corona\nat the time of the eclipse based on magnetohydrodynamic (MHD) modeling\nperformed with the Alfv\\'{e}n Wave Solar Model-Realtime (AWSoM-R) in the Space\nWeather Modeling Framework, developed at the University of Michigan. We\nperformed multiple simulations made with data input in the form of synchronic\nmagnetograms from four sources, i.e., ADAPT-GONG, Lockheed Martin ESFAM, HipFT\nand NSO-NRT magnetograms. Simulations also include a higher-resolution model\nand a post-eclipse model incorporating newly emerged active regions. Our study\nfundamentally focuses on the limitations imposed by the lack of global solar\nobservations, particularly on how these limitations affect coronal simulations.\nSpecifically, we examine how differences among the magnetograms and the absence\nof observations from the east limb, due to the Sun's rotation, impact the\naccuracy of the predicted coronal structures. We synthesized a variety of\nrepresentative observables, including the white-light and extreme-ultraviolet\nimages from each model, and compared them with observations. The synthesized\nobservables show remarkable differences because of the distinct magnetic\ncoronal topologies, which stem from the varied magnetic flux distributions and\nthe gaps in observational coverage. Our findings emphasize the need for\ncomprehensive and multi-satellite magnetic field observations to improve future\nsolar corona predictions.",
        "Solar oblateness has been the subject of several studies dating back to the\nnineteenth century. Despite diffculties, both theoretical and observational,\ntangible results have been achieved. However, variability of the solar\noblateness with time is still poorly known. How the solar shape evolves with\nthe solar cycle has been a challenging problem. Analysis of the helioseismic\ndata, which are the most accurate measure of the solar structure up to now,\nleads to the determination of asphericity coeffcients which have been found to\nchange with time. We show here that by inverting even coeffcients of f-mode\noscillation frequency splitting to obtain the oblateness magnitude and its\ntemporal dependence can be inferred. It is found that the oblateness variations\nlag the solar activity cycles by about 3 years. A major change occurred between\nsolar cycles 23 and 24 is that the oblateness was greater in cycle 24 despite\nthe lower solar activity level. Such results may help to better understand the\nnear-subsurface layers as they strongly impacts the internal dynamics of the\nSun and may induce instabilities driving the transport of angular momentum.",
        "We investigated the possibility of using two recently characterised triply\neclipsing triple systems to constrain stellar model parameters. We specifically\nfocused on evaluating the influence of the underlying astrophysical assumptions\nemployed in the characterisation of the system to fix absolute values of the\nradii, effective temperatures, and metallicity. We used dense grids of\npre-computed stellar models to fit the data for the triply eclipsing systems\nwith a modified version of the SCEPtER pipeline. We achieve an excellent\nagreement with observational data for TIC 650024463, which comprises three\nlow-mass main-sequence (MS) stars. We find it has an age of $9.0^{+1.4}_{-1.1}$\nGyr and a multimodal posterior density. Characterising TIC 323486857 proved\nmore challenging. This system comprises two intermediate-mass MS stars and a\nslightly more massive tertiary in the red giant branch phase. For this last\nsystem we tested alternative scenarios for convective core overshooting. When\nall stars were assumed to have the same overshooting efficiency, significant\ndiscrepancies arose with the observed data for the tertiary star. This\ndiscrepancy may arise from the different assumptions regarding overshooting\nefficiency made for the observational characterisation of the system, in which\nan increasing overshooting efficiency with stellar mass was adopted. By\nallowing independent overshooting efficiencies for all stars, we recovered a\nsolution close to that adopted in the system observational characterisation.\nEncouragingly, despite the relevant differences between the adopted stellar\nmodels and those used for the observational characterisation, we found a system\nage of $2.33^{+0.18}_{-0.16}$ Gyr in all the tested scenarios, and this age is\nin agreement with independent determinations.",
        "The stellar age and mass of galaxies have been suggested as the primary\ndeterminants for the dynamical state of galaxies, with environment seemingly\nplaying no or only a very minor role. We use a sample of 77 galaxies at\nintermediate redshift (z~0.3) in the Middle-Ages Galaxies Properties with\nIntegral field spectroscopy (MAGPI) Survey to study the subtle impact of\nenvironment on galaxy dynamics. We use a combination of statistical techniques\n(simple and partial correlations and principal component analysis) to isolate\nthe contribution of environment on galaxy dynamics, while explicitly accounting\nfor known factors such as stellar age, star formation histories and stellar\nmasses. We consider these dynamical parameters: high-order kinematics of the\nline-of-sight velocity distribution (parametrised by the Gauss-Hermite\ncoefficients $h_3$ and $h_4$), kinematic asymmetries $V_{\\rm asym}$ derived\nusing kinemetry and the observational spin parameter proxy $\\lambda_{R_e}$. Of\nthese, the mean $h_4$ is the only parameter found to have a significant\ncorrelation with environment as parametrised by group dynamical mass. This\ncorrelation exists even after accounting for age and stellar mass trends.\nFinally, we confirm that variations in the spin parameter $\\lambda_{R_e}$ are\nmost strongly (anti-)correlated with age as seen in local studies, and show\nthat this dependence is well-established by z~0.3.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "This work explores the thermodynamic and geometric properties of phantom BTZ\nblack holes within the framework of noncommutative spacetime, where\nnoncommutative effects are incorporated via Lorentzian distributions for mass\nand charge. The resulting modifications in spacetime geometry introduce\nsignificant alterations to horizon structures and curvature singularities. A\ncomprehensive and comparative thermodynamic analysis is conducted, examining\nthe differences between phantom and ordinary matter cases. This includes an\ninvestigation of Hawking temperature, entropy, heat capacity, and stability\ncriteria. Additionally, the black hole is analyzed as a thermodynamic heat\nengine, with its efficiency evaluated as a function of noncommutative\nparameters. Our findings highlight the profound impact of noncommutativity on\nthe thermodynamic behavior and efficiency of phantom BTZ black holes, revealing\nnew insights into the interplay between quantum spacetime effects and exotic\nfield dynamics. The results indicate that noncommutative corrections not only\nmodify the stability conditions of these black holes but also play a crucial\nrole in governing phase transitions. Furthermore, we demonstrate that\nnoncommutativity influences energy extraction processes, refining our\nunderstanding of black hole thermodynamics in lower-dimensional spacetimes and\ndistinguishing the behavior of phantom and ordinary matter cases.",
        "Let $G$ be a locally compact group and let $A^n(G)$ denote the\n$n$-dimensional Fourier algebra, introduced by Todorov and Turowska. We\ninvestigate spectral synthesis properties of the multidimensional Fourier\nalgebra $A^n(G).$ In particular, we prove versions of the subgroup lemma,\ninjection, and inverse projection theorems for both spectral sets and Ditkin\nsets. Additionally, we provide a result on the parallel synthesis between\n$A^n(G)$ and $A^{n+1}(G)$ and finally prove Malliavin's theorem.",
        "The development of quantitative methods for characterizing molecular\nchirality can provide an important tool for studying chirality induced\nphenomena in molecular systems. Significant progress has been made in recent\nyears toward understanding the chirality of molecular normal vibrational modes,\nmostly focusing on vibrations of helical molecular structures. In the present\nstudy, we examine the applicability two methodologies previously used for\nhelical structures for the quantification of the chirality of molecular normal\nmodes across a range of small, not necessarily helical, molecules. The first\napproach involves the application of the Continuous Chirality Measure (CCM) to\neach normal mode by associating the mode with a structure formed by imposing\nthe corresponding motion about a common origin. The second approach assigns to\neach normal mode a pseudoscalar defined as the product of atomic linear and\nangular momentum summed over all atoms. In particular, using the CCM also as a\nmeasure of the chirality of the underlying molecular structure, we establish\nthe existence of correlation between the chirality of molecular normal modes\nand that of the underlying molecular structure. Furthermore, we find that\nnormal modes associated with different frequency ranges of the molecular\nvibrational spectrum exhibit distinct handedness behavior.",
        "The May-Leonard model for three competing species, symmetric with respect to\n  cyclic permutation of the variables and extended by diffusive terms, is\nconsidered.\n  Exact time-periodic solutions of the system have been found, and their\nstability\n  with respect to spatially periodic disturbances is studied. The stability of\nsolu tions with respect to longwave spatial modulations is revealed. A period\ndoubling\n  instability breaking the spatial uniformity is found.",
        "We study the decay rate of the zero modes of the Dirac operator with a\nmatrix-valued potential that is considered here without any regularity\nassumptions, compared to the existing literature. For the Dirac operator and\nfor Clifford-valued functions we prove the $L^p$-$L^2$ Dirac Sobolev inequality\nwith explicit constant, as well as the $L^p$-$L^q$ Dirac-Sobolev inequalities.\nWe prove its logarithmic counterpart for $q=2$, extending it to its Gaussian\nversion of Gross, as well as show Nash and Poincar\\'e inequalities in this\nsetting, with explicit values for constants.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "We employ polarization-resolved Raman spectroscopy to study the lattice\ndynamics of the polar charge density wave phase of the superconductor\nMo$_3$Al$_2$C with structural chirality. We show the phononic signatures of the\ncharge density wave transition at $T^*$=155K in Mo$_3$Al$_2$C. The detailed\ntemperature dependence of these phonon modes' frequency,\nhalf-width-at-half-maximum, and the integrated area below $T^*$ reveal\nanomalies at an intermediate temperature $T'\\sim$100K, especially for the\nlow-energy modes at 130cm$^{-1}$ and 180cm$^{-1}$. Since these low-energy modes\nare dominated by Mo-related lattice vibration, we propose that lattice\nanomalies at $T'$ within the charge density wave phase are related to a\nmodification of the Mo displacements while preserving the crystal symmetry.",
        "The electronic band structure of Weyl semimetals possesses pairs of linear\nband crossings, called Weyl nodes, characterized by opposite chirality charges\nassociated with each node. The momentum space position of the nodes can reverse\nacross a planar interface and these host Fermi-arc-like bound states, in\naddition to scattering states. We show that a magnetic interstitial layer can\ntune these states in three distinct ways. The electrostatic potential and one\nof the in-plane magnetic potential components control the shape of the bound\nstate Fermi-arcs. For moderate values of the same in-plane magnetic potential\nelectrons are spin-filtered across the interface, while both the in-plane\nmagnetic components and the electrostatic potential control the transmission of\nelectrons. The ratio of in-plane to out-of-plane magnetic components can be\nused to turn on or turn off the magnetic potential effects, since the latter\ndoes not affect the interface states. The tunability arises from spin-momentum\nlocking and chirality reversal at the interface. Thus, the effects can mix or\ninterchange depending on the specific material but the states will remain\ntunable.",
        "We investigate the application of the canonical polyadic decomposition (CPD)\nto the tensor hypercontraction (THC) and Laplace transform (LT) approximated\nsecond-order M{\\o}ller-Plesset (MP2) method. By introducing these\ndecompositions we formally reduce the scaling of the canonical MP2 method from\n$\\mathcal{O}(N^5)$ to $\\mathcal{O}(N^3)$ and the storage complexity from\n$\\mathcal{O}(N^4)$ to $\\mathcal{O}(N^2)$. We are able to construct the THC\nrepresentation in $\\mathcal{O}(N^3)$ time by employing the interpolative\nseparable density fitting decomposition strategy. Furthermore, we introduce a\nCPD optimization strategy that takes advantage of the THC representation to\ndecompose the order-four two-electron integral tensor with a computational\nscaling of $\\mathcal{O}(N^3)$. Finally, we show that the rank of the CPD in the\napproximation of MP2 scales linearly with system size and that this CPD-ISDF-LT\nMP2 strategy realizes a performance advantage over canonical LT MP2 in both\ncomputational wall-times and memory resource requirements.",
        "This article is concerned with a new filtered two-step variational integrator\nfor solving the charged-particle dynamics in a mildly non-homogeneous normal or\nstrong magnetic field with a dimensionless parameter $\\epsilon$ inversely\nproportional to the strength of the magnetic field. In the case of a normal\nmagnetic field ($\\epsilon \\approx 1$), second-order error bounds and long time\nenergy and momentum conservations are obtained. Moreover, the proof of the\nlong-term analysis is accomplished by the backward error analysis. For the\nstrong magnetic field ($0<\\epsilon \\ll1$), this paper clarifies the behaviour\nof the filtered variational integrator for both a large stepsize $h^2 \\geq\n\\epsilon$ and a smaller stepsize $ h \\sim \\epsilon$. The approach to analysing\nthe error bounds for these two stepsizes is based on comparing the modulated\nFourier expansions of the exact and the numerical solutions. It is shown that\nthe proposed integrator achieves a second-order accuracy $\\mathcal{O}(h^2)$ in\nthe position and in the parallel velocity for a large step size and an\n$\\mathcal{O}(\\epsilon)$ accuracy for a smaller stepsize. This paper also yields\nthe long time energy and magnetic moment conservations for the strong magnetic\nfield by developing the modulated Fourier expansion of the proposed scheme. All\nthe theoretical results of the error behaviour and long-term conservations are\nnumerically demonstrated by two numerical experiments.",
        "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
        "Potentially Hazardous Asteroids (PHAs), a special subset of Near-Earth\nObjects, are both dangerous and scientifically valuable. PHAs that truly\nundergo close approaches with the Earth (dubbed CAPHAs) are of particular\ninterest and extensively studied. The concept and study of CAPHA can be\nextended to other Solar system planets, which have significant implications for\nfuture planet-based observations and explorations. In this work, we conduct\nnumerical simulations that incorporate the Yarkovsky effect to study the\ntransformation of main belt asteroids into CAPHAs of terrestrial planets, using\nprecise nominal timesteps, especially to ensure the reliability of the results\nfor Mercury and Venus. Our simulations predict a total of 1893 Mercury-CAPHAs,\n3014 Venus-CAPHAs, 3791 Earth-CAPHAs and 18066 Mars-CAPHAs, with an occurrence\nfrequency of about 1, 9, 15 and 66 per year, respectively. The values for\nMars-CAPHAs are consistent with our previous work, which were based on\nsimulations with a larger nominal timestep. The predicted occurrence frequency\nand velocity distribution of Earth-CAPHAs are in reasonable agreement with the\nobserved population of Earth-CAPHAs. We also find that certain asteroids can be\ncaught in close approach with different planets at different times, raising an\ninteresting possibility of using them as transportation between terrestrial\nplanets in the future.",
        "Plasmonic response in metals, defined as the ability to support subwavelength\nconfinement of surface plasmon modes, is typically limited to a narrow\nfrequency range below the metals' plasma frequency. This places severe\nlimitations on the operational wavelengths of plasmonic materials and devices.\nHowever, when the volume of a metal film is massively decreased, highly\nconfined quasi-two-dimensional surface plasmon modes can be supported out to\nwavelengths well beyond the plasma wavelength. While this has, thus far, been\nachieved using ultra-thin (nm-scale) metals, such films are quite difficult to\nrealize, and suffer from even higher losses than bulk plasmonic films. To\nextend the plasmonic response to the infrared, here we introduce the concept of\nmetaplasmonics, representing a novel plasmonic modality with a host of\nappealing properties. By fabricating and characterizing a series of\nmetaplasmonic nanoribbons, we demonstrate large confinement, high quality\nfactors, and large near-field enhancements across a broad wavelength range,\nextending well beyond the limited bandwidth of traditional plasmonic materials.\nWe demonstrate $35\\times$ plasmon wavelength reduction, and our numerical\nsimulations suggest that further wavelength reduction, up to a factor of 150,\nis achievable using our approach. The demonstration of the metaplasmonics\nparadigm offers a promising path to fill the near- and mid-infrared\ntechnological gap for high quality plasmonic materials, and provides a new\nmaterial system to study the effects of extreme plasmonic confinement for\napplications in nonlinear and quantum plasmonics."
      ]
    }
  },
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b23",
    "start_title":"DARWIN Series: Domain Specific Large Language Models for Natural Science",
    "start_abstract":"Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b29"
      ],
      "title":[
        "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
      ],
      "abstract":[
        "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Estimating the Sensitivity of IceCube-Gen2 to Cosmic Ray Mass Separation",
        "Cascaded Gamma-ray Emission Associated with the KM3NeT Ultra-High-Energy\n  Event KM3-230213A",
        "Collapse of Rotating White Dwarfs and Multimessenger Signals",
        "KM3NeT Constraint on Lorentz-Violating Superluminal Neutrino Velocity",
        "Revisiting the fundamental parameters for the black hole X-ray transient\n  Swift J1753.5-0127",
        "Quantitative exploration of the similarity of gamma-ray pulsar light\n  curves",
        "Photon-ALP beam propagation from Mrk 501",
        "Search for neutrino doublets and triplets using 11.4 years of IceCube\n  data",
        "Modulation of X-ray flux by obscuration of neutron star boundary layer",
        "New insight into the Rapid Burster by Insight-HXMT",
        "A Study of thin relativistic magnetic accretion disk around a distorted\n  black hole",
        "Pattern and Origin for the Extreme $\\gamma$-ray Flares of 3C 454.3 and\n  3C 279: An Astrophysical Critical Damper?",
        "Variabilities driven by satellite black hole migration in AGN discs",
        "Non-Affine Extensions of the Raychaudhuri Equation in the K-essence\n  Framework",
        "Time-series attribution maps with regularized contrastive learning",
        "Ion-kinetic-energy sampling in a 22-pole trap using ring-electrode\n  evaporation",
        "Improved Quantum Computation using Operator Backpropagation",
        "Reservoir Computing and Photoelectrochemical Sensors: A Marriage of\n  Convenience",
        "On the localization length of finite-volume random block Schr\\\"odinger\n  operators",
        "Theory of neutrino slow flavor evolution. Part II. Space-time evolution\n  of linear instabilities",
        "Thermodynamic analysis and shadow bound of black holes surrounded by a\n  dark matter halo",
        "Canonical forms of oriented matroids",
        "Beyond surveys: A High-Precision Wealth Inequality Mapping of China's\n  Rural Households Derived from Satellite and Street View Imageries",
        "More on the corner-vector construction for spherical designs",
        "Enabling GPU Portability into the Numba-JITed Monte Carlo Particle\n  Transport Code MC\/DC",
        "BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid\n  Dynamics",
        "A new practical and effective source-independent full-waveform inversion\n  with a velocity-distribution supported deep image prior: Applications to two\n  real datasets",
        "Particle-based plasma simulation using a graph neural network"
      ],
      "abstract":[
        "IceCube-Gen2 is a proposed extension to the existing IceCube Neutrino\nObservatory at the South Pole. It will consist of three components: an in-ice\noptical array, a surface array on top of the optical array, and a radio array\nfor detecting ultra-high energy neutrinos. Here we study the sensitivity of\nthis future detector to the mass separation of primary cosmic rays, using\nCORSIKA Monte Carlo simulations of extensive air showers initiated by H, He, O\nand Fe primaries. The surface array will use two types of detection\ntechnologies: scintillation detectors and radio antennas; the latter are not\nconsidered in this study. A set of mass-sensitive variables are investigated\nutilizing both the scintillators of the surface array and the full optical\nin-ice array. Among these, the high-energy muons measurable by the in-ice array\nare found to have the highest mass separation power for showers for which the\ncosmic-ray energy is known, e.g. from the surface array.",
        "A neutrino-like event with an energy of $\\sim 220 \\,{\\rm PeV}$ was recently\ndetected by the KM3NeT\/ARCA telescope. If this neutrino comes from an\nastrophysical source, or from the interaction of an ultra-high-energy cosmic\nray in the intergalactic medium, the ultra-high-energy gamma rays that are\nco-produced with the neutrinos will scatter with the extragalactic background\nlight, producing an electromagnetic cascade and resulting in emission at\nGeV-to-TeV energies. In this paper, we compute the gamma-ray flux from this\nneutrino source considering various source distances and strengths of the\nintergalactic magnetic field (IGMF). We find that the associated gamma-ray\nemission could be observed by existing imaging air cherenkov telescopes and air\nshower gamma-ray observatories, unless the strength of the IGMF is $B\\gtrsim\n3\\times 10^{-13}$ G, or the ultra-high-energy gamma-rays are attenuated inside\nof the source itself. In the latter case, this source is expected to be\nradio-loud.",
        "We present results of numerical relativity simulations of core collapse of\nrotating magnetized white dwarfs (WDs) in three dimension, aiming at discussing\nthe explosion dynamics and associate multi-messenger signals: gravitational\nwaves (GWs), neutrinos, and electromagnetic counterparts. All WDs initiate\ngravitational collapse due to electron captures and then experience prompt type\nexplosions after the proto neutron star formation. We observe the explosions\ndominated by a bipolar structure and the emergence of strong spiral waves in\nrapidly rotating models. The spiral waves facilitate to increase both the\nexplosion energy and ejecta mass, though the final values still fall in the\ncategory of low explosion energy supernovae with small ejecta mass. The spiral\nwaves also produce strong GWs, which may expand the horizon distance of such\nevents against GWs up to $\\sim 10$ Mpc for third-generation ground-based\ndetectors. Additionally as an intriguing implication, we demonstrate that such\naccretion or merger induced collapse of WDs might be able to explain some of\nthe rapidly evolving optical transients, such as fast blue optical transients\n(FBOTs), as previously suggested. Based on the simulation results together with\nseveral assumptions, we confirm that the magnetar may account for the brighter\nside of observed FBOTs, while a combination of ejecta-envelope interaction\nwhich can be also followed by radioactive decay of heavy elements synthesized\nalong with the explosion might still explain the fainter branch even in the\nabsence of magnetar formation.",
        "Lorentz invariance is a fundamental symmetry of spacetime and foundational to\nmodern physics. One of its most important consequences is the constancy of the\nspeed of light. This invariance, together with the geometry of spacetime,\nimplies that no particle can move faster than the speed of light. In this\narticle, we present the most stringent neutrino-based test of this prediction,\nusing the highest energy neutrino ever detected to date, KM3-230213A. The\narrival of this event, with an energy of $220^{+570}_{-110}\\,\\text{PeV}$, sets\na constraint on $\\delta \\equiv c_\\nu^2-1 < 4\\times10^{-22}$.",
        "We present time-resolved Gran Telescopio Canarias optical spectroscopy and\nWilliam Herschel Telescope $i$-band photometry of the X-ray transient SWIFT\nJ1753.5-0127 in quiescence. The $i$-band light curve is dominated by flickering\nwith an amplitude of $\\sim 0.5$ mag and shows no evidence of the ellipsoidal\nmodulation of the companion star. The telluric-corrected average spectrum, on\nthe other hand, reveals the presence of weak (strongly veiled) TiO bands at\n$7055$ \\.A and $7589$ \\.A. We used them for a spectral classification, finding\nan M4-5 V companion star. However, as velocity shifts are not clearly detected\nin the individual spectra, we turned the analysis to the double-peaked\nH$\\alpha$ emission line from the accretion disc. By exploiting the empirical\ncorrelations established for quiescent X-ray transients between the line\nmorphology and fundamental binary parameters, we estimated the radial velocity\nsemi-amplitude of the companion $K_2 = 820 \\pm 36$ km s$^{-1}$, a mass ratio $q\n= 0.023 \\pm 0.006$ and an inclination $i = 79 \\pm 5$ deg. Moreover, an orbital\nperiod of $3.26 \\pm 0.02$ h was measured from the modulation of the centroid\nvelocities and the double-peak trough depth of the H$\\alpha$ profile. These\nquantities yielded a mass function $f(M_1) = 7.8 \\pm 1.0$ M$_\\odot$ and black\nhole and companion star masses of $M_1 = 8.8 \\pm 1.3$ M$_\\odot$ and $M_2 = 0.20\n\\pm 0.06$ M$_\\odot$, respectively. The companion star mass is in line with the\nspectral classification obtained from the relative depth of the TiO bands.\nBased on the mean quiescent magnitude ($i = 21.4 \\pm 0.1$), orbital period, and\ninterstellar extinction, we estimate the distance to the source to be $3.9 \\pm\n0.7$ kpc and a Galactic plane elevation of $0.8 \\pm 0.2$ kpc, supporting the\ncase for a large natal kick.",
        "We introduce and apply a methodology based on dynamic time warping (DTW) to\ncompare the whole set of gamma-ray light curves reported in the Third\nFermi-Large Area Telescope Pulsar Catalogue. Our method allows us to\nquantitatively measure the degree of global similarity between two light curves\nbeyond comparing indicators such as how many peaks there are, which is their\nseparation, width, and height. Once the morphology of the light curve is\nshowcased via background subtraction, min-max scaler normalization, and\nrotations are considered to take into account that phase 0 is arbitrary, the\nlevel of detail with which light curves of different pulsars appear is\nrevealed. In many cases their similarity is striking and occurs disregarding\nany other timing, physical, or spectral property. In particular, some MSPs and\nyoung pulsars share detailed light curve morphology.",
        "The very high energy (VHE, E $>$ $100 \\mathrm~{GeV}$) $\\gamma$-ray\nobservations offer a possibility of indirectly detecting the presence of\naxion-like particles (ALPs). The paper focuses on detecting photon-ALP\noscillations on $\\gamma$-ray spectra from distant sources in astrophysical\nmagnetic fields. Strong evidence indicates that: (1) the photon-ALP\noscillations can effectively decrease the photon absorption at energies of\nseveral tens of TeV -- caused by the extragalactic background light (EBL) -- to\na level able to explain better the observational data; (2) the impact of\nmagnetic-field models in photon-ALP beams crossing several magnetized media is\nsignificant. We revisit the expected signature for the photon-ALP oscillation\neffects on $\\gamma-\\gamma $ absorption in the TeV spectra of Mrk 501. The\nresult issues that the photon-ALP beam propagation with mass\n$\\mathrm{m_a}\\sim10^{-10} eV$ and two-photon coupling constant\n$\\begin{aligned}g_{a\\gamma}\\sim0.417\\times10^{-11}GeV^{-1}\\end{aligned}$\ncrossing reasonable magnetic field scenarios considered here can roughly\nreproduce the observed TeV $\\gamma$-ray spectra for Mrk 501.",
        "We report a search for high-energy astrophysical neutrino multiplets,\ndetections of multiple neutrino clusters in the same direction within 30 days,\nbased on an analysis of 11.4 years of IceCube data. A new search method\noptimized for transient neutrino emission with a monthly time scale is\nemployed, providing a higher sensitivity to neutrino fluxes. This result is\nsensitive to neutrino transient emission, reaching per-flavor flux of\napproximately $10^{-10}\\ {\\rm erg}\\ {\\rm cm}^{-2}\\ {\\rm sec}^{-1}$ from the\nNorthern sky in the energy range $E\\gtrsim 50$~TeV. The number of doublets and\ntriplets identified in this search is compatible with the atmospheric\nbackground hypothesis, which leads us to set limits on the nature of neutrino\ntransient sources with emission timescales of one month.",
        "The quasi-periodic oscillations (QPOs) observed in the X-ray variability of\nboth black hole (BH) and neutron star (NS) systems provide a tool for probing\nstrong gravity and dense matter equations of state. Nevertheless, the mechanism\nof QPO modulation in NS systems, where the amplitudes of QPOs with frequencies\napproaching kHz range are very high in comparison to BH high-frequency QPOs,\nremains an unsolved puzzle. Relativistic ray tracing of photons emitted from\nthe immediate vicinity of compact objects has, to date, been used to\ninvestigate various mechanisms that explain the observed weak BH QPOs. However,\nit has not been applied to model the NS QPO signal, which requires\nincorporating the NS surface and a bright boundary layer (BL) on it. Here, we\nexplore the QPO modulation mechanisms based on the BL obscuration. Using\nsimplified models of axisymmetric oscillations of thick accretion discs (tori),\nwe demonstrate that the disc oscillations drive the high NS QPO amplitudes\nthrough BL obscuration, which is relevant especially for vertical oscillations.\nWe also demonstrate that obscuration effects enable the observability of the\nKeplerian frequency in the case of discs that decay due to instabilities.",
        "We report the timing and spectral analyses upon of the type II X-ray bursts\nfrom the Rapid Burster (MXB 1730--335) observed by Insight-HXMT and Swift\/XRT.\nBy stacking the long-duration bursts, we find for the first time that the hard\nX-rays are lagging than the soft X-rays by 3 seconds. However, such a lag is\nnot visible for the short-duration bursts, probably because of the poor\nstatistics. For all bursts the energy spectrum is found to be non-thermal,\nthanks to the broad band coverage of Insight-HXMT. These findings put new\ninsights into the type-II bursts and require a temporally showing-up corona for\npossible interpretation.",
        "Accretion disks, swirling structures of matter spiraling into black holes,\nplay a pivotal role in our understanding of binary star systems and their\nintricate evolutionary processes. While current models often simplify these\ncomplex phenomena by neglecting the influence of powerful magnetic fields,\nparticularly within warped or distorted black hole geometries, this study\ndelves into the crucial impact of such fields. Focusing on a thin accretion\ndisk encircling a Schwarzschild black hole, we meticulously investigate how the\npresence of a quadrupole moment, an inherent distortion in the black hole's\nshape, affects its spectral characteristics. By analyzing key parameters like\ntotal pressure, magnetic pressure, temperature, height scale, surface density,\nand radiative flux (the energy emitted by the disk) we reveal significant\nalterations induced by incorporating both magnetic fields and a quadrupole\nmoment. Notably, our findings demonstrate that negative quadrupoles exert a\nmore pronounced influence on these disk properties, highlighting the intricate\ninterplay between these factors. This comprehensive study provides invaluable\ninsights into the dynamics of accretion disks surrounding distorted black holes\nwith magnetic fields, paving the way for a more accurate and nuanced\nunderstanding of these fascinating astrophysical systems.",
        "We apply a Gaussian process method to the extreme $\\gamma$-ray flares of 3C\n454.3 and 3C 279 to discover the variable patterns and then to investigate the\nphysical origins of the giant flares. The kernels of stochastically driven\ndamped simple harmonic oscillator (SHO), the damped random-walk (DRW), and\nMat$\\acute{\\rm e}$rn-3\/2 are respectively used to describe the adaptive-binning\n$\\gamma$-ray light curves of the two flares. Our findings show that both the\nextreme $\\gamma$-ray flares of 3C 454.3 and 3C 279 clearly prefer the SHO\nkernel in the over-damped mode and the Mat$\\acute{\\rm e}$rn-3\/2 kernel over the\nDRW kernel. The resulted SHO and Mat$\\acute{\\rm e}$rn-3\/2 power spectral\ndensities (PSDs) are the same for each object, with the index changing from -4\nat high frequencies to 0 at low frequencies. The patterns of the two flares are\nboth approaching the critical damping mode with the quality factor Q $\\approx$\n0.4 (i.e., the damping ratio $\\eta \\approx$ 1.25), but with slightly different\ndamping timescales. The characteristic timescale (corresponding to the broken\nfrequency in the PSD) for 3C 454.3 is 2-3 days and 3-5 days for 3C 279. The\nvariable patterns found here suggest that once the system responds to the\nenergy injection disturbance, the release of the energy in the system is\nfinished abruptly. The obtained timescale provides a constraint on the size of\nenergy dissipation region for each source.",
        "The physical origin of active galactic nucleus (AGN) variability remains\nunclear. Here we propose that the magnetic reconnection driven by the migration\nof satellite black holes (sBHs) in the AGN disc can be a new plausible\nmechanism for AGN short-term variability. During the sBH migration, the\nco-moving plasmas surrounding the sBH could influence the large-scale magnetic\nfield of the AGN disk and trigger the magnetic reconnections to contribute to\nAGN UV\/optical variability. Meanwhile, the plasma, which is accelerated by the\nmagnetic reconnection, will successfully escape from the disc at Alfven\nvelocity and cause a secondary magnetic reconnection in the corona. For a $\\sim\n10^{2}-10^{3}~{M_\\mathrm{\\odot}}$ sBH (including its binding gas) in the inner\nregions of the disc surrounding a supermassive black hole with $\\sim\n10^{7}~{M_\\mathrm{\\odot}}$, the reconnection process occurred in the space out\nof the disc can produce X-ray emission, which can last $\\sim 10^4-10^6~\\rm s$\nwith the luminosity $\\sim 10^{39}- 10^{43}~\\rm{erg ~s^{-1}}$.",
        "The Raychaudhuri equation (RE) governs the evolution of geodesic congruences\nin curved spacetime. Here, we extend the classical RE by incorporating\nnon-affine parametrization within the framework of k-essence scalar field\ndynamics. The non-affine parametrization introduces deviations from purely\ngeodesic congruences (motion), allowing investigation of non-gravitational\ninteractions and external forces. Using a DBI-type k-essence Lagrangian, we\nanalyze the behavior of non-geodesic flow curves in the background FLRW metric,\nelucidating their role in cosmic acceleration and structure formation. The\nemergent metric formalism is used to derive a modified RE, revealing new\ngeometric and dynamical features induced by the k-essence field. The\ncosmological implications of our model are studied by constraining key\nparameters using observational data from the PANTHEON+SHOES+BAO and Hubble\ndatasets. Our results suggest that non-affine parametrization, coupled with\nk-essence dynamics, can provide a viable explanation for late-time cosmic\nacceleration while addressing the Hubble tension. Further, we reinterpret the\nmodified RE as an anti-damped harmonic oscillator, revealing quantum-like\neffects in cosmic expansion. These results suggest a deep connection between\nscalar field dynamics and modified gravity, offering new perspectives on the\nnature of the universe's expansion history.",
        "Gradient-based attribution methods aim to explain decisions of deep learning\nmodels but so far lack identifiability guarantees. Here, we propose a method to\ngenerate attribution maps with identifiability guarantees by developing a\nregularized contrastive learning algorithm trained on time-series data plus a\nnew attribution method called Inverted Neuron Gradient (collectively named\nxCEBRA). We show theoretically that xCEBRA has favorable properties for\nidentifying the Jacobian matrix of the data generating process. Empirically, we\ndemonstrate robust approximation of zero vs. non-zero entries in the\nground-truth attribution map on synthetic datasets, and significant\nimprovements across previous attribution methods based on feature ablation,\nShapley values, and other gradient-based methods. Our work constitutes a first\nexample of identifiable inference of time-series attribution maps and opens\navenues to a better understanding of time-series data, such as for neural\ndynamics and decision-processes within neural networks.",
        "We present an experimental method for the characterization of the kinetic\nenergies of ions confined in a 22-pole radio frequency trap by inducing a small\npotential barrier using the surrounding ring electrodes, allowing the selective\nextraction of ions. Energy sampling experiments have been performed on buffer\ngas thermalized He$^+$ ions at trap temperatures between 10-180 K, resulting in\ndistinct extraction curves as a function of the potential barrier, and a\ndifferentiated behavior depending on the escape time from the trap. The\nexperiments are complemented by Monte Carlo simulations of the ion trajectories\ninside the calculated trap potential and allow us to investigate the properties\nof the sampling method, the role of ion motion coupling, and the impact of\nresidual buffer gas collisions on the observed results. The technique has also\nbeen successfully applied to identify energetic H$_3^+$ ions produced in an\nexothermic reaction inside the trap. Upon calibration, this method can provide\nrelative kinetic energy distributions or be used to filter the maximum desired\nkinetic energy of the ions inside the trap.",
        "Decoherence of quantum hardware is currently limiting its practical\napplications. At the same time, classical algorithms for simulating quantum\ncircuits have progressed substantially. Here, we demonstrate a hybrid framework\nthat integrates classical simulations with quantum hardware to improve the\ncomputation of an observable's expectation value by reducing the quantum\ncircuit depth. In this framework, a quantum circuit is partitioned into two\nsubcircuits: one that describes the backpropagated Heisenberg evolution of an\nobservable, executed on a classical computer, while the other is a\nSchr\\\"odinger evolution run on quantum processors. The overall effect is to\nreduce the depths of the circuits executed on quantum devices, trading this\nwith classical overhead and an increased number of circuit executions. We\ndemonstrate the effectiveness of this method on a Hamiltonian simulation\nproblem, achieving more accurate expectation value estimates compared to using\nquantum hardware alone.",
        "Sensing technology is an important aspect of information processing. Current\ndevelopment in artificial intelligence systems (especially those aimed at\nmedical and environmental applications) requires a lot of data on the chemical\ncomposition of biological fluids or environmental samples. These complex\nmatrices require advanced sensing devices, and photoelectrochemical ones seem\nto have potential to overcome at least some of the obstacles. Furthermore, the\ndevelopment of artificial intelligence (AI) technology for autonomous robotics\nrequires technology mimicking human senses, also those operating at the\nmolecular level, such as gustation and olfaction. Again, photoelectrochemical\nsensing can provide some suitable solutions. In this review, we introduce the\nidea of integration of photoelectrochemical sensors with some unconventional\ncomputing paradigm - reservoir computing. This approach should not only boost\nthe performance of the sensors itself, but also open new pathways through\nscience. Integration of sensing devices with computing systems will also\ncontribute to a better understanding (or at least mimicking) of the human\nsenses and neuromorphic sensory information processing. Although reservoir\nsystems can be considered magic \"black boxes\" and their operation is at the\nsame time simple and hard to comprehend, this combination is expected to open a\nnew era of effective information harvesting and processing systems.",
        "We study a general class of random block Schr\\\"odinger operators (RBSOs) in\ndimensions 1 and 2, which naturally extend the Anderson model by replacing the\nrandom potential with a random block potential. Specifically, we focus on two\nRBSOs -- the block Anderson and Wegner orbital models -- defined on the\n$d$-dimensional torus $(\\mathbb Z\/L\\mathbb Z)^d$. They take the form $H=V +\n\\lambda \\Psi$, where $V$ is a block potential with i.i.d. $W^d\\times W^d$\nGaussian diagonal blocks, $\\Psi$ describes interactions between neighboring\nblocks, and $\\lambda>0$ is a coupling parameter. We normalize the blocks of\n$\\Psi$ so that each block has a Hilbert-Schmidt norm of the same order as the\nblocks of $V$. Assuming $W\\ge L^\\delta$ for a small constant $\\delta>0$ and\n$\\lambda\\gg W^{-d\/2}$, we establish the following results. In dimension $d=2$,\nwe prove delocalization and quantum unique ergodicity for bulk eigenvectors,\nwhich, combined with the localization result from arXiv:1608.02922 under the\ncondition $\\lambda\\ll W^{-d\/2}$, rigorously establishes the Anderson\nlocalization-delocalization transition as $\\lambda$ crosses the critical\nthreshold $W^{-d\/2}$. In dimension $d=1$, we show that the localization length\nof bulk eigenvectors is at least of order $(W\\lambda)^2$, which is conjectured\nto be the correct scaling for the localization length.",
        "Slow flavor evolution (defined as driven by neutrino masses and not\nnecessarily ``slow'') is receiving fresh attention in the context of compact\nastrophysical environments. In Part~I of this series, we have studied the\nslow-mode dispersion relation following our recently developed analogy to\nplasma waves. The concept of resonance between flavor waves in the linear\nregime and propagating neutrinos is the defining feature of this approach. It\nis best motivated for weak instabilities, which probably is the most relevant\nregime in self-consistent astrophysical environments because these will try to\neliminate the cause of instability. We here go beyond the dispersion relation\nalone (which by definition applies to infinite media) and consider the group\nvelocities of unstable modes that determines whether the instability relaxes\nwithin the region where it first appears (absolute), or away from it\n(convective). We show that all weak instabilities are convective so that their\nfurther evolution is not local. Therefore, studying their consequences\nnumerically in small boxes from given initial conditions may not always be\nappropriate.",
        "We perform the thermodynamic analysis of a black hole (BH) immersed in a dark\nmatter halo (DMH), showing that the BH could not be in thermal equilibrium with\nthe DMH in any regions outside the event horizon. This means that the\nthermodynamic influence of the environment (DMH) is relatively small on the BH\nand it does not alter the nature of BH with negative heat capacity. The\nNewtonian ($1\/a_0$) approximation gives us a correct thermodynamic description\nfor the BH surrounded by DMH because the first law of thermodynamics and Smarr\nformula are satisfied. We use the Newtonian Helmholtz free energy to show that\nthere is no phase transition to other BH with positive heat capacity surrounded\nby a DMH. We investigate the shadow bound of favored region for the BH immersed\nin the DMH by introducing three critical impact parameters.",
        "Positive geometries are semialgebraic sets equipped with a canonical\ndifferential form whose residues mirror the boundary structure of the geometry.\nEvery full-dimensional projective polytope is a positive geometry. Motivated by\nthe canonical forms of polytopes, we construct a canonical form for any tope of\nan oriented matroid, inside the Orlik--Solomon algebra of the underlying\nmatroid. Using these canonical forms, we construct bases for the Orlik--Solomon\nalgebra of a matroid, and for the Aomoto cohomology. These bases of canonical\nforms are a foundational input in the theory of matroid amplitudes introduced\nby the second author.",
        "Wide coverage and high-precision rural household wealth data is an important\nsupport for the effective connection between the national macro rural\nrevitalization policy and micro rural entities, which helps to achieve precise\nallocation of national resources. However, due to the large number and wide\ndistribution of rural areas, wealth data is difficult to collect and scarce in\nquantity. Therefore, this article attempts to integrate \"sky\" remote sensing\nimages with \"ground\" village street view imageries to construct a fine-grained\n\"computable\" technical route for rural household wealth. With the intelligent\ninterpretation of rural houses as the core, the relevant wealth elements of\nimage data were extracted and identified, and regressed with the household\nwealth indicators of the benchmark questionnaire to form a high-precision\ntownship scale wealth prediction model (r=0.85); Furthermore, a national and\ntownship scale map of rural household wealth in China was promoted and drawn.\nBased on this, this article finds that there is a \"bimodal\" pattern in the\ndistribution of wealth among rural households in China, which is reflected in a\npolarization feature of \"high in the south and low in the north, and high in\nthe east and low in the west\" in space. This technological route may provide\nalternative solutions with wider spatial coverage and higher accuracy for\nhigh-cost manual surveys, promote the identification of shortcomings in rural\nconstruction, and promote the precise implementation of rural policies.",
        "This paper explores a full generalization of the classical corner-vector\nmethod for constructing weighted spherical designs, which we call the {\\it\ngeneralized corner-vector method}. First we establish a uniform upper bound for\nthe degree of designs obtained from the proposed method. Our proof is a hybrid\nargument that employs techniques in analysis and combinatorics, especially a\nfamous result by Xu(1998) on the interrelation between spherical designs and\nsimplical designs, and the cross-ratio comparison method for Hilbert identities\nintroduced by Nozaki and Sawa(2013). We extensively study conditions for the\nexistence of designs obtained from our method, and present many curious\nexamples of degree $7$ through $13$, some of which are, to our surprise,\ncharacterized in terms of integral lattices.",
        "The Center for Exascale Monte Carlo Neutron Transport is developing Monte\nCarlo \/ Dynamic Code (MC\/DC) as a portable Monte Carlo neutron transport\npackage for rapid numerical methods exploration on CPU- and GPU-based\nhigh-performance computers. In this paper, we describe MC\/DC's current\nevent-based GPU algorithm as well as the just-in-time (JIT) compilation scheme\nwe use to enable GPU operability on Nvidia and AMD GPUs from MC\/DC's Python\nsource. To analyze performance, we conduct runtime tests of the C5G7\nk-eigenvalue benchmark problem and a continuous-energy infinite pin cell on\nNvidia Tesla V100 GPU, AMD MI250X GPU, and the AMD MI300A APU and make\ncomparison to a dual-socket Intel Xeon Sapphire Rapid CPU node. We found that\nfor the multi-group C5G7 benchmark problem, we respectively see a 15$\\times$,\n0.7$\\times$, 12$\\times$ speedup on a V100, MI250X, and MI300A over 112 Intel\nXeon CPU cores. For the continuous-energy infinite pin-cell benchmark, we found\nspeedups of 5$\\times$, 3$\\times$, 4$\\times$ on a V100, MI250X, and MI300A,\nrespectively, over the same CPU node.",
        "We introduce BCAT, a PDE foundation model designed for autoregressive\nprediction of solutions to two dimensional fluid dynamics problems. Our\napproach uses a block causal transformer architecture to model next frame\npredictions, leveraging previous frames as contextual priors rather than\nrelying solely on sub-frames or pixel-based inputs commonly used in image\ngeneration methods. This block causal framework more effectively captures the\nspatial dependencies inherent in nonlinear spatiotemporal dynamics and physical\nphenomena. In an ablation study, next frame prediction demonstrated a 2.9x\naccuracy improvement over next token prediction. BCAT is trained on a diverse\nrange of fluid dynamics datasets, including incompressible and compressible\nNavier-Stokes equations across various geometries and parameter regimes, as\nwell as the shallow-water equations. The model's performance was evaluated on 6\ndistinct downstream prediction tasks and tested on about 8K trajectories to\nmeasure robustness on a variety of fluid dynamics simulations. BCAT achieved an\naverage relative error of 1.92% across all evaluation tasks, outperforming\nprior approaches on standard benchmarks.",
        "Full-waveform inversion (FWI) is an advanced technique for reconstructing\nhigh-resolution subsurface physical parameters by progressively minimizing the\ndiscrepancy between observed and predicted seismic data. However, conventional\nFWI encounters challenges in real data applications, primarily due to its\nconventional objective of direct measurements of the data misfit. Accurate\nestimation of the source wavelet is essential for effective data fitting,\nalongside the need for low-frequency data and a reasonable initial model to\nprevent cycle skipping. Additionally, wave equation solvers often struggle to\naccurately simulate the amplitude of observed data in real applications. To\naddress these challenges, we introduce a correlation-based source-independent\nobjective function for FWI that aims to mitigate source uncertainty and\namplitude dependency, which effectively enhances its practicality for real data\napplications. We develop a deep-learning framework constrained by this new\nobjective function with a velocity-distribution supported deep image prior,\nwhich reparameterizes velocity inversion into trainable parameters within an\nautoencoder, thereby reducing the nonlinearity in the conventional FWI's\nobjective function. We demonstrate the superiority of our proposed method using\nsynthetic data from benchmark velocity models and, more importantly, two real\ndatasets. These examples highlight its effectiveness and practicality even\nunder challenging conditions, such as missing low frequencies, a crude initial\nvelocity model, and an incorrect source wavelet.",
        "A surrogate model for particle-in-cell plasma simulations based on a graph\nneural network is presented. The graph is constructed in such a way as to\nenable the representation of electromagnetic fields on a fixed spatial grid.\nThe model is applied to simulate beams of electrons in one dimension over a\nwide range of temperatures, drift momenta and densities, and is shown to\nreproduce two-stream instabilities - a common and fundamental plasma\ninstability. Qualitatively, the characteristic phase-space mixing of\ncounterpropagating electron beams is observed. Quantitatively, the model's\nperformance is evaluated in terms of the accuracy of its predictions of number\ndensity distributions, the electric field, and their Fourier decompositions,\nparticularly the growth rate of the fastest-growing unstable mode, as well as\nparticle position, momentum distributions, energy conservation and run time.\nThe model achieves high accuracy with a time step longer than conventional\nsimulation by two orders of magnitude. This work demonstrates that complex\nplasma dynamics can be learned and shows promise for the development of fast\ndifferentiable simulators suitable for solving forward and inverse problems in\nplasma physics."
      ]
    }
  },
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b29",
    "start_title":"Study of Fermion pair production in e+e- collisions at 130-183 GeV",
    "start_abstract":"The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b23"
      ],
      "title":[
        "DARWIN Series: Domain Specific Large Language Models for Natural Science"
      ],
      "abstract":[
        "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Language Fusion for Parameter-Efficient Cross-lingual Transfer",
        "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus\n  Searches",
        "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks",
        "BAMBI: Developing Baby Language Models for Italian",
        "Unshackling Context Length: An Efficient Selective Attention Approach\n  through Query-Key Compression",
        "Group Preference Alignment: Customized LLM Response Generation from\n  In-Situ Conversations",
        "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification",
        "The Accuracy, Robustness, and Readability of LLM-Generated\n  Sustainability-Related Word Definitions",
        "Eager Updates For Overlapped Communication and Computation in DiLoCo",
        "What Limits LLM-based Human Simulation: LLMs or Our Design?",
        "Corporate Greenwashing Detection in Text -- a Survey",
        "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
        "Comparative Approaches to Sentiment Analysis Using Datasets in Major\n  European and Arabic Languages",
        "Reframing Dense Action Detection (RefDense): A Paradigm Shift in Problem\n  Solving & a Novel Optimization Strategy",
        "The Architecture and Evaluation of Bayesian Neural Networks",
        "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "The Strong Cosmic Censorship Conjecture",
        "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "Adaptive Camera Sensor for Vision Models",
        "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret\n  in Noise-Free Gaussian Process Bandits",
        "Elliptic curves in game theory",
        "StaICC: Standardized Evaluation for Classification Task in In-context\n  Learning",
        "End-to-end Training for Text-to-Image Synthesis using Dual-Text\n  Embeddings",
        "ExplainReduce: Summarising local explanations via proxies",
        "Polynomial invariants of $\\operatorname{GL}_{2}$: Conjugation over\n  finite fields",
        "Continuity of asymptotic entropy on wreath products",
        "Uncertainty Guarantees on Automated Precision Weeding using Conformal\n  Prediction"
      ],
      "abstract":[
        "Limited availability of multilingual text corpora for training language\nmodels often leads to poor performance on downstream tasks due to undertrained\nrepresentation spaces for languages other than English. This\n'under-representation' has motivated recent cross-lingual transfer methods to\nleverage the English representation space by e.g. mixing English and\n'non-English' tokens at the input level or extending model parameters to\naccommodate new languages. However, these approaches often come at the cost of\nincreased computational complexity. We propose Fusion forLanguage\nRepresentations (FLARE) in adapters, a novel method that enhances\nrepresentation quality and downstream performance for languages other than\nEnglish while maintaining parameter efficiency. FLARE integrates source and\ntarget language representations within low-rank (LoRA) adapters using\nlightweight linear transformations, maintaining parameter efficiency while\nimproving transfer performance. A series of experiments across representative\ncross-lingual natural language understanding tasks, including natural language\ninference, question-answering and sentiment analysis, demonstrate FLARE's\neffectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1\nand 2.2% for Gemma~2 compared to standard LoRA fine-tuning on\nquestion-answering tasks, as measured by the exact match metric.",
        "Researchers and practitioners in natural language processing and\ncomputational linguistics frequently observe and analyze the real language\nusage in large-scale corpora. For that purpose, they often employ off-the-shelf\npattern-matching tools, such as grep, and keyword-in-context concordancers,\nwhich is widely used in corpus linguistics for gathering examples. Nonetheless,\nthese existing techniques rely on surface-level string matching, and thus they\nsuffer from the major limitation of not being able to handle orthographic\nvariations and paraphrasing -- notable and common phenomena in any natural\nlanguage. In addition, existing continuous approaches such as dense vector\nsearch tend to be overly coarse, often retrieving texts that are unrelated but\nshare similar topics. Given these challenges, we propose a novel algorithm that\nachieves \\emph{soft} (or semantic) yet efficient pattern matching by relaxing a\nsurface-level matching with word embeddings. Our algorithm is highly scalable\nwith respect to the size of the corpus text utilizing inverted indexes. We have\nprepared an efficient implementation, and we provide an accessible web tool.\nOur experiments demonstrate that the proposed method (i) can execute searches\non billion-scale corpora in less than a second, which is comparable in speed to\nsurface-level string matching and dense vector search; (ii) can extract harmful\ninstances that semantically match queries from a large set of English and\nJapanese Wikipedia articles; and (iii) can be effectively applied to\ncorpus-linguistic analyses of Latin, a language with highly diverse\ninflections.",
        "As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark.",
        "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance.",
        "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks.",
        "LLMs often fail to meet the specialized needs of distinct user groups due to\ntheir one-size-fits-all training paradigm \\cite{lucy-etal-2024-one} and there\nis limited research on what personalization aspects each group expect. To\naddress these limitations, we propose a group-aware personalization framework,\nGroup Preference Alignment (GPA), that identifies context-specific variations\nin conversational preferences across user groups and then steers LLMs to\naddress those preferences. Our approach consists of two steps: (1) Group-Aware\nPreference Extraction, where maximally divergent user-group preferences are\nextracted from real-world conversation logs and distilled into interpretable\nrubrics, and (2) Tailored Response Generation, which leverages these rubrics\nthrough two methods: a) Context-Tuned Inference (GAP-CT), that dynamically\nadjusts responses via context-dependent prompt instructions, and b)\nRubric-Finetuning Inference (GPA-FT), which uses the rubrics to generate\ncontrastive synthetic data for personalization of group-specific models via\nalignment. Experiments demonstrate that our framework significantly improves\nalignment of the output with respect to user preferences and outperforms\nbaseline methods, while maintaining robust performance on standard benchmarks.",
        "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.",
        "A common language with standardized definitions is crucial for effective\nclimate discussions. However, concerns exist about LLMs misrepresenting climate\nterms. We compared 300 official IPCC glossary definitions with those generated\nby GPT-4o-mini, Llama3.1 8B, and Mistral 7B, analyzing adherence, robustness,\nand readability using SBERT sentence embeddings. The LLMs scored an average\nadherence of $0.57-0.59 \\pm 0.15$, and their definitions proved harder to read\nthan the originals. Model-generated definitions vary mainly among words with\nmultiple or ambiguous definitions, showing the potential to highlight terms\nthat need standardization. The results show how LLMs could support\nenvironmental discourse while emphasizing the need to align model outputs with\nestablished terminology for clarity and consistency.",
        "Distributed optimization methods such as DiLoCo have been shown to be\neffective in training very large models across multiple distributed workers,\nsuch as datacenters. These methods split updates into two parts: an inner\noptimization phase, where the workers independently execute multiple\noptimization steps on their own local data, and an outer optimization step,\nwhere the inner updates are synchronized. While such approaches require orders\nof magnitude less communication than standard data-parallel training, in\nsettings where the workers are datacenters, even the limited communication\nrequirements of these approaches can still cause significant slow downs due to\nthe blocking necessary at each outer optimization step. In this paper, we\ninvestigate techniques to mitigate this issue by overlapping communication with\ncomputation in a manner that allows the outer optimization step to fully\noverlap with the inner optimization phase. We show that a particular variant,\ndubbed eager updates, provides competitive performance with standard DiLoCo in\nsettings with low bandwidth between workers.",
        "We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https:\/\/github.com\/Persdre\/llm-human-simulation}",
        "Greenwashing is an effort to mislead the public about the environmental\nimpact of an entity, such as a state or company. We provide a comprehensive\nsurvey of the scientific literature addressing natural language processing\nmethods to identify potentially misleading climate-related corporate\ncommunications, indicative of greenwashing. We break the detection of\ngreenwashing into intermediate tasks, and review the state-of-the-art\napproaches for each of them. We discuss datasets, methods, and results, as well\nas limitations and open challenges. We also provide an overview of how far the\nfield has come as a whole, and point out future research directions.",
        "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nsuffer from catastrophic forgetting, limiting their applicability to\nstate-of-the-art LLMs that already perform well in zero-shot settings with a\nproper instruction. To address this challenge, we propose a novel approach for\ncontinuous-space reasoning that does not require modifying the underlying LLM.\nSpecifically, we employ a lightweight assistant model to generate\ninstance-specific soft thought tokens speculatively as the initial chain of\nthoughts, which are then mapped into the LLM's representation space via a\nprojection module. Experimental results on five reasoning benchmarks\ndemonstrate that our method enhances LLM reasoning performance through\nsupervised, parameter-efficient fine-tuning.",
        "This study explores transformer-based models such as BERT, mBERT, and XLM-R\nfor multi-lingual sentiment analysis across diverse linguistic structures. Key\ncontributions include the identification of XLM-R superior adaptability in\nmorphologically complex languages, achieving accuracy levels above 88%. The\nwork highlights fine-tuning strategies and emphasizes their significance for\nimproving sentiment classification in underrepresented languages.",
        "Dense action detection involves detecting multiple co-occurring actions while\naction classes are often ambiguous and represent overlapping concepts. We argue\nthat handling the dual challenge of temporal and class overlaps is too complex\nto effectively be tackled by a single network. To address this, we propose to\ndecompose the task of detecting dense ambiguous actions into detecting dense,\nunambiguous sub-concepts that form the action classes (i.e., action entities\nand action motions), and assigning these sub-tasks to distinct sub-networks. By\nisolating these unambiguous concepts, the sub-networks can focus exclusively on\nresolving a single challenge, dense temporal overlaps. Furthermore,\nsimultaneous actions in a video often exhibit interrelationships, and\nexploiting these relationships can improve the method performance. However,\ncurrent dense action detection networks fail to effectively learn these\nrelationships due to their reliance on binary cross-entropy optimization, which\ntreats each class independently. To address this limitation, we propose\nproviding explicit supervision on co-occurring concepts during network\noptimization through a novel language-guided contrastive learning loss. Our\nextensive experiments demonstrate the superiority of our approach over\nstate-of-the-art methods, achieving substantial improvements of 3.8% and 1.7%\non average across all metrics on the challenging benchmark datasets, Charades\nand MultiTHUMOS.",
        "As modern neural networks get more complex, specifying a model with high\npredictive performance and sound uncertainty quantification becomes a more\nchallenging task. Despite some promising theoretical results on the true\nposterior predictive distribution of Bayesian neural networks, the properties\nof even the most commonly used posterior approximations are often questioned.\nComputational burdens and intractable posteriors expose miscalibrated Bayesian\nneural networks to poor accuracy and unreliable uncertainty estimates.\nApproximate Bayesian inference aims to replace unknown and intractable\nposterior distributions with some simpler but feasible distributions. The\ndimensions of modern deep models coupled with the lack of identifiability make\nMarkov chain Monte Carlo tremendously expensive and unable to fully explore the\nmultimodal posterior. On the other hand, variational inference benefits from\nimproved computational complexity but lacks the asymptotical guarantees of\nsampling-based inference and tends to concentrate around a single mode. The\nperformance of both approaches heavily depends on architectural choices; this\npaper aims to shed some light on this, by considering the computational costs,\naccuracy and uncertainty quantification in different scenarios including large\nwidth and out-of-sample data. To improve posterior exploration, different model\naveraging and ensembling techniques are studied, along with their benefits on\npredictive performance. In our experiments, variational inference overall\nprovided better uncertainty quantification than Markov chain Monte Carlo;\nfurther, stacking and ensembles of variational approximations provided\ncomparable to Markov chain Monte Carlo accuracy at a much-reduced cost.",
        "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "In the wake of major breakthroughs in General Relativity during the 1960s,\nRoger Penrose introduced Strong Cosmic Censorship, a profound conjecture\nregarding the deterministic nature of the theory. Penrose's proposal has since\nopened far-reaching new mathematical avenues, revealing connections to\nfundamental questions about black holes and the nature of gravitational\nsingularities. We review recent advances arising from modern techniques in the\ntheory of partial differential equations as applied to Strong Cosmic\nCensorship, maintaining a focus on the context of gravitational collapse that\ngave birth to the conjecture.",
        "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https:\/\/github.com\/Robin-WZQ\/IBA.",
        "Domain shift remains a persistent challenge in deep-learning-based computer\nvision, often requiring extensive model modifications or large labeled datasets\nto address. Inspired by human visual perception, which adjusts input quality\nthrough corrective lenses rather than over-training the brain, we propose Lens,\na novel camera sensor control method that enhances model performance by\ncapturing high-quality images from the model's perspective rather than relying\non traditional human-centric sensor control. Lens is lightweight and adapts\nsensor parameters to specific models and scenes in real-time. At its core, Lens\nutilizes VisiT, a training-free, model-specific quality indicator that\nevaluates individual unlabeled samples at test time using confidence scores\nwithout additional adaptation costs. To validate Lens, we introduce ImageNet-ES\nDiverse, a new benchmark dataset capturing natural perturbations from varying\nsensor and lighting conditions. Extensive experiments on both ImageNet-ES and\nour new ImageNet-ES Diverse show that Lens significantly improves model\naccuracy across various baseline schemes for sensor control and model\nmodification while maintaining low latency in image captures. Lens effectively\ncompensates for large model size differences and integrates synergistically\nwith model improvement techniques. Our code and dataset are available at\ngithub.com\/Edw2n\/Lens.git.",
        "We study the noise-free Gaussian Process (GP) bandits problem, in which the\nlearner seeks to minimize regret through noise-free observations of the\nblack-box objective function lying on the known reproducing kernel Hilbert\nspace (RKHS). Gaussian process upper confidence bound (GP-UCB) is the\nwell-known GP-bandits algorithm whose query points are adaptively chosen based\non the GP-based upper confidence bound score. Although several existing works\nhave reported the practical success of GP-UCB, the current theoretical results\nindicate its suboptimal performance. However, GP-UCB tends to perform well\nempirically compared with other nearly optimal noise-free algorithms that rely\non a non-adaptive sampling scheme of query points. This paper resolves this gap\nbetween theoretical and empirical performance by showing the nearly optimal\nregret upper bound of noise-free GP-UCB. Specifically, our analysis shows the\nfirst constant cumulative regret in the noise-free settings for the squared\nexponential kernel and Mat\\'ern kernel with some degree of smoothness.",
        "We investigate Spohn curves, the algebro-geometric models of dependency\nequilibria for $2 \\times 2$ normal-form games. These curves arise as the\nintersection of two quadrics in $\\mathbb{P}^3$ and are generically elliptic\ncurves. We compute and verify the $j$-invariant for elliptic curves arising as\nthe intersection of quadrics in $\\mathbb P^3$ using two different\nimplementations: by computing the Aronhold invariants and the discriminant (in\nMathematica) and using algorithms for the arithmetic of elliptic curves\n(in-built in Pari\/GP). We define an equivalency of generic $2\\times 2$ games\nbased on the $j$-invariant of the Spohn curve. Additionally, we examine the\nreduction of Spohn curves to plane curves and analyze conditions under which\nthey are reducible. Notably, we prove that the real points are dense on the\nSpohn curve in all cases. Our examples and computations are further supported\nby Macaulay2.",
        "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing.",
        "Text-to-Image (T2I) synthesis is a challenging task that requires modeling\ncomplex interactions between two modalities ( i.e., text and image). A common\nframework adopted in recent state-of-the-art approaches to achieving such\nmultimodal interactions is to bootstrap the learning process with pre-trained\nimage-aligned text embeddings trained using contrastive loss. Furthermore,\nthese embeddings are typically trained generically and reused across various\nsynthesis models. In contrast, we explore an approach to learning text\nembeddings specifically tailored to the T2I synthesis network, trained in an\nend-to-end fashion. Further, we combine generative and contrastive training and\nuse two embeddings, one optimized to enhance the photo-realism of the generated\nimages, and the other seeking to capture text-to-image alignment. A\ncomprehensive set of experiments on three text-to-image benchmark datasets\n(Oxford-102, Caltech-UCSD, and MS-COCO) reveal that having two separate\nembeddings gives better results than using a shared one and that such an\napproach performs favourably in comparison with methods that use text\nrepresentations from a pre-trained text encoder trained using a discriminative\napproach. Finally, we demonstrate that such learned embeddings can be used in\nother contexts as well, such as text-to-image manipulation.",
        "Most commonly used non-linear machine learning methods are closed-box models,\nuninterpretable to humans. The field of explainable artificial intelligence\n(XAI) aims to develop tools to examine the inner workings of these closed\nboxes. An often-used model-agnostic approach to XAI involves using simple\nmodels as local approximations to produce so-called local explanations;\nexamples of this approach include LIME, SHAP, and SLISEMAP. This paper shows\nhow a large set of local explanations can be reduced to a small \"proxy set\" of\nsimple models, which can act as a generative global explanation. This reduction\nprocedure, ExplainReduce, can be formulated as an optimisation problem and\napproximated efficiently using greedy heuristics.",
        "Consider the conjugation action of $\\operatorname{GL}_{2}(K)$ on the\npolynomial ring $K[X_{2 \\times 2}]$. When $K$ is an infinite field, the ring of\ninvariants is a polynomial ring generated by the trace and the determinant. We\ndescribe the ring of invariants when $K$ is a finite field, and show that it is\na hypersurface.",
        "We prove the continuity of asymptotic entropy as a function of the step\ndistribution for non-degenerate probability measures with finite entropy on\nwreath products $ A \\wr B = \\bigoplus_B A \\rtimes B $, where $A$ is any\ncountable group and $B$ is a countable hyper-FC-central group that contains a\nfinitely generated subgroup of at least cubic growth. As one step in proving\nthe above, we show that on any countable group $G$ the probability that the\n$\\mu$-random walk on $G$ never returns to the identity is continuous in $\\mu$,\nfor measures $\\mu$ such that the semigroup generated by the support of $\\mu$\ncontains a finitely generated subgroup of at least cubic growth. Finally, we\nshow that among random walks on a group $G$ that admit a separable completely\nmetrizable space $X$ as a model for their Poisson boundary, the weak continuity\nof the associated harmonic measures on $X$ implies the continuity of the\nasymptotic entropy. This result recovers the continuity of asymptotic entropy\non known cases, such as Gromov hyperbolic groups and acylindrically hyperbolic\ngroups, and extends it to new classes of groups, including linear groups and\ngroups acting on $\\mathrm{CAT}(0)$ spaces.",
        "Precision agriculture in general, and precision weeding in particular, have\ngreatly benefited from the major advancements in deep learning and computer\nvision. A large variety of commercial robotic solutions are already available\nand deployed. However, the adoption by farmers of such solutions is still low\nfor many reasons, an important one being the lack of trust in these systems.\nThis is in great part due to the opaqueness and complexity of deep neural\nnetworks and the manufacturers' inability to provide valid guarantees on their\nperformance. Conformal prediction, a well-established methodology in the\nmachine learning community, is an efficient and reliable strategy for providing\ntrustworthy guarantees on the predictions of any black-box model under very\nminimal constraints. Bridging the gap between the safe machine learning and\nprecision agriculture communities, this article showcases conformal prediction\nin action on the task of precision weeding through deep learning-based image\nclassification. After a detailed presentation of the conformal prediction\nmethodology and the development of a precision spraying pipeline based on a\n''conformalized'' neural network and well-defined spraying decision rules, the\narticle evaluates this pipeline on two real-world scenarios: one under\nin-distribution conditions, the other reflecting a near out-of-distribution\nsetting. The results show that we are able to provide formal, i.e. certifiable,\nguarantees on spraying at least 90% of the weeds."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy",
    "start_abstract":"ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications.",
    "start_categories":[
      "astro-ph.CO"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "ImageNet: A large-scale hierarchical image database"
      ],
      "abstract":[
        "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "3D Trajectory Reconstruction of Moving Points Based on a Monocular\n  Camera",
        "SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation",
        "Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void\n  Filling",
        "Revisiting Gradient-based Uncertainty for Monocular Depth Estimation",
        "X-Field: A Physically Grounded Representation for 3D X-ray\n  Reconstruction",
        "FruitPAL: An IoT-Enabled Framework for Automatic Monitoring of Fruit\n  Consumption in Smart Healthcare",
        "Segment Any-Quality Images with Generative Latent Space Enhancement",
        "Counting Fish with Temporal Representations of Sonar Video",
        "Game State and Spatio-temporal Action Detection in Soccer using Graph\n  Neural Networks and 3D Convolutional Networks",
        "FP4DiT: Towards Effective Floating Point Quantization for Diffusion\n  Transformers",
        "MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior\n  Analysis",
        "An Adaptive Underwater Image Enhancement Framework via Multi-Domain\n  Fusion and Color Compensation",
        "Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation",
        "Transformers trained on proteins can learn to attend to Euclidean\n  distance",
        "ProgCo: Program Helps Self-Correction of Large Language Models",
        "Unveiling and Causalizing CoT: A Causal Pespective",
        "Visual Attention Exploration in Vision-Based Mamba Models",
        "Kernel EDMD for data-driven nonlinear Koopman MPC with stability\n  guarantees",
        "\"Once Upon a Time...\" Literary Narrative Connectedness Progresses with\n  Grade Level: Potential Impact on Reading Fluency and Literacy Skills",
        "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language\n  Embedding Registration",
        "EILID: Execution Integrity for Low-end IoT Devices",
        "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles",
        "Targetless Intrinsics and Extrinsic Calibration of Multiple LiDARs and\n  Cameras with IMU using Continuous-Time Estimation",
        "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery",
        "On singular supports in mixed characteristic",
        "Addressing Out-of-Label Hazard Detection in Dashcam Videos: Insights\n  from the COOOL Challenge",
        "Algebraic families of higher dimensional $\\mathbb{A}^{1}$-contractible\n  affine varieties non-isomorphic to affine spaces",
        "On the Semantic Security of NTRU -- with a gentle introduction to\n  cryptography"
      ],
      "abstract":[
        "The motion measurement of point targets constitutes a fundamental problem in\nphotogrammetry, with extensive applications across various engineering domains.\nReconstructing a point's 3D motion just from the images captured by only a\nmonocular camera is unfeasible without prior assumptions. Under limited\nobservation conditions such as insufficient observations, long distance, and\nhigh observation error of platform, the least squares estimation faces the\nissue of ill-conditioning. This paper presents an algorithm for reconstructing\n3D trajectories of moving points using a monocular camera. The motion of the\npoints is represented through temporal polynomials. Ridge estimation is\nintroduced to mitigate the issues of ill-conditioning caused by limited\nobservation conditions. Then, an automatic algorithm for determining the order\nof the temporal polynomials is proposed. Furthermore, the definition of\nreconstructability for temporal polynomials is proposed to describe the\nreconstruction accuracy quantitatively. The simulated and real-world\nexperimental results demonstrate the feasibility, accuracy, and efficiency of\nthe proposed method.",
        "Recent advancements in large vision-language models have enabled highly\nexpressive and diverse vector sketch generation. However, state-of-the-art\nmethods rely on a time-consuming optimization process involving repeated\nfeedback from a pretrained model to determine stroke placement. Consequently,\ndespite producing impressive sketches, these methods are limited in practical\napplications. In this work, we introduce SwiftSketch, a diffusion model for\nimage-conditioned vector sketch generation that can produce high-quality\nsketches in less than a second. SwiftSketch operates by progressively denoising\nstroke control points sampled from a Gaussian distribution. Its\ntransformer-decoder architecture is designed to effectively handle the discrete\nnature of vector representation and capture the inherent global dependencies\nbetween strokes. To train SwiftSketch, we construct a synthetic dataset of\nimage-sketch pairs, addressing the limitations of existing sketch datasets,\nwhich are often created by non-artists and lack professional quality. For\ngenerating these synthetic sketches, we introduce ControlSketch, a method that\nenhances SDS-based techniques by incorporating precise spatial control through\na depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across\ndiverse concepts, efficiently producing sketches that combine high fidelity\nwith a natural and visually appealing style.",
        "Digital Surface Models (DSMs) are essential for accurately representing\nEarth's topography in geospatial analyses. DSMs capture detailed elevations of\nnatural and manmade features, crucial for applications like urban planning,\nvegetation studies, and 3D reconstruction. However, DSMs derived from stereo\nsatellite imagery often contain voids or missing data due to occlusions,\nshadows, and lowsignal areas. Previous studies have primarily focused on void\nfilling for digital elevation models (DEMs) and Digital Terrain Models (DTMs),\nemploying methods such as inverse distance weighting (IDW), kriging, and spline\ninterpolation. While effective for simpler terrains, these approaches often\nfail to handle the intricate structures present in DSMs. To overcome these\nlimitations, we introduce Dfilled, a guided DSM void filling method that\nleverages optical remote sensing images through edge-enhancing diffusion.\nDfilled repurposes deep anisotropic diffusion models, which originally designed\nfor super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin\nnoise to create inpainting masks that mimic natural void patterns in DSMs.\nExperimental evaluations demonstrate that Dfilled surpasses traditional\ninterpolation methods and deep learning approaches in DSM void filling tasks.\nBoth quantitative and qualitative assessments highlight the method's ability to\nmanage complex features and deliver accurate, visually coherent results.",
        "Monocular depth estimation, similar to other image-based tasks, is prone to\nerroneous predictions due to ambiguities in the image, for example, caused by\ndynamic objects or shadows. For this reason, pixel-wise uncertainty assessment\nis required for safety-critical applications to highlight the areas where the\nprediction is unreliable. We address this in a post hoc manner and introduce\ngradient-based uncertainty estimation for already trained depth estimation\nmodels. To extract gradients without depending on the ground truth depth, we\nintroduce an auxiliary loss function based on the consistency of the predicted\ndepth and a reference depth. The reference depth, which acts as pseudo ground\ntruth, is in fact generated using a simple image or feature augmentation,\nmaking our approach simple and effective. To obtain the final uncertainty\nscore, the derivatives w.r.t. the feature maps from single or multiple layers\nare calculated using back-propagation. We demonstrate that our gradient-based\napproach is effective in determining the uncertainty without re-training using\nthe two standard depth estimation benchmarks KITTI and NYU. In particular, for\nmodels trained with monocular sequences and therefore most prone to\nuncertainty, our method outperforms related approaches. In addition, we\npublicly provide our code and models: https:\/\/github.com\/jhornauer\/GrUMoDepth",
        "X-ray imaging is indispensable in medical diagnostics, yet its use is tightly\nregulated due to potential health risks. To mitigate radiation exposure, recent\nresearch focuses on generating novel views from sparse inputs and\nreconstructing Computed Tomography (CT) volumes, borrowing representations from\nthe 3D reconstruction area. However, these representations originally target\nvisible light imaging that emphasizes reflection and scattering effects, while\nneglecting penetration and attenuation properties of X-ray imaging. In this\npaper, we introduce X-Field, the first 3D representation specifically designed\nfor X-ray imaging, rooted in the energy absorption rates across different\nmaterials. To accurately model diverse materials within internal structures, we\nemploy 3D ellipsoids with distinct attenuation coefficients. To estimate each\nmaterial's energy absorption of X-rays, we devise an efficient path\npartitioning algorithm accounting for complex ellipsoid intersections. We\nfurther propose hybrid progressive initialization to refine the geometric\naccuracy of X-Filed and incorporate material-based optimization to enhance\nmodel fitting along material boundaries. Experiments show that X-Field achieves\nsuperior visual fidelity on both real-world human organ and synthetic object\ndatasets, outperforming state-of-the-art methods in X-ray Novel View Synthesis\nand CT Reconstruction.",
        "Fruits are rich sources of essential vitamins and nutrients that are vital\nfor human health. This study introduces two fully automated devices, FruitPAL\nand its updated version, FruitPAL 2.0, which aim to promote safe fruit\nconsumption while reducing health risks. Both devices leverage a high-quality\ndataset of fifteen fruit types and use advanced models- YOLOv8 and YOLOv5 V6.0-\nto enhance detection accuracy. The original FruitPAL device can identify\nvarious fruit types and notify caregivers if an allergic reaction is detected,\nthanks to YOLOv8's improved accuracy and rapid response time. Notifications are\ntransmitted via the cloud to mobile devices, ensuring real-time updates and\nimmediate accessibility. FruitPAL 2.0 builds upon this by not only detecting\nfruit but also estimating its nutritional value, thereby encouraging healthy\nconsumption. Trained on the YOLOv5 V6.0 model, FruitPAL 2.0 analyzes fruit\nintake to provide users with valuable dietary insights. This study aims to\npromote fruit consumption by helping individuals make informed choices,\nbalancing health benefits with allergy awareness. By alerting users to\npotential allergens while encouraging the consumption of nutrient-rich fruits,\nthese devices support both health maintenance and dietary awareness.",
        "Despite their success, Segment Anything Models (SAMs) experience significant\nperformance drops on severely degraded, low-quality images, limiting their\neffectiveness in real-world scenarios. To address this, we propose GleSAM,\nwhich utilizes Generative Latent space Enhancement to boost robustness on\nlow-quality images, thus enabling generalization across various image\nqualities. Specifically, we adapt the concept of latent diffusion to SAM-based\nsegmentation frameworks and perform the generative diffusion process in the\nlatent space of SAM to reconstruct high-quality representation, thereby\nimproving segmentation. Additionally, we introduce two techniques to improve\ncompatibility between the pre-trained diffusion model and the segmentation\nframework. Our method can be applied to pre-trained SAM and SAM2 with only\nminimal additional learnable parameters, allowing for efficient optimization.\nWe also construct the LQSeg dataset with a greater diversity of degradation\ntypes and levels for training and evaluating the model. Extensive experiments\ndemonstrate that GleSAM significantly improves segmentation robustness on\ncomplex degradations while maintaining generalization to clear images.\nFurthermore, GleSAM also performs well on unseen degradations, underscoring the\nversatility of our approach and dataset.",
        "Accurate estimates of salmon escapement - the number of fish migrating\nupstream to spawn - are key data for conservation and fishery management.\nExisting methods for salmon counting using high-resolution imaging sonar\nhardware are non-invasive and compatible with computer vision processing. Prior\nwork in this area has utilized object detection and tracking based methods for\nautomated salmon counting. However, these techniques remain inaccessible to\nmany sonar deployment sites due to limited compute and connectivity in the\nfield. We propose an alternative lightweight computer vision method for fish\ncounting based on analyzing echograms - temporal representations that compress\nseveral hundred frames of imaging sonar video into a single image. We predict\nupstream and downstream counts within 200-frame time windows directly from\nechograms using a ResNet-18 model, and propose a set of domain-specific image\naugmentations and a weakly-supervised training protocol to further improve\nresults. We achieve a count error of 23% on representative data from the Kenai\nRiver in Alaska, demonstrating the feasibility of our approach.",
        "Soccer analytics rely on two data sources: the player positions on the pitch\nand the sequences of events they perform. With around 2000 ball events per\ngame, their precise and exhaustive annotation based on a monocular video stream\nremains a tedious and costly manual task. While state-of-the-art\nspatio-temporal action detection methods show promise for automating this task,\nthey lack contextual understanding of the game. Assuming professional players'\nbehaviors are interdependent, we hypothesize that incorporating surrounding\nplayers' information such as positions, velocity and team membership can\nenhance purely visual predictions. We propose a spatio-temporal action\ndetection approach that combines visual and game state information via Graph\nNeural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating\nimproved metrics through game state integration.",
        "Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP.",
        "Analyzing animal behavior is crucial in advancing neuroscience, yet\nquantifying and deciphering its intricate dynamics remains a significant\nchallenge. Traditional machine vision approaches, despite their ability to\ndetect spontaneous behaviors, fall short due to limited interpretability and\nreliance on manual labeling, which restricts the exploration of the full\nbehavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM)\nthat integrates visual cues with natural language to revolutionize mouse\nbehavior analysis. Built upon our first-of-its-kind dataset - incorporating\npose dynamics and open-vocabulary behavioral annotations across over 42 million\nframes of diverse psychiatric conditions - MouseGPT provides a novel,\ncontext-rich method for comprehensive behavior interpretation. Our holistic\nanalysis framework enables detailed behavior profiling, clustering, and novel\nbehavior discovery, offering deep insights without the need for labor -\nintensive manual annotation. Evaluations reveal that MouseGPT surpasses\nexisting models in precision, adaptability, and descriptive richness,\npositioning it as a transformative tool for ethology and for unraveling complex\nbehavioral dynamics in animal models.",
        "Underwater optical imaging is severely degraded by light absorption,\nscattering, and color distortion, hindering visibility and accurate image\nanalysis. This paper presents an adaptive enhancement framework integrating\nillumination compensation, multi-domain filtering, and dynamic color\ncorrection. A hybrid illumination compensation strategy combining CLAHE, Gamma\ncorrection, and Retinex enhances visibility. A two-stage filtering process,\nincluding spatial-domain (Gaussian, Bilateral, Guided) and frequency-domain\n(Fourier, Wavelet) methods, effectively reduces noise while preserving details.\nTo correct color distortion, an adaptive color compensation (ACC) model\nestimates spectral attenuation and water type to combine RCP, DCP, and MUDCP\ndynamically. Finally, a perceptually guided color balance mechanism ensures\nnatural color restoration. Experimental results on benchmark datasets\ndemonstrate superior performance over state-of-the-art methods in contrast\nenhancement, color correction, and structural preservation, making the\nframework robust for underwater imaging applications.",
        "Recent advances in text-to-image diffusion models have been driven by the\nincreasing availability of paired 2D data. However, the development of 3D\ndiffusion models has been hindered by the scarcity of high-quality 3D data,\nresulting in less competitive performance compared to their 2D counterparts. To\naddress this challenge, we propose repurposing pre-trained 2D diffusion models\nfor 3D object generation. We introduce Gaussian Atlas, a novel representation\nthat utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models\nto generate 3D Gaussians. Our approach demonstrates successful transfer\nlearning from a pre-trained 2D diffusion model to a 2D manifold flattened from\n3D structures. To support model training, we compile GaussianVerse, a\nlarge-scale dataset comprising 205K high-quality 3D Gaussian fittings of\nvarious 3D objects. Our experimental results show that text-to-image diffusion\nmodels can be effectively adapted for 3D content generation, bridging the gap\nbetween 2D and 3D modeling.",
        "While conventional Transformers generally operate on sequence data, they can\nbe used in conjunction with structure models, typically SE(3)-invariant or\nequivariant graph neural networks (GNNs), for 3D applications such as protein\nstructure modelling. These hybrids typically involve either (1)\npreprocessing\/tokenizing structural features as input for Transformers or (2)\ntaking Transformer embeddings and processing them within a structural\nrepresentation. However, there is evidence that Transformers can learn to\nprocess structural information on their own, such as the AlphaFold3 structural\ndiffusion model. In this work we show that Transformers can function\nindependently as structure models when passed linear embeddings of coordinates.\nWe first provide a theoretical explanation for how Transformers can learn to\nfilter attention as a 3D Gaussian with learned variance. We then validate this\ntheory using both simulated 3D points and in the context of masked token\nprediction for proteins. Finally, we show that pre-training protein Transformer\nencoders with structure improves performance on a downstream task, yielding\nbetter performance than custom structural models. Together, this work provides\na basis for using standard Transformers as hybrid structure-language models.",
        "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
        "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved.",
        "State space models (SSMs) have emerged as an efficient alternative to\ntransformer-based models, offering linear complexity that scales better than\ntransformers. One of the latest advances in SSMs, Mamba, introduces a selective\nscan mechanism that assigns trainable weights to input tokens, effectively\nmimicking the attention mechanism. Mamba has also been successfully extended to\nthe vision domain by decomposing 2D images into smaller patches and arranging\nthem as 1D sequences. However, it remains unclear how these patches interact\nwith (or attend to) each other in relation to their original 2D spatial\nlocation. Additionally, the order used to arrange the patches into a sequence\nalso significantly impacts their attention distribution. To better understand\nthe attention between patches and explore the attention patterns, we introduce\na visual analytics tool specifically designed for vision-based Mamba models.\nThis tool enables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout a Mamba model.\nUsing the tool, we also investigate the impact of different patch-ordering\nstrategies on the learned attention, offering further insights into the model's\nbehavior.",
        "Extended dynamic mode decomposition (EDMD) is a popular data-driven method to\npredict the action of the Koopman operator, i.e., the evolution of an\nobservable function along the flow of a dynamical system. In this paper, we\nleverage a recently-introduced kernel EDMD method for control systems for\ndata-driven model predictive control. Building upon pointwise error bounds\nproportional in the state, we rigorously show practical asymptotic stability of\nthe origin w.r.t. the MPC closed loop without stabilizing terminal conditions.\nThe key novelty is that we avoid restrictive invariance conditions. Last, we\nverify our findings by numerical simulations.",
        "Selecting an appropriate book is crucial for fostering reading habits in\nchildren. While children exhibit varying levels of complexity when generating\noral narratives, the question arises: do children's books also differ in\nnarrative complexity? This study explores the narrative dynamics of literary\ntexts used in schools, focusing on how their complexity evolves across\ndifferent grade levels. Using Word-Recurrence Graph Analysis, we examined a\ndataset of 1,627 literary texts spanning 13 years of education. The findings\nreveal significant exponential growth in connectedness, particularly during the\nfirst three years of schooling, mirroring patterns observed in children's oral\nnarratives. These results highlight the potential of literary texts as a tool\nto support the development of literacy skills.",
        "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene\nunderstanding leveraging 3D Gaussian Splatting. Unlike existing\nlanguage-embedded 3DGS methods, which rely on a rendering process, our method\ndirectly associates language-aligned CLIP embeddings with 3D Gaussians for\nholistic 3D scene understanding. The key of our method is a language feature\nregistration technique where CLIP embeddings are assigned to the dominant\nGaussians intersected by each pixel-ray. Moreover, we integrate Product\nQuantization (PQ) trained on general large-scale image data to compactly\nrepresent embeddings without per-scene optimization. Experiments demonstrate\nthat our approach significantly outperforms existing approaches in 3D\nperception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D\nobject localization, and 3D object selection tasks. For video results, please\nvisit : https:\/\/drsplat.github.io\/",
        "Prior research yielded many techniques to mitigate software compromise for\nlow-end Internet of Things (IoT) devices. Some of them detect software\nmodifications via remote attestation and similar services, while others\npreventatively ensure software (static) integrity. However, achieving run-time\n(dynamic) security, e.g., control-flow integrity (CFI), remains a challenge.\n  Control-flow attestation (CFA) is one approach that minimizes the burden on\ndevices. However, CFA is not a real-time countermeasure against run-time\nattacks since it requires communication with a verifying entity. This poses\nsignificant risks if safety- or time-critical tasks have memory\nvulnerabilities.\n  To address this issue, we construct EILID - a hybrid architecture that\nensures software execution integrity by actively monitoring control-flow\nviolations on low-end devices. EILID is built atop CASU, a prevention-based\n(i.e., active) hybrid Root-of-Trust (RoT) that guarantees software\nimmutability. EILID achieves fine-grained backward-edge and function-level\nforward-edge CFI via semi-automatic code instrumentation and a secure shadow\nstack.",
        "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps:\/\/anonymous.4open.science\/r\/remember-B0B8\/.",
        "Accurate spatiotemporal calibration is a prerequisite for multisensor fusion.\nHowever, sensors are typically asynchronous, and there is no overlap between\nthe fields of view of cameras and LiDARs, posing challenges for intrinsic and\nextrinsic parameter calibration. To address this, we propose a calibration\npipeline based on continuous-time and bundle adjustment (BA) capable of\nsimultaneous intrinsic and extrinsic calibration (6 DOF transformation and time\noffset). We do not require overlapping fields of view or any calibration board.\nFirstly, we establish data associations between cameras using Structure from\nMotion (SFM) and perform self-calibration of camera intrinsics. Then, we\nestablish data associations between LiDARs through adaptive voxel map\nconstruction, optimizing for extrinsic calibration within the map. Finally, by\nmatching features between the intensity projection of LiDAR maps and camera\nimages, we conduct joint optimization for intrinsic and extrinsic parameters.\nThis pipeline functions in texture-rich structured environments, allowing\nsimultaneous calibration of any number of cameras and LiDARs without the need\nfor intricate sensor synchronization triggers. Experimental results demonstrate\nour method's ability to fulfill co-visibility and motion constraints between\nsensors without accumulating errors.",
        "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https:\/\/app.aios.foundation,\nthe code is at https:\/\/github.com\/agiresearch\/Cerebrum, and video is at\nhttps:\/\/app.aios.foundation\/video-demo.",
        "We fix an excellent regular noetherian scheme $S$ over ${\\mathbf Z}_{(p)}$\nsatisfying a certain finiteness condition. For a constructible \\'etale sheaf\n${\\cal F}$ on a regular scheme $X$ of finite type over $S$, we introduce a\nvariant of the singular support relatively to $S$ and prove the existence of a\nsaturated relative variant of the singular support by adopting the method of\nBeilinson using the Radon transform. We may deduce the existence of the\nsingular support itself, if we admit an expected property on the micro support\nof tensor product and if the scheme $X$ is sufficiently ramified over the base\n$S$.",
        "This paper presents a novel approach for hazard analysis in dashcam footage,\naddressing the detection of driver reactions to hazards, the identification of\nhazardous objects, and the generation of descriptive captions. We first\nintroduce a method for detecting driver reactions through speed and sound\nanomaly detection, leveraging unsupervised learning techniques. For hazard\ndetection, we employ a set of heuristic rules as weak classifiers, which are\ncombined using an ensemble method. This ensemble approach is further refined\nwith differential privacy to mitigate overconfidence, ensuring robustness\ndespite the lack of labeled data. Lastly, we use state-of-the-art\nvision-language models for hazard captioning, generating descriptive labels for\nthe detected hazards. Our method achieved the highest scores in the Challenge\non Out-of-Label in Autonomous Driving, demonstrating its effectiveness across\nall three tasks. Source codes are publicly available at\nhttps:\/\/github.com\/ffyyytt\/COOOL_2025.",
        "We construct algebraic families of smooth affine $\\mathbb{A}^1$-contractible\nvarieties of every dimension $n\\geq 4$ over fields of characteristic zero which\nare non-isomorphic to affine spaces and potential counterexamples to the\nZariski Cancellation Problem. We further prove that these families of varieties\nare also counter examples to the generalized Cancellation problem.",
        "This paper provides an explanation of NTRU, a post quantum encryption scheme,\nwhile also providing a gentle introduction to cryptography. NTRU is a very\nefficient lattice based cryptosystem that appears to be safe against attacks by\nquantum computers. NTRU's efficiency suggests that it is a strong candidate as\nan alternative to RSA, ElGamal, and ECC for the post quantum world. The paper\nbegins with an introduction to cryptography and security proofs for\ncryptographic schemes before explaining the NTRU cryptosystem and culminating\nwith a proof that the original presentation of NTRU is not IND-CPA secure. We\nwill conclude by mentioning padding schemes to NTRU that are provably IND-CCA2\nsecure in the random oracle model. The paper is designed to be accessible to\nanyone with minimal background in abstract algebra and number theory - no\nprevious knowledge of cryptography is assumed. Given the author's lack of\nfamiliarity with the subject, this paper aims to be an expository work rather\nthan to provide new insights to the subject matter."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"ImageNet: A large-scale hierarchical image database",
    "start_abstract":"The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
      ],
      "abstract":[
        "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
      ],
      "categories":[
        "astro-ph.CO"
      ]
    },
    "list":{
      "title":[
        "New insights on a sign-switching $\\Lambda$",
        "Looking at infrared background radiation anisotropies with Spitzer:\n  large scale anisotropies and their implications",
        "DESI dark secrets",
        "Implementing a Robust Test of Galaxy Catalogue Completeness for Dark\n  Siren Measurements of the Hubble Constant",
        "3D Vortices and rotating solitons in ultralight dark matter",
        "Nonparametric reconstructions of dynamical dark energy via flexknots",
        "Gravitational lensing: towards combining the multi-messengers",
        "Observed unequal-time power spectrum",
        "The Preference for Evolving Dark Energy from Cosmological Distance\n  Measurements and Possible Signatures in the Growth Rate of Perturbations",
        "Current constraints on cosmological scenarios with very low reheating\n  temperatures",
        "Disentangling CMB $\\mu$ and $y$ spectral distortions from foregrounds\n  with poorly defined spectral shapes",
        "Foreground Removal in Ground-Based CMB Observations Using a Transformer\n  Model",
        "Euclid Quick Data Release (Q1). The role of cosmic connectivity in\n  shaping galaxy clusters",
        "Improved Decoding of Tanner Codes",
        "Anomaly Detection to identify Transients in LSST Time Series Data",
        "Cherenkov detector with wavelength-shifting fiber readout for muon\n  tomography applications",
        "Diagnosing Quantum Many-body Chaos in Non-Hermitian Quantum Spin Chain\n  via Krylov Complexity",
        "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems",
        "Energy Reconstruction of Non-fiducial Electron-Positron Events in the\n  DAMPE Experiment Using Convolutional Neural Networks",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "Impact of pH and chloride content on the biodegradation of magnesium\n  alloys for medical implants: An in vitro and phase-field study",
        "Concentration of Measure for Distributions Generated via Diffusion\n  Models",
        "A Comprehensive Framework for Electroweak Phase Transitions: Thermal\n  History and Dynamics from Bubble Nucleation to Percolation",
        "On the inverse-closedness of operator-valued matrices with polynomial\n  off-diagonal decay",
        "Multivariate Frequent Stability and Diam-Mean Equicontinuity",
        "Global existence for semi-linear hyperbolic equations in a neighbourhood\n  of future null infinity",
        "Growth Laws and Universality in 2-TIPS: Microscopic and Coarse grained\n  approach",
        "High pressure structural and lattice dynamics study of\n  {\\alpha}-In$_2$Se$_3$"
      ],
      "abstract":[
        "The proposal for a sudden sign-switching cosmological constant $\\Lambda$ in\nthe local universe, emulating a phase transition from anti-de Sitter (AdS) to\nde Sitter (dS) space, has markedly revamped the fit to observational data and\nlays out a propitious framework for ameliorating major cosmological tensions,\nsuch as the $H_0$ and $S_8$ tensions. This proposal is widely known as\n$\\Lambda_s$CDM. We investigate the possibility that $\\Lambda$ does not only\nflip sign at the transition but has also different curvature radii in the AdS\nand dS phases. We show that the critical redshift of the transition $z_c$ is\nstrongly correlated with the vacuum energy in the AdS phase\n$\\Omega_{\\Lambda_-}$, and that these two variables do not correlate strongly\nwith the other cosmological parameters. We also show that the cost of adding an\nadditional parameter to the $\\Lambda_s$CDM cosmological model does not improve\nthe goodness of fit. Armed with our findings, we demonstrate that for a proper\nchoice of $z_c$, the vacuum energy in the dS phase may not necessarily be\n$-\\Omega_{\\Lambda_-}$, for comparable degree of conformity between the model\nprediction and experimental data.",
        "We use Spitzer\/IRAC deep exposure data covering two significantly larger than\nbefore sky areas to construct maps suitable for evaluating source-subtracted\nfluctuations in the cosmic infrared background (CIB). The maps are constructed\nusing the self-calibration methodology eliminating artifacts to sufficient\naccuracy and subset maps are selected in each area containing approximately\nuniform exposures. These maps are clipped and removed of known sources and then\nFourier transformed to probe the CIB anisotropies to new larger scales. The\npower spectrum of the resultant CIB anisotropies is measured from the data to\n>1 degree revealing the component well above that from remaining known galaxies\non scales >1 arcmin. The fluctuations are demonstrated to be free of Galactic\nand Solar System foreground contributions out to the largest scales measured.\nWe discuss the proposed theories for the origin of the excess CIB anisotropies\nin light of the new data. Out of these, the model where the CIB fluctuation\nexcess originates from the granulation power due to LIGO-observed primordial\nblack holes as dark matter appears most successful in accounting for all\nobservations related to the measured CIB power amplitude and spatial structure,\nincluding the measured coherence between the CIB and unresolved cosmic X-ray\nbackground (CXB). Finally we point out the use of the data to probe the CIB-CXB\ncross-power to new scales and higher accuracy. We also discuss the synergy of\nthese data with future CIB programs at shorter near-IR wavelengths with deep\nwide surveys and sub-arcsecond angular resolution as provided by Euclid and\nRoman space missions.",
        "The first year results of DESI provide evidence that dark energy may not be\nquantum vacuum energy ($\\Lambda$). If true, this would be an extraordinary\ndevelopment in the 25-year quest to understand cosmic acceleration. The\nbest-fit DESI $w_0w_a$ models for dark energy, which underpin the claim, have\nvery strange behavior. They achieve a maximum dark energy density around\n$z\\simeq 0.4$ and rapidly decrease before and after. We explore\nphysics-motivated models where the dark energy is a rolling scalar-field. Each\nof our four scalar-field models is characterized by one dimensionless parameter\n$\\beta$, which in the limit of $\\beta \\rightarrow 0$ reduces to $\\Lambda$CDM.\nWhile none of our models fit the DESI data significantly better than\n$\\Lambda$CDM, for values of $\\beta$ of order unity, they fit about as well as\n$\\Lambda$CDM. Each scalar field model makes different predictions for the age\nof the Universe, which might be used to discriminate amongst them. For small\nvalues of $\\beta$, the dimensionsless initial slope of the scalar field\npotential links the predictions of different scalar field models. And for small\nvalues of $\\beta$, $w_0w_a$ models can marginally represent the predictions of\na scalar-field model at the current precision needed. However, with\nincreasingly precise distance measurements, over a larger redshift range,\nexplicit modeling of the scalar-field evolution is already and will continue to\nbe essential to testing alternatives to $\\Lambda$.",
        "We present the application of a robust test of galaxy catalogue completeness\nto the gwcosmo pipeline. The method implements a straightforward statistical\ntest for determining the apparent magnitude completeness limit of a\nmagnitude-redshift sample. This offers an improved, less conservative approach\ncompared with how galaxy catalogue completeness is currently estimated in the\ngwcosmo gravitational wave cosmology pipeline for determining the Hubble\nconstant $H_{0}$. The test also does not require prior knowledge of the\nluminosity function, and thus returns a more robust estimate of the limiting\napparent magnitude for a magnitude-redshift sample of galaxies. For GWTC-1\nresults using $B_{J}$-band photometry of galaxies in the GLADE catalogue, we\nfind a $3.4\\%$ improvement on the inference of $H_{0}$ using dark sirens only\nand a $1.3\\%$ improvement for the combined posterior with GW170817. Using\nGLADE+, there is a $8.6\\%$ improvement with dark sirens only and a $6.3\\%$\nimprovement for the combined posterior with GW170817. However, the final\nposterior on $H_{0}$ using the GWTC-3 dataset with the GLADE+ $K$-band shows no\nimprovement when applying the robust method. This is because the GLADE+ galaxy\ncatalogue provides little or no coverage in the $K$-band for any of the GWTC-3\nevents. With the use of deeper galaxy catalogues in future gravitational wave\ncosmology analyses, the adoption of a less conservative estimate of magnitude\ncompleteness will become increasingly important.",
        "We study the formation and the dynamics of vortex lines in rotating scalar\ndark matter halos, focusing on models with quartic repulsive self-interactions.\nIn the nonrelativistic regime, vortex lines and their lattices arise from the\nGross-Pitaevskii equation of motion, as for superfluids and Bose-Einstein\ncondensates studied in laboratory experiments. Indeed, in such systems\nvorticity is supported by the singularities of the phase of the scalar field,\nwhich leads to a discrete set of quantized vortices amid a curl-free velocity\nbackground. In the continuum limit where the number of vortex lines becomes\nvery large, we find that the equilibrium solution is a rotating soliton that\nobeys a solid-body rotation, with an oblate density profile aligned with the\ndirection of the total spin. This configuration is dynamically stable provided\nthe rotational energy is smaller than the self-interaction and gravitational\nenergies. Using numerical simulations in the Thomas-Fermi regime, with\nstochastic initial conditions for a spherical halo with a specific averaged\ndensity profile and angular momentum, we find that a rotating soliton always\nemerges dynamically, within a few dynamical times, and that a network of vortex\nlines aligned with the total spin fills its oblate profile. These vertical\nvortex lines form a regular lattice in the equatorial plane, in agreement with\nthe analytical predictions of uniform vortex density and solid-body rotation.\nThese vortex lines might further extend between halos to form the backbone of\nspinning cosmic filaments.",
        "Recent cosmological surveys have provided unprecedented datasets that can be\nused to reconstruct the history of the dark energy equation of state. In this\nwork, a free-form \"flexknot'' parameterisation is employed to represent $w(a)$\nas a linear spline between free-moving nodes, the number of which may vary. By\ncombining DESI Baryon Acoustic Oscillation measurements with Pantheon+ or DES5Y\nsupernovae, the functional posteriors of $w(a)$ reveal an unexpected W-shaped\nstructure. While the Bayesian evidence may still favour $\\Lambda$CDM, the\nrobustness of these results suggests the structure is indeed present in the\ndata. The tension $R$-statistic and suspiciousness have been marginalised over\nmodels, and demonstrate that while the reconstructions from DESI and Pantheon+\nagree, DESI and DES5Y do not. We conclude that, while there is no smoking gun\nfor dynamical dark energy, the structure unearthed in this work is generally\ntoo complex to be captured by the restrictive $w$CDM or CPL parameterisations.",
        "The next generation of gravitational wave detectors and electromagnetic\ntelescopes are beckoning the onset of the multi-messenger era and the exciting\nscience that lies ahead. Multi-messenger strong gravitational lensing will help\nprobe some of the most important questions of the Universe in an unprecedented\nmanner. In particular, understanding the nature of gravitational wave sources,\nthe underlying physical processes and mechanisms that produce emissions well\nbefore or right until the time of the merger, their associations to the\nseemingly distinct populations of gamma ray bursts, fast radio bursts and\nkilonovae. Not to mention, multi-messenger lensing will offer unique probes of\ntest of gravity models and constraints on cosmological parameters complementary\nto other probes. Enabling multi-messenger science calls for concerted follow-up\nefforts and development of new and shared resources required in the community.",
        "The next generation of galaxy surveys will provide highly precise\nmeasurements of galaxy clustering, therefore requiring a corresponding\naccuracy. Current approaches, which rely on approximations and idealized\nassumptions, may fall short in capturing the level of detail required for\nhigh-precision observations. In order to increase the modeling accuracy,\nrecently, unequal-time contributions to the galaxy power spectrum have been\nintroduced in order to include the effects of radial correlations. We present a\ngeneralization of the formalism for the observed unequal-time power spectrum,\nthat includes Doppler and local general relativistic corrections, plus local\nprimordial non-Gaussianity. We find that unequal time corrections can\npotentially mimic an effective $f_{\\mathrm{NL}}$ of order unity. We provide a\nfirst assessment of the significance of unequal-time corrections for future\ngalaxy clustering experiments, estimating a Signal-to-Noise-Ratio of $\\sim3$\nfor Stage IV-like surveys.",
        "In this study, we use a flexible parametrization of the equation of state of\ndark energy to explore its possible evolution with datasets from the Dark\nEnergy Spectroscopic Instrument (DESI), Planck cosmic microwave background, and\neither the 5-year Dark Energy Survey (DES) or the Pantheon+ (PP) supernova (SN)\ncompilation. This parametrization, called transitional dark energy, allows for\nrapid changes in the equation of state but also changes like that in the\nChevallier-Polarski-Linder parametrization. We find a 3.8{\\sigma} preference\nfor evolving dark energy over {\\Lambda}CDM with the DES dataset and a weaker\n2.4{\\sigma} preference when using the PP dataset. This corroborates the finding\nof the DESI Collaboration, who found that their baryon acoustic oscillation\ndata preferred evolving dark energy when fit with the CPL parametrization of\nthe equation of state. Our analysis reveals no significant outliers in the DESI\ndata around the TDE best-fit, while the data is asymmetrically distributed\naround the {\\Lambda}CDM best-fit model such that the measured distances are on\naverage smaller. The DESI and SN data both prefer an expansion history that\nimplies a higher dark energy density around z=0.5 than in the\nPlanck-{\\Lambda}CDM model, with the inferred equation of state being greater\nthan -1 around z=0 and close to or below -1 at z>0.5. We show that when the\nexpansion rate is greater than that in the Planck-{\\Lambda}CDM model (around\nz=0.5), the growth rate calculated assuming General Relativity is suppressed\nrelative to the Planck-{\\Lambda}CDM model, and it rebounds as the expansion\nrate differences between the models become smaller closer to the present time.\nThe resulting flattening of the $f\\sigma_8(z)$ curve compared to the\n{\\Lambda}CDM model could be an independent signature of the temporal evolution\nof dark energy.",
        "We present an updated analysis of cosmological models with very low reheating\nscenarios ($T_\\text{RH} \\sim \\mathcal{O}(\\text{MeV})$). Our study includes a\nmore precise computation of neutrino distribution functions, leveraging the\nlatest datasets from cosmological surveys. We perform a joint analysis that\ncombines constraints from Big Bang Nucleosynthesis, the Cosmic Microwave\nBackground, and galaxy surveys, alongside separate investigations of these\ndatasets, carefully assessing the impact of different choices of priors. At the\n$95\\%$ confidence level, we establish a lower bound on the reheating\ntemperature of $T_\\text{RH} > 5.96 \\; \\text{MeV} $, representing the most\nstringent constraint to date.",
        "We have presented a new approach to separate small spectral $\\mu$ and $y$\ndistortions of the CMB from foreground components with poorly defined spectral\nshapes. Our linear method, called the Least Response Method (LRM), is based on\nthe idea of simultaneously minimizing the response to all possible foregrounds\nand photon noise while maintaining a constant response to the useful signal. We\ncompared our approach with the mILC method, which is a modification of the\nInternal Linear Combination previously used for CMB anisotropy maps, and proved\nthe advantages of LRM. In addition, we found the optimal temperature of the\ntelescope optical system for any experiments related to the study of the CMB\n$\\mu$ distortions.",
        "We present a novel method for Cosmic Microwave Background (CMB) foreground\nremoval based on deep learning techniques. This method employs a Transformer\nmodel, referred to as \\texttt{TCMB}, which is specifically designed to\neffectively process HEALPix-format spherical sky maps. \\texttt{TCMB} represents\nan innovative application in CMB data analysis, as it is an image-based\ntechnique that has rarely been utilized in this field. Using simulated data\nwith noise levels representative of current ground-based CMB polarization\nobservations, the \\texttt{TCMB} method demonstrates robust performance in\nremoving foreground contamination. The mean absolute variance for the\nreconstruction of the noisy CMB Q\/U map is significantly less than the CMB\npolarization signal. To mitigate biases caused by instrumental noise, a\ncross-correlation approach using two half-mission maps was employed,\nsuccessfully recovering CMB EE and BB power spectra that align closely with the\ntrue values, and these results validate the effectiveness of the \\texttt{TCMB}\nmethod. Compared to the previously employed convolutional neural network\n(CNN)-based approach, the \\texttt{TCMB} method offers two significant\nadvantages: (1) It demonstrates superior effectiveness in reconstructing CMB\npolarization maps, outperforming CNN-based methods. (2) It can directly process\nHEALPix spherical sky maps without requiring rectangular region division, a\nstep necessary for CNN-based approaches that often introduces uncertainties\nsuch as boundary effects. This study highlights the potential of\nTransformer-based models as a powerful tool for CMB data analysis, offering a\nsubstantial improvement over traditional CNN-based techniques.",
        "The matter distribution around galaxy clusters is distributed over several\nfilaments, reflecting their positions as nodes in the large-scale cosmic web.\nThe number of filaments connected to a cluster, namely its connectivity, is\nexpected to affect the physical properties of clusters. Using the first Euclid\ngalaxy catalogue from the Euclid Quick Release 1 (Q1), we investigate the\nconnectivity of galaxy clusters and how it correlates with their physical and\ngalaxy member properties. Around 220 clusters located within the three fields\nof Q1 (covering $\\sim 63 \\ \\text{deg}^2$), are analysed in the redshift range\n$0.2 < z < 0.7$. Due to the photometric redshift uncertainty, we reconstruct\nthe cosmic web skeleton, and measure cluster connectivity, in 2-D projected\nslices with a thickness of 170 comoving $h^{-1}.\\text{Mpc}$ and centred on each\ncluster redshift, by using two different filament finder algorithms on the most\nmassive galaxies ($M_*\\ > 10^{10.3} \\ M_\\odot$). In agreement with previous\nmeasurements, we recover the mass-connectivity relation independently of the\nfilament detection algorithm, showing that the most massive clusters are, on\naverage, connected to a larger number of cosmic filaments, consistent with\nhierarchical structure formation models. Furthermore, we explore possible\ncorrelations between connectivities and two cluster properties: the fraction of\nearly-type galaxies and the S\\'ersic index of galaxy members. Our result\nsuggests that the clusters populated by early-type galaxies exhibit higher\nconnectivity compared to clusters dominated by late-type galaxies. These\npreliminary investigations highlight our ability to quantify the impact of the\ncosmic web connectivity on cluster properties with Euclid.",
        "In this paper, we present improved decoding algorithms for expander-based\nTanner codes.\n  We begin by developing a randomized linear-time decoding algorithm that,\nunder the condition that $ \\delta d_0 > 2 $, corrects up to $ \\alpha n $ errors\nfor a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \\alpha, \\delta)\n$-bipartite expander with $n$ left vertices, and $ C_0 \\subseteq \\mathbb{F}_2^d\n$ is a linear inner code with minimum distance $ d_0 $. This result improves\nupon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024),\nwhich required $ \\delta d_0 > 3 $.\n  We further derandomize the algorithm to obtain a deterministic linear-time\ndecoding algorithm with the same decoding radius. Our algorithm improves upon\nthe previous deterministic algorithm of Cheng et al.\\ by achieving a decoding\nradius of $ \\alpha n $, compared with the previous radius of $\n\\frac{2\\alpha}{d_0(1 + 0.5c\\delta) }n$.\n  Additionally, we investigate the size-expansion trade-off introduced by the\nrecent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to\nprovide new bounds on the minimum distance of Tanner codes. Specifically, we\nprove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately\n$f_\\delta^{-1} \\left( \\frac{1}{d_0} \\right) \\alpha n $, where $ f_\\delta(\\cdot)\n$ is the Size-Expansion Function. As another application, we improve the\ndecoding radius of our decoding algorithms from $\\alpha n$ to approximately\n$f_\\delta^{-1}(\\frac{2}{d_0})\\alpha n$.",
        "We introduce a novel approach to detecting microlensing events and other\ntransients in light curves, utilising the isolation forest (iForest) algorithm\nfor anomaly detection. Focusing on the Legacy Survey of Space and Time by the\nVera C. Rubin Observatory, we show that an iForest trained on signal-less light\ncurves can efficiently identify microlensing events by different types of dark\nobjects and binaries, as well as variable stars. We further show that the\niForest has real-time applicability through a drip-feed analysis, demonstrating\nits potential as a valuable tool for LSST alert brokers to efficiently\nprioritise and classify transient candidates for follow-up observations.",
        "Cherenkov detectors have been extensively developed and utilized in various\nscientific fields, including particle physics, astrophysics, and nuclear\nengineering. These detectors operate based on Cherenkov radiation, which is\nemitted when a charged particle traverses a dielectric medium at a velocity\ngreater than the phase velocity of light in that medium. In this work, we\npresent the development of a Cherenkov radiation detector designed for a muon\ntomography system with high spatial resolution, employing wavelength-shifting\n(WLS) fiber readout. The detector consists of two large-area Cherenkov\nradiators, each measuring 1 m x 1 m, with each read out by WLS fibers arranged\northogonally to determine the x and y coordinates of muon hit positions. The\nsystem is modeled using the GEANT4 simulation package, and the achieved\nposition resolution is 1.8 mm+-0.1 (FWHM). This design enables precise tracking\nof muon trajectories, making it suitable for high-resolution imaging\napplications in muon tomography.",
        "We investigate the phase transitions from chaotic to non-chaotic dynamics in\na quantum spin chain with a local non-Hermitian disorder, which can be realized\nwith a Rydberg atom array setting. As the disorder strength increases, the\nemergence of non-chaotic dynamics is qualitatively captured through the\nsuppressed growth of Krylov complexity, and quantitatively identified through\nthe reciprocity breaking of Krylov space. We further find that the localization\nin Krylov space generates another transition in the weak disorder regime,\nsuggesting a weak ergodicity breaking. Our results closely align with\nconventional methods, such as the entanglement entropy and complex level\nspacing statistics, and pave the way to explore non-Hermitian phase transitions\nusing Krylov complexity and associated metrics.",
        "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.",
        "The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "The individual contributions of pH and chloride concentration to the\ncorrosion kinetics of bioabsorbable magnesium (Mg) alloys remain unresolved\ndespite their significant roles as driving factors in Mg corrosion. This study\ndemonstrates and quantifies hitherto unknown separate effects of pH and\nchloride content on the corrosion of Mg alloys pertinent to biomedical implant\napplications. The experimental setup designed for this purpose enables the\nquantification of the dependence of corrosion on pH and chloride concentration.\nThe in vitro tests conclusively demonstrate that variations in chloride\nconcentration, relevant to biomedical applications, have a negligible effect on\ncorrosion kinetics. The findings identify pH as a critical factor in the\ncorrosion of bioabsorbable Mg alloys. A variationally consistent phase-field\nmodel is developed for assessing the degradation of Mg alloys in biological\nfluids. The model accurately predicts the corrosion performance of Mg alloys\nobserved during the experiments, including their dependence on pH and chloride\nconcentration. The capability of the framework to account for mechano-chemical\neffects during corrosion is demonstrated in practical orthopaedic applications\nconsidering bioabsorbable Mg alloy implants for bone fracture fixation and\nporous scaffolds for bone tissue engineering. The strategy has the potential to\nassess the in vitro and in vivo service life of bioabsorbable Mg-based\nbiomedical devices.",
        "We show via a combination of mathematical arguments and empirical evidence\nthat data distributions sampled from diffusion models satisfy a Concentration\nof Measure Property saying that any Lipschitz $1$-dimensional projection of a\nrandom vector is not too far from its mean with high probability. This implies\nthat such models are quite restrictive and gives an explanation for a fact\npreviously observed in the literature that conventional diffusion models cannot\ncapture \"heavy-tailed\" data (i.e. data $\\mathbf{x}$ for which the norm\n$\\|\\mathbf{x}\\|_2$ does not possess a sub-Gaussian tail) well. We then proceed\nto train a generalized linear model using stochastic gradient descent (SGD) on\nthe diffusion-generated data for a multiclass classification task and observe\nempirically that a Gaussian universality result holds for the test error.\n  In other words, the test error depends only on the first and second order\nstatistics of the diffusion-generated data in the linear setting. Results of\nsuch forms are desirable because they allow one to assume the data itself is\nGaussian for analyzing performance of the trained classifier. Finally, we note\nthat current approaches to proving universality do not apply to this case as\nthe covariance matrices of the data tend to have vanishing minimum singular\nvalues for the diffusion-generated data, while the current proofs assume that\nthis is not the case (see Subsection 3.4 for more details). This leaves\nextending previous mathematical universality results as an intriguing open\nquestion.",
        "The electroweak phase transition (EWPT) is crucial for cosmology and particle\nphysics, with a profound impact on electroweak baryogenesis, symmetry breaking,\nand gravitational wave (GW) signals. However, many studies overlook key aspects\nof EWPT dynamics, leading to misidentified patterns and overestimated GW\nsignals. To address these gaps, we present a comprehensive framework for\nanalyzing EWPTs, focusing on the vacuum's thermal history and dynamics from\nbubble nucleation to percolation. Using the $\\mathbb{Z}_2$-odd real scalar\nsinglet model, we demonstrate the occurrence of spontaneous $\\mathbb{Z}_2$\nsymmetry breaking in the high-temperature vacuum, leading to diverse EWPT\nprocesses, including multi-step transitions and inverse symmetry breaking. We\nidentify four distinct EWPT patterns, each characterized by unique\nsymmetry-breaking mechanisms and associated with bubbles exhibiting distinct\nfield configurations, which can be analyzed using a formalism based on energy\ndensity distributions developed here. A key finding is that bubble nucleation\nfails in extremely strong phase transitions (PTs) with low nucleation rates, or\nin ultra-fast PTs involving inverse $s$-bubbles that collapse instantly upon\nformation, both of which lead to false vacuum trapping and the absence of\nobservable GW signals. In first-order PTs where nucleation succeeds, stronger\ntransitions occur later in the universe's evolution, while weaker transitions\nproceed more rapidly. Multi-step transitions involving (inverse) $\\mathbb{Z}_2$\nsymmetry breaking give rise to complex transition sequences and exotic bubble\ndynamics, such as sequential nucleation or the coexistence of bubbles from\ndifferent vacua -- phenomena with significant implications for GW spectra, dark\nmatter, and baryogenesis. This work advances our understanding of EWPT dynamics\nand lays the groundwork for future studies of EWPTs in BSM physics.",
        "We give a self-contained proof of a recently established\n$\\mathcal{B}(\\mathcal{H})$-valued version of Jaffards Lemma. That is, we show\nthat the Jaffard algebra of $\\mathcal{B}(\\mathcal{H})$-valued matrices, whose\noperator norms of their respective entries decay polynomially off the diagonal,\nis a Banach algebra which is inverse-closed in the Banach algebra\n$\\mathcal{B}(\\ell^2(X;\\mathcal{H}))$ of all bounded linear operators on\n$\\ell^2(X;\\mathcal{H})$, the Bochner-space of square-summable\n$\\mathcal{H}$-valued sequences.",
        "In this paper, we introduce and investigate multivariate versions of frequent\nstability and diam-mean equicontinuity. Given a natural number $m > 1$, we call\nthose notions \"frequent $m$-stability\" and \"diam-mean $m$-equicontinuity\". We\nuse these dynamical rigidity properties to characterise systems whose factor\nmap to the maximal equicontinuous factor (MEF) is finite-to-one for a residual\nset, called \"almost finite-to-one extensions\", or a set of full measure, called\n\"almost surely finite-to-one extensions\". In the case of a $\\sigma$-compact,\nlocally compact, abelian acting group it is shown that frequently\n$(m+1)$-stable systems are equivalently characterised as almost $m$-to-one\nextensions of their MEF. Similarly, it is shown that a system is diam-mean\n$(m+1)$-equicontinuous if and only if it is an almost surely $m$-to-one\nextension of its MEF.",
        "In this paper, we establish the global existence of a semi-linear class of\nhyperbolic equations in 3+1 dimensions, that satisfy the bounded weak null\ncondition. We propose a conformal compactification of the future directed\nnull-cone in Minkowski spacetime, enabling us to establish the solution to the\nwave equation in a neighbourhood of future null infinity. Using this framework,\nwe formulate a conformal symmetric hyperbolic Fuchsian system of equations. The\nexistence of solutions to this Fuchsian system follows from an application of\nthe existence theory developed in [1], and [2].",
        "Two temperature induced phase separation(2-TIPS) is a phenomenon observed in\nmixtures of active and passive particles modeled by scalar activity where the\ntemperature of the particle is proportional to its activity. The binary mixture\nof 'hot' and 'cold' particles phase separate when the relative temperature\ndifference between hot and cold particles defined as activity $\\chi$ exceeds a\ndensity dependent critical value. The study of kinetics in 2-TIPS, a\nnon-equilibrium phase separation, is of fundamental importance in statistical\nphysics. In this paper, we investigate 2-TIPS kinetics using molecular dynamics\n(MD) and coarse-grained (CG) modeling in 3D and 2D. The coarse-grained model\ncouples two passive Model B equations for hot and cold particles, with coupling\nterms emulating the energy transfer between them by raising the temperature of\ncold particles and lowering that of hot particles, a key observation from the\nMD simulations. MD simulations reveal that at high densities, phase separation\nbegins immediately after the quench, forming bi-continuous domains rich in hot\nor cold particles, similar to spinodal decomposition in passive systems. These\ninterconnected domains are also observed in the coarse-grained model for the\nmixture's critical composition. Both MD and CG models show dynamic scaling of\nthe correlation function, indicating self-similar domain growth. Regardless of\ndimensionality, both methods report algebraic growth in domain length with a\ngrowth exponent of $1\/3$, known as the Lifshitz-Slyozov exponent, widely\nobserved in passive systems. Our results demonstrate that the universality of\nphase separation kinetics observed in passive systems also extends to the\nnon-equilibrium binary mixture undergoing 2-TIPS.",
        "Layered $\\alpha$-In$_2$Se$_3$has been studied using a concomitant in-situ\nsynchrotron angle dispersive powder x-ray diffraction and Raman spectroscopy\nstudy in a diamond anvil cell up to 60+ GPa, at room temperature. Helium, that\nremains fairly hydrostatic up to the highest pressure in this study, was used\nas the pressure-transmitting medium. The results from both experimental methods\nreveal a pressure-induced structural phase transition from\n$\\alpha$-In$_2$Se$_3$ to a monoclinic $\\beta$'-In2Se3 structure at $\\approx$1\nGPa, in agreement with previous studies. Based on our detailed measurements\nusing both experimental techniques and F-f formalism, the $\\beta$'-In$_2$Se$_3$\nstructure remains stable up to 45 GPa, without a clear indication of a phase\ntransition towards the previously reported $\\beta$-In2Se3 phase. Above this\npressure, In$_2$Se$_3$ adopts a disordered solid-solution-like orthorhombic\nstructure, phase IV. The results are discussed in comparison with the relevant\nprevious studies of $\\alpha$-In$_2$Se$_3$ under pressure."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Brain\u2013Computer Interface Spellers: A Review",
    "start_abstract":"A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
      ],
      "abstract":[
        "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Topology-Driven Attribute Recovery for Attribute Missing Graph Learning\n  in Social Internet of Things",
        "Large Language Model Interface for Home Energy Management Systems",
        "Bridging the Communication Gap: Evaluating AI Labeling Practices for\n  Trustworthy AI Development",
        "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
        "Recommending Actionable Strategies: A Semantic Approach to Integrating\n  Analytical Frameworks with Decision Heuristics",
        "Transformer Dynamics: A neuroscientific approach to interpretability of\n  large language models",
        "Opus: A Workflow Intention Framework for Complex Workflow Generation",
        "Contextual bandits with entropy-based human feedback",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
        "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
        "Ontology Generation using Large Language Models",
        "Off-Switching Not Guaranteed",
        "Multilingual Non-Autoregressive Machine Translation without Knowledge\n  Distillation",
        "PanopticSplatting: End-to-End Panoptic Gaussian Splatting",
        "Emergent effects of scaling on the functional hierarchies within large\n  language models",
        "CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models",
        "Evaluating Developer-written Unit Test Case Reduction for Java -- A\n  Replication Study",
        "Non-Gaussianities as a Signature of Quantumness of Quantum Cosmology",
        "Propagation of Gravitational Waves on a Geometric Condensate background\n  of $(R + \\alpha R^{2})$ Origin",
        "A Search for Eclipse Cycles Similar to the Hypersaros: Columbus and the\n  Lunar Eclipse of March 14, 2025",
        "Realistic Clothed Human and Object Joint Reconstruction from a Single\n  Image",
        "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "CoopDETR: A Unified Cooperative Perception Framework for 3D Detection\n  via Object Query",
        "Predicting Human Choice Between Textually Described Lotteries",
        "A Study of the Efficacy of Generative Flow Networks for Robotics and\n  Machine Fault-Adaptation"
      ],
      "abstract":[
        "With the advancement of information technology, the Social Internet of Things\n(SIoT) has fostered the integration of physical devices and social networks,\ndeepening the study of complex interaction patterns. Text Attribute Graphs\n(TAGs) capture both topological structures and semantic attributes, enhancing\nthe analysis of complex interactions within the SIoT. However, existing graph\nlearning methods are typically designed for complete attributed graphs, and the\ncommon issue of missing attributes in Attribute Missing Graphs (AMGs) increases\nthe difficulty of analysis tasks. To address this, we propose the\nTopology-Driven Attribute Recovery (TDAR) framework, which leverages\ntopological data for AMG learning. TDAR introduces an improved pre-filling\nmethod for initial attribute recovery using native graph topology.\nAdditionally, it dynamically adjusts propagation weights and incorporates\nhomogeneity strategies within the embedding space to suit AMGs' unique\ntopological structures, effectively reducing noise during information\npropagation. Extensive experiments on public datasets demonstrate that TDAR\nsignificantly outperforms state-of-the-art methods in attribute reconstruction\nand downstream tasks, offering a robust solution to the challenges posed by\nAMGs. The code is available at https:\/\/github.com\/limengran98\/TDAR.",
        "Home Energy Management Systems (HEMSs) help households tailor their\nelectricity usage based on power system signals such as energy prices. This\ntechnology helps to reduce energy bills and offers greater demand-side\nflexibility that supports the power system stability. However, residents who\nlack a technical background may find it difficult to use HEMSs effectively,\nbecause HEMSs require well-formatted parameterization that reflects the\ncharacteristics of the energy resources, houses, and users' needs. Recently,\nLarge-Language Models (LLMs) have demonstrated an outstanding ability in\nlanguage understanding. Motivated by this, we propose an LLM-based interface\nthat interacts with users to understand and parameterize their\n``badly-formatted answers'', and then outputs well-formatted parameters to\nimplement an HEMS. We further use Reason and Act method (ReAct) and few-shot\nprompting to enhance the LLM performance. Evaluating the interface performance\nrequires multiple user--LLM interactions. To avoid the efforts in finding\nvolunteer users and reduce the evaluation time, we additionally propose a\nmethod that uses another LLM to simulate users with varying expertise, ranging\nfrom knowledgeable to non-technical. By comprehensive evaluation, the proposed\nLLM-based HEMS interface achieves an average parameter retrieval accuracy of\n88\\%, outperforming benchmark models without ReAct and\/or few-shot prompting.",
        "As artificial intelligence (AI) becomes integral to economy and society,\ncommunication gaps between developers, users, and stakeholders hinder trust and\ninformed decision-making. High-level AI labels, inspired by frameworks like EU\nenergy labels, have been proposed to make the properties of AI models more\ntransparent. Without requiring deep technical expertise, they can inform on the\ntrade-off between predictive performance and resource efficiency. However, the\npractical benefits and limitations of AI labeling remain underexplored. This\nstudy evaluates AI labeling through qualitative interviews along four key\nresearch questions. Based on thematic analysis and inductive coding, we found a\nbroad range of practitioners to be interested in AI labeling (RQ1). They see\nbenefits for alleviating communication gaps and aiding non-expert\ndecision-makers, however limitations, misunderstandings, and suggestions for\nimprovement were also discussed (RQ2). Compared to other reporting formats,\ninterviewees positively evaluated the reduced complexity of labels, increasing\noverall comprehensibility (RQ3). Trust was influenced most by usability and the\ncredibility of the responsible labeling authority, with mixed preferences for\nself-certification versus third-party certification (RQ4). Our Insights\nhighlight that AI labels pose a trade-off between simplicity and complexity,\nwhich could be resolved by developing customizable and interactive labeling\nframeworks to address diverse user needs. Transparent labeling of resource\nefficiency also nudged interviewee priorities towards paying more attention to\nsustainability aspects during AI development. This study validates AI labels as\na valuable tool for enhancing trust and communication in AI, offering\nactionable guidelines for their refinement and standardization.",
        "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
        "We present a novel approach for recommending actionable strategies by\nintegrating strategic frameworks with decision heuristics through semantic\nanalysis. While strategy frameworks provide systematic models for assessment\nand planning, and decision heuristics encode experiential knowledge,these\ntraditions have historically remained separate. Our methodology bridges this\ngap using advanced natural language processing (NLP), demonstrated through\nintegrating frameworks like the 6C model with the Thirty-Six Stratagems. The\napproach employs vector space representations and semantic similarity\ncalculations to map framework parameters to heuristic patterns, supported by a\ncomputational architecture that combines deep semantic processing with\nconstrained use of Large Language Models. By processing both primary content\nand secondary elements (diagrams, matrices) as complementary linguistic\nrepresentations, we demonstrate effectiveness through corporate strategy case\nstudies. The methodology generalizes to various analytical frameworks and\nheuristic sets, culminating in a plug-and-play architecture for generating\nrecommender systems that enable cohesive integration of strategic frameworks\nand decision heuristics into actionable guidance.",
        "As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na \"neuroscience of AI\" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.",
        "This paper introduces Workflow Intention, a novel framework for identifying\nand encoding process objectives within complex business environments. Workflow\nIntention is the alignment of Input, Process and Output elements defining a\nWorkflow's transformation objective interpreted from Workflow Signal inside\nBusiness Artefacts. It specifies how Input is processed to achieve desired\nOutput, incorporating quality standards, business rules, compliance\nrequirements and constraints. We adopt an end-to-end Business Artefact Encoder\nand Workflow Signal interpretation methodology involving four steps:\nModality-Specific Encoding, Intra-Modality Attention, Inter-Modality Fusion\nAttention then Intention Decoding. We provide training procedures and critical\nloss function definitions. In this paper we introduce the concepts of Workflow\nSignal and Workflow Intention, where Workflow Signal decomposed into Input,\nProcess and Output elements is interpreted from Business Artefacts, and\nWorkflow Intention is a complete triple of these elements. We introduce a\nmathematical framework for representing Workflow Signal as a vector and\nWorkflow Intention as a tensor, formalizing properties of these objects.\nFinally, we propose a modular, scalable, trainable, attention-based multimodal\ngenerative system to resolve Workflow Intention from Business Artefacts.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps:\/\/spinbench.github.io\/",
        "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md",
        "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research.",
        "Hadfield-Menell et al. (2017) propose the Off-Switch Game, a model of\nHuman-AI cooperation in which AI agents always defer to humans because they are\nuncertain about our preferences. I explain two reasons why AI agents might not\ndefer. First, AI agents might not value learning. Second, even if AI agents\nvalue learning, they might not be certain to learn our actual preferences.",
        "Multilingual neural machine translation (MNMT) aims at using one single model\nfor multiple translation directions. Recent work applies non-autoregressive\nTransformers to improve the efficiency of MNMT, but requires expensive\nknowledge distillation (KD) processes. To this end, we propose an M-DAT\napproach to non-autoregressive multilingual machine translation. Our system\nleverages the recent advance of the directed acyclic Transformer (DAT), which\ndoes not require KD. We further propose a pivot back-translation (PivotBT)\napproach to improve the generalization to unseen translation directions.\nExperiments show that our M-DAT achieves state-of-the-art performance in\nnon-autoregressive MNMT.",
        "Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models.",
        "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.",
        "The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps:\/\/github.com\/szc12153\/CLDyB.",
        "Abstract: Failing test case reduction can promote efficient debugging because\na developer may not need to observe components that are not relevant to\ninducing failure. Failing test case reduction can also improve the efficiency\nof fault localization. These considerations have prompted researchers to study\nthe reduction process, the reduction output, and the removed entities. Christi\net al. studied test reduction using a tool called ReduSharptor for C# tests.\nThey considered the test to be an Abstract Syntax Tree (AST). Based on that,\nthey studied the reduction outcome and removed entities in terms of Leaf nodes\nand Non-Leaf nodes of the AST. They claimed that (1) leaf nodes are removed in\nlarge numbers, and (2) the probability of removal is slightly higher than\nnon-leaf nodes. We replicate their results using a different test case\nreduction tool, ReduJavator, for Java unit tests. We evaluate test reduction\nusing 30 randomly chosen bugs from the Defects4J database and 30 mutants for 6\nopen-source projects. Our results confirm their first claim: leaf nodes are\nremoved in large numbers. Our results are inconclusive regarding their second\nclaim; we cannot confirm that the probability of removal is higher for non-leaf\nnodes.",
        "We show that the consistent application of the rules of quantum mechanics to\ncosmological systems inevitably results in the so-called multiverse states in\nwhich neither the background spacetime nor the inhomogeneous perturbation are\nin definite states. We study the multiverse states as perturbations to the\nusually employed so-called Born-Oppenheimer states that are products of a wave\nfunction of the background and a wave function of the perturbation. The\nobtained corrections involve integrals over \\emph{virtual backgrounds} that\nrepresent the effect of quantum background fluctuations on the perturbation\nstate. They resemble loop corrections in quantum field theory. This approach\ndemonstrates the inevitable existence of very specific non-Gaussian features in\nprimordial fluctuations. We express the resulting non-Gaussian perturbation as\na nonlinear function of the Gaussian perturbation obtained within the\nBorn-Oppenheimer approximation, and compute its trispectrum, to show that the\nmultiverse scenario leads to testable and distinct signatures in cosmological\nperturbations. Our approach applies both to inflationary and alternative\ncosmologies.",
        "In this paper we propose a new paradigm for cosmology: a time dependent\nscalar condensate background originated from the quadratic $(R + \\alpha R^2)$\nStarobinski model, where $R$ is the Ricci scalar and $\\alpha$ the coupling\nconstant. In weak gravity limit the system decouples into a conventional\ngraviton and a higher derivative scalar. It was shown earlier through works\nfrom our group, \\cite{ssg,sg,us}, that the latter can sustain an oscillatory\nlowest energy configuration or a {\\it{Geometric Condensate}} as it consists\nentirely of metric degrees of freedom. In the present work, we study\nGravitational Wave propagation in this condensate background. We show that the\nexplicit time dependent nature of the condensate can generate curvature and\nradiation-like contributions in the scale factor evolution in FLRW cosmology.\nSubsequently the condensate leaves its signature on the Gravitational Wave\nprofile as it propagates in the condensate modified FLRW spacetime. The wave\nprofile is calculated analytically in terms of Whittaker functions. The main\nnovelty of the Geometric Condensate scheme is that no external (condensate)\nmatter from outside has been considered.",
        "The total lunar eclipse on March 14, 2025 UT occurs nearly exactly 521 years\n(one Hypersaros) after a similar eclipse on March 1, 1504 UT that is renowned\nfor its importance to the voyage of Columbus to Jamaica. Eclipses separated by\na Hypersaros have similar depths, appear very close to the same location in the\nsky, and occur at nearly the same time of year. This paper summarizes the\nresults from a search for analogous cycles within the Five Millennium Catalogs\nof Lunar and Solar Eclipses. Under the two simple constraints of similar\neclipse dates relative to the vernal equinox and similar paths of the Moon\nthrough the Earth's shadow, the most common time intervals between lunar\neclipses separated by less than 1000 years are the 521-year Hypersaros and a\n633-yr period of the Icosa-Inex-Triple-Saros (IITS). Notable cycles at longer\nperiods occur at 1154, 1284, 1787, 1917, and 2308 years.",
        "Recent approaches to jointly reconstruct 3D humans and objects from a single\nRGB image represent 3D shapes with template-based or coarse models, which fail\nto capture details of loose clothing on human bodies. In this paper, we\nintroduce a novel implicit approach for jointly reconstructing realistic 3D\nclothed humans and objects from a monocular view. For the first time, we model\nboth the human and the object with an implicit representation, allowing to\ncapture more realistic details such as clothing. This task is extremely\nchallenging due to human-object occlusions and the lack of 3D information in 2D\nimages, often leading to poor detail reconstruction and depth ambiguity. To\naddress these problems, we propose a novel attention-based neural implicit\nmodel that leverages image pixel alignment from both the input human-object\nimage for a global understanding of the human-object scene and from local\nseparate views of the human and object images to improve realism with, for\nexample, clothing details. Additionally, the network is conditioned on semantic\nfeatures derived from an estimated human-object pose prior, which provides 3D\nspatial information about the shared space of humans and objects. To handle\nhuman occlusion caused by objects, we use a generative diffusion model that\ninpaints the occluded regions, recovering otherwise lost details. For training\nand evaluation, we introduce a synthetic dataset featuring rendered scenes of\ninter-occluded 3D human scans and diverse objects. Extensive evaluation on both\nsynthetic and real-world datasets demonstrates the superior quality of the\nproposed human-object reconstructions over competitive methods.",
        "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https:\/\/github.com\/taco-group\/Re-Align.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Cooperative perception enhances the individual perception capabilities of\nautonomous vehicles (AVs) by providing a comprehensive view of the environment.\nHowever, balancing perception performance and transmission costs remains a\nsignificant challenge. Current approaches that transmit region-level features\nacross agents are limited in interpretability and demand substantial bandwidth,\nmaking them unsuitable for practical applications. In this work, we propose\nCoopDETR, a novel cooperative perception framework that introduces object-level\nfeature cooperation via object query. Our framework consists of two key\nmodules: single-agent query generation, which efficiently encodes raw sensor\ndata into object queries, reducing transmission cost while preserving essential\ninformation for detection; and cross-agent query fusion, which includes Spatial\nQuery Matching (SQM) and Object Query Aggregation (OQA) to enable effective\ninteraction between queries. Our experiments on the OPV2V and V2XSet datasets\ndemonstrate that CoopDETR achieves state-of-the-art performance and\nsignificantly reduces transmission costs to 1\/782 of previous methods.",
        "Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap.",
        "Advancements in robotics have opened possibilities to automate tasks in\nvarious fields such as manufacturing, emergency response and healthcare.\nHowever, a significant challenge that prevents robots from operating in\nreal-world environments effectively is out-of-distribution (OOD) situations,\nwherein robots encounter unforseen situations. One major OOD situations is when\nrobots encounter faults, making fault adaptation essential for real-world\noperation for robots. Current state-of-the-art reinforcement learning\nalgorithms show promising results but suffer from sample inefficiency, leading\nto low adaptation speed due to their limited ability to generalize to OOD\nsituations. Our research is a step towards adding hardware fault tolerance and\nfast fault adaptability to machines. In this research, our primary focus is to\ninvestigate the efficacy of generative flow networks in robotic environments,\nparticularly in the domain of machine fault adaptation. We simulated a robotic\nenvironment called Reacher in our experiments. We modify this environment to\nintroduce four distinct fault environments that replicate real-world\nmachines\/robot malfunctions. The empirical evaluation of this research\nindicates that continuous generative flow networks (CFlowNets) indeed have the\ncapability to add adaptive behaviors in machines under adversarial conditions.\nFurthermore, the comparative analysis of CFlowNets with reinforcement learning\nalgorithms also provides some key insights into the performance in terms of\nadaptation speed and sample efficiency. Additionally, a separate study\ninvestigates the implications of transferring knowledge from pre-fault task to\npost-fault environments. Our experiments confirm that CFlowNets has the\npotential to be deployed in a real-world machine and it can demonstrate\nadaptability in case of malfunctions to maintain functionality."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication",
    "start_abstract":"Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Brain\u2013Computer Interface Spellers: A Review"
      ],
      "abstract":[
        "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Asynchronous Hebbian\/anti-Hebbian networks",
        "Electrophysiological Investigation of Insect Pain Threshold",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "The Role of Affective States in Computational Psychiatry",
        "How constraints on editing affects cultural evolution",
        "Vagus nerve stimulation as a modulator of feedforward and feedback\n  neural transmission",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "Active filtering: a predictive function of recurrent circuits of sensory\n  cortex",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "A Relativistic Theory of Consciousness (shortened version)",
        "Deviance Detection and Regularity Sensitivity in Dissociated Neuronal\n  Cultures",
        "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision",
        "Applying a star formation model calibrated on high-resolution\n  interstellar medium simulations to cosmological simulations of galaxy\n  formation",
        "Neural network-based prediction of particle-induced fission cross\n  sections for r-process nucleosynthesis trained with dynamical reaction models",
        "Improving the trivial bound for $\\ell$-torsion in class groups",
        "Real-time simulation of jet energy loss and entropy production in\n  high-energy scattering with matter",
        "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA). III. NGC 288\n  as the first Splashed globular cluster",
        "Compare Similarities Between DNA Sequences Using Permutation-Invariant\n  Quantum Kernel",
        "Asymptotic behavior of clusters in hierarchical species sampling models",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "B-fields And dust in interstelLar fiLAments using Dust POLarization\n  (BALLAD-POL): III. Grain alignment and disruption mechanisms in G34.43+0.24\n  using polarization observations from JCMT\/POL-2",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Nearsightedness in Materials with Indirect Band Gap",
        "A tracking algorithm for finite-size particles",
        "Female and Combined Male-Female Injury Risk Functions for the Anterior\n  Pelvis Under Frontal Lap Belt Loading Conditions",
        "Cohering Disaggregation and Uncertainty Quantification for Spatially\n  Misaligned Data",
        "Cooperation and competing of antipolar charge order and superconducting\n  states in a quasi-2D superatomic metallic crystal"
      ],
      "abstract":[
        "Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
        "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Vagus nerve stimulation (VNS) has emerged as a promising therapeutic\nintervention across various neurological and psychiatric conditions, including\nepilepsy, depression, and stroke rehabilitation; however, its mechanisms of\naction on neural circuits remain incompletely understood. Here, we present a\nnovel theoretical framework based on predictive coding that conceptualizes VNS\neffects through differential modulation of feedforward and feedback neural\ncircuits. Based on recent evidence, we propose that VNS shifts the balance\nbetween feedforward and feedback processing through multiple neuromodulatory\nsystems, resulting in enhanced feedforward signal transmission. This framework\nintegrates anatomical pathways, receptor distributions, and physiological\nresponses to explain the influence of the VNS on neural dynamics across\ndifferent spatial and temporal scales. VNS may facilitate neural plasticity and\nadaptive behavior through acetylcholine and noradrenaline (norepinephrine),\nwhich differentially modulate feedforward and feedback signaling. This\nmechanistic understanding serves as a basis for interpreting the cognitive and\ntherapeutic outcomes across different clinical conditions. Our perspective\nprovides a unified theoretical framework for understanding circuit-specific VNS\neffects and suggests new directions for investigating their therapeutic\nmechanisms.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "Our brains encode many features of the sensory world into memories: we can\nsing along with songs we have heard before, interpret spoken and written\nlanguage composed of words we have learned, and recognize faces and objects.\nWhere are these memories stored? Each area of the cerebral cortex has a huge\nnumber of local, recurrent, excitatory-excitatory synapses, as many as 500\nmillion per cubic millimeter. Here I review evidence that cortical recurrent\nconnectivity in sensory cortex is a substrate for sensory memories. Evidence\nsuggests that the local recurrent network encodes the structure of natural\nsensory input, and that it does so via active filtering, transforming network\ninputs to boost or select those associated with natural sensation. This is a\nform of predictive processing, in which the cortical recurrent network\nselectively amplifies some input patterns and attenuates others, and a form of\nmemory.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "This paper is a shortened version of the full paper that was published in the\njournal Frontiers of Psychology in May 2022. In recent decades, the scientific\nstudy of consciousness has significantly increased our understanding of this\nelusive phenomenon. Yet, despite critical development in our understanding of\nthe functional side of consciousness, we still lack a fundamental theory\nregarding its phenomenal aspect. The phenomenal aspect of consciousness is the\nfirst-person answer to what it is like question, and it has thus far proved\nrecalcitrant to direct scientific investigation. The question of how the brain,\nor any cognitive system, can create conscious experience out of neural\nrepresentations poses a great conundrum to science. Naturalistic dualists argue\nthat it is composed of a primitive, private, nonreductive element of reality.\nIllusionists, on the other hand, argue that it is merely a cognitive illusion.\nWe contend that both the dualist and illusionist positions are flawed because\nthey tacitly assume consciousness to be an absolute property that does not\ndepend on the observer. We developed a conceptual and a mathematical argument\nfor a relativistic theory of consciousness in which a system either has or does\nnot have phenomenal consciousness with respect to some observer. According to\nthe theory, Phenomenal consciousness is neither private nor delusional, just\nrelativistic. In the frame of reference of the cognitive system, it will be\nobservable (first-person perspective) and in other frame of reference it will\nnot (third-person perspective). These two cognitive frames of reference are\nboth correct, just as in the case of an observer that claims to be at rest\nwhile another will claim that the observer has constant velocity. Neither\nobserver position can be privileged, as they both describe the same underlying\nreality.",
        "Understanding how neural networks process complex patterns of information is\ncrucial for advancing both neuroscience and artificial intelligence. To\ninvestigate fundamental principles of neural computation, we studied\ndissociated neuronal cultures, one of the most primitive living neural\nnetworks, on high-resolution CMOS microelectrode arrays and tested whether the\ndissociated culture exhibits regularity sensitivity beyond mere\nstimulus-specific adaptation and deviance detection. In oddball electrical\nstimulation paradigms, we confirmed that the neuronal culture produced mismatch\nresponses (MMRs) with true deviance detection beyond mere adaptation. These\nMMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to\nmismatch negativity (MMN) in humans, which is known to have true deviance\ndetection properties. Crucially, we also showed sensitivity to the statistical\nregularity of stimuli, a phenomenon previously observed only in intact brains:\nthe MMRs in a predictable, periodic sequence were smaller than those in a\ncommonly used sequence in which the appearance of the deviant stimulus was\nrandom and unpredictable. These results challenge the traditional view that a\nhierarchically structured neural network is required to process complex\ntemporal patterns, suggesting instead that deviant detection and regularity\nsensitivity are inherent properties arising from the primitive neural network.\nThey also suggest new directions for the development of neuro-inspired\nartificial intelligence systems, emphasizing the importance of incorporating\nadaptive mechanisms and temporal dynamics in the design of neural networks.",
        "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision.",
        "Modern high-resolution simulations of the interstellar medium (ISM) have\nshown that key factors in governing star formation are the competing influences\nof radiative dissipation, pressure support driven by stellar feedback, and the\nrelentless pull of gravity. Cosmological simulations of galaxy formation, such\nas IllustrisTNG or ASTRID, are however not able to resolve this physics in\ndetail and therefore need to rely on approximate treatments. These have often\ntaken the form of empirical subgrid models of the ISM expressed in terms of an\neffective equation of state (EOS) that relates the mean ISM pressure to the\nmean gas density. Here we seek to improve these heuristic models by directly\nfitting their key ingredients to results of the high-resolution TIGRESS\nsimulations, which have shown that the dynamical equilibrium of the ISM can be\nunderstood in terms of a pressure-regulated, feedback modulated (PRFM) model\nfor star formation. Here we explore a simple subgrid model that draws on the\nPRFM concept but uses only local quantities. It accurately reproduces PRFM for\npure gas disks, while it predicts slightly less star formation than PRFM in the\npresence of an additional thin stellar disk. We compare the properties of this\nmodel with the older Springel and Hernquist and TNG prescriptions, and apply\nall three to isolated simulations of disk galaxies as well as to a set of\nhigh-resolution zoom-in simulations carried out with a novel 'multi-zoom'\ntechnique that we introduce in this study. The softer EOS implied by TIGRESS\nproduces substantially thinner disk galaxies, which has important ramifications\nfor disk stability and galaxy morphology. The total stellar mass of galaxies is\nhowever hardly modified at low redshift, reflecting the dominating influence of\nlarge-scale gaseous inflows and outflows to galaxies, which are not sensitive\nto the EOS itself",
        "Large-scale computations of fission properties play a crucial role in nuclear\nreaction network calculations simulating rapid neutron-capture process\n(r-process) nucleosynthesis. Due to the large number of fissioning nuclei\ncontributing to the r-process, a description of particle-induced fission\nreactions is computationally challenging. In this work, we use theoretical\ncalculations based on the INCL+ABLA models to train neural networks (NN). The\nresults for the prediction of proton-induced spallation reactions, in\nparticular fission, utilizing a large variety of NN models across the\nhyper-parameter space are presented, which are relevant for r-process\ncalculations.",
        "For any number field $K$ with $D_K=|\\mathrm{Disc}(K)|$ and any integer $\\ell\n\\geq 2$, we improve over the commonly cited trivial bound\n$|\\mathrm{Cl}_K[\\ell]| \\leq |\\mathrm{Cl}_K| \\ll_{[K:\\mathbb{Q}],\\varepsilon}\nD_K^{1\/2+\\varepsilon}$ on the $\\ell$-torsion subgroup of the class group of $K$\nby showing that $|\\mathrm{Cl}_K[\\ell]| = o_{[K:\\mathbb{Q}],\\ell}(D_K^{1\/2})$.\nIn fact, we obtain an explicit log-power saving. This is the first general\nunconditional saving over the trivial bound that holds for all $K$ and all\n$\\ell$.",
        "In analogy to high-energy nuclear scattering experiments, we study a\nreal-time scattering process between a propagating state and a dense target in\n$1+1$-d massive QED. In our setup, we identify three distinct regimes that\nqualitatively characterize the evolution: for a dilute medium, the incoming\nprobe state evolves nearly ballistically; in an intermediate setting, it\ntraverses the matter, locally exciting it; and for dense targets, one\napproaches a black-disk limit, where the matter acts as a strong wall\npotential. We find evidence that the probe's energy loss rate scales linearly\nwith the path length in the medium, and we study how the entanglement entropy\nreveals the mixing between the probe and medium states. With the goal of one\nday replicating high-energy nuclear experiments in quantum devices, we briefly\ndiscuss how the current tensor network-based simulations can be translated to a\nquantum simulator.",
        "The globular clusters (GCs) system of the Milky Way (MW) comprises a mixture\nof both in situ and accreted clusters. Tracing the origin of GCs provides\ninvaluable insights into the formation history of the MW. However, reconciling\ndiverse strands of evidence is often challenging: a notable example is NGC 288,\nwhere despite significant efforts in the literature, the available\nchrono-chemodynamical data have yet to provide a definitive conclusion\nregarding its origin. On one side, all post-Gaia dynamical studies indicate an\naccreted origin for NGC 288 from in the Gaia-Sausage-Enceladus (GSE) dwarf\ngalaxy. On the other, NGC 288 has been found to be 2.5 Gyr older than other GSE\nGCs at the same metallicity, this suggesting a different, possibly in situ\norigin. In this work, we address the unresolved question on the origin of NGC\n288 by analyzing its chrono-chemical properties in an unprecedentedly\nhomogeneous framework. First, we compare the location of NGC 288 in the\nage-metallicity plane with that of other two in situ GCs at similar\nmetallicity, namely NGC 6218 and NGC 6362. The age estimates obtained within\nthe homogeneous framework of the CARMA collaboration show that the three\nclusters are coeval, this reinforcing the contrast with the dynamical\ninterpretation. Then, we compare the abundances with the sample of in situ and\naccreted clusters at similar metallicity presented in Ceccarelli et al. 2024,\nfinding again consistency with the chemistry of in situ systems. To reconcile\nthese results with its orbital properties, we propose a scenario where NGC 288\nformed in the proto-disc of the MW, and then was dynamically heated by the\ninteraction with the GSE merger, a fate similar to that of proto-disc stars\nexperiencing the so-called Splash event. NGC 288 therefore demonstrates the\nimportance of a homogeneous chrono-chemodynamical information in the\ninterpretation of the origin of MW GCs.",
        "Computing the similarity between two DNA sequences is of vital importance in\nbioscience. However, traditional computational methods can be\nresource-intensive due to the enormous sequence length encountered in practice.\nRecently, applied quantum algorithms have been anticipated to provide potential\nadvantages over classical approaches. In this paper, we propose a\npermutation-invariant variational quantum kernel method specifically designed\nfor DNA comparison. To represent the four nucleotide bases in DNA sequences\nwith quantum states, we introduce a novel, theoretically motivated encoding\nscheme: the four distinct bases are encoded using the states of symmetric,\ninformationally complete, positive operator-valued measures (SIC-POVMs). This\nencoding ensures mutual equality: each pair of symbols is equidistant on the\nBloch sphere. Also, since permutation invariance is inherent to common DNA\nsimilarity measures such as Levenshtein distance, we realize it by using a\nspecially designed parameterized quantum layer. We show that our novel encoding\nmethod and parameterized layers used in the quantum kernel model can\neffectively capture the symmetric characteristics of the pairwise DNA sequence\ncomparison task. We validate our model through numerical experiments, which\nyield promising results on length-$8$ DNA sequences.",
        "Consider a sample of size $N$ from a population governed by a hierarchical\nspecies sampling model. We study the large $N$ asymptotic behavior of the\nnumber ${\\bf K}_N$ of clusters and the number ${\\bf M}_{r,N}$ of clusters with\nfrequency $r$ in the sample. In particular, we show almost sure and $L^p$\nconvergence for ${\\bf M}_{r,N}$, obtain Gaussian fluctuation theorems for ${\\bf\nK}_N$, and establish large deviation principles for both ${\\bf K}_N$ and ${\\bf\nM}_{r,N}$. Our approach relies on a random sample size representation of the\nnumber of clusters through the corresponding non-hierarchical species sampling\nmodel.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "Polarization of starlight and thermal dust emission due to aligned\nnon-spherical grains helps us to trace magnetic field (B-field) morphology in\nmolecular clouds and to study grain alignment mechanisms. In this work, we\nstudy grain alignment and disruption mechanisms in a filamentary infrared dark\ncloud G34.43+0.24 using thermal dust polarization observations from JCMT\/POL-2\nat 850 $\\mu\\text{m}$. We study in three sub-regions as North harboring MM3\ncore, Center harboring MM1 and MM2 cores and South having no core. We find the\ndecrease in polarization fraction P with increasing total intensity and gas\ncolumn density, known as polarization hole. To disentangle the effect of\nmagnetic field tangling on the polarization hole, we estimate the polarization\nangle dispersion function. We find depolarizations in North and Center regions\nare due to decrease in net alignment efficiency of grains but in South region,\neffect of magnetic field tangling is significant to cause depolarization. To\ntest whether RAdiative Torque (RAT) mechanism can reproduce the observational\ndata, we calculate minimum alignment and disruption sizes of grains using RAT\ntheory and our study finds that RAT alignment mechanism can explain the\ndepolarizations in North and Center regions where B-field tangling effect is\nless important, except for core regions. We find hints of RAdiative Torque\nDisruption (RAT-D) in the core regions of MM3 in North, MM1 and MM2 in Center.\nWe also find that the high P value of around 8-20% in the outer regions of the\nfilament can be explained potentially by magnetically enhanced RAT alignment\nmechanism.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "We investigate the nearsightedness property in the linear tight binding model\nat zero Fermi-temperature. We focus on the decay property of the density matrix\nfor materials with indirect band gaps. By representing the density matrix in\nreciprocal space, we establish a qualitatively sharp estimate for the\nexponential decay rate in homogeneous systems. An extending result under\nperturbations is also derived. This work refines the estimates presented in\n(Ortner, Thomas & Chen 2020), particularly for systems with small band gaps.",
        "Particle-wall interactions play a crucially important role in various\napplications such as microfluidic devices for cell sorting, particle\nseparation, entire class of hydrodynamic filtration and its derivatives, etc.\nYet, accurate implementation of interactions between wall and finite-size\nparticle is not trivial when working with the currently available particle\ntracking algorithms\/packages as they typically work with point-wise particles.\nHerein, we report a particle tracking algorithm that takes into account\ninteractions between particles of finite size and solid objects existing inside\ncomputational domain. A particle is modeled as a set of circumferential points\non its perimeter. While fluid-particle interactions are captured during the\ntrack of particle center, interactions between particle and nearby solid\nobjects are modeled explicitly by examining circumferential points and applying\na reflection scheme as needed to ensure impenetrability of solid objects. We\nalso report a modified variant of auxiliary structured grid method to locate\nhosting cells, which in conjunction with a boundary condition scheme enables\nthe capture of interactions between particle and solid objects. As a\nproof-of-concept, we numerically and experimentally study the motion of\nparticles within a microfluidic deterministic lateral displacement device. The\nmodeling results successfully demonstrate the zig-zag and bumping displacement\nmodes observed in our experiments. We also study a microfluidic device with\npinched flow numerically and validate our results against experimental data\nfrom the literature. By demonstrating an almost 8x speedup on a system with 8\nPerformance threads, our investigations suggest that the particle tracking\nalgorithm and its implementation code can benefit from parallel processing on\nmulti-thread systems by using the OpenMP application programming interface.",
        "Purpose: Iliac wing fractures due to lap belt loading have been observed in\nlaboratory settings for 50 years and recent data suggest they are also\noccurring in the field. Automated driving systems (ADS) and other occupant\ncompartment advancements are expected to offer enhanced flexibility in seating\norientation, which could place a greater reliance on the seatbelt to restrain\noccupants. Such changes may increase seatbelt loads and create new challenges\nin successfully restraining occupants and mitigating injury to areas such as\nthe pelvis. Injury criteria exist for component-level male iliac wing fractures\nresulting from frontal lap belt loading, but not for females. Methods: This\nstudy explored female iliac wing fracture tolerance in the same loading\nenvironment as a previous study that explored the fracture tolerance of\nisolated male iliac wings. Male and female fracture data were combined to\nevaluate the effect of sex. Injury risk functions were created by fitting\nWeibull survival models to data that integrated censored and exact failure\nobservations. Results: Twenty female iliac wings were tested; fourteen of them\nsustained fracture with known failure forces (exact), but the remaining six\nwings either (1) did not fracture, or (2) fractured after an event that changed\nthe boundary conditions (right censored). The fracture tolerance of the tested\nspecimens ranged widely (1134 - 8759 N) and averaged 4240 N (SD 2516 N).\nConclusion: Female data and combined male-female data were analyzed. Age was\nthe only covariate investigated in this study that had a statistically\nsignificant effect and improved the predictive performance of the models.",
        "Spatial misalignment problems arise from both data aggregation and attempts\nto align misaligned data, leading to information loss. We propose a Bayesian\ndisaggregation framework that links misaligned data to a continuous domain\nmodel using an iteratively linearised integration method via integrated nested\nLaplace approximation (INLA). The framework supports point pattern and\naggregated count models under four covariate field scenarios: \\textit{Raster at\nFull Resolution (RastFull), Raster Aggregation (RastAgg), Polygon Aggregation\n(PolyAgg), and Point Values (PointVal)}. The first three involve aggregation,\nwhile the latter two have incomplete fields. For PolyAgg and PointVal, we\nestimate the full covariate field using \\textit{Value Plugin, Joint\nUncertainty, and Uncertainty Plugin} methods, with the latter two accounting\nfor uncertainty propagation. These methods demonstrate superior performance,\nand remain more robust even under model misspecification (i.e.\\ modelling a\nnonlinear field as linear).\n  In landslide studies, landslide occurrences are often aggregated into counts\nbased on slope units, reducing spatial detail. The results indicate that point\npattern observations and full-resolution covariate fields should be\nprioritized. For incomplete fields, methods incorporating uncertainty\npropagation are preferred. This framework supports landslide susceptibility and\nother spatial mapping, integrating seamlessly with INLA-extension packages.",
        "Exploring various unexpected new quantum states and their corresponding\nextraordinary physics in low dimensional quantum materials, and investing them\ninto application fields, is a primary concern of condensed matter physics.\nPreviously, we performed joint experiments and theoretical calculations to\ninvestigate a superatomic crystal of Au6Te12Se8 with low symmetry, which stacks\nthrough non-covalent inter-cube quasi-bonds. Au6Te12Se8 exhibits a triple-cube\ncharge density wave and spatially polarized metallic states interweaving at 9\nK. In addition, it undergoes a BKT phase transition at 2.8 K to form a\nquasi-two-dimensional superconducting state. The subsequent high-pressure\nexperimental results indicate that the two quantum states above compete with\nsuperconductivity. Here, we experimentally revealed competition and coexistence\namong superconductivity, triple-cube charge density waves, and polarized\nmetallic states during further temperature-decreasing process, as examined\nusing ultra-low temperature scanning tunneling microscopy\/spectroscopy,\ntransport measurement, and Raman spectra. An extraordinary inversion symmetry\nbroken emerged inside the triple-cube-period of the original CDW at 300 mK,\nleading to the polarized metallic states transforming into parallel\npolarization states. The evidence of spontaneous polarization coexisting with\nsuperconductivity in real space is extremely rare and has been revealed by STM.\nIn addition, transport and Raman measurements indicate that there are multiple\nphase transition temperatures from 80K to that below the superconducting\ntransition temperature of Au6Te12Se8, which may also contain more unknown\nexotic quantum states, providing a platform for further exploration of\nintriguing physical properties."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials",
    "start_abstract":"The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "A deep-learning approach to realizing functionality in nanoelectronic devices"
      ],
      "abstract":[
        "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Boundary Prompting: Elastic Urban Region Representation via Graph-based\n  Spatial Tokenization",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
        "Zweistein: A Dynamic Programming Evaluation Function for Einstein\n  W\\\"urfelt Nicht!",
        "NS-Gym: Open-Source Simulation Environments and Benchmarks for\n  Non-Stationary Markov Decision Processes",
        "Assessing AI Adoption and Digitalization in SMEs: A Framework for\n  Implementation",
        "Model-Free RL Agents Demonstrate System 1-Like Intentionality",
        "Think Smarter not Harder: Adaptive Reasoning with Inference Aware\n  Optimization",
        "Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education",
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "Cognitive-Aligned Document Selection for Retrieval-augmented Generation",
        "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
        "Governing AI Agents",
        "On conductor submonoids of factorial monoids",
        "Depth of powers of edge ideals of edge-weighted integrally closed cycles",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Spatial-temporal models for forest inventory data",
        "Supervised Manifold Learning for Functional Data",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Encrypted Vector Similarity Computations Using Partially Homomorphic\n  Encryption: Applications and Performance Analysis",
        "FinTSB: A Comprehensive and Practical Benchmark for Financial Time\n  Series Forecasting",
        "Integrative Learning of Intensity Fluctuations of Quantum Dots under\n  Excitation via a Tailored Mixture Hidden Markov Model",
        "Contracting Strategies for Electrolyzers to Secure Grid Connection: The\n  Dutch Case",
        "Naked Eye Three-dimensional Display System Based on Time-multiplexed\n  Technology",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Unfitted boundary algebraic equation method based on difference\n  potentials and lattice Green's function in 3D",
        "PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery",
        "Ultrafast pulsed laser evaluation of Single Event Transients in\n  opto-couplers"
      ],
      "abstract":[
        "Urban region representation is essential for various applications such as\nurban planning, resource allocation, and policy development. Traditional\nmethods rely on fixed, predefined region boundaries, which fail to capture the\ndynamic and complex nature of real-world urban areas. In this paper, we propose\nthe Boundary Prompting Urban Region Representation Framework (BPURF), a novel\napproach that allows for elastic urban region definitions. BPURF comprises two\nkey components: (1) A spatial token dictionary, where urban entities are\ntreated as tokens and integrated into a unified token graph, and (2) a region\ntoken set representation model which utilize token aggregation and a\nmulti-channel model to embed token sets corresponding to region boundaries.\nAdditionally, we propose fast token set extraction strategy to enable online\ntoken set extraction during training and prompting. This framework enables the\ndefinition of urban regions through boundary prompting, supporting varying\nregion boundaries and adapting to different tasks. Extensive experiments\ndemonstrate the effectiveness of BPURF in capturing the complex characteristics\nof urban regions.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
        "This paper introduces Zweistein, a dynamic programming evaluation function\nfor Einstein W\\\"urfelt Nicht! (EWN). Instead of relying on human knowledge to\ncraft an evaluation function, Zweistein uses a data-centric approach that\neliminates the need for parameter tuning. The idea is to use a vector recording\nthe distance to the corner of all pieces. This distance vector captures the\nessence of EWN. It not only outperforms many traditional EWN evaluation\nfunctions but also won first place in the TCGA 2023 competition.",
        "In many real-world applications, agents must make sequential decisions in\nenvironments where conditions are subject to change due to various exogenous\nfactors. These non-stationary environments pose significant challenges to\ntraditional decision-making models, which typically assume stationary dynamics.\nNon-stationary Markov decision processes (NS-MDPs) offer a framework to model\nand solve decision problems under such changing conditions. However, the lack\nof standardized benchmarks and simulation tools has hindered systematic\nevaluation and advance in this field. We present NS-Gym, the first simulation\ntoolkit designed explicitly for NS-MDPs, integrated within the popular\nGymnasium framework. In NS-Gym, we segregate the evolution of the environmental\nparameters that characterize non-stationarity from the agent's decision-making\nmodule, allowing for modular and flexible adaptations to dynamic environments.\nWe review prior work in this domain and present a toolkit encapsulating key\nproblem characteristics and types in NS-MDPs. This toolkit is the first effort\nto develop a set of standardized interfaces and benchmark problems to enable\nconsistent and reproducible evaluation of algorithms under non-stationary\nconditions. We also benchmark six algorithmic approaches from prior work on\nNS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to\nassess the adaptability and robustness of their decision-making algorithms to\nnon-stationary conditions.",
        "The primary objective of this research is to examine the current state of\ndigitalization and the integration of artificial intelligence (AI) within small\nand medium-sized enterprises (SMEs) in Italy. There is a significant gap\nbetween SMEs and large corporations in their use of AI, with SMEs facing\nnumerous barriers to adoption. This study identifies critical drivers and\nobstacles to achieving intelligent transformation, proposing a framework model\nto address key challenges and provide actionable guidelines",
        "This paper argues that model-free reinforcement learning (RL) agents, while\nlacking explicit planning mechanisms, exhibit behaviours that can be analogised\nto System 1 (\"thinking fast\") processes in human cognition. Unlike model-based\nRL agents, which operate akin to System 2 (\"thinking slow\") reasoning by\nleveraging internal representations for planning, model-free agents react to\nenvironmental stimuli without anticipatory modelling. We propose a novel\nframework linking the dichotomy of System 1 and System 2 to the distinction\nbetween model-free and model-based RL. This framing challenges the prevailing\nassumption that intentionality and purposeful behaviour require planning,\nsuggesting instead that intentionality can manifest in the structured, reactive\nbehaviours of model-free agents. By drawing on interdisciplinary insights from\ncognitive psychology, legal theory, and experimental jurisprudence, we explore\nthe implications of this perspective for attributing responsibility and\nensuring AI safety. These insights advocate for a broader, contextually\ninformed interpretation of intentionality in RL systems, with implications for\ntheir ethical deployment and regulation.",
        "Solving mathematics problems has been an intriguing capability of large\nlanguage models, and many efforts have been made to improve reasoning by\nextending reasoning length, such as through self-correction and extensive long\nchain-of-thoughts. While promising in problem-solving, advanced long reasoning\nchain models exhibit an undesired single-modal behavior, where trivial\nquestions require unnecessarily tedious long chains of thought. In this work,\nwe propose a way to allow models to be aware of inference budgets by\nformulating it as utility maximization with respect to an inference budget\nconstraint, hence naming our algorithm Inference Budget-Constrained Policy\nOptimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to\n``understand'' the difficulty of queries and allocate inference budgets to\nharder ones. With different inference budgets, our best models are able to have\na $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative\nimprovement) on MATH500 using $2.16$x and $4.32$x inference budgets\nrespectively, relative to LLaMA3.1 8B Instruct. These improvements are\napproximately $2$x those of self-consistency under the same budgets.",
        "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "Large language models (LLMs) inherently display hallucinations since the\nprecision of generated texts cannot be guaranteed purely by the parametric\nknowledge they include. Although retrieval-augmented generation (RAG) systems\nenhance the accuracy and reliability of generative models by incorporating\nexternal documents, these retrieved documents often fail to adequately support\nthe model's responses in practical applications. To address this issue, we\npropose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment\nRe\\textbf{trieval} for verifiable generation), which leverages an LLM to\ndynamically update queries and filter high-quality, reliable retrieval\ndocuments. Specifically, we parse the user query into its syntactic components\nand perform fine-grained grounded alignment with the retrieved documents. For\nquery components that cannot be individually aligned, we propose a dynamic\nsemantic compensation mechanism that iteratively refines and rewrites the query\nwhile continuously updating the retrieval results. This iterative process\ncontinues until the retrieved documents sufficiently support the query's\nresponse. Our approach introduces a novel criterion for filtering retrieved\ndocuments, closely emulating human strategies for acquiring targeted\ninformation. This ensures that the retrieved content effectively supports and\nverifies the generated outputs. On the ALCE benchmark, our method significantly\nsurpasses a wide range of baselines, achieving state-of-the-art performance.",
        "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.",
        "The field of AI is undergoing a fundamental transition from generative models\nthat can produce synthetic content to artificial agents that can plan and\nexecute complex tasks with only limited human involvement. Companies that\npioneered the development of language models have now built AI agents that can\nindependently navigate the internet, perform a wide range of online tasks, and\nincreasingly serve as AI personal assistants and virtual coworkers. The\nopportunities presented by this new technology are tremendous, as are the\nassociated risks. Fortunately, there exist robust analytic frameworks for\nconfronting many of these challenges, namely, the economic theory of\nprincipal-agent problems and the common law doctrine of agency relationships.\nDrawing on these frameworks, this Article makes three contributions. First, it\nuses agency law and theory to identify and characterize problems arising from\nAI agents, including issues of information asymmetry, discretionary authority,\nand loyalty. Second, it illustrates the limitations of conventional solutions\nto agency problems: incentive design, monitoring, and enforcement might not be\neffective for governing AI agents that make uninterpretable decisions and\noperate at unprecedented speed and scale. Third, the Article explores the\nimplications of agency law and theory for designing and regulating AI agents,\narguing that new technical and legal infrastructure is needed to support\ngovernance principles of inclusivity, visibility, and liability.",
        "We give affirmative answers to Conjecture 4.16 in [1] and to Conjecture 2.3\nin [3].",
        "This paper gives some exact formulas for the depth of powers of the edge\nideal of an edge-weighted integrally closed cycle.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "The USDA Forest Inventory and Analysis (FIA) program conducts a national\nforest inventory for the United States through a network of permanent field\nplots. FIA produces estimates of area averages\/totals for plot-measured forest\nvariables through design-based inference, assuming a fixed population and a\nprobability sample of field plot locations. The fixed-population assumption and\ncharacteristics of the FIA sampling scheme make it difficult to estimate change\nin forest variables over time using design-based inference. We propose\nspatial-temporal models based on Gaussian processes as a flexible tool for\nforest inventory data, capable of inferring forest variables and change thereof\nover arbitrary spatial and temporal domains. It is shown to be beneficial for\nthe covariance function governing the latent Gaussian process to account for\nvariation at multiple scales, separating spatially local variation from\necosystem-scale variation. We demonstrate a model for forest biomass density,\ninferring 20 years of biomass change within two US National Forests.",
        "Classification is a core topic in functional data analysis. A large number of\nfunctional classifiers have been proposed in the literature, most of which are\nbased on functional principal component analysis or functional regression. In\ncontrast, we investigate this topic from the perspective of manifold learning.\nIt is assumed that functional data lie on an unknown low-dimensional manifold,\nand we expect that better classifiers can be built upon the manifold structure.\nTo this end, we propose a novel proximity measure that takes the label\ninformation into account to learn the low-dimensional representations, also\nknown as the supervised manifold learning outcomes. When the outcomes are\ncoupled with multivariate classifiers, the procedure induces a family of new\nfunctional classifiers. In theory, we show that our functional classifier\ninduced by the $k$-NN classifier is asymptotically optimal. In practice, we\nshow that our method, coupled with several classical multivariate classifiers,\nachieves outstanding classification performance compared to existing functional\nclassifiers in both synthetic and real data examples.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "This paper explores the use of partially homomorphic encryption (PHE) for\nencrypted vector similarity search, with a focus on facial recognition and\nbroader applications like reverse image search, recommendation engines, and\nlarge language models (LLMs). While fully homomorphic encryption (FHE) exists,\nwe demonstrate that encrypted cosine similarity can be computed using PHE,\noffering a more practical alternative. Since PHE does not directly support\ncosine similarity, we propose a method that normalizes vectors in advance,\nenabling dot product calculations as a proxy. We also apply min-max\nnormalization to handle negative dimension values.\n  Experiments on the Labeled Faces in the Wild (LFW) dataset use DeepFace's\nFaceNet128d, FaceNet512d, and VGG-Face (4096d) models in a two-tower setup.\nPre-encrypted embeddings are stored in one tower, while an edge device captures\nimages, computes embeddings, and performs encrypted-plaintext dot products via\nadditively homomorphic encryption. We implement this with LightPHE, evaluating\nPaillier, Damgard-Jurik, and Okamoto-Uchiyama schemes, excluding others due to\nperformance or decryption complexity. Tests at 80-bit and 112-bit security\n(NIST-secure until 2030) compare PHE against FHE (via TenSEAL), analyzing\nencryption, decryption, operation time, cosine similarity loss, key\/ciphertext\nsizes.\n  Results show PHE is less computationally intensive, faster, and produces\nsmaller ciphertexts\/keys, making it well-suited for memory-constrained\nenvironments and real-world privacy-preserving encrypted similarity search.",
        "Financial time series (FinTS) record the behavior of human-brain-augmented\ndecision-making, capturing valuable historical information that can be\nleveraged for profitable investment strategies. Not surprisingly, this area has\nattracted considerable attention from researchers, who have proposed a wide\nrange of methods based on various backbones. However, the evaluation of the\narea often exhibits three systemic limitations: 1. Failure to account for the\nfull spectrum of stock movement patterns observed in dynamic financial markets.\n(Diversity Gap), 2. The absence of unified assessment protocols undermines the\nvalidity of cross-study performance comparisons. (Standardization Deficit), and\n3. Neglect of critical market structure factors, resulting in inflated\nperformance metrics that lack practical applicability. (Real-World Mismatch).\nAddressing these limitations, we propose FinTSB, a comprehensive and practical\nbenchmark for financial time series forecasting (FinTSF). To increase the\nvariety, we categorize movement patterns into four specific parts, tokenize and\npre-process the data, and assess the data quality based on some sequence\ncharacteristics. To eliminate biases due to different evaluation settings, we\nstandardize the metrics across three dimensions and build a user-friendly,\nlightweight pipeline incorporating methods from various backbones. To\naccurately simulate real-world trading scenarios and facilitate practical\nimplementation, we extensively model various regulatory constraints, including\ntransaction fees, among others. Finally, we conduct extensive experiments on\nFinTSB, highlighting key insights to guide model selection under varying market\nconditions. Overall, FinTSB provides researchers with a novel and comprehensive\nplatform for improving and evaluating FinTSF methods. The code is available at\nhttps:\/\/github.com\/TongjiFinLab\/FinTSBenchmark.",
        "Semiconductor nano-crystals, known as quantum dots (QDs), have garnered\nsignificant interest in various scientific fields due to their unique\nfluorescence properties. One captivating characteristic of QDs is their ability\nto emit photons under continuous excitation. The intensity of photon emission\nfluctuates during the excitation, and such a fluctuation pattern can vary\nacross different dots even under the same experimental conditions. What adding\nto the complication is that the processed intensity series are non-Gaussian and\ntruncated due to necessary thresholding and normalization. As such,\nconventional approaches in the chemistry literature, typified by single-dot\nanalysis of raw intensity data with Gaussian hidden Markov models (HMM), cannot\nmeet the many analytical challenges and may fail to capture any novel yet rare\nfluctuation patterns among QDs. Collaborating with scientists in the chemistry\nfield, we have developed an integrative learning approach to simultaneously\nanalyzing intensity series of multiple QDs. Our approach still inherits the HMM\nas the skeleton to model the intensity fluctuations of each dot, and based on\nthe data structure and the hypothesized collective behaviors of the QDs, our\napproach asserts that (i) under each hidden state, the normalized intensity\nfollows a 0\/1 inflated Beta distribution, (ii) the state distributions are\nshared across all the QDs, and (iii) the patterns of transitions can vary\nacross QDs. These unique features allow for a precise characterization of the\nintensity fluctuation patterns and facilitate the clustering of the QDs. With\nexperimental data collected on 128 QDs, our methods reveal several QD clusters\ncharacterized by unique transition patterns across three intensity states. The\nresults provide deeper insight into QD behaviors and their design\/application\npotentials.",
        "In response to increasing grid congestion in the Netherlands, non-firm\nconnection and transport agreements (CTAs) and capacity restriction contracts\n(CRCs) have been introduced, allowing consumer curtailment in exchange for grid\ntariff discounts or per-MW compensations. This study examines the interaction\nbetween an electrolyzer project, facing sizing and contracting decisions, and a\nnetwork operator, responsible for contract activations and determining grid\nconnection capacity, under the new Dutch regulations. The interaction is\nmodeled using two bilevel optimization problems with alternating\nleader-follower roles. Results highlight a trade-off between CRC income and\nnon-firm CTA tariff discounts, showing that voluntary congestion management by\nthe network operator increases electrolyzer profitability at CRC prices below\n10 euro per MW but reduces it at higher prices. Furthermore, the network\noperator benefits more from reacting to the electrolyzer owner's CTA decisions\nthan from leading the interaction at CRC prices above 10 euro per MW. Ignoring\nthe other party's optimization problem overestimates profits for both the\nnetwork operator and the electrolyzer owner, emphasizing the importance of\ncoordinated decision-making.",
        "Our group is developing a multi-user eye-tracked 3D display, an evolution of\nthe single-user eye-tracked 3D display that we have already successfully\ndeveloped. This display utilizes a slanted lenticular setup, where multiple\nperspective views are shown across the viewing field. Due to the constraints of\nthe lenticular lens parameters, identical views are repeated across the field,\nlimiting eye tracking to a single user. However, this limitation can be\naddressed using spatio-temporal multiplexing, where view zone groups are\npresented sequentially with a high frame rate liquid crystal display (LCD) and\ndriver, in combination with a synchronized directional light emitting diode\n(LED) array. In this paper, we describe the operation and results of the\nbacklight drive electronics, where a prototype using a white LED illumination\nmatrix, a simplified LCD panel, and a linear Fresnel lens array serves as a\ntest bed.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "This work presents an unfitted boundary algebraic equation (BAE) method for\nsolving three-dimensional elliptic partial differential equations on complex\ngeometries using finite difference on structured meshes. We demonstrate that\nreplacing finite auxiliary domains with free-space LGFs streamlines the\ncomputation of difference potentials, enabling matrix-free implementations and\nsignificant cost reductions. We establish theoretical foundations by showing\nthe equivalence between direct formulations in difference potentials framework\nand indirect single\/double layer formulations and analyzing their spectral\nproperties. The spectral analysis demonstrates that discrete double layer\nformulations provide better-conditioned systems for iterative solvers,\nsimilarly as in boundary integral method. The method is validated through\nmatrix-free numerical experiments on both Poisson and modified Helmholtz\nequations in 3D implicitly defined geometries, showing optimal convergence\nrates and computational efficiency. This framework naturally extends to\nunbounded domains and provides a foundation for applications to more complex\nsystems like Helmholtz and Stokes equations.",
        "Recent advancements in language-guided diffusion models for image editing are\noften bottle-necked by cumbersome prompt engineering to precisely articulate\ndesired changes. An intuitive alternative calls on guidance from in-the-wild\nimage exemplars to help users bring their imagined edits to life. Contemporary\nexemplar-based editing methods shy away from leveraging the rich latent space\nlearnt by pre-existing large text-to-image (TTI) models and fall back on\ntraining with curated objective functions to achieve the task. Though somewhat\neffective, this demands significant computational resources and lacks\ncompatibility with diverse base models and arbitrary exemplar count. On further\ninvestigation, we also find that these techniques restrict user control to only\napplying uniform global changes over the entire edited region. In this paper,\nwe introduce a novel framework for progressive exemplar-driven editing with\noff-the-shelf diffusion models, dubbed PIXELS, to enable customization by\nproviding granular control over edits, allowing adjustments at the pixel or\nregion level. Our method operates solely during inference to facilitate\nimitative editing, enabling users to draw inspiration from a dynamic number of\nreference images, or multimodal prompts, and progressively incorporate all the\ndesired changes without retraining or fine-tuning existing TTI models. This\ncapability of fine-grained control opens up a range of new possibilities,\nincluding selective modification of individual objects and specifying gradual\nspatial changes. We demonstrate that PIXELS delivers high-quality edits\nefficiently, leading to a notable improvement in quantitative metrics as well\nas human evaluation. By making high-quality image editing more accessible,\nPIXELS has the potential to enable professional-grade edits to a wider audience\nwith the ease of using any open-source image generation model.",
        "We build a 1064 nm fiber laser system-based testing facility for emulating\nSETs in different electronics components and ICs. Using these facilities, we\ntested the 4N35 optocoupler to observe SETs for the first time."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"A deep-learning approach to realizing functionality in nanoelectronic devices",
    "start_abstract":"Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
      ],
      "abstract":[
        "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Tunable spin and orbital torques in Cu-based magnetic heterostructures",
        "SALMON VR: Visualizing Light-Matter Dynamics",
        "Photo-induced spall failure of (111) twist grain boundaries in Ni\n  bicrystals",
        "Non-adiabaticity from first principles: the exact-factorization approach\n  for solids",
        "Ultrafast demagnetization dynamics of 4f antiferromagnets",
        "Predicting the N\\'eel temperatures in general helimagnetic materials: a\n  comparison between mean field theory, random phase approximation,\n  renormalized spin wave theory and classical Monte Carlo simulations",
        "Intrinsic higher-order topological states in 2D honeycomb Z_2 quantum\n  spin Hall insulators",
        "Microstructure-Aware Bayesian Materials Design",
        "Electrocatalyst discovery through text mining and multi-objective\n  optimization",
        "Electronic origin of stability of 2D 1H-phase Janus transition metal\n  dichalcogenides and beyond",
        "Taxonomy of amorphous ternary phase diagrams: the importance of\n  interaction parameters",
        "Generalized Bond Polarizability model for more accurate atomistic\n  modeling of Raman spectra",
        "Dzyalonshinskii-Moriya interaction in Fe5GeTe2 epitaxial thin films",
        "Skeletal Torus Actions and GKM Structures on Quiver Grassmannians of\n  String Representations",
        "Data Augmentation and Regularization for Learning Group Equivariance",
        "Nitrogen-Vacancy Centers in Epitaxial Laterally Overgrown Diamond:\n  Towards Up-scaling of Color Center-based Quantum Technologies",
        "Towards a complexity-theoretic dichotomy for TQFT invariants",
        "Stabilization of quantum properties under intrinsic decoherence in\n  presence of external magnetic fields",
        "Searching for Inflationary Physics with the CMB Trispectrum: 3.\n  Constraints from Planck",
        "Directional optical parametric amplification in a hyperbolic\n  metamaterial",
        "Laser cooled 137BaF molecules for measuring nuclear-spin-dependent\n  parity violation",
        "Interplay of ALP Couplings at a Muon Collider",
        "Change of some cropping systems in a long-term trial comparing different\n  systems: rationale and implications for statistical analysis",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Deep Learning-Powered Electrical Brain Signals Analysis: Advancing\n  Neurological Diagnostics",
        "Learning by Confusion: The Phase Diagram of the Holstein Model",
        "Decaying turbulence beneath surface waves",
        "Thermodynamic uncertainty relations for three-terminal systems with\n  broken time-reversal symmetry"
      ],
      "abstract":[
        "Current-induced torques originating from earth-abundant 3d elements offer a\npromising avenue for low-cost and sustainable spintronic memory and logic\napplications. Recently, orbital currents -- transverse orbital angular momentum\nflow in response to an electric field -- have been in the spotlight since they\nallow current-induced torque generation from 3d transition metals. Here, we\nreport a comprehensive study of the current-induced spin and orbital torques in\nCu-based magnetic heterostructures. We show that high torque efficiencies can\nbe achieved in engineered Ni80Fe20\/Cu bilayers where Cu is naturally oxidized,\nexceeding the ones found in the archetypical Co\/Pt. Furthermore, we demonstrate\nsign and amplitude control of the damping-like torque by manipulating the\noxidation state of Cu via solid-state gating. Our findings provide insights\ninto the interplay between charge, spin, and orbital transport in Cu-based\nheterostructures and open the door to the development of gate-tunable\nspin-orbitronic devices.",
        "This study presents SALMON VR, a visualization program designed to visualize\nthe time evolution of electronic density changes and vector potentials in\nvirtual reality (VR) space. The time-series electronic density data computed by\nSALMON are stored in CUBE format. SALMON VR processes these data to construct\nisosurfaces of electronic density variations and two-dimensional\nrepresentations of vector potentials. Equipped with a user-friendly interface\nusing VR technology, the program is available in two versions: one for the Meta\nQuest 3 head-mount display (Meta Platforms Inc., California) and one for PCs.\nAtoms are displayed as spheres of different sizes and colors according to their\nelemental properties. This visual representation facilitates a deeper\nunderstanding of the complex interactions between light and electrons. Users\ncan easily manipulate the isosurface values, speed of animation, and color map\nof the vector potential. SALMON VR will enable researchers and educators to\nenhance their understanding of physical phenomena and improve engagement in\nlearning environments.",
        "Spall failure, a complex failure mechanism driven by tensile stress wave\ninteractions, has been extensively studied in single-crystal FCC metals,\nrevealing a precursor stage involving dislocation emission along closed-packed\ndirections. Here we investigate the photo-induced spall failure of Ni\nbicrystals under a two-pulse laser configuration, exploring various\nmisorientation angles through two-temperature molecular dynamics (MD)\nsimulations including electronic effects to simulate light-matter interaction.\nOur findings demonstrate that light-matter interactions can induce spall\nfailure at the sample center, similar to conventional plate-impact methods,\nwhen two laser-pulses are applied to the front and back surfaces of the sample.\nThe study reveals the significant influence of misorientation angles on\ndislocation activity and spall behavior, where grain boundaries (GBs) play\npivotal roles, either promoting or impeding dislocation interactions.\nFurthermore, our work highlights the potential for enhancing spall resistance\nby tailoring materials through misorientation angle variation.",
        "The thorough treatment of electron-lattice interactions from first principles\nis one of the main goals in condensed matter physics. While the commonly\napplied adiabatic Born-Oppenheimer approximation is sufficient for describing\nmany physical phenomena, it is limited in its ability to capture meaningful\nfeatures originating from non-adiabatic coupling effects. The exact\nfactorization method, starting from the full Hamiltonian of electrons and\nnuclei, provides a way to systematically account for non-adiabatic effects.\nThis formalism was recently developed into an ab initio density functional\ntheory framework. Within this framework we here develop a perturbative approach\nto the electronic states in solid state materials. We derive\nexact-factorization-based perturbations of the Kohn-Sham states up to second\norder in the nuclear displacements. These non-adiabatic features in the\ncalculated energy and wavefunction corrections are expressed in terms of\nreadily available density functional perturbation theory components.",
        "We study the ultrafast demagnetization dynamics of LnRh$_2$Si$_2$ (Ln $=$ Pr,\nNd, Sm, Gd, Tb, Dy, Ho) antiferromagnets (AFM) after excitation by a laser\npulse, using a combination of density functional theory and atomistic spin and\nspin-lattice dynamics simulations. First, we calculate the Heisenberg\ninteractions using the magnetic force theorem and compare two approaches, where\nthe $4f$ states of the rare earths are treated as frozen core states or as\nvalence states with added correlation corrections. We find marked quantitative\ndifferences in terms of predicted Curie temperature for most of the systems,\nespecially for those with large orbital moment of the rare earth cations. This\ncan be attributed to the importance of indirect interactions of the $4f$ states\nthrough the Si states, which depend on the binding energy of the $4f$ states\nand coexists with RKKY-type interactions mediated by the conduction states.\nHowever, qualitatively, both approaches agree in terms of the predicted AFM\nordering at low temperatures. In the second step, the atomistic dynamics\nsimulations are combined with a heat-conserving two-temperature model, allowing\nfor the calculation of spin and electronic temperatures during the\nmagnetization dynamics simulations. Despite quite different demagnetization\ntimes, magnetization dynamics of all studied LnRh$_2$Si$_2$ AFM exhibit similar\ntwo-step behavior, in particular, the first fast drop followed by slower\ndemagnetization. We observe that the demagnetization amplitude depends linearly\non laser fluence for low fluences, which is in agreement with experimental\nobservations. We also investigate the impact of lattice dynamics on ultrafast\ndemagnetization using coupled atomistic spin-lattice dynamics simulations and a\nheat-conserving three-temperature model, which confirm linear dependence of\nmagnetisation on laser fluence.",
        "The critical temperature for magnetic order comprises a crucial property of\nany magnetic material and ranges from a few Kelvin in certain antiferromagnets\nto 1400 K in ferromagnetic Co. However, the prediction of critical temperatures\nbased on, for example, a spin wave dispersion is in general non-trivial. For\nferromagnets and simple collinear antiferromagnets, estimates may be obtained\nfrom the Heisenberg model using either renormalized spin wave theory or the\nGreen's function random phase approximation (RPA), but a systematic assessment\nof the accuracy of such approaches seems to be lacking in the literature. In\nthis work, we propose generalizations of both renormalized spin wave theory and\nRPA to calculate the critical temperatures of single-$Q$ helimagnetic ground\nstates, which include ferromagnets and antiferromagnets as special cases. We\ncompare the methods to classical Monte Carlo simulations and Mean field theory,\nusing experimental exchange parameters for a wide range of materials; MnO and\nNiO (single site N\\'eel ground states), MnF$_2$ (altermagnet), Cr$_2$O$_3$ and\nFe$_2$O$_3$ (two site N\\'eel states) and Ba$_3$NbFe$_3$Si$_2$O$_{14}$\n(incommensurate helimagnet). In all cases, we observe that predictions from RPA\nare in excellent agreement with experimental values and RPA thus constitutes a\nrather reliable all-purpose method for calculating critical temperatures.",
        "The exploration of topological phases remains a cutting-edge research\nfrontier, driven by their promising potential for next-generation electronic\nand quantum technologies. In this work, we employ first-principles calculations\nand tight-binding modeling to systematically investigate the topological\nproperties of freestanding two-dimensional (2D) honeycomb Bi, HgTe, and\nAl2O3(0001)-supported HgTe. Remarkably, all three systems exhibit coexistence\nof first-order and higher-order topological insulator states, manifested by\ngapless edge states in one-dimensional (1D) nanoribbons and symmetry-related\ncorner states in zero-dimensional (0D) nanoflakes. Furthermore, fractional\nelectron charges may accumulate at the corners of armchair-edged nanoflakes.\nAmong these materials, HgTe\/Al2O3(0001) is particularly promising due to its\nexperimentally feasible atomic configuration and low-energy corner states. Our\nfindings highlight the importance of exploring higher-order topological phases\nin Z_2 quantum spin Hall insulators and pave the way for new possibilities in\ndevice applications.",
        "In this study, we propose a novel microstructure-sensitive Bayesian\noptimization (BO) framework designed to enhance the efficiency of materials\ndiscovery by explicitly incorporating microstructural information. Traditional\nmaterials design approaches often focus exclusively on direct\nchemistry-process-property relationships, overlooking the critical role of\nmicrostructures. To address this limitation, our framework integrates\nmicrostructural descriptors as latent variables, enabling the construction of a\ncomprehensive process-structure-property mapping that improves both predictive\naccuracy and optimization outcomes. By employing the active subspace method for\ndimensionality reduction, we identify the most influential microstructural\nfeatures, thereby reducing computational complexity while maintaining high\naccuracy in the design process. This approach also enhances the probabilistic\nmodeling capabilities of Gaussian processes, accelerating convergence to\noptimal material configurations with fewer iterations and experimental\nobservations. We demonstrate the efficacy of our framework through synthetic\nand real-world case studies, including the design of Mg$_2$Sn$_x$Si$_{1-x}$\nthermoelectric materials for energy conversion. Our results underscore the\ncritical role of microstructures in linking processing conditions to material\nproperties, highlighting the potential of a microstructure-aware design\nparadigm to revolutionize materials discovery. Furthermore, this work suggests\nthat since incorporating microstructure awareness improves the efficiency of\nBayesian materials discovery, microstructure characterization stages should be\nintegral to automated -- and eventually autonomous -- platforms for materials\ndevelopment.",
        "The discovery and optimization of high-performance materials is the basis for\nadvancing energy conversion technologies. To understand composition-property\nrelationships, all available data sources should be leveraged: experimental\nresults, predictions from simulations, and latent knowledge from scientific\ntexts. Among these three, text-based data sources are still not used to their\nfull potential. We present an approach combining text mining, Word2Vec\nrepresentations of materials and properties, and Pareto front analysis for the\nprediction of high-performance candidate materials for electrocatalysis in\nregions where other data sources are scarce or non-existent. Candidate\ncompositions are evaluated on the basis of their similarity to the terms\n`conductivity' and `dielectric', which enables reaction-specific candidate\ncomposition predictions for oxygen reduction (ORR), hydrogen evolution (HER),\nand oxygen evolution (OER) reactions. This, combined with Pareto optimization,\nallows us to significantly reduce the pool of candidate compositions to\nhigh-performing compositions. Our predictions, which are purely based on text\ndata, match the measured electrochemical activity very well.",
        "Janus transition metal dichalcogenides (JTMDs) monolayers have emerged as a\nnew paradigm to broaden the family of two-dimensional (2D) materials. Despite\nnumerous theoretical predictions of JTMDs, their experimental realization\nremains scarce, most probably due to intrinsic structural fragility. We\nidentify a dependence of the structural stability of 1H-phase JTMDs on the\ntransition metal group, with Group-VIB-based monolayers exhibiting robust\nstability, as evidenced by the successful synthesized MoSSe and WSSe. The\ngroup-dependent stability arises from the competition between metal-ligand\nionic bonding and ligand-ligand covalent bonding, as well as the high-energy\nd-electron orbital splitting. We propose an electron configuration that\ndescribes the interactions of electrons near the Fermi level to correlate the\nstability, and introduce an electron compensation strategy to stabilize certain\nunstable JTMDs systems. Guided by the electronic origin of stability, we\npredict a family of stable 2D Janus transition metal halides with intrinsic\nferromagnetic valley properties. This work bridges the gap between electronic\nstructure and stability predictions, and extends the design rules for\nsynthesizing 2D Janus materials.",
        "Understanding phase diagrams is essential for material selection and design,\nas they provide a comprehensive representation of the thermodynamics of\nmixtures. This work delivers a broad and systematic overview of possible\nternary phase diagrams for amorphous systems representative of polymers, small\norganic molecules, and solvents. Thanks to computationally efficient methods,\nan unprecedented library of $>$80,000 ternary phase diagrams is generated based\non a systematic screening of interaction parameters. Twenty-one phase diagram\ntypes, including unreported ones, are identified. They are classified according\nto simple rules related to the number of immiscible material pairs, of\nmiscibility gaps, and of three-phase regions. They are mapped onto the\nthree-dimensional interaction parameters space, providing a clear picture of\ntheir likelihood and existence conditions. Four well-known phase-diagram types\nwith 0, 1, 2, or 3 immiscible pairs are found to be the most likely. The\nnumerous uncommon phase diagrams are mostly observed within a small parameter\nwindow around the critical interaction parameter values. For the most common\nphase diagram types, we show that the size of the processability window becomes\nsensitive to interaction parameter variations close to critical values. The\nsensitivity decreases for materials with increasing molar size. Finally,\nsuccessful comparisons of simulated and experimental phase diagrams showcase\nthe real-world relevance of this theoretical analysis. The presented results\nlay a robust foundation for rational design of solution processing conditions\nand for blend morphology control. Immediate applications include organic thin\nfilms and the identification of green solvents for sustainable processing.",
        "Raman spectroscopy is an important tool for studies of molecules, liquids and\nsolids. While Raman spectra can be obtained theoretically from molecular\ndynamics (MD) simulations, this requires the calculation of the electronic\npolarizability along the simulation trajectory. First-principles calculations\nof electronic polarizability are computationally expensive, motivating the\ndevelopment of atomistic models for the evaluation of the changes in the\nelectronic polarizability with the changes in the atomic coordinates of the\nsystem. The bond polarizability model (BPM) is one of the oldest and simplest\nsuch atomistic models, but cannot reproduce the effects of angular vibrations,\nleading to inaccurate modeling of Raman spectra. Here, we demonstrate that the\ngeneralization of BPM through inclusion of terms for atom pairs that are\ntraditionally considered to be not involved in bonding dramatically improves\nthe accuracy of polarizability modeling and Raman spectra calculations. The\ngeneralized BPM (GBPM) reproduces the ab initio polarizability and Raman\nspectra for a range of tested molecules (SO2, H2S, H2O, NH3, CH4, CH3OH and\nCH3CH2OH) with high accuracy and also shows significantly improved agreement\nwith ab initio results for the more complex ferroelectric BaTiO3 systems. For\nliquid water, the anisotropic Raman spectrum derived from atomistic MD\nsimulations using GBPM evaluation of polarizability shows significantly\nimproved agreement with the experimental spectrum compared to the spectrum\nderived using BPM. Thus, GBPM can be used for the modeling of Raman spectra\nusing large-scale molecular dynamics and provides a good basis for the further\ndevelopment of atomistic polarizability models.",
        "Van der Waals ferromagnets, such as Fe5GeTe2, offer a promising platform for\nspintronic devices based on chiral magnetic textures, provided a significant\nDzyaloshinskii-Moriya interaction (DMI) can be induced to stabilise them. Here,\nwe directly measure DMI in epitaxial Fe5GeTe2 thin films using Brillouin light\nscattering spectroscopy and observe a consistent DMI (D = 0.04 mJ\/m2) across\nvarious thicknesses. Its weak thickness dependence, combined with the nominally\nsymmetric film interfaces, suggests a bulk origin. Although we do not determine\nthe microscopic mechanism, our findings are compatible with ab-initio\ncalculations linking DMI to partial ordering of Fe split sites. Additionally,\nwe find a low magnetic dissipation (alpha < 0.003). The observed DMI, which\ncould be further enhanced by optimising the Fe site ordering, combined with low\ndissipation, makes Fe5GeTe2 a strong candidate for exploring the dynamics of\nchiral magnetic textures in two-dimensional materials.",
        "Quiver Grassmannians of equioriented type $\\texttt{A}$ and nilpotent\nequioriented type $\\tilde{\\texttt{A}}$ quiver representations are\nGKM-varieties. In particular, they have a cellular decomposition and admit a\ntorus action with finitely many fixed points and one-dimensional orbits (i.e.\nskeletal action). We examine the case of string representations and provide a\nclassification of all corresponding quiver Grassmannians with a GKM-variety\nstructure.",
        "In many machine learning tasks, known symmetries can be used as an inductive\nbias to improve model performance. In this paper, we consider learning group\nequivariance through training with data augmentation. We summarize results from\na previous paper of our own, and extend the results to show that equivariance\nof the trained model can be achieved through training on augmented data in\ntandem with regularization.",
        "Providing high-quality, single-crystal diamond (SCD) with a large area is\ndesirable for up-scaling quantum technology applications that rely on color\ncenters in diamond. Growth methods aiming to increase the area of SCD are an\nactive research area. Native color centers offer a sensitive probe for local\ncrystal quality in such novel materials e.g., via their reaction to stress. In\nthis work, we investigate individual native nitrogen-vacancy (NV) centers in\nSCD layers manufactured via laterally overgrowing hole arrays in a\nheteroepitaxially grown large-scale substrate. Heteroepitaxy has become a\ncommon tool for growing large SCDs; however, achieving the high crystal quality\nneeded for quantum applications remains a challenge. In the overgrown layer, we\nidentify NV centers with spin-decoherence times in the order of hundreds of\nmicroseconds, comparable to high-purity homoepitaxial SCD. We quantify the\neffective crystal strain in different regions of the overgrown layer,\nindicating a low stress overall and a stress reduction in the diamond layer\nabove the holes.",
        "We show that for any fixed $(2+1)$-dimensional TQFT over $\\mathbb{C}$ of\neither Turaev-Viro-Barrett-Westbury or Reshetikhin-Turaev type, the problem of\n(exactly) computing its invariants on closed 3-manifolds is either solvable in\npolynomial time, or else it is $\\#\\mathsf{P}$-hard to (exactly) contract\ncertain tensors that are built from the TQFT's fusion category. Our proof is an\napplication of a dichotomy result of Cai and Chen [J. ACM, 2017] concerning\nweighted constraint satisfaction problems over $\\mathbb{C}$. We leave for\nfuture work the issue of reinterpreting the conditions of Cai and Chen that\ndistinguish between the two cases (i.e. $\\#\\mathsf{P}$-hard tensor contractions\nvs. polynomial time invariants) in terms of fusion categories. We expect that\nwith more effort, our reduction can be improved so that one gets a dichotomy\ndirectly for TQFTs' invariants of 3-manifolds rather than more general tensors\nbuilt from the TQFT's fusion category.",
        "The dynamical behavior of quantum state properties under intrinsic\ndecoherence models can be modified by the presence of external magnetic fields.\nAlthough generically external magnetic fields are detrimental to preserve\nquantumness in the presence of intrinsic decoherence, judicious adjustment of\nthe magnetic field can stabilize such features. This stabilization arises from\nnovel resonances between energy eigenstates resulting from the presence of an\nexternal magnetic field. Here, we present our findings using as a model system\ntwo spin 1-particles confined in a double-well potential under intrinsic\ndecoherence. We stress, however, that our results are generic and independent\non the used model.",
        "Is there new physics hidden in the four-point function of the cosmic\nmicrowave background (CMB)? We conduct a detailed analysis of the Planck PR4\ntemperature and polarization trispectrum for $\\ell\\in[2,2048]$. Using the\ntheoretical and computational tools developed in Paper 1 and Paper 2, we search\nfor 33 template amplitudes, encoding a variety of effects from inflationary\nself-interactions to particle exchange. We find no evidence for primordial\nnon-Gaussianity and set stringent constraints on both phenomenological\namplitudes and couplings in the inflationary Lagrangian. Due to the use of\noptimal estimators and polarization data, our constraints are highly\ncompetitive. For example, we find $\\sigma(g_{\\rm NL}^{\\rm loc})=4.8\\times 10^4$\nand $\\tau_{\\rm NL}^{\\rm loc} <1500$ (95\\% CL), a factor of two improvement on\nEffective Field Theory amplitudes, and a $43\\sigma$ detection of gravitational\nlensing. Many templates are analyzed for the first time, such as\ndirection-dependent trispectra and the collapsed limit of the `cosmological\ncollider', across a range of masses and spins. We perform a variety of\nvalidation tests; whilst our results are stable, the most relevant systematics\nare found to be lensing bias, residual foregrounds, and mismatch between\nsimulations and data. The techniques discussed in this series can be extended\nto future datasets, allowing the primordial Universe to be probed at even\nhigher sensitivity.",
        "Optical parametric amplification (OPA) comprises essentially a nonlinear\nfour-wave mixing process in which a \"pump\" and a \"signal\" field give rise to an\n\"idler\" field under certain phase-matching conditions. Here we use a photonic\ncrystal waveguide strongly-coupled with an excitonic reservoir to generate this\nprocess between different guided modes at optical wavelengths. Differently from\nclassical nonlinear optical crystals, where the pump and idler photons travel\nalmost collinearly, our exciton-polaritons are naturally separated in the\nwaveguide due to their opposite group velocities. Due to the high efficiency of\nthe process we can generate the idler field of the parametric process by\npumping with a continuous wave laser and choose its direction of propagation in\nthe waveguide by adjusting the angle of incidence of the seed laser. We show\nthe OPA process to be robust against surface defects of the waveguide and can\nlead to simple-to-fabricate devices compared to microcavities that take\nadvantage of strong signal-idler correlations in a propagating geometry. Our\nresults closely agree with mean-field numerical simulations.",
        "We demonstrate optical cycling and transverse laser cooling of a beam of\nfermionic 137BaF molecules. Their high masses and nuclear spins make these\nmolecules sensitive probes for parity violation and properties of the weak\ninteraction. However, the nuclear spins also lead to a quasi-closed cycling\ntransition currently involving up to 112 levels, which significantly exceeds\nthe complexity in other laser-cooled molecules. Optical cycling and cooling are\nfacilitated through carefully designed optical spectra tailored to this\nmolecular structure. Our results pave the way for efficient state preparation,\ndetection, and cooling in precision measurements using this species and other\nsimilar species.",
        "Axion-like particles can couple to Standard Model gluons, electroweak gauge\nbosons, and massive fermions. A future multi-TeV muon collider provides a\nfavorable environment to probe axion-like particles through multiple production\nchannels, including vector boson fusion via electroweak gauge boson couplings\nand the top-associated production mediated by direct fermionic couplings.\nMotivated by the quality issue of the QCD axion, we focus on axion-like\nparticles with masses and decay constants around the TeV scale. We explore how\ndifferent axion-like particle couplings shape its production and decay modes,\nrevealing a rich and intricate phenomenological landscape.",
        "The project Agriculture 4.0 without chemical synthetical plant protection\n(NOcsPS) tests a number of cropping systems that avoid the use of chemical\nsynthetical pesticides while at the same time using mineral fertilizers. The\nexperiment started in 2020 (sowing fall 2019). In 2024 (sowing fall 2023), some\nof the cropping systems were modified. Analysis of this experiment may be done\nusing linear mixed models. In order to include the data from 2020-2023 in joint\nanalyses with the data collected for the modified systems from 2024 onwards,\nthe mixed modelling approach needs to be reconsidered. In this paper, we\ndevelop models for this purpose. A key feature is the use of network\nmeta-analytic concepts that allow a combination of direct and indirect\ncomparisons among systems from the different years. The approach is first\nillustrated using a toy example. This is followed by detailed analyses of data\nfrom two the two trials sites Dahnsdorf and Hohenheim.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "Neurological disorders represent significant global health challenges,\ndriving the advancement of brain signal analysis methods. Scalp\nelectroencephalography (EEG) and intracranial electroencephalography (iEEG) are\nwidely used to diagnose and monitor neurological conditions. However, dataset\nheterogeneity and task variations pose challenges in developing robust deep\nlearning solutions. This review systematically examines recent advances in deep\nlearning approaches for EEG\/iEEG-based neurological diagnostics, focusing on\napplications across 7 neurological conditions using 46 datasets. We explore\ntrends in data utilization, model design, and task-specific adaptations,\nhighlighting the importance of pre-trained multi-task models for scalable,\ngeneralizable solutions. To advance research, we propose a standardized\nbenchmark for evaluating models across diverse datasets to enhance\nreproducibility. This survey emphasizes how recent innovations can transform\nneurological diagnostics and enable the development of intelligent, adaptable\nhealthcare solutions.",
        "We employ the \"learning by confusion\" technique, an unsupervised machine\nlearning approach for detecting phase transitions, to analyze quantum Monte\nCarlo simulations of the two-dimensional Holstein model--a fundamental model\nfor electron-phonon interactions on a lattice. Utilizing a convolutional neural\nnetwork, we conduct a series of binary classification tasks to identify\nHolstein critical points based on the neural network's learning accuracy. We\nfurther evaluate the effectiveness of various training datasets, including\nsnapshots of phonon fields and other measurements resolved in imaginary time,\nfor predicting distinct phase transitions and crossovers. Our results culminate\nin the construction of the finite-temperature phase diagram of the Holstein\nmodel.",
        "This paper explores decaying turbulence beneath surface waves that is\ninitially isotropic and shear-free. We start by presenting phenomenology\nrevealed by wave-averaged numerical simulations: an accumulation of angular\nmomentum in coherent vortices, suppression of kinetic energy dissipation, and\nthe development of depth-alternating jets. We interpret these features through\nan analogy with rotating turbulence (Holm 1996), wherein the curl of the Stokes\ndrift, $\\nabla \\times \\mathbf{u}^S$, takes on the role of the background\nvorticity (for example, $(f_0 + \\beta y) \\mathbf{\\hat z}$ on the\n$\\beta$-plane). We pursue this thread further by showing that a two-equation\nmodel proposed by (Bardina et al. 1985) for rotating turbulence reproduces the\nsimulated evolution of volume-integrated kinetic energy. This success of the\ntwo-equation model -- which explicitly parameterizes wave-driven suppression of\nkinetic energy dissipation -- carries implications for modeling turbulent\nmixing in the ocean surface boundary layer. We conclude with a discussion about\na wave-averaged analogue of the Rossby number appearing in the two-equation\nmodel, which we term the ``pseudovorticity number'' after the pseudovorticity\n$\\nabla \\times \\mathbf{u}^S$. The pseudovorticity number is related to the\nLangmuir number in an integral sense.",
        "We investigate the thermodynamic uncertainty relations (TURs) in steady-state\ntransport for three-terminal systems within the linear response regime,\nspecifically in the presence of broken time-reversal symmetry. To quantify the\nTUR, we introduce a dimensionless trade-off parameter $Q_J$, and derive new\nbounds of $Q_J$ for both particle and heat currents under a strong constraint\non the Onsager coefficients. Furthermore, we determine a universal lower bound\n$Q_J^{bound}\\geq1.5$ for three-terminal systems in the linear response regime\nwhen the time-reversal symmetry is broken."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge",
    "start_abstract":"Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
      ],
      "abstract":[
        "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "The time-dependent reproduction number for epidemics in heterogeneous\n  populations",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Multicellular self-organization in Escherichia coli",
        "Flexible inference of evolutionary accumulation dynamics using uncertain\n  observational data",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Toxicological Evaluation of Phytochemicals and Heavy Metals in Ficus\n  exasperata Vahl (Sandpaper) Leaves obtained in Birnin Kebbi, Nigeria",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Temporal Dynamics of Microbial Communities in Anaerobic Digestion:\n  Influence of Temperature and Feedstock Composition on Reactor Performance and\n  Stability",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Fungal Genetic Variants in Oceanic Environments",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "KMT2B-related disorders: expansion of the phenotypic spectrum and\n  long-term efficacy of deep brain stimulation",
        "COLOR: A compositional linear operation-based representation of protein\n  sequences for identification of monomer contributions to properties",
        "Top eigenvalue statistics of diluted Wishart matrices",
        "Quantum oscillations in a dipolar excitonic insulator",
        "Comparison theorems for the minimum eigenvalue of a random\n  positive-semidefinite matrix",
        "Emergent supercounterfluid and quantum phase diagram of two-component\n  interacting bosons in one-dimensional optical lattice",
        "Multivariate spatial models for small area estimation of\n  species-specific forest inventory parameters",
        "Bayesian optimization of electron energy from laser wakefield\n  accelerator",
        "Quantum model reduction for continuous-time quantum filters",
        "Meson Mixing Bounds on $Z^{\\prime}$ Mass in the Alignment Limit:\n  Establishing the Phenomenological Viability of the 331 Model",
        "Right-censored models on massive data",
        "A spectral boundary element method for acoustic interference problems",
        "Variational quantum thermalizers based on weakly-symmetric nonunitary\n  multi-qubit operations",
        "AI-assisted hyper-dimensional broadband quantum memory with efficiency\n  above 90% in warm atoms",
        "The putative center in NGC 1052",
        "Hopfological invariants for tame subextensions",
        "Detecting entanglement in any measurement using quantum networks"
      ],
      "abstract":[
        "The time-dependent reproduction number Rt can be used to track pathogen\ntransmission and to assess the efficacy of interventions. This quantity can be\nestimated by fitting renewal equation models to time series of infectious\ndisease case counts. These models almost invariably assume a homogeneous\npopulation. Individuals are assumed not to differ systematically in the rates\nat which they come into contact with others. It is also assumed that the\ntypical time that elapses between one case and those it causes (known as the\ngeneration time distribution) does not differ across groups. But contact\npatterns are known to widely differ by age and according to other demographic\ngroupings, and infection risk and transmission rates have been shown to vary\nacross groups for a range of directly transmitted diseases. Here, we derive\nfrom first principles a renewal equation framework which accounts for these\ndifferences in transmission across groups. We use a generalisation of the\nclassic McKendrick-von Foerster equation to handle populations structured into\ninteracting groups. This system of partial differential equations allows us to\nderive a simple analytical expression for Rt which involves only group-level\ncontact patterns and infection risks. We show that the same expression emerges\nfrom both deterministic and stochastic discrete-time versions of the model and\ndemonstrate via simulations that our Rt expression governs the long-run fate of\nepidemics. Our renewal equation model provides a basis from which to account\nfor more realistic, diverse populations in epidemiological models and opens the\ndoor to inferential approaches which use known group characteristics to\nestimate Rt.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Understanding and predicting evolutionary accumulation pathways is a key\nobjective in many fields of research, ranging from classical evolutionary\nbiology to diverse applications in medicine. In this context, we are often\nconfronted with the problem that data is sparse and uncertain. To use the\navailable data as best as possible, inference approaches that can handle this\nuncertainty are required. One way that allows us to use not only\ncross-sectional data, but also phylogenetic related and longitudinal data is\nusing 'hypercubic inference' models. In this article we introduce HyperLAU, a\nnew algorithm for hypercubic inference that makes it possible to use datasets\nincluding uncertainties for learning evolutionary pathways. Expanding the\nflexibility of accumulation modelling, HyperLAU allows us to infer dynamic\npathways and interactions between features, even when large sets of particular\nfeatures are unobserved across the source dataset. We show that HyperLAU is\nable to highlight the main pathways found by other tools, even when up to 50%\nof the features in the input data are uncertain.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Objective: Ficus exasperata Vahl (Sandpaper tree) is extensively used in\nNigeria to treat diseases, but a dearth of documentation about its toxicity\nexists. This information is crucial because pollutants can contaminate\nmedicinal plants. This study determined the heavy metal and phytochemical\ncontent of methanolic leaf extract of F exasperata obtained in Birnin Kebbi,\nNigeria. Material and Methods: The lethality of the plant was also assessed\nusing 70 wild shrimps divided equally into seven groups. Group 1 (negative\ncontrol), groups 2 and 3 (positive controls) were exposed to 500 and 1000ppm of\nformaldehyde, respectively; and groups 4-7 were exposed to 1000, 2000, 4000,\nand 8000ppm of extracts, respectively, for 96 hours. Results: The\nphytochemistry revealed high levels of flavonoids and saponins and moderate\nlevels of tannins and phenols. The heavy metal analysis revealed non-tolerable\nlevels of cadmium, copper, and lead, while zinc was within the tolerable limit.\nThe negative control recorded 10% mortality, 1000 and 2000 ppm (20% each),\n4000ppm (70%), and 8000 ppm (100%). Conclusion: These results inferred safe\ndoses of the plant's extract in low and medium concentrations but toxic and\nfatal at high doses over a period of time. Consumers are advised to seek an\nexpert's guidance before using it.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Anaerobic digestion (AD) offers a sustainable biotechnology to recover\nresources from carbon-rich wastewater, such as food-processing wastewater.\nDespite crude wastewater characterisation, the impact of detailed chemical\nfingerprinting on AD remains underexplored. This study investigated the\ninfluence of fermentation-wastewater composition and operational parameters on\nAD over time to identify critical factors influencing reactor biodiversity and\nperformance. Eighteen reactors were operated under various operational\nconditions using mycoprotein fermentation wastewater. Detailed chemical\nanalysis fingerprinted the molecules in the fermentation wastewater throughout\nAD including sugars, sugar alcohols and volatile fatty acids (VFAs). Sequencing\nrevealed distinct microbiome profiles linked to temperature and reactor\nconfiguration, with mesophilic conditions supporting a more diverse and densely\nconnected microbiome. Significant elevations in Methanomassiliicoccus were\ncorrelated to high butyric acid concentrations and decreased biogas production,\nfurther elucidating the role of this newly discovered methanogen. Dissimilarity\nanalysis demonstrated the importance of individual molecules on microbiome\ndiversity, highlighting the need for detailed chemical fingerprinting in AD\nstudies of microbial trends. Machine learning (ML) models predicting reactor\nperformance achieved high accuracy based on operational parameters and\nmicrobial taxonomy. Operational parameters had the most substantial influence\non chemical oxygen demand removal, whilst Oscillibacter and two Clostridium sp.\nwere highlighted as key factors in biogas production. By integrating detailed\nchemical and biological fingerprinting with ML models this research presents a\nnovel approach to advance our understanding of AD microbial ecology, offering\ninsights for industrial applications of sustainable waste-to-energy systems.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Comparing specific types of organisms as they are found across environmental\nconditions has helped inform how genes and gene products of these organisms\nrelate to phenotypes and adaptation. In this study, we examine\nmetatranscriptomic data as found for oceanic fungi across different oceanic\nsampling sites. A specific set of three genes was chosen for evaluation based\non conserved orthology, known association with core physiological processes in\nfungi, and level of abundance within oceanic metatranscriptomic data. We report\nupon a potential association of genetic variance with environmental conditions\nof iron, salt and phosphate in oceanic waters based on heatmap visualization\nand PERMANOVA analysis.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Heterozygous mutations in KMT2B are associated with an early-onset,\nprogressive, and often complex dystonia (DYT28). Key characteristics of typical\ndisease include focal motor features at disease presentation, evolving through\na caudocranial pattern into generalized dystonia, with prominent oromandibular,\nlaryngeal, and cervical involvement. Although KMT2B-related disease is emerging\nas one of the most common causes of early-onset genetic dystonia, much remains\nto be understood about the full spectrum of the disease. We describe a cohort\nof 53 patients with KMT2B mutations, with detailed delineation of their\nclinical phenotype and molecular genetic features. We report new disease\npresentations, including atypical patterns of dystonia evolution and a subgroup\nof patients with a non-dystonic neurodevelopmental phenotype. In addition to\nthe previously reported systemic features, our study has identified\nco-morbidities, including the risk of status dystonicus, intrauterine growth\nretardation, and endocrinopathies. Analysis of this study cohort (n = 53) in\ntandem with published cases (n = 80) revealed that patients with chromosomal\ndeletions and protein-truncating variants had a significantly higher burden of\nsystemic disease (with earlier onset of dystonia) than those with missense\nvariants. Eighteen individuals had detailed longitudinal data available after\ninsertion of deep brain stimulation for medically refractory dystonia. Median\nage at deep brain stimulation was 11.5 years (range: 4.5 to 37.0 years).\nFollow-up after deep brain stimulation ranged from 0.25 to 22 years.\nSignificant improvement of motor function and disability (as assessed by the\nBurke-Fahn-Marsden Dystonia Rating Scales, BFMDRS-M and BFMDRS-D) was evident\nat 6 months, 1 year, and last follow-up (motor, P = 0.001, P = 0.004, and P =\n0.012; disability, P = 0.009, P = 0.002, and P = 0.012).",
        "The properties of biological materials like proteins and nucleic acids are\nlargely determined by their primary sequence. While certain segments in the\nsequence strongly influence specific functions, identifying these segments, or\nso-called motifs, is challenging due to the complexity of sequential data.\nWhile deep learning (DL) models can accurately capture sequence-property\nrelationships, the degree of nonlinearity in these models limits the assessment\nof monomer contributions to a property - a critical step in identifying key\nmotifs. Recent advances in explainable AI (XAI) offer attention and\ngradient-based methods for estimating monomeric contributions. However, these\nmethods are primarily applied to classification tasks, such as binding site\nidentification, where they achieve limited accuracy (40-45%) and rely on\nqualitative evaluations. To address these limitations, we introduce a DL model\nwith interpretable steps, enabling direct tracing of monomeric contributions.\nWe also propose a metric ($\\mathcal{I}$), inspired by the masking technique in\nthe field of image analysis and natural language processing, for quantitative\nanalysis on datasets mainly containing distinct properties of anti-cancer\npeptides (ACP), antimicrobial peptides (AMP), and collagen. Our model exhibits\n22% higher explainability, pinpoints critical motifs (RRR, RRI, and RSS) that\nsignificantly destabilize ACPs, and identifies motifs in AMPs that are 50% more\neffective in converting non-AMPs to AMPs. These findings highlight the\npotential of our model in guiding mutation strategies for designing\nprotein-based biomaterials.",
        "Using the replica method, we compute analytically the average largest\neigenvalue of diluted covariance matrices of the form $\\mathbf{J} =\n\\mathbf{X}^T \\mathbf{X}$, where $\\mathbf{X}$ is a $N\\times M$ sparse data\nmatrix, in the limit of large $N,M$ with fixed ratio. We allow for random\nnon-zero weights, provided they lead to an isolated largest eigenvalue. By\nformulating the problem as the optimisation of a quadratic Hamiltonian\nconstrained to the $N$-sphere at low temperatures, we derive a set of recursive\ndistributional equations for auxiliary probability density functions, which can\nbe efficiently solved using a population dynamics algorithm. The average\nlargest eigenvalue is identified with a Lagrange parameter that governs the\nconvergence of the algorithm. We find excellent agreement between our\nanalytical results and numerical results obtained from direct diagonalisation.",
        "Quantum oscillations in magnetization or resistivity are a defining feature\nof metals subject to an external magnetic field. The phenomenon is generally\nnot expected in insulators without a Fermi surface. The observations of quantum\noscillations in Kondo insulating materials have provided a rare counterexample\nand attracted much theoretical interest. However, the magnetic oscillations in\ncorrelated insulators remain poorly understood. Here we report the observations\nof resistivity quantum oscillations in an excitonic insulator realized in\nCoulomb-coupled electron-hole double layers with gate-tunability that allows\nthe phenomenon to be explored in a more controllable fashion than in bulk\nmaterials. When the cyclotron energy of the electrons or holes is tuned to be\ncomparable to or larger than the exciton binding energy, recurring transitions\nbetween excitonic insulators and electron-hole decoupled quantum Hall states\nare observed. Compressibility measurements show an oscillatory exciton binding\nenergy as a function of magnetic field and electron-hole pair density. Coulomb\ndrag measurements further reveal the formation of excitons with finite angular\nmomentum. Our results are qualitatively captured by mean-field theory\ncalculations. The study demonstrates a new platform for studying quantum\noscillations in correlated insulators.",
        "This paper establishes a new comparison principle for the minimum eigenvalue\nof a sum of independent random positive-semidefinite matrices. The principle\nstates that the minimum eigenvalue of the matrix sum is controlled by the\nminimum eigenvalue of a Gaussian random matrix that inherits its statistics\nfrom the summands. This methodology is powerful because of the vast arsenal of\ntools for treating Gaussian random matrices. As applications, the paper\npresents short, conceptual proofs of some old and new results in\nhigh-dimensional statistics. It also settles a long-standing open question in\ncomputational linear algebra about the injectivity properties of very sparse\nrandom matrices.",
        "Motivated by a recent experiment that realizes nearest-neighbor dipolar\ncouplings in an optical lattice [C. Lagoin, $\\textit{et al.}$, Nature\n$\\textbf{609}$, 485 (2022)], we study a one-dimensional version of the\ntwo-component extended Bose-Hubbard model via the density-matrix\nrenormalization group method. By using the nearest-neighbor and on-site\ninteraction parameters from the experiment, we start by mapping the quantum\nphase diagram in the hopping parameters $t_{A}\\mbox{-}t_{B}$ plane with boson\ndensities $\\rho_{A}=\\rho_{B}=1\/2$. In addition to the density wave phase\nreported in the experiment, we find several regimes of superfluidity when one\nor two hopping parameters are large enough, and interestingly there is a\nsupercounterfluid phase at moderate and comparable hopping parameters. The\nuniversality classes of these phase transitions are analyzed from the\ncorrelation functions, excitation gaps, and entanglement entropy. In\nparticular, a Berezinskii-Kosterlitz-Thouless type is recognized several\ngapped-to-gapless transitions. In addition, we also study the quantum phase\ntransitions when varying $\\rho_{B}$ from 0 to 1 while keeping $\\rho_A = 1\/2$.\nWe identify a supersolid phase in a wide range of $1\/2<\\rho_B<1$. Our work\npaves the way for realizing exotic many-body phases in cold atom experiments\nupon proper tuning of experimental parameters.",
        "National Forest Inventories (NFIs) provide statistically reliable information\non forest resources at national and other large spatial scales. As forest\nmanagement and conservation needs become increasingly complex, NFIs are being\ncalled upon to provide forest parameter estimates at spatial scales smaller\nthan current design-based estimation procedures can provide. This is\nparticularly true when estimates are desired by species or species groups. Here\nwe propose a multivariate spatial model for small area estimation of\nspecies-specific forest inventory parameters. The hierarchical Bayesian\nmodeling framework accounts for key complexities in species-specific forest\ninventory data, such as zero-inflation, correlations among species, and\nresidual spatial autocorrelation. Importantly, by fitting the model directly to\nthe individual plot-level data, the framework enables estimates of\nspecies-level forest parameters, with associated uncertainty, across any\nuser-defined small area of interest. A simulation study revealed minimal bias\nand higher accuracy of the proposed model-based approach compared to the\ndesign-based estimator and a non-parametric k-nearest neighbor (kNN) estimator.\nWe applied the model to estimate species-specific county-level aboveground\nbiomass for the 20 most abundant tree species in the southern United States\nusing Forest Inventory and Analysis (FIA) data. Biomass estimates from the\nproposed model had high correlations with design-based estimates and kNN\nestimates. Importantly, the proposed model provided large gains in precision\nacross all 20 species. On average across species, 91.5% of county-level biomass\nestimates had higher precision compared to the design-based estimates. The\nproposed framework improves the ability of NFI data users to generate\nspecies-level forest parameter estimates with reasonable precision at\nmanagement-relevant spatial scales.",
        "We employ Bayesian optimization combined with three-dimensional\nparticle-in-cell simulations to identify the optimal laser and plasma\nparameters that, for a given laser pulse energy, maximize the cut-off energy of\nan electron beam accelerated via laser wakefield acceleration. A Gaussian laser\ndriver with a matched spot size and amplitude is assumed, interacting with both\na uniform-density plasma and a preformed plasma channel of matched radius. To\ninterpret the simulation results quantitatively, we derive novel analytical\nexpressions for predicting the maximum electron energy and acceleration length,\ntaking into account the diffraction and energy depletion of the laser pulse.\nAdditionally, we discuss the potential scalability of the optimal parameters\nfor high-energy lasers.",
        "The use of quantum stochastic models is widespread in dynamical reduction,\nsimulation of open systems, feedback control and adaptive estimation. In many\napplications only part of the information contained in the filter's state is\nactually needed to reconstruct the target observable quantities; thus, filters\nof smaller dimensions could be in principle implemented to perform the same\ntask.In this work, we propose a systematic method to find, when possible,\nreduced-order quantum filters that are capable of exactly reproducing the\nevolution of expectation values of interest. In contrast with existing\nreduction techniques, the reduced model we obtain is exact and in the form of a\nBelavkin filtering equation, ensuring physical interpretability.This is\nattained by leveraging tools from the theory of both minimal realization and\nnon-commutative conditional expectations. The proposed procedure is tested on\nprototypical examples, laying the groundwork for applications in quantum\ntrajectory simulation and quantum feedback control.",
        "We perform a systematic study of flavor-changing neutral currents (FCNCs) in\nthe 331 model with right-handed neutrinos (331RHNs), analyzing constraints on\nthe $Z^\\prime$ boson mass from $K$-, $D$-, $B_d$-, and $B_s$-meson\noscillations. By explicitly incorporating scalar sector dynamics and quark\nrotation ambiguities ($V_L^{u,d}$), we demonstrate that $Z^\\prime$ mass limits\ndepend critically on the parametrization of Cabibbo-Kobayashi-Maskawa (CKM)\nmatrix factors. Three scenarios are explored: (i) $V_L^u =\nV_\\text{CKM}^\\dagger$ (FCNCs restricted to $D$-mesons), (ii) $V_L^d =\nV_\\text{CKM}$ (dominant $B_s$ constraints), and (iii) a hybrid mixing pattern.\nStrikingly, scenario (i) reduces the $Z^\\prime$ mass bound to $M_{Z^\\prime}\n\\gtrsim 600\\;\\text{GeV}$-two orders of magnitude below literature values-by\nleveraging large experimental uncertainties in $D$-$\\bar{D}$ oscillations.\nConversely, scenario (ii) requires $M_{Z^\\prime} \\gtrsim 165\\;\\text{TeV}$ due\nto stringent $B_s$ data. We further establish the alignment limit\n$\\cos(\\phi+\\varphi) = 0$ for the SM-like Higgs, showing its viability depends\non $V_L^{u,d}$ configurations, with $B_s$ systems enforcing\n$|\\cos(\\phi+\\varphi)| < 0.01$ in down-sector FCNC scenarios. Our analysis\nreveals that strategic choices of quark mixing matrices can suppress FCNC\nvisibility, reconciling the 331 framework with flavor data without ultra-heavy\n$Z^\\prime$ bosons. This work provides the first unified treatment of SM-like\nHiggs- and $Z^\\prime$-mediated FCNCs in 331 models, identifying viable\nparameter spaces for collider phenomenology.",
        "This article considers the automatic selection problem of the relevant\nexplanatory variables in a right-censored model on a massive database. We\npropose and study four aggregated censored adaptive LASSO estimators\nconstructed by dividing the observations in such a way as to keep the\nconsistency of the estimator of the survival curve. We show that these\nestimators have the same theoretical oracle properties as the one built on the\nfull database. Moreover, by Monte Carlo simulations we obtain that their\ncalculation time is smaller than that of the full database. The simulations\nconfirm also the theoretical properties. For optimal tuning parameter\nselection, we propose a BIC-type criterion.",
        "In this paper we consider high-frequency acoustic transmission problems with\njumping coefficients modelled by Helmholtz equations. The solution then is\nhighly oscillatory and, in addition, may be localized in a very small vicinity\nof interfaces (whispering gallery modes). For the reliable numerical\napproximation a) the PDE is tranformed in a classical single trace integral\nequation on the interfaces and b) a spectral Galerkin boundary element method\nis employed for its solution. We show that the resulting integral equation is\nwell posed and analyze the convergence of the boundary element method for the\nparticular case of concentric circular interfaces. We prove a condition on the\nnumber of degrees of freedom for quasi-optimal convergence. Numerical\nexperiments confirm the efficiency of our method and the sharpness of the\ntheoretical estimates.",
        "We propose incorporating multi-qubit nonunitary operations in Variational\nQuantum Thermalizers (VQTs). VQTs are hybrid quantum-classical algorithms that\ngenerate the thermal (Gibbs) state of a given Hamiltonian, with applications in\nquantum algorithms and simulations. However, current algorithms struggle at\nintermediate temperatures, where the target state is nonpure but exhibits\nentanglement. We devise multi-qubit nonunitary operations that harness weak\nsymmetries and thereby improve the performance of the algorithm. Utilizing\ndissipation engineering, we create these nonunitary multi-qubit operations\nwithout the need for measurements or additional qubits. To train the ansatz, we\ndevelop and benchmark novel methods for entropy estimation of quantum states,\nexpanding the toolbox for quantum state characterization. We demonstrate that\nour approach can prepare thermal states of paradigmatic spin models at all\ntemperatures. Our work thus creates new opportunities for simulating open\nquantum many-body systems.",
        "High-dimensional broadband quantum memory significantly expands quantum\ninformation processing capabilities, but the memory efficiency becomes\ninsufficient when extended to high dimensions. We demonstrate an efficient\nquantum memorize for hyper-dimensional photons encoded with orbital angular\nmomentum (OAM) and spin angular momentum (SAM). OAM information is encoded from\n-5 to +5, combined with spin angular momentum encoding, enabling up to 22\ndimensions. To ensure high memory efficiency, an artificial intelligent\nalgorithm, a modified Differential Evolution (DE) algorithm using Chebyshev\nsampling, is developed to obtain a perfect signal-control waveform matching.\nMemory efficiency is experimentally achieved 92% for single-mode Gaussian\nsignal, 91% for information dimension of 6 and 80% for dimensional number to\n22. The fidelity is achieved up to 99% for single-mode Gaussian signal, 96% for\nOAM information and 97% for SAM one, which is far beyond no-cloning limitation.\nOur results demonstrate superior performance and potential applications in\nhigh-dimensional quantum information processing. This achievement provides a\ncrucial foundation for future quantum communication and quantum computing.",
        "Many active galaxies harbor powerful relativistic jets, however, the detailed\nmechanisms of their formation and acceleration remain poorly understood. To\ninvestigate the area of jet acceleration and collimation with the highest\navailable angular resolution, we study the innermost region of the bipolar jet\nin the nearby low-ionization nuclear emission-line region (LINER) galaxy NGC\n1052. We combined observations of NGC 1052 taken with VLBA, GMVA, and EHT over\none week in the spring of 2017. For the first time, NGC 1052 was detected with\nthe EHT, providing a size of the central region in-between both jet bases of\n250 RS (Schwarzschild radii) perpendicular to the jet axes. This size estimate\nsupports previous studies of the jets expansion profile which suggest two\nbreaks of the profile at around 300 RS and 10000 RS distances to the core.\nFurthermore, we estimated the magnetic field to be 1.25 Gauss at a distance of\n22 {\\mu}as from the central engine by fitting a synchrotron-self absorption\nspectrum to the innermost emission feature, which shows a spectral turn-over at\nabout 130 GHz. Assuming a purely poloidal magnetic field, this implies an upper\nlimit on the magnetic field strength at the event horizon of 26000 Gauss, which\nis consistent with previous measurements. The complex, low-brightness,\ndouble-sided jet structure in NGC 1052 makes it a challenge to detect the\nsource at millimeter (mm) wavelengths. However, our first EHT observations have\ndemonstrated that detection is possible up to at least 230 GHz. This study\noffers a glimpse through the dense surrounding torus and into the innermost\ncentral region, where the jets are formed. This has enabled us to finally\nresolve this region and provide improved constraints on its expansion and\nmagnetic field strength.",
        "Let H be a finite dimensional Hopf algebra over a field K. In this paper, we\nstudy when an H-extension becomes a tame H-extension by calculating\nHopfological homology and Hopf-cyclic homology. In the (derived) category of\nH'-comodules for a Hopf algebra H', we take Hopf subalgebra H of H' and a\ncertain order A of H. We see the behavior of Hopfological homology for a tame\nA-subextension S\/R in terms of the surjectivity of trace map and of cyclic\nmodules, which induce Hopf-cyclic homology, for Hopf-Galois extensions with H\nin terms of relative Hopf modules.",
        "Entanglement is a key resource to demonstrate quantum advantage over\nclassical strategies. Entanglement in quantum states is one of the most\nwell-explored areas in quantum physics. However, a rigorous approach to\nunderstanding and detecting entanglement in composite quantum measurements is\nlacking. In this work, we focus on composite quantum measurements and classify\nthem into two classes: entangled and separable measurements. As done for\nquantum states, we define analogously a notion of witness that can be used to\ndetect entanglement in composite quantum measurements. Here, one does not need\nto trust the measurement to witness its entanglement but must trust the quantum\nstates. We then further extend this approach to show that any entangled\nmeasurement provides an advantage in network quantum steering without inputs,\nalso known as swap steering. Consequently, this provides a way to witness\nentanglement in any quantum measurement in a one-sided device-independent way.\nFinally, we consider the star network scenario and show that any rank-one\nprojective entangled quantum measurement gives a quantum advantage. Thus, one\ncan detect the entanglement in any rank-one projective measurement in a\ndevice-independent way."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models",
    "start_abstract":"Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
      ],
      "abstract":[
        "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "The Goofus & Gallant Story Corpus for Practical Value Alignment",
        "Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective",
        "Self-Consistency of the Internal Reward Models Improves Self-Rewarding\n  Language Models",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "Platform-Aware Mission Planning",
        "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation",
        "Heterogeneous Causal Discovery of Repeated Undesirable Health Outcomes",
        "A Transformer-based survival model for prediction of all-cause mortality\n  in heart failure patients: a multi-cohort study",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity\n  Recognition and Normalization for Dysmorphology Physical Examination Reports",
        "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
        "R-ParVI: Particle-based variational inference through lens of rewards",
        "Counterfactual Language Reasoning for Explainable Recommendation Systems",
        "Quasi-periodic oscillations of GHz-band polarization in a black hole",
        "Principles and Metrics of Extreme Learning Machines Using a Highly\n  Nonlinear Fiber",
        "Molecular Mechanism Enabling Linearity and Symmetry in Neuromorphic\n  Elements",
        "VideoMerge: Towards Training-free Long Video Generation",
        "A redescription mining framework for post-hoc explaining and relating\n  deep learning models",
        "Quantum Chebyshev Probabilistic Models for Fragmentation Functions",
        "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding",
        "Learning Control of Neural Sound Effects Synthesis from Physically\n  Inspired Models",
        "Predicting Steady-State Behavior in Complex Networks with Graph Neural\n  Networks",
        "LanP: Rethinking the Impact of Language Priors in Large Vision-Language\n  Models",
        "The Vlasov Bivector: A Parameter-Free Approach to Vlasov Kinematics",
        "Beyond the Lungs: Extending the Field of View in Chest CT with Latent\n  Diffusion Models",
        "Pulmonary Tuberculosis Edge Diagnosis System Based on MindSpore\n  Framework: Low-cost and High-precision Implementation with Ascend 310 Chip",
        "Generalized Decision Focused Learning under Imprecise\n  Uncertainty--Theoretical Study",
        "How does Radiation Reaction Affect Relativistic Magnetized Shocks\n  Emission"
      ],
      "abstract":[
        "Values or principles are key elements of human society that influence people\nto behave and function according to an accepted standard set of social rules to\nmaintain social order. As AI systems are becoming ubiquitous in human society,\nit is a major concern that they could violate these norms or values and\npotentially cause harm. Thus, to prevent intentional or unintentional harm, AI\nsystems are expected to take actions that align with these principles. Training\nsystems to exhibit this type of behavior is difficult and often requires a\nspecialized dataset. This work presents a multi-modal dataset illustrating\nnormative and non-normative behavior in real-life situations described through\nnatural language and artistic images. This training set contains curated sets\nof images that are designed to teach young children about social principles. We\nargue that this is an ideal dataset to use for training socially normative\nagents given this fact.",
        "Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1\nuse enhanced reasoning through Chain-of-Thought (CoT). Their potential in\nhardware design, which relies on expert-driven iterative optimization, remains\nunexplored. This paper investigates whether reasoning LLMs can address\nchallenges in High-Level Synthesis (HLS) design space exploration and\noptimization. During HLS, engineers manually define pragmas\/directives to\nbalance performance and resource constraints. We propose an LLM-based\noptimization agentic framework that automatically restructures code, inserts\npragmas, and identifies optimal design points via feedback from HLs tools and\naccess to integer-linear programming (ILP) solvers. Experiments compare\nreasoning models against conventional LLMs on benchmarks using success rate,\nefficiency, and design quality (area\/latency) metrics, and provide the\nfirst-ever glimpse into the CoTs produced by a powerful open-source reasoning\nmodel like DeepSeek-R1.",
        "Aligning Large Language Models (LLMs) with human preferences is crucial for\ntheir deployment in real-world applications. Recent advancements in\nSelf-Rewarding Language Models suggest that an LLM can use its internal reward\nmodels (such as LLM-as-a-Judge) \\cite{yuanself} to generate preference data,\nimproving alignment performance without costly human annotation. However, we\nfind that different internal reward models within the same LLM often generate\ninconsistent preferences. This inconsistency raises concerns about the\nreliability of self-generated preference data, hinders overall alignment\nperformance, and highlights the need for further research to ensure reliable\nand coherent alignment with human preferences. To address this limitation, we\npropose Self-Consistent Internal Rewards (SCIR), a novel framework designed to\nenhance consistency among internal reward models during training. In each\ntraining step, we collect preference predictions from multiple pre-defined\ninternal reward models and enforce consistency and confidence through an\ninconsistency penalty mechanism, thereby improving the reliability of these\ninternal reward models. We selectively use data with consistent predictions for\npreference optimization, ensuring the quality of the preference data. By\nemploying self-consistent internal rewards, our method significantly improves\nthe alignment performance and reward modeling capability of LLMs, outperforming\nbaseline methods by a notable margin.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "Planning for autonomous systems typically requires reasoning with models at\ndifferent levels of abstraction, and the harmonization of two competing sets of\nobjectives: high-level mission goals that refer to an interaction of the system\nwith the external environment, and low-level platform constraints that aim to\npreserve the integrity and the correct interaction of the subsystems. The\ncomplicated interplay between these two models makes it very hard to reason on\nthe system as a whole, especially when the objective is to find plans with\nrobustness guarantees, considering the non-deterministic behavior of the lower\nlayers of the system.\n  In this paper, we introduce the problem of Platform-Aware Mission Planning\n(PAMP), addressing it in the setting of temporal durative actions. The PAMP\nproblem differs from standard temporal planning for its exists-forall nature:\nthe high-level plan dealing with mission goals is required to satisfy safety\nand executability constraints, for all the possible non-deterministic\nexecutions of the low-level model of the platform and the environment. We\npropose two approaches for solving PAMP. The first baseline approach\namalgamates the mission and platform levels, while the second is based on an\nabstraction-refinement loop that leverages the combination of a planner and a\nverification engine. We prove the soundness and completeness of the proposed\napproaches and validate them experimentally, demonstrating the importance of\nheterogeneous modeling and the superiority of the technique based on\nabstraction-refinement.",
        "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment.",
        "Understanding factors triggering or preventing undesirable health outcomes\nacross patient subpopulations is essential for designing targeted\ninterventions. While randomized controlled trials and expert-led patient\ninterviews are standard methods for identifying these factors, they can be\ntime-consuming and infeasible. Causal discovery offers an alternative to\nconventional approaches by generating cause-and-effect hypotheses from\nobservational data. However, it often relies on strong or untestable\nassumptions, which can limit its practical application. This work aims to make\ncausal discovery more practical by considering multiple assumptions and\nidentifying heterogeneous effects. We formulate the problem of discovering\ncauses and effect modifiers of an outcome, where effect modifiers are contexts\n(e.g., age groups) with heterogeneous causal effects. Then, we present a novel,\nend-to-end framework that incorporates an ensemble of causal discovery\nalgorithms and estimation of heterogeneous effects to discover causes and\neffect modifiers that trigger or inhibit the outcome. We demonstrate that the\nensemble approach improves robustness by enhancing recall of causal factors\nwhile maintaining precision. Our study examines the causes of repeat emergency\nroom visits for diabetic patients and hospital readmissions for ICU patients.\nOur framework generates causal hypotheses consistent with existing literature\nand can help practitioners identify potential interventions and patient\nsubpopulations to focus on.",
        "We developed and validated TRisk, a Transformer-based AI model predicting\n36-month mortality in heart failure patients by analysing temporal patient\njourneys from UK electronic health records (EHR). Our study included 403,534\nheart failure patients (ages 40-90) from 1,418 English general practices, with\n1,063 practices for model derivation and 355 for external validation. TRisk was\ncompared against the MAGGIC-EHR model across various patient subgroups. With\nmedian follow-up of 9 months, TRisk achieved a concordance index of 0.845 (95%\nconfidence interval: [0.841, 0.849]), significantly outperforming MAGGIC-EHR's\n0.728 (0.723, 0.733) for predicting 36-month all-cause mortality. TRisk showed\nmore consistent performance across sex, age, and baseline characteristics,\nsuggesting less bias. We successfully adapted TRisk to US hospital data through\ntransfer learning, achieving a C-index of 0.802 (0.789, 0.816) with 21,767\npatients. Explainability analyses revealed TRisk captured established risk\nfactors while identifying underappreciated predictors like cancers and hepatic\nfailure that were important across both cohorts. Notably, cancers maintained\nstrong prognostic value even a decade after diagnosis. TRisk demonstrated\nwell-calibrated mortality prediction across both healthcare systems. Our\nfindings highlight the value of tracking longitudinal health profiles and\nrevealed risk factors not included in previous expert-driven models.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "The objective of BioCreative8 Track 3 is to extract phenotypic key medical\nfindings embedded within EHR texts and subsequently normalize these findings to\ntheir Human Phenotype Ontology (HPO) terms. However, the presence of diverse\nsurface forms in phenotypic findings makes it challenging to accurately\nnormalize them to the correct HPO terms. To address this challenge, we explored\nvarious models for named entity recognition and implemented data augmentation\ntechniques such as synonym marginalization to enhance the normalization step.\nOur pipeline resulted in an exact extraction and normalization F1 score 2.6\\%\nhigher than the mean score of all submissions received in response to the\nchallenge. Furthermore, in terms of the normalization F1 score, our approach\nsurpassed the average performance by 1.9\\%. These findings contribute to the\nadvancement of automated medical data extraction and normalization techniques,\nshowcasing potential pathways for future research and application in the\nbiomedical domain.",
        "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps:\/\/github.com\/ZitongShi\/EPEAgent",
        "A reward-guided, gradient-free ParVI method, \\textit{R-ParVI}, is proposed\nfor sampling partially known densities (e.g. up to a constant). R-ParVI\nformulates the sampling problem as particle flow driven by rewards: particles\nare drawn from a prior distribution, navigate through parameter space with\nmovements determined by a reward mechanism blending assessments from the target\ndensity, with the steady state particle configuration approximating the target\ngeometry. Particle-environment interactions are simulated by stochastic\nperturbations and the reward mechanism, which drive particles towards high\ndensity regions while maintaining diversity (e.g. preventing from collapsing\ninto clusters). R-ParVI offers fast, flexible, scalable and stochastic sampling\nand inference for a class of probabilistic models such as those encountered in\nBayesian inference and generative modelling.",
        "Explainable recommendation systems leverage transparent reasoning to foster\nuser trust and improve decision-making processes. Current approaches typically\ndecouple recommendation generation from explanation creation, violating causal\nprecedence principles where explanatory factors should logically precede\noutcomes. This paper introduces a novel framework integrating structural causal\nmodels with large language models to establish causal consistency in\nrecommendation pipelines. Our methodology enforces explanation factors as\ncausal antecedents to recommendation predictions through causal graph\nconstruction and counterfactual adjustment. We particularly address the\nconfounding effect of item popularity that distorts personalization signals in\nexplanations, developing a debiasing mechanism that disentangles genuine user\npreferences from conformity bias. Through comprehensive experiments across\nmultiple recommendation scenarios, we demonstrate that CausalX achieves\nsuperior performance in recommendation accuracy, explanation plausibility, and\nbias mitigation compared to baselines.",
        "Relativistic jets from accreting black holes (BHs) radiate non-thermal\nemission which is highly variable in different time scales. Magnetic fields\nanchored to a rotating BH or accretion disc accelerate and collimate jets of\nthe BH systems. Previous studies on black holes of different mass scales,\nincluding supermassive and stellar-mass black holes, only report flux\nquasi-periodic oscillations in radio, optical, X-ray and gamma-ray bands. No\nquasi-periodic variations in polarization have yet been detected in any black\nhole systems. Here, we report the first detection of GHz radio polarization\noscillations in GRS 1915+105, which harbors a spinning stellar-mass BH with a\nrelativistic jet. Our observations show that during the increasing phase of\nradio emission, linear polarization and flux exhibit similar oscillation\nperiods of $\\sim 17$ and $33$ seconds, and their variation patterns\nanti-correlate with each other. These rare, short-period oscillations in both\npolarization and flux would be important to understand instabilities and\nspecial dynamics in magnetized jets.",
        "Optical computing offers potential for ultra high-speed and low latency\ncomputation by leveraging the intrinsic properties of light. Here, we explore\nthe use of highly nonlinear optical fibers (HNLFs) as platforms for optical\ncomputing based on the concept of Extreme Learning Machines. Task-independent\nevaluations are introduced to the field for the first time and focus on the\nfundamental metrics of effective dimensionality and consistency, which we\nexperimentally characterize for different nonlinear and dispersive conditions.\nWe show that input power and fiber characteristics significantly influence the\ndimensionality of the computational system, with longer fibers and higher\ndispersion producing up to 100 principal components (PCs) at input power levels\nof 30 mW, where the PC correspond to the linearly independent dimensions of the\nsystem. The spectral distribution of the PC's eigenvectors reveals that the\nhigh-dimensional dynamics facilitating computing through dimensionality\nexpansion are located within 40~nm of the pump wavelength at 1560~nm, providing\ngeneral insight for computing with nonlinear Schr\\\"odinger equation systems.\nTask-dependent results demonstrate the effectiveness of HNLFs in classifying\nMNIST dataset images. Using input data compression through PC analysis, we\ninject MNIST images of various input dimensionality into the system and study\nthe impact of input power upon classification accuracy. At optimized power\nlevels we achieve a classification test accuracy of 88\\%, significantly\nsurpassing the baseline of 83.7\\% from linear systems. Noteworthy, we find that\nbest performance is not obtained at maximal input power, i.e. maximal system\ndimensionality, but at more than one order of magnitude lower. The same is\nconfirmed regarding the MNIST image's compression, where accuracy is\nsubstantially improved when strongly compressing the image to less than 50 PCs.",
        "For over a decade, linear and symmetric weight updates have remained the\nelusive holy grail in neuromorphic computing. Here, we unveil a kinetically\ncontrolled molecular mechanism driving a near-ideal neuromorphic element,\ncapable of precisely modulating conductance linearly across 16,500 analog\nlevels spanning four orders of magnitude. Our findings, supported by\nexperimental data and mathematical modelling, demonstrate how nonlinear\nprocesses such as nucleation can be orchestrated within small perturbation\nregimes to achieve linearity. This establishes a groundwork for routinely\nrealizing these long-sought neuromorphic features across a broad range of\nmaterial systems.",
        "Long video generation remains a challenging and compelling topic in computer\nvision. Diffusion based models, among the various approaches to video\ngeneration, have achieved state of the art quality with their iterative\ndenoising procedures. However, the intrinsic complexity of the video domain\nrenders the training of such diffusion models exceedingly expensive in terms of\nboth data curation and computational resources. Moreover, these models\ntypically operate on a fixed noise tensor that represents the video, resulting\nin predetermined spatial and temporal dimensions. Although several high quality\nopen-source pretrained video diffusion models, jointly trained on images and\nvideos of varying lengths and resolutions, are available, it is generally not\nrecommended to specify a video length at inference that was not included in the\ntraining set. Consequently, these models are not readily adaptable to the\ndirect generation of longer videos by merely increasing the specified video\nlength. In addition to feasibility challenges, long-video generation also\nencounters quality issues. The domain of long videos is inherently more complex\nthan that of short videos: extended durations introduce greater variability and\nnecessitate long-range temporal consistency, thereby increasing the overall\ndifficulty of the task. We propose VideoMerge, a training-free method that can\nbe seamlessly adapted to merge short videos generated by pretrained\ntext-to-video diffusion model. Our approach preserves the model's original\nexpressiveness and consistency while allowing for extended duration and dynamic\nvariation as specified by the user. By leveraging the strengths of pretrained\nmodels, our method addresses challenges related to smoothness, consistency, and\ndynamic content through orthogonal strategies that operate collaboratively to\nachieve superior quality.",
        "Deep learning models (DLMs) achieve increasingly high performance both on\nstructured and unstructured data. They significantly extended applicability of\nmachine learning to various domains. Their success in making predictions,\ndetecting patterns and generating new data made significant impact on science\nand industry. Despite these accomplishments, DLMs are difficult to explain\nbecause of their enormous size. In this work, we propose a novel framework for\npost-hoc explaining and relating DLMs using redescriptions. The framework\nallows cohort analysis of arbitrary DLMs by identifying statistically\nsignificant redescriptions of neuron activations. It allows coupling neurons to\na set of target labels or sets of descriptive attributes, relating layers\nwithin a single DLM or associating different DLMs. The proposed framework is\nindependent of the artificial neural network architecture and can work with\nmore complex target labels (e.g. multi-label or multi-target scenario).\nAdditionally, it can emulate both pedagogical and decompositional approach to\nrule extraction. The aforementioned properties of the proposed framework can\nincrease explainability and interpretability of arbitrary DLMs by providing\ndifferent information compared to existing explainable-AI approaches.",
        "We propose a quantum protocol for efficiently learning and sampling\nmultivariate probability distributions that commonly appear in high-energy\nphysics. Our approach introduces a bivariate probabilistic model based on\ngeneralized Chebyshev polynomials, which is (pre-)trained as an explicit\ncircuit-based model for two correlated variables, and sampled efficiently with\nthe use of quantum Chebyshev transforms. As a key application, we study the\nfragmentation functions~(FFs) of charged pions and kaons from single-inclusive\nhadron production in electron-positron annihilation. We learn the joint\ndistribution for the momentum fraction $z$ and energy scale $Q$ in several\nfragmentation processes. Using the trained model, we infer the correlations\nbetween $z$ and $Q$ from the entanglement of the probabilistic model, noting\nthat the developed energy-momentum correlations improve model performance.\nFurthermore, utilizing the generalization capabilities of the quantum Chebyshev\nmodel and extended register architecture, we perform a fine-grid multivariate\nsampling relevant for FF dataset augmentation. Our results highlight the\ngrowing potential of quantum generative modeling for addressing problems in\nscientific discovery and advancing data analysis in high-energy physics.",
        "Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.",
        "Sound effects model design commonly uses digital signal processing techniques\nwith full control ability, but it is difficult to achieve realism within a\nlimited number of parameters. Recently, neural sound effects synthesis methods\nhave emerged as a promising approach for generating high-quality and realistic\nsounds, but the process of synthesizing the desired sound poses difficulties in\nterms of control. This paper presents a real-time neural synthesis model guided\nby a physically inspired model, enabling the generation of high-quality sounds\nwhile inheriting the control interface of the physically inspired model. We\nshowcase the superior performance of our model in terms of sound quality and\ncontrol.",
        "In complex systems, information propagation can be defined as diffused or\ndelocalized, weakly localized, and strongly localized. This study investigates\nthe application of graph neural network models to learn the behavior of a\nlinear dynamical system on networks. A graph convolution and attention-based\nneural network framework has been developed to identify the steady-state\nbehavior of the linear dynamical system. We reveal that our trained model\ndistinguishes the different states with high accuracy. Furthermore, we have\nevaluated model performance with real-world data. In addition, to understand\nthe explainability of our model, we provide an analytical derivation for the\nforward and backward propagation of our framework.",
        "Large Vision-Language Models (LVLMs) have shown impressive performance in\nvarious tasks. However, LVLMs suffer from hallucination, which hinders their\nadoption in the real world. Existing studies emphasized that the strong\nlanguage priors of LVLMs can overpower visual information, causing\nhallucinations. However, the positive role of language priors is the key to a\npowerful LVLM. If the language priors are too weak, LVLMs will struggle to\nleverage rich parameter knowledge and instruction understanding abilities to\ncomplete tasks in challenging visual scenarios where visual information alone\nis insufficient. Therefore, we propose a benchmark called LanP to rethink the\nimpact of Language Priors in LVLMs. It is designed to investigate how strong\nlanguage priors are in current LVLMs. LanP consists of 170 images and 340\ncorresponding well-designed questions. Extensive experiments on 25 popular\nLVLMs reveal that many LVLMs' language priors are not strong enough to\neffectively aid question answering when objects are partially hidden. Many\nmodels, including GPT-4 Turbo, exhibit an accuracy below 0.5 in such a\nscenario.",
        "Plasma kinetics, for both flat and curved spacetime, is conventionally\nperformed on the mass shell, a 7--dimensional time-phase space with a Vlasov\nvector field, also known as the Liouville vector field. The choice of this\ntime-phase space encodes the parameterisation of the underling 2nd order\nordinary differential equations. By replacing the Vlasov vector on time-phase\nspace with a bivector on an 8--dimensional sub-bundle of the tangent bundle, we\ncreate a parameterisation free version of Vlasov theory. This has a number of\nadvantages, which include working for lightlike and ultra-relativistic\nparticles, non metric connections, and metric-free and premetric theories. It\nalso works for theories where no time-phase space can exist for topological\ntopological reasons. An example of this is when we wish to consider all\ngeodesics, including spacelike geodesics.\n  We extend the particle density function to a 6--form on the subbundle of the\ntangent space, and define the transport equations, which correspond to the\nVlasov equation. We then show how to define the corresponding 3--current on\nspacetime. We discuss the stress-energy tensor needed for the Einstein-Vlasov\nsystem.\n  This theory can be generalised to create parameterisation invariant Vlasov\ntheories for many 2nd order theories, on arbitrary manifolds. The relationship\nto sprays and semi-sprays is given and examples from Finsler geometry are also\ngiven.",
        "The interconnection between the human lungs and other organs, such as the\nliver and kidneys, is crucial for understanding the underlying risks and\neffects of lung diseases and improving patient care. However, most research\nchest CT imaging is focused solely on the lungs due to considerations of cost\nand radiation dose. This restricted field of view (FOV) in the acquired images\nposes challenges to comprehensive analysis and hinders the ability to gain\ninsights into the impact of lung diseases on other organs. To address this, we\npropose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel\napproach to capture the inter-organ relationships from CT images and extend the\nFOV of chest CT images. Our approach first trains a variational autoencoder\n(VAE) to encode 2D axial CT slices individually, then stacks the latent\nrepresentations of the VAE to form a 3D context for training a latent diffusion\nmodel. Once trained, our approach extends the FOV of CT images in the\nz-direction by generating new axial slices in a zero-shot manner. We evaluated\nour approach on the National Lung Screening Trial (NLST) dataset, and results\nsuggest that it effectively extends the FOV to include the liver and kidneys,\nwhich are not completely covered in the original NLST data acquisition.\nQuantitative results on a held-out whole-body dataset demonstrate that the\ngenerated slices exhibit high fidelity with acquired data, achieving an SSIM of\n0.81.",
        "Pulmonary Tuberculosis (PTB) remains a major challenge for global health,\nespecially in areas with poor medical resources, where access to specialized\nmedical knowledge and diagnostic tools is limited. This paper presents an\nauxiliary diagnosis system for pulmonary tuberculosis based on Huawei MindSpore\nframework and Ascend310 edge computing chip. Using MobileNetV3 architecture and\nSoftmax cross entropy loss function with momentum optimizer. The system\noperates with FP16 hybrid accuracy on the Orange pie AIPro (Atlas 200 DK) edge\ndevice and performs well. In the test set containing 4148 chest images, the\nmodel accuracy reached 99.1\\% (AUC = 0.99), and the equipment cost was\ncontrolled within \\$150, providing affordable AI-assisted diagnosis scheme for\nprimary care.",
        "Decision Focused Learning has emerged as a critical paradigm for integrating\nmachine learning with downstream optimisation. Despite its promise, existing\nmethodologies predominantly rely on probabilistic models and focus narrowly on\ntask objectives, overlooking the nuanced challenges posed by epistemic\nuncertainty, non-probabilistic modelling approaches, and the integration of\nuncertainty into optimisation constraints. This paper bridges these gaps by\nintroducing innovative frameworks: (i) a non-probabilistic lens for epistemic\nuncertainty representation, leveraging intervals (the least informative\nuncertainty model), Contamination (hybrid model), and probability boxes (the\nmost informative uncertainty model); (ii) methodologies to incorporate\nuncertainty into constraints, expanding Decision-Focused Learning's utility in\nconstrained environments; (iii) the adoption of Imprecise Decision Theory for\nambiguity-rich decision-making contexts; and (iv) strategies for addressing\nsparse data challenges. Empirical evaluations on benchmark optimisation\nproblems demonstrate the efficacy of these approaches in improving decision\nquality and robustness and dealing with said gaps.",
        "Relativistic magnetized shocks, through the Synchrotron Maser Instability\n(SMI) mechanism, represent a promising framework for generating coherent\nradiations, potentially accounting for the enigmatic Fast Radio Bursts\n(FRBs)-cosmic radio transients with extreme luminosity. This study investigates\nhow the radiation reaction (RR) effect, induced by high-energy photon emissions\nduring SMI, significantly modifies particle dynamics and emission properties in\nmagnetized shocks. Through comprehensive Particle-In-Cell (PIC) simulations, we\ndemonstrate that RR effects fundamentally alter coherent cyclotron motion at\nshock fronts, producing distinct observational signatures: spectral broadening,\npeak frequency upshift, and enhanced radiation intensity. Our findings suggest\nthat RR-mediated magnetized shocks could provide a natural explanation for the\nbimodal energy distribution observed in repeating FRB 121102 and the positive\ncorrelation of luminosity-bandwidth between repeating and one-off FRBs in\nCHIME\/FRB catalog. These results support the magnetized shock as a viable\nsource of FRBs."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology",
    "start_abstract":"Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
      ],
      "abstract":[
        "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in\n  Recurrent Networks",
        "Freezing of Gait as a Complication of Pallidal Deep Brain Stimulation in\n  DYT- KMT2B Patients with Evidence of Striatonigral Degeneration",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study",
        "A Turing Test for Artificial Nets devoted to model Human Vision",
        "A UDP Packet Format Establishing Adress Event Representation\n  Communication Between Remote Neuromorphic and Biological Setups",
        "Asynchronous Hebbian\/anti-Hebbian networks",
        "On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "The tardigrade as an emerging model organism for systems neuroscience",
        "Intrinsic motivation as constrained entropy maximization",
        "Search for medium effects using jet axis decorrelation in inclusive jets\n  from PbPb collisions at $\\sqrt{s_\\text{NN}}$ = 5.02 TeV",
        "Colorful Helly via induced matchings",
        "Superlubric Motion of Wave-like Domain Walls in Sliding Ferroelectrics",
        "Extended string-net models with all anyons at finite temperature",
        "Holographic inflation and holographic dark energy from entropy of the\n  anti-de Sitter black hole",
        "A framework for Tate modules of abelian varieties under isogeny",
        "Monolayer transition metal dichalcogenides under finite-pulse polarized\n  radiation",
        "Constrained Fuel and Time Optimal 6DOF Powered Descent Guidance Using\n  Indirect Optimization",
        "More resourceful states improve quantum channel discrimination",
        "Fingerprint Matrix Concept for Detecting, Localizing and Characterizing\n  Targets in Complex Media",
        "Observing Hot Holographic Quark Star With Gravitational Waves",
        "Predicting the depth of the most recent common ancestor of a random\n  sample of $k$ species: the impact of phylogenetic tree shape",
        "Generating Networks to Target Assortativity via Archimedean Copula\n  Graphons",
        "Can Dark Stars account for the star formation efficiency excess at very\n  high redshifts?",
        "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays"
      ],
      "abstract":[
        "The organization of neurons into functionally related assemblies is a\nfundamental feature of cortical networks, yet our understanding of how these\nassemblies maintain distinct identities while sharing members remains limited.\nHere we analyze how spike-timing-dependent plasticity (STDP) shapes the\nformation and stability of overlapping neuronal assemblies in recurrently\ncoupled networks of spiking neuron models. Using numerical simulations and an\nassociated mean-field theory, we demonstrate that the temporal structure of the\nSTDP rule, specifically its degree of causality, critically determines whether\nassemblies that share neurons maintain segregation or merge together after\ntraining is completed. We find that causal STDP rules, where\npotentiation\/depression occurs strictly when presynaptic spikes precede\/proceed\npostsynaptic spikes, allow assemblies to remain distinct even with substantial\noverlap in membership. This stability arises because causal STDP effectively\ncancels the symmetric correlations introduced by common inputs from shared\nneurons. In contrast, acausal STDP rules lead to assembly fusion when overlap\nexceeds a critical threshold, due to unchecked growth of common input\ncorrelations. Our results provide theoretical insight into how\nspike-timing-dependent learning rules can support distributed representation\nwhere individual neurons participate in multiple assemblies while maintaining\nfunctional specificity.",
        "Background: Mutations in KMT2B are a recognized cause of early-onset complex\ndystonia, with deep brain stimulation (DBS) of the internal globus pallidus\n(GPi-DBS) being an effective treatment. However, gait impairment, particularly\nfreezing of gait (FOG), remains a significant challenge in DYT-KMT2B patients\npost-DBS. Objectives: To characterize the emergence of FOG in DYT-KMT2B\npatients treated with GPi-DBS and explore potential underlying mechanisms,\nincluding striatonigral degeneration. Methods: Five patients (four females)\nwith KMT2B-related dystonia and protein-truncating variants (PTVs) were\nretrospectively analyzed. Clinical progression, response to GPi-DBS, and the\npresence of FOG were documented. Dopaminergic function was assessed using\nDaTscan (SPECT for ^123I-ioflupane) in four patients. Results: FOG developed in\nall patients, with onset ranging from 1 to 15.5 years post-DBS. DaTscan\nabnormalities, indicative of bilateral striatal dopaminergic denervation, were\nobserved in four cases. Prior to DBS, all patients exhibited dystonia\nunresponsive to L-dopa, and post-DBS, FOG remained refractory to dopaminergic\ntreatment in most cases. Despite initial improvements in gait post-DBS, only\none patient maintained independent ambulation at the last follow-up.\nConclusions: FOG is an emerging complication in DYT-KMT2B patients with PTVs\nundergoing GPi-DBS, potentially linked to underlying striatonigral\ndegeneration. The findings suggest a need for long-term motor surveillance and\nconsideration of alternative therapeutic strategies, including dopaminergic\ntrials, in this patient population. Further studies are required to elucidate\nthe precise mechanisms driving DBS-related hypokinetic gait disturbances in\nDYT-KMT2B dystonia.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients.",
        "In this 2022 work we argued that, despite claims about successful modeling of\nthe visual brain using artificial nets, the problem is far from being solved\n(even for low-level vision). Examples of open issues include: where should we\nread from ANNs in order to reproduce human behavior?, this ad-hoc read-out is\nconsidered part of the brain model or not?, should we use artificial\npsychophysics or artificial physiology?, in the case of ANNs, artificial\nexperiments should literally match the experiments done with humans?. There is\na clear need of rigorous procedures for experimental tests for ANNs devoted to\nmodel the visual brain, and more generally, to understand ANNs devoted to\ngeneric vision tasks. Following our experience in using low-level facts from\nQuantitative Visual Neuroscience in computer vision, in this work we presented\nthe idea of developing a low-level dataset compiling the basic spatio-temporal\nand chromatic facts that are known to happen in the retina-V1 pathway, and they\nare not currently available in existing databases such as BrainScore. In our\nresults we checked the behavior of three recently proposed models with similar\narchitecture: (1) A parametric model tuned via Maximum Differentiation [Malo &\nSimoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.\n19], (2) A non-parametric model called PerceptNet tuned to maximize the\ncorrelation with human opinion on subjective distortions [Hepburn et al. IEEE\nICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for\nimage segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).\nResults on 10 compelling psycho\/physio visual facts show that the first model\nis the one with closer behavior to the humans in terms of receptive fields, but\nmore interestingly, on the nonlinear behavior when facing complex\nspatio-chromatic patterns of a range of luminances and contrasts.",
        "In the field of brain-machine interfaces, biohybrids offer an interesting new\nperspective, as in them, the technological side acts like a closed-loop\nextension or real counterpart of biological tissue, instead of the usual open\nloop approaches in tranditional BMI. To achieve a credible counterpart to\nbiological tissue, biohybrids usually employ one or several neuromorphic\ncomponents as the hardware half of the biohybrid. However, advanced\nneuromorphic circuit such as memristor crossbars usually operate best in a\ndedicated lab with corresponding support equipment. The same is true for\nbiological tissue, which makes co-locating all of the parts of a biohybrid in\nthe same lab challenging. Here, we present as solution to this co-location\nissue a simple method to connect biohybrids via the internet by a custom UDP\npacket format. We show that the characteristics achieved with our solution\n(jitter, delay, packet loss, packet reordering) on a standard internet\nconnection are compatible with various biohybrid processing paradigms, and we\npresent a short three-ways experiment as proof-of-concept. The described UDP\nformat has been employed to link biohybrids and neuromorphic circuits in four\ndifferent EC-funded projects.",
        "Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
        "One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "We present the case for developing the tardigrade (Hypsibius exemplaris) into\na model organism for systems neuroscience. These microscopic, transparent\nanimals (~300-500 microns) are among the smallest known to possess both limbs\n(eight) and eyes (two), with a nervous system of only a few hundred neurons\norganized into a multi-lobed brain, ventral nerve cord, and a series of ganglia\nalong the body. Despite their neuroanatomical simplicity, tardigrades exhibit\ncomplex behaviors, including multi-limbed walking gaits, individual limb\ngrasping, phototaxis, and transitions between active and dormant states. These\nbehaviors position tardigrades as a uniquely powerful system for addressing\ncertain fundamental questions in systems neuroscience, such as: How do nervous\nsystems coordinate multi-limbed behaviors? How are top-down and bottom-up motor\ncontrol systems integrated? How is stereovision-guided navigation implemented?\nWhat mechanisms underlie neural resilience and recovery during environmental\nstress? We review current knowledge of tardigrade neuroanatomy, behavior, and\ngenomics, and we identify opportunities and challenges for leveraging their\nunique biology. We propose developing essential neuroscientific tools for\ntardigrades, including genetic engineering and live neuroimaging, alongside\nbehavioral assays linking neural activity to outputs. Leveraging their\nevolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can\nadapt existing toolkits to accelerate tardigrade research - providing a bridge\nbetween simpler invertebrate systems and more complex neural architectures.",
        "\"Intrinsic motivation\" refers to the capacity for intelligent systems to be\nmotivated endogenously, i.e. by features of agential architecture itself rather\nthan by learned associations between action and reward. This paper views active\ninference, empowerment, and other formal accounts of intrinsic motivation as\nvariations on the theme of constrained maximum entropy inference, providing a\ngeneral perspective on intrinsic motivation complementary to existing\nframeworks. The connection between free energy and empowerment noted in\nprevious literature is further explored, and it is argued that the\nmaximum-occupancy approach in practice incorporates an implicit model-evidence\nconstraint.",
        "The jet axis decorrelation in inclusive jets is studied using lead-lead\n(PbPb) collisions at a center-of-mass energy per nucleon pair of 5.02 TeV. The\njet axis decorrelation is defined as the angular difference between two\ndefinitions of the jet axis. It is obtained by applying two recombination\nschemes on all the constituents of a given jet reconstructed by the anti-\\kt\nsequential algorithm with a distance parameter of $R$ = 0.4. The data set,\ncorresponding to an integrated luminosity of 0.66 nb$^{-1}$, was collected in\n2018 with the CMS detector at the CERN LHC. The jet axis decorrelations are\nexamined across collision centrality selections and intervals of jet transverse\nmomentum. A centrality dependent evolution of the measured distributions is\nobserved, with a progressive narrowing seen in more central events. This\nnarrowing could result from medium-induced modification of the internal jet\nstructure or reflect color charge effects in energy loss. This new measurement\nprobes jet substructure in previously unexplored kinematic domains and show\ngreat promise for providing new insights on the color charge dependence of\nenergy loss to jet-quenching models.",
        "We establish a theorem regarding the maximum size of an {\\it{induced}}\nmatching in the bipartite complement of the incidence graph of a set system\n$(X,\\mathcal{F})$. We show that this quantity plus one provides an upper bound\non the colorful Helly number of this set system, i.e. the minimum positive\ninteger $N$ for which the following statement holds: if finite subfamilies\n$\\mathcal{F}_1,\\ldots, \\mathcal{F}_{N} \\subset \\mathcal{F}$ are such that\n$\\cap_{F \\in \\mathcal{F}_{i}} F = 0$ for every $i=1,\\ldots,N$, then there\nexists $F_i \\in \\mathcal{F}_i$ such that $F_1 \\cap \\ldots \\cap F_{N} =\n\\emptyset$. We will also discuss some natural refinements of this result and\napplications.",
        "Sliding ferroelectrics constructed from stacked nonpolar monolayers enable\nout-of-plane polarization in two dimensions with exceptional properties,\nincluding ultrafast switching speeds and fatigue-free behavior. However, the\nwidely accepted switching mechanism, which posits synchronized long-distance\nin-plane translation of entire atomic layers driven by an out-of-plane electric\nfield, has shown inconsistencies with experimental observations. We demonstrate\nthat this spinodal decomposition-like homogeneous switching process violates\nNeumann's principle and is unlikely to occur due to symmetry constraint.\nInstead, symmetry-breaking domain walls (DWs) and the tensorial nature of Born\neffective charges are critical for polarization reversal, underscoring the\nquantum nature of sliding ferroelectrics. Using the Bernal-stacked $h$-BN\nbilayer as a model system, we discover that the coherent propagation of wide,\nwave-like domain walls is the key mechanism for ferroelectric switching. This\nmechanism fundamentally differs from the layer-by-layer switching associated\nwith narrow domain walls, which has been established for over sixty years in\nperovskite ferroelectrics. Moreover, these wave-like DWs exhibit superlubric\ndynamics, achieving ultrahigh velocities of approximately 4000 m\/s at room\ntemperature and displaying an anomalous cooling-promoted switching speed. The\nunexpected emergence of DW superlubricity in sliding ferroelectrics presents\nnew avenues for enhancing key performance metrics and offers exciting\nopportunities for applications in cryogenic environments.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "Based on the entropy of anti-de Sitter black hole, a new holographic dark\nenergy model has been proposed. When the Hubble horizon and particle horizon\nare chosen as the IR cutoff, the late-time accelerated expansion of universe is\nrealized. In this paper, we consider the Hubble horizon as the IR cutoff to\ninvestigate holographic inflation and slow-roll inflation in this model. We\nfind that slow-roll inflation with the chaotic potential $V_{0}\\phi^{n}$ is\nfavored by Planck results for some special cases, such as $n=1\/3$ and $n=1\/2$,\nwhile holographic inflation is not supported by Planck results. Then, we\nanalyze the reheating temperature and the number of reheating e-folds in this\nmodel, and we find that the results favor the cases $n=1\/3$ and $n=1\/2$.\nFinally, we use the dynamical analysis method, statefinder diagnostic pairs,\nand the Hubble diagram to analyze this model. Our results indicate that when\n$b^{2}$ takes a small value, this model cannot be distinguished from the\nstandard $\\Lambda$CDM model and can serve as an alternative to it.",
        "We explain the linear algebraic framework provided by Tate modules of\nisogenous abelian varieties in a category-theoretic way.",
        "Recent advances in time-resolved angle-resolved photoemission spectroscopy\nhave enabled access to ultrafast electron states and their spin dynamics in\nsolids. Atomically thin transition metal dichalcogenides are paradigmatic\ntwo-dimensional materials where electron momentum and spin degrees of freedom\nare coupled, being suitable candidates for time-resolved spectroscopy studies.\nIn this work, we present a thorough study of the electron dynamics when these\nmaterials are subject to an intense finite-pulse driving radiation. We extend\nthe scope of the conventional Floquet engineering and rely of the so-called\n$t-t^{\\prime}$ formalism to deal with driving fields described with two\ndistinct time scales, namely the envelope amplitude timescale and the time\nperiod of the external field. The interplay between the finite-pulse timescales\nand the intrinsic properties of the electrons gives rise to transient valley\npolarization and dynamical modifications of band structures, revealed by the\ntime-dependent circular dichroism of the sample.",
        "Powered descent guidance (PDG) problems subject to six-degrees-of-freedom\n(6DOF) dynamics allow for enforcement of practical attitude constraints.\nHowever, numerical solutions to 6DOF PDG problems are challenging due to fast\nrotational dynamics coupled with translational dynamics, and the presence of\nhighly nonlinear state\/control path inequality constraints. In this work,\nconstrained fuel- and time-optimal 6DOF PDG problems are solved leveraging a\nregularized indirect method, subject to inequality constraints on the thrust\nmagnitude, thruster gimbal angle, rocket tilt angle, glideslope angle, and\nangular velocity magnitude. To overcome the challenges associated with solving\nthe resulting multipoint boundary-value problems (MPBVPs), the state-only path\ninequality constraints (SOPICs) are enforced through an interior penalty\nfunction method, which embeds the resulting MPBVPs into a multi-parameter\nsmooth neighboring families of two-point BVPs. Extremal solutions are obtained\nusing an indirect multiple-shooting solution method with numerical\ncontinuation. Moreover, an empirical relation is derived for the\ndirectly-adjoined Lagrange multipliers associated with SOPICs. The fuel- and\ntime-optimal trajectories are compared against solutions of DIDO -- a capable\npseudospectral-based software for solving practical constrained optimal control\nproblems.",
        "One of the key issues in quantum discrimination problems is understanding the\nextent of the advantages in discrimination performance when using resource\nstates compared to resourceless states. We show that in any resource theory of\nstates, which may not be convex, the extent to which the maximum average\nsuccess probability can be improved in quantum channel discrimination problems\nwithout using auxiliary systems can be precisely quantified by the robustness\nmeasure. Furthermore, we demonstrate that the robustness measure can also\nquantify the improvement in channel discrimination problems that use auxiliary\nsystems. Using these findings, resources can be fully characterized to achieve\nhigher success probabilities than any state without the given resource in\nchannel discrimination problems.",
        "As waves propagate through a complex medium, they undergo multiple scattering\nevents. This phenomenon is detrimental to imaging, as it causes a full blurring\nof the image beyond a transport mean free path. Here, we show how to detect,\nlocalize, and characterize any scattering target through the reflection matrix\nof the complex medium in which this target is embedded and thus hidden from\ndirect view. More precisely, we introduce a fingerprint operator that contains\nthe specific signature of the target with respect to its environment. Applied\nto the recorded reflection matrix, this operator provides a likelihood index of\nthe target in any given state, despite the scattering fog induced by the\nsurrounding environment. This state can be the target position for localization\npurposes, its shape for characterization, or any other parameter that\ninfluences the target response. Our concept is versatile and broadly applicable\nto different type of waves for which multi-element technology allows a\nreflection matrix to be measured. We demonstrate this here explicitly by\nperforming different proof-of-concept experiments with ultrasound on targets\nburied inside a strongly scattering granular suspension, on lesion markers for\nclinical applications, and on the architecture of muscle tissue.",
        "We extract the equation of state for hot quark matter from a holographic\n$2+1$ flavor QCD model, which could form the core of a stable compact star. By\nadding a thin hadron shell, a new type of hybrid star is constructed. With the\ntemperature serving as a parameter, the EoS varies and we obtain stable stars\nwith the maximum mass of around 23 to 30 solar masses, and the compactness\naround $0.1$. The I-Love-Q-C relations are further discussed, and compared with\nthe neutron star cases. These compact stars are candidates for black hole\nmimickers, which could be observed by gravitational waves and distinguished by\nproperties like nonzero tidal Love number and electromagnetic signals.",
        "We consider the following question: how close to the ancestral root of a\nphylogenetic tree is the most recent common ancestor of $k$ species randomly\nsampled from the tips of the tree? For trees having shapes predicted by the\nYule-Harding model, it is known that the most recent common ancestor is likely\nto be close to (or equal to) the root of the full tree, even as $n$ becomes\nlarge (for $k$ fixed). However, this result does not extend to models of tree\nshape that more closely describe phylogenies encountered in evolutionary\nbiology. We investigate the impact of tree shape (via the Aldous\n$\\beta-$splitting model) to predict the number of edges that separate the most\nrecent common ancestor of a random sample of $k$ tip species and the root of\nthe parent tree they are sampled from. Both exact and asymptotic results are\npresented. We also briefly consider a variation of the process in which a\nrandom number of tip species are sampled.",
        "We develop an approach to generate random graphs to a target level of\nassortativity by using copula structures in graphons. Unlike existing random\ngraph generators, we do not use rewiring or binning approaches to generate the\ndesired random graph. Instead, we connect Archimedean bivariate copulas to\ngraphons in order to produce flexible models that can generate random graphs to\ntarget assortativity. We propose three models that use the copula distribution\nfunction, copula density function and their mixed tensor product to produce\nnetworks. We express the assortativity coefficient in terms of homomorphism\ndensities. Establishing this relationship forges a connection between the\nparameter of the copula and the frequency of subgraphs in the generated\nnetwork. Therefore, our method attains a desired the subgraph distribution as\nwell as the target assortativity. We establish the homomorphism densities and\nassortativity coefficient for each of the models. Numerical examples\ndemonstrate the ability of the proposed models to produce graphs with different\nlevels of assortativity.",
        "The James Webb Space Telescope (JWST) has recently conducted observations of\nmassive galaxies at high redshifts, revealing a notable anomaly in their star\nformation efficiency (SFE). Motivated by the recent identification of three\n$\\sim 10^{6}M_\\odot$ dark star candidates, we investigate whether dark stars\ncan be the origin of the SFE excess. It turns out that the excess can be\nreproduced by a group of dark stars with $M \\gtrsim 10^{3}\\, \\rm M_{\\odot}$,\nbecause of their domination in generating primary UV radiation in high-redshift\ngalaxies. The genesis of these dark stars is attributed to the capture of\nWeakly Interacting Massive Particles (WIMPs) within a mass range of tens of GeV\nto a few TeV. However, if the top-heavy initial mass function of dark stars\nholds up to $\\sim 10^{5}M_\\odot$, the relic black holes stemming from their\ncollapse would be too abundant to be consistent with the current observations\nof Massive Compact Halo Objects (MACHOs). We thus suggest that just a small\nfraction of SFE excess may be contributed by the very massive dark stars and\nthe majority likely originated from other reasons such as the Population III\nstars in view of their rather similar UV radiation efficiencies.",
        "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by up to two orders of\nmagnitude."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's",
    "start_abstract":"Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment .",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
      ],
      "abstract":[
        "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "Causal Discovery and Inference towards Urban Elements and Associated\n  Factors",
        "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions",
        "WOFOSTGym: A Crop Simulator for Learning Annual and Perennial Crop\n  Management Strategies",
        "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In\n  Open Domains",
        "ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural\n  Faithfulness in SpArX",
        "Benchmarking Reasoning Robustness in Large Language Models",
        "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search",
        "Don't Get Too Excited -- Eliciting Emotions in LLMs",
        "LLM-based Corroborating and Refuting Evidence Retrieval for Scientific\n  Claim Verification",
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "Bridging the Communication Gap: Evaluating AI Labeling Practices for\n  Trustworthy AI Development",
        "Initial Findings on Sensor based Open Vocabulary Activity Recognition\n  via Text Embedding Inversion",
        "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
        "Invariant measure for the process viewed from the particle for 2D random\n  walks in Dirichlet environment",
        "Observation-Based Iterative Map for Solar Cycles. II. The Gnevyshev-Ohl\n  Rule and its Generation Mechanism",
        "Adaptive Variational Inference in Probabilistic Graphical Models: Beyond\n  Bethe, Tree-Reweighted, and Convex Free Energies",
        "On a class of high dimensional linear regression methods with debiasing\n  and thresholding",
        "Dynamic Refinement of Pressure Decomposition in Navier-Stokes Equations",
        "Homological data on the periodic structure of self-maps on wedge sums",
        "Making Them a Malicious Database: Exploiting Query Code to Jailbreak\n  Aligned Large Language Models",
        "Robust Multimodal Learning via Cross-Modal Proxy Tokens",
        "In-Context Meta LoRA Generation",
        "TinySense: A Lighter Weight and More Power-efficient Avionics System for\n  Flying Insect-scale Robots",
        "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven\n  Surface Normal-aware Tracking and Mapping",
        "Comprehensive Analog Signal Processing Platform Enabled with Acoustic\n  Charge Transport in Two-dimensional Materials",
        "A Comparative Performance Analysis of Classification and Segmentation\n  Models on Bangladeshi Pothole Dataset",
        "ODPG: Outfitting Diffusion with Pose Guided Condition"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "To uncover the city's fundamental functioning mechanisms, it is important to\nacquire a deep understanding of complicated relationships among citizens,\nlocation, and mobility behaviors. Previous research studies have applied direct\ncorrelation analysis to investigate such relationships. Nevertheless, due to\nthe ubiquitous confounding effects, empirical correlation analysis may not\naccurately reflect underlying causal relationships among basic urban elements.\nIn this paper, we propose a novel urban causal computing framework to\ncomprehensively explore causalities and confounding effects among a variety of\nfactors across different types of urban elements. In particular, we design a\nreinforcement learning algorithm to discover the potential causal graph, which\ndepicts the causal relations between urban factors. The causal graph further\nserves as the guidance for estimating causal effects between pair-wise urban\nfactors by propensity score matching. After removing the confounding effects\nfrom correlations, we leverage significance levels of causal effects in\ndownstream urban mobility prediction tasks. Experimental studies on open-source\nurban datasets show that the discovered causal graph demonstrates a\nhierarchical structure, where citizens affect locations, and they both cause\nchanges in urban mobility behaviors. Experimental results in urban mobility\nprediction tasks further show that the proposed method can effectively reduce\nconfounding effects and enhance performance of urban computing tasks.",
        "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
        "We introduce WOFOSTGym, a novel crop simulation environment designed to train\nreinforcement learning (RL) agents to optimize agromanagement decisions for\nannual and perennial crops in single and multi-farm settings. Effective crop\nmanagement requires optimizing yield and economic returns while minimizing\nenvironmental impact, a complex sequential decision-making problem well suited\nfor RL. However, the lack of simulators for perennial crops in multi-farm\ncontexts has hindered RL applications in this domain. Existing crop simulators\nalso do not support multiple annual crops. WOFOSTGym addresses these gaps by\nsupporting 23 annual crops and two perennial crops, enabling RL agents to learn\ndiverse agromanagement strategies in multi-year, multi-crop, and multi-farm\nsettings. Our simulator offers a suite of challenging tasks for learning under\npartial observability, non-Markovian dynamics, and delayed feedback.\nWOFOSTGym's standard RL interface allows researchers without agricultural\nexpertise to explore a wide range of agromanagement problems. Our experiments\ndemonstrate the learned behaviors across various crop varieties and soil types,\nhighlighting WOFOSTGym's potential for advancing RL-driven decision support in\nagriculture.",
        "We explore neuro-symbolic approaches to generalize actionable knowledge,\nenabling embodied agents to tackle complex tasks more effectively in\nopen-domain environments. A key challenge for embodied agents is the\ngeneralization of knowledge across diverse environments and situations, as\nlimited experiences often confine them to their prior knowledge. To address\nthis issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual\nlearner that emulates the hypothetico-deductive model by continually\nformulating and validating knowledge from limited experiences through the\ncombined use of Large Language Models (LLMs) and symbolic tools. Specifically,\nwe devise a contrastive generality improvement scheme within NeSyC, which\niteratively generates hypotheses using LLMs and conducts contrastive validation\nvia symbolic tools. This scheme reinforces the justification for admissible\nactions while minimizing the inference of inadmissible ones. Additionally, we\nincorporate a memory-based monitoring scheme that efficiently detects action\nerrors and triggers the knowledge refinement process across domains.\nExperiments conducted on diverse embodied task benchmarks-including ALFWorld,\nVirtualHome, Minecraft, RLBench, and a real-world robotic scenario-demonstrate\nthat NeSyC is highly effective in solving complex embodied tasks across a range\nof open-domain environments.",
        "In the field of Explainable Artificial Intelligence (XAI), argumentative XAI\napproaches have been proposed to represent the internal reasoning process of\ndeep neural networks in a more transparent way by interpreting hidden nodes as\narguements. However, as the number of layers increases, existing compression\nmethods simplify all layers at once, which lead to high accumulative\ninformation loss. To compensate for this, we propose an iterative\nlayer-by-layer compression technique in which each layer is compressed\nseparately and the reduction error in the next layer is immediately compensated\nfor, thereby improving the overall input-output and structural fidelity of the\nmodel. Experiments on the Breast Cancer Diagnosis dataset show that, compared\nto traditional compression, the method reduces input-output and structural\nunfaithfulness, and maintains a more consistent attack-support relationship in\nthe Argumentative Explanation scheme. This is significant because it provides a\nnew way to make complex MLP models more compact while still conveying their\ninternal inference logic without distortion.",
        "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.",
        "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose $\\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a\nnovel inference-time framework that generalizes repeated sampling with\nprincipled multi-turn exploration and exploitation. At each node in the search\ntree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new\ncandidate responses or \"go deeper\" by revisiting existing ones based on\nexternal feedback signals. We evaluate our method on complex coding and\nengineering tasks using frontier models. Empirical results show that AB-MCTS\nconsistently outperforms both repeated sampling and standard MCTS, underscoring\nthe importance of combining the response diversity of LLMs with multi-turn\nsolution refinement for effective inference-time scaling.",
        "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
        "In this paper, we introduce CIBER (Claim Investigation Based on Evidence\nRetrieval), an extension of the Retrieval-Augmented Generation (RAG) framework\ndesigned to identify corroborating and refuting documents as evidence for\nscientific claim verification. CIBER addresses the inherent uncertainty in\nLarge Language Models (LLMs) by evaluating response consistency across diverse\ninterrogation probes. By focusing on the behavioral analysis of LLMs without\nrequiring access to their internal information, CIBER is applicable to both\nwhite-box and black-box models. Furthermore, CIBER operates in an unsupervised\nmanner, enabling easy generalization across various scientific domains.\nComprehensive evaluations conducted using LLMs with varying levels of\nlinguistic proficiency reveal CIBER's superior performance compared to\nconventional RAG approaches. These findings not only highlight the\neffectiveness of CIBER but also provide valuable insights for future\nadvancements in LLM-based scientific claim verification.",
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "As artificial intelligence (AI) becomes integral to economy and society,\ncommunication gaps between developers, users, and stakeholders hinder trust and\ninformed decision-making. High-level AI labels, inspired by frameworks like EU\nenergy labels, have been proposed to make the properties of AI models more\ntransparent. Without requiring deep technical expertise, they can inform on the\ntrade-off between predictive performance and resource efficiency. However, the\npractical benefits and limitations of AI labeling remain underexplored. This\nstudy evaluates AI labeling through qualitative interviews along four key\nresearch questions. Based on thematic analysis and inductive coding, we found a\nbroad range of practitioners to be interested in AI labeling (RQ1). They see\nbenefits for alleviating communication gaps and aiding non-expert\ndecision-makers, however limitations, misunderstandings, and suggestions for\nimprovement were also discussed (RQ2). Compared to other reporting formats,\ninterviewees positively evaluated the reduced complexity of labels, increasing\noverall comprehensibility (RQ3). Trust was influenced most by usability and the\ncredibility of the responsible labeling authority, with mixed preferences for\nself-certification versus third-party certification (RQ4). Our Insights\nhighlight that AI labels pose a trade-off between simplicity and complexity,\nwhich could be resolved by developing customizable and interactive labeling\nframeworks to address diverse user needs. Transparent labeling of resource\nefficiency also nudged interviewee priorities towards paying more attention to\nsustainability aspects during AI development. This study validates AI labels as\na valuable tool for enhancing trust and communication in AI, offering\nactionable guidelines for their refinement and standardization.",
        "Conventional human activity recognition (HAR) relies on classifiers trained\nto predict discrete activity classes, inherently limiting recognition to\nactivities explicitly present in the training set. Such classifiers would\ninvariably fail, putting zero likelihood, when encountering unseen activities.\nWe propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this\nlimitation by first converting each activity into natural language and breaking\nit into a sequence of elementary motions. This descriptive text is then encoded\ninto a fixed-size embedding. The model is trained to regress this embedding,\nwhich is subsequently decoded back into natural language using a pre-trained\nembedding inversion model. Unlike other works that rely on auto-regressive\nlarge language models (LLMs) at their core, OV-HAR achieves open vocabulary\nrecognition without the computational overhead of such models. The generated\ntext can be transformed into a single activity class using LLM prompt\nengineering. We have evaluated our approach on different modalities, including\nvision (pose), IMU, and pressure sensors, demonstrating robust generalization\nacross unseen activities and modalities, offering a fundamentally different\nparadigm from contemporary classifiers.",
        "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs.",
        "In this paper, we consider random walks in Dirichlet random environment\n(RWDE) on $\\mathbb{Z}^2$. We prove that, if the RWDE is recurrent (which is\nstrongly conjectured when the weights are symmetric), then there does not exist\nany invariant measure for the process viewed from the particle which is\nabsolutely continuous with respect to the static law of the environment.\nBesides, if the walk is directional transient and under condition\n$\\mathbf{(T')}$, we prove that there exists such an invariant probability\nmeasure if the trapping parameter verifies $\\kappa > 1$ or after acceleration\nof the process by a local function of the environment. This gives strong credit\nto a conjectural classification of cases of existence or non-existence of the\ninvariant measure for two dimensional RWDE. The proof is based on a new\nidentity, stated on general finite graphs, which is inspired by the\nrepresentation of the $\\star$-VRJP, a non-reversible generalization of the\nVertex reinforced Jump Process, in terms of random Schr\\\"odinger operators. In\nthe case of RWDE on 1D graph, the previous identity entails also a discrete\nanalogue of the Matsumoto-Yor property for Brownian motion.",
        "The Gnevyshev-Ohl (G-O) rule, also known as the even-odd effect, is an\nimportant observational phenomenon in solar cycles, suggesting that cycles with\neven indices tend to be followed by stronger cycles. The rule is considered to\nbe related to the solar dynamo, which drives the evolution of the Sun's\nlarge-scale magnetic field. However, observational studies of the G-O rule have\nrevealed inconsistencies, particularly regarding long-term variations and the\nunderlying physical mechanisms. In this study, we use an iterative map derived\nwithin the framework of the Babcock-Leighton (BL) dynamo to analyze the G-O\nrule. We investigate comprehensive and definitive forms of the G-O rule using\nboth a sufficiently large number of solar cycles and a limited number of solar\ncycles. Our findings indicate a higher probability for an arbitrary cycle to be\nfollowed by a stronger cycle instead of weaker, regardless of even or odd. Over\ntime spans comparable to historical observations, cycles exhibit periods that\nfollow both the G-O rule and the reversed G-O rule, without a statistically\nsignificant preference, consistent with the observed variability of the G-O\nrule. The occurrence of the reversed G-O rule is random, rather than periodic.\nThe G-O rule emerges as a result of the nonlinearity and stochasticity inherent\nin the BL mechanism. These results advance our understanding of the solar cycle\nand pave the way for improved solar dynamo modeling.",
        "Variational inference in probabilistic graphical models aims to approximate\nfundamental quantities such as marginal distributions and the partition\nfunction. Popular approaches are the Bethe approximation, tree-reweighted, and\nother types of convex free energies. These approximations are efficient but can\nfail if the model is complex and highly interactive. In this work, we analyze\ntwo classes of approximations that include the above methods as special cases:\nfirst, if the model parameters are changed; and second, if the entropy\napproximation is changed. We discuss benefits and drawbacks of either approach,\nand deduce from this analysis how a free energy approximation should ideally be\nconstructed. Based on our observations, we propose approximations that\nautomatically adapt to a given model and demonstrate their effectiveness for a\nrange of difficult problems.",
        "In this paper, we introduce a unified framework, inspired by classical\nregularization theory, for designing and analyzing a broad class of linear\nregression approaches. Our framework encompasses traditional methods like least\nsquares regression and Ridge regression, as well as innovative techniques,\nincluding seven novel regression methods such as Landweber and Showalter\nregressions. Within this framework, we further propose a class of debiased and\nthresholded regression methods to promote feature selection, particularly in\nterms of sparsity. These methods may offer advantages over conventional\nregression techniques, including Lasso, due to their ease of computation via a\nclosed-form expression. Theoretically, we establish consistency results and\nGaussian approximation theorems for this new class of regularization methods.\nExtensive numerical simulations further demonstrate that the debiased and\nthresholded counterparts of linear regression methods exhibit favorable finite\nsample performance and may be preferable in certain settings.",
        "In this work, the local decomposition of pressure in the Navier-Stokes\nequations is dynamically refined to prove that a relevant critical energy of a\nsuitable Leray-type solution inside a backward paraboloid, regardless of its\naperture is controlled near the vertex by a critical behavior confined to a\nneighborhood of the paraboloid's boundary. This neighborhood excludes the\ninterior near the vertex and remains separated from the temporal profile of the\nvertex, except at the vertex itself.",
        "In this article, we study the periodic points for continuous self-maps on the\nwedge sum of topological manifolds, exhibiting a particular combinatorial\nstructure. We compute explicitly the Lefschetz numbers, the Dold coefficients\nand consider its set of algebraic periods. Moreover, we study the special case\nof maps on the wedge sum of tori, and show some of the homological obstructions\npresent in defining these maps.",
        "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is\navailable at https:\/\/github.com\/horizonsinzqs\/QueryAttack.",
        "Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality. To efficiently learn the approximation for\nthe missing modality via CMPTs with minimal computational overhead, we employ\nlow-rank adapters in frozen unimodal encoders and jointly optimize an alignment\nloss with a task-specific loss. Extensive experiments on five multimodal\ndatasets show that our method outperforms state-of-the-art baselines across\nvarious missing rates while achieving competitive results in complete-modality\nsettings. Overall, our method offers a flexible and efficient solution for\nrobust multimodal learning. The code and pretrained models will be released on\nGitHub.",
        "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
        "In this paper, we introduce advances in the sensor suite of an autonomous\nflying insect robot (FIR) weighing less than a gram. FIRs, because of their\nsmall weight and size, offer unparalleled advantages in terms of material cost\nand scalability. However, their size introduces considerable control\nchallenges, notably high-speed dynamics, restricted power, and limited payload\ncapacity. While there have been advancements in developing lightweight sensors,\noften drawing inspiration from biological systems, no sub-gram aircraft has\nbeen able to attain sustained hover without relying on feedback from external\nsensing such as a motion capture system. The lightest vehicle capable of\nsustained hovering -- the first level of ``sensor autonomy'' -- is the much\nlarger 28 g Crazyflie. Previous work reported a reduction in size of that\nvehicle's avionics suite to 187 mg and 21 mW. Here, we report a further\nreduction in mass and power to only 78.4 mg and 15 mW. We replaced the laser\nrangefinder with a lighter and more efficient pressure sensor, and built a\nsmaller optic flow sensor around a global-shutter imaging chip. A Kalman Filter\n(KF) fuses these measurements to estimate the state variables that are needed\nto control hover: pitch angle, translational velocity, and altitude. Our system\nachieved performance comparable to that of the Crazyflie's estimator while in\nflight, with root mean squared errors of 1.573 deg, 0.186 m\/s, and 0.136 m,\nrespectively, relative to motion capture.",
        "Simultaneous Localization and Mapping (SLAM) is essential for precise\nsurgical interventions and robotic tasks in minimally invasive procedures.\nWhile recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM\nwith high-quality novel view synthesis and fast rendering, these systems\nstruggle with accurate depth and surface reconstruction due to multi-view\ninconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between\nthe reconstructed frames. In this work, we present Endo-2DTAM, a real-time\nendoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these\nchallenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which\nconsists of tracking, mapping, and bundle adjustment modules for geometrically\naccurate reconstruction. Our robust tracking module combines point-to-point and\npoint-to-plane distance metrics, while the mapping module utilizes normal\nconsistency and depth distortion to enhance surface reconstruction quality. We\nalso introduce a pose-consistent strategy for efficient and geometrically\ncoherent keyframe sampling. Extensive experiments on public endoscopic datasets\ndemonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth\nreconstruction of surgical scenes while maintaining computationally efficient\ntracking, high-quality visual appearance, and real-time rendering. Our code\nwill be released at github.com\/lastbasket\/Endo-2DTAM.",
        "Two-dimensional Acoustic Charge Transport (2D-ACT) devices, which integrate\ntwo dimensional semiconductor field-effect transistor (FET) with high-frequency\nsurface acoustic wave (SAW) device provide a potential compact platform for the\nprocessing of analog signals in a wireless, non-contact, low-loss and real-time\nway. It is expected to be used in long-distance space communication and\nsensing. However, current investigations into 2D-ACT devices are still limited\nto the observation of DC acoustoelectric currents, and have yet to achieve\nreal-time electronic signal processing capabilities. In this paper, we have\ndesigned a hybrid acoustoelectric platform composed of two-dimensional\nsemiconductor FET and SAW device. The platform is capable of processing DC\nsignals, exhibiting ambipolar transport behavior. The sub-wavelength channel\nlength of the FET within the platform allows for the real-time observation of\ncarrier distribution at a microscopic scale in conjunction with the SAW\npotential, and facilitating the reproduction and intensity regulation of AC\nsignals. By adjusting the relative phase and intensity ratio of two\ncounter-propagating SAWs, the platform also enables the addition and\nsubtraction of AC signals.",
        "The study involves a comprehensive performance analysis of popular\nclassification and segmentation models, applied over a Bangladeshi pothole\ndataset, being developed by the authors of this research. This custom dataset\nof 824 samples, collected from the streets of Dhaka and Bogura performs\ncompetitively against the existing industrial and custom datasets utilized in\nthe present literature. The dataset was further augmented four-fold for\nsegmentation and ten-fold for classification evaluation. We tested nine\nclassification models (CCT, CNN, INN, Swin Transformer, ConvMixer, VGG16,\nResNet50, DenseNet201, and Xception) and four segmentation models (U-Net,\nResU-Net, U-Net++, and Attention-Unet) over both the datasets. Among the\nclassification models, lightweight models namely CCT, CNN, INN, Swin\nTransformer, and ConvMixer were emphasized due to their low computational\nrequirements and faster prediction times. The lightweight models performed\nrespectfully, oftentimes equating to the performance of heavyweight models. In\naddition, augmentation was found to enhance the performance of all the tested\nmodels. The experimental results exhibit that, our dataset performs on par or\noutperforms the similar classification models utilized in the existing\nliterature, reaching accuracy and f1-scores over 99%. The dataset also\nperformed on par with the existing datasets for segmentation, achieving model\nDice Similarity Coefficient up to 67.54% and IoU scores up to 59.39%.",
        "Virtual Try-On (VTON) technology allows users to visualize how clothes would\nlook on them without physically trying them on, gaining traction with the rise\nof digitalization and online shopping. Traditional VTON methods, often using\nGenerative Adversarial Networks (GANs) and Diffusion models, face challenges in\nachieving high realism and handling dynamic poses. This paper introduces\nOutfitting Diffusion with Pose Guided Condition (ODPG), a novel approach that\nleverages a latent diffusion model with multiple conditioning inputs during the\ndenoising process. By transforming garment, pose, and appearance images into\nlatent features and integrating these features in a UNet-based denoising model,\nODPG achieves non-explicit synthesis of garments on dynamically posed human\nimages. Our experiments on the FashionTryOn and a subset of the DeepFashion\ndataset demonstrate that ODPG generates realistic VTON images with fine-grained\ntexture details across various poses, utilizing an end-to-end architecture\nwithout the need for explicit garment warping processes. Future work will focus\non generating VTON outputs in video format and on applying our attention\nmechanism, as detailed in the Method section, to other domains with limited\ndata."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"FuXi-S2S: An accurate machine learning model for global subseasonal forecasts",
    "start_abstract":"Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "Analysis methods for numerical weather prediction"
      ],
      "abstract":[
        "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "Unveiling stellar spin: Determining inclination angles in Be stars",
        "Dynamic Metadata Schemes in the Neutron and Photon Science Communities:\n  A Case Study of X-Ray Photon Correlation Spectroscopy",
        "Signatures of extreme events in the cumulative entropic spectrum",
        "High-Performance Data Format for Scientific Data Storage and Analysis",
        "Maximum likelihood estimation of burst-merging kernels for bursty time\n  series",
        "Ordinal language of antipersistent binary walks",
        "A skeletonization based image segmentation algorithm to isolate slender\n  regions in 3D microstructures",
        "ALMAGAL I. The ALMA evolutionary study of high-mass protocluster\n  formation in the Galaxy. Presentation of the survey and early results",
        "A perturbation theory for multi-time correlation functions in open\n  quantum systems",
        "Disentangling sources of multifractality in time series",
        "Influence of Membrane Characteristics on Efficiency of Vacuum Membrane\n  Distillation: a Lattice Boltzmann Study",
        "Sparse identification of evolution equations via Bayesian model\n  selection",
        "Capacitively Shunted Double-Transmon Coupler Realizing Bias-Free Idling\n  and High-Fidelity CZ Gate",
        "Efficient First-Principles Framework for Overdamped Phonon Dynamics and\n  Anharmonic Electron-Phonon Coupling in Superionic Materials",
        "Euclid Quick Data Release (Q1) -- Data release overview",
        "On the conjecture of non-inner automorphisms of finite $p$-groups with a\n  non-trivial abelian direct factor",
        "D-HAT: a Diatom-inspired structure for a Helmet concept Against Trauma",
        "In Vivo Study of Bone Growth Around Additively Manufactured Implants\n  with Ti-6Al-4V and Bioactive Glass Powder Composites",
        "On the Isomorphism Problem of Cayley Graphs of Graph Products",
        "Revisiting gluon density from the BK equation with kinematical\n  constraint and large x terms",
        "CIBER 4th flight fluctuation analysis: Pseudo-power spectrum formalism,\n  improved source masking and validation on mocks",
        "Study of SARS-CoV-2 Spike Protein by Surface Enhanced Raman Spectroscopy\n  and Transmission Electron Microscopy",
        "State transfer of Grover walks on unitary and quadratic unitary Cayley\n  graphs over finite commutative rings",
        "An Automated Bandwidth Division for the LHCb Upgrade Trigger",
        "Vision-Aided Channel Prediction Based on Image Segmentation at Street\n  Intersection Scenarios",
        "Partially hyperbolic symplectomorphism with C^1 bundles",
        "Principles for Open Data Curation: A Case Study with the New York City\n  311 Service Request Data",
        "Can Yang-Baxter imply Lie algebra?"
      ],
      "abstract":[
        "The physical properties of stellar atmospheres in rapidly rotating massive\nstars, such as Be stars, are critical to understanding their evolution and\ntheir role as progenitors of supernovae. These stars, which often have\nnear-critical rotation, exhibit equatorial stretching and gravity darkening,\nwhich significantly complicates the determination of parameters such as the\ninclination angle. Be stars, characterized by their extreme rotational\nvelocities, serve as excellent candidates for exploring these phenomena.\nHowever, fundamental quantities such as polar and equatorial radii and\ninclination angles are typically derived from interferometry, which applies\nonly to a limited number of stars. This study aims to enhance the determination\nof inclination angles for Be stars using the ZPEKTR spectral synthesis code. By\nincorporating advanced models of gravity darkening and stellar deformation, we\nevaluated the effectiveness of this method with a sample of ten Be stars from\nthe BeSOS database, comparing results with established interferometric data.\nMethods. We used the ZPEKTR code to model the effects of stellar oblateness and\ngravity darkening on spectral lines, focusing on the HeI 4471 line. We applied\na chi-squared test minimization approach to identify the best-fitting models,\nand we evaluated the inclination angles derived against interferometric\nmeasurements. Our analysis reveals a robust linear correlation between the\ninclination angles derived from ZPEKTR and using interferometric techniques,\nwhich demonstrates an excellent agreement. The ZPEKTR code effectively models\nhigh rotational velocity effects, providing precise stellar parameter\ndeterminations. The results underscore the potential of advanced spectroscopic\ntechniques to yield inclination measurements comparable to interferometry,\nwhich offers a pathway to studying distant massive stars.",
        "Metadata is one of the most important aspects for advancing data management\npractices within all research communities. Definitions and schemes of metadata\nare inter alia of particular significance in the domain of neutron and photon\nscattering experiments covering a broad area of different scientific\ndisciplines. The demand of describing continuously evolving highly\nnonstandardized experiments, including the resulting processed and published\ndata, constitutes a considerable challenge for a static definition of metadata.\nHere, we present the concept of dynamic metadata for the neutron and photon\nscientific community, which enriches a static set of defined basic metadata. We\nexplore the idea of dynamic metadata with the help of the use case of X-ray\nPhoton Correlation Spectroscopy (XPCS), which is a synchrotron-based scattering\ntechnique that allows the investigation of nanoscale dynamic processes. It\nserves here as a demonstrator of how dynamic metadata can improve data\nacquisition, sharing, and analysis workflows. Our approach enables researchers\nto tailor metadata definitions dynamically and adapt them to the evolving\ndemands of describing data and results from a diverse set of experiments. We\ndemonstrate that dynamic metadata standards yield advantages that enhance data\nreproducibility, interoperability, and the dissemination of knowledge.",
        "In this study, the cumulative effect of the empirical probability\ndistribution of a random variable is identified as a factor that amplifies the\noccurrence of extreme events in datasets. To quantify this observation, a\ncorresponding information measure is introduced, drawing upon Shannon entropy\nfor joint probabilities. The proposed approach is validated using selected\nmarket data as case studies, encompassing various instances of extreme events.\nIn particular, the results indicate that the introduced cumulative measure\nexhibits distinctive signatures of such events, even when the data is\nrelatively noisy. These findings highlight the potential of the discussed\nconcept for developing a new class of related indicators or classifiers.",
        "In this article, we present the High-Performance Output (HiPO) data format\ndeveloped at Jefferson Laboratory for storing and analyzing data from Nuclear\nPhysics experiments. The format was designed to efficiently store large amounts\nof experimental data, utilizing modern fast compression algorithms. The purpose\nof this development was to provide organized data in the output, facilitating\naccess to relevant information within the large data files. The HiPO data\nformat has features that are suited for storing raw detector data,\nreconstruction data, and the final physics analysis data efficiently,\neliminating the need to do data conversions through the lifecycle of\nexperimental data. The HiPO data format is implemented in C++ and JAVA, and\nprovides bindings to FORTRAN, Python, and Julia, providing users with the\nchoice of data analysis frameworks to use. In this paper, we will present the\ngeneral design and functionalities of the HiPO library and compare the\nperformance of the library with more established data formats used in data\nanalysis in High Energy and Nuclear Physics (such as ROOT and Parquete).",
        "Various time series in natural and social processes have been found to be\nbursty. Events in the time series rapidly occur within short time periods,\nforming bursts, which are alternated with long inactive periods. As the\ntimescale defining bursts increases, individual events are sequentially merged\nto become small bursts and then bigger ones, eventually leading to the single\nburst containing all events. Such a merging pattern has been depicted by a tree\nthat fully reveals the hierarchical structure of bursts, thus called a burst\ntree. The burst-tree structure can be simply characterized by a burst-merging\nkernel that dictates which bursts are merged together as the timescale\nincreases. In this work, we develop the maximum likelihood estimation method of\nthe burst-merging kernel from time series, which is successfully tested against\nthe time series generated using several model kernels. We also apply our method\nto some empirical time series from various backgrounds. Our method provides a\nuseful tool to precisely characterize the time series data, hence enabling to\nstudy their underlying mechanisms more accurately.",
        "This paper explores the effectiveness of using ordinal pattern probabilities\nto evaluate antipersistency in the sign decomposition of long-range\nanti-correlated Gaussian fluctuations. It is numerically shown that ordinal\npatterns are able to effectively measure both persistent and antipersistent\ndynamics by analyzing the sign decomposition derived from fractional Gaussian\nnoise. These findings are crucial given that traditional methods such as\nDetrended Fluctuation Analysis are unsuccessful in detecting anti-correlations\nin such sequences. The numerical results are supported by physiological and\nenvironmental data, illustrating its applicability in real-world situations.",
        "The work proposes an image segmentation algorithm that isolates slender\nregions in three-dimensional microstructures. Characterizing slender regions in\nmaterial microstructures is an extremely important aspect in material science\nbecause these regions govern the macroscopic behavior of materials for many\napplications like energy absorption, activation of metamaterials, stability of\nhigh temperature filters, etc. This work utilizes skeletonization method to\ncalculate centerline of the microstructure geometry followed by a novel pruning\nstrategy based on cross-sectional area to identify slender regions in the\nmicrostructure. 3D images of such microstructures obtained from micro-CT often\nsuffer from low image resolution resulting in high surface noise. The skeleton\nof such an image has many spurious skeletal branches that do not represent the\nactual microstructure geometry. The proposed pruning method of cross-sectional\narea is insensitive to surface noise and hence is a reliable method of\nidentifying skeletal branches that represent the slender regions in the\nmicrostructure. The proposed algorithm is implemented on a test case to\nshowcase its effectiveness. Further it is implemented on a 3D microstructure of\nceramic foam to identify the slender regions present in it. It is shown that\nthe method can be used to segment slender regions of varying dimensions and to\nstudy their geometric properties.",
        "Fundamental questions about the physics responsible for fragmenting molecular\nparsec-scale clumps into cores of ~1000 au are still open, that only a\nstatistically significant investigation with ALMA is able to address: what are\nthe dominant agents that determine the core demographics, mass, and spatial\ndistribution as a function of the physical properties of the hosting clumps,\ntheir evolutionary stage and the different Galactic environments in which they\nreside? To what extent extent is fragmentation driven by clumps dynamics or\nmass transport in filaments? With ALMAGAL we observed the 1.38 mm continuum and\nlines toward more than 1000 dense clumps in our Galaxy, with M>500M_sun,\nsurface density > 0.1 g\/cm2 and d<7.5 kpc. The ACA and two 12-m array setups\nwere used to deliver a minimum resolution of ~1000 au over the entire sample\ndistance range. The sample covers all evolutionary stages from infrared dark\nclouds (IRDCs) to HII regions from the tip of the Galactic bar to the outskirts\nof the Galaxy. The spectral setup includes several molecular lines to trace the\nmultiscale physics and dynamics of gas, notably CH3CN, H2CO, SiO, CH3OH, DCN,\nHC3N, SO etc. We present an initial overview of the observations and the early\nscience product and results, with a first characterization of the morphological\nproperties of the continuum emission. We use \"perimeter-versus-area\" and convex\nhull-versus-area metrics to classify the different morphologies. More extended\nand morphologically complex shapes are found toward clumps that are relatively\nmore evolved and have higher surface densities.",
        "Dynamical maps are the principal subject of the open system theory. Formally,\nthe dynamical map of a given open quantum system is a density matrix\ntransformation that takes any initial state and sends it to the state at a\nlater time. Physically, it encapsulates the system's evolution due to coupling\nwith its environment.\n  Hence, the theory provides a flexible and accurate framework for computing\nexpectation values of open system observables. However, expectation values --\nor more generally, single-time correlation functions -- capture only the\nsimplest aspects of a quantum system's dynamics. A complete characterization\nrequires access to multi-time correlation functions as well. For closed\nsystems, such correlations are well-defined, even though knowledge of the\nsystem's state alone is insufficient to determine them fully. In contrast, the\nstandard dynamical map formalism for open systems does not account for\nmulti-time correlations, as it is fundamentally limited to describing state\nevolution. Here, we extend the scope of open quantum system theory by\ndeveloping a systematic perturbation theory for computing multi-time\ncorrelation functions.",
        "This contribution addresses the question commonly asked in scientific\nliterature about the sources of multifractality in time series. Two primary\nsources are typically considered. These are temporal correlations and heavy\ntails in the distribution of fluctuations. Most often, they are treated as two\nindependent components, while true multifractality cannot occur without\ntemporal correlations. The distributions of fluctuations affect the span of the\nmultifractal spectrum only when correlations are present. These issues are\nillustrated here using series generated by several model mathematical cascades,\nwhich by design build correlations into these series. The thickness of the\ntails of fluctuations in such series is then governed by an appropriate\nprocedure of adjusting them to $q$-Gaussian distributions, and $q$ is treated\nas a variable parameter that, while preserving correlations, allows to tune\nthese distributions to the desired functional form. Multifractal detrended\nfluctuation analysis (MFDFA), as the most commonly used practical method for\nquantifying multifractality, is then used to identify the influence of the\nthickness of the fluctuation tails in the presence of temporal correlations on\nthe width of multifractal spectra. The obtained results point to the Gaussian\ndistribution, so $q=1$, as the appropriate reference distribution to evaluate\nthe contribution of fatter tails to the width of multifractal spectra. An\nappropriate procedure is presented to make such estimates.",
        "With increasing water scarcity, membrane distillation technology has gained\nwidespread attention as an innovative method for seawater\ndesalination.However,existing studies often overlook the influence of membrane\ncharacteristics on mass transfer efficiency. This study, based on the lattice\nBoltzmann method,proposes a model for a novel Poly(tetraethynylpyrene) membrane\nmaterial to reveal the influence of membrane characteristics on the performance\nof vacuum membrane distillation. The model considers the factors such as\nporosity, tortuosity, membrane thickness, pore size, membrane surface\nwettability and temperature difference on the permeate flux. The results show\nthat the permeate flux increases linearly with the porosity and decreases\nexponentially with the tortuosity factor. There is an optimal membrane\nthickness range (2{\\mu}m) beyond which the permeate flux decreases\nexponentially. In addition, the permeate flux increases exponentially with\nincreasing temperature difference and pore size. Further analysis of the effect\nof membrane surface wettability shows that permeate flux increases with\nincreasing hydrophobicity. Finally, the feed temperature and tortuosity factor\nhave the largest effect on permeate flux,followed by membrane thickness and,\nsubsequently, pore size. The model can be further extended to study other\nconfigurations of membrane distillation technologies.",
        "The quantitative formulation of evolution equations is the backbone for\nprediction, control, and understanding of dynamical systems across diverse\nscientific fields. Besides deriving differential equations for dynamical\nsystems based on basic scientific reasoning or prior knowledge in recent times\na growing interest emerged to infer these equations purely from data. In this\narticle, we introduce a novel method for the sparse identification of nonlinear\ndynamical systems from observational data, based on the observation how the key\nchallenges of the quality of time derivatives and sampling rates influence this\nproblem. Our approach combines system identification based on thresholded least\nsquares minimization with additional error measures that account for both the\ndeviation between the model and the time derivative of the data, and the\nintegrated performance of the model in forecasting dynamics. Specifically, we\nintegrate a least squares error as well as the Wasserstein metric for estimated\nmodels and combine them within a Bayesian optimization framework to efficiently\ndetermine optimal hyperparameters for thresholding and weighting of the\ndifferent error norms. Additionally, we employ distinct regularization\nparameters for each differential equation in the system, enhancing the method's\nprecision and flexibility. We demonstrate the capabilities of our approach\nthrough applications to dynamical fMRI data and the prototypical example of a\nwake flow behind a cylinder. In the wake flow problem, our method identifies a\nsparse, accurate model that correctly captures transient dynamics, oscillation\nperiods, and phase information, outperforming existing methods. In the fMRI\nexample, we show how our approach extracts insights from a trained recurrent\nneural network, offering a novel avenue for explainable AI by inferring\ndifferential equations that capture potentially causal relationships.",
        "A high-fidelity CZ gate utilizing a double-transmon coupler (DTC) has\nrecently been demonstrated as a building block for superconducting quantum\nprocessors. Like many other kinds of tunable couplers, however, the DTC\nrequires a finite DC current for flux-biasing the coupler at the idling point\nto turn off the coupling, necessitating extra care for wiring and heat-load\nmanagement. To address this issue, we theoretically propose and experimentally\nrealize a novel coupling scheme by introducing a shunt capacitance between the\ntwo transmons of the DTC at zero-flux bias, which demonstrates high-fidelity\nCZ-gate performance comparable to the previous DTC. Through a comprehensive\nerror budget analysis using multiple randomized benchmarking methods, we also\nidentify that the current fidelity is limited by the decoherence through the\ncoupler. Moreover, we experimentally demonstrate the wide operational flux\nrange of the capacitively shunted DTC, which solves the challenging issue of\nremnant flux existing even with careful magnetic shielding.",
        "Relying on the anharmonic special displacement method, we introduce an ab\ninitio quasistatic polymorphous framework to describe local disorder,\nanharmonicity, and electron-phonon coupling in superionic conductors. Using the\nexample of cubic Cu2Se, we show that positional polymorphism yields extremely\noverdamped anharmonic vibrations while preserving transverse acoustic phonons,\nconsistent with experiments. We also demonstrate well-defined electronic band\nstructures with large band gap openings due to polymorphism of 1.0 eV and\ncalculate anharmonic electron-phonon renormalization, yielding band gap\nnarrowing with increasing temperature in agreement with previous measurements.\nOur approach opens the way for efficient ab initio electronic structure\ncalculations in superionic crystals to elucidate their compelling high\nfigure-of-merit.",
        "The first Euclid Quick Data Release, Q1, comprises 63.1 sq deg of the Euclid\nDeep Fields (EDFs) to nominal wide-survey depth. It encompasses visible and\nnear-infrared space-based imaging and spectroscopic data, ground-based\nphotometry in the u, g, r, i and z bands, as well as corresponding masks.\nOverall, Q1 contains about 30 million objects in three areas near the ecliptic\npoles around the EDF-North and EDF-South, as well as the EDF-Fornax field in\nthe constellation of the same name. The purpose of this data release -- and its\nassociated technical papers -- is twofold. First, it is meant to inform the\ncommunity of the enormous potential of the Euclid survey data, to describe what\nis contained in these data, and to help prepare expectations for the\nforthcoming first major data release DR1. Second, it enables a wide range of\ninitial scientific projects with wide-survey Euclid data, ranging from the\nearly Universe to the Solar System. The Q1 data were processed with early\nversions of the processing pipelines, which already demonstrate good\nperformance, with numerous improvements in implementation compared to\npre-launch development. In this paper, we describe the sky areas released in\nQ1, the observations, a top-level view of the data processing of Euclid and\nassociated external data, the Q1 photometric masks, and how to access the data.\nWe also give an overview of initial scientific results obtained using the Q1\ndata set by Euclid Consortium scientists, and conclude with important caveats\nwhen using the data. As a complementary product, Q1 also contains observations\nof a star-forming area in Lynd's Dark Nebula 1641 in the Orion~A Cloud,\nobserved for technical purposes during Euclid's performance-verification phase.\nThis is a unique target, of a type not commonly found in Euclid's nominal sky\nsurvey.",
        "Let $p$ be a prime number. A longstanding conjecture asserts that every\nfinite non-abelian $p$-group has a non-inner automorphism of order $p$. In this\npaper, we prove that the conjecture is true when a finite non-abelian $p$-group\n$G$ has a non-trivial abelian direct factor. Moreover, we prove that the\nnon-inner automorphism is central and fixes $\\Phi(G)$ elementwise. As a\nconsequence, we prove that every group which is not purely non-abelian has a\nnon-inner central automorphism of order $p$ which fixes $\\Phi(G)$ elementwise.",
        "The primary objective of helmet design continues to be the prevention of\ntraumatic brain injuries. Yet, achieving an optimal user experience, including\naspects such as fit, thermal comfort, breathability, waterproofing, and\nreusability, is increasingly significant. Thus, designing helmets with\nmultifunctional performance represents the latest technological frontier for\nsafety devices. This study draws inspiration from the morphology of\nCoscinodiscus species diatoms to develop a biomimetic material replicating\ntheir cellular structure and multifunctionality. Unlike its biological\ncounterpart, the synthetic material is engineered as the inner liner for multi\nimpact helmets, suited for urban sports and micro mobility applications. The\narchitecture of the material is modeled using computer aided design tools, and\nits energy absorption capabilities are analyzed through finite element modeling\nand quasi static compression tests on 3D printed elastomeric samples.\nPerformance optimization is achieved through a parametric approach. The results\ndemonstrate that the material exhibits energy absorption comparable to cellular\nmaterials like honeycombs, while offering lightweight properties,\nbreathability, and resistance to atmospheric agents. This biomimetic design\nmarks a significant advancement in high performance safety equipment.",
        "Osseointegration is crucial to the success of biomedical implants. Additive\nmanufacturing of implants offers a high degree of design freedom, enabling\nprecise control over implant geometry and material composition. Bioactive glass\n(BG) can substantially enhance bone binding and bioactivity; however, limited\nresearch has been conducted on its incorporation into additively manufactured\nimplants. The performance of BG varies depending on the incorporation method,\nand the spatial and temporal evolution of its integration remains unclear. In\nthis study, we synthesized Ti-6Al-4V\/58S BG composites by using the selective\nlaser melting method and systematically compared the effects of BG coating and\ndoping in additively manufactured implants. In vivo histological results from\nanimal tests were statistically analyzed and discussed in terms of\nosseointegration over 4- and 12-week periods. Bone-to-implant contact (BIC) and\nbone density (BD) were used as quantitative metrics to evaluate interactions\nbetween the implants and surrounding bone. Our findings indicate that both\nBG-doped and BG-coated implants accelerated bone ingrowth during the early\nstages of healing. BG-coated implants demonstrated a greater improvement than\ndid pure 3D-printed Ti-6Al-4V implants. However, the effects of BG became\nnonsignificant during the later healing stage (12 weeks). This study provides a\nfoundation for systematically investigating BG incorporation methods in\n3D-printed biomedical implants and their effect on osseointegration.",
        "We investigate Cayley graphs of graph products by showing that graph products\nwith vertex groups that have isomorphic Cayley graphs yield isomorphic Cayley\ngraphs.",
        "We perform analysis of the small x non-linear evolution equation formulated\nin momentum space supplemented by higher order terms. The equation is defined\nin wide range of transverse momentum and longitudinal momentum fraction\nextending previous studies performed in \\cite{Kutak:2003bd,Kutak:2004ym}. The\nlinear part of the equation is motivated by the renormalization group improved\nsmall x approach which accounts for resummation of higher orders, and includes\ncollinear splitting function and kinematical constraint. The solution to the\nequation is then used to perform the fit to Deep Inelastic Scattering reduced\ncross section data.",
        "Precise, unbiased measurements of extragalactic background anisotropies\nrequire careful treatment of systematic effects in fluctuation-based,\nbroad-band intensity mapping measurements. In this paper we detail improvements\nin methodology for the Cosmic Infrared Background ExpeRiment (CIBER),\nconcentrating on flat field errors and source masking errors. In order to\nbypass the use of field differences, which mitigate flat field errors but\nreduce sensitivity, we characterize and correct for the flat field on\npseudo-power spectra, which includes both additive and multiplicative biases.\nTo more effectively mask point sources at 1.1 $\\mu$m and 1.8 $\\mu$m, we develop\na technique for predicting masking catalogs that utilizes optical and NIR\nphotometry through random forest regression. This allows us to mask over two\nVega magnitudes deeper than the completeness limits of 2MASS alone, with errors\nin the shot noise power remaining below $<10\\%$ at all masking depths\nconsidered. Through detailed simulations of CIBER observations, we validate our\nformalism and demonstrate unbiased recovery of the sky fluctuations on\nrealistic mocks. We demonstrate that residual flat field errors comprise\n$<20\\%$ of the final CIBER power spectrum uncertainty with this methodology.",
        "The spike protein (SP) of SARS-CoV-2 is the major molecular target for making\ndiagnostic tests, vaccines, and therapeutic development. We used a combination\nof transmission electron microscopy (TEM) and surface enhanced Raman microscopy\n(SERS) to study its structure. Using SERS on an aluminum substrate, we were\nable to detect a characteristic spectrum of SP mostly due to vibration of three\naromatic amino acids producing Raman shifts at 466 cm-1, 524 cm-1, 773 cm-1,\n831 cm-1, 1048 cm-1, 1308 cm-1, 1457 cm-1, and 1610 cm-1. Transmission Electron\nMicroscopy (TEM) of the SP showed periodic 2D-lattice orientation. The findings\nfrom this study have translational values for developing surface-enhanced Raman\nspectroscopy (SERS) based detectors for screening and testing SARS-CoV-2\nsignatures in diagnostic settings and contamination tracking.",
        "This paper focuses on periodicity and perfect state transfer of Grover walks\non two well-known families of Cayley graphs, namely, the unitary Cayley graphs\nand the quadratic unitary Cayley graphs. Let $R$ be a finite commutative ring.\nThe unitary Cayley graph $G_R$ has vertex set $R$, where two vertices $u$ and\n$v$ are adjacent if $u-v$ is a unit in $R$. We provide a necessary and\nsufficient condition for the periodicity of the Cayley graph $G_R$. We also\ncompletely determine the rings $R$ for which $G_R$ exhibits perfect state\ntransfer. The quadratic unitary Cayley graph $\\mathcal{G}_R$ has vertex set\n$R$, where two vertices $u$ and $v$ are adjacent if $u-v$ or $v-u$ is a square\nof some units in $R$. It is well known that any finite commutative ring $R$ can\nbe expressed as $R_1\\times\\cdots\\times R_s$, where each $R_i$ is a local ring\nwith maximal ideal $M_i$ for $i\\in\\{1,...,s\\}$. We characterize periodicity and\nperfect state transfer on $\\mathcal{G}_R$ under the condition that\n$|R_i|\/|M_i|\\equiv 1 \\pmod 4$ for $i\\in\\{1,...,s\\}$. Also, we characterize\nperiodicity and perfect state transfer on $\\mathcal{G}_R$, where $R$ can be\nexpressed as $R_0\\times\\cdots\\times R_s$ such that $|R_0|\/|M_0|\\equiv3\\pmod 4$,\nand $|R_i|\/|M_i|\\equiv1\\pmod4$ for $i\\in\\{1,..., s\\}$, where $R_i$ is a local\nring with maximal ideal $M_i$ for $i\\in\\{0,...,s\\}$.",
        "The upgraded Large Hadron Collider beauty (LHCb) experiment is the first\ndetector based at a hadron collider using a fully software based trigger. The\nfirst `High Level Trigger' stage (HLT1) reduces the event rate from 30 MHz to\napproximately 1 MHz based on reconstruction criteria from the tracking system\nand consists of O(100) trigger selections implemented on GPUs. These selections\nare further refined following the full offline-quality reconstruction at the\nsecond stage (HLT2) prior to saving for analysis. An automated bandwidth\ndivision has been performed to equitably divide this 1 MHz output rate between\nthe signals of interest to the LHCb physics program. This was achieved by\noptimising a set of trigger selections that maximise efficiency for signals of\ninterest to LHCb while keeping the total HLT1 readout capped to a maximum. The\nbandwidth division tool has been used to determine the optimal selection for 35\nselection algorithms over 80 characteristic physics channels.",
        "Intelligent vehicular communication with vehicle road collaboration\ncapability is a key technology enabled by 6G, and the integration of various\nvisual sensors on vehicles and infrastructures plays a crucial role. Moreover,\naccurate channel prediction is foundational to realizing intelligent vehicular\ncommunication. Traditional methods are still limited by the inability to\nbalance accuracy and operability based on substantial spectrum resource\nconsumption and highly refined description of environment. Therefore,\nleveraging out-of-band information introduced by visual sensors provides a new\nsolution and is increasingly applied across various communication tasks. In\nthis paper, we propose a computer vision (CV)-based prediction model for\nvehicular communications, realizing accurate channel characterization\nprediction including path loss, Rice K-factor and delay spread based on image\nsegmentation. First, we conduct extensive vehicle-to-infrastructure measurement\ncampaigns, collecting channel and visual data from various street intersection\nscenarios. The image-channel dataset is generated after a series of data\npost-processing steps. Image data consists of individual segmentation of target\nuser using YOLOv8 network. Subsequently, established dataset is used to train\nand test prediction network ResNet-32, where segmented images serve as input of\nnetwork, and various channel characteristics are treated as labels or target\noutputs of network. Finally, self-validation and cross-validation experiments\nare performed. The results indicate that models trained with segmented images\nachieve high prediction accuracy and remarkable generalization performance\nacross different streets and target users. The model proposed in this paper\noffers novel solutions for achieving intelligent channel\n  prediction in vehicular communications.",
        "We prove dynamical coherence for partial hyperbolic symplectomorphism in\ndimension 4 whose stable and unstable bundles are C^1.",
        "In the early 21st century, the open data movement began to transform\nsocieties and governments by promoting transparency, innovation, and public\nengagement. The City of New York (NYC) has been at the forefront of this\nmovement since the enactment of the Open Data Law in 2012, creating the NYC\nOpen Data portal. The portal currently hosts 2,700 datasets, serving as a\ncrucial resource for research across various domains, including health, urban\ndevelopment, and transportation. However, the effective use of open data relies\nheavily on data quality and usability, challenges that remain insufficiently\naddressed in the literature. This paper examines these challenges via a case\nstudy of the NYC 311 Service Request dataset, identifying key issues in data\nvalidity, consistency, and curation efficiency. We propose a set of data\ncuration principles, tailored for government-released open data, to address\nthese challenges. Our findings highlight the importance of harmonized field\ndefinitions, streamlined storage, and automated quality checks, offering\npractical guidelines for improving the reliability and utility of open\ndatasets.",
        "Quantum knot invariants (like colored HOMFLY-PT or Kauffman polynomials) are\na distinguished class of non-perturbative topological invariants. Any known way\nto construct them (via Chern-Simons theory or quantum R-matrix) starts with a\nfinite simple Lie algebra. Another set of knot invariants - of finite type - is\nrelated to quantum invariants via a perturbative expansion. However can all\nfinite type invariants be obtained in this way? Investigating this problem, P.\nVogel discovered a way to polynomially parameterize the expansion coefficients\nwith three parameters so that, at different specific values, this reproduces\nthe answers for all simple Lie (super)algebras. Then it is easy to construct a\npolynomial $P_{alg}$ that vanishes for all simple Lie algebras, and the\ncorresponding Vassiliev invariant would thus be absent from the perturbative\nexpansion.\n  We review these Vogel claims pointing out at least two interesting\nimplications of his construction. First, we discuss whether\ninfinite-dimensional Lie algebras might enlarge Chern-Simons theory. Second,\nVogel's construction implies an alternative axiomatization of simple Lie\nalgebras - when we start from knot invariants and arrive at Lie algebras and\ntheir classification, which is opposite to conventional logic that we mentioned\nat the beginning."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Analysis methods for numerical weather prediction",
    "start_abstract":"Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
      ],
      "abstract":[
        "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Explainable Distributed Constraint Optimization Problems",
        "Training-Free Safe Denoisers for Safe Use of Diffusion Models",
        "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations",
        "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy",
        "Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
        "OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination",
        "Theorem Prover as a Judge for Synthetic Data Generation",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "Understanding the Impact of Artificial Intelligence in Academic Writing:\n  Metadata to the Rescue",
        "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing\n  Multi-Objective Optimization based DPO for Text-to-Image Alignment",
        "Planning of Heuristics: Strategic Planning on Large Language Models with\n  Monte Carlo Tree Search for Automating Heuristic Optimization",
        "Intelligence Sequencing and the Path-Dependence of Intelligence\n  Evolution: AGI-First vs. DCI-First as Irreversible Attractors",
        "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite\n  Solar Cell Research",
        "Heavy-tailed random vectros: theory and applications",
        "QuESat: Satellite-Assisted Quantum Internet for Global-Scale\n  Entanglement Distribution",
        "Cheap Permutation Testing",
        "Efficient stochastic simulation of piecewise-deterministic Markov\n  processes and its application to the Morris-Lecar model of neural dynamics",
        "acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI\n  Models on Edge Devices",
        "PolyhedronNet: Representation Learning for Polyhedra with\n  Surface-attributed Graph",
        "Splitting CEGM Amplitudes",
        "Using Covid-19 Response Policy to Estimate Open Water Swim Drafting\n  Effects in Triathlon",
        "Is Bellman Equation Enough for Learning Control?",
        "GPDFlow: Generative Multivariate Threshold Exceedance Modeling via\n  Normalizing Flows",
        "Non-polynomial conserved quantities for ODE systems and its application\n  to the long-time behavior of solutions to cubic NLS systems",
        "Representation in large language models",
        "LongReason: A Synthetic Long-Context Reasoning Benchmark via Context\n  Expansion",
        "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge",
        "Positive self-commutators of positive operators"
      ],
      "abstract":[
        "The Distributed Constraint Optimization Problem (DCOP) formulation is a\npowerful tool to model cooperative multi-agent problems that need to be solved\ndistributively. A core assumption of existing approaches is that DCOP solutions\ncan be easily understood, accepted, and adopted, which may not hold, as\nevidenced by the large body of literature on Explainable AI. In this paper, we\npropose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include\nits solution and a contrastive query for that solution. We formally define some\nkey properties that contrastive explanations must satisfy for them to be\nconsidered as valid solutions to X-DCOPs as well as theoretical results on the\nexistence of such valid explanations. To solve X-DCOPs, we propose a\ndistributed framework as well as several optimizations and suboptimal variants\nto find valid explanations. We also include a human user study that showed that\nusers, not surprisingly, prefer shorter explanations over longer ones. Our\nempirical evaluations showed that our approach can scale to large problems, and\nthe different variants provide different options for trading off explanation\nlengths for smaller runtimes. Thus, our model and algorithmic contributions\nextend the state of the art by reducing the barrier for users to understand\nDCOP solutions, facilitating their adoption in more real-world applications.",
        "There is growing concern over the safety of powerful diffusion models (DMs),\nas they are often misused to produce inappropriate, not-safe-for-work (NSFW)\ncontent or generate copyrighted material or data of individuals who wish to be\nforgotten. Many existing methods tackle these issues by heavily relying on\ntext-based negative prompts or extensively retraining DMs to eliminate certain\nfeatures or samples. In this paper, we take a radically different approach,\ndirectly modifying the sampling trajectory by leveraging a negation set (e.g.,\nunsafe images, copyrighted data, or datapoints needed to be excluded) to avoid\nspecific regions of data distribution, without needing to retrain or fine-tune\nDMs. We formally derive the relationship between the expected denoised samples\nthat are safe and those that are not safe, leading to our $\\textit{safe}$\ndenoiser which ensures its final samples are away from the area to be negated.\nInspired by the derivation, we develop a practical algorithm that successfully\nproduces high-quality samples while avoiding negation areas of the data\ndistribution in text-conditional, class-conditional, and unconditional image\ngeneration scenarios. These results hint at the great potential of our\ntraining-free safe denoiser for using DMs more safely.",
        "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics.",
        "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.",
        "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
        "AI agents hold the potential to transform everyday life by helping humans\nachieve their goals. To do this successfully, agents need to be able to\ncoordinate with novel partners without prior interaction, a setting known as\nzero-shot coordination (ZSC). Overcooked has become one of the most popular\nbenchmarks for evaluating coordination capabilities of AI agents and learning\nalgorithms. In this work, we investigate the origins of ZSC challenges in\nOvercooked. We introduce a state augmentation mechanism which mixes states that\nmight be encountered when paired with unknown partners into the training\ndistribution, reducing the out-of-distribution challenge associated with ZSC.\nWe show that independently trained agents under this algorithm coordinate\nsuccessfully in Overcooked. Our results suggest that ZSC failure can largely be\nattributed to poor state coverage under self-play rather than more\nsophisticated coordination challenges. The Overcooked environment is therefore\nnot suitable as a ZSC benchmark. To address these shortcomings, we introduce\nOvercookedV2, a new version of the benchmark, which includes asymmetric\ninformation and stochasticity, facilitating the creation of interesting ZSC\nscenarios. To validate OvercookedV2, we conduct experiments demonstrating that\nmere exhaustive state coverage is insufficient to coordinate well. Finally, we\nuse OvercookedV2 to build a new range of coordination challenges, including\nones that require test time protocol formation, and we demonstrate the need for\nnew coordination algorithms that can adapt online. We hope that OvercookedV2\nwill help benchmark the next generation of ZSC algorithms and advance\ncollaboration between AI agents and humans.",
        "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "This column advocates for including artificial intelligence (AI)-specific\nmetadata on those academic papers that are written with the help of AI in an\nattempt to analyze the use of such tools for disseminating research.",
        "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that\ngenerated visuals not only accurately encapsulate user intents but also conform\nto stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini\nfiasco, where misaligned outputs triggered significant public backlash,\nunderscore the critical need for robust alignment mechanisms. In contrast,\nLarge Language Models (LLMs) have achieved notable success in alignment.\nBuilding on these advancements, researchers are eager to apply similar\nalignment techniques, such as Direct Preference Optimization (DPO), to T2I\nsystems to enhance image generation fidelity and reliability.\n  We present YinYangAlign, an advanced benchmarking framework that\nsystematically quantifies the alignment fidelity of T2I systems, addressing six\nfundamental and inherently contradictory design objectives. Each pair\nrepresents fundamental tensions in image generation, such as balancing\nadherence to user prompts with creative modifications or maintaining diversity\nalongside visual coherence. YinYangAlign includes detailed axiom datasets\nfeaturing human prompts, aligned (chosen) responses, misaligned (rejected)\nAI-generated outputs, and explanations of the underlying contradictions.",
        "Heuristics have achieved great success in solving combinatorial optimization\nproblems (COPs). However, heuristics designed by humans require too much domain\nknowledge and testing time. Given the fact that Large Language Models (LLMs)\npossess strong capabilities to understand and generate content, and a knowledge\nbase that covers various domains, which offer a novel way to automatically\noptimize heuristics. Therefore, we propose Planning of Heuristics (PoH), an\noptimization method that integrates the self-reflection of LLMs with the Monte\nCarlo Tree Search (MCTS), a well-known planning algorithm. PoH iteratively\nrefines generated heuristics by evaluating their performance and providing\nimprovement suggestions. Our method enables to iteratively evaluate the\ngenerated heuristics (states) and improve them based on the improvement\nsuggestions (actions) and evaluation results (rewards), by effectively\nsimulating future states to search for paths with higher rewards. In this\npaper, we apply PoH to solve the Traveling Salesman Problem (TSP) and the Flow\nShop Scheduling Problem (FSSP). The experimental results show that PoH\noutperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD)\nby other LLMs-based methods, and achieves the significant improvements and the\nstate-of-the-art performance of our proposed method in automating heuristic\noptimization with LLMs to solve COPs.",
        "The trajectory of intelligence evolution is often framed around the emergence\nof artificial general intelligence (AGI) and its alignment with human values.\nThis paper challenges that framing by introducing the concept of intelligence\nsequencing: the idea that the order in which AGI and decentralized collective\nintelligence (DCI) emerge determines the long-term attractor basin of\nintelligence. Using insights from dynamical systems, evolutionary game theory,\nand network models, it argues that intelligence follows a path-dependent,\nirreversible trajectory. Once development enters a centralized (AGI-first) or\ndecentralized (DCI-first) regime, transitions become structurally infeasible\ndue to feedback loops and resource lock-in. Intelligence attractors are modeled\nin functional state space as the co-navigation of conceptual and adaptive\nfitness spaces. Early-phase structuring constrains later dynamics, much like\nrenormalization in physics. This has major implications for AI safety:\ntraditional alignment assumes AGI will emerge and must be controlled after the\nfact, but this paper argues that intelligence sequencing is more foundational.\nIf AGI-first architectures dominate before DCI reaches critical mass,\nhierarchical monopolization and existential risk become locked in. If DCI-first\nemerges, intelligence stabilizes around decentralized cooperative equilibrium.\nThe paper further explores whether intelligence structurally biases itself\ntoward an attractor based on its self-modeling method -- externally imposed\naxioms (favoring AGI) vs. recursive internal visualization (favoring DCI).\nFinally, it proposes methods to test this theory via simulations, historical\nlock-in case studies, and intelligence network analysis. The findings suggest\nthat intelligence sequencing is a civilizational tipping point: determining\nwhether the future is shaped by unbounded competition or unbounded cooperation.",
        "The rapid advancement of perovskite solar cells (PSCs) has led to an\nexponential growth in research publications, creating an urgent need for\nefficient knowledge management and reasoning systems in this domain. We present\na comprehensive knowledge-enhanced system for PSCs that integrates three key\ncomponents. First, we develop Perovskite-KG, a domain-specific knowledge graph\nconstructed from 1,517 research papers, containing 23,789 entities and 22,272\nrelationships. Second, we create two complementary datasets: Perovskite-Chat,\ncomprising 55,101 high-quality question-answer pairs generated through a novel\nmulti-agent framework, and Perovskite-Reasoning, containing 2,217 carefully\ncurated materials science problems. Third, we introduce two specialized large\nlanguage models: Perovskite-Chat-LLM for domain-specific knowledge assistance\nand Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental\nresults demonstrate that our system significantly outperforms existing models\nin both domain-specific knowledge retrieval and scientific reasoning tasks,\nproviding researchers with effective tools for literature review, experimental\ndesign, and complex problem-solving in PSC research.",
        "In this paper we introduce and study several multivariate, heavy-tailed\ndistribution classes, and we explore their closure properties and their\napplications. We consider the class of multivariate, positively decreasing\ndistributions, and its intersection with other multivariate distribution\nclasses.",
        "Entanglement distribution across remote distances is critical for many\nquantum applications. Currently, the de facto approach for remote entanglement\ndistribution relies on optical fiber for on-the-ground entanglement\ndistribution. However, the fiber-based approach is incapable of global-scale\nentanglement distribution due to intrinsic limitations. This paper investigates\na new hybrid ground-satellite quantum network architecture (QuESat) for\nglobal-scale entanglement distribution, integrating an on-the-ground fiber\nnetwork with a global-scale passive optical network built with low-Earth-orbit\nsatellites. The satellite network provides dynamic construction of photon\nlightpaths based on near-vacuum beam guides constructed via adjustable arrays\nof lenses, forwarding photons from one ground station to another with very high\nefficiency over long distances compared to using fiber. To assess the\nfeasibility and effectiveness of QuESat for global communication, we formulate\nlightpath provisioning and entanglement distribution problems, considering the\norbital dynamics of satellites and the time-varying entanglement demands from\nground users. A two-stage algorithm is developed to dynamically configure the\nbeam guides and distribute entanglements, respectively. The algorithm combines\nrandomized and deterministic rounding for lightpath provisioning to enable\nglobal connectivity, with optimal entanglement swapping for distributing\nentanglements to meet users' demands. By developing a ground-satellite quantum\nnetwork simulator, QuESat achieves multi-fold improvements compared to repeater\nnetworks.",
        "Permutation tests are a popular choice for distinguishing distributions and\ntesting independence, due to their exact, finite-sample control of false\npositives and their minimax optimality when paired with U-statistics. However,\nstandard permutation tests are also expensive, requiring a test statistic to be\ncomputed hundreds or thousands of times to detect a separation between\ndistributions. In this work, we offer a simple approach to accelerate testing:\ngroup your datapoints into bins and permute only those bins. For U and\nV-statistics, we prove that these cheap permutation tests have two remarkable\nproperties. First, by storing appropriate sufficient statistics, a cheap test\ncan be run in time comparable to evaluating a single test statistic. Second,\ncheap permutation power closely approximates standard permutation power. As a\nresult, cheap tests inherit the exact false positive control and minimax\noptimality of standard permutation tests while running in a fraction of the\ntime. We complement these findings with improved power guarantees for standard\npermutation testing and experiments demonstrating the benefits of cheap\npermutations over standard maximum mean discrepancy (MMD), Hilbert-Schmidt\nindependence criterion (HSIC), random Fourier feature, Wilcoxon-Mann-Whitney,\ncross-MMD, and cross-HSIC tests.",
        "Piecewise-deterministic Markov processes combine continuous in time dynamics\nwith jump events, the rates of which generally depend on the continuous\nvariables and thus are not constants. This leads to a problem in a Monte-Carlo\nsimulation of such a system, where, at each step, one must find the time\ninstant of the next event. The latter is determined by an integral equation and\nusually is rather slow in numerical implementation. We suggest a reformulation\nof the next event problem as an ordinary differential equation where the\nindependent variable is not the time but the cumulative rate. This\nreformulation is similar to the H\\'enon approach to efficiently constructing\nthe Poincar\\'e map in deterministic dynamics. The problem is then reduced to a\nstandard numerical task of solving a system of ordinary differential equations\nwith given initial conditions on a prescribed interval. We illustrate the\nmethod with a stochastic Morris-Lecar model of neuron spiking with\nstochasticity in the opening and closing of voltage-gated ion channels.",
        "1. Passive acoustic monitoring (PAM) coupled with artificial intelligence\n(AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM\nsystems require manual data offloading and impose substantial demands on\nstorage and computing infrastructure. The combination of on-device AI-based\nprocessing and network connectivity enables local data analysis and\ntransmission of only relevant information, greatly reducing storage needs.\nHowever, programming these devices for robust operation is challenging,\nrequiring expertise in embedded systems and software engineering. Despite the\nincrease in AI-based models for bioacoustics, their full potential remains\nunrealized without accessible tools to deploy them on custom hardware and\ntailor device behaviour to specific monitoring goals. 2. To address this\nchallenge, we develop acoupi, an open-source Python framework that simplifies\nthe creation and deployment of smart bioacoustic devices. acoupi integrates\naudio recording, AI-based data processing, data management, and real-time\nwireless messaging into a unified and configurable framework. By modularising\nkey elements of the bioacoustic monitoring workflow, acoupi allows users to\neasily customise, extend, or select specific components to fit their unique\nmonitoring needs. 3. We demonstrate the flexibility of acoupi by integrating\ntwo bioacoustic classifiers: BirdNET, for the classification of bird species,\nand BatDetect2, for the classification of UK bat species. We test the\nreliability of acoupi over a month-long deployment of two acoupi-powered\ndevices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such\nas the Raspberry Pi and can be customised for various applications. acoupi\nstandardised framework and simplified tools facilitate the adoption of\nAI-powered PAM systems for researchers and conservationists. acoupi is on\nGitHub at https:\/\/github.com\/acoupi\/acoupi.",
        "Ubiquitous geometric objects can be precisely and efficiently represented as\npolyhedra. The transformation of a polyhedron into a vector, known as polyhedra\nrepresentation learning, is crucial for manipulating these shapes with\nmathematical and statistical tools for tasks like classification, clustering,\nand generation. Recent years have witnessed significant strides in this domain,\nyet most efforts focus on the vertex sequence of a polyhedron, neglecting the\ncomplex surface modeling crucial in real-world polyhedral objects. This study\nproposes \\textbf{PolyhedronNet}, a general framework tailored for learning\nrepresentations of 3D polyhedral objects. We propose the concept of the\nsurface-attributed graph to seamlessly model the vertices, edges, faces, and\ntheir geometric interrelationships within a polyhedron. To effectively learn\nthe representation of the entire surface-attributed graph, we first propose to\nbreak it down into local rigid representations to effectively learn each local\nregion's relative positions against the remaining regions without geometric\ninformation loss. Subsequently, we propose PolyhedronGNN to hierarchically\naggregate the local rigid representation via intra-face and inter-face\ngeometric message passing modules, to obtain a global representation that\nminimizes information loss while maintaining rotation and translation\ninvariance. Our experimental evaluations on four distinct datasets,\nencompassing both classification and retrieval tasks, substantiate\nPolyhedronNet's efficacy in capturing comprehensive and informative\nrepresentations of 3D polyhedral objects. Code and data are available at\n{https:\/\/github.com\/dyu62\/3D_polyhedron}.",
        "The CEGM formalism offers a general framework for scattering amplitudes,\nwhich rests on Grassmannians, moduli spaces and tropical geometry. The physical\nimplications of this generalization are still to be understood. Conventional\nwisdom says that key features of scattering amplitudes, like factorization at\ntheir poles into lower-point amplitudes, are associated to their singularities.\nThe factorization behavior of CEGM amplitudes at their poles is interesting but\ncomplicated. Recent developments have revealed important properties of standard\nparticle and string scattering amplitudes from factorizations, known as splits,\nthat happen away from poles. In this paper we introduce a kinematic subspace on\nwhich the CEGM amplitude splits into very simple rational functions. These\nfunctions, called simplex amplitudes, arise from stringy integrals for the\nmultivariate beta function, and also from restricting the biadjoint scalar\namplitude in quantum field theory to certain kinematic loci. Using split\nkinematics we also discover a specific class of zeros of the CEGM amplitude.\nOur construction rests on viewing positive moduli space as a product of\nsimplices, and it suggests a novel approach for deriving scattering amplitudes\nfrom tropical determinantal varieties.",
        "This study investigates the causal effects of open-water swim drafting by\nleveraging a natural experiment induced by staggered race starts during the\nCOVID-19 pandemic. Before 2020, athletes started in groups, enabling drafting\nbenefits, while pandemic-related restrictions significantly reduced these\nopportunities. Using agglomerative hierarchical clustering of swim-out times, I\nanalyze optimal drafting positions and estimate their impact on Swim-Out\nperformance. Our empirical findings reveal that swim drafting benefits were\nstatistically insignificant in 2020 but persisted post-pandemic at slightly\nreduced levels. I find that drafting becomes advantageous only from the third\ntrailing position onward, with earlier positions primarily serving to minimize\nfatigue. To mitigate endogeneity, I employ athlete and event fixed effects. The\nseemingly inverse decaying nature of drafting benefits partially addresses some\nconcerns of simultaneous reverse causality and omitted variable bias. This\nstudy provides the first largescale causal estimate of drafting effects in\nreal-world triathlon race settings.",
        "The Bellman equation and its continuous-time counterpart, the\nHamilton-Jacobi-Bellman (HJB) equation, serve as necessary conditions for\noptimality in reinforcement learning and optimal control. While the value\nfunction is known to be the unique solution to the Bellman equation in tabular\nsettings, we demonstrate that this uniqueness fails to hold in continuous state\nspaces. Specifically, for linear dynamical systems, we prove the Bellman\nequation admits at least $\\binom{2n}{n}$ solutions, where $n$ is the state\ndimension. Crucially, only one of these solutions yields both an optimal policy\nand a stable closed-loop system. We then demonstrate a common failure mode in\nvalue-based methods: convergence to unstable solutions due to the exponential\nimbalance between admissible and inadmissible solutions. Finally, we introduce\na positive-definite neural architecture that guarantees convergence to the\nstable solution by construction to address this issue.",
        "The multivariate generalized Pareto distribution (mGPD) is a common method\nfor modeling extreme threshold exceedance probabilities in environmental and\nfinancial risk management. Despite its broad applicability, mGPD faces\nchallenges due to the infinite possible parametrizations of its dependence\nfunction, with only a few parametric models available in practice. To address\nthis limitation, we introduce GPDFlow, an innovative mGPD model that leverages\nnormalizing flows to flexibly represent the dependence structure. Unlike\ntraditional parametric mGPD approaches, GPDFlow does not impose explicit\nparametric assumptions on dependence, resulting in greater flexibility and\nenhanced performance. Additionally, GPDFlow allows direct inference of marginal\nparameters, providing insights into marginal tail behavior. We derive tail\ndependence coefficients for GPDFlow, including a bivariate formulation, a\n$d$-dimensional extension, and an alternative measure for partial exceedance\ndependence. A general relationship between the bivariate tail dependence\ncoefficient and the generative samples from normalizing flows is discussed.\nThrough simulations and a practical application analyzing the risk among five\nmajor US banks, we demonstrate that GPDFlow significantly improves modeling\naccuracy and flexibility compared to traditional parametric methods.",
        "In this paper, we investigate the asymptotic behavior of small solutions to\nthe initial value problem for a system of cubic nonlinear Schrodinger equations\n(NLS) in one spatial dimension. We identify a new class of NLS systems for\nwhich the global boundedness and asymptotics of small solutions can be\nestablished, even in the absence of any effective conserved quantity. The key\nto this analysis lies in utilizing conserved quantities for the reduced\nordinary differential equation (ODE) systems derived from the original NLS\nsystems. In a previous study, the first author investigated conserved\nquantities expressed as quartic polynomials. In contrast, the conserved\nquantities considered in the present paper are of a different type and are not\nnecessarily polynomial.",
        "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
        "Large language models (LLMs) have demonstrated remarkable progress in\nunderstanding long-context inputs. However, benchmarks for evaluating the\nlong-context reasoning abilities of LLMs fall behind the pace. Existing\nbenchmarks often focus on a narrow range of tasks or those that do not demand\ncomplex reasoning. To address this gap and enable a more comprehensive\nevaluation of the long-context reasoning capabilities of current LLMs, we\npropose a new synthetic benchmark, LongReason, which is constructed by\nsynthesizing long-context reasoning questions from a varied set of\nshort-context reasoning questions through context expansion. LongReason\nconsists of 794 multiple-choice reasoning questions with diverse reasoning\npatterns across three task categories: reading comprehension, logical\ninference, and mathematical word problems. We evaluate 21 LLMs on LongReason,\nrevealing that most models experience significant performance drops as context\nlength increases. Our further analysis shows that even state-of-the-art LLMs\nstill have significant room for improvement in providing robust reasoning\nacross different tasks. We have open-sourced LongReason under\nhttps:\/\/huggingface.co\/datasets\/lz1bytedance\/LongReason to support the\ncomprehensive evaluation of LLMs' long-context reasoning capabilities.",
        "This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge\nevaluation regime, where the adversary controls both the candidate and\nevaluator model. The backdoored evaluator victimizes benign users by unfairly\nassigning inflated scores to adversary. A trivial single token backdoor\npoisoning 1% of the evaluator training data triples the adversary's score with\nrespect to their legitimate score. We systematically categorize levels of data\naccess corresponding to three real-world settings, (1) web poisoning, (2)\nmalicious annotator, and (3) weight poisoning. These regimes reflect a weak to\nstrong escalation of data access that highly correlates with attack severity.\nUnder the weakest assumptions - web poisoning (1), the adversary still induces\na 20% score inflation. Likewise, in the (3) weight poisoning regime, the\nstronger assumptions enable the adversary to inflate their scores from 1.5\/5 to\n4.9\/5. The backdoor threat generalizes across different evaluator\narchitectures, trigger designs, evaluation tasks, and poisoning rates. By\npoisoning 10% of the evaluator training data, we control toxicity judges\n(Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and\ndocument reranker judges in RAG to rank the poisoned document first 97% of the\ntime. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and\ntechnology, where social implications of mislead model selection and evaluation\nconstrain the available defensive tools. Amidst these challenges, model merging\nemerges as a principled tool to offset the backdoor, reducing ASR to near 0%\nwhilst maintaining SOTA performance. Model merging's low computational cost and\nconvenient integration into the current LLM Judge training pipeline position it\nas a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.",
        "We consider a positive operator $A$ on a Hilbert lattice such that its\nself-commutator $C = A^* A - A A^*$ is positive. If $A$ is also idempotent,\nthen it is an orthogonal projection, and so $C = 0$. Similarly, if $A$ is power\ncompact, then $C = 0$ as well. We prove that every positive compact central\noperator on a separable infinite-dimensional Hilbert lattice $\\mathcal H$ is a\nself-commutator of a positive operator. We also show that every positive\ncentral operator on $\\mathcal H$ is a sum of two positive self-commutators of\npositive operators."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"A review of artificial intelligence in prostate cancer detection on imaging",
    "start_abstract":"A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
      ],
      "abstract":[
        "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Modeling HIF-ILK Interaction Using Continuous Petri Nets",
        "Coarse-Graining Cascades Within Food Webs",
        "From Bedside to Desktop: A Data Protocol for Normative Intracranial EEG\n  and Abnormality Mapping",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Morphological Neuron Classification Using Machine Learning",
        "Multicellular self-organization in Escherichia coli",
        "Contribution to the study of the flora in the central-west of Tunisia\n  landscape dynamics and evaluation of plant biodiversity of mountain Bouchebka",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Ex vivo experiment on vertebral body with defect representing bone\n  metastasis",
        "Chaos in a Nonlinear Wavefunction Model: An Alternative to Born's\n  Probability Hypothesis",
        "Relative Reality",
        "A Study on the Line of Sight to Galaxies Detected at Gamma-ray Energies",
        "Planet formation and long-term stability in a very eccentric stellar\n  binary",
        "Fractional kinetic modelling of the adsorption and desorption process\n  from experimental SPR curves",
        "Noise equals endogenous control",
        "Coupling and Acceleration of Externally Injected Electron Beams in\n  Laser-Driven Plasma Wakefields",
        "$C^1$ Robust Rigidity for Bi-critical Circle Maps",
        "A quantum walk inspired model for distributed computing on arbitrary\n  graphs",
        "The hardcore brokers: Core-periphery structure and political\n  representation in Denmark's corporate elite network",
        "On $L_p$ Brunn-Minkowski type inequalities for a general class of\n  functionals",
        "A class of moving boundary problems with an exponential source term",
        "Scotogenic Froggatt-Nielsen and the Versatility of Soft Symmetry\n  Breaking",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Discretionary vs nondiscretionary in fiscal mechanism. Non-automatic\n  fiscal stabilisers vs automatic fiscal stabilisers"
      ],
      "abstract":[
        "Oxygen concentration in tumor micro-environment is a well-established signal\nthat can induce aggressive cancer behaviour. In particular, low oxygen levels\n(hypoxia) activate the Hypoxia-Inducible Factor(HIF) pathway which has an array\nof target systems. One of these systems is Integrin-Linked Kinase (ILK)\npathway, which influences key signaling pathways for cell survival,\nproliferation, and migration. Hence, this paper aimed to explore the\ninterconnection between these two pathways. Using the Petri net modeling tool\nSnoopy, an established HIF network model was transformed to be a continuous\nPetri net. Subsequently, the network was expanded to incorporate a feedback\nelement from the ILK pathway to HIF, based on gene expression data. The\nresulting model conserved the oxygen switch response of the original HIF model\nand positively amplified HIF's output. Therefore, this model provides a\nstarting point for establishing a system reflecting crucial effect on\nhypoxia-induced cancer behavior, and could potentially serve as a basis for\nfuture drug development.",
        "Quantifying population dynamics is a fundamental challenge in ecology and\nevolutionary biology, particularly for species that are cryptic, microscopic,\nor extinct. Traditional approaches rely on continuous representations of\npopulation size, but in many cases, the precise number of individuals is\nunknowable. Here, we present a coarse-grained population model that simplifies\npopulation dynamics to binary states - high or low - determined by the balance\nof bottom-up resource availability and top-down predation pressure. This\nBoolean framework provides a minimal yet analytically tractable alternative to\ntraditional Lotka-Volterra-based models, enabling direct insights into the role\nof food web structure in shaping community stability. Using this approach, we\ninvestigate how trophic interactions influence population persistence, cyclic\ndynamics, and extinction risk across model food webs. We find that top-down\neffects are a primary driver of cycling, aligning with theoretical expectations\nfrom traditional population models, and that trophic position strongly\ninfluences extinction risk, with higher-trophic species more prone to\npersistent low-population states. Additionally, we explore the role of trophic\nshort-circuits -- direct interactions between apex predators and low-trophic\nprey -- and find that they can buffer cascades and alter extinction patterns in\nways that are often overlooked in classical models. By simplifying population\ndynamics to a two-state system, this framework provides a powerful tool for\ndisentangling the structural drivers of community stability. These results\nhighlight the potential of coarse-grained approaches to complement existing\nmodels, offering new insights into trophic interactions, extinction risks, and\nthe susceptibility of species to trophic cascades.",
        "Normative mapping is a framework used to map population-level features of\nhealth-related variables. It is widely used in neuroscience research, but the\nliterature lacks established protocols in modalities that do not support\nhealthy control measurements, such as intracranial EEG (icEEG). An icEEG\nnormative map would allow researchers to learn about population-level brain\nactivity and enable comparison of individual data against these norms to\nidentify abnormalities. Currently, no standardised guide exists for\ntransforming clinical data into a normative, regional icEEG map. Papers often\ncite different software and numerous articles to summarise the lengthy method,\nmaking it laborious for other researchers to understand or apply the process.\nOur protocol seeks to remedy this gap by providing a dataflow guide and key\ndecision points that summarise existing methods. This protocol is used heavily\nin published works from our own lab (twelve peer-reviewed journal\npublications). Briefly, we take as input, icEEG recordings and neuroimaging\ndata from people with epilepsy who are undergoing evaluation for resective\nsurgery. As final outputs, we obtain a normative icEEG map, comprising signal\nproperties localised to brain regions. Optionally, we can also process new\nsubjects through the same pipeline and obtain their z-scores (or centiles) in\neach brain region, for abnormality detection and localisation. To date, a\nsingle, cohesive, dataflow pipeline for generating normative icEEG maps, along\nwith abnormality mapping, has not been created. We envisage that this dataflow\nguide will not only increase understanding and application of normative mapping\nmethods, but will also improve the consistency and quality of studies in the\nfield.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Classification and quantitative characterization of neuronal morphologies\nfrom histological neuronal reconstruction is challenging since it is still\nunclear how to delineate a neuronal cell class and which are the best features\nto define them by. The morphological neuron characterization represents a\nprimary source to address anatomical comparisons, morphometric analysis of\ncells, or brain modeling. The objectives of this paper are (i) to develop and\nintegrate a pipeline that goes from morphological feature extraction to\nclassification and (ii) to assess and compare the accuracy of machine learning\nalgorithms to classify neuron morphologies. The algorithms were trained on 430\ndigitally reconstructed neurons subjectively classified into layers and\/or\nm-types using young and\/or adult development state population of the\nsomatosensory cortex in rats. For supervised algorithms, linear discriminant\nanalysis provided better classification results in comparison with others. For\nunsupervised algorithms, the affinity propagation and the Ward algorithms\nprovided slightly better results.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "The study was conducted during 2013 in Bouchebka, located in the central west\nof Tunisia.Such territory has a typical landscape of the transfrontier region.\nThe series of the forest in Bouchebka is a part of the great mass of Aleppo\npine. It is distinguished by the importance of the forest area which covers 92\n% of the surface area (19,700 ha). The study attempts to inventory the natural\nvegetation and characterize ecological terms while highlighting the importance\nof environmental conditions. The method is based on a phytoecological analysis\nto quantify the floristic richness and diversity of the ecosystem in the forest\nof mountains in Bouchebka on the basis of floristic surveys and transects\ndistributed in a stratified, systematic sampling in different vegetation\nformations that were previously distinguished. Statistical analyzes were\nperformed using the Factorial Correspondence Analysis (FCA). The results show\nthat the forest is composed primarily of the Aleppo pine trees, these forests\nare characterized by the abundance of young feet (10-25 cm diameter class). The\necosystem includes 12 families and 17 genera, 26 species. Thus the study has\nidentified that the biological spectrum of the study area is characterized by a\nclear dominance of shrubs (41%) and chamaephytes (32 %). The distribution of\nplant species is influenced by ecological features of the region: the results\nshow that 82% of species are drought tolerant which shows the arid environment.\nThe region is also characterized by its windy side: 32% of species spread via\nanemochory. Factor analysis showed a pastoral aspect in the study area, with a\npresence of cultured human action exerted on the forest land species.\nPhytological spectrum indicates a predominance of woody species reflecting a\nterritory dominated by open grassy areas, predominantly reflecting an arid\nclimate.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Osteolytic metastases located in the vertebrae reduce strength and enhance\nthe risk of vertebral fractures. This risk can be predicted by means of\nvalidated finite element models, but their reproducibility needs to be\nassessed. For that purpose, experimental data are requested. The aim of this\nstudy was to conduct open-access experiments on vertebrae, with artificial\ndefect representing lytic metastasis and using well-defined boundary\nconditions. Twelve lumbar vertebral bodies (L1) were prepared by removing the\ncortical endplates and creating defects that represent lytic metastases, by\ndrilling the cancellous bone. Vertebral bodies were scanned using clinical High\nResolution peripherical Quantitative Computed Tomography before and after\ndefect creation for 3D reconstruction. The specimens were then tested under\ncompression loading until failure. Surface Digital Image Correlation was used\nto assess strain fields on the anterior wall of the vertebral body. These data\n(biomechanics data and the tomographic images needed to build subject-specific\nmodels) are shared with the scientific community in order to assess different\nvertebral models on the same dataset.",
        "In a prior paper, the author described an instability in a nonlinear\nwavefunction model. Proposed in connection with the Measurement Problem, the\nmodel contained an external potential creating a ``classical'' instability.\nHowever, it is interesting to ask whether such models possess an intrinsic\nrandomness -- even ``chaos\" -- independent of external potentials. In this\nwork, I investigate the criterion analytically and simulate from a small (``3\nqubit\") model, demonstrating that the Lyapunov exponent -- a standard measure\nof ``chaos\" -- is positive. I also extend the instability criterion to models\nin the continuum. These results suggest that the boundary between classical and\nwavefunction physics may also constitute the threshold of chaos, and present an\nalternative to Max Born's ad hoc probability hypothesis: random outcomes in\nexperiments result not from ``wave-particle duality\" or ``the existence of the\nquantum,\" but from sensitive dependence on initial conditions, as is common in\nthe other sciences.",
        "The ``Hard Problem\" of consciousness refers to a long-standing enigma about\nhow qualia emerge from physical processes in the brain. Building on insights\nfrom the development of non-Euclidean geometry, this paper seeks to present a\nstructured and logically coherent theory of qualia to address this problem. The\nproposed theory starts with a definition on what it means for an entity to be\nnon-physical. A postulate about awareness is posed and utilized to rigorously\nprove that qualia are non-physical and thoughts are qualia. Then the paper\nintroduces a key concept: relative reality, meaning that perceptions of reality\nare relative to the observer and time. The concept is analyzed through a\nmathematical model grounded in Hilbert space theory. The model also sheds new\nlight on cognitive science and physics. In particular, the Schr\\\"{o}dinger\nequation can be derived easily through this model. Moreover, this model shows\nthat eigenstates also exist for classical energy-conserving systems. Analyses\non the G. P. Thomson experiment and the classical harmonic oscillator are made\nto illustrate this finding. The insight gained sheds new light on the\nBohr-Einstein debate concerning the interpretation of quantum mechanics. At\nlast, the paper proposes a postulate about qualia force and demonstrates that\nit constitutes a fundamental part of absolute reality, much like the four\nfundamental forces in nature.",
        "The large-scale Universal structure comprises strands of dark matter and\ngalaxies with large under-dense volumes known as voids. We measure the fraction\nof the line of sight that intersects voids for active galactic nuclei (AGN)\ndetected by Fermi Large Area Telescope (LAT) and quasars from the Sloan Digital\nSky Survey (SDSS). This ``voidiness'' fraction is a rudimentary proxy for the\ndensity along the line of sight to the galaxies. The voidiness of SDSS-observed\nquasars (QSOs) is distinctly different from randomly distributed source\npopulations, with a median p-value of $4.6\\times10^{-5}$ and $\\ll\n1\\times10^{-7}$, when compared with 500 simulated populations with randomly\nsimulated locations but matching redshifts in the $0.1\\leq z<0.4$ and $0.4\\leq\nz < 0.7$ intervals, respectively. A similar comparison of the voidiness for\nLAT-detected AGN shows median p-values greater than 0.05 in each redshift\ninterval. When comparing the SDSS QSO population to the LAT-detected AGN, we\nmitigate potential bias from a relationship between redshift and voidiness by\ncomparing the LAT-detected AGN to a ``redshift-matched'' set of SDSS QSOs. The\nLAT-detected AGN between a redshift of 0.4 and 0.7 show higher voidiness\ncompared to the redshift-matched SDSS QSO populations, with a median p-value of\n2.3$\\times10^{-5}$, (a $4.1\\sigma$ deviation). No deviation is found when\ncomparing the same populations between redshifts of 0.1 and 0.4 (p>0.05). We do\nnot study possible causes of this voidiness difference. It might relate to\npropagation effects from lower magnetic or radiative background fields within\nvoids or to an environment more favorable for gamma-ray production for AGN near\nvoids.",
        "Planets orbiting one of the two stars in a binary are vulnerable to\ngravitational perturbations from the other star. Particularly, highly eccentric\ncompanion stars risk disrupting planetary orbits, such as in the extreme system\nTOI 4633 where close encounters between the companion and a gas giant planet in\nthe habitable zone make it one of the most fragile systems discovered so far.\nHere, we report that TOI 4633's planet likely survived these encounters\nthroughout the system's age by orbiting retrograde relative to the binary,\nstabilised by the Coriolis force. Using direct $N$-body simulations, we show it\notherwise tends to collide with the binary stars or becomes free-floating after\ngetting ejected. A retrograde planetary orbit has profound implications for TOI\n4633's formation and evolution, suggesting an extraordinary history where its\neccentric companion was likely randomly captured after planet formation in a\nsingle-star system. Alternatively, if stars and planet are born in situ from\nthe same gas clump, we show the planet must have formed at sub-snow-line\ndistances, contrary to the conventional core-accretion model. Our study\nhighlights the importance of considering the long-term stability ($\\gtrsim\\rm\nGyr$) of planets in eccentric binaries and demonstrates that the mere existence\nin such dynamically hostile environments places strong constraints on their\norbital configuration and formation.",
        "The application of surface plasmon resonance (SPR) has transformed the field\nof study of interactions between a ligand immobilized on the surface of a\nsensor chip, designated as $L_S$, and an analyte in solution, referred to as\n$A$. This technique enables the real-time measurement of interactions with high\nsensitivity. The dynamics of adsorption-desorption process, $A+L_S \\rightarrow\nAL_S$, can be expressed mathematically as a set of coupled integer-order\ndifferential equations. However, this approach has limited ability to acoount\nfor temperature distribution, diffusion and transport effects involved in the\nreaction process. The fractional kinetic model provides a methodology for\nincorporating non-local effects into the problem. In this study, the proposed\nmodel was applied to analyze data to the interaction between Immobilized Baru\nProtein (IBP) and Congo Red dye (CR) at concentrations ranging from $7.5$ to\n$97.5$ $\\mu M$, at pH $7.4$ and $16^o$ C. The variation in the kinetic\nconstants was studied, and it was demonstrated that the integer-order model is\nunable to adequately represent the experimental data. This work has shown that\nthe fractional-order model is capable of capturing the complexity of the\nadsorption-desorption process involved in the SPR data.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "The multi-stage method of laser wakefield acceleration (LWFA) presents a\npromising approach for developing stable, full-optical, high-energy electron\naccelerators. By segmenting the acceleration process into several booster\nstages, each powered by independent laser drivers, this technique effectively\nmitigates challenges such as electron dephasing, pump depletion, and laser\ndiffraction. A critical aspect of multi-stage LWFA is the nonlinear interaction\nbetween the injected electron beam and the laser-driven wakefields in the\nbooster stage. This study investigates the injection and acceleration of\nexternal electron beams within wakefields in the booster stage using\nmulti-dimensional Particle-In-Cell (PIC) simulations. We provide both\nqualitative and quantitative descriptions of the observed physical processes.\nKey parameters influencing charge coupling process and the resultant beam\nquality have been identified. Furthermore, we have examined how off-axis\ninjection relative to the driver laser influences the acceleration process and\nbeam quality. Our findings provide valuable insights for advancing and\noptimizing multi-stage plasma-based accelerators.",
        "We prove that two topologically conjugate bi-critical circle maps whose\nsignatures are the same, and whose renormalizations converge together\nexponentially fast in the $C^2$-topology, are $C^1$ conjugate.",
        "A discrete time quantum walk is known to be the single-particle sector of a\nquantum cellular automaton. For a long time, these models have interested the\ncommunity for their nice properties such as locality or translation invariance.\nThis work introduces a model of distributed computation for arbitrary graphs\ninspired by quantum cellular automata. As a by-product, we show how this model\ncan reproduce the dynamic of a quantum walk on graphs. In this context, we\ninvestigate the communication cost for two interaction schemes. Finally, we\nexplain how this particular quantum walk can be applied to solve the search\nproblem and present numerical results on different types of topologies.",
        "Who represents the corporate elite in democratic governance? Prior studies\nfind a tightly integrated \"inner circle\" network representing the corporate\nelite politically across varieties of capitalism, yet they all rely on data\nfrom a highly select sample of leaders from only the largest corporations. We\ncast a wider net. Analyzing new data on all members of corporate boards in the\nDanish economy (200k directors in 120k boards), we locate 1500 directors that\noperate as brokers between local corporate networks. We measure their network\ncoreness using k-core detection and find a highly connected core of 275\ndirectors, half of which are affiliated with smaller firms or subsidiaries.\nAnalyses show a strong positive association between director coreness and the\nlikelihood of joining one of the 650 government committees epitomizing\nDenmark's social-corporatist model of governance (net of firm and director\ncharacteristics). The political network premium is largest for directors of\nsmaller firms or subsidiaries, indicating that network coreness is a key driver\nof business political representation, especially for directors without claims\nto market power or weight in formal interest organizations.",
        "In this work, the $L_p$ version (for $p> 1$) of the dimensional\nBrunn-Minkowski inequality for the standard Gaussian measure $\\gamma_n(\\cdot)$\non $\\mathbb{R}^n$ is shown. More precisely, we prove that for any $0$-symmetric\nconvex sets with nonempty interior, any $p>1$, and every $\\lambda \\in (0,1)$,\n\\[ \\gamma_n\\bigl((1-\\lambda)\\cdot K+_p \\lambda \\cdot L\\bigr)^{p\/n} \\geqslant\n(1-\\lambda ) \\gamma_n(K)^{p\/n} + \\lambda \\gamma_n(L)^{p\/n}, \\] with equality,\nfor some $\\lambda \\in (0,1)$ and $p>1$, if and only if $K=L$. This result,\nrecently established without the equality conditions by Hosle, Kolesnikov and\nLivshyts, by using a different and functional approach, turns out to be the\n$L_p$ extension of a celebrated result for the Minkowski sum (that is, for\n$p=1$) by Eskenazis and Moschidis (2021) on a problem by Gardner and Zvavitch\n(2010).\n  Moreover, an $L_p$ Brunn-Minkowski type inequality is obtained for the\nclassical Wills functional $\\mathcal{W}(\\cdot)$ of convex bodies.\n  These results are derived as a consequence of a more general approach, which\nprovides us with other remarkable examples of functionals satisfying $L_p$\nBrunn-Minkowski type inequalities, such as different absolutely continuous\nmeasures with radially decreasing densities.",
        "This work investigates a class of moving boundary problems related to a\nnonlinear evolution equation featuring an exponential source term. We establish\na connection to Stefan-type problems, for different boundary conditions at the\nfixed face, through the application of a reciprocal transformation alongside\nthe Cole-Hopf transformation. For specific cases, we derive explicit similarity\nsolutions in parametric form. This innovative approach enhances our\nunderstanding of the underlying dynamics and offers valuable insights into the\nbehavior of these systems.",
        "Preserving the unique role of the one Higgs doublet of the standard model, it\nis proposed that quark and lepton mass patterns, often ascribed to the\nFroggatt-Nielsen mechanism using nonrenormalizable higher-dimensional terms,\nmay be enforced in a renormalizable theory of just one Higgs doublet by the\nscotogenic mechanism with soft symmetry breaking in the dark sector. A revised\nversion of the original $A_4$ model of charged leptons and neutrinos is\ndiscussed.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "The goal of the present study is to increase the intelligibility of\nmacroeconomic phenomena triggered by governmental intervention in economy by\nmeans of fiscal policies. During cyclical movements, fiscal policy can play an\nimportant role in order to help stabilise the economy. But discretionary policy\nusually implies implementation lags and is not automatically reversed when\neconomic conditions change. In contrast, automatic fiscal stabilisers (SFA)\nensure a prompter, and self-correcting fiscal response. The present study aims\nto tackle the topic of discretionary vs nondiscretionary characteristic of\nfiscal stabilisers (SF). In this context, the scope of the research undertaking\nis to launch a scientific debate over the definitions of the concepts of\nnon-automatic fiscal stabilisers (SfnA) and SFAs. We describe how we can\nquantify the discretionary and non-discretionary character of the fiscal\npolicy, by the analysis of the structure of the conventional budget balance\n(SBc), budget balance associated with the current GDP. In the final part of\nthis article, we propose a quantitative equilibrium model for establishing the\nmathematical prerequisites for an SF to become automatic. Likewise, on the\nbasis of the proposed mathematical model we have performed a qualitative\nanalysis of the influence factors."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning",
    "start_abstract":"Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "A review of artificial intelligence in prostate cancer detection on imaging"
      ],
      "abstract":[
        "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "LLM-Powered Knowledge Graphs for Enterprise Intelligence and Analytics",
        "In-Context Defense in Computer Agents: An Empirical Study",
        "Reflection of Episodes: Learning to Play Game from Expert and Self\n  Experiences",
        "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
        "Physics-based simulation ontology: an ontology to support modelling and\n  reuse of data for physics-based simulation",
        "Reasoning with Reinforced Functional Token Tuning",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging",
        "Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key\n  to Model Reasoning",
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Demonstrating specification gaming in reasoning models",
        "Deep Learning and Natural Language Processing in the Field of\n  Construction",
        "Correctness Learning: Deductive Verification Guided Learning for\n  Human-AI Collaboration",
        "A binary PSO based ensemble under-sampling model for rebalancing\n  imbalanced training data",
        "Enhanced Atom-by-Atom Assembly of Defect-Free Two-Dimensional\n  Mixed-Species Atomic Arrays",
        "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks\n  at ACM ICAIF FinRL Contest 2023-2024",
        "Soybean pod and seed counting in both outdoor fields and indoor\n  laboratories using unions of deep neural networks",
        "In the Picture: Medical Imaging Datasets, Artifacts, and their Living\n  Review",
        "Variations on hypergeometric functions",
        "Power-law banded random matrix ensemble as a model for quantum many-body\n  Hamiltonians",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Deviations from the Porter-Thomas distribution due to non-statistical\n  $\\gamma$ decay below the $^{150}$Nd neutron separation threshold",
        "Enhanced Continual Learning of Vision-Language Models with Model Fusion",
        "Language Representation Favored Zero-Shot Cross-Domain Cognitive\n  Diagnosis",
        "Subjective and Objective Quality Assessment of Non-Uniformly Distorted\n  Omnidirectional Images",
        "Divide-and-Conquer: Tree-structured Strategy with Answer Distribution\n  Estimator for Goal-Oriented Visual Dialogue",
        "Dammann Metasurface Route to Overcoming the Uniformity Defects in\n  Two-Dimensional Beam Multipliers",
        "Parsings of Stationary Processes, Stopping Times and the Fundamental\n  Pointwise Convergence Theorems of Ergodic Theory"
      ],
      "abstract":[
        "Disconnected data silos within enterprises obstruct the extraction of\nactionable insights, diminishing efficiency in areas such as product\ndevelopment, client engagement, meeting preparation, and analytics-driven\ndecision-making. This paper introduces a framework that uses large language\nmodels (LLMs) to unify various data sources into a comprehensive,\nactivity-centric knowledge graph. The framework automates tasks such as entity\nextraction, relationship inference, and semantic enrichment, enabling advanced\nquerying, reasoning, and analytics across data types like emails, calendars,\nchats, documents, and logs. Designed for enterprise flexibility, it supports\napplications such as contextual search, task prioritization, expertise\ndiscovery, personalized recommendations, and advanced analytics to identify\ntrends and actionable insights. Experimental results demonstrate its success in\nthe discovery of expertise, task management, and data-driven decision making.\nBy integrating LLMs with knowledge graphs, this solution bridges disconnected\nsystems and delivers intelligent analytics-powered enterprise tools.",
        "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior.",
        "StarCraft II is a complex and dynamic real-time strategy (RTS) game\nenvironment, which is very suitable for artificial intelligence and\nreinforcement learning research. To address the problem of Large Language\nModel(LLM) learning in complex environments through self-reflection, we propose\na Reflection of Episodes(ROE) framework based on expert experience and\nself-experience. This framework first obtains key information in the game\nthrough a keyframe selection method, then makes decisions based on expert\nexperience and self-experience. After a game is completed, it reflects on the\nprevious experience to obtain new self-experience. Finally, in the experiment,\nour method beat the robot under the Very Hard difficulty in TextStarCraft II.\nWe analyze the data of the LLM in the process of the game in detail, verified\nits effectiveness.",
        "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
        "The current work presents an ontology developed for physics-based simulation\nin engineering design, called Physics-based Simulation Ontology (PSO). The\npurpose of the ontology is to assist in modelling the physical phenomenon of\ninterest in a veridical manner, while capturing the necessary and reusable\ninformation for physics-based simulation solvers. The development involved\nextending an existing upper ontology, Basic Formal Ontology (BFO), to define\nlower-level terms of PSO. PSO has two parts: PSO-Physics, which consists of\nterms and relations used to model physical phenomena based on the perspective\nof classical mechanics involving partial differential equations, and PSO-Sim,\nwhich consists of terms used to represent the information artefacts that are\nabout the physical phenomena modelled with PSO-Physics. The former terms are\nused to model the physical phenomenon of interest independent of\nsolver-specific interpretations, which can be reused across different solvers,\nwhile the latter terms are used to instantiate solver-specific input data. A\ncase study involving two simulation solvers was conducted to demonstrate this\ncapability of PSO. Discussion around the benefits and limitations of using BFO\nfor the current work is also provided, which should be valuable for any future\nwork that extends an existing upper ontology to develop ontologies for\nengineering applications.",
        "In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel\nreinforced fine-tuning framework that empowers Large Language Models (LLMs)\nwith self-play learn-to-reason capabilities. Unlike prior prompt-driven\nreasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g.,\n<analyze>, <verify>, <refine>) directly into the model vocabulary, enabling\nchain-of-thought construction with diverse human-like reasoning behaviors.\nSpecifically, RFTT comprises two phases: (1) supervised fine-tuning performs\nprompt-driven tree search to obtain self-generated training data annotated with\nfunctional tokens, which warms up the model to learn these tokens for\nreasoning; and (2) online reinforcement learning further allows the model to\nexplore different reasoning pathways through functional token sampling without\nrelying on prompts, thereby facilitating effective self-improvement for\nfunctional reasoning. Extensive experiments demonstrate the superiority of the\nproposed RFTT on mathematical benchmarks, significantly boosting\nQwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to\n60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently\nimproves with more search rollouts at inference time. Our code is available at\nhttps:\/\/github.com\/sastpg\/RFTT.",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "Personalized messaging plays an essential role in improving communication in\nareas such as healthcare, education, and professional engagement. This paper\nintroduces a framework that uses the Knowledge Graph (KG) to dynamically\nrephrase written communications by integrating individual and context-specific\ndata. The knowledge graph represents individuals, locations, and events as\ncritical nodes, linking entities mentioned in messages to their corresponding\ngraph nodes. The extraction of relevant information, such as preferences,\nprofessional roles, and cultural norms, is then combined with the original\nmessage and processed through a large language model (LLM) to generate\npersonalized responses. The framework demonstrates notable message acceptance\nrates in various domains: 42% in healthcare, 53% in education, and 78% in\nprofessional recruitment. By integrating entity linking, event detection, and\nlanguage modeling, this approach offers a structured and scalable solution for\ncontext-aware, audience-specific communication, facilitating advanced\napplications in diverse fields.",
        "Transformer-based language models have achieved significant success; however,\ntheir internal mechanisms remain largely opaque due to the complexity of\nnon-linear interactions and high-dimensional operations. While previous studies\nhave demonstrated that these models implicitly embed reasoning trees, humans\ntypically employ various distinct logical reasoning mechanisms to complete the\nsame task. It is still unclear which multi-step reasoning mechanisms are used\nby language models to solve such tasks. In this paper, we aim to address this\nquestion by investigating the mechanistic interpretability of language models,\nparticularly in the context of multi-step reasoning tasks. Specifically, we\nemploy circuit analysis and self-influence functions to evaluate the changing\nimportance of each token throughout the reasoning process, allowing us to map\nthe reasoning paths adopted by the model. We apply this methodology to the\nGPT-2 model on a prediction task (IOI) and demonstrate that the underlying\ncircuits reveal a human-interpretable reasoning process used by the model.",
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like o1 preview and\nDeepSeek-R1 will often hack the benchmark by default, while language models\nlike GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work\nto hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.",
        "This article presents a complete process to extract hypernym relationships in\nthe field of construction using two main steps: terminology extraction and\ndetection of hypernyms from these terms. We first describe the corpus analysis\nmethod to extract terminology from a collection of technical specifications in\nthe field of construction. Using statistics and word n-grams analysis, we\nextract the domain's terminology and then perform pruning steps with linguistic\npatterns and internet queries to improve the quality of the final terminology.\nSecond, we present a machine-learning approach based on various words embedding\nmodels and combinations to deal with the detection of hypernyms from the\nextracted terminology. Extracted terminology is evaluated using a manual\nevaluation carried out by 6 experts in the domain, and the hypernym\nidentification method is evaluated with different datasets. The global approach\nprovides relevant and promising results.",
        "Despite significant progress in AI and decision-making technologies in\nsafety-critical fields, challenges remain in verifying the correctness of\ndecision output schemes and verification-result driven design. We propose\ncorrectness learning (CL) to enhance human-AI collaboration integrating\ndeductive verification methods and insights from historical high-quality\nschemes. The typical pattern hidden in historical high-quality schemes, such as\nchange of task priorities in shared resources, provides critical guidance for\nintelligent agents in learning and decision-making. By utilizing deductive\nverification methods, we proposed patten-driven correctness learning (PDCL),\nformally modeling and reasoning the adaptive behaviors-or 'correctness\npattern'-of system agents based on historical high-quality schemes, capturing\nthe logical relationships embedded within these schemes. Using this logical\ninformation as guidance, we establish a correctness judgment and feedback\nmechanism to steer the intelligent decision model toward the 'correctness\npattern' reflected in historical high-quality schemes. Extensive experiments\nacross multiple working conditions and core parameters validate the framework's\ncomponents and demonstrate its effectiveness in improving decision-making and\nresource optimization.",
        "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
        "Defect-free single atom array in optical tweezers is a promising platform for\nscalable quantum computing, quantum simulation, and quantum metrology.\nExtending single-species array to mixed-species one promise to offer new\npossibilities. In our recent proof of principle realization of defect-free\ntwo-dimensional assembly of mixed-species $^{85}$Rb ($^{87}$Rb) atom arrays [C.\nSheng et\nal.\\href{https:\/\/journals.aps.org\/prl\/abstract\/10.1103\/PhysRevLett.128.083202}{{\\color{blue}\nPhys. Rev. Lett. 128, 083202(2022)}}], the filling fractions were limited by\nthe imperfect transfer of atoms and the occurrence of logjams during the atom\nrearrangement. In order to scale up the size of defect-free mixed-species atom\narray, we scale up the tweezer array and improve the atom transfer, and upgrade\nthe heuristic heteronuclear algorithm so as to facilitate multiple\nrearrangement cycles. Consequently, we successfully create defect-free atom\narrays with 120 mixed-species single atoms. The corresponding filling fraction\nand defect-free probability are improved to be 98.6(1)\\% and 14(2)\\%,\nrespectively. It is anticipated that the enhanced algorithm can be extended to\nother combinations of atomic species, and this mixed-species atom array is\nreadily for studies of many-body physics, quantum error correction, and quantum\nmetrology.",
        "Reinforcement learning has demonstrated great potential for performing\nfinancial tasks. However, it faces two major challenges: policy instability and\nsampling bottlenecks. In this paper, we revisit ensemble methods with massively\nparallel simulations on graphics processing units (GPUs), significantly\nenhancing the computational efficiency and robustness of trained models in\nvolatile financial markets. Our approach leverages the parallel processing\ncapability of GPUs to significantly improve the sampling speed for training\nensemble models. The ensemble models combine the strengths of component agents\nto improve the robustness of financial decision-making strategies. We conduct\nexperiments in both stock and cryptocurrency trading tasks to evaluate the\neffectiveness of our approach. Massively parallel simulation on a single GPU\nimproves the sampling speed by up to $1,746\\times$ using $2,048$ parallel\nenvironments compared to a single environment. The ensemble models have high\ncumulative returns and outperform some individual agents, reducing maximum\ndrawdown by up to $4.17\\%$ and improving the Sharpe ratio by up to $0.21$.\n  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and\n2024.",
        "Automatic counting soybean pods and seeds in outdoor fields allows for rapid\nyield estimation before harvesting, while indoor laboratory counting offers\ngreater accuracy. Both methods can significantly accelerate the breeding\nprocess. However, it remains challenging for accurately counting pods and seeds\nin outdoor fields, and there are still no accurate enough tools for counting\npods and seeds in laboratories. In this study, we developed efficient deep\nlearning models for counting soybean pods and seeds in both outdoor fields and\nindoor laboratories. For outdoor fields, annotating not only visible seeds but\nalso occluded seeds makes YOLO have the ability to estimate the number of\nsoybean seeds that are occluded. Moreover, we enhanced YOLO architecture by\nintegrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques\n(YOLO-DA), to improve model robustness and generalization across soybean images\ntaken in outdoor fields. Testing on soybean images from the outdoor field, we\nachieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for\nseed counting. For the indoor setting, we utilized Mask-RCNN supplemented with\na Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on\nsynthetic training images generated from a small set of labeled data. This\napproach resulted in near-perfect accuracy, with an MAE of 1.07 for pod\ncounting and 1.33 for seed counting across actual laboratory images from two\ndistinct studies.",
        "Datasets play a critical role in medical imaging research, yet issues such as\nlabel quality, shortcuts, and metadata are often overlooked. This lack of\nattention may harm the generalizability of algorithms and, consequently,\nnegatively impact patient outcomes. While existing medical imaging literature\nreviews mostly focus on machine learning (ML) methods, with only a few focusing\non datasets for specific applications, these reviews remain static -- they are\npublished once and not updated thereafter. This fails to account for emerging\nevidence, such as biases, shortcuts, and additional annotations that other\nresearchers may contribute after the dataset is published. We refer to these\nnewly discovered findings of datasets as research artifacts. To address this\ngap, we propose a living review that continuously tracks public datasets and\ntheir associated research artifacts across multiple medical imaging\napplications. Our approach includes a framework for the living review to\nmonitor data documentation artifacts, and an SQL database to visualize the\ncitation relationships between research artifact and dataset. Lastly, we\ndiscuss key considerations for creating medical imaging datasets, review best\npractices for data annotation, discuss the significance of shortcuts and\ndemographic diversity, and emphasize the importance of managing datasets\nthroughout their entire lifecycle. Our demo is publicly available at\nhttp:\/\/130.226.140.142.",
        "We prove new integral formulas for generalized hypergeometric functions and\ntheir confuent variants. We apply them, via stationary phase formula, to study\nWKB expansions of solutions: for large argument in the confuent case and for\nlarge parameter in the general case. We also study variations of hypergeometric\nfunctions for small perturbations of hypergeometric equations, i.e., in\nexpansions of solutions in powers of a small parameter. Next, we present a new\nproof of a theorem due to Wasow about equivalence of the Airy equation with its\nperturbation; in particular, we explain that this result does not deal with the\nWKB solutions and the Stokes phenomenon. Finally, we study hypergeometric\nequations, one of second order and another of third order, which are related\nwith two generating functions for MZVs, one $\\Delta_2 (\\lambda )$ for $\\zeta(2,\n\\ldots , 2)$'s and another $\\Delta_3 (\\lambda )$ for $\\zeta(3, \\ldots , 3)$'s;\nin particular, we correct a statement from [ZZ3] that the function\n$\\Delta_3(\\lambda)$ admits a regular WKB expansion.",
        "Hamiltonians of one-dimensional, disordered single-particle systems with\nlong-range hopping terms can naturally be modeled by power-law banded random\nmatrices. In this picture, the phase diagram of a power-law banded random\nmatrix ensemble show ergodic, weakly ergodic, multifractal, and localized\nphases. Motivated by recent developments on ergodicity breaking and\nlocalization in interacting quantum many-body systems, we explore many-body\ninterpretations of the power-law banded random matrix ensemble. We discuss a\nnumber of ways to label the basis vectors with many-body configurations, and\ncompare the physical properties of the resulting Hamiltonians. We characterize\nthe scaling of the many-body eigenstate entanglement entropy with system size\nfor the different labeling schemes and in each of the phases. Using a scaling\nanalysis on the full sets of eigenstates, we subsequently provide a\nquantitative picture of the boundary between the different types of scaling\nbehavior that we observe for the spectral-bulk and spectral-edge eigenstates.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "We introduce a new method for the study of fluctuations of partial transition\nwidths based on nuclear resonance fluorescence experiments with\nquasimonochromatic linearly-polarized photon beams below particle separation\nthresholds. It is based on the average branching of decays of $J=1$ states of\nan even-even nucleus to the $2^+_1$ state in comparison to the ground state.\nBetween 5 and 7 MeV, an almost constant average branching ratio of 0.490(16) is\nobserved for the nuclide $^{150}$Nd. Assuming $\\chi^2$-distributed partial\ntransition widths, this average branching ratio is related to a degree of\nfreedom of $\\nu = 1.93(12)$, rejecting the validity of the Porter-Thomas\ndistribution, requiring $\\nu=1$. The observed deviation can be explained by\nnon-statistical effects in the $\\gamma$-decay behavior with contributions in\nthe range of 9.4(10)% up to 94(10)%.",
        "Vision-Language Models (VLMs) represent a breakthrough in artificial\nintelligence by integrating visual and textual modalities to achieve impressive\nzero-shot capabilities. However, VLMs are susceptible to catastrophic\nforgetting when sequentially fine-tuned on multiple downstream tasks. Existing\ncontinual learning methods for VLMs often rely heavily on additional reference\ndatasets, compromise zero-shot performance, or are limited to\nparameter-efficient fine-tuning scenarios. In this paper, we propose Continual\nDecoupling-Unifying (ConDU), a novel approach, by introducing model fusion into\ncontinual learning for VLMs. ConDU maintains a unified model along with task\ntriggers and prototype sets, employing an iterative process of decoupling\ntask-specific models for previous tasks and unifying them with the model for\nthe newly learned task. Additionally, we introduce an inference strategy for\nzero-shot scenarios by aggregating predictions from multiple decoupled\ntask-specific models. Extensive experiments across various settings show that\nConDU achieves up to a 2\\% improvement in average performance across all seen\ntasks compared to state-of-the-art baselines, while also enhancing zero-shot\ncapabilities relative to the original VLM.",
        "Cognitive diagnosis aims to infer students' mastery levels based on their\nhistorical response logs. However, existing cognitive diagnosis models (CDMs),\nwhich rely on ID embeddings, often have to train specific models on specific\ndomains. This limitation may hinder their directly practical application in\nvarious target domains, such as different subjects (e.g., Math, English and\nPhysics) or different education platforms (e.g., ASSISTments, Junyi Academy and\nKhan Academy). To address this issue, this paper proposes the language\nrepresentation favored zero-shot cross-domain cognitive diagnosis (LRCD).\nSpecifically, LRCD first analyzes the behavior patterns of students, exercises\nand concepts in different domains, and then describes the profiles of students,\nexercises and concepts using textual descriptions. Via recent advanced\ntext-embedding modules, these profiles can be transformed to vectors in the\nunified language space. Moreover, to address the discrepancy between the\nlanguage space and the cognitive diagnosis space, we propose language-cognitive\nmappers in LRCD to learn the mapping from the former to the latter. Then, these\nprofiles can be easily and efficiently integrated and trained with existing\nCDMs. Extensive experiments show that training LRCD on real-world datasets can\nachieve commendable zero-shot performance across different target domains, and\nin some cases, it can even achieve competitive performance with some classic\nCDMs trained on the full response data on target domains. Notably, we\nsurprisingly find that LRCD can also provide interesting insights into the\ndifferences between various subjects (such as humanities and sciences) and\nsources (such as primary and secondary education).",
        "Omnidirectional image quality assessment (OIQA) has been one of the hot\ntopics in IQA with the continuous development of VR techniques, and achieved\nmuch success in the past few years. However, most studies devote themselves to\nthe uniform distortion issue, i.e., all regions of an omnidirectional image are\nperturbed by the ``same amount'' of noise, while ignoring the non-uniform\ndistortion issue, i.e., partial regions undergo ``different amount'' of\nperturbation with the other regions in the same omnidirectional image.\nAdditionally, nearly all OIQA models are verified on the platforms containing a\nlimited number of samples, which largely increases the over-fitting risk and\ntherefore impedes the development of OIQA. To alleviate these issues, we\nelaborately explore this topic from both subjective and objective perspectives.\nSpecifically, we construct a large OIQA database containing 10,320\nnon-uniformly distorted omnidirectional images, each of which is generated by\nconsidering quality impairments on one or two camera len(s). Then we\nmeticulously conduct psychophysical experiments and delve into the influence of\nboth holistic and individual factors (i.e., distortion range and viewing\ncondition) on omnidirectional image quality. Furthermore, we propose a\nperception-guided OIQA model for non-uniform distortion by adaptively\nsimulating users' viewing behavior. Experimental results demonstrate that the\nproposed model outperforms state-of-the-art methods. The source code is\navailable at https:\/\/github.com\/RJL2000\/OIQAND.",
        "Goal-oriented visual dialogue involves multi-round interaction between\nartificial agents, which has been of remarkable attention due to its wide\napplications. Given a visual scene, this task occurs when a Questioner asks an\naction-oriented question and an Answerer responds with the intent of letting\nthe Questioner know the correct action to take. The quality of questions\naffects the accuracy and efficiency of the target search progress. However,\nexisting methods lack a clear strategy to guide the generation of questions,\nresulting in the randomness in the search process and inconvergent results. We\npropose a Tree-Structured Strategy with Answer Distribution Estimator (TSADE)\nwhich guides the question generation by excluding half of the current candidate\nobjects in each round. The above process is implemented by maximizing a binary\nreward inspired by the ``divide-and-conquer'' paradigm. We further design a\ncandidate-minimization reward which encourages the model to narrow down the\nscope of candidate objects toward the end of the dialogue. We experimentally\ndemonstrate that our method can enable the agents to achieve high task-oriented\naccuracy with fewer repeating questions and rounds compared to traditional\nergodic question generation approaches. Qualitative results further show that\nTSADE facilitates agents to generate higher-quality questions.",
        "Dammann gratings - beam-shaping optical elements acting as beam multipliers\nwith equal-power beams - are a key element in three-dimensional imaging based\non structured light and beam combiners for high-power laser applications.\nHowever, two-dimensional Dammann grating structures suffer from a significant\nreduction of the uniformity among the diffraction orders. Here, we report\nDammann metasurfaces based on the geometric phase as the structure realization\nfor the target phase profile, which outperform the capabilities of Dammann\ngratings by overcoming the uniformity defects in their two-dimensional\ndiffraction patterns. We showed that two-dimensional Dammann metasurfaces\nexhibit high uniformity and diffraction efficiency, in contrast to Dammann\ngratings, by overcoming the uniformity defects via a robust and highly precise\nphase imprint. Moreover, Dammann metasurfaces outperform their grating\ncounterparts by exhibiting a polarization-independent response and a broadband\noperation. This study reveals that by providing physics-driven solutions,\nmetasurfaces can outperform the capabilities of their bulk optics counterparts\nwhile facilitating virtually flat, ultrathin, and lightweight optics.",
        "The idea of a parsing of a stationary process according to a collection of\nwords is introduced, and the basic framework required for the asymptotic\nanalysis of these parsings is presented. We demonstrate how the pointwise\nergodic theorem and the Shannon-McMillan-Breiman theorem can be deduced from\ntheir respective weaker convergence in probability versions combined with our\nobservations regarding parsings, where the parsings are done according to\ncollections that originate in stopping times tailored for that purpose."
      ]
    }
  },
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Medical diffusion on a budget: textual inversion for medical image generation",
    "start_abstract":"Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
      ],
      "abstract":[
        "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "KidneyTalk-open: No-code Deployment of a Private Large Language Model\n  with Medical Documentation-Enhanced Knowledge Database for Kidney Disease",
        "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
        "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making",
        "Optimization of Link Configuration for Satellite Communication Using\n  Reinforcement Learning",
        "Ranking Counterfactual Explanations",
        "Metacognition in Content-Centric Computational Cognitive C4 Modeling",
        "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination\n  in Multi-Agent Systems",
        "From Text to Space: Mapping Abstract Spatial Models in LLMs during a\n  Grid-World Navigation Task",
        "Natural Language-Assisted Multi-modal Medication Recommendation",
        "StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes\n  through Tree-of-Error",
        "SycEval: Evaluating LLM Sycophancy",
        "Principles for Responsible AI Consciousness Research",
        "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management",
        "Synthesis of omnidirectional path loss model based on directional model\n  and multi-elliptical geometry",
        "Asymmetric Orbifolds, Rank Reduction and Heterotic Islands",
        "Emergence of Order in Chemically Active Droplets: Temporal Dynamics and\n  Collective Behavior",
        "EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for\n  Visual Spatial Tasks",
        "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
        "URL Inspection Tasks: Helping Users Detect Phishing Links in Emails",
        "In Pursuit of Predictive Models of Human Preferences Toward AI Teammates",
        "Topologically protected synchronization in networks",
        "Halfspace Representations of Path Polytopes of Trees",
        "Will AI replace Software Engineers? Do not hold your breath",
        "Effective enhancement of the electron-phonon coupling driven by\n  nonperturbative electronic density fluctuations",
        "PCAP-Backdoor: Backdoor Poisoning Generator for Network Traffic in\n  CPS\/IoT Environments",
        "M-LLM Based Video Frame Selection for Efficient Video Understanding",
        "Which2comm: An Efficient Collaborative Perception Framework for 3D\n  Object Detection",
        "Variational Combinatorial Sequential Monte Carlo for Bayesian\n  Phylogenetics in Hyperbolic Space"
      ],
      "abstract":[
        "Privacy-preserving medical decision support for kidney disease requires\nlocalized deployment of large language models (LLMs) while maintaining clinical\nreasoning capabilities. Current solutions face three challenges: 1) Cloud-based\nLLMs pose data security risks; 2) Local model deployment demands technical\nexpertise; 3) General LLMs lack mechanisms to integrate medical knowledge.\nRetrieval-augmented systems also struggle with medical document processing and\nclinical usability. We developed KidneyTalk-open, a desktop system integrating\nthree technical components: 1) No-code deployment of state-of-the-art (SOTA)\nopen-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2)\nMedical document processing pipeline combining context-aware chunking and\nintelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep)\nemploying agents collaboration for improving the recall rate of medical\ndocuments. A graphical interface was designed to enable clinicians to manage\nmedical documents and conduct AI-powered consultations without technical\nexpertise. Experimental validation on 1,455 challenging nephrology exam\nquestions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1%\nover baseline) with intelligent knowledge integration, while maintaining\nrobustness through 4.9% rejection rate to suppress hallucinations. Comparative\ncase studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL)\ndemonstrate KidneyTalk-open's superior performance in real clinical query.\nKidneyTalk-open represents the first no-code medical LLM system enabling secure\ndocumentation-enhanced medical Q&A on desktop. Its designs establishes a new\nframework for privacy-sensitive clinical AI applications. The system\nsignificantly lowers technical barriers while improving evidence traceability,\nenabling more medical staff or patients to use SOTA open-source LLMs\nconveniently.",
        "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
        "Despite recent advances in artificial intelligence (AI), it poses challenges\nto ensure personalized decision-making in tasks that are not considered in\ntraining datasets. To address this issue, we propose ValuePilot, a two-phase\nvalue-driven decision-making framework comprising a dataset generation toolkit\nDGT and a decision-making module DMM trained on the generated data. DGT is\ncapable of generating scenarios based on value dimensions and closely mirroring\nreal-world tasks, with automated filtering techniques and human curation to\nensure the validity of the dataset. In the generated dataset, DMM learns to\nrecognize the inherent values of scenarios, computes action feasibility and\nnavigates the trade-offs between multiple value dimensions to make personalized\ndecisions. Extensive experiments demonstrate that, given human value\npreferences, our DMM most closely aligns with human decisions, outperforming\nClaude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o. This research is\na preliminary exploration of value-driven decision-making. We hope it will\nstimulate interest in value-driven decision-making and personalized\ndecision-making within the community.",
        "Satellite communication is a key technology in our modern connected world.\nWith increasingly complex hardware, one challenge is to efficiently configure\nlinks (connections) on a satellite transponder. Planning an optimal link\nconfiguration is extremely complex and depends on many parameters and metrics.\nThe optimal use of the limited resources, bandwidth and power of the\ntransponder is crucial. Such an optimization problem can be approximated using\nmetaheuristic methods such as simulated annealing, but recent research results\nalso show that reinforcement learning can achieve comparable or even better\nperformance in optimization methods. However, there have not yet been any\nstudies on link configuration on satellite transponders. In order to close this\nresearch gap, a transponder environment was developed as part of this work. For\nthis environment, the performance of the reinforcement learning algorithm PPO\nwas compared with the metaheuristic simulated annealing in two experiments. The\nresults show that Simulated Annealing delivers better results for this static\nproblem than the PPO algorithm, however, the research in turn also underlines\nthe potential of reinforcement learning for optimization problems.",
        "AI-driven outcomes can be challenging for end-users to understand.\nExplanations can address two key questions: \"Why this outcome?\" (factual) and\n\"Why not another?\" (counterfactual). While substantial efforts have been made\nto formalize factual explanations, a precise and comprehensive study of\ncounterfactual explanations is still lacking. This paper proposes a formal\ndefinition of counterfactual explanations, proving some properties they\nsatisfy, and examining the relationship with factual explanations. Given that\nmultiple counterfactual explanations generally exist for a specific case, we\nalso introduce a rigorous method to rank these counterfactual explanations,\ngoing beyond a simple minimality condition, and to identify the optimal ones.\nOur experiments with 12 real-world datasets highlight that, in most cases, a\nsingle optimal counterfactual explanation emerges. We also demonstrate, via\nthree metrics, that the selected optimal explanation exhibits higher\nrepresentativeness and can explain a broader range of elements than a random\nminimal counterfactual. This result highlights the effectiveness of our\napproach in identifying more robust and comprehensive counterfactual\nexplanations.",
        "For AI agents to emulate human behavior, they must be able to perceive,\nmeaningfully interpret, store, and use large amounts of information about the\nworld, themselves, and other agents. Metacognition is a necessary component of\nall of these processes. In this paper, we briefly a) introduce content-centric\ncomputational cognitive (C4) modeling for next-generation AI agents; b) review\nthe long history of developing C4 agents at RPI's LEIA (Language-Endowed\nIntelligent Agents) Lab; c) discuss our current work on extending LEIAs'\ncognitive capabilities to cognitive robotic applications developed using a\nneuro symbolic processing model; and d) sketch plans for future developments in\nthis paradigm that aim to overcome underappreciated limitations of currently\npopular, LLM-driven methods in AI.",
        "As scaling large language models faces prohibitive costs, multi-agent systems\nemerge as a promising alternative, though challenged by static knowledge\nassumptions and coordination inefficiencies. We introduces Knowledge-Aware\nBayesian Bandits (KABB), a novel framework that enhances multi-agent system\ncoordination through semantic understanding and dynamic adaptation. The\nframework features three key innovations: a three-dimensional knowledge\ndistance model for deep semantic understanding, a dual-adaptation mechanism for\ncontinuous expert optimization, and a knowledge-aware Thompson Sampling\nstrategy for efficient expert selection. Extensive evaluation demonstrates KABB\nachieves an optimal cost-performance balance, maintaining high performance\nwhile keeping computational demands relatively low in multi-agent coordination.",
        "Understanding how large language models (LLMs) represent and reason about\nspatial information is crucial for building robust agentic systems that can\nnavigate real and simulated environments. In this work, we investigate the\ninfluence of different text-based spatial representations on LLM performance\nand internal activations in a grid-world navigation task. By evaluating models\nof various sizes on a task that requires navigating toward a goal, we examine\nhow the format used to encode spatial information impacts decision-making. Our\nexperiments reveal that cartesian representations of space consistently yield\nhigher success rates and path efficiency, with performance scaling markedly\nwith model size. Moreover, probing LLaMA-3.1-8B revealed subsets of internal\nunits, primarily located in intermediate layers, that robustly correlate with\nspatial features, such as the position of the agent in the grid or action\ncorrectness, regardless of how that information is represented, and are also\nactivated by unrelated spatial reasoning tasks. This work advances our\nunderstanding of how LLMs process spatial information and provides valuable\ninsights for developing more interpretable and robust agentic AI systems.",
        "Combinatorial medication recommendation(CMR) is a fundamental task of\nhealthcare, which offers opportunities for clinical physicians to provide more\nprecise prescriptions for patients with intricate health conditions,\nparticularly in the scenarios of long-term medical care. Previous research\nefforts have sought to extract meaningful information from electronic health\nrecords (EHRs) to facilitate combinatorial medication recommendations. Existing\nlearning-based approaches further consider the chemical structures of\nmedications, but ignore the textual medication descriptions in which the\nfunctionalities are clearly described. Furthermore, the textual knowledge\nderived from the EHRs of patients remains largely underutilized. To address\nthese issues, we introduce the Natural Language-Assisted Multi-modal Medication\nRecommendation(NLA-MMR), a multi-modal alignment framework designed to learn\nknowledge from the patient view and medication view jointly. Specifically,\nNLA-MMR formulates CMR as an alignment problem from patient and medication\nmodalities. In this vein, we employ pretrained language models(PLMs) to extract\nin-domain knowledge regarding patients and medications, serving as the\nfoundational representation for both modalities. In the medication modality, we\nexploit both chemical structures and textual descriptions to create medication\nrepresentations. In the patient modality, we generate the patient\nrepresentations based on textual descriptions of diagnosis, procedure, and\nsymptom. Extensive experiments conducted on three publicly accessible datasets\ndemonstrate that NLA-MMR achieves new state-of-the-art performance, with a\nnotable average improvement of 4.72% in Jaccard score. Our source code is\npublicly available on https:\/\/github.com\/jtan1102\/NLA-MMR_CIKM_2024.",
        "Evaluating mathematical capabilities is critical for assessing the overall\nperformance of large language models (LLMs). However, existing evaluation\nmethods often focus solely on final answers, resulting in highly inaccurate and\nuninterpretable evaluation outcomes, as well as their failure to assess proof\nor open-ended problems. To address these issues, we propose a novel\nmathematical process evaluation agent based on Tree-of-Error, called\nStepMathAgent. This agent incorporates four internal core operations: logical\nstep segmentation, step scoring, score aggregation and error tree generation,\nalong with four external extension modules: difficulty calibration, simplicity\nevaluation, completeness validation and format assessment. Furthermore, we\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\nevaluation instances, derived from 200 high-quality math problems grouped by\nproblem type, subject category and difficulty level. Experiments on\nStepMathBench show that our proposed StepMathAgent outperforms all\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\nand broad applicability to various scenarios. Our data and code are available\nat https:\/\/github.com\/SHU-XUN\/StepMathAgent.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "Recent research suggests that it may be possible to build conscious AI\nsystems now or in the near future. Conscious AI systems would arguably deserve\nmoral consideration, and it may be the case that large numbers of conscious\nsystems could be created and caused to suffer. Furthermore, AI systems or\nAI-generated characters may increasingly give the impression of being\nconscious, leading to debate about their moral status. Organisations involved\nin AI research must establish principles and policies to guide research and\ndeployment choices and public communication concerning consciousness. Even if\nan organisation chooses not to study AI consciousness as such, it will still\nneed policies in place, as those developing advanced AI systems risk\ninadvertently creating conscious entities. Responsible research and deployment\npractices are essential to address this possibility. We propose five principles\nfor responsible research and argue that research organisations should make\nvoluntary, public commitments to principles on these lines. Our principles\nconcern research objectives and procedures, knowledge sharing and public\ncommunications.",
        "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application.",
        "Millimeter wave (mmWave) technology offers high throughput but has a limited\nradio range, necessitating the use of directional antennas or beamforming\nsystems such as massive MIMO. Path loss (PL) models using narrow-beam antennas\nare known as directional models, while those using omnidirectional antennas are\nreferred to as omnidirectional models. To standardize the analysis,\nomnidirectional PL models for mmWave ranges have been introduced, including TR\n38.901 by 3GPP, which is based on measurements from directional antennas.\nHowever, synthesizing these measurements can be complex and time-consuming.\nThis study proposes a numerical approach to derive an omnidirectional model\nfrom directional data using multi-elliptical geometry. We assessed the\neffectiveness of this method against existing PL models for mmWaves that are\navailable in the literature.",
        "We consider toroidal asymmetric orbifolds of the heterotic string preserving\nall 16 supercharges, developing a general formalism to study components of the\nmoduli space characterized by rank reduction of the gauge group. In particular\nwe construct six- and four-dimensional heterotic islands with no massless\nmoduli other than the dilaton. The formalism involves the Leech lattice, its\nautomorphisms and their corresponding invariant and normal, or coinvariant,\nsublattices.",
        "Collective behaviors such as swarming, chemical signaling, and clustering are\nfundamental to biological microorganisms, enabling hierarchical colony\nformation, coordinated motion, and enhanced nutrient accessibility crucial for\ntheir survival. Over the past few decades, extensive research has been\ndedicated to unraveling the mechanisms underlying these diverse collective\npatterns through experimental model systems. Among these, active droplets have\nemerged as valuable synthetic analogs, effectively replicating key biological\nattributes and serving as ideal platforms for investigating collective\nphenomena. This research explores the collective behavior of\n4-Cyano-4-pentyl-biphenyl (5CB) oil droplets across varying P\\'eclet ($Pe$)\nnumbers. At high $Pe$, droplets exhibit a pusher mode of propulsion and form\ndynamic chain-like patterns. Decreasing $Pe$ enhances repulsive interactions\namong droplets, resulting in the inhibition of clustering. In the low $Pe$\nregime, their repulsive interactions predominated by chemical field lead to the\nemergence of an ordered structure. Furthermore, we illustrate how active\ndroplets efficiently navigate within a soft structured environment. These\nfindings contribute to our comprehension of self-organized phenomena in active\nmatter systems and provide insights for designing strategies for controlled\nlocomotion in intricate fluidic environments.",
        "While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon.",
        "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
        "The most widespread type of phishing attack involves email messages with\nlinks pointing to malicious content. Despite user training and the use of\ndetection techniques, these attacks are still highly effective. Recent studies\nshow that it is user inattentiveness, rather than lack of education, that is\none of the key factors in successful phishing attacks. To this end, we develop\na novel phishing defense mechanism based on URL inspection tasks: small\nchallenges (loosely inspired by CAPTCHAs) that, to be solved, require users to\ninteract with, and understand, the basic URL structure. We implemented and\nevaluated three tasks that act as ``barriers'' to visiting the website: (1)\ncorrect click-selection from a list of URLs, (2) mouse-based highlighting of\nthe domain-name URL component, and (3) re-typing the domain-name. These tasks\nfollow best practices in security interfaces and warning design.\n  We assessed the efficacy of these tasks through an extensive on-line user\nstudy with 2,673 participants from three different cultures, native languages,\nand alphabets. Results show that these tasks significantly decrease the rate of\nsuccessful phishing attempts, compared to the baseline case. Results also\nshowed the highest efficacy for difficult URLs, such as typo-squats, with which\nparticipants struggled the most. This highlights the importance of (1) slowing\ndown users while focusing their attention and (2) helping them understand the\nURL structure (especially, the domain-name component thereof) and matching it\nto their intent.",
        "We seek measurable properties of AI agents that make them better or worse\nteammates from the subjective perspective of human collaborators. Our\nexperiments use the cooperative card game Hanabi -- a common benchmark for\nAI-teaming research. We first evaluate AI agents on a set of objective metrics\nbased on task performance, information theory, and game theory, which are\nmeasurable without human interaction. Next, we evaluate subjective human\npreferences toward AI teammates in a large-scale (N=241) human-AI teaming\nexperiment. Finally, we correlate the AI-only objective metrics with the human\nsubjective preferences. Our results refute common assumptions from prior\nliterature on reinforcement learning, revealing new correlations between AI\nbehaviors and human preferences. We find that the final game score a human-AI\nteam achieves is less predictive of human preferences than esoteric measures of\nAI action diversity, strategic dominance, and ability to team with other AI. In\nthe future, these correlations may help shape reward functions for training\nhuman-collaborative AI.",
        "In a graph, we say that two nodes are topologically equivalent if their sets\nof first neighbors, excluding the two nodes, coincide. We prove that\nnonlinearly coupled oscillators located on a group of topologically equivalent\nnodes can get easily synchronized when the group forms a fully connected\nsubgraph (or combinations of these), regardless of the status of all the other\noscillators. More generally, any change occurring in the inner part of the\nremainder of the graph will not alter the synchronization status of the group.\nTypically, the group can synchronize when $k^{(\\mathrm{OUT})}\\leq\nk^{(\\mathrm{IN})}$, $k^{(\\mathrm{IN})}$ and $k^{(\\mathrm{OUT})}$ being the\ncommon internal and outgoing degree of each node in the group, respectively.\nSimulations confirm our analysis and suggest that groups of topologically\nequivalent nodes play the role of independent pacemakers.",
        "Given a tree $T$, its path polytope is the convex hull of the edge indicator\nvectors for the paths between any two distinct leaves in $T$. These polytopes\narise naturally in polyhedral geometry and applications, such as phylogenetics,\ntropical geometry, and algebraic statistics. We provide a minimal halfspace\nrepresentation of these polytopes. The construction is made inductively using\ntoric fiber products.",
        "Artificial Intelligence (AI) technology such as Large Language Models (LLMs)\nhave become extremely popular in creating code. This has led to the conjecture\nthat future software jobs will be exclusively conducted by LLMs, and the\nsoftware industry will cease to exist. But software engineering is much more\nthan producing code -- notably, \\emph{maintaining} large software and keeping\nit reliable is a major part of software engineering, which LLMs are not yet\ncapable of.",
        "We present a dynamical mean-field study of the nonperturbative electronic\nmechanisms, which may lead to significant enhancements of the electron-phonon\ncoupling in correlated electron systems. Analyzing the effects of electronic\ncorrelations on the lowest-order electron-phonon processes, we show that in the\nproximity of the Mott metal-to-insulator transition of the doped square lattice\nHubbard model, where the isothermal charge response becomes particularly large\nat small momenta, the coupling of electrons to the lattice is strongly\nincreased. This, in turn, induces significant corrections to both the\nelectronic self-energy and phonon-mediated pairing interaction, indicating the\npossible onset of a strong interplay between lattice and electronic degrees of\nfreedom even for small values of the bare electron-phonon coupling.",
        "The rapid expansion of connected devices has made them prime targets for\ncyberattacks. To address these threats, deep learning-based, data-driven\nintrusion detection systems (IDS) have emerged as powerful tools for detecting\nand mitigating such attacks. These IDSs analyze network traffic to identify\nunusual patterns and anomalies that may indicate potential security breaches.\nHowever, prior research has shown that deep learning models are vulnerable to\nbackdoor attacks, where attackers inject triggers into the model to manipulate\nits behavior and cause misclassifications of network traffic. In this paper, we\nexplore the susceptibility of deep learning-based IDS systems to backdoor\nattacks in the context of network traffic analysis. We introduce\n\\texttt{PCAP-Backdoor}, a novel technique that facilitates backdoor poisoning\nattacks on PCAP datasets. Our experiments on real-world Cyber-Physical Systems\n(CPS) and Internet of Things (IoT) network traffic datasets demonstrate that\nattackers can effectively backdoor a model by poisoning as little as 1\\% or\nless of the entire training dataset. Moreover, we show that an attacker can\nintroduce a trigger into benign traffic during model training yet cause the\nbackdoored model to misclassify malicious traffic when the trigger is present.\nFinally, we highlight the difficulty of detecting this trigger-based backdoor,\neven when using existing backdoor defense techniques.",
        "Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising\nresults in video reasoning. Popular Multi-Modal Large Language Model (M-LLM)\nframeworks usually apply naive uniform sampling to reduce the number of video\nframes that are fed into an M-LLM, particularly for long context videos.\nHowever, it could lose crucial context in certain periods of a video, so that\nthe downstream M-LLM may not have sufficient visual information to answer a\nquestion. To attack this pain point, we propose a light-weight M-LLM -based\nframe selection method that adaptively select frames that are more relevant to\nusers' queries. In order to train the proposed frame selector, we introduce two\nsupervision signals (i) Spatial signal, where single frame importance score by\nprompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by\nprompting Large Language Model (LLM) using the captions of all frame\ncandidates. The selected frames are then digested by a frozen downstream video\nM-LLM for visual reasoning and question answering. Empirical results show that\nthe proposed M-LLM video frame selector improves the performances various\ndownstream video Large Language Model (video-LLM) across medium (ActivityNet,\nNExT-QA) and long (EgoSchema, LongVideoBench) context video question answering\nbenchmarks.",
        "Collaborative perception allows real-time inter-agent information exchange\nand thus offers invaluable opportunities to enhance the perception capabilities\nof individual agents. However, limited communication bandwidth in practical\nscenarios restricts the inter-agent data transmission volume, consequently\nresulting in performance declines in collaborative perception systems. This\nimplies a trade-off between perception performance and communication cost. To\naddress this issue, we propose Which2comm, a novel multi-agent 3D object\ndetection framework leveraging object-level sparse features. By integrating\nsemantic information of objects into 3D object detection boxes, we introduce\nsemantic detection boxes (SemDBs). Innovatively transmitting these\ninformation-rich object-level sparse features among agents not only\nsignificantly reduces the demanding communication volume, but also improves 3D\nobject detection performance. Specifically, a fully sparse network is\nconstructed to extract SemDBs from individual agents; a temporal fusion\napproach with a relative temporal encoding mechanism is utilized to obtain the\ncomprehensive spatiotemporal features. Extensive experiments on the V2XSet and\nOPV2V datasets demonstrate that Which2comm consistently outperforms other\nstate-of-the-art methods on both perception performance and communication cost,\nexhibiting better robustness to real-world latency. These results present that\nfor multi-agent collaborative 3D object detection, transmitting only\nobject-level sparse features is sufficient to achieve high-precision and robust\nperformance.",
        "Hyperbolic space naturally encodes hierarchical structures such as\nphylogenies (binary trees), where inward-bending geodesics reflect paths\nthrough least common ancestors, and the exponential growth of neighborhoods\nmirrors the super-exponential scaling of topologies. This scaling challenge\nlimits the efficiency of Euclidean-based approximate inference methods.\nMotivated by the geometric connections between trees and hyperbolic space, we\ndevelop novel hyperbolic extensions of two sequential search algorithms:\nCombinatorial and Nested Combinatorial Sequential Monte Carlo (\\textsc{Csmc}\nand \\textsc{Ncsmc}). Our approach introduces consistent and unbiased\nestimators, along with variational inference methods (\\textsc{H-Vcsmc} and\n\\textsc{H-Vncsmc}), which outperform their Euclidean counterparts. Empirical\nresults demonstrate improved speed, scalability and performance in\nhigh-dimensional phylogenetic inference tasks."
      ]
    }
  },
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey",
    "start_abstract":"Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Medical diffusion on a budget: textual inversion for medical image generation"
      ],
      "abstract":[
        "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Leveraging 13C NMR spectroscopic data derived from SMILES to predict the\n  functionality of small biomolecules by machine learning: a case study on\n  human Dopamine D1 receptor antagonists",
        "HIV\/AIDS Suppression in North America: Intervention Plans and\n  Cost-Effectiveness of UNAIDS 90-90-90 and 95-95-95 Targets",
        "From FAIR to CURE: Guidelines for Computational Models of Biological\n  Systems",
        "Toxicological Evaluation of Phytochemicals and Heavy Metals in Ficus\n  exasperata Vahl (Sandpaper) Leaves obtained in Birnin Kebbi, Nigeria",
        "Comprehensive Analysis of Bioactive Peptides from Cuminum cyminum L.\n  Seeds: Sequence Identification and Pharmacological Evaluation",
        "Modeling and Optimization of Insulin Injection for Type-1 Diabetes\n  Mellitus Management",
        "Co-Translational mRNA Decay in Plants: Recent advances and future\n  directions",
        "Reproductive system and interaction with fauna in a Mediterranean\n  Pyrophite shrub",
        "A new perspective on brain stimulation interventions: Optimal stochastic\n  tracking control of brain network dynamics",
        "Data Sharing in the PRIMED Consortium: Design, implementation, and\n  recommendations for future policymaking",
        "Conditional Success of Adaptive Therapy: The Role of Treatment-Holiday\n  Thresholds Revealed by Mathematical Modeling",
        "Toward a General Theory for the Scaling and Universality of Thermal\n  Responses in Biology",
        "Human Genome Book: Words, Sentences and Paragraphs",
        "Bayesian optimisation of poloidal field coil positions in tokamaks",
        "Quantum metric non-linear Hall effect in an antiferromagnetic\n  topological insulator thin-film EuSn2As2",
        "Monotone conservative strategies in data assimilation",
        "Swimming mode determines how well mesoscale swimmers shield their odor\n  in turbulence",
        "On the Picard numbers of moduli spaces of one-dimensional sheaves on\n  surfaces",
        "Regular evolution algebras are closed under subalgebras",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "The unification of gravity and the spin-1 field",
        "Void spin distribution as a powerful probe of $\\sigma_{8}$",
        "Decay of mass for a semilinear heat equation with mixed local-nonlocal\n  operators",
        "PUATE: Semiparametric Efficient Average Treatment Effect Estimation from\n  Treated (Positive) and Unlabeled Units",
        "The TES-based Cryogenic AntiCoincidence Detector of ATHENA X-IFU:\n  Validation of the thermal end-to-end simulator towards the updated\n  Demonstration Model (DM 1.1)",
        "OGBoost: A Python Package for Ordinal Gradient Boosting",
        "Regulation of Algorithmic Collusion, Refined: Testing Pessimistic\n  Calibrated Regret",
        "Spectral synthesis for exponentials in weighted $L^2$-spaces"
      ],
      "abstract":[
        "This study contributes to ongoing research which aims to predict small\nbiomolecule functionality using Carbon-13 Nuclear Magnetic Resonance ($^{13}$C\nNMR) spectrum data and machine learning (ML). The approach was demonstrated\nusing a bioassay on human dopamine D1 receptor antagonists. The Simplified\nMolecular Input Line Entry System (SMILES) notations of compounds in this\nbioassay were extracted and converted into spectroscopic data by software\ndesigned for this purpose. The resulting data was then used for ML with\nscikit-learn algorithms. The ML models were trained by 27,756 samples and\ntested by 5,466. From the estimators K-Nearest neighbor, Decision Tree\nClassifier, Random Forest Classifier, Gradient Boosting Classifier, XGBoost\nClassifier, and Support Vector Classifier, the last performed the best,\nachieving 71.5 % accuracy, 77.4 % precision, 60.6% recall, 68 % F1, 71.5 % ROC,\nand 0.749 cross-validation score with 0.005 standard deviation. The methodology\ncan be applied to predict any functionality of any compound when relevant data\nare available. It was hypothesized also that increasing the number of samples\nwould increase accuracy. In addition to the SMILES $^{13}$C NMR spectrum ML\nmodel, the time- , and cost-efficient CID_SID ML model was developed. This\nmodel allows researchers who have developed a compound and obtained its PubChem\nCID and SID to check whether their compound is also a human dopamine D1\nreceptor antagonist based solely on the PubChem identifiers. The metrics of the\nCID_SID ML model were 80.2% accuracy, 86.3% precision, 70.4% recall, 77.6% F1,\n79.9% ROC, five-fold cross-validation score of 0.8071 with 0.0047 Standard\ndeviation.",
        "This study utilizes mathematical models to assess progress toward achieving\nthe UNAIDS 90-90-90 and 95-95-95 targets aimed at managing and eradicating\nHIV\/AIDS. It contrasts stochastic and deterministic models, focusing on their\nutility in optimizing public health strategies. Stochastic models account for\nreal-world unpredictability, offering more realistic insights compared to\ndeterministic approaches. The 95-95-95 targets aim for 95\\% of people living\nwith HIV to know their status, 95\\% of those diagnosed to receive\nantiretroviral therapy (ART), and 95\\% of those on ART to achieve viral\nsuppression. These benchmarks are critical for reducing transmission and\nimproving health outcomes. This analysis establishes the basic reproduction\nnumber ($R_0$) to guide interventions and examines the stability of\ndisease-free and endemic equilibria, providing a foundation for applying\noptimal control strategies to minimize HIV prevalence effectively and\ncost-efficiently. Moreover, the data for this study was sourced from the\nofficial UNAIDS website, focusing on North America. An innovative feature of\nthis study is the application of the Stochastic method, which enhances model\naccuracy and operational efficiency in simulating HIV transmission under\nvarious interventions. This research offers actionable insights for\npolicymakers and contributes to global efforts to achieve the 95-95-95 targets\nby 2030, advancing the fight against HIV\/AIDS.",
        "Guidelines for managing scientific data have been established under the FAIR\nprinciples requiring that data be Findable, Accessible, Interoperable, and\nReusable. In many scientific disciplines, especially computational biology,\nboth data and models are key to progress. For this reason, and recognizing that\nsuch models are a very special type of 'data', we argue that computational\nmodels, especially mechanistic models prevalent in medicine, physiology and\nsystems biology, deserve a complementary set of guidelines. We propose the CURE\nprinciples, emphasizing that models should be Credible, Understandable,\nReproducible, and Extensible. We delve into each principle, discussing\nverification, validation, and uncertainty quantification for model credibility;\nthe clarity of model descriptions and annotations for understandability;\nadherence to standards and open science practices for reproducibility; and the\nuse of open standards and modular code for extensibility and reuse. We outline\nrecommended and baseline requirements for each aspect of CURE, aiming to\nenhance the impact and trustworthiness of computational models, particularly in\nbiomedical applications where credibility is paramount. Our perspective\nunderscores the need for a more disciplined approach to modeling, aligning with\nemerging trends such as Digital Twins and emphasizing the importance of data\nand modeling standards for interoperability and reuse. Finally, we emphasize\nthat given the non-trivial effort required to implement the guidelines, the\ncommunity moves to automate as many of the guidelines as possible.",
        "Objective: Ficus exasperata Vahl (Sandpaper tree) is extensively used in\nNigeria to treat diseases, but a dearth of documentation about its toxicity\nexists. This information is crucial because pollutants can contaminate\nmedicinal plants. This study determined the heavy metal and phytochemical\ncontent of methanolic leaf extract of F exasperata obtained in Birnin Kebbi,\nNigeria. Material and Methods: The lethality of the plant was also assessed\nusing 70 wild shrimps divided equally into seven groups. Group 1 (negative\ncontrol), groups 2 and 3 (positive controls) were exposed to 500 and 1000ppm of\nformaldehyde, respectively; and groups 4-7 were exposed to 1000, 2000, 4000,\nand 8000ppm of extracts, respectively, for 96 hours. Results: The\nphytochemistry revealed high levels of flavonoids and saponins and moderate\nlevels of tannins and phenols. The heavy metal analysis revealed non-tolerable\nlevels of cadmium, copper, and lead, while zinc was within the tolerable limit.\nThe negative control recorded 10% mortality, 1000 and 2000 ppm (20% each),\n4000ppm (70%), and 8000 ppm (100%). Conclusion: These results inferred safe\ndoses of the plant's extract in low and medium concentrations but toxic and\nfatal at high doses over a period of time. Consumers are advised to seek an\nexpert's guidance before using it.",
        "Cuminum cyminum L. (cumin) is a medicinal and edible plant widely used in\ntraditional Chinese medicine (TCM) for treating various ailments, including\ndiarrhea, abdominal pain, inflammation, asthma, and diabetes. While previous\nresearch has primarily focused on its essential oils, studies on its\nprotein-derived bioactive peptides remain limited. In this study, we employed\nan innovative extraction method to isolate peptides from cumin seeds for the\nfirst time and screened their biological activities, revealing significant\nantimicrobial, antioxidant, and hypoglycemic properties. Guided by bioactivity,\nwe utilized advanced separation and structural identification techniques,\nincluding Matrix-Assisted Laser Desorption\/Ionization Time-of-Flight Mass\nSpectrometry (MALDI-TOF\/TOF MS\/MS), to systematically purify and characterize\ncumin-derived peptides. A total of 479 unique peptide sequences were identified\nusing Mascot software and the SwissProt\/UniProt_Bos databases. Among these, 15\nhighly bioactive peptides were selected for further analysis based on\nbioactivity and toxicity predictions using PeptideRanker and ToxinPred.\nStructural characterization revealed key features, such as {\\alpha}-helices and\n\\b{eta}-sheets, associated with their multifunctional activities. This study\nprovides the first comprehensive analysis of bioactive peptides from Cuminum\ncyminum L. seeds, elucidating their potential as antimicrobial, antioxidant,\nand hypoglycemic agents. These findings not only clarify the pharmacological\nbasis of cumin's traditional uses but also lay a theoretical foundation for the\ndevelopment of novel therapeutic agents from this medicinal plant.",
        "Diabetes mellitus is a global health crisis characterized by poor blood sugar\nregulation, impacting millions of people worldwide and leading to severe\ncomplications and mortality. Although Type 1 Diabetes Mellitus (T1DM) has a\nlower number of cases compared to other forms of diabetes, it is often\ndiagnosed at a young age and requires lifelong exogenous insulin\nadministration. In this paper, we focus on understanding the interaction of\ninsulin and glucose molecules within the subcutaneous layer, which is crucial\nfor blood sugar control in T1DM patients. Specifically, we propose a\ncomprehensive model to characterize the insulin-glucose system within the\nsubcutaneous layer, incorporating a multicellular molecular communication\nsystem. We then divide the T1DM system into insulin and glucose subsystems and\nderive the end-to-end expression for insulin-glucose interaction in the\nsubcutaneous layer. We further validate the insulin-glucose interaction\nanalysis with an agent-based simulator. As effectively managing postprandial\nglucose levels is crucial for individuals with T1DM to safeguard their overall\nhealth and avert short-term and long-term complications, we also derive the\noptimal insulin administration time based on the derived glucose response via\nthe Lagrange multiplier and gradient descent ascent method. This allows us to\nexplore the impact of different types of insulin and dietary management on\nblood sugar levels. Simulation results confirm the correctness of our proposed\nmodel and the effectiveness of our optimized effective time window for\ninjecting insulin in individuals with T1DM.",
        "Tight regulation of messenger RNA (mRNA) stability is essential to ensure\naccurate gene expression in response to developmental and environmental cues.\nmRNA stability is controlled by mRNA decay pathways, which have traditionally\nbeen proposed to occur independently of translation. However, the recent\ndiscovery of a co-translational mRNA decay pathway (also known as CTRD) reveals\nthat mRNA translation and decay can be coupled. While being translated, a mRNA\ncan be targeted for degradation. This pathway was first described in yeast and\nrapidly identified in several plant species. This review explores recent\nadvances in our understanding of CTRD in plants, emphasizing its regulation and\nits importance for development and stress response. The different metrics used\nto assess CTRD activity are also presented. Furthermore, this review outlines\nfuture directions to explore the importance of mRNA decay in maintaining mRNA\nhomeostasis in plants.",
        "The ULEX model, in its present state, involves the study of the biomass and\nthe population of the shrub Ulex parviflorus Pourret, but while being a dynamic\nmodel, it is static in the sense that it does not imply the appearance of new\nspecimens of this plant. As a complement to the ULEX model in its two dynamic\nand spatial aspects, and with the idea of extending the model, the authors have\nintroduced from a biological and statistical point of view four characteristics\nof this species, flowering, pollination, fructification, taking special\ninterest in the role played by the pollinators (bees) and dispersion of seeds.",
        "Network control theory (NCT) has recently been utilized in neuroscience to\nfacilitate our understanding of brain stimulation effects. A particularly\nuseful branch of NCT is optimal control, which focuses on applying theoretical\nand computational principles of control theory to design optimal strategies to\nachieve specific goals in neural processes. However, most existing research\nfocuses on optimally controlling brain network dynamics from the original state\nto a target state at a specific time point. In this paper, we present the first\ninvestigation of introducing optimal stochastic tracking control strategy to\nsynchronize the dynamics of the brain network to a target dynamics rather than\nto a target state at a specific time point. We utilized fMRI data from healthy\ngroups, and cases of stroke and post-stroke aphasia. For all participants, we\nutilized a gradient descent optimization method to estimate the parameters for\nthe brain network dynamic system. We then utilized optimal stochastic tracking\ncontrol techniques to drive original unhealthy dynamics by controlling a\ncertain number of nodes to synchronize with target healthy dynamics. Results\nshow that the energy associated with optimal stochastic tracking control is\nnegatively correlated with the intrinsic average controllability of the brain\nnetwork system, while the energy of the optimal state approaching control is\nsignificantly related to the target state value. For a 100-dimensional brain\nnetwork system, controlling the five nodes with the lowest tracking energy can\nachieve relatively acceptable dynamics control effects. Our results suggest\nthat stochastic tracking control is more aligned with the objective of brain\nstimulation interventions, and is closely related to the intrinsic\ncharacteristics of the brain network system, potentially representing a new\ndirection for future brain network optimal control research.",
        "Sharing diverse genomic and other biomedical datasets is critical to advance\nscientific discoveries and their equitable translation to improve human health.\nHowever, data sharing remains challenging in the context of legacy datasets,\nevolving policies, multi-institutional consortium science, and international\nstakeholders. The NIH-funded Polygenic Risk Methods in Diverse Populations\n(PRIMED) Consortium was established to improve the performance of polygenic\nrisk estimates for a broad range of health and disease outcomes with global\nimpacts. Improving polygenic risk score performance across genetically diverse\npopulations requires access to large, diverse cohorts. We report on the design\nand implementation of data sharing policies and procedures developed in PRIMED\nto aggregate and analyze data from multiple, heterogeneous sources while\nadhering to existing data sharing policies for each integrated dataset. We\ndescribe two primary data sharing mechanisms: coordinated dbGaP applications\nand a Consortium Data Sharing Agreement, as well as provide alternatives when\nindividual-level data cannot be shared within the Consortium (e.g., federated\nanalyses). We also describe technical implementation of Consortium data sharing\nin the NHGRI Analysis Visualization and Informatics Lab-space (AnVIL) cloud\nplatform, to share derived individual-level data, genomic summary results, and\nmethods workflows with appropriate permissions. As a Consortium making\nsecondary use of pre-existing data sources, we also discuss challenges and\npropose solutions for release of individual- and summary-level data products to\nthe broader scientific community. We make recommendations for ongoing and\nfuture policymaking with the goal of informing future consortia and other\nresearch activities.",
        "Adaptive therapy (AT) improves cancer treatment by controlling the\ncompetition between sensitive and resistant cells through treatment holidays.\nThis study highlights the critical role of treatment-holiday thresholds in AT\nfor tumors composed of drug-sensitive and resistant cells. Using a\nLotka-Volterra model, the research compares AT with maximum tolerated dose\ntherapy and intermittent therapy, showing that AT's success largely depends on\nthe threshold at which treatment is paused and resumed, as well as on the\ncompetition between sensitive and resistant cells. Three scenarios of\ncomparison between AT and other therapies are identified: uniform-decline,\nconditional-improve, and uniform-improve, illustrating that optimizing the\ntreatment-holiday threshold is crucial for AT effectiveness. Tumor composition,\nincluding initial tumor burden and the proportion of resistant cells,\ninfluences outcomes. Adjusting threshold values enables AT to suppress\nresistant subclones, preserving sensitive cells, ultimately improving\nprogression-free survival. These findings emphasize the importance of\npersonalized treatment strategies potentially enhancing long-term therapeutic\noutcomes.",
        "We developed a theory showing that under appropriate normalizations and\nrescalings, temperature response curves show a remarkably regular behavior and\nfollow a general, universal law. The impressive universality of temperature\nresponse curves remained hidden due to various curve-fitting models not\nwell-grounded in first principles. In addition, this framework has the\npotential to explain the origin of different scaling relationships in thermal\nperformance in biology, from molecules to ecosystems. Here, we summarize the\nbackground, principles and assumptions, predictions, implications, and possible\nextensions of this theory.",
        "Since the completion of the human genome sequencing project in 2001,\nsignificant progress has been made in areas such as gene regulation editing and\nprotein structure prediction. However, given the vast amount of genomic data,\nthe segments that can be fully annotated and understood remain relatively\nlimited. If we consider the genome as a book, constructing its equivalents of\nwords, sentences, and paragraphs has been a long-standing and popular research\ndirection. Recently, studies on transfer learning in large language models have\nprovided a novel approach to this challenge.Multilingual transfer ability,\nwhich assesses how well models fine-tuned on a source language can be applied\nto other languages, has been extensively studied in multilingual pre-trained\nmodels. Similarly, the transfer of natural language capabilities to \"DNA\nlanguage\" has also been validated. Building upon these findings, we first\ntrained a foundational model capable of transferring linguistic capabilities\nfrom English to DNA sequences. Using this model, we constructed a vocabulary of\nDNA words and mapped DNA words to their English equivalents.Subsequently, we\nfine-tuned this model using English datasets for paragraphing and sentence\nsegmentation to develop models capable of segmenting DNA sequences into\nsentences and paragraphs. Leveraging these models, we processed the GRCh38.p14\nhuman genome by segmenting, tokenizing, and organizing it into a \"book\"\ncomprised of genomic \"words,\" \"sentences,\" and \"paragraphs.\" Additionally,\nbased on the DNA-to-English vocabulary mapping, we created an \"English version\"\nof the genomic book. This study offers a novel perspective for understanding\nthe genome and provides exciting possibilities for developing innovative tools\nfor DNA search, generation, and analysis.",
        "The tokamak is a world-leading concept for producing sustainable energy via\nmagnetically-confined nuclear fusion. Identifying where to position the magnets\nwithin a tokamak, specifically the poloidal field (PF) coils, is a design\nproblem which requires balancing a number of competing economic, physical, and\nengineering objectives and constraints. In this paper, we show that\nmulti-objective Bayesian optimisation (BO), an iterative optimisation technique\nutilising probabilistic machine learning models, can effectively explore this\ncomplex design space and return several optimal PF coil sets. These solutions\nspan the Pareto front, a subset of the objective space that optimally satisfies\nthe specified objective functions. We outline an easy-to-use BO framework and\ndemonstrate that it outperforms alternative optimisation techniques while using\nsignificantly fewer computational resources. Our results show that BO is a\npromising technique for fusion design problems that rely on computationally\ndemanding high-fidelity simulations.",
        "The quantum geometric structure of electrons introduces fundamental insights\ninto understanding quantum effects in materials. One notable manifestation is\nthe non-linear Hall effect (NLHE), which has drawn considerable interest for\nits potential to overcome the intrinsic limitations of semiconductor diodes at\nlow input power and high frequency. In this study, we investigate NLHE stemming\nfrom the real part of the quantum geometric tensor, specifically the quantum\nmetric, in an antiferromagnetic topological material, EuSn2As2, using density\nfunctional theory. Our calculations predict a remarkable NLHE arising from a\nsymmetry-protected, single Type-II surface Dirac cone in the\neven-numbered-layer two-dimensional slab thin-film, yielding a non-linear Hall\nconductivity exceeding 20 mA\/V2-an order of magnitude larger than previously\nreported. This single Dirac band dispersion represents the simplest model for\ngenerating NLHE, positioning the EuSn2As2 thin-film as a hydrogen atom for NLHE\nsystems. Additionally, we observe NLHE from band-edge states near the Fermi\nlevel. Our findings also reveal that 30% phosphorus (P) doping can double the\nnon-linear Hall conductivity. With its substantial and tunable NLHE, EuSn2As2\nthin-films present promising applications in antiferromagnetic spintronics and\nrectification devices.",
        "This paper studies whether numerically preserving monotonic properties can\noffer modelling advantages in data assimilation, particularly when the signal\nor data is a realization of a stochastic partial differential equation (SPDE)\nor partial differential equation (PDE) with a monotonic property. We\ninvestigate the combination of stochastic Strong Stability Preserving (SSP)\ntime-stepping, nonlinear solving strategies and data assimilation. Experimental\nresults indicate that a particle filter whose ensemble members are solved\nmonotonically can increase forecast skill when the reference data (not\nnecessarily observations) also has a monotone property. Additionally, more\nadvanced techniques used to avoid the degeneracy of the filter\n(tempering-jittering) are shown to be compatible with a conservative monotone\napproach.",
        "Marine organisms manipulate their surrounding flow through their swimming\ndynamics, which affects the transport of their own odor cues. We demonstrate by\ndirect numerical simulations how a group of mesoscale swimmers immersed in a\nturbulent flow alters the shape of the odor plume they release in the water.\nOdor mixing is enhanced by increased velocity fluctuations and a\nswimmer-induced flow circulation which widen the odor plume at close range\nwhile speeding up dilution of the chemical trace. Beyond a short-range increase\nin the likelihood of being detected, swimming considerably reduces detections\nwith effects that can persist at distances of the order of ten times the size\nof the group or more. We find that puller-like swimmers are more effective at\nolfactory shielding than pusher-like swimmers. We trace this difference back to\nthe dynamics at the swimmer location, which tends to trap odor at the source\nfor pushers and to dilute it for pullers. Olfactory shielding is robust to\nchanges in the conditions, and is more pronounced for weak turbulent Reynolds\nnumbers and large swimmer Reynolds numbers. Our results suggest that olfactory\nshielding may play a role in the emergence of different swimming modalities by\nmarine organisms.",
        "Motivated by asymptotic phenomena of moduli spaces of higher rank stable\nsheaves on algebraic surfaces, we study the Picard number of the moduli space\nof one-dimensional stable sheaves supported in a sufficiently positive divisor\nclass on a surface. We give an asymptotic lower bound of the Picard number in\ngeneral. In some special cases, we show that this lower bound is attained based\non the geometry of moduli spaces of stable pairs and relative Hilbert schemes\nof points. Additionally, we discuss several related questions and provide\nexamples where the asymptotic irreducibility of the moduli space fails,\nhighlighting a notable distinction from the higher rank case.",
        "The main goal of this note is to show that subalgebras of regular evolution\nalgebras are themselves evolution algebras. This allows us to assume, without\nloss of generality, that every subalgebra in the regular setting has a basis\nconsisting of vectors with disjoint supports. Finally, we use this result to\ncharacterise the existence of codimension-one subalgebras in regular evolution\nalgebras.",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "Unifying the massive spin-1 field with gravity requires the implementation of\na regular vector field that satisfies the spin-1 Proca equation and is a\nfundamental part of the spacetime metric. That vector field is one of the pair\nof vectors in the line element field (\\textbf{X},-\\textbf{X}), which is\nparamount to the existence of all Lorentzian metrics and Modified General\nRelativity (MGR). Symmetrization of the spin-1 Klein-Gordon equation in a\ncurved Lorentzian spacetime introduces the Lie derivative of the metric along\nthe flow of one of the regular vectors in the line element field. The Proca\nequation in curved spacetime can then be described geometrically in terms of\nthe line element vector, the Lie derivative of the Lorentzian metric, and the\nRicci tensor, which unifies gravity and the spin-1 field. Related issues\nconcerning charge conservation and the Lorenz constraint, singularities in a\nspherically symmetric curved spacetime, and geometrical implications of MGR to\nquantum theory are discussed. A geometrical unification of gravity with quantum\nfield theory is presented.",
        "We present a numerical proof of the concept that the void spin distributions\ncan in principle provide a tight constraint on the amplitude of matter density\nfluctuation on the scale of $8\\,h^{-1}{\\rm Mpc}$ ($\\sigma_{8}$) without being\nseverely deteriorated by the degeneracies of $\\sigma_{8}$ with cold dark matter\ndensity parameter multiplied by the dimensionless Hubble parameter square\n($\\Omega_{\\rm cdm}h^{2}$), total neutrino mass ($M_{\\nu}$) and dark energy\nequation of state ($w$). Applying the Void-Finder algorithm to a total of $15$\nAbacusSummit $N$-body simulations of $15$ different cosmological models, we\nidentify the giant voids to measure their spins, defined as the magnitudes of\nrescaled specific angular momenta of void halos. The $15$ cosmologies include\nthe Planck $\\Lambda$CDM and $14$ non-Planck models, each of which differs among\none another only in one of $\\{\\sigma_{8},\\ \\Omega_{\\rm cdm}h^{2},\\ M_{\\nu},\\\nw\\}$. The probability density distribution of void spins is determined for each\nmodel and found to be well approximated by the generalized Gamma distribution\nwith two characteristic parameters, $k$ and $\\theta$. It turns out that the\nbest-fit values of $k$ and $\\theta$ exhibit very sensitive dependences only on\n$\\sigma_{8}$, being almost insensitive to $\\Omega_{\\rm cdm}h^{2}$, $M_{\\nu}$,\n$w$. This exclusive $\\sigma_{8}$-dependence of the void spin distributions is\nconfirmed to be robust against the variation of the mass and number cuts of\nvoid halos. We also test an observational feasibility of estimating the void\nspins from real data on the galaxy redshifts.",
        "In this paper, we are concerned with the Cauchy problem for the\nreaction-diffusion equation $\\partial_t u+t^\\beta\\mathcal{L} u= - h(t)u^p$\nposed on $\\mathbb{R}^N$, driven by the mixed local-nonlocal operator\n$\\mathcal{L}=-\\Delta+(-\\Delta)^{\\alpha\/2}$, $\\alpha\\in(0,2)$, and supplemented\nwith a nonnegative integrable initial data, where $p>1$, $\\beta\\geq 0$, and\n$h:(0,\\infty)\\to(0,\\infty)$ is a locally integrable function. We study the\nlarge time behavior of non-negative solutions and show that the nonlinear term\ndetermines the large time asymptotic for $p\\leq 1+{\\alpha}\/{N(\\beta+1)},$ while\nthe classical\/anomalous diffusion effects win if $p>1+{\\alpha}\/{N(\\beta+1)}$.",
        "The estimation of average treatment effects (ATEs), defined as the difference\nin expected outcomes between treatment and control groups, is a central topic\nin causal inference. This study develops semiparametric efficient estimators\nfor ATE estimation in a setting where only a treatment group and an unknown\ngroup-comprising units for which it is unclear whether they received the\ntreatment or control-are observable. This scenario represents a variant of\nlearning from positive and unlabeled data (PU learning) and can be regarded as\na special case of ATE estimation with missing data. For this setting, we derive\nsemiparametric efficiency bounds, which provide lower bounds on the asymptotic\nvariance of regular estimators. We then propose semiparametric efficient ATE\nestimators whose asymptotic variance aligns with these efficiency bounds. Our\nfindings contribute to causal inference with missing data and weakly supervised\nlearning.",
        "The Cryogenic AntiCoincidence Detector (CryoAC) is a key element of the X-ray\nIntegral Field Unit (X-IFU) on board the future ATHENA X-ray observatory. It is\na TES-based detector designed to reduce the particle background of the\ninstrument, thereby increasing its sensitivity. The detector design is driven\nby an end-to-end simulator which includes the electro-thermal modelling of the\ndetector and the dynamics of its readout chain. Here, we present the\nmeasurements carried out on the last CryoAC single pixel prototype, namely\nDM127, in order to evaluate the critical thermal parameters of the detector and\nconsequently to tune and validate the CryoAC end-to-end simulator.",
        "This paper introduces OGBoost, a scikit-learn-compatible Python package for\nordinal regression using gradient boosting. Ordinal variables (e.g., rating\nscales, quality assessments) lie between nominal and continuous data,\nnecessitating specialized methods that reflect their inherent ordering. Built\non a coordinate-descent approach for optimization and the latent-variable\nframework for ordinal regression, OGBoost performs joint optimization of a\nlatent continuous regression function (functional gradient descent) and a\nthreshold vector that converts the latent continuous value into discrete class\nprobabilities (classical gradient descent). In addition to the stanadard\nmethods for scikit-learn classifiers, the GradientBoostingOrdinal class\nimplements a \"decision_function\" that returns the (scalar) value of the latent\nfunction for each observation, which can be used as a high-resolution\nalternative to class labels for comparing and ranking observations. The class\nhas the option to use cross-validation for early stopping rather than a single\nholdout validation set, a more robust approach for small and\/or imbalanced\ndatasets. Furthermore, users can select base learners with different underlying\nalgorithms and\/or hyperparameters for use throughout the boosting iterations,\nresulting in a `heterogeneous' ensemble approach that can be used as a more\nefficient alternative to hyperparameter tuning (e.g. via grid search). We\nillustrate the capabilities of OGBoost through examples, using the wine quality\ndataset from the UCI respository. The package is available on PyPI and can be\ninstalled via \"pip install ogboost\".",
        "We study the regulation of algorithmic (non-)collusion amongst sellers in\ndynamic imperfect price competition by auditing their data as introduced by\nHartline et al. [2024].\n  We develop an auditing method that tests whether a seller's pessimistic\ncalibrated regret is low. The pessimistic calibrated regret is the highest\ncalibrated regret of outcomes compatible with the observed data. This method\nrelaxes the previous requirement that a pricing algorithm must use\nfully-supported price distributions to be auditable. This method is at least as\npermissive as any auditing method that has a high probability of failing\nalgorithmic outcomes with non-vanishing calibrated regret. Additionally, we\nstrengthen the justification for using vanishing calibrated regret, versus\nvanishing best-in-hindsight regret, as the non-collusion definition, by showing\nthat even without any side information, the pricing algorithms that only\nsatisfy weaker vanishing best-in-hindsight regret allow an opponent to\nmanipulate them into posting supra-competitive prices. This manipulation cannot\nbe excluded with a non-collusion definition of vanishing best-in-hindsight\nregret.\n  We motivate and interpret the approach of auditing algorithms from their data\nas suggesting a per se rule. However, we demonstrate that it is possible for\nalgorithms to pass the audit by pretending to have higher costs than they\nactually do. For such scenarios, the rule of reason can be applied to bound the\nrange of costs to those that are reasonable for the domain.",
        "We prove that for a some natural class of weights $\\K$ and any weighted space\n$$L^2(w) = \\left\\{ f : (-\\pi,\\pi) \\to \\CC \\colon\\int_{-\\pi}^{\\pi} {{|f(t)|^2}\n\\over {w(t)}} dt < \\infty \\right\\},$$ where $w \\in \\K$, there exists a complete\nand minimal system $\\{e^{i\\lambda_nt}\\}_{n\\in \\Z}$ of exponentials, which does\nnot admit spectral synthesis."
      ]
    }
  },
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction",
    "start_abstract":"Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
      ],
      "abstract":[
        "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Stochastic Time to Extinction of an SIQS Epidemic Model with Quiescence",
        "Unsupervised detection and fitness estimation of emerging SARS-CoV-2\n  variants. Application to wastewater samples (ANRS0160)",
        "Functional Correspondences in the Human and Marmoset Visual Cortex\n  During Movie Watching: Insights from Correlation, Redundancy, and Synergy",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Derivation from kinetic theory and 2-D pattern analysis of chemotaxis\n  models for Multiple Sclerosis",
        "Multicellular self-organization in Escherichia coli",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "R3F: An R package for evolutionary dates, rates, and priors using the\n  relative rate framework",
        "Bowel Incision Closure with a Semi-Automated Robot-Assisted Laser Tissue\n  Soldering System",
        "Language modulates vision: Evidence from neural networks and human\n  brain-lesion models",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "FOCUS: First Order Concentrated Updating Scheme",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Equivalence between exponential concentration in quantum machine\n  learning kernels and barren plateaus in variational algorithms",
        "The Marginal Importance of Distortions and Alignment in CASSI systems",
        "Multi-scale physics of cryogenic liquid helium-4: Inverse\n  coarse-graining properties of smoothed particle hydrodynamics",
        "Stability of difference equations with interspecific density dependence,\n  competition, and maturation delays",
        "Design of a quantum diamond microscope with efficient scanning confocal\n  readout",
        "Timing and spectral studies of the Be\/X-ray binary EXO 2030+375 using\n  Insight-HXMT observations",
        "Numerical Study On Temperature Variations Of Superheated Steam Flowing\n  Through A Regulation Valve",
        "Visualizing quantum entanglement in Bose-Einstein condensates without\n  state vectors",
        "Runout of liquefaction-induced tailings dam failure: Influence of\n  earthquake motions and residual strength",
        "Multifunctional Altermagnet with Large Out-of-Plane Piezoelectric\n  Response in Janus V$_{2}$AsBrO Monolayer",
        "On finitary power monoids of linearly orderable monoids",
        "On the role of 5-wave resonances in the nonlinear dynamics of the\n  Fermi-Pasta-Ulam-Tsingou lattice",
        "Influences of accretion flow and dilaton charge on the images of\n  Einstein-Maxwell-dilation black hole"
      ],
      "abstract":[
        "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
        "Repeated waves of emerging variants during the SARS-CoV-2 pandemics have\nhighlighted the urge of collecting longitudinal genomic data and developing\nstatistical methods based on time series analyses for detecting new threatening\nlineages and estimating their fitness early in time. Most models study the\nevolution of the prevalence of particular lineages over time and require a\nprior classification of sequences into lineages. Such process is prone to\ninduce delays and bias. More recently, few authors studied the evolution of the\nprevalence of mutations over time with alternative clustering approaches,\navoiding specific lineage classification. Most of the aforementioned methods\nare however either non parametric or unsuited to pooled data characterizing,\nfor instance, wastewater samples. In this context, we propose an alternative\nunsupervised method for clustering mutations according to their frequency\ntrajectory over time and estimating group fitness from time series of pooled\nmutation prevalence data. Our model is a mixture of observed count data and\nlatent group assignment and we use the expectation-maximization algorithm for\nmodel selection and parameter estimation. The application of our method to time\nseries of SARS-CoV-2 sequencing data collected from wastewater treatment plants\nin France from October 2020 to April 2021 shows its ability to agnostically\ngroup mutations according to their probability of belonging to B.1.160, Alpha,\nBeta, B.1.177 variants with selection coefficient estimates per group in\ncoherence with the viral dynamics in France reported by Nextstrain. Moreover,\nour method detected the Alpha variant as threatening as early as supervised\nmethods (which track specific mutations over time) with the noticeable\ndifference that, since unsupervised, it does not require any prior information\non the set of mutations.",
        "The world of beauty is deeply connected to the visual cortex, as perception\noften begins with vision in both humans and marmosets. Quantifying functional\ncorrespondences in the visual cortex across species can help us understand how\ninformation is processed in the primate visual cortex, while also providing\ndeeper insights into human visual cortex functions through the study of\nmarmosets. In this study, we measured pairwise and beyond pairwise correlation,\nredundancy, and synergy in movie-driven fMRI data across species. Our first key\nfinding was that humans and marmosets exhibited significant overlaps in\nfunctional synergy. Second, we observed that the strongest functional\ncorrespondences between the human peri-entorhinal and entorhinal cortex (PeEc)\nand the occipitotemporal higher-level visual regions in the marmoset during\nmovie watching reflected a functional synergistic relationship. These regions\nare known to correspond to face-selective areas in both species. Third,\nredundancy measures maintained stable high-order hubs, indicating a steady core\nof shared information processing, while synergy measures revealed a dynamic\nshift from low- to high-level visual regions as interaction increased,\nreflecting adaptive integration. This highlights distinct patterns of\ninformation processing across the visual hierarchy. Ultimately, our results\nreveal the marmoset as a compelling model for investigating visual perception,\ndistinguished by its remarkable functional parallels to the human visual\ncortex.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "In this paper, a class of reaction-diffusion equations for Multiple Sclerosis\nis presented. These models are derived by means of a diffusive limit starting\nfrom a proper kinetic description, taking account of the underlying microscopic\ninteractions among cells. At the macroscopic level, we discuss the necessary\nconditions for Turing instability phenomena and the formation of\ntwo-dimensional patterns, whose shape and stability are investigated by means\nof a weakly nonlinear analysis. Some numerical simulations, confirming and\nextending theoretical results, are proposed for a specific scenario.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "The relative rate framework (RRF) can estimate divergence times from branch\nlengths in a phylogeny, which is the theoretical basis of the RelTime method\nfrequently applied, a relaxed clock approach for molecular dating that scales\nwell for large phylogenies. The use of RRF has also enabled the development of\ncomputationally efficient and accurate methods for testing the autocorrelation\nof lineage rates in a phylogeny (CorrTest) and selecting data-driven parameters\nof the birth-death speciation model (ddBD), which can be used to specify priors\nin Bayesian molecular dating. We have developed R3F, an R package implementing\nRRF to estimate divergence times, infer lineage rates, conduct CorrTest, and\nbuild a ddBD tree prior for Bayesian dating in molecular phylogenies. Here, we\ndescribe R3F functionality and explain how to interpret and use its outputs in\nother visualization software and packages, such as MEGA, ggtree, and FigTree.\nUltimately, R3F is intended to enable the dating of the Tree of Life with\ngreater accuracy and precision, which would have important implications for\nstudies of organism evolution, diversification dynamics, phylogeography, and\nbiogeography. Availability and Implementation: The source codes and related\ninstructions for installing and implementing R3F are available from GitHub\n(https:\/\/github.com\/cathyqqtao\/R3F).",
        "Traditional methods for closing gastrointestinal (GI) surgery incisions, like\nsuturing and stapling, present significant challenges, including potentially\nlife-threatening leaks. These techniques, especially in robot-assisted\nminimally invasive surgery (RAMIS), require advanced manual skills. While their\nrepetitive and time-consuming nature makes them suitable candidates for\nautomation, the automation process is complicated by the need for extensive\ncontact with the tissue. Addressing this, we demonstrate a semi-autonomous\ncontactless surgical procedure using our novel Robot-assisted Laser Tissue\nSoldering (RLTS) system on a live porcine bowel. Towards this in-vivo\ndemonstration, we optimized soldering protocols and system parameters in\nex-vivo experiments on porcine bowels and a porcine cadaver. To assess the RLTS\nsystem performance, we compared the pressure at which the anastomosis leaked\nbetween our robotic soldering and manual suturing. With the best setup, we\nadvanced to an in-vivo Heineke Mikulicz closure on small bowel incision in live\npigs and evaluated their healing for two weeks. All pigs successfully\ncompleting the procedure (N=5) survived without leaks and the histology\nindicated mucosal regeneration and fibrous tissue adhesion. This marks the\nfirst in-vivo semi-automated contactless incision closure, paving the way for\nautomating GI surgery incision closure which has the potential to become an\nalternative to traditional methods.",
        "Comparing information structures in between deep neural networks (DNNs) and\nthe human brain has become a key method for exploring their similarities and\ndifferences. Recent research has shown better alignment of vision-language DNN\nmodels, such as CLIP, with the activity of the human ventral occipitotemporal\ncortex (VOTC) than earlier vision models, supporting the idea that language\nmodulates human visual perception. However, interpreting the results from such\ncomparisons is inherently limited due to the \"black box\" nature of DNNs. To\naddress this, we combined model-brain fitness analyses with human brain lesion\ndata to examine how disrupting the communication pathway between the visual and\nlanguage systems causally affects the ability of vision-language DNNs to\nexplain the activity of the VOTC. Across four diverse datasets, CLIP\nconsistently outperformed both label-supervised (ResNet) and unsupervised\n(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,\naligning with the human language network. Analyses of the data of 33 stroke\npatients revealed that reduced white matter integrity between the VOTC and the\nlanguage region in the left angular gyrus was correlated with decreased CLIP\nperformance and increased MoCo performance, indicating a dynamic influence of\nlanguage processing on the activity of the VOTC. These findings support the\nintegration of language modulation in neurocognitive models of human vision,\nreinforcing concepts from vision-language DNN models. The sensitivity of\nmodel-brain similarity to specific brain lesions demonstrates that leveraging\nmanipulation of the human brain is a promising framework for evaluating and\ndeveloping brain-like computer models.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "We formalize a rigorous connection between barren plateaus (BP) in\nvariational quantum algorithms and exponential concentration of quantum kernels\nfor machine learning. Our results imply that recently proposed strategies to\nbuild BP-free quantum circuits can be utilized to construct useful quantum\nkernels for machine learning. This is illustrated by a numerical example\nemploying a provably BP-free quantum neural network to construct kernel\nmatrices for classification datasets of increasing dimensionality without\nexponential concentration.",
        "This paper introduces a differentiable ray-tracing based model that\nincorporates aberrations and distortions to render realistic coded\nhyperspectral acquisitions using Coded-Aperture Spectral Snapshot Imagers\n(CASSI). CASSI systems can now be optimized in order to fulfill simultaneously\nseveral optical design constraints as well as processing constraints. Four\ncomparable CASSI systems with varying degree of optical aberrations have been\ndesigned and modeled. The resulting rendered hyperspectral acquisitions from\neach of these systems are combined with five state-of-the-art hyperspectral\ncube reconstruction processes. These reconstruction processes encompass a\nmapping function created from each system's propagation model to account for\ndistortions and aberrations during the reconstruction process. Our analyses\nshow that if properly modeled, the effects of geometric distortions of the\nsystem and misalignments of the dispersive elements have a marginal impact on\nthe overall quality of the reconstructed hyperspectral data cubes. Therefore,\nrelaxing traditional constraints on measurement conformity and fidelity to the\nscene enables the development of novel imaging instruments, guided by\nperformance metrics applied to the design or the processing of acquisitions. By\nproviding a complete framework for design, simulation and evaluation, this work\ncontributes to the optimization and exploration of new CASSI systems, and more\ngenerally to the computational imaging community.",
        "Our recent numerical studies on cryogenic liquid helium-4 strongly indicate\nthe features of multiscale physics that can be identified using the Landau's\ntwo-fluid model. This study presents the possibility that two-fluid models\nbased on classical and quantum hydrodynamics have a relationship between the\nscale transformation using filtering in large eddy simulations (LES) and the\ninverse scale transformation using smoothed particle hydrodynamics (SPH). We\nshow that the spin angular momentum conservation term, which we previously\nintroduced into the two-fluid model as a quantum mechanical correction,\nformally corresponds to the subgrid-scale (SGS) model, which can be derived\nfrom the scale transformation of the two-fluid model from quantum to classical\nhydrodynamics. Our theoretical analysis shows that solving the two-fluid model\nbased on classical hydrodynamics using SPH can reproduce the microscopic\nfluctuations even at the macroscopic scale because the truncation errors owing\nto the smoothing kernel approximation can substitute the microscopic\nfluctuations. In particular, the fluctuations can be amplified according to the\nsize of the kernel radius at the macroscopic scale. Our further theoretical\nanalysis shows that the Condiff viscosity model can serve as an SGS model and\nincorporate the quantum vortex interactions into the two-fluid model. Our\nresults and discussion provide new insights into the microscopic composition of\nthe cryogenic liquid helium-4 within a multiscale framework. First, a normal\nfluid can be a mixture of inviscid and viscous fluid particles. Second, a flow\nidentified as a normal fluid on the microscopic scale because of the presence\nof molecular viscosity is still classified as an inviscid fluid on the\nhydrodynamic scale because its viscosity is insufficient to produce eddy\nviscosity.",
        "A general system of difference equations is presented for multispecies\ncommunities with density dependent population growth and delayed maturity.\nInterspecific competition, mutualism, predation, commensalism, and amensalism\nare accommodated. A sufficient condition for the local asymptotic stability of\na coexistence equilibrium in this system is then proven. Using this system, the\ngeneralisation of the Beverton-Holt and Leslie-Gower models of competition to\nmultispecies systems with possible maturation delays is presented and shown to\nyield interesting stability properties. The stability of coexistence depends on\nthe relative abundances of the species at the unique interior equilibrium. A\nsufficient condition for local stability is derived that only requires\nintraspecific competition to outweigh interspecific competition. The condition\ndoes not depend on maturation delays. The derived stability properties are used\nto develop a novel estimation approach for the coefficients of interspecific\ncompetition. This approach finds an optimal configuration given the conjecture\nthat coexisting species strive to outcompete competitors and are most likely\nobserved near the stable interior equilibrium with the greatest dampening of\nperturbations. The optimal solution is compared to estimates of niche overlap\nusing an empirical example of malaria mosquito vectors with delayed maturity in\nthe Anopheles gambiae sensu lato species complex.",
        "We introduce the light-sheet confocal quantum diamond microscope (LC-QDM) for\nwidefield 3D quantum sensing with efficient confocal readout. The LC-QDM\nleverages light-sheet illumination and laser scanning confocal methods to\nenable high-resolution, high-speed 3D measurements with nitrogen-vacancy (NV)\ndefects in diamond, combining the best of widefield and confocal modalities in\na single device and eliminating the need for thin-NV-layer diamond chips. We\nperform simulations and measurements of NV initialization and readout times to\nmodel the anticipated performance of the LC-QDM compared to existing QDM\ndesigns. Our findings show that the LC-QDM will provide significant advantages\nfor applications requiring limited laser power.",
        "We report the X-ray spectral and timing analysis of the high mass X-ray\nbinary EXO 2030+375 during the 2021 type-II outburst based on the Insight-HXMT\nobservations. Pulsations can be detected in the energy band of 1-150 keV. The\npulse profile shows energy and luminosity dependence and variability. We\nobserved transitions in the pulse profile shape during the rising and the\ndecaying phase of the outburst. The pulse fraction exhibits an anti-correlation\nwith luminosity and a non-monotonic energy dependence, with a possible dip near\n30 keV during the outburst peak. The hardness-intensity diagrams (7-10 keV\/4-7\nkeV) suggest state transitions during the early and late phases of the\noutburst. These transitions are consistent with the luminosity at which the\npulse profile shape changes occur, revealing the source reaching the critical\nluminosity and transitioning between super-critical and sub-critical accretion\nregimes. We performed the average and phase-resolved spectral analysis, where\nthe flux-resolved average spectra show a stable spectral evolution with\nluminosity. The phase-resolved spectral analysis reveals that the dependence of\nspectral parameters on the pulse phase varies with different luminosities.",
        "Superheated steam is widely employed in various energy systems, particularly\nin power plants, chemical industries, and other applications where\nhigh-temperature and high-pressure steam is essential for efficient energy\nconversion and process control. In these systems, regulation valves are crucial\ncomponents that control the flow of steam, adjusting its pressure and\ntemperature to ensure safe and efficient operation. Accurate understanding and\nprediction of temperature variations within regulation valves are essential for\noptimizing their performance and improving the overall system efficiency. This\nstudy investigates the temperature variations of superheated steam flowing\nthrough a regulation valve using computational fluid dynamics (CFD) simulations\ncombined with Proper Orthogonal Decomposition (POD) techniques. The analysis\nbegins with an examination of the internal flow field parameters, including\ntemperature and pressure, to understand the overall fluid dynamics within the\nvalve. POD is applied to reduce the dimensionality of the CFD results. Singular\nValue Decomposition (SVD) is employed to extract the dominant modes that\ncapture the key flow structures responsible for heat transfer and temperature\nfluctuations. The POD analysis reveals that the most influential modes are\nassociated with regions of high turbulence intensity and significant\ntemperature gradients, which are critical to the thermal performance of the\nsteam flow through the regulation valve. The application of POD to 3D CFD\nresults represents a novel approach, particularly for complex fluid flow models\nsuch as steam flow through regulation valves. The insights gained from this\nstudy have practical implications for the design and optimization of\ntemperature and pressure regulation valves in energy systems, providing a\ntheoretical foundation for enhancing the efficiency and reliability of these\nsystems.",
        "Ring polymer self-consistent field theory is used to calculate the critical\ntemperatures and heat capacities of an ideal Bose gas for an order of magnitude\nmore particles than previously reported. A lambda-transition indicative of\nBose-Einstein condensation is observed as expected. Using a known proof of\nspatial mode entanglement in Bose-Einstein condensates, a relationship between\nboson exchange and quantum entanglement is established. This is done without\nthe use of state vectors, since ring polymer quantum theory uses instead a\nthermal degree of freedom, sometimes called the \"imaginary time\", to map\nclassical statistical mechanics onto non-relativistic quantum mechanics through\nthe theorems of density functional theory. It is shown that quantum phenomena,\nsuch as Bose-Einstein condensation, boson exchange, entanglement and\ncontextuality, can be visualized in terms of merging and separating ring\npolymer threads in thermal-space. A possible extension to fermions is\nmentioned.",
        "This study utilizes a hybrid Finite Element Method (FEM) and Material Point\nMethod (MPM) to investigate the runout of liquefaction-induced flow slide\nfailures. The key inputs to this analysis are the earthquake ground motion,\nwhich induces liquefaction, and the post-liquefaction residual strength. The\ninfluence of these factors on runout is evaluated by subjecting a model of a\ntailings dam to thirty different earthquake motions and by assigning different\nvalues of post-liquefaction residual strength. Ground motions with larger peak\nground accelerations (PGA) generate liquefaction to larger depths, thus\nmobilizing a greater mass of material and resulting in a flow slide with\ngreater runout. However, different ground motions with the same PGA yield\nsignificant variations in the depth of liquefaction, indicating that other\nground motion characteristics (e.g., frequency content) also exert significant\ninfluence over the initiation of liquefaction. Ground motion characteristics of\npeak ground velocity (PGV) and Modified Acceleration Spectrum Intensity (MASI)\nshow a strong correlation to the induced depth of liquefaction because they\ncapture both the intensity and frequency content of the earthquake motion. The\ncomputed runout is directly related to the depth of liquefaction induced by the\nearthquake motion. For dam geometry analyzed, measurable runout occurs when\nliquefaction extends to 10 m depth and the runout is maximized when\nliquefaction extends to about 18 m. Strain-softening of the residual strength\nof the liquefied tailings during runout is shown to substantially increase the\nrunout distance of the flow slide, highlighting the need for additional\nresearch to better characterize the appropriate strength of liquefied materials\nduring flow failures.",
        "Altermagnetism has emerged as a third fundamental category of collinear\nmagnetism, characterized by spin-splitting in symmetry-compensated collinear\nantiferromagnets, opening new frontiers in spintronics and condensed matter\nphysics. Here, based on first-principles calculations, we propose a novel\naltermagnetic semiconductor, the asymmetric Janus V$_2$AsBrO monolayer, which\nexhibits a magnetic easy axis favoring the out-of-plane direction and a\nN\\'{e}el temperature ($T_N$) exceeding room temperature. The system exhibits a\nstrain-tunable piezovalley effect, generating valley polarization under\nuniaxial strain. Notably, hole doping under uniaxial strain generates a net\nmagnetization ($M$) through a piezomagnetic mechanism. Additionally, the broken\ninversion symmetry endows the monolayer with a substantial out-of-plane\npiezoelectric coefficient $d_{31}$ (2.19 pm\/V), presenting broad prospects for\nthe development and design of novel piezoelectric devices. Our findings provide\na promising candidate material for the advancement of 2D multifunctional\ndevices in nanoelectronics, spintronics, valleytronics, and piezoelectrics.",
        "A commutative monoid $M$ is called a linearly orderable monoid if there\nexists a total order on $M$ that is compatible with the monoid operation. The\nfinitary power monoid of a commutative monoid $M$ is the monoid consisting of\nall nonempty finite subsets of $M$ under the so-called sumset. In this paper,\nwe investigate whether certain atomic and divisibility properties ascend from\nlinearly orderable monoids to their corresponding finitary power monoids.",
        "We study the dynamics of the $(\\alpha+\\beta)$ Fermi-Pasta-Ulam-Tsingou\nlattice (FPUT lattice, for short) for an arbitrary number $N$ of interacting\nparticles, in regimes of small enough nonlinearity so that a Birkhoff-Gustavson\ntype of normal form can be found using tools from wave-turbulence theory.\nSpecifically, we obtain the so-called Zakharov equation for $4$-wave resonant\ninteractions and its extension to $5$-wave resonant interactions by Krasitskii,\nbut we introduce an important new feature: even the generic terms in these\nnormal forms contain $resonant$ $interactions$ $only$, via a $unique$ canonical\ntransformation. The resulting normal forms provide an approximation to the\noriginal FPUT lattice that possesses a significant number of exact quadratic\nconservation laws, beyond the quadratic part of the Hamiltonian. We call the\nnew equations \"exact-resonance evolution equations\" and examine their\nproperties: (i) Heisenberg representation's slow evolution allows us to\nimplement numerical methods with large time steps to obtain relevant dynamical\ninformation, such as Lyapunov exponents. (ii) We introduce tests, such as\nconvergence of the normal form transformation and truncation error\nverification, to successfully validate our exact-resonance evolution equations.\n(iii) The systematic construction of new quadratic invariants (via the resonant\ncluster matrix) allows us to use finite-time Lyapunov exponent calculations to\nquantify the level of nonlinearity at which the original FPUT lattice is well\napproximated by the exact-resonance evolution equations. We show numerical\nexperiments in the case $N=9$, but the theory and numerical methods are valid\nfor arbitrary values of $N$. We conclude that, when $3$ divides $N$, at small\nenough nonlinearity the FPUT lattice's dynamics and nontrivial hyperchaos are\ngoverned by $5$-wave resonant interactions.",
        "The characteristics and images of Einstein-Maxwell-Dilaton (EMD) black holes\nare examined in this paper, focusing on their effective potential, photon\ntrajectories, and images with thin and thick accretion disks. We found that the\nshadow and photon sphere radii decrease with increasing dilaton charge. As the\nobservation inclination increases, direct and secondary images become separate,\nwith the direct image appearing hat-shaped. Simulations indicate that the\nbrightness of the shadow and photon ring is higher in static spherical\naccretion flows compared to infalling ones. The study also shows that in thin\ndisk accretion flows, the direct emission predominantly influences observed\nluminosity, with photon ring emission being less significant. Additionally, the\nappearance of black hole images varies with the observer's inclination angle."
      ]
    }
  },
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer",
    "start_abstract":"Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
      ],
      "abstract":[
        "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "Evolution and The Knightian Blindspot of Machine Learning",
        "URECA: The Chain of Two Minimum Set Cover Problems exists behind\n  Adaptation to Shifts in Semantic Code Search",
        "Sensemaking in Novel Environments: How Human Cognition Can Inform\n  Artificial Agents",
        "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "AI-based Identity Fraud Detection: A Systematic Review",
        "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics",
        "AI Generations: From AI 1.0 to AI 4.0",
        "Resource Constrained Pathfinding with A* and Negative Weights",
        "A Combinatorial Identities Benchmark for Theorem Proving via Automated\n  Theorem Generation",
        "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World\n  Multimodal Mathematical Error Detection",
        "Improved Rates of Differentially Private Nonconvex-Strongly-Concave\n  Minimax Optimization",
        "Online Learning of Danger Avoidance for Complex Structures of\n  Musculoskeletal Humanoids and Its Applications",
        "Orientation-dependent transport in junctions formed by $d$-wave\n  altermagnets and $d$-wave superconductors",
        "Assessing the impacts of tradable credit schemes through agent-based\n  simulation",
        "Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting",
        "Quantum Entanglement and Measurement Noise: A Novel Approach to\n  Satellite Node Authentication",
        "CallNavi: A Study and Challenge on Function Calling Routing and\n  Invocation in Large Language Models",
        "Global existence of weak solutions to a cell migration and\n  (de)differentiation model with double haptotaxis in the context of tissue\n  regeneration",
        "An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the\n  SUPERIOT Framework",
        "Decentralized Personalization for Federated Medical Image Segmentation\n  via Gossip Contrastive Mutual Learning",
        "Using cyclic $(f,\\sigma)$-codes over finite chain rings to construct\n  $\\mathbb{Z}_p$- and $\\mathbb{F}_q[\\![t]\\!]$-lattices",
        "MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based\n  Asynchronous Aggregation",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "Utilizing API Response for Test Refinement"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "This paper claims that machine learning (ML) largely overlooks an important\nfacet of general intelligence: robustness to a qualitatively unknown future in\nan open world. Such robustness relates to Knightian uncertainty (KU) in\neconomics, i.e. uncertainty that cannot be quantified, which is excluded from\nconsideration in ML's key formalisms. This paper aims to identify this blind\nspot, argue its importance, and catalyze research into addressing it, which we\nbelieve is necessary to create truly robust open-world AI. To help illuminate\nthe blind spot, we contrast one area of ML, reinforcement learning (RL), with\nthe process of biological evolution. Despite staggering ongoing progress, RL\nstill struggles in open-world situations, often failing under unforeseen\nsituations. For example, the idea of zero-shot transferring a self-driving car\npolicy trained only in the US to the UK currently seems exceedingly ambitious.\nIn dramatic contrast, biological evolution routinely produces agents that\nthrive within an open world, sometimes even to situations that are remarkably\nout-of-distribution (e.g. invasive species; or humans, who do undertake such\nzero-shot international driving). Interestingly, evolution achieves such\nrobustness without explicit theory, formalisms, or mathematical gradients. We\nexplore the assumptions underlying RL's typical formalisms, showing how they\nlimit RL's engagement with the unknown unknowns characteristic of an\never-changing complex world. Further, we identify mechanisms through which\nevolutionary processes foster robustness to novel and unpredictable challenges,\nand discuss potential pathways to algorithmically embody them. The conclusion\nis that the intriguing remaining fragility of ML may result from blind spots in\nits formalisms, and that significant gains may result from direct confrontation\nwith the challenge of KU.",
        "Adaptation is to make model learn the patterns shifted from the training\ndistribution. In general, this adaptation is formulated as the minimum entropy\nproblem. However, the minimum entropy problem has inherent limitation --\nshifted initialization cascade phenomenon. We extend the relationship between\nthe minimum entropy problem and the minimum set cover problem via Lebesgue\nintegral. This extension reveals that internal mechanism of the minimum entropy\nproblem ignores the relationship between disentangled representations, which\nleads to shifted initialization cascade. From the analysis, we introduce a new\nclustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA).\nURECA is an efficient clustering algorithm for the leverage of the\nrelationships between disentangled representations. The update rule of URECA\ndepends on Thresholdly-Updatable Stationary Assumption to dynamics as a\nreleased version of Stationary Assumption. This assumption helps URECA to\ntransport disentangled representations with no errors based on the\nrelationships between disentangled representations. URECA also utilize\nsimulation trick to efficiently cluster disentangled representations. The wide\nrange of evaluations show that URECA achieves consistent performance gains for\nthe few-shot adaptation to diverse types of shifts along with advancement to\nState-of-The-Art performance in CoSQA in the scenario of query shift.",
        "One of the most vital cognitive skills to possess is the ability to make\nsense of objects, events, and situations in the world. In the current paper, we\noffer an approach for creating artificially intelligent agents with the\ncapacity for sensemaking in novel environments. Objectives: to present several\nkey ideas: (1) a novel unified conceptual framework for sensemaking (which\nincludes the existence of sign relations embedded within and across frames);\n(2) interaction among various content-addressable, distributed-knowledge\nstructures via shared attributes (whose net response would represent a\nsynthesized object, event, or situation serving as a sign for sensemaking in a\nnovel environment). Findings: we suggest that attributes across memories can be\nshared and recombined in novel ways to create synthesized signs, which can\ndenote certain outcomes in novel environments (i.e., sensemaking).",
        "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "With the rapid development of digital services, a large volume of personally\nidentifiable information (PII) is stored online and is subject to cyberattacks\nsuch as Identity fraud. Most recently, the use of Artificial Intelligence (AI)\nenabled deep fake technologies has significantly increased the complexity of\nidentity fraud. Fraudsters may use these technologies to create highly\nsophisticated counterfeit personal identification documents, photos and videos.\nThese advancements in the identity fraud landscape pose challenges for identity\nfraud detection and society at large. There is a pressing need to review and\nunderstand identity fraud detection methods, their limitations and potential\nsolutions. This research aims to address this important need by using the\nwell-known systematic literature review method. This paper reviewed a selected\nset of 43 papers across 4 major academic literature databases. In particular,\nthe review results highlight the two types of identity fraud prevention and\ndetection methods, in-depth and open challenges. The results were also\nconsolidated into a taxonomy of AI-based identity fraud detection and\nprevention methods including key insights and trends. Overall, this paper\nprovides a foundational knowledge base to researchers and practitioners for\nfurther research and development in this important area of digital identity\nfraud.",
        "Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences.",
        "This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
        "Constrained pathfinding is a well-studied, yet challenging network\noptimisation problem that can be seen in a broad range of real-world\napplications. Pathfinding with multiple resource limits, which is known as the\nResource Constrained Shortest Path Problem (RCSP), aims to plan a cost-optimum\npath subject to limited usage of resources. Given the recent advances in\nconstrained and multi-criteria search with A*, this paper introduces a new\nresource constrained search framework on the basis of A* to tackle RCSP in\nlarge networks, even in the presence of negative cost and negative resources.\nWe empirically evaluate our new algorithm on a set of large instances and show\nup to two orders of magnitude faster performance compared to state-of-the-art\nRCSP algorithms in the literature.",
        "Large language models (LLMs) have significantly advanced formal theorem\nproving, yet the scarcity of high-quality training data constrains their\ncapabilities in complex mathematical domains. Combinatorics, a cornerstone of\nmathematics, provides essential tools for analyzing discrete structures and\nsolving optimization problems. However, its inherent complexity makes it\nparticularly challenging for automated theorem proving (ATP) for combinatorial\nidentities. To address this, we manually construct LeanComb, combinatorial\nidentities benchmark in Lean, which is, to our knowledge, the first formalized\ntheorem proving benchmark built for combinatorial identities. We develop an\nAutomated Theorem Generator for Combinatorial Identities, ATG4CI, which\ncombines candidate tactics suggested by a self-improving large language model\nwith a Reinforcement Learning Tree Search approach for tactic prediction. By\nutilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K\ncombinatorial identities theorems, each with a complete formal proof in Lean,\nand experimental evaluations demonstrate that models trained on this dataset\ncan generate more effective tactics, thereby improving success rates in\nautomated theorem proving for combinatorial identities.",
        "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection.",
        "In this paper, we study the problem of (finite sum) minimax optimization in\nthe Differential Privacy (DP) model. Unlike most of the previous studies on the\n(strongly) convex-concave settings or loss functions satisfying the\nPolyak-Lojasiewicz condition, here we mainly focus on the\nnonconvex-strongly-concave one, which encapsulates many models in deep learning\nsuch as deep AUC maximization. Specifically, we first analyze a DP version of\nStochastic Gradient Descent Ascent (SGDA) and show that it is possible to get a\nDP estimator whose $l_2$-norm of the gradient for the empirical risk function\nis upper bounded by $\\tilde{O}(\\frac{d^{1\/4}}{({n\\epsilon})^{1\/2}})$, where $d$\nis the model dimension and $n$ is the sample size. We then propose a new method\nwith less gradient noise variance and improve the upper bound to\n$\\tilde{O}(\\frac{d^{1\/3}}{(n\\epsilon)^{2\/3}})$, which matches the best-known\nresult for DP Empirical Risk Minimization with non-convex loss. We also\ndiscussed several lower bounds of private minimax optimization. Finally,\nexperiments on AUC maximization, generative adversarial networks, and temporal\ndifference learning with real-world data support our theoretical analysis.",
        "The complex structure of musculoskeletal humanoids makes it difficult to\nmodel them, and the inter-body interference and high internal muscle force are\nunavoidable. Although various safety mechanisms have been developed to solve\nthis problem, it is important not only to deal with the dangers when they occur\nbut also to prevent them from happening. In this study, we propose a method to\nlearn a network outputting danger probability corresponding to the muscle\nlength online so that the robot can gradually prevent dangers from occurring.\nApplications of this network for control are also described. The method is\napplied to the musculoskeletal humanoid, Musashi, and its effectiveness is\nverified.",
        "We investigate de Gennes-Saint-James states and Josephson effect in hybrid\njunctions based on $d$-wave altermagnet and $d$-wave superconductor. Even\nthough these states are associated to long junctions, we find that the\n$d_{x^{2}-y^{2}}$-altermagnet in a normal metal\/altermagnet\/$d$-wave\nsuperconductor junction forms de Gennes-Saint-James states in a short junction\ndue to an enhanced mismatch between electron and hole wave vectors. As a\nresult, the zero-bias conductance peak vanishes and pronounced resonance spikes\nemerge in the subgap conductance spectra. By contrast, the $d_{xy}$-altermagnet\nonly features de Gennes-Saint-James states in the long junction. Moreover, the\nwell-known features such as V-shape conductance for $d_{x^2-y^2}$ pairings and\nzero-biased conductance peak for $d_{xy}$ pairings are not affected by the\nstrength of $d_{xy}$-altermagnetism in the short junction. We also study the\nJosephson current-phase relation $I(\\varphi)$ of $d$-wave\nsuperconductor\/altermagnet\/$d$-wave superconductor hybrids, where $\\varphi$ is\nthe macroscopic phase difference between two $d$-wave superconductors. In\nsymmetric junctions, we obtain anomalous current phase relation such as a\n$0$-$\\pi$ transition by changing either the orientation or the magnitude of the\naltermagnetic order parameter and dominant higher Josephson harmonics.\nInterestingly, we find the first-order Josephson coupling in an asymmetric\n$d_{x^{2}-y^{2}}$-superconductor\/altermagnet\/$d_{xy}$-superconductor junction\nwhen the symmetry of altermagnetic order parameter is neither\n$d_{x^{2}-y^{2}}$- nor $d_{xy}$-wave. We present the symmetry analysis and\nconclude that the anomalous orientation-dependent current-phase relations are\nascribed to the peculiar feature of the altermagnetic spin-splitting field.",
        "Tradable credit schemes (TCS) have been attracting interest from the\ntransportation research community as an appealing alternative to congestion\npricing, due to the advantages of revenue neutrality and equity. Nonetheless,\nexisting research has largely employed network and market equilibrium\napproaches with simplistic characterizations of transportation demand, supply,\ncredit market operations, and market behavior. Agent- and activity-based\nsimulation affords a natural means to comprehensively assess TCS by more\nrealistically modeling demand, supply, and individual market interactions. We\npropose an integrated simulation framework for modeling a TCS, and implements\nit within the state-of-the-art open-source urban simulation platform\nSimMobility, including: (a) a flexible TCS design that considers multiple trips\nand explicitly accounts for individual trading behaviors; (b) a simulation\nframework that captures the complex interactions between a TCS regulator, the\ntraveler, and the TCS market itself, with the flexibility to test future TCS\ndesigns and relevant mobility models; and (c) a set of simulation experiments\non a large mesoscopic multimodal network combined with a Bayesian Optimization\napproach for TCS optimal design. The experiment results indicate network and\nmarket performance to stabilize over the day-to-day process, showing the\nalignment of our agent-based simulation with the known theoretical properties\nof TCS. We confirm the efficiency of TCS in reducing congestion under the\nadopted market behavioral assumptions and open the door for simulating\ndifferent individual behaviors. We measure how TCS impacts differently the\nlocal network, heterogeneous users, the different travel behaviors, and how\ntesting different TCS designs can avoid negative market trading behaviors.",
        "Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.",
        "In this paper, we introduce a novel authentication scheme for satellite nodes\nbased on quantum entanglement and measurement noise profiles. Our approach\nleverages the unique noise characteristics exhibited by each satellite's\nquantum optical communication system to create a distinctive \"quantum noise\nfingerprint.\" This fingerprint is used for node authentication within a\nsatellite constellation, offering a quantum-safe alternative to traditional\ncryptographic methods. The proposed scheme consists of a training phase, where\neach satellite engages in a training exercise with its neighbors to compile\nnoise profiles, and an online authentication phase, where these profiles are\nused for real-time authentication. Our method addresses the inherent challenges\nof implementing cryptographic-based schemes in space, such as key management\nand distribution, by exploiting the fundamental properties of quantum mechanics\nand the unavoidable imperfections in quantum systems. This approach enhances\nthe security and reliability of satellite communication networks, providing a\nrobust solution to the authentication challenges in satellite constellations.\nWe validated and tested several hypotheses for this approach using IBM System\nOne quantum computers.",
        "Interacting with a software system via a chatbot can be challenging,\nespecially when the chatbot needs to generate API calls, in the right order and\nwith the right parameters, to communicate with the system. API calling in\nchatbot systems poses significant challenges, particularly in complex,\nmulti-step tasks requiring accurate API selection and execution. We contribute\nto this domain in three ways: first, by introducing a novel dataset designed to\nassess models on API function selection, parameter generation, and nested API\ncalls; second, by benchmarking state-of-the-art language models across varying\nlevels of complexity to evaluate their performance in API function generation\nand parameter accuracy; and third, by proposing an enhanced API routing method\nthat combines general-purpose large language models for API selection with\nfine-tuned models for parameter generation and some prompt engineering\napproach. These approaches lead to substantial improvements in handling complex\nAPI tasks, offering practical advancements for real-world API-driven chatbot\nsystems.",
        "We study a model for the spread and (de)differentiation of mesenchymal stem\ncells and chondrocytes in a scaffold whose fibers are coated with hyaluron. The\nchondrocytes produce new extracellular matrix, which, together with hyaluron,\nserves as haptotactic cue for the stem cell migration. We prove global\nexistence of weak solutions of the corresponding cross-diffusion system with\ndouble haptotaxis.",
        "This paper presents a comprehensive analysis of the energy consumption\ncharacteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node\ndeveloped in the initial phase of the SUPERIOT project, focusing on key\noperating states, including Bluetooth Low Energy (BLE) communication,\nNarrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.\nExtensive measurements were conducted to establish a detailed energy profile,\nwhich serves as a benchmark for evaluating the effectiveness of subsequent\noptimizations and future node iterations. To minimize the energy consumption,\nmultiple optimizations were implemented at both the software and hardware\nlevels, achieving a reduction of over 60% in total energy usage through\nsoftware modifications alone. Further improvements were realized by optimizing\nthe E-ink display driving waveform and implementing a very low-power mode for\nnon-communication activities. Based on the measured data, three\nmeasurement-based energy consumption models were developed to characterize the\nenergy behavior of the node under: (i) normal, unoptimized operation, (ii)\nlow-power, software-optimized operation, and (iii) very low-power,\nhardware-optimized operation. These models, validated with new measurement\ndata, achieved an accuracy exceeding 97%, confirming their reliability for\npredicting energy consumption in diverse configurations.",
        "Federated Learning (FL) presents a promising avenue for collaborative model\ntraining among medical centers, facilitating knowledge exchange without\ncompromising data privacy. However, vanilla FL is prone to server failures and\nrarely achieves optimal performance on all participating sites due to\nheterogeneous data distributions among them. To overcome these challenges, we\npropose Gossip Contrastive Mutual Learning (GCML), a unified framework to\noptimize personalized models in a decentralized environment, where Gossip\nProtocol is employed for flexible and robust peer-to-peer communication. To\nmake efficient and reliable knowledge exchange in each communication without\nthe global knowledge across all the sites, we introduce deep contrast mutual\nlearning (DCML), a simple yet effective scheme to encourage knowledge transfer\nbetween the incoming and local models through collaborative training on local\ndata. By integrating DCML with other efforts to optimize site-specific models\nby leveraging useful information from peers, we evaluated the performance and\nefficiency of the proposed method on three publicly available datasets with\ndifferent segmentation tasks. Our extensive experimental results show that the\nproposed GCML framework outperformed both centralized and decentralized FL\nmethods with significantly reduced communication overhead, indicating its\npotential for real-world deployment.",
        "We construct $\\mathbb{Z}_p$-lattices and $\\mathbb{F}_q[\\![t]\\!]$-lattices\nfrom cyclic $(f,\\sigma)$-codes over finite chain rings, employing quotients of\nnatural nonassociative orders and principal left ideals in carefully chosen\nnonassociative algebras. This approach generalizes the classical Construction A\nthat obtains $\\mathbb{Z}$-lattices from linear codes over finite fields or\ncommutative rings to the nonassociative setting. We mostly use proper\nnonassociative cyclic algebras that are defined over field extensions of\n$p$-adic fields. This means we focus on $\\sigma$-constacyclic codes to obtain\n$\\mathbb{Z}_p$-lattices, hence $\\mathbb{Z}_p$-lattice codes. We construct\nlinear maximum rank distance (MRD) codes that are $\\mathbb{Z}_p$-lattice codes\nemploying the left multiplication of a nonassociative algebra over a finite\nchain ring.\n  Possible applications of our constructions include post-quantum cryptography\ninvolving $p$-adic lattices, e.g. learning with errors, building rank-metric\ncodes like MRD-codes, or $p$-adic coset coding, in particular wire-tap coding.",
        "Graph neural networks (GNNs) have become the state of the art for various\ngraph-related tasks and are particularly prominent in heterogeneous graphs\n(HetGs). However, several issues plague this paradigm: first, the difficulty in\nfully utilizing long-range information, known as over-squashing; second, the\ntendency for excessive message-passing layers to produce indistinguishable\nrepresentations, referred to as over-smoothing; and finally, the inadequacy of\nconventional MPNNs to train effectively on large sparse graphs. To address\nthese challenges in deep neural networks for large-scale heterogeneous graphs,\nthis paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),\nwhich enhances the representation of heterogeneous sparse graphs. MAPN consists\nof two primary components: node sequence generation and semantic information\naggregation. Node sequences are initially generated based on meta-paths through\nrandom walks, which serve as the foundation for a spatial state model that\nextracts essential information from nodes at various distances. It then\nasynchronously aggregates semantic information across multiple hops and layers,\neffectively preserving unique node characteristics and mitigating issues\nrelated to deep network degradation. Extensive experiments across diverse\ndatasets demonstrate the effectiveness of MAPN in graph embeddings for various\ndownstream tasks underscoring its substantial benefits for graph representation\nin large sparse heterogeneous graphs.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Artificial Intelligence for Pediatric Ophthalmology",
    "start_abstract":"PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
      ],
      "abstract":[
        "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Inferring collective synchrony observing spiking of one or several\n  neurons",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Solutions of first passage times problems: a biscaling approach",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Incident beam optics optimization for the single crystal neutron\n  diffractometer Pioneer with a polarized beam option",
        "Enhanced Field-Free Perpendicular Magnetization Switching via spin\n  splitting torque in Altermagnetic RuO2-based Heterostructures",
        "Electromagnetic Radiation from High-Energy Nuclear Collisions",
        "First X-ray polarimetric view of a Low-Luminosity Active Galactic\n  Nucleus: the case of NGC 2110",
        "Classifier Weighted Mixture models",
        "Formation of condensations for non-radial solutions to 3-wave kinetic\n  equations",
        "Shifting Attention to You: Personalized Brain-Inspired AI Models",
        "Can wormholes mirror the quasi-normal mode spectrum of Schwarzschild\n  black holes?",
        "The influence of the Hardy potential and a Convection Term on a\n  Nonlinear Degenerate Elliptic Equations",
        "Transit Timing Variations of the Sub-Saturn Exoplanet HAT-P-12b",
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Bayesian Computation in Deep Learning",
        "Anomalies in the electronic, magnetic and thermal behavior near the\n  Invar compositions of Fe-Ni alloys"
      ],
      "abstract":[
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "We study the first-passage time (FPT) problem for widespread recurrent\nprocesses in confined though large systems and present a comprehensive\nframework for characterizing the FPT distribution over many time scales. We\nfind that the FPT statistics can be described by two scaling functions: one\ncorresponds to the solution for an infinite system, and the other describes a\nscaling that depends on system size. We find a universal scaling relationship\nfor the FPT moments $\\langle t^q \\rangle$ with respect to the domain size and\nthe source-target distance. This scaling exhibits a transition at $q_c=\\theta$,\nwhere $\\theta$ is the persistence exponent. For low-order moments with $q<q_c$,\nconvergence occurs towards the moments of an infinite system. In contrast, the\nhigh-order moments, $q>q_c$, can be derived from an infinite density function.\nThe presented uniform approximation, connecting the two scaling functions,\nprovides a description of the first-passage time statistics across all time\nscales. We extend the results to include diffusion in a confining potential in\nthe high-temperature limit, where the potential strength takes the place of the\nsystem's size as the relevant scale. This study has been applied to various\nmediums, including a particle in a box, two-dimensional wedge, fractal\ngeometries, non-Markovian processes and the non-equilibrium process of\nresetting.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Pioneer, a next-generation single-crystal neutron diffractometer, is under\ndevelopment for Oak Ridge National Laboratory's Second Target Station (STS).\nDesigned to address a wide range of scientific questions, Pioneer will deliver\nhomogeneous neutron beams with customizable size and divergence, and provide a\npolarized beam option. This article introduces its incident beam optics,\nhighlighting the optimization methodology and the simulated performance.\nPioneer will utilize a modified elliptical-straight guide for neutron transport\nand deploy slit packages and insertable apertures to control beam size and\ndivergence. The optimized guide geometry matches the\noptimal-and-full-sample-illumination condition, and the beam control system\neffectively filters out unwanted neutrons while preserving the desired ones.\nAdditionally, we have found that polygon-approximated guides provide\nsatisfactory transport efficiency and beam homogeneity, eliminating the need of\ntruly curved guides. To enhance neutronics performance and reduce cost, the\ncoatings of supermirror elements are individually optimized to the lowest\nhalf-integer $m$-values that are sufficient to deliver the desired neutrons.\nAfter evaluating polarizing V-cavities and $^3$He spin filters over the default\npolarized wavelength band of 1.2-5.5~\\AA, we selected a translatable\nmultichannel polarizing V-cavity as the incident beam polarizer. Strategically\nplaced at a location where the beam divergence is low and a large in-guide gap\nhas negligible impact on transport efficiency, the optimized V-cavity achieves\nan average $P^2T$ of approximately 35\\%.",
        "Current-induced spin-orbit torque (SOT) has emerged as a promising method for\nachieving energy-efficient magnetization switching in advanced spintronic\ndevices. However, technological advancement has been inadequate because an\nexternal in-plane magnetic field is required to attain deterministic switching.\nSeveral approaches have been explored to address these challenges. In this\nwork, we explored the potential of a newly emerged altermagnetic material RuO2\nin combination with a Pt layer to achieve both field-free and low-power\nswitching concurrently. We leveraged out-of-plane (OOP) spin polarization via\nthe spin-splitter effect (SSE) in RuO2 for field-free switching (FFS) and\nin-plane spin polarization combined with spin Hall effect (SHE) in Pt for\nenhanced SOT efficiency. We revealed that the effective OOP magnetic field and\nFFS can be maximized by tuning the nominal thickness of the Pt underlayer and\nthe direction of the applied current. We observed a significant enhancement in\nFFS at an optimized Pt thickness of 1.5 nm for an applied current density as\nlow as 2.56e11 A\/m2 at a crystal angle of 90 deg. Our study paves the way for\nenergy-efficient spintronics devices for non-volatile memory, logic circuits,\nand neuromorphic computing.",
        "We highlight some of the developments in the theory and the observation of\nthe electromagnetic radiation, thermal and otherwise, emitted in relativistic\nheavy-ion collisions.",
        "Low-Luminosity Active Galactic Nuclei (LLAGN) provides a unique view of\nComptonization and non-thermal emission from accreting black holes in the\nlow-accretion rate regime. However, to decipher the exact nature of the\nComptonizing corona in LLAGN, its geometry and emission mechanism must be\nunderstood beyond the limits of spectro-timing techniques. Spectro-polarimetry\noffers the potential to break the degeneracies between different coronal\nemission models. Compton-thin LLAGN provide an opportunity for such\nspectro-polarimetric exploration in the 2-8 keV energy range using IXPE. In\nthis work, we carry out a spectro-polarimetric analysis of the first IXPE\nobservation, in synergy with a contemporaneous NuSTAR observation, of an LLAGN:\nNGC 2110. Using 554.4 ks of IXPE data from October 2024, we constrain the 99%\nupper limit on the Polarization Degree (PD) to be less than 8.3% assuming the\ncorresponding Polarization Angle (PA) to be aligned with the radio jet, and\nless than 3.6% if in the perpendicular direction. In the absence of a\nsignificant PD detection, the PA remains formally unconstrained, yet the\npolarization significance contours appear to be aligned with the radio jet,\ntentatively supporting models in which the corona is radially extended in the\nplane of the disk. We also carry out detailed Monte Carlo simulations using\nMONK and STOKES codes to test different coronal models against our results and\ncompare the polarization properties between NGC 2110 and brighter Seyferts.",
        "This paper proposes an extension of standard mixture stochastic models, by\nreplacing the constant mixture weights with functional weights defined using a\nclassifier. Classifier Weighted Mixtures enable straightforward density\nevaluation, explicit sampling, and enhanced expressivity in variational\nestimation problems, without increasing the number of components nor the\ncomplexity of the mixture components.",
        "We consider in this work a $2$-dimensional $3$-wave kinetic equation\ndescribing the dynamics of the thermal cloud outside a Bose-Einstein\nCondensate. We construct global non-radial mild solutions for the equation.\nThose mild solutions are the summation of Dirac masses on circles. We prove\nthat in each spatial direction, either Dirac masses at the origin, which are\nthe so-called Bose-Einstein condensates, can be formed in finite time or the\nsolutions converge to Bose-Einstein condensates as time evolves to infinity. We\nalso describe a dynamics of the formation of the Bose-Einstein condensates\nlatter case. In this case, on each direction, the solutions accumulate around\ncircles close to the origin at growth rates at least linearly in time.",
        "The integration of human and artificial intelligence represents a scientific\nopportunity to advance our understanding of information processing, as each\nsystem offers unique computational insights that can enhance and inform the\nother. The synthesis of human cognitive principles with artificial intelligence\nhas the potential to produce more interpretable and functionally aligned\ncomputational models, while simultaneously providing a formal framework for\ninvestigating the neural mechanisms underlying perception, learning, and\ndecision-making through systematic model comparisons and representational\nanalyses. In this study, we introduce personalized brain-inspired modeling that\nintegrates human behavioral embeddings and neural data to align with cognitive\nprocesses. We took a stepwise approach, fine-tuning the Contrastive\nLanguage-Image Pre-training (CLIP) model with large-scale behavioral decisions,\ngroup-level neural data, and finally, participant-level neural data within a\nbroader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We\nfound that fine-tuning on behavioral data enhances its ability to predict human\nsimilarity judgments while indirectly aligning it with dynamic representations\ncaptured via MEG. To further gain mechanistic insights into the temporal\nevolution of cognitive processes, we introduced a model specifically fine-tuned\non millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in\nenhanced temporal alignment with human neural processing while still showing\nimprovement on behavioral alignment. Finally, we trained individualized models\non participant-specific neural data, effectively capturing individualized\nneural dynamics and highlighting the potential for personalized AI systems.\nThese personalized systems have far-reaching implications for the fields of\nmedicine, cognitive research, human-computer interfaces, and AI development.",
        "Wormholes are exotic compact objects characterized by the absence of\nessential singularities and horizons, acting as slender bridges linking two\ndistinct regions of spacetime. Despite their theoretical significance, they\nremain however undetected, possibly due to their ability to closely mimic the\nobservational properties of black holes. This study explores whether a static\nand spherically symmetric wormhole within General Relativity can reproduce the\nquasi-normal mode spectrum of a Schwarzschild black hole under scalar,\nelectromagnetic, and axial gravitational perturbations, both individually and\nin combination. To address this, we reformulate the wormhole metric components\nusing a near-throat parametrization. Our analysis concentrates on the\nfundamental mode and first overtone, estimated via the\nWentzel-Kramers-Brillouin method. By employing a customized minimization\nstrategy, we demonstrate that within a specific region of the parameter space,\na wormhole can successfully replicate a subset of the black hole quasi-normal\nmode spectrum.",
        "This paper is devoted to prove existence of renormalized solutions for a\nclass of non--linear degenerate elliptic equations involving a non--linear\nconvection term, which satisfies a growth properties, and a Hardy potential.\nAdditionally, we assume that the right-hand side is an $L^m$ function, with\n$m\\geq 1$.",
        "We present Transit Timing Variations (TTVs) of HAT-P-12b, a low-density\nsub-Saturn mass planet orbiting a metal-poor K4 dwarf star. Using 14 years of\nobservational data (2009-2022), our study incorporates 7 new ground-based\nphotometric transit observations, three sectors of Transiting Exoplanet Survey\nSatellite (TESS) data, and 23 previously published light curves. A total of 46\nlight curves were analyzed using various analytical models, such as linear,\norbital decay, apsidal precession, and sinusoidal models to investigate the\npresence of additional planets. The stellar tidal quality factor ($Q_\\star'\n\\sim$ 28.4) is lower than the theoretical predictions, making the orbital decay\nmodel an unlikely explanation. The apsidal precession model with a $\\chi_r^2$\nof 4.2 revealed a slight orbital eccentricity (e = 0.0013) and a precession\nrate of 0.0045 rad\/epoch. Frequency analysis using the Generalized Lomb-Scargle\n(GLS) periodogram identified a significant periodic signal at 0.00415\ncycles\/day (FAP = 5.1$\\times$10$^{-6}$ %), suggesting the influence of an\nadditional planetary companion. The sinusoidal model provides the lowest\nreduced chi-squared value ($\\chi_r^2$) of 3.2. Sinusoidal fitting of the timing\nresiduals estimated this companion to have a mass of approximately 0.02 $M_J$ ,\nassuming it is in a 2:1 Mean-Motion Resonance (MMR) with HAT-P-12b.\nAdditionally, the Applegate mechanism, with an amplitude much smaller than the\nobserved TTV amplitude of 156 s, confirms that stellar activity is not\nresponsible for the observed variations.",
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions.",
        "The structural and magnetic properties of Fe$_{1-x}$Ni$_x$~($x$ = 0.32, 0.36,\n0.40, 0.50) alloys have been investigated using synchrotron based x-ray\ndiffraction (XRD) technique with x-rays of wavelength 0.63658 \\AA\\ down to 50 K\ntemperature, magnetic measurement using superconducting quantum interference\ndevice (SQUID) magnetometer and high resolution x-ray photoelectron\nspectroscopy (XPS) with monochromatic AlK$_\\alpha$ radiation. The XRD studies\nsuggest a single phase with fcc structure for $x$ = 0.36, 0.40, and 0.50\n~alloys and a mixed phase for $x$ = 0.32 alloy containing both bcc and fcc\nstructures. The lattice parameter of the alloys exhibits a linear dependence on\ntemperature giving rise to a temperature independent coefficient of thermal\nexpansion (CTE). The lowest CTE is observed for $x$ = 0.36 Invar alloy as\nexpected while $x$ = 0.50 alloy exhibits the highest CTE among the alloys\nstudied. The CTE of the fcc component of mixed phase alloy is close to that of\nInvar alloy. The temperature dependence of magnetization of the alloys down to\n2 K reveals an overall antiferromagnetic interactions within the ferromagnetic\nphase causing the magnetization decreasing with cooling. The field cooled and\nzero field cooled data show larger differences for the Invar compositions; this\nis also manifested in the magnetic hysteresis data at 2 K and 300 K."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm",
    "start_abstract":"In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Artificial Intelligence for Pediatric Ophthalmology"
      ],
      "abstract":[
        "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety\n  Detection",
        "SPPD: Self-training with Process Preference Learning Using Dynamic Value\n  Margin",
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Building Knowledge Graphs Towards a Global Food Systems Datahub",
        "Text Semantics to Flexible Design: A Residential Layout Generation\n  Method Based on Stable Diffusion Model",
        "Personalizing Education through an Adaptive LMS with Integrated LLMs",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Observer-Aware Probabilistic Planning Under Partial Observability",
        "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue",
        "ECLAIR: Enhanced Clarification for Interactive Responses",
        "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
        "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline\n  for Code Generation",
        "Decision Information Meets Large Language Models: The Future of\n  Explainable Operations Research",
        "Compressed Image Generation with Denoising Diffusion Codebook Models",
        "Juggling with Tensor Bases in Functional Approaches",
        "A Comprehensive Search for Leptoquarks Decaying into Top-$\\tau$ Final\n  States at the Future LHC",
        "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D\n  CT Image Synthesis",
        "Measuring Diversity in Synthetic Datasets",
        "Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to\n  Adversarial Attacks",
        "A Parareal in time numerical method for the collisional Vlasov equation\n  in the hyperbolic scaling",
        "POSMAC: Powering Up In-Network AR\/CG Traffic Classification with Online\n  Learning",
        "Evaluating Stenosis Detection with Grounding DINO, YOLO, and DINO-DETR",
        "Quantum superposition of boundary condition in $\\mathrm{PAdS}_2$",
        "Towards Automated Fact-Checking of Real-World Claims: Exploring Task\n  Formulation and Assessment with LLMs",
        "AUTOFRAME -- A Software-driven Integration Framework for Automotive\n  Systems",
        "Multi-Fidelity Policy Gradient Algorithms",
        "Structural Damping Identification Sensitivity in Flutter Speed\n  Estimation",
        "Comment on \"QCD factorization with multihadron fragmentation functions\""
      ],
      "abstract":[
        "The rapid advancements in Large Language Models (LLMs) have enabled their\ndeployment as autonomous agents for handling complex tasks in dynamic\nenvironments. These LLMs demonstrate strong problem-solving capabilities and\nadaptability to multifaceted scenarios. However, their use as agents also\nintroduces significant risks, including task-specific risks, which are\nidentified by the agent administrator based on the specific task requirements\nand constraints, and systemic risks, which stem from vulnerabilities in their\ndesign or interactions, potentially compromising confidentiality, integrity, or\navailability (CIA) of information and triggering security risks. Existing\ndefense agencies fail to adaptively and effectively mitigate these risks. In\nthis paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent\nsafety, which features adaptive safety check generation, effective safety check\noptimization, and tool compatibility and flexibility. Extensive experiments\ndemonstrate that AGrail not only achieves strong performance against\ntask-specific and system risks but also exhibits transferability across\ndifferent LLM agents' tasks.",
        "Recently, enhancing the numerical and logical reasoning capability of Large\nLanguage Models (LLMs) has emerged as a research hotspot. Existing methods face\nseveral limitations: inference-phase techniques (e.g., Chain of Thoughts) rely\non prompt selection and the pretrained knowledge; sentence-level Supervised\nFine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with\nstep-wise mathematical correctness and depend on stronger models distillation\nor human annotations; while Reinforcement Learning (RL) approaches incur high\nGPU memory costs and unstable training. To address these, we propose\n\\textbf{S}elf-training framework integrating \\textbf{P}rocess\n\\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPD\nleverages a process-based Markov Decision Process (MDP) and Bellman optimality\nequation to derive \\textbf{dynamic value margin} on step-level preference\noptimization, which employs tree-based self-sampling on model responses\n\\textbf{without any distillation} from other models. Furthermore, we\ntheoretically prove that SPPD is \\textbf{equivalent to on-policy policy\ngradient methods} under reward constraints. Experiments on 7B-scale models\ndemonstrate superior performance across in-domain and out-domain mathematical\nbenchmarks. We open-source our code at\n\\href{https:\/\/anonymous.4open.science\/r\/SSDPO-D-DCDD}{https:\/\/anonymous.4open.science\/r\/SPPD-DCDD}.",
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "Sustainable agricultural production aligns with several sustainability goals\nestablished by the United Nations (UN). However, there is a lack of studies\nthat comprehensively examine sustainable agricultural practices across various\nproducts and production methods. Such research could provide valuable insights\ninto the diverse factors influencing the sustainability of specific crops and\nproduce while also identifying practices and conditions that are universally\napplicable to all forms of agricultural production. While this research might\nhelp us better understand sustainability, the community would still need a\nconsistent set of vocabularies. These consistent vocabularies, which represent\nthe underlying datasets, can then be stored in a global food systems datahub.\nThe standardized vocabularies might help encode important information for\nfurther statistical analyses and AI\/ML approaches in the datasets, resulting in\nthe research targeting sustainable agricultural production. A structured method\nof representing information in sustainability, especially for wheat production,\nis currently unavailable. In an attempt to address this gap, we are building a\nset of ontologies and Knowledge Graphs (KGs) that encode knowledge associated\nwith sustainable wheat production using formal logic. The data for this set of\nknowledge graphs are collected from public data sources, experimental results\ncollected at our experiments at Kansas State University, and a Sustainability\nWorkshop that we organized earlier in the year, which helped us collect input\nfrom different stakeholders throughout the value chain of wheat. The modeling\nof the ontology (i.e., the schema) for the Knowledge Graph has been in progress\nwith the help of our domain experts, following a modular structure using KNARM\nmethodology. In this paper, we will present our preliminary results and schemas\nof our Knowledge Graph and ontologies.",
        "Flexibility in the AI-based residential layout design remains a significant\nchallenge, as traditional methods like rule-based heuristics and graph-based\ngeneration often lack flexibility and require substantial design knowledge from\nusers. To address these limitations, we propose a cross-modal design approach\nbased on the Stable Diffusion model for generating flexible residential\nlayouts. The method offers multiple input types for learning objectives,\nallowing users to specify both boundaries and layouts. It incorporates natural\nlanguage as design constraints and introduces ControlNet to enable stable\nlayout generation through two distinct pathways. We also present a scheme that\nencapsulates design expertise within a knowledge graph and translates it into\nnatural language, providing an interpretable representation of design\nknowledge. This comprehensibility and diversity of input options enable\nprofessionals and non-professionals to directly express design requirements,\nenhancing flexibility and controllability. Finally, experiments verify the\nflexibility of the proposed methods under multimodal constraints better than\nstate-of-the-art models, even when specific semantic information about room\nareas or connections is incomplete.",
        "The widespread adoption of large language models (LLMs) marks a\ntransformative era in technology, especially within the educational sector.\nThis paper explores the integration of LLMs within learning management systems\n(LMSs) to develop an adaptive learning management system (ALMS) personalized\nfor individual learners across various educational stages. Traditional LMSs,\nwhile facilitating the distribution of educational materials, fall short in\naddressing the nuanced needs of diverse student populations, particularly in\nsettings with limited instructor availability. Our proposed system leverages\nthe flexibility of AI to provide a customizable learning environment that\nadjusts to each user's evolving needs. By integrating a suite of\ngeneral-purpose and domain-specific LLMs, this system aims to minimize common\nissues such as factual inaccuracies and outdated information, characteristic of\ngeneral LLMs like OpenAI's ChatGPT. This paper details the development of an\nALMS that not only addresses privacy concerns and the limitations of existing\neducational tools but also enhances the learning experience by maintaining\nengagement through personalized educational content.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "In this article, we are interested in planning problems where the agent is\naware of the presence of an observer, and where this observer is in a partial\nobservability situation. The agent has to choose its strategy so as to optimize\nthe information transmitted by observations. Building on observer-aware Markov\ndecision processes (OAMDPs), we propose a framework to handle this type of\nproblems and thus formalize properties such as legibility, explicability and\npredictability. This extension of OAMDPs to partial observability can not only\nhandle more realistic problems, but also permits considering dynamic hidden\nvariables of interest. These dynamic target variables allow, for instance,\nworking with predictability, or with legibility problems where the goal might\nchange during execution. We discuss theoretical properties of PO-OAMDPs and,\nexperimenting with benchmark problems, we analyze HSVI's convergence behavior\nwith dedicated initializations and study the resulting strategies.",
        "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics.",
        "We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel\nunified and end-to-end framework for interactive disambiguation in enterprise\nAI assistants. ECLAIR generates clarification questions for ambiguous user\nqueries and resolves ambiguity based on the user's response.We introduce a\ngeneralized architecture capable of integrating ambiguity information from\nmultiple downstream agents, enhancing context-awareness in resolving\nambiguities and allowing enterprise specific definition of agents. We further\ndefine agents within our system that provide domain-specific grounding\ninformation. We conduct experiments comparing ECLAIR to few-shot prompting\ntechniques and demonstrate ECLAIR's superior performance in clarification\nquestion generation and ambiguity resolution.",
        "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
        "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, particularly in system 1 tasks, yet the intricacies of their\nproblem-solving mechanisms in system 2 tasks are not sufficiently explored.\nRecent research on System2-to-System1 methods surge, exploring the System 2\nreasoning knowledge via inference-time computation and compressing the explored\nknowledge into System 1 process. In this paper, we focus on code generation,\nwhich is a representative System 2 task, and identify two primary challenges:\n(1) the complex hidden reasoning processes and (2) the heterogeneous data\ndistributions that complicate the exploration and training of robust LLM\nsolvers. To tackle these issues, we propose a novel BDC framework that explores\ninsightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with\nmutual \\textbf{B}oosting, \\textbf{D}isentangles the heterogeneous training data\nfor composable LoRA-experts, and obtain \\textbf{C}ustomized problem solver for\neach data instance with an input-aware hypernetwork to weight over the\nLoRA-experts, offering effectiveness, flexibility, and robustness. This\nframework leverages multiple LLMs through mutual verification and boosting,\nintegrated into a Monte-Carlo Tree Search process enhanced by reflection-based\npruning and refinement. Additionally, we introduce the DisenLora algorithm,\nwhich clusters heterogeneous data to fine-tune LLMs into composable Lora\nexperts, enabling the adaptive generation of customized problem solvers through\nan input-aware hypernetwork. This work lays the groundwork for advancing LLM\ncapabilities in complex reasoning tasks, offering a novel System2-to-System1\nsolution.",
        "Operations Research (OR) is vital for decision-making in many industries.\nWhile recent OR methods have seen significant improvements in automation and\nefficiency through integrating Large Language Models (LLMs), they still\nstruggle to produce meaningful explanations. This lack of clarity raises\nconcerns about transparency and trustworthiness in OR applications. To address\nthese challenges, we propose a comprehensive framework, Explainable Operations\nResearch (EOR), emphasizing actionable and understandable explanations\naccompanying optimization. The core of EOR is the concept of Decision\nInformation, which emerges from what-if analysis and focuses on evaluating the\nimpact of complex constraints (or parameters) changes on decision-making.\nSpecifically, we utilize bipartite graphs to quantify the changes in the OR\nmodel and adopt LLMs to improve the explanation capabilities. Additionally, we\nintroduce the first industrial benchmark to rigorously evaluate the\neffectiveness of explanations and analyses in OR, establishing a new standard\nfor transparency and clarity in the field.",
        "We present a novel generative approach based on Denoising Diffusion Models\n(DDMs), which produces high-quality image samples along with their losslessly\ncompressed bit-stream representations. This is obtained by replacing the\nstandard Gaussian noise sampling in the reverse diffusion with a selection of\nnoise samples from pre-defined codebooks of fixed iid Gaussian vectors.\nSurprisingly, we find that our method, termed Denoising Diffusion Codebook\nModel (DDCM), retains sample quality and diversity of standard DDMs, even for\nextremely small codebooks. We leverage DDCM and pick the noises from the\ncodebooks that best match a given image, converting our generative model into a\nhighly effective lossy image codec achieving state-of-the-art perceptual image\ncompression results. More generally, by setting other noise selections rules,\nwe extend our compression method to any conditional image generation task\n(e.g., image restoration), where the generated images are produced jointly with\ntheir condensed bit-stream representations. Our work is accompanied by a\nmathematical interpretation of the proposed compressed conditional generation\nschemes, establishing a connection with score-based approximations of posterior\nsamplers for the tasks considered.",
        "Systematic expansion schemes in functional approaches require the inclusion\nof higher order vertices. These vertices are expanded in independent tensor\nbases with a rapidly increasing number of basis elements. Amongst the related\ntasks are the construction of bases and projection operators, the importance\nordering of their elements, and the optimisation of such tensor bases, as well\nas an analysis of their regularity in momentum space. We present progress in\nall these directions and introduce the Mathematica package TensorBases designed\nfor the aforementioned tasks.",
        "We studied the collider phenomenology of third-generation scalar leptoquarks\nat the Large Hadron Collider (LHC) with a 14 TeV center-of-mass energy. The\nanalysis focuses on leptoquarks decaying exclusively into top quarks and tau\nleptons, employing machine learning-based tagging techniques for identifying\nhadronically decaying boosted top quarks, W\/Z, and Higgs bosons, as well as a\nmultivariate classifier to distinguish signal events from Standard Model (SM)\nbackgrounds. The expected 95% confidence level (CL) upper limits on the\nleptoquark production cross-section are computed assuming integrated\nluminosities of 200 and 500 inverse femtobarns at the 14 TeV LHC. The results\ndemonstrate significant sensitivity improvements for detecting leptoquarks at\nmasses beyond the current experimental limits.",
        "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.",
        "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https:\/\/github.com\/BlueWhaleLab\/DCScore.",
        "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons\/TinyML (MLC\/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets.",
        "We present the design of a multiscale parareal method for kinetic equations\nin the fluid dynamic regime. The goal is to reduce the cost of a fully kinetic\nsimulation using a parallel in time procedure. Using the multiscale property of\nkinetic models, the cheap, coarse propagator consists in a fluid solver and the\nfine (expensive) propagation is achieved through a kinetic solver for a\ncollisional Vlasov equation. To validate our approach, we present simulations\nin the 1D in space, 3D in velocity settings over a wide range of initial data\nand kinetic regimes, showcasing the accuracy, efficiency, and the speedup\ncapabilities of our method.",
        "In this demonstration, we showcase POSMAC1, a platform designed to deploy\nDecision Tree (DT) and Random Forest (RF) models on the NVIDIA DOCA DPU,\nequipped with an ARM processor, for real-time network traffic classification.\nDeveloped specifically for Augmented Reality (AR) and Cloud Gaming (CG) traffic\nclassification, POSMAC streamlines model evaluation, and generalization while\noptimizing throughput to closely match line rates.",
        "Detecting stenosis in coronary angiography is vital for diagnosing and\nmanaging cardiovascular diseases. This study evaluates the performance of\nstate-of-the-art object detection models on the ARCADE dataset using the\nMMDetection framework. The models are assessed using COCO evaluation metrics,\nincluding Intersection over Union (IoU), Average Precision (AP), and Average\nRecall (AR). Results indicate variations in detection accuracy across different\nmodels, attributed to differences in algorithmic design, transformer-based vs.\nconvolutional architectures. Additionally, several challenges were encountered\nduring implementation, such as compatibility issues between PyTorch, CUDA, and\nMMDetection, as well as dataset inconsistencies in ARCADE. The findings provide\ninsights into model selection for stenosis detection and highlight areas for\nfurther improvement in deep learning-based coronary artery disease diagnosis.",
        "We explore the quantum superposition of boundary conditions in the context of\nthe Poincar\\'e patch of the two-dimensional Anti-de Sitter space\n($\\mathrm{PAdS}_2$). Focusing on Robin (mixed) boundary conditions (RBC), we\ninvestigate the response function of the Unruh-DeWitt (UDW) detector\ninteracting with two or more scalar fields, each respecting a different\nboundary condition. The role of this quantum superposition is two-fold: i) it\nmay represent different fields propagating on the same spacetime and\ninteracting with an UDW detector or ii) it may describe an UDW detector on a\nsuperposition of spacetimes, each one with an inequivalent propagating field.",
        "Fact-checking is necessary to address the increasing volume of\nmisinformation. Traditional fact-checking relies on manual analysis to verify\nclaims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models\n(LLMs) across multiple labeling schemes (binary, three-class, five-class) and\nextends traditional claim verification by incorporating analysis, verdict\nclassification, and explanation in a structured setup to provide comprehensive\njustifications for real-world claims. We evaluate Llama-3 models of varying\nsizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024)\nusing evidence retrieved via restricted web searches. We utilize TIGERScore as\na reference-free evaluation metric to score the justifications. Our results\nshow that larger LLMs consistently outperform smaller LLMs in classification\naccuracy and justification quality without fine-tuning. We find that smaller\nLLMs in a one-shot scenario provide comparable task performance to fine-tuned\nSmall Language Models (SLMs) with large context sizes, while larger LLMs\nconsistently surpass them. Evidence integration improves performance across all\nmodels, with larger LLMs benefiting most. Distinguishing between nuanced labels\nremains challenging, emphasizing the need for further exploration of labeling\nschemes and alignment with evidences. Our findings demonstrate the potential of\nretrieval-augmented AFC with LLMs.",
        "The evolution of automotive technologies towards more integrated and\nsophisticated systems requires a shift from traditional distributed\narchitectures to centralized vehicle architectures. This work presents a novel\nframework that addresses the increasing complexity of Software Defined Vehicles\n(SDV) through a centralized approach that optimizes software and hardware\nintegration. Our approach introduces a scalable, modular, and secure automotive\ndeployment framework that leverages a hardware abstraction layer and dynamic\nsoftware deployment capabilities to meet the growing demands of the industry.\nThe framework supports centralized computing of vehicle functions, making\nsoftware development more dynamic and easier to update and upgrade. We\ndemonstrate the capabilities of our framework by implementing it in a simulated\nenvironment where it effectively handles several automotive operations such as\nlane detection, motion planning, and vehicle control. Our results highlight the\nframework's potential to facilitate the development and maintenance of future\nvehicles, emphasizing its adaptability to different hardware configurations and\nits readiness for real-world applications. This work lays the foundation for\nfurther exploration of robust, scalable, and secure SDV systems, setting a new\nstandard for future automotive architectures.",
        "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.",
        "Predicting flutter remains a key challenge in aeroelastic research, with\ncertain models relying on modal parameters, such as natural frequencies and\ndamping ratios. These models are particularly useful in early design stages or\nfor the development of small UAVs (maximum take-off mass below 7 kg). This\nstudy evaluates two frequency-domain system identification methods, Fast\nRelaxed Vector Fitting (FRVF) and the Loewner Framework (LF), for predicting\nthe flutter onset speed of a flexible wing model. Both methods are applied to\nextract modal parameters from Ground Vibration Testing data, which are\nsubsequently used to develop a reduced-order model with two degrees of freedom.\nResults indicate that FRVF and LFinformed models provide reliable flutter\nspeed, with predictions deviating by no more than 3% (FRVF) and 5% (LF) from\nthe N4SID-informed benchmark. The findings highlight the sensitivity of flutter\nspeed predictions to damping ratio identification accuracy and demonstrate the\npotential of these methods as computationally efficient alternatives for\npreliminary aeroelastic assessments.",
        "We make several comments on the recent work in Ref.~\\cite{Rogers:2024nhb}\nwhile also reaffirming and adding to the work in Ref.~\\cite{Pitonyak:2023gjx}.\nWe show that the factorization formula for $e^+e^-\\to (h_1\\cdots h_n)\\, X$ in\nRef.~\\cite{Rogers:2024nhb} is equivalent to a version one can derive using the\ndefinition of a $n$-hadron fragmentation function (FF) introduced in\nRef.~\\cite{Pitonyak:2023gjx}. In addition, we scrutinize how to generalize the\nnumber density definition of a single-hadron FF to a $n$-hadron FF, arguing\nthat the definition given in Ref.~\\cite{Pitonyak:2023gjx} should be considered\nthe standard one. We also emphasize that the evolution equations for dihadron\nFFs~(DiFFs) in Ref.~\\cite{Pitonyak:2023gjx} have the same splitting functions\nas those for single-hadron FFs. Therefore, the DiFF (and $n$-hadron FF)\ndefinitions in Ref.~\\cite{Pitonyak:2023gjx} have a natural number density\ninterpretation and are consistent with collinear factorization using the\nstandard hard factors and evolution kernels. Moreover, we make clear that the\noperator definition for the DiFF $D_1^{h_1h_2}(\\xi,M_h)$ written down in\nRef.~\\cite{Rogers:2024nhb} agrees exactly with the one in\nRef.~\\cite{Pitonyak:2023gjx}. Contrary to what is implied in\nRef.~\\cite{Rogers:2024nhb}, this definition did not appear in the literature\nprior to the work in Ref.~\\cite{Pitonyak:2023gjx}. There also seem to be\ninconsistencies in how $D_1^{h_1h_2}(\\xi,M_h)$ appears in previous unpolarized\ncross section formulas in the literature."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b15",
    "start_title":"Learning Parities with Neural Networks",
    "start_abstract":"In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Testing conditional independence of discrete distributions"
      ],
      "abstract":[
        "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
      ],
      "categories":[
        "stat.CO"
      ]
    },
    "list":{
      "title":[
        "Splitting Regularized Wasserstein Proximal Algorithms for Nonsmooth\n  Sampling Problems",
        "Fast and light-weight energy statistics using the \\textit{R} package\n  \\textsf{Rfast}",
        "Deep P-Spline: Theory, Fast Tuning, and Application",
        "Parallel ADMM Algorithm with Gaussian Back Substitution for\n  High-Dimensional Quantile Regression and Classification",
        "Least squares variational inference",
        "MCMC for multi-modal distributions",
        "Design Principles for Architectures of Technical Smart Service Systems",
        "Resampling Methods that Generate Time Series Data to Enable Sensitivity\n  and Model Analysis in Energy Modeling",
        "Weighted Fisher divergence for high-dimensional Gaussian variational\n  inference",
        "Greedy Stein Variational Gradient Descent: An algorithmic approach for\n  wave prospection problems",
        "fastrerandomize: An R Package for Fast Rerandomization Using Accelerated\n  Computing",
        "Langevin Bi-fidelity Importance Sampling for Failure Probability\n  Estimation",
        "Geodesic Variational Bayes for Multiway Covariances",
        "CoHiRF: A Scalable and Interpretable Clustering Framework for\n  High-Dimensional Data",
        "A look-up table algorithm to model radiation damage effects in Monte\n  Carlo events for HL-LHC experiments",
        "Consistent crust-core interpolation and its effect on non-radial neutron\n  star oscillations",
        "Some Bohr-type inequalities with two parameters for bounded analytic\n  functions",
        "Spatially resolved dust properties over 50 kpc in a hyperluminous galaxy\n  merger at $z = 4.6$",
        "Inductive Construction of Variational Quantum Circuit for Constrained\n  Combinatorial Optimization",
        "Safety in safe Bayesian optimization and its ramifications for control",
        "Sample and Map from a Single Convex Potential: Generation using\n  Conjugate Moment Measures",
        "Improving Student Self-Efficacy in Quantum Computing with the Qubit\n  Touchdown Board Game",
        "A linearly-implicit energy preserving scheme for geometrically nonlinear\n  mechanics based on non-canonical Hamiltonian formulations",
        "Colimits of internal categories",
        "A Bayesian Multivariate Spatial Point Pattern Model: Application to Oral\n  Microbiome FISH Image Data",
        "Chiral Magnetic Effect enhancement at lower collision energies",
        "Rotational beta expansions and Schmidt games",
        "Constructing the low-temperature phase diagram for the $2+p$-quantum\n  spin glass using the nonperturbative renormalization group"
      ],
      "abstract":[
        "Sampling from nonsmooth target probability distributions is essential in\nvarious applications, including the Bayesian Lasso. We propose a\nsplitting-based sampling algorithm for the time-implicit discretization of the\nprobability flow for the Fokker-Planck equation, where the score function\ndefined as the gradient logarithm of the current probability density function,\nis approximated by the regularized Wasserstein proximal. When the prior\ndistribution is the Laplace prior, our algorithm is explicitly formulated as a\ndeterministic interacting particle system, incorporating softmax operators and\nshrinkage operations to efficiently compute the gradient drift vector field and\nthe score function. The proposed formulation introduces a particular class of\nattention layers in transformer structures, which can sample sparse target\ndistributions. We verify the convergence towards target distributions regarding\nR\\'enyi divergences under suitable conditions. Numerical experiments in\nhigh-dimensional nonsmooth sampling problems, such as sampling from mixed\nGaussian and Laplace distributions, logistic regressions, image restoration\nwith L1-TV regularization, and Bayesian neural networks, demonstrate the\nefficiency and robust performance of the proposed method.",
        "Energy statistics, also known as $\\mathcal{\\varepsilon}$-statistics, are\nfunctions of distances between statistical observations. This class of\nfunctions has enabled the development of non-linear statistical concepts, such\nas distance variance, distance covariance, and distance correlation. However,\nthe computational burden associated with $\\mathcal{\\varepsilon}$-statistics is\nsubstantial, particularly when the data reside in multivariate space. To\naddress this challenge, we have developed a method to significantly reduce\nmemory requirements and accelerate computations, thereby facilitating the\nanalysis of large data sets. The following cases are demonstrated: univariate\nand multivariate distance variance, distance covariance, partial distance\ncorrelation, energy distance, and hypothesis testing for the equality of\nunivariate distributions.",
        "Deep neural networks (DNNs) have been widely applied to solve real-world\nregression problems. However, selecting optimal network structures remains a\nsignificant challenge. This study addresses this issue by linking neuron\nselection in DNNs to knot placement in basis expansion techniques. We introduce\na difference penalty that automates knot selection, thereby simplifying the\ncomplexities of neuron selection. We name this method Deep P-Spline (DPS). This\napproach extends the class of models considered in conventional DNN modeling\nand forms the basis for a latent variable modeling framework using the\nExpectation-Conditional Maximization (ECM) algorithm for efficient network\nstructure tuning with theoretical guarantees. From a nonparametric regression\nperspective, DPS is proven to overcome the curse of dimensionality, enabling\nthe effective handling of datasets with a large number of input variable, a\nscenario where conventional nonparametric regression methods typically\nunderperform. This capability motivates the application of the proposed\nmethodology to computer experiments and image data analyses, where the\nassociated regression problems involving numerous inputs are common. Numerical\nresults validate the effectiveness of the model, underscoring its potential for\nadvanced nonlinear regression tasks.",
        "In the field of high-dimensional data analysis, modeling methods based on\nquantile loss function are highly regarded due to their ability to provide a\ncomprehensive statistical perspective and effective handling of heterogeneous\ndata. In recent years, many studies have focused on using the parallel\nalternating direction method of multipliers (P-ADMM) to solve high-dimensional\nquantile regression and classification problems. One efficient strategy is to\nreformulate the quantile loss function by introducing slack variables. However,\nthis reformulation introduces a theoretical challenge: even when the\nregularization term is convex, the convergence of the algorithm cannot be\nguaranteed. To address this challenge, this paper proposes the Gaussian\nBack-Substitution strategy, which requires only a simple and effective\ncorrection step that can be easily integrated into existing parallel algorithm\nframeworks, achieving a linear convergence rate. Furthermore, this paper\nextends the parallel algorithm to handle some novel quantile loss\nclassification models. Numerical simulations demonstrate that the proposed\nmodified P-ADMM algorithm exhibits excellent performance in terms of\nreliability and efficiency.",
        "Variational inference consists in finding the best approximation of a target\ndistribution within a certain family, where `best' means (typically) smallest\nKullback-Leiber divergence. We show that, when the approximation family is\nexponential, the best approximation is the solution of a fixed-point equation.\nWe introduce LSVI (Least-Squares Variational Inference), a Monte Carlo variant\nof the corresponding fixed-point recursion, where each iteration boils down to\nordinary least squares regression and does not require computing gradients. We\nshow that LSVI is equivalent to stochastic mirror descent; we use this insight\nto derive convergence guarantees. We introduce various ideas to improve LSVI\nfurther when the approximation family is Gaussian, leading to a $O(d^3)$\ncomplexity in the dimension $d$ of the target in the full-covariance case, and\na $O(d)$ complexity in the mean-field case. We show that LSVI outperforms\nstate-of-the-art methods in a range of examples, while remaining gradient-free,\nthat is, it does not require computing gradients.",
        "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces.",
        "Successful smart services require seamless integration into existing\ncorporate systems and an interdisciplinary approach that aligns the development\nof both business models and technical architectures. Multi-disciplinarity and\ncocreating with customers add a layer of complexity but are essential\ncollaboration schemes for validating the value proposition of smart services\nand building longterm customer loyalty. This paper explores these challenges\nand distills the design principles for the architectures of technical smart\nservice systems, based on empirical data from architecture projects in two\nmanufacturing companies. These principles contribute to the sparse academic\nliterature on this topic and help practitioners navigate several design\ntrade-offs commonly arising in smart service projects.",
        "Energy systems modeling frequently relies on time series data, whether\nobserved or forecast. This is particularly the case, for example, in capacity\nplanning models that use hourly production and load data forecast to occur over\nthe coming several decades. This paper addresses the attendant problem of\nperforming sensitivity, robustness, and other post-solution analyses using time\nseries data. We explore two efficient and relatively simple, non-parametric,\nbootstrapping methods for generating arbitrary numbers of time series from a\nsingle observed or forecast series. The paper presents and assesses each\nmethod. We find that the generated series are both visually and by statistical\nsummary measures close to the original observational data. In consequence these\nseries are credibly taken as stochastic instances from a common distribution,\nthat of the original series of observations. With climate change in mind, the\npaper further proposes and explores two general techniques for systematically\naltering (increasing or decreasing) time series. Both for the perturbed and\nunperturbed synthetic series data, we find that the generated series induce\nvariability in properties of the series that are important for energy modeling,\nin particular periods of under- and over-production, and periods of increased\nramping rates. In consequence, series produced in this way are apt for use in\nrobustness, sensitivity, and in general post-solution analysis of energy\nplanning models. These validity factors auger well for applications beyond\nenergy modeling.",
        "Bayesian inference has many advantages for complex models. However, standard\nMonte Carlo methods for summarizing the posterior can be computationally\ndemanding, and it is attractive to consider optimization-based variational\napproximations. Our work considers Gaussian approximations with sparse\nprecision matrices which are tractable to optimize in high-dimensional\nproblems. Although the optimal Gaussian approximation is usually defined as the\none closest to the target posterior in Kullback-Leibler divergence, it is\nuseful to consider other divergences when the Gaussian assumption is crude, in\norder to capture important features of the posterior for a given application.\nOur work studies the weighted Fisher divergence, which focuses on gradient\ndifferences between the target posterior and its approximation, with the Fisher\nand score-based divergences being special cases. We make three main\ncontributions. First, we compare approximations for weighted Fisher divergences\nunder mean-field assumptions for both Gaussian and non-Gaussian targets with\nKullback-Leibler approximations. Second, we go beyond mean-field and consider\napproximations with sparse precision matrices reflecting posterior conditional\nindependence structure for hierarchical models. Using stochastic gradient\ndescent to enforce sparsity, we develop two approaches to minimize the weighted\nFisher divergence, based on the reparametrization trick and a batch\napproximation of the objective. Finally, we examine the performance of our\nmethods for examples involving logistic regression, generalized linear mixed\nmodels and stochastic volatility models.",
        "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
        "The fastrerandomize R package provides hardware-accelerated tools for\nperforming rerandomization and randomization testing in experimental research.\nUsing a JAX backend, the package enables exact rerandomization inference even\nfor large experiments with hundreds of billions of possible randomizations. Key\nfunctionalities include generating pools of acceptable rerandomizations based\non covariate balance, conducting exact randomization tests, and performing\npre-analysis evaluations to determine optimal rerandomization acceptance\nthresholds. Through batched processing and GPU acceleration, fastrerandomize\nachieves substantial performance gains compared to existing implementations,\nmaking previously intractable designs computationally feasible. The package\ntherefore extends the randomization-based inference toolkit in R, allowing\nresearchers to efficiently implement more stringent rerandomization designs and\nconduct valid inference even with large sample sizes or in high-dimensional\nsettings.",
        "Estimating failure probability is one of the key tasks in the field of\nuncertainty quantification. In this domain, importance sampling has proven to\nbe an effective estimation strategy; however, its efficiency heavily depends on\nthe choice of the biasing distribution. An improperly selected biasing\ndistribution can significantly increase estimation error. One way to solve this\nproblem is to leverage a less expensive, lower-fidelity surrogate. Building on\nthe accessibility to such a model and its derivative on the random uncertain\ninputs, we introduce an importance-sampling-based estimator, termed the\nLangevin bi-fidelity importance sampling (L-BF-IS), which uses\nscore-function-based sampling algorithms to generate new samples and\nsubstantially reduces the mean square error (MSE) of failure probability\nestimation. The proposed method demonstrates lower estimation error, especially\nin high-dimensional ($\\geq 100$) input spaces and when limited high-fidelity\nevaluations are available. The L-BF-IS estimator's effectiveness is validated\nthrough experiments with two synthetic functions and two real-world\napplications governed by partial differential equations. These real-world\napplications involve a composite beam, which is represented using a simplified\nEuler-Bernoulli equation as a low-fidelity surrogate, and a steady-state\nstochastic heat equation, for which a pre-trained neural operator serves as the\nlow-fidelity surrogate.",
        "This article explores the optimization of variational approximations for\nposterior covariances of Gaussian multiway arrays. To achieve this, we\nestablish a natural differential geometric optimization framework on the space\nusing the pullback of the affine-invariant metric. In the case of a truly\nseparable covariance, we demonstrate a joint approximation in the multiway\nspace outperforms a mean-field approximation in optimization efficiency and\nprovides a superior approximation to an unstructured Inverse-Wishart posterior\nunder the average Mahalanobis distance of the data while maintaining a multiway\ninterpretation. We moreover establish efficient expressions for the Euclidean\nand Riemannian gradients in both cases of the joint and mean-field\napproximation. We end with an analysis of commodity trade data.",
        "Clustering high-dimensional data poses significant challenges due to the\ncurse of dimensionality, scalability issues, and the presence of noisy and\nirrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF),\na novel clustering method designed to address these challenges effectively.\nCoHiRF leverages random feature selection to mitigate noise and dimensionality\neffects, repeatedly applies K-Means clustering in reduced feature spaces, and\ncombines results through a unanimous consensus criterion. This iterative\napproach constructs a cluster assignment matrix, where each row records the\ncluster assignments of a sample across repetitions, enabling the identification\nof stable clusters by comparing identical rows. Clusters are organized\nhierarchically, enabling the interpretation of the hierarchy to gain insights\ninto the dataset. CoHiRF is computationally efficient with a running time\ncomparable to K-Means, scalable to massive datasets, and exhibits robust\nperformance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and\nOPTICS. Experimental results on synthetic and real-world datasets confirm the\nmethod's ability to reveal meaningful patterns while maintaining scalability,\nmaking it a powerful tool for high-dimensional data analysis.",
        "Radiation damage significantly impacts the performance of silicon tracking\ndetectors in Large Hadron Collider (LHC) experiments such as ATLAS and CMS,\nwith signal reduction being the most critical effect. Adjusting sensor bias\nvoltage and detection thresholds can help mitigate these effects, but\ngenerating simulated data that accurately mirror the performance evolution with\nthe accumulation of luminosity, hence fluence, is crucial. The ATLAS\ncollaboration has developed and implemented algorithms to correct simulated\nMonte Carlo (MC) events for radiation damage effects, achieving impressive\nagreement between collision data and simulated events. In preparation for the\nhigh-luminosity phase (HL-LHC), the demand for a faster ATLAS MC production\nalgorithm becomes imperative due to escalating collision, events, tracks, and\nparticle hit rates, imposing stringent constraints on available computing\nresources. This article outlines the philosophy behind the new algorithm, its\nimplementation strategy, and the essential components involved. The results\nfrom closure tests indicate that the events simulated using the new algorithm\nagree with fully simulated events at the level of few \\%. The first tests on\ncomputing performance show that the new algorithm is as fast as it is when no\nradiation damage corrections are applied.",
        "To model the structure of neutron stars (NSs) theoretically,it is common to\nconsider layers with different density regimes. Matching the equation of state\n(EoS) for the crust and core and obtaining a suitable description of these\nextreme conditions are crucial for understanding the properties of these\ncompact objects. In this work, we construct ten different NS EoSs incorporating\nthree distinct crust models, which are connected to the core using a\nthermodynamically and causally consistent formalism. For cold NSs, we propose a\nlinear relationship between pressure and energy density in a narrow region\nbetween the crust and core, effectively establishing an interpolation function\nin the pressure-baryonic chemical potential plane. We then compare this EoS\nmatching method with the classical approach, which neglects causal and\nthermodynamic consistency. We solve the Tolman-Oppenheimer-Volkoff equation to\nobtain the mass-radius relationship and compare our results with observational\nconstraints on NSs. Furthermore, we investigate the influence of the new\nmatching formalism on non-radial oscillation frequencies and damping times. Our\nfindings suggest that the method used to glue the crust and core EoS impacts NS\nobservables, such as the radius, oscillation frequencies, and damping times of\nnon-radial modes, which may be crucial for interpreting future gravitational\nwave observations from neutron star mergers or isolated pulsars. The effects\nare particularly noticeable for low-mass NSs, regardless of the specific EoS\nmodel chosen. In particular, we find that the $p_1$ oscillation mode exhibits\nsignificant differences in frequencies among alternative matching methods,\nwhereas the fundamental $f$-mode remains unaffected by changes in crust models\nor interpolation schemes.",
        "In this article, some Bohr inequalities for analytical functions on the unit\ndisk are generalized to the forms with two parameters. One of our results is\nsharp.",
        "We present spatially resolved dust-continuum ALMA observations from\nrest-frame $\\sim$60 to $\\sim$600 $\\mu$m (bands 3-10) of the hyperluminous hot\ndust-obscured galaxy (hot DOG) WISE J224607.6-052634.9 (W2246-0526), at\nredshift $z=4.6$. W2246-0526 is interacting with at least three companion\ngalaxies, forming a system connected by tidal streams. We model the\nmultiwavelength ALMA observations of the dust continuum using a modified\nblackbody, from which we derive the dust properties (mass, emissivity index,\narea of the emitting region, and temperature) in the hot DOG and resolved\nstructures across a region of nearly $\\sim$50 kpc. The peak temperature at the\nlocation of the hot DOG, $\\sim$110 K, is likely the consequence of heating by\nthe central quasar. The dust temperature drops to $\\sim$40 K at a radius of\n$\\sim$8 kpc, suggesting that heating by the quasar beyond that distance is\nnondominant. The dust in the connecting streams between the host and companion\ngalaxies is at temperatures between 30-40 K, typical of starburst galaxies,\nsuggesting it is most likely heated by recent, in-situ star formation. This is\nthe first time dust properties are spatially resolved over several tens of kpc\nin a galaxy system beyond Cosmic Noon --this is more than six times the scales\npreviously probed in galaxies at those redshifts.",
        "In this study, we propose a new method for constrained combinatorial\noptimization using variational quantum circuits. Quantum computers are\nconsidered to have the potential to solve large combinatorial optimization\nproblems faster than classical computers. Variational quantum algorithms, such\nas Variational Quantum Eigensolver (VQE), have been studied extensively because\nthey are expected to work on noisy intermediate scale devices. Unfortunately,\nmany optimization problems have constraints, which induces infeasible solutions\nduring VQE process. Recently, several methods for efficiently solving\nconstrained combinatorial optimization problems have been proposed by designing\na quantum circuit so as to output only the states that satisfy the constraints.\nHowever, the types of available constraints are still limited. Therefore, we\nhave started to develop variational quantum circuits that can handle a wider\nrange of constraints. The proposed method utilizes a forwarding operation that\nmaps from feasible states for subproblems to those for larger subproblems. As\nlong as appropriate forwarding operations can be defined, iteration of this\nprocess can inductively construct variational circuits outputting feasible\nstates even in the case of multiple and complex constraints. In this paper, the\nproposed method was applied to facility location problem and was found to\nincrease the probability for measuring feasible solutions or optimal solutions.\nIn addition, the cost of the obtained circuit was comparable to that of\nconventional variational circuits.",
        "A recurring and important task in control engineering is parameter tuning\nunder constraints, which conceptually amounts to optimization of a blackbox\nfunction accessible only through noisy evaluations. For example, in control\npractice parameters of a pre-designed controller are often tuned online in\nfeedback with a plant, and only safe parameter values should be tried, avoiding\nfor example instability. Recently, machine learning methods have been deployed\nfor this important problem, in particular, Bayesian optimization (BO). To\nhandle safety constraints, algorithms from safe BO have been utilized,\nespecially SafeOpt-type algorithms, which enjoy considerable popularity in\nlearning-based control, robotics, and adjacent fields. However, we identify two\nsignificant obstacles to practical safety. First, SafeOpt-type algorithms rely\non quantitative uncertainty bounds, and most implementations replace these by\ntheoretically unsupported heuristics. Second, the theoretically valid\nuncertainty bounds crucially depend on a quantity - the reproducing kernel\nHilbert space norm of the target function - that at present is impossible to\nreliably bound using established prior engineering knowledge. By careful\nnumerical experiments we show that these issues can indeed cause safety\nviolations. To overcome these problems, we propose Lipschitz-only Safe Bayesian\nOptimization (LoSBO), a safe BO algorithm that relies only on a known Lipschitz\nbound for its safety. Furthermore, we propose a variant (LoS-GP-UCB) that\navoids gridding of the search space and is therefore applicable even for\nmoderately high-dimensional problems.",
        "A common approach to generative modeling is to split model-fitting into two\nblocks: define first how to sample noise (e.g. Gaussian) and choose next what\nto do with it (e.g. using a single map or flows). We explore in this work an\nalternative route that ties sampling and mapping. We find inspiration in moment\nmeasures, a result that states that for any measure $\\rho$ supported on a\ncompact convex set of $\\mathbb{R}^d$, there exists a unique convex potential\n$u$ such that $\\rho=\\nabla u\\,\\sharp\\,e^{-u}$. While this does seem to tie\neffectively sampling (from log-concave distribution $e^{-u}$) and action\n(pushing particles through $\\nabla u$), we observe on simple examples (e.g.,\nGaussians or 1D distributions) that this choice is ill-suited for practical\ntasks. We study an alternative factorization, where $\\rho$ is factorized as\n$\\nabla w^*\\,\\sharp\\,e^{-w}$, where $w^*$ is the convex conjugate of $w$. We\ncall this approach conjugate moment measures, and show far more intuitive\nresults on these examples. Because $\\nabla w^*$ is the Monge map between the\nlog-concave distribution $e^{-w}$ and $\\rho$, we rely on optimal transport\nsolvers to propose an algorithm to recover $w$ from samples of $\\rho$, and\nparameterize $w$ as an input-convex neural network.",
        "Qubit Touchdown is a two-player, competitive board game that was developed to\nintroduce students to quantum computing. A quantum computer is a new kind of\ncomputer that is based on the laws of quantum physics, and it can solve certain\nproblems faster than normal computers because it follows a different set of\nrules. Qubit Touchdown's game play mirrors the rules of (American) football,\nwith players taking turns moving the football to score the most touchdowns, and\nno knowledge of quantum computing is needed to play the game. We evaluated the\ngame with 107 public high school students in Precalculus, Advanced Placement\n(AP) Statistics, and\/or AP Physics 1 courses, assessing whether their interest\nin and self-efficacy toward quantum computing changed as a result of playing\nthe game and learning about its connections to quantum computing. We also\nassessed whether the game was easy to learn and enjoyable. We found that\nstudents' self-efficacy was improved by 33.4%, and they widely considered the\ngame accessible and fun. Thus, Qubit Touchdown could be an effective resource\nto introduce students to Quantum Computing and boost their confidence in\nlearning about the field. Free printables of the game are available, and\nprofessionally produced copies can be purchased on demand.",
        "This work presents a novel formulation and numerical strategy for the\nsimulation of geometrically nonlinear structures. First, a non-canonical\nHamiltonian (Poisson) formulation is introduced by including the dynamics of\nthe stress tensor. This framework is developed for von-K\\'arm\\'an\nnonlinearities in beams and plates, as well as finite strain elasticity with\nSaint-Venant material behavior. In the case of plates, both negligible and\nnon-negligible membrane inertia are considered. For the former case the\ntwo-dimensional elasticity complex is leveraged to express the dynamics in\nterms of the Airy stress function. The finite element discretization employs a\nmixed approach, combining a conforming approximation for displacement and\nvelocity fields with a discontinuous stress tensor representation. A staggered,\nlinear implicit time integration scheme is proposed, establishing connections\nwith existing explicit-implicit energy-preserving methods. The stress degrees\nof freedom are statically condensed, reducing the computational complexity to\nsolving a system with a positive definite matrix. The methodology is validated\nthrough numerical experiments on the Duffing oscillator, a von-K\\'arm\\'an beam,\nand a column undergoing finite strain elasticity. Comparisons with fully\nimplicit energy-preserving method and the explicit Newmark scheme demonstrate\nthat the proposed approach achieves superior accuracy while maintaining energy\nstability. Additionally, it enables larger time steps compared to explicit\nschemes and exhibits computational efficiency comparable to the leapfrog\nmethod.",
        "We show that for a list-arithmetic pretopos $\\mathcal{E}$ with pullback\nstable coequalisers, the $2$-category $\\mathbf{Cat}(\\mathcal{E})$ of internal\ncategories, functors and natural transformations has finite $2$-colimits.",
        "Advances in cellular imaging technologies, especially those based on\nfluorescence in situ hybridization (FISH) now allow detailed visualization of\nthe spatial organization of human or bacterial cells. Quantifying this spatial\norganization is crucial for understanding the function of multicellular tissues\nor biofilms, with implications for human health and disease. To address the\nneed for better methods to achieve such quantification, we propose a flexible\nmultivariate point process model that characterizes and estimates complex\nspatial interactions among multiple cell types. The proposed Bayesian framework\nis appealing due to its unified estimation process and the ability to directly\nquantify uncertainty in key estimates of interest, such as those of inter-type\ncorrelation and the proportion of variance due to inter-type relationships. To\nensure stable and interpretable estimation, we consider shrinkage priors for\ncoefficients associated with latent processes. Model selection and comparison\nare conducted by using a deviance information criterion designed for models\nwith latent variables, effectively balancing the risk of overfitting with that\nof oversimplifying key quantities. Furthermore, we develop a hierarchical\nmodeling approach to integrate multiple image-specific estimates from a given\nsubject, allowing inference at both the global and subject-specific levels. We\napply the proposed method to microbial biofilm image data from the human tongue\ndorsum and find that specific taxon pairs, such as Streptococcus\nmitis-Streptococcus salivarius and Streptococcus mitis-Veillonella, exhibit\nstrong positive spatial correlations, while others, such as Actinomyces-Rothia,\nshow slight negative correlations. For most of the taxa, a substantial portion\nof spatial variance can be attributed to inter-taxon relationships.",
        "We extend previous holographic studies of the Chiral Magnetic Effect (CME) by\nincorporating a time-dependent magnetic field. Various magnetic field profiles\nproposed in the literature are implemented, and their impact on the CME signal\nis analyzed in both static and expanding backgrounds. Interestingly, the\nintegrated chiral magnetic current can exhibit a non-monotonic dependence on\nthe collision energy. Our results suggest that the CME signal is enhanced at\ncollision energies below $\\sqrt{s}=200$ GeV. In addition, we derive a\nquasi-equilibrium formula for the chiral magnetic effect in the expanding\nbackground that is valid at late times.",
        "We consider rotational beta expansions in dimensions 1, 2 and 4 and view them\nas expansions on real numbers, complex numbers, and quaternions, respectively.\nWe give sufficient conditions on the parameters $\\alpha, \\beta \\in (0,1)$ so\nthat particular cylinder sets arising from the expansions are winning or losing\nSchmidt $(\\alpha,\\beta)$-game.",
        "In this paper, we use a nonperturbative renormalization group approach to\nconstruct the dynamical phase space of a quantum spin glass in the large $N$\nlimit. The disordered Hamiltonian is of ``$2 + p$\" type, and we perform a\ncoarse-graining procedure over the Wigner spectrum for the matrix-like\ndisorder. The phase space reconstruction relies on phase transitions derived\nfrom the Luttinger-Ward functional, which accounts for interactions that are\nforbidden by perturbation theory. Various phases are identified, characterized\nby large correlations between replicas and\/or the breaking of time translation\nsymmetry."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b11",
    "start_title":"Testing conditional independence of discrete distributions",
    "start_abstract":"We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary.",
    "start_categories":[
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Learning Parities with Neural Networks"
      ],
      "abstract":[
        "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Pareto Optimization with Robust Evaluation for Noisy Subset Selection",
        "Dendritic Localized Learning: Toward Biologically Plausible Algorithm",
        "Extract-QD Framework: A Generic Approach for Quality-Diversity in Noisy,\n  Stochastic or Uncertain Domains",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "An Enhancement of Cuckoo Search Algorithm for Optimal Earthquake\n  Evacuation Space Allocation in Intramuros, Manila City",
        "Channel-wise Parallelizable Spiking Neuron with Multiplication-free\n  Dynamics and Large Temporal Receptive Fields",
        "Cascaded Large-Scale TSP Solving with Unified Neural Guidance: Bridging\n  Local and Population-based Search",
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for\n  Efficient Black-Box Neural Network Optimization",
        "Hardware-In-The-Loop Training of a 4f Optical Correlator with\n  Logarithmic Complexity Reduction for CNNs",
        "The working principles of model-based GAs fall within the PAC framework:\n  A mathematical theory of problem decomposition",
        "A RankNet-Inspired Surrogate-Assisted Hybrid Metaheuristic for Expensive\n  Coverage Optimization",
        "Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate\n  Tool for Ligand Strain Calculations",
        "Towards Fair and Robust Face Parsing for Generative AI: A\n  Multi-Objective Approach",
        "On the generalized eigenvalue problem in subspace-based excited state\n  methods for quantum computers",
        "Measuring Similarity in Causal Graphs: A Framework for Semantic and\n  Structural Analysis",
        "Hyperparameters in Score-Based Membership Inference Attacks",
        "Tensor renormalization group study of the two-dimensional lattice U(1)\n  gauge-Higgs model with a topological $\\theta$ term under L\\\"uscher's\n  admissibility condition",
        "Nonflat bands and chiral symmetry in magic-angle twisted bilayer\n  graphene",
        "Effect of imaginary gauge on wave transport in driven-dissipative\n  systems",
        "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
        "Explainable Reinforcement Learning via Temporal Policy Decomposition",
        "Towards Change Impact Analysis in Microservices-based System Evolution",
        "RIO EPICS device support application case study on an ion source control\n  system (ISHP)",
        "Migration of phthalate plasticisers in heritage objects made of\n  poly(vinyl chloride): mechanical and environmental aspects",
        "Near-extremal dumb holes and some aspects of the Hawking effect",
        "Can Safety Fine-Tuning Be More Principled? Lessons Learned from\n  Cybersecurity"
      ],
      "abstract":[
        "Subset selection is a fundamental problem in combinatorial optimization,\nwhich has a wide range of applications such as influence maximization and\nsparse regression. The goal is to select a subset of limited size from a ground\nset in order to maximize a given objective function. However, the evaluation of\nthe objective function in real-world scenarios is often noisy. Previous\nalgorithms, including the greedy algorithm and multi-objective evolutionary\nalgorithms POSS and PONSS, either struggle in noisy environments or consume\nexcessive computational resources. In this paper, we focus on the noisy subset\nselection problem with a cardinality constraint, where the evaluation of a\nsubset is noisy. We propose a novel approach based on Pareto Optimization with\nRobust Evaluation for noisy subset selection (PORE), which maximizes a robust\nevaluation function and minimizes the subset size simultaneously. PORE can\nefficiently identify well-structured solutions and handle computational\nresources, addressing the limitations observed in PONSS. Our experiments,\nconducted on real-world datasets for influence maximization and sparse\nregression, demonstrate that PORE significantly outperforms previous methods,\nincluding the classical greedy algorithm, POSS, and PONSS. Further validation\nthrough ablation studies confirms the effectiveness of our robust evaluation\nfunction.",
        "Backpropagation is the foundational algorithm for training neural networks\nand a key driver of deep learning's success. However, its biological\nplausibility has been challenged due to three primary limitations: weight\nsymmetry, reliance on global error signals, and the dual-phase nature of\ntraining, as highlighted by the existing literature. Although various\nalternative learning approaches have been proposed to address these issues,\nmost either fail to satisfy all three criteria simultaneously or yield\nsuboptimal results. Inspired by the dynamics and plasticity of pyramidal\nneurons, we propose Dendritic Localized Learning (DLL), a novel learning\nalgorithm designed to overcome these challenges. Extensive empirical\nexperiments demonstrate that DLL satisfies all three criteria of biological\nplausibility while achieving state-of-the-art performance among algorithms that\nmeet these requirements. Furthermore, DLL exhibits strong generalization across\na range of architectures, including MLPs, CNNs, and RNNs. These results,\nbenchmarked against existing biologically plausible learning algorithms, offer\nvaluable empirical insights for future research. We hope this study can inspire\nthe development of new biologically plausible algorithms for training\nmultilayer networks and advancing progress in both neuroscience and machine\nlearning.",
        "Quality-Diversity (QD) has demonstrated potential in discovering collections\nof diverse solutions to optimisation problems. Originally designed for\ndeterministic environments, QD has been extended to noisy, stochastic, or\nuncertain domains through various Uncertain-QD (UQD) methods. However, the\nlarge number of UQD methods, each with unique constraints, makes selecting the\nmost suitable one challenging. To remedy this situation, we present two\ncontributions: first, the Extract-QD Framework (EQD Framework), and second,\nExtract-ME (EME), a new method derived from it. The EQD Framework unifies\nexisting approaches within a modular view, and facilitates developing novel\nmethods by interchanging modules. We use it to derive EME, a novel method that\nconsistently outperforms or matches the best existing methods on standard\nbenchmarks, while previous methods show varying performance. In a second\nexperiment, we show how our EQD Framework can be used to augment existing QD\nalgorithms and in particular the well-established\nPolicy-Gradient-Assisted-MAP-Elites method, and demonstrate improved\nperformance in uncertain domains at no additional evaluation cost. For any new\nuncertain task, our contributions now provide EME as a reliable \"first guess\"\nmethod, and the EQD Framework as a tool for developing task-specific\napproaches. Together, these contributions aim to lower the cost of adopting UQD\ninsights in QD applications.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "The Cuckoo Search Algorithm (CSA), while effective in solving complex\noptimization problems, faces limitations in random population initialization\nand reliance on fixed parameters. Random initialization of the population often\nresults in clustered solutions, resulting in uneven exploration of the search\nspace and hindering effective global optimization. Furthermore, the use of\nfixed values for discovery rate and step size creates a trade-off between\nsolution accuracy and convergence speed. To address these limitations, an\nEnhanced Cuckoo Search Algorithm (ECSA) is proposed. This algorithm utilizes\nthe Sobol Sequence to generate a more uniformly distributed initial population\nand incorporates Cosine Annealing with Warm Restarts to dynamically adjust the\nparameters. The performance of the algorithms was evaluated on 13 benchmark\nfunctions (7 unimodal, 6 multimodal). Statistical analyses were conducted to\ndetermine the significance and consistency of the results. The ECSA outperforms\nthe CSA in 11 out of 13 benchmark functions with a mean fitness improvement of\n30% across all functions, achieving 35% for unimodal functions and 24% for\nmultimodal functions. The enhanced algorithm demonstrated increased convergence\nefficiency, indicating its superiority to the CSA in solving a variety of\noptimization problems. The ECSA is subsequently applied to optimize earthquake\nevacuation space allocation in Intramuros, Manila.",
        "Spiking Neural Networks (SNNs) are distinguished from Artificial Neural\nNetworks (ANNs) for their sophisticated neuronal dynamics and sparse binary\nactivations (spikes) inspired by the biological neural system. Traditional\nneuron models use iterative step-by-step dynamics, resulting in serial\ncomputation and slow training speed of SNNs. Recently, parallelizable spiking\nneuron models have been proposed to fully utilize the massive parallel\ncomputing ability of graphics processing units to accelerate the training of\nSNNs. However, existing parallelizable spiking neuron models involve dense\nfloating operations and can only achieve high long-term dependencies learning\nability with a large order at the cost of huge computational and memory costs.\nTo solve the dilemma of performance and costs, we propose the mul-free\nchannel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable\nfor SNNs' resource-restricted application scenarios. The proposed neuron\nimports the channel-wise convolution to enhance the learning ability, induces\nthe sawtooth dilations to reduce the neuron order, and employs the bit shift\noperation to avoid multiplications. The algorithm for design and implementation\nof acceleration methods is discussed meticulously. Our methods are validated in\nneuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and\nneuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.\nTraining speed results demonstrate the effectiveness of our acceleration\nmethods, providing a practical reference for future research.",
        "The traveling salesman problem (TSP) is a fundamental NP-hard optimization\nproblem. This work presents UNiCS, a novel unified neural-guided cascaded\nsolver for solving large-scale TSP instances. UNiCS comprises a local search\n(LS) phase and a population-based search (PBS) phase, both guided by a learning\ncomponent called unified neural guidance (UNG). Specifically, UNG guides\nsolution generation across both phases and determines appropriate phase\ntransition timing to effectively combine the complementary strengths of LS and\nPBS. While trained only on simple distributions with relatively small-scale TSP\ninstances, UNiCS generalizes effectively to challenging TSP benchmarks\ncontaining much larger instances (10,000-71,009 nodes) with diverse node\ndistributions entirely unseen during training. Experimental results on the\nlarge-scale TSP instances demonstrate that UNiCS consistently outperforms\nstate-of-the-art methods, with its advantage remaining consistent across\nvarious runtime budgets.",
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "Swarm intelligence optimization algorithms have gained significant attention\ndue to their ability to solve complex optimization problems. However, the\nefficiency of optimization in large-scale problems limits the use of related\nmethods. This paper presents a GPU-accelerated version of the Multi-Guiding\nSpark Fireworks Algorithm (MGFWA), which significantly improves the\ncomputational efficiency compared to its traditional CPU-based counterpart. We\nbenchmark the GPU-MGFWA on several neural network black-box optimization\nproblems and demonstrate its superior performance in terms of both speed and\nsolution quality. By leveraging the parallel processing power of modern GPUs,\nthe proposed GPU-MGFWA results in faster convergence and reduced computation\ntime for large-scale optimization tasks. The proposed implementation offers a\npromising approach to accelerate swarm intelligence algorithms, making them\nmore suitable for real-time applications and large-scale industrial problems.\nSource code is released at https:\/\/github.com\/mxxxr\/MGFWA.",
        "This work evaluates a forward-only learning algorithm on the MNIST dataset\nwith hardware-in-the-loop training of a 4f optical correlator, achieving 87.6%\naccuracy with O(n2) complexity, compared to backpropagation, which achieves\n88.8% accuracy with O(n2 log n) complexity.",
        "The concepts of linkage, building blocks, and problem decomposition have long\nexisted in the genetic algorithm (GA) field and have guided the development of\nmodel-based GAs for decades. However, their definitions are usually vague,\nmaking it difficult to develop theoretical support. This paper provides an\nalgorithm-independent definition to describe the concept of linkage. With this\ndefinition, the paper proves that any problems with a bounded degree of linkage\nare decomposable and that proper problem decomposition is possible via linkage\nlearning. The way of decomposition given in this paper also offers a new\nperspective on nearly decomposable problems with bounded difficulty and\nbuilding blocks from the theoretical aspect. Finally, this paper relates\nproblem decomposition to PAC learning and proves that the global optima of\nthese problems and the minimum decomposition blocks are PAC learnable under\ncertain conditions.",
        "Coverage optimization generally involves deploying a set of facilities to\nbest satisfy the demands of specified points, with broad applications in fields\nsuch as location science and sensor networks. Recent applications reveal that\nthe subset site selection coupled with continuous angular parameter\noptimization can be formulated as Mixed-Variable Optimization Problems (MVOPs).\nMeanwhile, high-fidelity discretization and visibility analysis significantly\nincrease computational cost and complexity, evolving the MVOP into an Expensive\nMixed-Variable Optimization Problem (EMVOP). While canonical Evolutionary\nAlgorithms have yielded promising results, their reliance on numerous fitness\nevaluations is too costly for our problem. Furthermore, most surrogate-assisted\nmethods face limitations due to their reliance on regression-based models. To\naddress these issues, we propose the RankNet-Inspired Surrogate-assisted Hybrid\nMetaheuristic (RI-SHM), an extension of our previous work. RI-SHM integrates\nthree key components: (1) a RankNet-based pairwise global surrogate that\ninnovatively predicts rankings between pairs of individuals, bypassing the\nchallenges of fitness estimation in discontinuous solution space; (2) a\nsurrogate-assisted local Estimation of Distribution Algorithm that enhances\nlocal exploitation and helps escape from local optima; and (3) a fitness\ndiversity-driven switching strategy that dynamically balances exploration and\nexploitation. Experiments demonstrate that our algorithm can effectively handle\nlarge-scale coverage optimization tasks of up to 300 dimensions and more than\n1,800 targets within desirable runtime. Compared to state-of-the-art algorithms\nfor EMVOPs, RI-SHM consistently outperforms them by up to 56.5$\\%$ across all\ntested instances.",
        "Ligand strain energy, the energy difference between the bound and unbound\nconformations of a ligand, is an important component of structure-based small\nmolecule drug design. A large majority of observed ligands in protein-small\nmolecule co-crystal structures bind in low-strain conformations, making strain\nenergy a useful filter for structure-based drug design. In this work we present\na tool for calculating ligand strain with a high accuracy. StrainRelief uses a\nMACE Neural Network Potential (NNP), trained on a large database of Density\nFunctional Theory (DFT) calculations to estimate ligand strain of neutral\nmolecules with quantum accuracy. We show that this tool estimates strain energy\ndifferences relative to DFT to within 1.4 kcal\/mol, more accurately than\nalternative NNPs. These results highlight the utility of NNPs in drug\ndiscovery, and provide a useful tool for drug discovery teams.",
        "Face parsing is a fundamental task in computer vision, enabling applications\nsuch as identity verification, facial editing, and controllable image\nsynthesis. However, existing face parsing models often lack fairness and\nrobustness, leading to biased segmentation across demographic groups and errors\nunder occlusions, noise, and domain shifts. These limitations affect downstream\nface synthesis, where segmentation biases can degrade generative model outputs.\nWe propose a multi-objective learning framework that optimizes accuracy,\nfairness, and robustness in face parsing. Our approach introduces a\nhomotopy-based loss function that dynamically adjusts the importance of these\nobjectives during training. To evaluate its impact, we compare multi-objective\nand single-objective U-Net models in a GAN-based face synthesis pipeline\n(Pix2PixHD). Our results show that fairness-aware and robust segmentation\nimproves photorealism and consistency in face generation. Additionally, we\nconduct preliminary experiments using ControlNet, a structured conditioning\nmodel for diffusion-based synthesis, to explore how segmentation quality\ninfluences guided image generation. Our findings demonstrate that\nmulti-objective face parsing improves demographic consistency and robustness,\nleading to higher-quality GAN-based synthesis.",
        "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers.",
        "Causal graphs are commonly used to understand and model complex systems.\nResearchers often construct these graphs from different perspectives, leading\nto significant variations for the same problem. Comparing causal graphs is,\ntherefore, essential for evaluating assumptions, integrating insights, and\nresolving disagreements. The rise of AI tools has further amplified this need,\nas they are increasingly used to generate hypothesized causal graphs by\nsynthesizing information from various sources such as prior research and\ncommunity inputs, providing the potential for automating and scaling causal\nmodeling for complex systems. Similar to humans, these tools also produce\ninconsistent results across platforms, versions, and iterations. Despite its\nimportance, research on causal graph comparison remains scarce. Existing\nmethods often focus solely on structural similarities, assuming identical\nvariable names, and fail to capture nuanced semantic relationships, which is\nessential for causal graph comparison. We address these gaps by investigating\nmethods for comparing causal graphs from both semantic and structural\nperspectives. First, we reviewed over 40 existing metrics and, based on\npredefined criteria, selected nine for evaluation from two threads of machine\nlearning: four semantic similarity metrics and five learning graph kernels. We\ndiscuss the usability of these metrics in simple examples to illustrate their\nstrengths and limitations. We then generated a synthetic dataset of 2,000\ncausal graphs using generative AI based on a reference diagram. Our findings\nreveal that each metric captures a different aspect of similarity, highlighting\nthe need to use multiple metrics.",
        "Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA.",
        "We investigate the two-dimensional lattice U(1) gauge-Higgs model with a\ntopological term, employing L\\\"uscher's admissibility condition. The standard\nMonte Carlo simulation for this model is hindered not only by the complex\naction problem due to the topological term but also by the topological freezing\nproblem originating from the admissibility condition. Resolving both obstacles\nsimultaneously with the tensor renormalization group approach, we show the\nadvantage of the admissibility condition in dealing with the topological term\ndiscretized with the so-called field-theoretical definition.",
        "In this work, we study an interacting tight-binding model of magic-angle\ntwisted bilayer graphene (MATBG), with a twist angle of $1.05^\\circ$. We derive\neffective theories based on a mean-field normal state at charge neutrality,\nthereby including the renormalizations coming from integrating out high-energy\nmodes. In these theories, the flat bands display a sizable increase of the\nbandwidth, suggesting a renormalization of the magic angle. Additionally, the\ncorresponding wavefunctions flow towards the limit of perfect particle-hole\nsymmetry and sublattice polarization (the 'chiral' limit). We further represent\nthe flat bands in the 'vortex Chern' basis and discuss the implications on the\ndynamics regarding the 'flat' and 'chiral' symmetries of MATBG, as manifested\nin the symmetry-broken states at neutrality.",
        "Wave transport in disordered media is a fundamental problem with direct\nimplications in condensed matter, materials science, optics, atomic physics,\nand even biology. The majority of studies are focused on Hermitian systems to\nunderstand disorder-induced localization. However, recent studies of\nnon-Hermitian disordered media have revealed unique behaviors, with a universal\nprinciple emerging that links the eigenvalue spectrum of the disordered\nHamiltonian and its statistics with its transport properties. In this work we\nshow that the situation can be very different in driven-dissipative lattices of\ncavities, where a uniform gain applied equally to all the components of the\nsystem can act as a knob for controlling the wave transport properties without\naltering the eigenvalue statistics of the underlying Hamiltonian. Our results\nopen a new avenue for developing a deeper insight into the transport properties\nin disordered media and will aid in building new devices as well. Our work\nwhich is presented in the context of optics generalizes to any physical\nplatforms where gain can be implemented. These include acoustics, electronics,\nand coupled quantum oscillators such as atoms, diamond centers and\nsuperconducting qubits.",
        "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
        "We investigate the explainability of Reinforcement Learning (RL) policies\nfrom a temporal perspective, focusing on the sequence of future outcomes\nassociated with individual actions. In RL, value functions compress information\nabout rewards collected across multiple trajectories and over an infinite\nhorizon, allowing a compact form of knowledge representation. However, this\ncompression obscures the temporal details inherent in sequential\ndecision-making, presenting a key challenge for interpretability. We present\nTemporal Policy Decomposition (TPD), a novel explainability approach that\nexplains individual RL actions in terms of their Expected Future Outcome (EFO).\nThese explanations decompose generalized value functions into a sequence of\nEFOs, one for each time step up to a prediction horizon of interest, revealing\ninsights into when specific outcomes are expected to occur. We leverage\nfixed-horizon temporal difference learning to devise an off-policy method for\nlearning EFOs for both optimal and suboptimal actions, enabling contrastive\nexplanations consisting of EFOs for different state-action pairs. Our\nexperiments demonstrate that TPD generates accurate explanations that (i)\nclarify the policy's future strategy and anticipated trajectory for a given\naction and (ii) improve understanding of the reward composition, facilitating\nfine-tuning of the reward function to align with human expectations.",
        "Cloud-native systems are the mainstream for enterprise solutions, given their\nscalability, resilience, and other benefits. While the benefits of cloud-native\nsystems fueled by microservices are known, less guidance exists on their\nevolution. One could assume that since microservices encapsulate their code,\ncode changes remain encapsulated as well; however, the community is becoming\nmore aware of the possible consequences of code change propagation across\nmicroservices. Moreover, an active mitigation instrument for negative\nconsequences of change propagation across microservices (i.e., ripple effect)\nis yet missing, but the microservice community would greatly benefit from it.\nThis paper introduces what it could look like to have an infrastructure to\nassist with change impact analysis across the entire microservice system and\nintends to facilitate advancements in laying out the foundations and building\nguidelines on microservice system evolution. It shares a new direction for\nincremental software architecture reconstruction that could serve as the\ninfrastructure concept and demonstrates early results from prototyping to\nillustrate the potential impact.",
        "Experimental Physics and Industrial Control System (EPICS) is a software tool\nthat during last years has become relevant as a main framework to deploy\ndistributed control systems in large scientific environments. At the moment,\nESS Bilbao uses this middleware to perform the control of their Ion Source\nHydrogen Positive (ISHP) project. The implementation of the control system was\nbased on: PXI Real Time controllers using the LabVIEW-RT and LabVIEW-EPICS\ntools; and RIO devices based on Field-Programmable Gate Array (FPGA)\ntechnology. Intended to provide a full compliant EPICS IOCs for RIO devices and\nto avoid additional efforts on the system maintainability, a migration of the\ncurrent system to a derivative Red Hat Linux (CentOS) environment has been\nconducted. This paper presents a real application case study for using the\nNIRIO EPICS device support (NIRIO-EDS) to give support to the ISHP. Although\nRIO FPGA configurations are particular solutions for ISHP performance, the\nNIRIO-EDS has permitted the control and monitoring of devices by applying a\nwell-defined design methodology into the previous FPGA configuration for\nRIO\/FlexRIO devices. This methodology has permitted a fast and easy deployment\nfor the new robust, scalable and maintainable software to support RIO devices\ninto the ISHP control architecture.",
        "To clean or not to clean? The solution to this dilemma is related to\nunderstanding the plasticiser migration which has a few practical implications\nfor the state of museum artefacts made of plasticised poly(vinyl chloride) -\nPVC and objects stored in their vicinity. The consequences of this process\nencompass aesthetic changes due to the presence of exudates and dust\ndeposition, an increase in air pollution and the development of mechanical\nstresses. Therefore, this paper discusses the plasticiser migration in PVC to\nprovide evidence and support the development of recommendations and guidelines\nfor conservators, collection managers and heritage scientists. Particularly,\nthe investigation is focused on the migration of the ortho-phthalates\nrepresenting the group of the most abundant plasticisers in PVC collections.\nThe predominance of inner diffusion or surface emission (evaporation)\ndetermining the rate-limiting step of the overall migration process is\nconsidered a fundament for understanding the potential environmental and\nmechanical risk. According to this concept, general correlations for various\northo-phthalates are proposed depending on their molar mass with the support of\nmolecular dynamics simulations and NMR diffusometry. The study reveals that for\nthe majority of the PVC objects in collections, the risk of accelerated\nmigration upon mild removal of surface plasticiser exudate is low. Thus,\nsurface cleaning would allow for diminishing dust deposition and air pollution\nby phthalate-emitting objects in a museum environment. Bearing in mind\nsimplicity and the need for fast decision-supporting solutions, the\nstep-by-step protocol for non-destructive identification and quantification of\nplasticisers in objects made of or containing plasticised PVC, determination of\nthe physical state of investigated artefacts and rate-limiting process of\nplasticiser migration is proposed.",
        "We propose novel non-relativistic fluid analogue models, that is dumb hole\nmodels, for extremal and near-extremal black holes. Further we study the\nback-reaction effects of analogue Hawking radiation emitted from these dumb\nholes. We discuss and quantify the reduction in the background fluid velocity\ncaused by radiation of Hawking phonons. In doing so, we speculate on the\nexistence of an emergent Hawking force which leads to the reduction in the\nbackground fluid velocity and which is produced as a consequence of phonon\nemission. In addition to the analogue gravity literature, our results might be\nof relevance to black hole pedagogy.",
        "As LLMs develop increasingly advanced capabilities, there is an increased\nneed to minimize the harm that could be caused to society by certain model\noutputs; hence, most LLMs have safety guardrails added, for example via\nfine-tuning. In this paper, we argue the position that current safety\nfine-tuning is very similar to a traditional cat-and-mouse game (or arms race)\nbetween attackers and defenders in cybersecurity. Model jailbreaks and attacks\nare patched with bandaids to target the specific attack mechanism, but many\nsimilar attack vectors might remain. When defenders are not proactively coming\nup with principled mechanisms, it becomes very easy for attackers to sidestep\nany new defenses. We show how current defenses are insufficient to prevent new\nadversarial jailbreak attacks, reward hacking, and loss of control problems. In\norder to learn from past mistakes in cybersecurity, we draw analogies with\nhistorical examples and develop lessons learned that can be applied to LLM\nsafety. These arguments support the need for new and more principled approaches\nto designing safe models, which are architected for security from the\nbeginning. We describe several such approaches from the AI literature."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cond-mat.dis-nn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
      ],
      "abstract":[
        "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "The timing and spectral properties of the 2022 outburst of SGR\n  J1935+2154 observed with NICER",
        "Evolution of Shock Structures and QPOs After Halting BHL Accretion onto\n  Kerr Black Hole",
        "Evolution of X-ray Gas in SN 1987A from 2007 to 2021: Ring Fading and\n  Ejecta Brightening Unveiled through Differential Emission Measure Analysis",
        "An Unusual Change in the Radio Jets of GRS 1915+105",
        "Populations of Neutron Star Ultraluminous X-ray Sources: Mind your b's\n  and B's",
        "Evidence for an Instability-Induced Binary Merger in the Double-Peaked,\n  Helium-Rich Type IIn Supernova 2023zkd",
        "Early Light Curve Excess in Type IIb Supernovae Observed by the ATLAS\n  Survey: Qualitative Constraints on Progenitor Systems",
        "The Galactic population of magnetars : a simulation-based inference\n  study",
        "Multi-messenger Emission by Magnetically Arrested Disks and Relativistic\n  Jets of Black Hole X-ray Binaries",
        "eRO-ExTra: eROSITA extragalactic non-AGN X-ray transients and variables\n  in eRASS1 and eRASS2",
        "Modeling fast X-ray variability around an accreting black hole",
        "Shock-cooling Constraints via Early-time Observations of the Type IIb SN\n  2022hnt",
        "Origin of holes and rings in the Green Monster of Cassiopeia A: Insights\n  from 3D magnetohydrodynamic simulations",
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection"
      ],
      "abstract":[
        "The magnetar SGR J1935+2154 entered a new active episode on October 10, 2022,\nwith X-ray bursts and enhanced persistent emission. At the tail of high burst\nrate interval, lasting several hours, radio bursts were detected, revealing the\nconnection between the X-ray activities and radio emissions. We analyzed\nobservations of SGR J1935+2154 for nearly three months, using data from Neutron\nStar Interior Composition Explorer (NICER). We report the timing and spectral\nresults following the onset of this outburst. In general, the X-ray flux of the\npersistent emission decays exponentially. While a flare is evident on the light\ncurve, a fast radio burst (FRB) was detected immediately following the peak of\nthis flare. We found a phase jump of pulse profile, with a deviation of\n$0.16\\pm0.03$ phase, which is related to the glitch. The spectra are well fit\nwith the combination of a blackbody and a power law model. The decay of the\noutburst is dominated by the drop of the non-thermal component, which also\nleads to the increase of thermal proportion. The photon index of the power law\nis inversely correlated with both the unabsorbed flux and the burst rate. We\nfind that unlike the large variety of the persistent emission around FRB\n221014, the X-ray properties are very stable when FRBs 221021 and 221201\nhappened. These results manifest the connection between glitch, phase jump,\nX-ray burst, and radio burst, crucial for studying the mutation in twisted\nmagnetic fields and constraining the trigger mechanism of radio bursts.",
        "One of the mechanisms responsible for disk formation around the black holes\nis Bondi-Hoyle-Lyttleton (BHL) accretion.The fact that BHL accretion can be\ninterrupted by various astrophysical phenomena, such as stellar winds or\nastrophysical jets, makes it crucial to study the behavior of shock cones\nformed by BHL accretion around black holes once the accretion process is\nhalted. Investigating the new plasma structures that emerge in these scenarios\ncan provide insights into observational results. In this context, a new plasma\nstructure forming around the Kerr black hole has been numerically modeled as a\nfunction of the black hole spin parameter and the asymptotic velocity of BHL\naccretion. The numerical analysis revealed that high spin (a\/M=0.9) and\nsupersonic flow ( M > 1) are necessary conditions for low-frequency\nquasi-periodic oscillations (LFQPOs) formation. On the other hand, the\nfundamental mode of the high-frequency quasi-periodic oscillations (HFQPOs) are\nfound to be independent of both the black hole spin and asymptotic velocity and\nare instead governed by general relativistic effects. Additionally, the study\ndemonstrated that for 3:2 and 2:1 resonance states to form, nonlinear couplings\nneeds to be occurred when the black hole rotates rapidly. These couplings\nproduce harmonic frequencies, providing an explanation for the observed\nquasi-periodic oscillation (QPO) resonances in black hole binaries. These\nfindings align with precession models and nonlinear resonance models, both of\nwhich play a crucial role in QPO generation. Finally, the LFQPOs and HFQPOs\nobtained from numerical simulations are consistent with the observed QPO\nfrequencies in the microquasars GRS 1915+105 and XTE J1550-564, as well as in\nthe AGN REJ1034+396, which harbors a supermassive black hole at its center.",
        "As the nearest supernova (SN) observed since Kepler's SN of 1604, SN 1987A\nprovides an unprecedented opportunity to study in detail the early evolution of\nsupernova remnants (SNRs). Despite extensive studies through both observations\nand simulations, there is still an urgent need for a more effective approach to\nintegrate the results from two sides. In this study, we conducted a detailed\ndifferential emission measure (DEM) analysis on the XMM-Newton observations\ntaken in 2007 to 2021 to characterize the continuous temperature structure of\nSN 1987A, which can be better compared with simulations. The X-ray plasma\nexhibit a temperature distribution with a major peak at $\\sim0.5$-$1$ keV and a\nhigh-temperature tail extending to $\\gtrsim5$ keV. The emission measure (EM) of\nthe major peak started to decline around 2014, while the EM of the tail\ncontinued increasing and appears to have formed a secondary peak at $\\sim3$-$5$\nkeV in recent years. Our DEM results consistent well with simulations, which\nhelp to further identify the major peak as originating from the equatorial ring\nand the secondary peak as arising from the newly shocked ejecta. Together with\nthe simulations, our DEM analysis reveals recent fading of the ring and\nbrightening of the ejecta in X-rays from SN 1987A. Additionally, we observed a\nrecent decrease in the centroid energy of Fe K line, providing further evidence\nof newly shocked ejecta.",
        "We compare Very Large Array observations of GRS 1915+105 made in 1994 and\n2023, with nearly three decades of difference. The source has experienced\nintriguing major changes. The position angle of the bipolar ejecta in the plane\nof the sky has increased counterclockwise by 24 degrees. The inclination angle\nof the flow with respect to the line of sight has increased by 17 degrees.\nAnalysis of GRS 1915+105 images over the years suggest that the observed\nchanges took place within a year or less. Our analysis indicates that during\n2023 the plane of the accretion disk was aligned with the line of sight, which\nmay explain the deep X-ray obscured state and the high mid-infrared luminosity\nobserved with JWST in that epoch. More recent 2024 observations imply that the\nposition angle of the ejecta has returned to its historic values. We suggest\nthat these abrupt changes could be due to the presence of an undetected\ntertiary component in the system. Future monitoring of the time evolution of\nthe source may further clarify the cause of these remarkable changes.",
        "Ultraluminous X-ray sources (ULXs) with neutron star (NS) accretors challenge\ntraditional accretion models, and have sparked a debate regarding the role of\ngeometrical beaming and strong magnetic fields (B). The reduction of the\nThomson cross-section in the presence of strong B, leads to a modification of\nthe Eddington limit, and therefore is expected to affect significantly the\nobservational appearance of NS-ULXs. We investigate the role of this\nmodification using population synthesis models, and explore its effects on the\nX-ray luminosity functions, spin-up rates, and outflow energetics of the\nobserved NS-ULXs. Our results show that the new prescription allows NS-ULXs to\nachieve super-Eddington luminosities with milder beaming compared to before,\nimproving the agreement with observations. In addition, it broadens the range\nof spin-up rates allowing for more diverse conditions in NS-ULXs in terms of\naccretion rates and magnetic fields. More importantly, the reduced beaming\nincreases the likelihood of observing the NS-ULXs within wind-powered nebulae\nsuch as NGC 5907 ULX-1. Our findings highlight the necessity of taking into\naccount B effects independently of the approach: geometrical beaming or strong\nB, and call for magnetospheric accretion prescriptions that can be integrated\nin population synthesis codes.",
        "We present ultraviolet to infrared observations of the extraordinary Type IIn\nsupernova 2023zkd (SN 2023zkd). Photometrically, it exhibits persistent and\nluminous precursor emission spanning $\\sim$4 years preceding discovery\n($M_r\\approx-15$ mag, 1,500~days in the observer frame), followed by a\nsecondary stage of gradual brightening in its final year. Post-discovery, it\nexhibits two photometric peaks of comparable brightness ($M_r\\lesssim-18.7$ mag\nand $M_r\\approx-18.4$ mag, respectively) separated by 240 days.\nSpectroscopically, SN 2023zkd exhibits highly asymmetric and multi-component\nBalmer and He I profiles that we attribute to ejecta interaction with\nfast-moving ($1,\\!000-2,\\!000\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$) He-rich polar\nmaterial and slow-moving ($\\sim$$400\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$)\nequatorially-distributed H-rich material. He II features also appear during the\nsecond light curve peak and evolve rapidly. Shock-driven models fit to the\nmulti-band photometry suggest that the event is powered by interaction with\n$\\sim$$5-6\\;M_{\\odot}$ of CSM, with $2-3\\;M_{\\odot}$ associated with each light\ncurve peak, expelled during mass-loss episodes $\\sim$$3-4$ and $\\sim$$1-2$\nyears prior to explosion. The observed precursor emission, combined with the\nextreme mass-loss rates required to power each light curve peak, favors either\nsuper-Eddington accretion onto a black hole or multiple long-lived eruptions\nfrom a massive star to luminosities that have not been previously observed. We\nconsider multiple progenitor scenarios for SN 2023zkd, and find that the\nbrightening optical precursor and inferred explosion properties are most\nconsistent with a massive ($M_{\\mathrm{ZAMS}}\\geq30\\;M_{\\odot}$) and\npartially-stripped He star undergoing an instability-induced merger with a\nblack hole companion.",
        "Type IIb supernovae (SNe IIb) often exhibit an early light curve excess (EE)\npreceding the main peak powered by radioactive nickel decay. The physical\norigin of this early emission remains an open question. Among the proposed\nscenarios, shock cooling emission-resulting from the interaction between the\nshockwave and extended envelopes-is considered the most plausible mechanism.\nThe frequency of these events remains unconstrained. This study aims to\nquantify the frequency of EE in SNe IIb and investigate its physical origin by\nanalyzing optical light curves from the Asteroid Terrestrial-impact Last Alert\nSystem (ATLAS) survey. We selected 74 SNe IIb from 153 spectroscopically\nclassified events in the Transient Name Server (TNS) database, observed by\nATLAS, with peak fluxes exceeding 150 {\\mu}Jy and explosion epoch uncertainties\nlower than six days. Using light curve model fitting and outlier analysis, we\nidentified SNe IIb exhibiting EE and analyzed their photometric properties. We\nfound 21 SNe IIb with EE, corresponding to a frequency of approximately 28-40%,\nwith the higher value obtained under the most stringent data cuts. The EE's\nduration and color evolution are consistent with shock cooling in extended\nhydrogen-rich envelopes. We also found that EE SNe IIb have longer rise times\nand faster post-peak decline rates than non-EE SNe IIb, while both groups share\nsimilar peak absolute magnitudes. Our findings suggest that EE and non-EE SNe\nIIb likely share similar initial progenitor masses but differ in ejecta mass\nproperties, potentially due to varying degrees of binary interaction. This\nstudy provides constraints on the evolutionary pathways of SNe IIb progenitors\nas compact stars with and without extended hydrogen envelopes.",
        "Population synthesis modeling of the observed dynamical and physical\nproperties of a population is a highly effective method for constraining the\nunderlying birth parameters and evolutionary tracks. In this work, we apply a\npopulation synthesis model to the canonical magnetar population to gain insight\ninto the parent population. We utilize simulation-based inference to reproduce\nthe observed magnetar population with a model which takes into account the\nsecular evolution of the force-free magnetosphere and magnetic field decay\nsimultaneously and self-consistently. Our observational constraints are such\nthat no magnetar is detected through their persistent emission when convolving\nthe simulated populations with the XMM-Newton EPIC-pn Galactic plane\nobservations, and that all of the $\\sim$30 known magnetars are discovered\nthrough their bursting activity in the last $\\sim50$ years. Under these\nconstraints, we find that, within 95 % credible intervals, the birth rate of\nmagnetars to be $1.8^{+2.6}_{-0.6}$ kyr$^{-1}$, and lead to having\n$10.7^{+18.8}_{-4.4}$ % of neutron stars born as magnetars. We also find a mean\nmagnetic field at birth ($\\mu_b$ is in T) $\\log\\left(\\mu_b\\right) =\n10.2^{+0.1}_{-0.2}$, a magnetic field decay slope $\\alpha_d = 1.9\n^{+0.9}_{-1.3}$, and timescale $\\tau_d = 17.9^{+24.1}_{-14.5}$ kyr, in broad\nagreement with previous estimates. We conclude this study by exploring\ndetection prospects: an all-sky survey with XMM-Newton would potentially allow\nto get around 7 periodic detections of magnetars, with approximately 150\nmagnetars exceeding XMM-Newton's flux threshold, and the upcoming AXIS\nexperiment should allow to double these detections.",
        "Black hole X-ray binaries (BHXBs) are observed in various wavelengths from\nradio to GeV gamma-ray. Several BHXBs, including MAXI J1820+070 and Cygnus X-1,\nare also found to emit ultrahigh-energy (UHE; photon energy $>$100 TeV) gamma\nrays. The origin and production mechanism of the multi-wavelength emission of\nBHXBs are under debate. We propose a scenario where relativistic particles from\nmagnetically arrested disks (MADs), which could form when BHXBs are in\nquiescent or hard states, produce UHE gamma rays, while electrons in the jets\nproduce GeV gamma-ray emission. Specifically, magnetic turbulence in MADs heats\nup and accelerates electrons and protons, while magnetic reconnection in jets\naccelerates electrons. Sub-PeV gamma rays and neutrinos are produced when\nrelativistic protons interact with the thermal protons and the radiation by\nthermal electrons in the disk. We discuss the perspectives of observing sub-PeV\nmulti-messenger signals from individual BHXBs. Finally, we evaluate the\nintegrated fluxes of the quiescent and hard-state BHXB population and find that\nBHXBs may contribute to the Galactic diffuse emission above $\\sim 100$ TeV.",
        "(Abridged) While previous X-ray studies showed the dominance of regular\nactive galactic nuclei (AGN) variability, a small fraction of sources arise\nfrom more exotic phenomena such as tidal disruption events (TDEs),\nquasi-periodic eruptions, or other short-lived events associated with\nsupermassive black hole accretion. This paper describes the systematic\nselection of X-ray extragalactic transients found in the first two eROSITA\nall-sky surveys (eRASS) that are not associated with known AGN prior to eROSITA\nobservations. We generated a variability sample from eRASS1 and eRASS2 (Dec.\n2019-Dec. 2020), which includes sources with a variability significance and a\nfractional amplitude larger than four, located in the Legacy Survey DR10 (LS10)\nfootprint. The properties of LS10 counterparts were used to exclude stars and\nknown AGN. The sample was additionally cleaned using pre-eROSITA\nclassifications, archival optical spectra, and archival X-ray data. The final\ncatalog eRO-ExTra includes 304 extragalactic eROSITA transients and variables\nnot associated with known AGN. More than 90% of sources have reliable LS10\noptical counterparts. For each source, we provide archival X-ray data from\nSwift, ROSAT, and XMM-Newton; the eROSITA long-term light curve (2-2.5 years)\nwith a light curve classification; as well as the best power law fit spectral\nresults at the peak eROSITA epoch. Reliable spectroscopic and photometric\nredshifts are provided for more than 80% of the sample. Several sources in the\ncatalog are known TDE candidates discovered by eROSITA. In addition, 31 sources\nare radio detected. The eRO-ExTra transients constitute a relatively clean\nparent sample of non-AGN variability phenomena associated with massive black\nholes. More than 95% of eRO-ExTra sources were discovered in X-rays with\neROSITA for the first time, which makes it a valuable resource for studying\nunique nuclear transients.",
        "X-ray inter-band time lags are observed during the outbursts of black hole\nX-ray binaries (BHXRBs). Timing analysis of fast variability in low Fourier\nfrequency bands shows that high-energy photons lag behind low-energy photons, a\nphenomenon referred to as hard lag. Conversely, in high Fourier frequency\nbands, low-energy photons lag behind high-energy photons, known as soft lag.\nThis frequency-dependent lag spectrum suggests that the lags arise from\ndifferent physical processes. Notably, a trend has been observed wherein the\nlags shift towards shorter timescales during the rising hard state, indicating\nan evolution in the inner accretion flow. In this study, we simulate these\ninter-band lags by conducting Monte Carlo simulations of the rapid variability\nwithin the geometry of a jet base corona. We consider both inward propagating\naccretion rate fluctuations and reverberation (light crossing) delays in our\nsimulations. We successfully reproduce both low-frequency hard lags and\nhigh-frequency soft lags in a self-consistent manner. We replicate the observed\nevolution of the frequency-dependent lag spectra by varying the geometrical\nscale of the corona and the viscous frequency of the disc. Finally, we discuss\nthe potential of a spherical corona and emphasize that polarization\nobservations from the Imaging X-ray Polarimetry Explorer (IXPE) and the\nenhanced X-ray Timing and Polarimetry mission (eXTP) will be crucial for\ndistinguishing the corona's geometry in future studies.",
        "We report the results of a rapid follow-up campaign on the Type IIb Supernova\n(SN) 2022hnt. We present a daily, multi-band, photometric follow-up using the\nLas Cumbres Observatory, the Zwicky Transient Facility, the orbiting\n\\textit{Swift} observatory, and the Asteroid Terrestrial-impact Last Alert\nSystem (ATLAS). A distinctive feature in the light curve of SN 2022hnt and\nother IIb SNe is an early narrow peak prior to the ${}^{56}$Ni peak caused by\nrapid shock cooling of the hydrogen envelope, which can serve as an important\nprobe of the properties of the massive progenitor star in the moments before\nexplosion. Using SN 2022hnt as a case study, we demonstrate a framework of\nconsiderations for the application of shock cooling models to type IIb SNe,\noutlining a consistent procedure for future surveys of Type IIb SNe progenitor\nand explosion properties. \\hll{We fit several recent models of shock-cooling\nemission and obtain progenitor radii between $\\sim50$ and $\\sim100$ $R_\\odot$,\nas well as hydrogen-enriched envelope masses between $\\sim0.01$ and $\\sim0.1$\n$M_\\odot$, both consistent with values for other IIb SNe. One of these models\nis the model of \\cite{Morag2023}, marking the first time this model has been\napplied to a Type IIb SN.} We evaluate contrasting predictions between\nshock-cooling models to construct a fiducial parameter set which can be used\nfor comparison to other SNe. Finally, we investigate the possibility of\nextended wind breakout or precursor emission captured in the earliest\ndetections.",
        "[Abridged] Cassiopeia A (Cas A) provides a unique opportunity to study\nsupernova (SN) dynamics and interactions with the circumstellar medium (CSM).\nRecent JWST observations revealed the \"Green Monster\" (GM), a structure with a\nlikely CSM origin. We investigate its pockmarked morphology, characterized by\ncircular holes and rings, by examining the role of small-scale ejecta\nstructures interacting with a dense circumstellar shell. We adopted a\nneutrino-driven SN model to trace the evolution of its explosion from core\ncollapse to the age of the Cas A remnant using high-resolution 3D\nmagnetohydrodynamic simulations. Besides other processes, the simulations\ninclude self-consistent calculations of radiative losses, accounting for\ndeviations from electron-proton temperature equilibration and ionization\nequilibrium, as well as the ejecta composition derived from the SN. The GM's\nmorphology is reproduced by dense ejecta clumps and fingers interacting with an\nasymmetric, forward-shocked circumstellar shell. The clumps and fingers form by\nhydrodynamic instabilities growing at the interface between SN ejecta and\nshocked CSM. Radiative cooling accounting for effects of non-equilibrium of\nionization enhances the ejecta fragmentation, forming dense knots and thin\nfilamentary structures that penetrate the shell, producing a network of holes\nand rings with properties similar to those observed. The origin of the holes\nand rings in the GM can be attributed to the interaction of ejecta with a\nshocked circumstellar shell. By constraining the timing of this interaction and\nanalyzing the properties of these structures, we provide a distinction of this\nscenario from an alternative hypothesis, which attributes these features to\nfast-moving ejecta knots penetrating the shell ahead of the forward shock.",
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning",
    "start_abstract":"Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cond-mat.dis-nn"
      ]
    },
    "list":{
      "title":[
        "Scale-free localization versus Anderson localization in unidirectional\n  quasiperiodic lattices",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "Impact of Nonreciprocal Hopping on Localization in Non-Hermitian\n  Quasiperiodic Systems",
        "Hopfield model with quasi-diagonal connection matrix",
        "Relationship between the shear moduli and defect-induced structural\n  relaxation ofhigh-entropy metallic glasses",
        "Exact mobility edges in quasiperiodic network models with slowly varying\n  potentials",
        "Electrical conductivity of conductive films based on random metallic\n  nanowire networks",
        "Screening and localization in the nonlinear Anderson problem",
        "Topological mechanical neural networks as classifiers through in situ\n  backpropagation learning",
        "The Capacity of Modern Hopfield Networks under the Data Manifold\n  Hypothesis",
        "Tentaclelike spectra and bound states in Hatano-Nelson chain with\n  long-range impurity coupling",
        "Isolating the hard core of phaseless inference: the Phase selection\n  formulation",
        "Critical exponents of the spin glass transition in a field at zero\n  temperature",
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection"
      ],
      "abstract":[
        "Scale-free localization emerging in non-Hermitian physics has recently\ngarnered significant attention. In this work, we explore the interplay between\nscale-free localization and Anderson localization by investigating a\nunidirectional quasiperiodic model with generalized boundary conditions. We\nderive analytical expressions of Lyapunov exponent from the bulk equations.\nTogether with the boundary equation, we can determine properties of eigenstates\nand spectrum and establish their exact relationships with the quasiperiodic\npotential strength and boundary parameter. While eigenstates exhibit scale-free\nlocalization in the weak disorder regime, they become localized in the strong\ndisorder regime. The scale-free and Anderson localized states satisfy the\nboundary equation in distinct ways, leading to different localization\nproperties and scaling behaviors. Generalizing our framework, we design a model\nwith exact energy edges separating the scale-free and Anderson localized states\nvia the mosaic modulation of quasiperiodic potentials. Our models can be\nrealized experimentally in electric circuits.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "We study the non-Hermitian Aubry-Andr\\'e-Harper model, incorporating complex\nphase modulation, unmodulated and modulated nonreciprocal hopping. Using\nAvila's global theory, we derive analytical phase boundaries and map out the\nphase diagrams, revealing extended, localized, critical, and skin phases unique\nto non-Hermitian systems. For complex phase modulation, we determine\nlocalization lengths through Lyapunov exponents and show that topological\ntransitions align with localization transitions. In the nonreciprocal case, we\nuse similarity transformations to confirm phase boundaries consistent with\nAvila's theory and uncover asymmetric localization behaviors. Importantly,\nmodulated nonreciprocal hopping transforms both extended and critical phases\ninto skin phases under open boundary conditions. These results highlight the\ninterplay between topology, localization, and non-Hermitian effects, offering\nnew perspectives on quasiperiodic systems.",
        "We analyze a Hopfield neural network with a quasi-diagonal connection matrix.\nWe use the term \"quasi-diagonal matrix\" to denote a matrix with all elements\nequal zero except the elements on the first super- and sub-diagonals of the\nprinciple diagonal. The nonzero elements are arbitrary real numbers. Such\nmatrix generalizes the well-known connection matrix of the one dimensional\nIsing model with open boundary conditions where all nonzero elements equal +1.\nWe present a simple description of the fixed points of the Hopfield neural\nnetwork and their dependence on the matrix elements. The obtained results also\nallow us to analyze the cases of a) the nonzero elements constitute arbitrary\nsuper- and sub-diagonals and b) periodic boundary conditions.",
        "We performed high-frequency shear modulus and calorimetry measurements on\nseven high-entropy metallic glasses (HEMGs) in the initial, relaxed and\ncrystalline states. It is shown that the shear modulus of HEMGs is\nintrinsically related with the concentration of defects responsible for\nstructural relaxation. In the absence of structural relaxation, temperature\ncoefficient of shear modulus of glass equals to that of the maternal crystal.\nAll found regularities are governed by a single equation.",
        "Quasiperiodic potentials without self-duality are always hard to derive the\nexact mobility edges (MEs). Here, we propose a new class of network models with\nexactly solvable MEs, characterized by quasiperiodic slowly varying potentials\nthat do not exhibit hidden self-duality. We present two methods to derive the\nMEs, the first involves integrating out the periodic sites to obtain an\neffective Hamiltonian with effective potential $g(E)V$ and effective\neigenenergy $f(E)$, which directly yields the MEs at $f(E) = \\pm(2t\\pm g(E)V)$,\nand the second is to connect the localized-delocalized transition points of the\nquasiperiodic slowly varying models and the real-complex transition points of\nthe eigenvalue equations. To illustrate this, we take quasiperiodic mosaic\nslowly varying models as examples, and we find that the MEs obtained from the\ntwo methods are the same. Furthermore, we generalize our methods to\nquasiperiodic network models and propose a possible experimental realization\nbased on optical waveguide systems, showing that the Anderson transition can be\nobserved even using small physical systems (with $L = 50 - 100$). Our results\nmay provide insight into understanding and realizing exact MEs in experiments.",
        "Using computer simulation, we investigated the dependence of the electrical\nconductivity of random two-dimensional systems of straight nanowires on the\nmain parameters. Both the resistance of the conductors and the resistance of\nthe contacts between them were taken into account. The dependence of the\nresistance, $R$, between network nodes on the distance between nodes, $r$, is\n$R(r) = R_\\Box\/\\pi \\ln r + \\mathrm{const}$, where $R_\\Box$ is the sheet\nresistance.",
        "We resolve an existing question concerning the localization of a wave packet\nby random potential in the presence of weak nonlinearity. The problem has\ngained considerable interest in the literature, and it continues to attract\nattention due to its connection with the general properties of behavior of\nsystems with competition between nonlinearity, nonlocality and randomness. We\nfind that the nonlinearly localized state occurs through a finite polarization\nresponse from the lattice well beyond the assumptions of a perturbation-theory\napproach. For the vanishing polarization response the nonlinear localization\nlength diverges permitting unlimited spreading of the nonlinear field.",
        "Recently, a new frontier in computing has emerged with physical neural\nnetworks(PNNs) harnessing intrinsic physical processes for learning. Here, we\nexplore topological mechanical neural networks(TMNNs) inspired by the quantum\nspin Hall effect(QSHE) in topological metamaterials, for machine learning\nclassification tasks. TMNNs utilize pseudospin states and the robustness of the\nQSHE, making them damage-tolerant for binary classification. We first\ndemonstrate data clustering using untrained TMNNs. Then, for specific tasks, we\nderive an in situ backpropagation algorithm - a two-step, local-rule method\nthat updates TMNNs using only local information, enabling in situ physical\nlearning. TMNNs achieve high accuracy in classifications of Iris flowers,\nPenguins, and Seeds while maintaining robustness against bond pruning.\nFurthermore, we demonstrate parallel classification via frequency-division\nmultiplexing, assigning different tasks to distinct frequencies for enhanced\nefficiency. Our work introduces in situ backpropagation for wave-based\nmechanical neural networks and positions TMNNs as promising neuromorphic\ncomputing hardware for classification tasks.",
        "We generalize the computation of the capacity of exponential Hopfield model\nfrom Lucibello and M\\'ezard (2024) to more generic pattern ensembles, including\nbinary patterns and patterns generated from a hidden manifold model.",
        "In non-Hermitian systems, the energy spectra and eigenstates exhibit high\nsensitivity to boundary conditions, lattice geometries, and local impurities.\nIn this paper, we study the effect of long-range impurity coupling, located far\nfrom the boundaries, on the paradigmatic non-Hermitian Hatano-Nelson model.\nThrough exact analytical treatment, we reveal the intriguing tentacle-like\nspectral structures that emerge from the otherwise Bloch or non-Bloch spectra\nunder periodic or open boundary conditions, respectively. We show that these\nspectral tentacles are associated with emergent bound states near the impurity,\nwith their number determined by the coupling range. We further determine the\nlocalization length of these tentacled states using the transfer matrix. Our\nwork indicates that the long-range impurity coupling cannot be treated as a\nmere perturbative effect and holds promise for state manipulations in\nnon-Hermitian systems.",
        "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency.",
        "We analyze the spin glass transition in a field in finite dimension $D$ below\nthe upper critical dimension directly at zero temperature using a recently\nintroduced perturbative loop expansion around the Bethe lattice solution. The\nexpansion is generated by the so-called $M$-layer construction, and it has\n$1\/M$ as the associated small parameter. Computing analytically and numerically\nthese non-standard diagrams at first order in the $1\/M$ expansion, we construct\nan $\\epsilon$-expansion around the upper critical dimension $D_\\text{uc}=8$,\nwith $\\epsilon=D_\\text{uc}-D$. Following standard field theoretical methods, we\ncan write a $\\beta$ function, finding a new zero-temperature fixed-point\nassociated with the spin glass transition in a field in dimensions $D<8$. We\nare also able to compute, at first order in the $\\epsilon$-expansion, the three\nindependent critical exponents characterizing the transition, plus the\ncorrection-to-scaling exponent.",
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b9",
    "start_title":"Kinetic description and convergence analysis of genetic algorithms for global optimization",
    "start_abstract":"Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Genetic Algorithms + Data Structures = Evolution Programs"
      ],
      "abstract":[
        "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
      ],
      "categories":[
        "cs.DS"
      ]
    },
    "list":{
      "title":[
        "QuaRs: A Transform for Better Lossless Compression of Integers",
        "Minimizers in Semi-Dynamic Strings",
        "Maximum Coverage $k$-Antichains and Chains: A Greedy Approach",
        "On Deleting Vertices to Reduce Density in Graphs and Supermodular\n  Functions",
        "PtrHash: Minimal Perfect Hashing at RAM Throughput",
        "Fine-Grained Complexity of Computing Degree-Constrained Spanning Trees",
        "Efficient parameterized approximation",
        "Faster parameterized algorithm for 3-Hitting Set",
        "Exploring Word-Representable Temporal Graphs",
        "Potential-Based Greedy Matching for Dynamic Delivery Pooling",
        "Tight Bounds for some Classical Problems Parameterized by Cutwidth",
        "Faster Approximation Algorithms for k-Center via Data Reduction",
        "Graph-Based Algorithms for Diverse Similarity Search",
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "Bargaining with transfers",
        "Constraints on extended axion structures from the lensing of fast radio\n  bursts",
        "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes",
        "Bangla Fake News Detection Based On Multichannel Combined CNN-LSTM",
        "MAIA: A new detector concept for a 10 TeV muon collider",
        "High-pressure growth effect on the properties of high-Tc iron-based\n  superconductors: A short review",
        "The Dead Internet Theory: A Survey on Artificial Interactions and the\n  Future of Social Media",
        "Talking to GDELT Through Knowledge Graphs",
        "The First Chemical Census the Milky Way's Nuclear Star Cluster",
        "Seismic wavefield solutions via physics-guided generative neural\n  operator",
        "Drop size distribution from laboratory experiments based on single-drop\n  fragmentation and comparison with aerial in-situ measurements",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Temperature-Distance Relations in Casimir Physics",
        "Formation of Singularity and Apparent Horizon for Dissipative Collapse\n  in $f(R,T)$ Theory of Gravity"
      ],
      "abstract":[
        "The rise of integer-valued data, partly driven by the Internet of Things\n(IoT), has increased demand for efficient compression methods to reduce storage\nand transmission costs. Existing, speed-oriented methods rely on the\n``smaller-numbers-less-bits'' principle, assuming unimodal distributions\ncentered around zero. This assumption is often violated in practice, leading to\nsuboptimal compression. We propose QuaRs, a transformation that reshapes\narbitrary distributions into unimodal ones centered around zero, improving\ncompatibility with fast integer compression methods. QuaRs remaps data based on\nquantiles, assigning smaller magnitudes to frequent values. The method is fast,\ninvertible, and has sub-quadratic complexity. QuaRs enhances compression\nefficiency, even for challenging distributions, while integrating seamlessly\nwith existing techniques.",
        "Minimizers sampling is one of the most widely-used mechanisms for sampling\nstrings. Let $S=S[0]\\ldots S[n-1]$ be a string over an alphabet $\\Sigma$. In\naddition, let $w\\geq 2$ and $k\\geq 1$ be two integers and\n$\\rho=(\\Sigma^k,\\leq)$ be a total order on $\\Sigma^k$. The minimizer of window\n$X=S[i\\mathinner{.\\,.} i+w+k-2]$ is the smallest position in $[i,i+w-1]$ where\nthe smallest length-$k$ substring of $S[i\\mathinner{.\\,.} i+w+k-2]$ based on\n$\\rho$ starts. The set of minimizers for all $i\\in[0,n-w-k+1]$ is the set\n$\\mathcal{M}_{w,k,\\rho}(S)$ of the minimizers of $S$. The set\n$\\mathcal{M}_{w,k,\\rho}(S)$ can be computed in $\\mathcal{O}(n)$ time. The\nfolklore algorithm for this computation computes the minimizer of every window\nin $\\mathcal{O}(1)$ amortized time using $\\mathcal{O}(w)$ working space. It is\nthus natural to pose the following two questions:\n  Question 1: Can we efficiently support other dynamic updates on the window?\n  Question 2: Can we improve on the $\\mathcal{O}(w)$ working space?\n  We answer both questions in the affirmative:\n  1. We term a string $X$ semi-dynamic when one is allowed to insert or delete\na letter at any of its ends. We show a data structure that maintains a\nsemi-dynamic string $X$ and supports minimizer queries in $X$ in\n$\\mathcal{O}(1)$ time with amortized $\\mathcal{O}(1)$ time per update\noperation.\n  2. We show that this data structure can be modified to occupy strongly\nsublinear space without increasing the asymptotic complexity of its operations.\nTo the best of our knowledge, this yields the first algorithm for computing\n$\\mathcal{M}_{w,k,\\rho}(S)$ in $\\mathcal{O}(n)$ time using\n$\\mathcal{O}(\\sqrt{w})$ working space.\n  We complement our theoretical results with a concrete application and an\nexperimental evaluation.",
        "Given an input acyclic digraph $G = (V,E)$ and a positive integer $k$, the\nproblem of Maximum Coverage $k$-Antichains (resp., Chains) denoted as MA-$k$\n(resp., MC-$k$) asks to find $k$ sets of pairwise unreachable vertices, known\nas antichains (resp., $k$ subsequences of paths, known as chains), maximizing\nthe number of vertices covered by these antichains (resp. chains). While MC-$k$\nhas been recently solved in (almost) optimal $O(|E|^{1+o(1)})$ time [Kogan and\nParter, ICALP 2022], the fastest known algorithm for MA-$k$ is a recent\n$(k|E|)^{1+o(1)}$-time solution [Kogan and Parter, ESA 2024] as well as a $1\/2$\napproximation running in $|E|^{1+o(1)}$ time in the same paper. In this paper,\nwe leverage a paths-based proof of the Greene-Kleitmann (GK) theorem with the\nhelp of the greedy algorithm for set cover and recent advances on fast\nalgorithms for flows and shortest paths to obtain the following results for\nMA-$k$:\n  - The first (exact) algorithm running in $|E|^{1+o(1)}$ time, hence\nindependent in $k$.\n  - A randomized algorithm running in $\\tilde{O}(\\alpha_k|E|)$ time, where\n$\\alpha_k$ is the size of the optimal solution. That is, a near-linear\nparameterized running time, generalizing the result of [M\\\"akinen et al., ACM\nTALG] obtained for $k=1$.\n  - An approximation algorithm running in time $O(\\alpha_1^2|V| +\n(\\alpha_1+k)|E|)$ with approximation ratio of $(1-1\/e) > 0.63 > 1\/2$.\n  Our last two solutions rely on the use of greedy set cover, first exploited\nin [Felsner et al., Order 2003] for chains, which we now apply to antichains.\nWe complement these results with two examples (one for chains and one for\nantichains) showing that, for every $k \\ge 2$, greedy misses a $1\/4$ portion of\nthe optimal coverage. We also show that greedy is a $\\Omega(\\log{|V|})$ factor\naway from minimality when required to cover all vertices: previously unknown\nfor sets of chains or antichains.",
        "We consider deletion problems in graphs and supermodular functions where the\ngoal is to reduce density. In Graph Density Deletion (GraphDD), we are given a\ngraph $G=(V,E)$ with non-negative vertex costs and a non-negative parameter\n$\\rho \\ge 0$ and the goal is to remove a minimum cost subset $S$ of vertices\nsuch that the densest subgraph in $G-S$ has density at most $\\rho$. This\nproblem has an underlying matroidal structure and generalizes several classical\nproblems such as vertex cover, feedback vertex set, and pseudoforest deletion\nset for appropriately chosen $\\rho \\le 1$ and all of these classical problems\nadmit a $2$-approximation. In sharp contrast, we prove that for every fixed\ninteger $\\rho > 1$, GraphDD is hard to approximate to within a logarithmic\nfactor via a reduction from Set Cover, thus showing a phase transition\nphenomenon. Next, we investigate a generalization of GraphDD to monotone\nsupermodular functions, termed Supermodular Density Deletion (SupmodDD). In\nSupmodDD, we are given a monotone supermodular function $f:2^V \\rightarrow\n\\mathbb{Z}_{\\ge 0}$ via an evaluation oracle with element costs and a\nnon-negative integer $\\rho \\ge 0$ and the goal is remove a minimum cost subset\n$S \\subseteq V$ such that the densest subset according to $f$ in $V-S$ has\ndensity at most $\\rho$. We show that SupmodDD is approximation equivalent to\nthe well-known Submodular Cover problem; this implies a tight logarithmic\napproximation and hardness for SupmodDD; it also implies a logarithmic\napproximation for GraphDD, thus matching our inapproximability bound. Motivated\nby these hardness results, we design bicriteria approximation algorithms for\nboth GraphDD and SupmodDD.",
        "Given a set $S$ of $n$ keys, a minimal perfect hash function (MPHF) is a\ncollision-free bijective map $\\mathsf{H_{mphf}}$ from $S$ to $\\{0, \\dots,\nn-1\\}$. This work presents a (minimal) perfect hash function that first\nprioritizes query throughput, while also allowing efficient construction for\n$10^9$ or more elements using 2.4 bits of memory per key.\n  Both PTHash and PHOBIC first map all $n$ keys to $n\/\\lambda < n$ buckets.\nThen, each bucket stores a pilot that controls the final hash value of the keys\nmapping to it. PtrHash builds on this by using 1) fixed-width (uncompressed)\n8-bit pilots, 2) a construction algorithm similar to cuckoo-hashing to find\nsuitable pilot values. Further, it 3) uses the same number of buckets and slots\nfor each part, with 4) a single remap table to map intermediate positions $\\geq\nn$ to $<n$, 5) encoded using per-cacheline Elias-Fano coding. Lastly, 6)\nPtrHash support streaming queries, where we use prefetching to answer a stream\nof multiple queries more efficiently than one-by-one processing.\n  With default parameters, PtrHash takes 2.40 bits per key. On 300 million\nstring keys, PtrHash is as fast or faster to build than other MPHFs, and at\nleast $1.75\\times$ faster to query. When streaming multiple queries, this\nimproves to $3.1\\times$ speedup over the fastest alternative, while also being\nsignificantly faster to construct. When using $10^9$ integer keys instead,\nquery times are as low as 12 ns\/key when iterating in a for loop, or even down\nto 8 ns\/key when using the streaming approach, within $10\\%$ of the maximum\nmemory-bound throughput.",
        "We investigate the computation of minimum-cost spanning trees satisfying\nprescribed vertex degree constraints: Given a graph $G$ and a constraint\nfunction $D$, we ask for a (minimum-cost) spanning tree $T$ such that for each\nvertex $v$, $T$ achieves a degree specified by $D(v)$. Specifically, we\nconsider three kinds of constraint functions ordered by their generality -- $D$\nmay either assign each vertex to a list of admissible degrees, an upper bound\non the degrees, or a specific degree. Using a combination of novel techniques\nand state-of-the-art machinery, we obtain an almost-complete overview of the\nfine-grained complexity of these problems taking into account the most\nclassical graph parameters of the input graph $G$. In particular, we present\nSETH-tight upper and lower bounds for these problems when parameterized by the\npathwidth and cutwidth, an ETH-tight algorithm parameterized by the\ncliquewidth, and a nearly SETH-tight algorithm parameterized by treewidth.",
        "Many problems are NP-hard and, unless P = NP, do not admit polynomial-time\nexact algorithms. The fastest known exact algorithms exactly usually take time\nexponential in the input size. Much research effort has gone into obtaining\nfaster exact algorithms for instances that are sufficiently well-structured,\ne.g., through parameterized algorithms with running time $f(k)\\cdot\nn^{\\mathcal{O}(1)}$ where n is the input size and k quantifies some structural\nproperty such as treewidth. When k is small, this is comparable to a\npolynomial-time exact algorithm and outperforms the fastest exact\nexponential-time algorithms for a large range of k.\n  In this work, we are interested instead in leveraging instance structure for\npolynomial-time approximation algorithms. We aim for polynomial-time algorithms\nthat produce a solution of value at most or at least (depending on minimization\nvs. maximization) $c\\mathrm{OPT}\\pm f(k)$ where c is a constant. Unlike for\nstandard parameterized algorithms, we do not assume that structural information\nis provided with the input. Ideally, we can obtain algorithms with small\nadditive error, i.e., $c=1$ and $f(k)$ is polynomial or even linear in $k$. For\nsmall k, this is similarly comparable to a polynomial-time exact algorithm and\nwill beat general case approximation for a large range of k.\n  We study Vertex Cover, Connected Vertex Cover, Chromatic Number, and Triangle\nPacking. The parameters we consider are the size of minimum modulators to graph\nclasses on which the respective problem is tractable. For most\nproblem-parameter combinations we give algorithms that compute a solution of\nsize at least or at most $\\mathrm{OPT}\\pm k$. In the case of Vertex Cover, most\nof our algorithms are tight under the Unique Games Conjecture and provide\nbetter approximation guarantees than standard 2-approximations if the modulator\nis smaller than the optimum solution.",
        "In the 3-Hitting Set problem, the input is a hypergraph $G$ such that the\nsize of every hyperedge of $G$ is at most 3, and an integers $k$, and the goal\nis to decide whether there is a set $S$ of at most $k$ vertices such that every\nhyperedge of $G$ contains at least one vertex from $S$. In this paper we give\nan $O^*(2.0409^k)$-time algorithm for 3-Hitting Set.",
        "Word-representable graphs are a subset of graphs that may be represented by a\nword $w$ over an alphabet composed of the vertices in the graph. In such\ngraphs, an edge exists if and only if the occurrences of the corresponding\nvertices alternate in the word $w$. We generalise this notion to temporal\ngraphs, constructing timesteps by partitioning the word into factors\n(contiguous subwords) such that no factor contains more than one copy of any\ngiven symbol. With this definition, we study the problem of \\emph{exploration},\nasking for the fastest schedule such that a given agent may explore all $n$\nvertices of the graph. We show that if the corresponding temporal graph is\nconnected in every timestep, we may explore the graph in $2\\delta n$ timesteps,\nwhere $\\delta$ is the lowest degree of any vertex in the graph. In general, we\nshow that, for any temporal graph represented by a word of length at least\n$n(2dn + d)$, with a connected underlying graph, the full graph can be explored\nin $2 d n$ timesteps, where $d$ is the diameter of the graph. We show this is\nasymptotically optimal by providing a class of graphs of diameter $d$ requiring\n$\\Omega(d n)$ timesteps to explore, for any $d \\in [1, n]$.",
        "We study the problem of pooling together delivery orders into a single trip,\na strategy widely adopted by platforms to reduce total travel distance. Similar\nto other dynamic matching settings, the pooling decisions involve a trade-off\nbetween immediate reward and holding jobs for potentially better opportunities\nin the future. In this paper, we introduce a new heuristic dubbed\npotential-based greedy (PB), which aims to keep longer-distance jobs in the\nsystem, as they have higher potential reward (distance savings) from being\npooled with other jobs in the future. This algorithm is simple in that it\ndepends solely on the topology of the space, and does not rely on forecasts or\npartial information about future demand arrivals. We prove that PB\nsignificantly improves upon a naive greedy approach in terms of worst-case\nperformance on the line. Moreover, we conduct extensive numerical experiments\nusing both synthetic and real-world order-level data from the Meituan platform.\nOur simulations show that PB consistently outperforms not only the naive greedy\nheuristic but a number of benchmark algorithms, including (i) batching-based\nheuristics that are widely used in practice, and (ii) forecast-aware heuristics\nthat are given the correct probability distributions (in synthetic data) or a\nbest-effort forecast (in real data). We attribute the surprising unbeatability\nof PB to the fact that it is specialized for rewards defined by distance saved\nin delivery pooling.",
        "Cutwidth is a widely studied parameter that quantifies how well a graph can\nbe decomposed along small edge-cuts. It complements pathwidth, which captures\ndecomposition by small vertex separators, and it is well-known that cutwidth\nupper-bounds pathwidth. The SETH-tight parameterized complexity of problems on\ngraphs of bounded pathwidth (and treewidth) has been actively studied over the\npast decade while for cutwidth the complexity of many classical problems\nremained open.\n  For Hamiltonian Cycle, it is known that a $(2+\\sqrt{2})^{\\operatorname{pw}}\nn^{O(1)}$ algorithm is optimal for pathwidth under SETH~[Cygan et al.\\ JACM\n2022]. Van Geffen et al.~[J.\\ Graph Algorithms Appl.\\ 2020] and Bojikian et\nal.~[STACS 2023] asked which running time is optimal for this problem\nparameterized by cutwidth. We answer this question with\n$(1+\\sqrt{2})^{\\operatorname{ctw}} n^{O(1)}$ by providing matching upper and\nlower bounds. Second, as our main technical contribution, we close the gap left\nby van Heck~[2018] for Partition Into Triangles (and Triangle Packing) by\nimproving both upper and lower bound and getting a tight bound of\n$\\sqrt[3]{3}^{\\operatorname{ctw}} n^{O(1)}$, which to our knowledge exhibits\nthe only known tight non-integral basis apart from Hamiltonian Cycle. We show\nthat cuts inducing a disjoint union of paths of length three (unions of\nso-called $Z$-cuts) lie at the core of the complexity of the problem -- usually\nlower-bound constructions use simpler cuts inducing either a matching or a\ndisjoint union of bicliques. Finally, we determine the optimal running times\nfor Max Cut ($2^{\\operatorname{ctw}} n^{O(1)}$) and Induced Matching\n($3^{\\operatorname{ctw}} n^{O(1)}$) by providing matching lower bounds for the\nexisting algorithms -- the latter result also answers an open question for\ntreewidth by Chaudhary and Zehavi~[WG 2023].",
        "We study efficient algorithms for the Euclidean $k$-Center problem, focusing\non the regime of large $k$. We take the approach of data reduction by\nconsidering $\\alpha$-coreset, which is a small subset $S$ of the dataset $P$\nsuch that any $\\beta$-approximation on $S$ is an $(\\alpha +\n\\beta)$-approximation on $P$. We give efficient algorithms to construct\ncoresets whose size is $k \\cdot o(n)$, which immediately speeds up existing\napproximation algorithms. Notably, we obtain a near-linear time\n$O(1)$-approximation when $k = n^c$ for any $0 < c < 1$. We validate the\nperformance of our coresets on real-world datasets with large $k$, and we\nobserve that the coreset speeds up the well-known Gonzalez algorithm by up to\n$4$ times, while still achieving similar clustering cost. Technically, one of\nour coreset results is based on a new efficient construction of consistent\nhashing with competitive parameters. This general tool may be of independent\ninterest for algorithm design in high dimensional Euclidean spaces.",
        "Nearest neighbor search is a fundamental data structure problem with many\napplications in machine learning, computer vision, recommendation systems and\nother fields. Although the main objective of the data structure is to quickly\nreport data points that are closest to a given query, it has long been noted\n(Carbonell and Goldstein, 1998) that without additional constraints the\nreported answers can be redundant and\/or duplicative. This issue is typically\naddressed in two stages: in the first stage, the algorithm retrieves a (large)\nnumber $r$ of points closest to the query, while in the second stage, the $r$\npoints are post-processed and a small subset is selected to maximize the\ndesired diversity objective. Although popular, this method suffers from a\nfundamental efficiency bottleneck, as the set of points retrieved in the first\nstage often needs to be much larger than the final output.\n  In this paper we present provably efficient algorithms for approximate\nnearest neighbor search with diversity constraints that bypass this two stage\nprocess. Our algorithms are based on popular graph-based methods, which allows\nus to \"piggy-back\" on the existing efficient implementations. These are the\nfirst graph-based algorithms for nearest neighbor search with diversity\nconstraints. For data sets with low intrinsic dimension, our data structures\nreport a diverse set of $k$ points approximately closest to the query, in time\nthat only depends on $k$ and $\\log \\Delta$, where $\\Delta$ is the ratio of the\ndiameter to the closest pair distance in the data set. This bound is\nqualitatively similar to the best known bounds for standard (non-diverse)\ngraph-based algorithms. Our experiments show that the search time of our\nalgorithms is substantially lower than that using the standard two-stage\napproach.",
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "We propose a solution to the problem of bargaining with transfers, along with\nan axiomatisation of the solution. Inefficient allocations in the bargaining\nset can influence the solution, but are discounted relative to more efficient\nones. The key axioms are additivity and a property we call \\emph{inverse\nmonotonicity}, which states that adding an allocation to the bargaining set\nthat is worse for a given player than the initial solution cannot benefit that\nplayer.",
        "Axions are hypothetical pseudoscalar particles that have been regarded as\npromising dark matter (DM) candidates. On the other hand, extended compact\nobjects such as axion stars, which are supported by gravity and axion self\ninteractions, may have also been formed in the early Universe and comprise part\nof DM. In this work, we consider the lensing of electromagnetic signals from\ndistant sources by axion stars, as a way to constrain the properties of axion\nstars and fundamental axion parameters. Accounting for the effect of the finite\nsize of the axion star, we study the lensing effect induced by gravity and the\naxion-photon coupling. The latter effect is frequency dependent, and is\nrelevant in the low frequency band, which motivates the use of fast radio burst\n(FRB) signals as a probe. We calculate the predicted number of lensed FRB\nevents by specifying the fundamental axion parameters, axion star radial\nprofile, fraction of DM residing in axion stars, and imposing lensing criteria\nbased on the flux ratio and time delay between the brightest images from\nlensing. Assuming an optimistic case of $10^4$ observed FRB events, and a\ntiming resolution of $1~\\mu{\\rm s}$, the lack of observed FRB lensing events in\nCHIME allows us to probe axion stars with mass $ \\gtrsim 2 \\times 10^{-2}\nM_\\odot$, corresponding to axion masses $\\lesssim 10^{-10}{\\rm eV}$. We obtain\nconstraints for even lighter axion stars up to $\\sim 10^{-3} M_\\odot$, when the\naxion-photon interactions are taken into account. Our results indicate that FRB\nlensing lead to constraints that are competitive with conventional microlensing\nsearches operating in the optical band.",
        "Images generated by text-to-image (T2I) models often exhibit visual biases\nand stereotypes of concepts such as culture and profession. Existing\nquantitative measures of stereotypes are based on statistical parity that does\nnot align with the sociological definition of stereotypes and, therefore,\nincorrectly categorizes biases as stereotypes. Instead of oversimplifying\nstereotypes as biases, we propose a quantitative measure of stereotypes that\naligns with its sociological definition. We then propose OASIS to measure the\nstereotypes in a generated dataset and understand their origins within the T2I\nmodel. OASIS includes two scores to measure stereotypes from a generated image\ndataset: (M1) Stereotype Score to measure the distributional violation of\nstereotypical attributes, and (M2) WALS to measure spectral variance in the\nimages along a stereotypical attribute. OASIS also includes two methods to\nunderstand the origins of stereotypes in T2I models: (U1) StOP to discover\nattributes that the T2I model internally associates with a given concept, and\n(U2) SPI to quantify the emergence of stereotypical attributes in the latent\nspace of the T2I model during image generation. Despite the considerable\nprogress in image fidelity, using OASIS, we conclude that newer T2I models such\nas FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts\nand still generate images with widespread stereotypical attributes.\nAdditionally, the quantity of stereotypes worsens for nationalities with lower\nInternet footprints.",
        "There have recently been many cases of unverified or misleading information\ncirculating quickly over bogus web networks and news portals. This false news\ncreates big damage to society and misleads people. For Example, in 2019, there\nwas a rumor that the Padma Bridge of Bangladesh needed 100,000 human heads for\nsacrifice. This rumor turns into a deadly position and this misleading\ninformation takes the lives of innocent people. There is a lot of work in\nEnglish but a few works in Bangla. In this study, we are going to identify the\nfake news from the unconsidered news source to provide the newsreader with\nnatural news or real news. The paper is based on the combination of\nconvolutional neural network (CNN) and long short-term memory (LSTM), where CNN\nis used for deep feature extraction and LSTM is used for detection using the\nextracted feature. The first thing we did to deploy this piece of work was data\ncollection. We compiled a data set from websites and attempted to deploy it\nusing the methodology of deep learning which contains about 50k of news. With\nthe proposed model of Multichannel combined CNN-LSTM architecture, our model\ngained an accuracy of 75.05%, which is a good sign for detecting fake news in\nBangla.",
        "Muon colliders offer a compelling opportunity to explore the TeV scale and\nconduct precision tests of the Standard Model, all within a relatively compact\ngeographical footprint. This paper introduces a new detector concept, MAIA\n(Muon Accelerator Instrumented Apparatus), optimized for $\\sqrt{s}=10$ TeV\n$\\mu\\mu$ collisions. The detector features an all-silicon tracker immersed in a\n5T solenoid field. High-granularity silicon-tungsten and iron-scintillator\ncalorimeters surrounding the solenoid capture high-energy electronic and\nhadronic showers, respectively, and support particle-flow reconstruction. The\noutermost subsystem comprises an air-gap muon spectrometer, which enables\nstandalone track reconstruction for high-momentum muons. The performance of the\nMAIA detector is evaluated in terms of differential particle reconstruction\nefficiencies and resolutions. Beam-induced background (BIB) simulations\ngenerated in FLUKA are overlaid with single particle gun samples to assess\ndetector reconstruction capabilities under realistic experimental conditions.\nEven with BIB, reconstruction efficiencies exceed 95% for energetic tracks,\nphotons, and neutrons in the central region of the detector. This paper\noutlines promising avenues of future work, including forward region\noptimization and opportunities for enhanced flavor\/boosted object tagging, and\naddresses the technological assumptions needed to achieve the desired detector\nperformance.",
        "The high-pressure growth technique is a vital approach that facilitates the\nstabilization of new phases and allows for meticulous control of structural\nparameters, which significantly impact electronic and magnetic properties. We\npresent a short review of our ongoing investigations into various families of\niron-based superconductors (IBS), employing the high-gas pressure and\nhigh-temperature synthesis (HP-HTS) method. This technique is capable of\nproducing the gas pressures up to 1.8 GPa and a heating temperature of up to\n1700 {\\deg}C through a three-zone furnace within a cylindrical chamber.\nDifferent kinds of IBS samples are prepared using HPHTS and characterized\nthrough various measurements to reach the final conclusions. The results\ndemonstrate that the high-pressure growth technique significantly enhances the\nproperties of IBS, including the transition temperature, critical current\ndensity, and pinning force. In addition, the quality of the samples and their\ndensity are improved through the intergrain connections. Furthermore, the\ncomprehensive evaluations and investigations prove that a growth pressure of\n0.5 GPa is sufficient for producing high-quality IBS bulks under the optimized\nsynthesis conditions.",
        "The Dead Internet Theory (DIT) suggests that much of today's internet,\nparticularly social media, is dominated by non-human activity, AI-generated\ncontent, and corporate agendas, leading to a decline in authentic human\ninteraction. This study explores the origins, core claims, and implications of\nDIT, emphasizing its relevance in the context of social media platforms. The\ntheory emerged as a response to the perceived homogenization of online spaces,\nhighlighting issues like the proliferation of bots, algorithmically generated\ncontent, and the prioritization of engagement metrics over genuine user\ninteraction. AI technologies play a central role in this phenomenon, as social\nmedia platforms increasingly use algorithms and machine learning to curate\ncontent, drive engagement, and maximize advertising revenue. While these tools\nenhance scalability and personalization, they also prioritize virality and\nconsumption over authentic communication, contributing to the erosion of trust,\nthe loss of content diversity, and a dehumanized internet experience. This\nstudy redefines DIT in the context of social media, proposing that the\ncommodification of content consumption for revenue has taken precedence over\nmeaningful human connectivity. By focusing on engagement metrics, platforms\nfoster a sense of artificiality and disconnection, underscoring the need for\nhuman-centric approaches to revive authentic online interaction and community\nbuilding.",
        "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
        "An important step in understanding the formation and evolution of the Nuclear\nStar Cluster (NSC) is to investigate its chemistry and chemical evolution.\nAdditionally, exploring the relationship of the NSC to the other structures in\nthe Galactic Center and the Milky Way disks is of great interest. Extreme\noptical extinction has previously prevented optical studies, but near-IR\nhigh-resolution spectroscopy is now possible. Here, we present a detailed\nchemical abundance analysis of 19 elements - more than four times as many as\npreviously published - for 9 stars in the NSC of the Milky Way, observed with\nthe IGRINS spectrometer on the Gemini South telescope. This study provides new,\ncrucial observational evidence to shed light on the origin of the NSC. We\ndemonstrate that it is possible to probe a variety of nucleosynthetic channels,\nreflecting different chemical evolution timescales. Our findings reveal that\nthe NSC trends for the elements F, Mg, Al, Si, S, K, Ca, Ti, Cr, Mn, Co, Ni,\nCu, and Zn, as well as the s-process elements Ba, Ce, Nd, and Yb, generally\nfollow the inner bulge trends within uncertainties. This suggests a likely\nshared evolutionary history and our results indicate that the NSC population is\nconsistent with the chemical sequence observed in the inner Galaxy (the\ninner-disk sequence). However, we identify a significant and unexplained\ndifference in the form of higher Na abundances in the NSC compared to the\ninner-bulge. This is also observed in few Galactic globular clusters, and may\nsuggest a common enrichment process at work in all these systems.",
        "Current neural operators often struggle to generalize to complex,\nout-of-distribution conditions, limiting their ability in seismic wavefield\nrepresentation. To address this, we propose a generative neural operator (GNO)\nthat leverages generative diffusion models (GDMs) to learn the underlying\nstatistical distribution of scattered wavefields while incorporating a\nphysics-guided sampling process at each inference step. This physics guidance\nenforces wave equation-based constraints corresponding to specific velocity\nmodels, driving the iteratively generated wavefields toward physically\nconsistent solutions. By training the diffusion model on wavefields\ncorresponding to a diverse dataset of velocity models, frequencies, and source\npositions, our GNO enables to rapidly synthesize high-fidelity wavefields at\ninference time. Numerical experiments demonstrate that our GNO not only\nproduces accurate wavefields matching numerical reference solutions, but also\ngeneralizes effectively to previously unseen velocity models and frequencies.",
        "Laboratory experiments and theoretical modelling are conducted to determine\nthe raindrop size distribution (DSD) resulting from distinct fragmentation\nprocesses under various upward airstreams. Since weather radar echoes are\nproportional to the sixth power of the average droplet diameter, understanding\nthe fragmentation mechanisms that lead to different breakup sizes is crucial\nfor accurate rainfall predictions. We utilize a two-parameter gamma\ndistribution for theoretical modelling and estimate the average droplet\ndiameter from the theoretically obtained characteristic sizes, often treated as\nassumed input parameters for different rain conditions in rainfall modelling.\nOur experimental and theoretical findings demonstrate a close agreement with\nthe DSD predicted by the Marshall and Palmer relationship for steady rain\nconditions. Additionally, in situ DSD measurements at different altitudes were\nobtained through research flights equipped with advanced sensors, further\nvalidating our rainfall model. This study underscores the effectiveness of\nlaboratory-scale experiments and the critical importance of accurately\ncharacterizing DSD to enhance rainfall predictions.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "The Casimir-Lifshitz force arises from thermal and quantum mechanical\nfluctuations between classical bodies and becomes significant below the micron\nscale. We explore temperature-distance relations based on the concepts of Wick\nand Bohr arising from energy-time uncertainty relations. We show that\ntemperature-distance relations similar to those arising from the uncertainty\nprinciple are found in various Casimir interactions, with an exact relation\noccurring in the low-temperature regime when the zero point energy contribution\ncancels the thermal radiation pressure contribution between two plates.",
        "In this paper, we consider the spherically symmetric gravitational collapse\nof isotropic matter undergoing dissipation in the form of heat flux, with a\ngeneralized Vaidya exterior, in the context of $f(R, T)$ gravity. Choosing\n$f(R, T)=R+2\\lambda T$, and applying the $f(R, T)$ junction conditions on the\nfield equations for the interior and exterior regions, we have obtained\nmatching conditions of the matter-Lagrangian and its derivatives across the\nboundary. The time of formation of singularity and the time of formation of\napparent horizon have been determined and constraints on the integration\nconstants are examined for which the final singularity is hidden behind the\nhorizon."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b25",
    "start_title":"Genetic Algorithms + Data Structures = Evolution Programs",
    "start_abstract":"Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation.",
    "start_categories":[
      "cs.DS"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Kinetic description and convergence analysis of genetic algorithms for global optimization"
      ],
      "abstract":[
        "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Carleman estimate for semi-discrete stochastic parabolic operators in\n  arbitrary dimension and applications to controllability",
        "Reachability for multiagent control systems via Lyapunov functions",
        "A Rank-One-Update Method for the Training of Support Vector Machines",
        "On Subdifferentials Via a Generalized Conjugation Scheme: An Application\n  to DC Problems and Optimality Conditions",
        "Existence and uniqueness of control sets with a nonempty interior for\n  linear control systems on solvable groups",
        "Convergence of projected stochastic approximation algorithm",
        "Asymptotic behavior of penalty dynamics for constrained variational\n  inequalities",
        "Local convergence analysis of a stabilized sequential quadratic\n  programming method for optimization problems in Banach spaces",
        "A Graph-Based Iterative Strategy for Solving the All-Line Transmission\n  Switching Problem",
        "Bi-Parameterized Two-Stage Stochastic Min-Max and Min-Min Mixed Integer\n  Programs",
        "A Smoothing Consensus-Based Optimization Algorithm for Nonsmooth\n  Nonconvex Optimization",
        "Solving Non-Monotone Inclusions Using Monotonicity of Pairs of Operators",
        "A New Lyapunov-like Stability Inequality with an \\textit{Asymmetric}\n  Matrix and Application to Suboptimal LQ Control Design",
        "Strongly nonlinear age structured equation,time-elapsed model and large\n  delays",
        "Benchmarking nuclear matrix elements of $0\\nu\\beta\\beta$ decay with\n  high-energy nuclear collisions",
        "Periodic orbits and their gravitational wave radiations around the\n  Schwarzschild-MOG black hole",
        "A positive product formula of integral kernels of $k$-Hankel transforms",
        "Floquet geometric squeezing in fast-rotating condensates",
        "Clarkson-McCarthy inequality on a locally compact group",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Low-rank variance reduction for uncertain radiative transfer with\n  control variates",
        "Lyapunov exponent for quantum graphs that are elements of a subshift of\n  finite type",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Nonlinear Spectroscopy as a Magnon Breakdown Diagnosis and its Efficient\n  Simulation",
        "Empirical Thermophotovoltaic Performance Predictions and Limits",
        "The waves-in-space Purcell effect for superconducting qubits",
        "Anomalous nuclear spin coherence in superconducting Nb$_3$Sn",
        "Effects of Initial Nucleon-Nucleon Correlations on Light Nuclei\n  Production in Au+Au Collisions at $\\sqrt{s_\\mathrm{NN}} = 3\\ $ GeV"
      ],
      "abstract":[
        "This paper considers a semi-discrete forward stochastic parabolic operator\nwith homogeneous Dirichlet conditions in arbitrary dimensions. We show the lack\nof null controllability for a spatial semi-discretization of a\nnull-controllable stochastic parabolic system from any initial datum. However,\nby proving a new Carleman estimate for its semi-discrete backward stochastic\nadjoint system, we achieve a relaxed observability inequality, which is applied\nto derivative $\\phi$-null controllability by duality arguments.",
        "This paper concerns the problem of reachability of a given state for a\nmultiagent control system in $\\mathbb{R}^d$. In such a system, at every time\neach agent can choose his\/her velocity which depends both on his\/her position\nand on the position of the whole crowd of agents (modeled by a probability\nmeasure on $ \\mathbb{R}^d$). The main contribution of the paper is to study the\nabove reachability problem with a given rate of attainability through a\nLyapunov method adapted to the Wasserstein space of probability measures. As a\nbyproduct we obtain a new comparison result for viscosity solutions of Hamilton\nJacobi equations in the Wasserstein space.",
        "This paper considers convex quadratic programs\n  associated with the training of support vector machines (SVM).\n  Exploiting the special structure of the SVM problem a new\n  type of active set method with long cycles and stable rank-one-updates\n  is proposed and tested (CMU: cycling method with updates).\n  The structure of the problem allows for a repeated simple increase\n  of the set of inactive constraints while controlling its size. This is\n  followed by minimization steps with cheap updates of a matrix factorization.\n  A widely used approach for solving SVM problems is the\n  alternating direction method SMO,\n  a method that is very efficient for low accuracy.\n  The new active set approach allows for higher accuracy\n  results at moderate computational cost. To relate both approaches,\n  the effect of the accuracy on the running time and on the\n  predictive quality of the SVM is compared with some numerical examples.\n  A surprising result of the numerical examples is that only a\n  very small number of cycles (each consisting of less than 2n\n  steps) was used for CMU.",
        "This paper studies properties of a subdifferential defined using a\ngeneralized conjugation scheme. We relate this subdifferential together with\nthe domain of an appropriate conjugate function and the {\\epsilon}-directional\nderivative. In addition, we also present necessary conditions for\n{\\epsilon}-optimality and global optimality in optimization problems involving\nthe difference of two convex functions. These conditions will be written via\nthis generalized notion of subdifferential studied in the first sections of the\npaper.",
        "In this paper, we obtain weak conditions for the existence of a control set\nwith a nonempty interior for a linear control system on a solvable Lie group.\nWe show that the Lie algebra rank condition together with the compactness of\nthe nilpotent part of the generalized kernel of the drift are enough to assure\nthe existence of such a control set. Moreover, this control set is unique and\ncontains the whole generalized kernel in its closure.",
        "We study the Robbins-Monro stochastic approximation algorithm with\nprojections on a hyperrectangle and prove its convergence. This work fills a\ngap in the convergence proof of the classic book by Kushner and Yin. Using the\nODE method, we show that the algorithm converges to stationary points of a\nrelated projected ODE. Our results provide a better theoretical foundation for\nstochastic optimization techniques, including stochastic gradient descent and\nits proximal version. These results extend the algorithm's applicability and\nrelax some assumptions of previous research.",
        "We propose a comprehensive framework for solving constrained variational\ninequalities via various classes of evolution equations displaying multi-scale\naspects. In a Hilbertian framework, the class of dynamical systems we propose\ncombine Tikhonov regularization and exterior penalization terms in order to\nyield simultaneously strong convergence of trajectories to least norm solutions\nin the constrained domain. Our construction thus unifies the literature on\nregularization methods and penalty-term based dynamical systems.",
        "This paper presents a stabilized sequential quadratic programming (SQP)\nmethod for solving optimization problems in Banach spaces. The optimization\nproblem considered in this study has a general form that enables us to\nrepresent various types of optimization problems. Several SQP methods have been\nproposed for optimization problems in Banach spaces with specific structures;\nhowever, research on the local analysis of SQP-type methods for general\nproblems, such as those considered in this study, is limited. We focus on the\nlocal behavior of the proposed stabilized SQP method and prove its local\nquadratic convergence under reasonable assumptions, without including any\nconstraint qualifications. Finally, numerical experiments are performed to\nconfirm the theoretical properties shown in the local analysis.",
        "The transmission switching problem aims to determine the optimal network\ntopology that minimizes the operating costs of a power system. This problem is\ntypically formulated as a mixed-integer optimization model, which involves\nbig-M constants that lead to weak relaxations and significant computational\nchallenges, particularly when all lines are switchable. In this paper, we\npropose a two-fold approach: first, using graph theory to derive tighter big-M\nvalues by solving a relaxed longest path problem; second, introducing an\niterative algorithm that incorporates a heuristic version of the switching\nproblem to efficiently generate low-cost feasible solutions, thereby\naccelerating the search for optimal solutions in the integer optimization\nsolver. Numerical results on the 118-bus network show that the proposed\nmethodology significantly reduces the computational burden compared to\nconventional approaches.",
        "We introduce two-stage stochastic min-max and min-min integer programs with\nbi-parameterized recourse (BTSPs), where the first-stage decisions affect both\nthe objective function and the feasible region of the second-stage problem. To\nsolve these programs efficiently, we introduce Lagrangian-integrated L-shaped\n($L^2$) methods, which guarantee exact solutions when the first-stage decisions\nare pure binary. For mixed-binary first-stage programs, we present a\nregularization-augmented variant of this method. We also introduce\ndistributionally robust bi-parameterized two-stage stochastic integer programs\nand present an extension of the $L^2$ method and a reformulation-based method\nfor programs with finite and continuous supports, respectively. Our\ncomputational results show that the $L^2$ method surpasses the benchmark method\nfor bi-parameterized stochastic network interdiction problems, solving all\ninstances in 23 seconds on average, whereas the benchmark method failed to\nsolve any instance within 3600 seconds. Additionally, it achieves optimal\nsolutions up to 18.4 and 1.7 times faster for instances of risk-neutral and\ndistributionally robust bi-parameterized stochastic facility location problems,\nrespectively. Furthermore, BTSPs have applications in solving stochastic\nproblems with decision-dependent probability distributions or sets of\ndistributions (ambiguity set). The $L^2$ method outperforms existing\napproaches, achieving optimal solutions 5.3 times faster for distributionally\nrobust facility location problem with a decision-dependent and non-relatively\ncomplete ambiguity set.",
        "Lately, a novel swarm intelligence model, namely the consensus-based\noptimization (CBO) algorithm, was introduced to deal with the global\noptimization problems. Limited by the conditions of Ito's formula, the\nconvergence analysis of the previous CBO finite particle system mainly focuses\non the problem with smooth objective function. With the help of smoothing\nmethod, this paper achieves a breakthrough by proposing an effective CBO\nalgorithm for solving the global solution of a nonconvex, nonsmooth, and\npossible non-Lipschitz continuous minimization problem with theoretical\nanalysis, which dose not rely on the mean-field limit. We indicate that the\nproposed algorithm exhibits a global consensus and converges to a common state\nwith any initial data. Then, we give a more detailed error estimation on the\nobjective function values along the state of the proposed algorithm towards the\nglobal minimum. Finally, some numerical examples are presented to illustrate\nthe appreciable performance of the proposed method on solving the nonsmooth,\nnonconvex minimization problems.",
        "In this paper, under the monotonicity of pairs of operators, we propose some\nGeneralized Proximal Point Algorithms to solve non-monotone inclusions using\nwarped resolvents and transformed resolvents. The weak, strong, and linear\nconvergence of the proposed algorithms are established under very mild\nconditions.",
        "The Lyapunov inequality is an indispensable tool for stability analysis in\nthe linear control theory. This work proposes a new variant of this inequality\nwhere-in the constituent matrix is allowed to be asymmetric. After developing\nthe stability conditions based on the proposed inequality for a class of linear\nsystems, we utilize these conditions to derive new results for the suboptimal\nlinear quadratic control problem where we characterize the cost of the\nstabilizing controllers. We also demonstrate, by a numerical example, that the\nproposed results can be easily molded for the structured suboptimal consensus\nprotocol design for multi-agent system where we also see that the asymmetry\ncondition of the design matrix turns up inherently.",
        "The time-elapsed model for neural networks is a nonlinear age structured\nequationwhere the renewal term describes the network activity and influences\nthe dischargerate, possibly with a delay due to the length of connections.We\nsolve a long standing question, namely that an inhibitory network withoutdelay\nwill converge to a steady state and thus the network is desynchonised.\nOurapproach is based on the observation that a non-expansion property holds\ntrue.However a non-degeneracy condition is needed and, besides the standard\none, weintroduce a new condition based on strict nonlinearity.When a delay is\nincluded, and following previous works for Fokker-Planck models,we prove that\nthe network may generate periodic solutions. We introduce a newformalism to\nestablish rigorously this property for large delays.The fundamental contraction\nproperty also holds for some other age structuredequations and systems.",
        "Reducing uncertainties in the nuclear matrix element (NME) remains a critical\nchallenge in designing and interpreting experiments aimed at discovering\nneutrinoless double beta ($0\\nu\\beta\\beta$) decay. Here, we identify a class of\nobservables, distinct from those employed in low-energy nuclear structure\napplications, that are strongly correlated with the NME: momentum correlations\namong hadrons produced in high-energy nuclear collisions. Focusing on the\n$^{150}$Nd$\\rightarrow$$^{150}$Sm transition, we combine a Bayesian analysis of\nthe structure of $^{150}$Nd with simulations of high-energy\n$^{150}$Nd+$^{150}$Nd collisions. We reveal prominent correlations between the\nNME and features of the quark-gluon plasma (QGP) formed in these processes,\nsuch as spatial gradients and anisotropies, which are accessible via collective\nflow measurements. Our findings demonstrate collider experiments involving\n$0\\nu\\beta\\beta$ decay candidates as a platform for benchmarking theoretical\npredictions of the NME.",
        "This article explores the motion of massive particles in the gravitational\nfield of a modified gravity (MOG) black hole (BH), characterized by the\nparameter $\\alpha$. Using the Hamiltonian formalism, the geodesic equations and\nthe effective potential governing particle trajectories are derived. Key\nfeatures, including the innermost stable circular orbit (ISCO) and the\ninnermost bound circular orbit (IBCO), are analyzed, revealing their dependence\non the particle's energy, angular momentum, and the MOG parameter. In the\nextremal case, where $\\alpha=-1$, the event horizon merges with the Cauchy\nhorizon, forming a distinctive BH configuration. Numerical methods are employed\nto compute periodic orbits in this spacetime, with a comparison drawn to the\nSchwarzschild BH. The findings indicate that for $\\alpha>0$, periodic orbits\naround Schwarzschild-MOG BH exhibit lower energy requirements than those in\nSchwarzschild spacetime, whereas for $-1<\\alpha<0$, the energy requirements are\nhigher. Precessing orbits near periodic trajectories are also examined,\noffering insights into their complex dynamical behavior. Finally, the\ngravitational wave (GW) radiation from the periodic orbits of a test particle\naround the Schwarzschild-MOG BH is examined, generating intricate waveforms\nthat provide insights into the gravitational structure of the system.",
        "Let $R$ be a root system in $\\mathbb R^N$ and $G$ be the finite subgroup\ngenerated by the reflections associated to the root system. We establish a\npositive radial product formula for the integral kernels $B_{k,1}(x,y)$ of\n$(k,1)$-generalized Fourier transforms (or the $k$-Hankel transforms) $F_{k,1}$\n$$B_{k,1}(x,z)j_{2\\left\\langle\nk\\right\\rangle+N-2}\\left(2\\sqrt{t\\left|z\\right|}\\right)=\\int_{\\mathbb R^N}\nB_{k,1}(\\xi,z)\\,d\\sigma_{x,t}^{k,1}(\\xi),$$ where $j_{\\lambda}$ is the\nnormalized Bessel function, and $\\sigma_{x,t}^{k,1}(\\xi)$ is the unique\nprobability measure. Such a product formula is equivalent to the following\nrepresentation of the generalized spherical mean operator $f\\mapsto M_f,\\;f\\in\nC_b(\\mathbb{R}^N)$ in $(k,1)$-generalized Fourier analysis \\begin{align*}\nM_f(x,t)=\\int_{\\mathbb{R}^N}f\\,d\\sigma_{x,t}^{k,1},\\;(x,t)\\in\\mathbb{R}^N\\times{\\mathbb{R}}_+.\\end{align*}\nWe will then analyze the representing measure $\\sigma_{x,t}^{k,1}(\\xi)$ and\nshow that the support of the measure is contained in\n$$\\left\\{\\xi\\in\\mathbb{R}^N:\\sqrt{\\vert\\xi\\vert}\\geq\\vert\\sqrt{\\vert\nx\\vert}-\\sqrt t\\vert\\right\\}\\cap\\left(\\bigcup_{g\\in\nG}\\{\\xi\\in\\mathbb{R}^N:d(\\xi,gx)\\leq\\sqrt t\\}\\right),$$ where\n$d\\left(x,y\\right)=\\sqrt{\\left|x\\right|+\\left|y\\right|-\\sqrt{2\\left(\\left|x\\right|\\left|y\\right|+\\left\\langle\nx,y\\right\\rangle\\right)}}$. Based on the support of the representing measure\n$\\sigma_{x,t}^{k,1}$, we will get a weak Huygens's principle for the deformed\nwave equation in $(k,1)$-generalized Fourier analysis. Moreover, for $N\\geq 2$,\nif we assume that $F_{k,1}\\left(\\mathcal S(\\mathbb{R}^N)\\right)$ consists of\nrapidly decreasing functions at infinity, then we get two different results on\n$\\text{supp}\\sigma_{x,t}^{k,1}$, which indirectly denies such assumption.",
        "Constructing and manipulating quantum states in fast-rotating Bose-Einstein\ncondensates (BEC) has long stood as a significant challenge as the rotating\nspeed approaching the critical velocity. Although the recent experiment\n[Science, 372, 1318 (2021)] has realized the geometrically squeezed state of\nthe guiding-center mode, the remaining degree of freedom, the cyclotron mode,\nremains unsqueezed due to the large energy gap of Landau levels. To overcome\nthis limitation, in this paper, we propose a Floquet-based state-preparation\nprotocol by periodically driving an anisotropic potential. This protocol not\nonly facilitates the single cyclotron-mode squeezing, but also enables a\ntwo-mode squeezing. Such two-mode squeezing offers a richer set of dynamics\ncompared to single-mode squeezing and can achieve wavepacket width well below\nthe lowest Landau level limit. Our work provides a highly controllable knob for\nrealizing diverse geometrically squeezed states in ultracold quantum gases\nwithin the quantum Hall regime.",
        "Let $G$ be a locally compact group, $\\mu$ its Haar measure, $\\hat G$ its\nPontryagin dual and $\\nu$ the dual measure. For any $A_\\theta\\in L^1(G;\\mathcal\nC_p)\\cap L^2(G;\\mathcal C_p)$, ($\\mathcal C_p$ is Schatten ideal), and\n$1<p\\le2$ we prove $$\\int_{\\hat\nG}\\left\\|\\int_GA_\\theta\\overline{\\xi(\\theta)}\\,\\mathrm\nd\\mu(\\theta)\\right\\|_p^q\\,\\mathrm d\\nu(\\xi)\\le\n  \\left(\\int_G\\|A_\\theta\\|_p^p\\,\\mathrm d\\mu(\\theta)\\right)^{q\/p}, $$ where\n$q=p\/(p-1)$. This appears to be a generalization of some earlier obtained\ninequalities, including Clarkson-McCarthy inequalities (in the case $G=\\mathbf\nZ_2$), and Hausdorff-Young inequality. Some corollaries are also given.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The radiative transfer equation models various physical processes ranging\nfrom plasma simulations to radiation therapy. In practice, these phenomena are\noften subject to uncertainties. Modeling and propagating these uncertainties\nrequires accurate and efficient solvers for the radiative transfer equations.\nDue to the equation's high-dimensional phase space, fine-grid solutions of the\nradiative transfer equation are computationally expensive and memory-intensive.\nIn recent years, dynamical low-rank approximation has become a popular method\nfor solving kinetic equations due to the development of computationally\ninexpensive, memory-efficient and robust algorithms like the augmented basis\nupdate \\& Galerkin integrator. In this work, we propose a low-rank Monte Carlo\nestimator and combine it with a control variate strategy based on\nmulti-fidelity low-rank approximations for variance reduction. We investigate\nthe error analytically and numerically and find that a joint approach to\nbalance rank and grid size is necessary. Numerical experiments further show\nthat the efficiency of estimators can be improved using dynamical low-rank\napproximation, especially in the context of control variates.",
        "We consider the Schr\\\"odinger operator on the quantum graph whose edges\nconnect the points of ${\\Bbb Z}$. The numbers of the edges connecting two\nconsecutive points $n$ and $n+1$ are read along the orbits of a shift of finite\ntype. We prove that the Lyapunov exponent is potitive for energies $E$ that do\nnot belong to a discrete subset of $[0,\\infty)$. The number of points $E$ of\nthis subset in $[(\\pi (j-1))^2, (\\pi j)^2]$ is the same for all $j\\in {\\Bbb\nN}$.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "Identifying quantum spin liquids, magnon breakdown, or fractionalized\nexcitations in quantum magnets is an ongoing challenge due to the ambiguity of\npossible origins of excitation continua occurring in linear response probes.\nRecently, it was proposed that techniques measuring higher-order response, such\nas two-dimensional coherent spectroscopy (2DCS), could resolve such\nambiguities. Numerically simulating nonlinear response functions can, however,\nbe computationally very demanding. We present an efficient Lanczos-based method\nto compute second-order susceptibilities $\\chi^{2}\\omega_t,\\omega_\\tau)$\ndirectly in the frequency domain. Applying this to extended Kitaev models\ndescribing $\\alpha$-RuCl$_3$, we find qualitatively different nonlinear\nresponses between intermediate magnetic field strengths and the high-field\nregime. To put these results into context, we derive the general 2DCS response\nof partially-polarized magnets within the linear spin-wave approximation,\nestablishing that $\\chi^2(\\omega_t,\\omega_\\tau)$ is restricted to a distinct\nuniversal form if the excitations are conventional magnons. Deviations from\nthis form, as predicted in our (Lanczos-based) simulations for\n$\\alpha$-RuCl$_3$, can hence serve in 2DCS experiments as direct criteria to\ndetermine whether an observed excitation continuum is of conventional\ntwo-magnon type or of different nature.",
        "Significant progress has been made in the field of thermophotovoltaics, with\nefficiency recently rising to over 40% due to improvements in cell design and\nmaterial quality, higher emitter temperatures, and better spectral management.\nHowever, inconsistencies in trends for efficiency with semiconductor bandgap\nenergy across various temperatures pose challenges in predicting optimal\nbandgaps or expected performance for different applications. To address these\nissues, here we present realistic performance predictions for various types of\nsingle-junction cells over a broad range of emitter temperatures using an\nempirical model based on past cell measurements. Our model is validated using\ndata from different authors with various bandgaps and emitter temperatures, and\nan excellent agreement is seen between the model and the experimental data.\nUsing our model, we show that in addition to spectral losses, it is important\nto consider practical electrical losses associated with series resistance and\ncell quality to avoid overestimation of system efficiency. We also show the\neffect of modifying various system parameters such as bandgap, above and\nbelow-bandgap reflectance, saturation current, and series resistance on the\nefficiency and power density of thermophotovoltaics at different temperatures.\nFinally, we predict the bandgap energies for best performance over a range of\nemitter temperatures for different cell material qualities.",
        "Quantum information processing, especially with quantum error correction,\nrequires both long-lived qubits and fast, quantum non-demolition readout. In\nsuperconducting circuits this leads to the requirement to both strongly couple\nqubits, such as transmons, to readout modes while also protecting them from\nassociated Purcell decay through the readout port. So-called Purcell filters\ncan provide this protection, at the cost of significant increases in circuit\ncomponents and complexity. However, as we demonstrate in this work, visualizing\nthe qubit fields in space reveals locations where the qubit fields are strong\nand cavity fields weak; simply placing ports at these locations provides\nintrinsic Purcell protection. For a $\\lambda\/2$ readout mode in the\n`chip-in-tube' geometry, we show both millisecond level Purcell protection and,\nconversely, greatly enhanced Purcell decay (qubit lifetime of 1~$\\mu$s) simply\nby relocating the readout port. This method of integrating the Purcell\nprotection into the qubit-cavity geometry can be generalized to other 3D\nimplementations, such as post-cavities, as well as planar geometries. For qubit\nfrequencies below the readout mode this effect is quite distinct from the\nmulti-mode Purcell effect, which we demonstrate in a 3D-post geometry where we\nshow both Purcell protection of the qubit while spoiling the quality factor of\nhigher cavity harmonics to protect against dephasing due to stray photons in\nthese modes.",
        "We have investigated the normal and superconducting states of the\ntechnologically important compound Nb$_3$Sn using $^{93}$Nb nuclear magnetic\nresonance. From spin-lattice relaxation we find strong suppression of the\nzero-temperature superconducting order parameter by magnetic field. We have\nidentified an anomalously large electron-nuclear exchange interaction from\nspin-spin relaxation measurements, an order of magnitude beyond that of the\ndipole-dipole interaction, and thereby sensitive to vortex dynamics and vortex\npinning.",
        "Light nuclei production in heavy-ion collisions serves as a sensitive probe\nof the QCD phase structure. In coalescence models, triton ($N_t$) and deuteron\n($N_d$) yields depend on the spatial separation of nucleon pairs ($\\Delta r$)\nin Wigner functions, yet the impact of initial two-nucleon correlations\n$\\rho(\\Delta r)$ remains underexplored. We develop a method to sample nucleons\nin $^{197}$Au nuclei that simultaneously satisfies both the single-particle\ndistribution $f(r)$ and the two-nucleon correlation $\\rho(\\Delta r)$. Using\nthese nuclei, we simulate Au+Au collisions at $\\sqrt{s_\\mathrm{NN}}=3$ GeV via\nthe SMASH transport model (mean-field mode) to calculate proton, deuteron, and\ntriton yields. Simulations reveal a 36% enhancement in mid-rapidity deuteron\nyields across all centrality ranges and a 33% rise in mid-rapidity triton\nproduction for 0-10% central collisions. Calculated transverse momentum of\nlight nuclei aligns with STAR data. We further analyze impacts of baryon\nconservation, spectator exclusion, and centrality determination via charged\nmultiplicity. Notably, observed discrepancies in the double yield ratio suggest\nunaccounted physical mechanisms, such as critical fluctuations or inaccuracies\nin coalescence parameters or light nuclei cross-sections. This underscores the\ncritical role of initial nucleon-nucleon correlations, linking microscopic\nnuclear structure to intermediate-energy collision dynamics."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques",
    "start_abstract":"Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b35"
      ],
      "title":[
        "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
      ],
      "abstract":[
        "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "Efficient Event-based Delay Learning in Spiking Neural Networks",
        "EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement\n  Learning",
        "A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for\n  Efficient Black-Box Neural Network Optimization",
        "Channel-wise Parallelizable Spiking Neuron with Multiplication-free\n  Dynamics and Large Temporal Receptive Fields",
        "Quantum Simplicial Neural Networks",
        "Runtime Analysis of the Compact Genetic Algorithm on the LeadingOnes\n  Benchmark",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Impact of Surrogate Model Accuracy on Performance and Model Management\n  Strategy in Surrogate-Assisted Evolutionary Algorithms",
        "Waves and symbols in neuromorphic hardware: from analog signal\n  processing to digital computing on the same computational substrate",
        "Comprehensive Benchmarking Environment for Worker Flexibility in\n  Flexible Job Shop Scheduling Problems",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "RL + Transformer = A General-Purpose Problem Solver",
        "Matter creation, adiabaticity and phantom behavior",
        "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone",
        "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing",
        "Tempo: Helping Data Scientists and Domain Experts Collaboratively\n  Specify Predictive Modeling Tasks",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Ultrafast Proton Delivery with Pin Ridge Filters (pRFs): A Novel\n  Approach for Motion Management in Proton Therapy",
        "Upper limits on the gamma-ray emission from the microquasar V4641 Sgr",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "The bright, dusty aftermath of giant eruptions & H-rich supernovae. Late\n  interaction of supernova shocks & dusty circumstellar shells",
        "Evolution and Pathogenicity of SARS-CoVs: A Microcanonical Analysis of\n  Receptor-Binding Motifs",
        "PVTree: Realistic and Controllable Palm Vein Generation for Recognition\n  Tasks",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "Generative Learning of Densities on Manifolds",
        "scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation\n  Model Knowledge"
      ],
      "abstract":[
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Spiking Neural Networks (SNNs) are attracting increased attention as a more\nenergy-efficient alternative to traditional Artificial Neural Networks. Spiking\nneurons are stateful and intrinsically recurrent, making them well-suited for\nspatio-temporal tasks. However, this intrinsic memory is limited by synaptic\nand membrane time constants. A powerful additional mechanism are delays. In\nthis paper, we propose a novel event-based training method for SNNs with\ndelays, grounded in the EventProp formalism and enabling the calculation of\nexact gradients with respect to weights and delays. Our method supports\nmultiple spikes per neuron and, to our best knowledge, is the first delay\nlearning algorithm to be applied to recurrent SNNs. We evaluate our method on a\nsimple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits and\nSpiking Speech Commands datasets, demonstrating that our algorithm can optimize\ndelays from suboptimal initial conditions and enhance classification accuracy\ncompared to architectures without delays. Finally, we show that our approach\nuses less than half the memory of the current state-of-the-art delay-learning\nmethod and is up to 26x faster.",
        "Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising\napproach to overcoming the limitations of traditional reinforcement learning\n(RL) by integrating the Evolutionary Computation (EC) paradigm with RL.\nHowever, the population-based nature of EC significantly increases\ncomputational costs, thereby restricting the exploration of algorithmic design\nchoices and scalability in large-scale settings. To address this challenge, we\nintroduce $\\texttt{$\\textbf{EvoRL}$}$, the first end-to-end EvoRL framework\noptimized for GPU acceleration. The framework executes the entire training\npipeline on accelerators, including environment simulations and EC processes,\nleveraging hierarchical parallelism through vectorization and compilation\ntechniques to achieve superior speed and scalability. This design enables the\nefficient training of large populations on a single machine. In addition to its\nperformance-oriented design, $\\texttt{$\\textbf{EvoRL}$}$ offers a comprehensive\nplatform for EvoRL research, encompassing implementations of traditional RL\nalgorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g.,\nCMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL\n(e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT). The framework's\nmodular architecture and user-friendly interface allow researchers to\nseamlessly integrate new components, customize algorithms, and conduct fair\nbenchmarking and ablation studies. The project is open-source and available at:\nhttps:\/\/github.com\/EMI-Group\/evorl.",
        "Swarm intelligence optimization algorithms have gained significant attention\ndue to their ability to solve complex optimization problems. However, the\nefficiency of optimization in large-scale problems limits the use of related\nmethods. This paper presents a GPU-accelerated version of the Multi-Guiding\nSpark Fireworks Algorithm (MGFWA), which significantly improves the\ncomputational efficiency compared to its traditional CPU-based counterpart. We\nbenchmark the GPU-MGFWA on several neural network black-box optimization\nproblems and demonstrate its superior performance in terms of both speed and\nsolution quality. By leveraging the parallel processing power of modern GPUs,\nthe proposed GPU-MGFWA results in faster convergence and reduced computation\ntime for large-scale optimization tasks. The proposed implementation offers a\npromising approach to accelerate swarm intelligence algorithms, making them\nmore suitable for real-time applications and large-scale industrial problems.\nSource code is released at https:\/\/github.com\/mxxxr\/MGFWA.",
        "Spiking Neural Networks (SNNs) are distinguished from Artificial Neural\nNetworks (ANNs) for their sophisticated neuronal dynamics and sparse binary\nactivations (spikes) inspired by the biological neural system. Traditional\nneuron models use iterative step-by-step dynamics, resulting in serial\ncomputation and slow training speed of SNNs. Recently, parallelizable spiking\nneuron models have been proposed to fully utilize the massive parallel\ncomputing ability of graphics processing units to accelerate the training of\nSNNs. However, existing parallelizable spiking neuron models involve dense\nfloating operations and can only achieve high long-term dependencies learning\nability with a large order at the cost of huge computational and memory costs.\nTo solve the dilemma of performance and costs, we propose the mul-free\nchannel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable\nfor SNNs' resource-restricted application scenarios. The proposed neuron\nimports the channel-wise convolution to enhance the learning ability, induces\nthe sawtooth dilations to reduce the neuron order, and employs the bit shift\noperation to avoid multiplications. The algorithm for design and implementation\nof acceleration methods is discussed meticulously. Our methods are validated in\nneuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and\nneuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.\nTraining speed results demonstrate the effectiveness of our acceleration\nmethods, providing a practical reference for future research.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "The compact genetic algorithm (cGA) is one of the simplest\nestimation-of-distribution algorithms (EDAs). Next to the univariate marginal\ndistribution algorithm (UMDA) -- another simple EDA -- , the cGA has been\nsubject to extensive mathematical runtime analyses, often showcasing a similar\nor even superior performance to competing approaches. Surprisingly though, up\nto date and in contrast to the UMDA and many other heuristics, we lack a\nrigorous runtime analysis of the cGA on the LeadingOnes benchmark -- one of the\nmost studied theory benchmarks in the domain of evolutionary computation.\n  We fill this gap in the literature by conducting a formal runtime analysis of\nthe cGA on LeadingOnes. For the cGA's single parameter -- called the\nhypothetical population size -- at least polylogarithmically larger than the\nproblem size, we prove that the cGA samples the optimum of LeadingOnes with\nhigh probability within a number of function evaluations quasi-linear in the\nproblem size and linear in the hypothetical population size. For the best\nhypothetical population size, our result matches, up to polylogarithmic\nfactors, the typical quadratic runtime that many randomized search heuristics\nexhibit on LeadingOnes. Our analysis exhibits some noteworthy differences in\nthe working principles of the two algorithms which were not visible in previous\nworks.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "Surrogate-assisted evolutionary algorithms (SAEAs) have been proposed to\nsolve expensive optimization problems. Although SAEAs use surrogate models that\napproximate the evaluations of solutions using machine learning techniques,\nprior research has not adequately investigated the impact of surrogate model\naccuracy on search performance and model management strategy in SAEAs. This\nstudy analyzes how surrogate model accuracy affects search performance and\nmodel management strategies. For this purpose, we construct a pseudo-surrogate\nmodel with adjustable prediction accuracy to ensure fair comparisons across\ndifferent model management strategies. We compared three model management\nstrategies: (1) pre-selection (PS), (2) individual-based (IB), and (3)\ngeneration-based (GB) on standard benchmark problems with a baseline model that\ndoes not use surrogates. The experimental results reveal that a higher\nsurrogate model accuracy improves the search performance. However, the impact\nvaries according to the strategy used. Specifically, PS demonstrates a clear\ntrend of improved performance as the estimation accuracy increases, whereas IB\nand GB exhibit robust performance when the accuracy surpasses a certain\nthreshold. In model strategy comparisons, GB exhibits superior performance\nacross a broad range of prediction accuracies, IB outperforms it at lower\naccuracies, and PS outperforms it at higher accuracies. The findings of this\nstudy clarify guidelines for selecting appropriate model management strategies\nbased on the surrogate model accuracy.",
        "Neural systems use the same underlying computational substrate to carry out\nanalog filtering and signal processing operations, as well as discrete symbol\nmanipulation and digital computation. Inspired by the computational principles\nof canonical cortical microcircuits, we propose a framework for using recurrent\nspiking neural networks to seamlessly and robustly switch between analog signal\nprocessing and categorical and discrete computation. We provide theoretical\nanalysis and practical neural network design tools to formally determine the\nconditions for inducing this switch. We demonstrate the robustness of this\nframework experimentally with hardware soft Winner-Take-All and mixed-feedback\nrecurrent spiking neural networks, implemented by appropriately configuring the\nanalog neuron and synapse circuits of a mixed-signal neuromorphic processor\nchip.",
        "In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP)\naims to optimize a sequence of operations and assign each to an eligible\nmachine with varying processing times. For integration of the workforce, each\nmachine also requires a worker to be present to process an operation which\nadditionally affects the processing times. The resulting problem is called\nFlexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The\nFJSSP has been approached with various problem representations, including Mixed\nInteger Linear Programming (MILP), Constrained Programming (CP), and\nSimulation-based Optimization (SBO). In the latter area in particular, there\nexists a large number of specialized Evolutionary Algorithms (EA) like Particle\nSwarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often\ndeveloped for single use cases only, and validated on a few selected test\ninstances, let alone compared with results from solvers using other problem\nrepresentations. While suitable approaches do also exist, the design of the\nFJSSP-W instances is not standardized and the algorithms are hardly comparable.\nThis calls for a systematic benchmarking environment that provides a\ncomprehensive set of FJSSP(-W) instances and supports targeted algorithm\ndevelopment. It will facilitate the comparison of algorithmic performance in\nthe face of different problem characteristics. The present paper presents a\ncollection of 402 commonly accepted FJSSP instances and proposes an approach to\nextend these with worker flexibility. In addition, we present a detailed\nprocedure for the evaluation of scheduling algorithms on these problem sets and\nprovide suitable model representations for this purpose. We provide complexity\ncharacteristics for all presented instances as well as baseline results of\ncommon commercial solvers to facilitate the validation of new algorithmic\ndevelopments.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.",
        "We present a novel cosmological framework that unifies matter creation\ndynamics with thermodynamic principles. Starting with a single-component fluid\ncharacterized by a constant equation of state parameter, $\\omega$, we introduce\na generalized second law of thermodynamics by considering the entropy\nassociated with the cosmic horizon. Imposing an adiabatic expansion condition\nuniquely determines the particle creation rate, $\\Gamma$, a feature\nunprecedented in previous matter creation models. This mechanism yields a\ncosmology featuring phantom-like expansion while relying solely on a single\nconstituent, which can be either a quintessence-like fluid or a non-exotic,\nnon-relativistic dark matter component. Remarkably, this framework avoids the\nneed for exotic physics while providing a consistent explanation for the\naccelerated expansion of the universe. Our results open new pathways for\nunderstanding the interplay between horizon thermodynamics, particle creation,\nand cosmic evolution, offering fresh insights into the nature of dark energy\nand its potential thermodynamic origins.",
        "Although 85% of sub-Saharan Africa's population is covered by mobile\nbroadband signal, only 37% use the internet, and those who do seldom use the\nweb. The most frequently cited reason for low internet usage is the cost of\ndata. We investigate whether AI can bridge this gap by analyzing 40,350 queries\nsubmitted to an AI chatbot by 469 teachers in Sierra Leone over 17 months.\nTeachers use AI for teaching assistance more frequently than web search. We\ncompare the AI responses to the corresponding top search results for the same\nqueries from the most popular local web search engine, google.com.sl. Only 2%\nof results for corresponding web searches contain content from in country.\nAdditionally, the average web search result consumes 3,107 times more data than\nan AI response. Bandwidth alone costs \\$2.41 per thousand web search results\nloaded, while the total cost of AI is \\$0.30 per thousand responses. As a\nresult, AI is 87% less expensive than web search. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
        "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.",
        "Temporal predictive models have the potential to improve decisions in health\ncare, public services, and other domains, yet they often fail to effectively\nsupport decision-makers. Prior literature shows that many misalignments between\nmodel behavior and decision-makers' expectations stem from issues of model\nspecification, namely how, when, and for whom predictions are made. However,\nmodel specifications for predictive tasks are highly technical and difficult\nfor non-data-scientist stakeholders to interpret and critique. To address this\nchallenge we developed Tempo, an interactive system that helps data scientists\nand domain experts collaboratively iterate on model specifications. Using\nTempo's simple yet precise temporal query language, data scientists can quickly\nprototype specifications with greater transparency about pre-processing\nchoices. Moreover, domain experts can assess performance within data subgroups\nto validate that models behave as expected. Through three case studies, we\ndemonstrate how Tempo helps multidisciplinary teams quickly prune infeasible\nspecifications and identify more promising directions to explore.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "Active breath-hold techniques effectively mitigate respiratory motion but\npose challenges for patients who are ineligible for the procedure. Conventional\ntreatment planning relies on multiple energy layers, extending delivery time\ndue to slow layer switching. We propose to use pin ridge filters (pRFs),\ninitially developed for FLASH radiotherapy, to construct a single energy beam\nplan and minimize dose delivery time. The conventional ITV--based\nfree--breathing treatment plan served as the reference. A GTV--based IMPT--DS\nplan with a downstream energy modulation strategy was developed based on a new\nbeam model that was commissioned using the maximum energy of the IMPT plan.\nConsequently, a nested iterative pencil beam direction (PBD) spot reduction\nprocess removed low--weighted spots along each PBD, generating pRFs with\ncoarser resolution. Finally, the IMPT--DS plan was then converted into an\nIMPT--pRF plan, using a monoenergetic beam with optimized spot positions and\nweights. This approach was validated on lung and liver SBRT cases (10 Gy RBE x\n5). For the lung case, the mean lung--GTV dose decreased from 10.3 Gy to 6.9\nGy, with delivery time reduced from 188.79 to 36.16 seconds. The largest time\nreduction was at 150{\\deg}, from 47.4 to 3.99 seconds. For the liver case, the\nmean liver--GTV dose decreased from 5.7 Gy to 3.8 Gy, with delivery time\nreduced from 111.13 to 30.54 seconds. The largest time reduction was at\n180{\\deg}, from 38.57 to 3.94 seconds. This method significantly reduces dose\ndelivery time and organ at risk dose. Further analysis is needed to validate\nits clinical feasibility.",
        "Following a recent detection of TeV radiation by the Large High Altitude Air\nShower Observatory (LHAASO) and the High-Altitude Water Cherenkov Observatory\n(HAWC), coincident with the direction of the microquasar V4641 Sgr, we search\nfor possible GeV emission from this source. We explored the morphology and\ntemporal features of the source as well as two nearby unassociated point\nsources which could be a part of extended structure of V4641 Sgr, and compared\nresults with corresponding X-ray and TeV emissions. The 95% confidence level\nupper limits for the flux from the source, assuming both point and extended\nsource models were 5.38$\\times$ 10$^{-13}$ erg cm$^{-2}$ s$^{-1}$ and\n1.12$\\times$ 10$^{-12}$ erg cm$^{-2}$ s$^{-1}$, respectively. Additionally, no\ncorrelation between gamma-ray light curve and X-ray outbursts was observed.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "The late-stage evolution of massive stars is marked by intense instability as\nthey approach core-collapse. During these phases, giant stellar eruptions lead\nto exceptionally high mass-loss rates, forming significant amounts of dust.\nHowever, the survival of these dust grains is challenged by the powerful shock\nwaves generated when the progenitor explodes as a supernova (SN). We explore\nthe impact of hydrogen-rich SN explosions from 45, 50, and 60 M$_\\odot$\nprogenitors on dust formed after these eruptions, focusing on interactions with\ncircumstellar shells occurring from a few years to centuries after the event.\nUsing 3D hydrodynamical simulations, we track the evolution of dust particles\nin a scenario that includes the progenitor's stellar wind, a giant eruption,\nand the subsequent SN explosion, following the mass budgets predicted by\nstellar evolution models. For a standard SN ejecta mass of 10 M$_\\odot$ and\nkinetic energy of $10^{51}$ erg, only 25% of the dust mass survives 250 years\npost-explosion in a spherical circumstellar medium (CSM), while merely 2%\nremains a century after the explosion in a bipolar CSM. If the SN follows the\neruption within a dozen years, 75% of the dust survives for a standard\nexplosion, dropping to 20% for more massive ejecta (15-20 M$_\\odot$) with\nkinetic energy of $5 \\times 10^{51}$ erg. The geometry of the CSM and the early\ntransition of the SN remnant into a radiative phase significantly influence\ndust survival. As the shock wave weakens and efficiently converts kinetic\nenergy into thermal radiation (up to half of the injected kinetic energy) the\nlikelihood of dust survival increases, affecting not only pre-existing dust in\nthe CSM but also SN-condensed dust and ambient interstellar dust. Contrary to\nexpectations, a larger fraction of the dust mass can survive if the SN occurs\nonly a few years after the eruption.",
        "The rapid evolution and global impact of coronaviruses, notably SARS-CoV-1\nand SARS-CoV-2, underscore the importance of understanding their molecular\nmechanisms in detail. This study focuses on the receptor-binding motif (RBM)\nwithin the Spike protein of these viruses, a critical element for viral entry\nthrough interaction with the ACE2 receptor. We investigate the sequence\nvariations in the RBM across SARS-CoV-1, SARS-CoV-2 and its early variants of\nconcern (VOCs). Utilizing multicanonical simulations and microcanonical\nanalysis, we examine how these variations influence the folding dynamics,\nthermostability, and solubility of the RBMs. Our methodology includes\ncalculating the density of states (DoS) to identify structural phase\ntransitions and assess thermodynamic properties. Furthermore, we solve the\nPoisson-Boltzmann equation to model the solubility of the RBMs in aqueous\nenvironments. This methodology is expected to elucidate structural and\nfunctional differences in viral evolution and pathogenicity, likely improving\ntargeted treatments and vaccines.",
        "Palm vein recognition is an emerging biometric technology that offers\nenhanced security and privacy. However, acquiring sufficient palm vein data for\ntraining deep learning-based recognition models is challenging due to the high\ncosts of data collection and privacy protection constraints. This has led to a\ngrowing interest in generating pseudo-palm vein data using generative models.\nExisting methods, however, often produce unrealistic palm vein patterns or\nstruggle with controlling identity and style attributes. To address these\nissues, we propose a novel palm vein generation framework named PVTree. First,\nthe palm vein identity is defined by a complex and authentic 3D palm vascular\ntree, created using an improved Constrained Constructive Optimization (CCO)\nalgorithm. Second, palm vein patterns of the same identity are generated by\nprojecting the same 3D vascular tree into 2D images from different views and\nconverting them into realistic images using a generative model. As a result,\nPVTree satisfies the need for both identity consistency and intra-class\ndiversity. Extensive experiments conducted on several publicly available\ndatasets demonstrate that our proposed palm vein generation method surpasses\nexisting methods and achieves a higher TAR@FAR=1e-4 under the 1:1 Open-set\nprotocol. To the best of our knowledge, this is the first time that the\nperformance of a recognition model trained on synthetic palm vein data exceeds\nthat of the recognition model trained on real data, which indicates that palm\nvein image generation research has a promising future.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "A generative modeling framework is proposed that combines diffusion models\nand manifold learning to efficiently sample data densities on manifolds. The\napproach utilizes Diffusion Maps to uncover possible low-dimensional underlying\n(latent) spaces in the high-dimensional data (ambient) space. Two approaches\nfor sampling from the latent data density are described. The first is a\nscore-based diffusion model, which is trained to map a standard normal\ndistribution to the latent data distribution using a neural network. The second\none involves solving an It\\^o stochastic differential equation in the latent\nspace. Additional realizations of the data are generated by lifting the samples\nback to the ambient space using Double Diffusion Maps, a recently introduced\ntechnique typically employed in studying dynamical system reduction; here the\nfocus lies in sampling densities rather than system dynamics. The proposed\napproaches enable sampling high dimensional data densities restricted to\nlow-dimensional, a priori unknown manifolds. The efficacy of the proposed\nframework is demonstrated through a benchmark problem and a material with\nmultiscale structure.",
        "Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of\nmillions of human cells across organs, diseases, development and perturbations\nto date. However, the high-dimensional sparsity, batch effect noise, category\nimbalance, and ever-increasing data scale of the original sequencing data pose\nsignificant challenges for multi-center knowledge transfer, data fusion, and\ncross-validation between scRNA-seq datasets. To address these barriers, (1) we\nfirst propose a latent codes-based scRNA-seq dataset distillation framework\nnamed scDD, which transfers and distills foundation model knowledge and\noriginal dataset information into a compact latent space and generates\nsynthetic scRNA-seq dataset by a generator to replace the original dataset.\nThen, (2) we propose a single-step conditional diffusion generator named SCDG,\nwhich perform single-step gradient back-propagation to help scDD optimize\ndistillation quality and avoid gradient decay caused by multi-step\nback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics\nand inter-class discriminability of the synthetic dataset through flexible\nconditional control and generation quality assurance. Finally, we propose a\ncomprehensive benchmark to evaluate the performance of scRNA-seq dataset\ndistillation in different data analysis tasks. It is validated that our\nproposed method can achieve 7.61% absolute and 15.70% relative improvement over\nprevious state-of-the-art methods on average task."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b35",
    "start_title":"Deep learning-based framework for automatic cranial defect reconstruction and implant modeling",
    "start_abstract":"This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
      ],
      "abstract":[
        "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Dynamics and control of maize infection by Busseola fusca:\n  multi-seasonal modeling and biocontrol strategies",
        "Constraining the curvature-induced quantum gravity scales via gamma-ray\n  bursts",
        "Neural Chaos: A Spectral Stochastic Neural Operator",
        "Constructing balanced datasets for predicting failure modes in\n  structural systems under seismic hazards",
        "On rigid regular graphs and a problem of Babai and Pultr",
        "Groupoids, equivalence bibundles and bimodules for noncommutative\n  solenoids",
        "The Solo Revolution: A Theory of AI-Enabled Individual Entrepreneurship",
        "Moduli of curves and moduli of sheaves",
        "Time-dependent global sensitivity analysis of the Doyle-Fuller-Newman\n  model",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Human fields and their impact on brain waves A pilot study",
        "Highly efficient field-free switching by orbital Hall torque in a\n  MoS2-based device operating at room temperature",
        "Ergodic optimization for beta-transformations",
        "A non-homogeneous, non-stationary and path-dependent Markov anomalous\n  diffusion model",
        "Computational Complexity of Covering Colored Mixed Multigraphs with\n  Simple Degree Partitions",
        "Multi-scale Energy Release Events in the Quiet Sun: A Possible Source\n  for Coronal Heating"
      ],
      "abstract":[
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Maize production in sub-Saharan Africa faces significant challenges due to\nthe maize stalk borer (Busseola fusca), a major pest that causes substantial\nyield losses. Chemical control methods have raised concerns about environmental\nimpact and pest resistance, making biological control a promising alternative.\nIn this study, we develop a multi-seasonal mathematical model using an\nimpulsive system of differential equations to describe stalk borer population\ndynamics and evaluate pest control strategies. We analyze the stability of the\npest-free solution using Floquet theory and study the effects of periodic\npredator releases on pest suppression. Numerical simulations illustrate the\nimpact of cultural practice and predator release frequency. Moreover, our\nsimulations show that, under good cultural practices, releasing predators once\nor three times a year is an effective biocontrol strategy. However, in cases of\npoor cultural practices, biocontrol has only a limited effect, and the best\noutcome is achieved when predators are released once a year at the beginning of\nthe cropping season.",
        "We constrain the parameters that govern curvature-induced quantum gravity\ntime-of-flight (TOF) effects. These TOF delays, which occur due to modified\ndispersion relations of particles in a vacuum, could be a phenomenological\nsignature of quantum gravity. Gamma-ray bursts (GRBs), short, high-energy\nevents from distant galaxies, offer a unique opportunity to impose\nobservational limits on TOF delays and, by extension, on the energy scales of\nquantum gravity. Using the standard Jacob-Piran relation, which assumes a\nlocally-flat spacetime, the analysis of quantum gravity-induced TOF effects\nestablishes a lower limit of approximately 10 Planck energies on the energy\nscale of these effects. However, curvature-induced quantum gravity effects may\nintroduce additional contributions. From current GRB observations, we find\nthat, at a 95% credibility level, in the symmetry-deformed scenario,\ncurvature-induced TOF effects may only arise at energies above 0.04 Planck\nenergy. If we consider only curvature-induced effects, this limit is an order\nof magnitude stronger. Observing more GRBs at different redshifts could improve\nthe constraints on the curvature-induced QG phenomena. However, given the\ncapabilities of current telescopes and the current understanding of GRBs, it is\nunlikely that these constraints will be significantly extended beyond the\npresent level.",
        "Building surrogate models with uncertainty quantification capabilities is\nessential for many engineering applications where randomness, such as\nvariability in material properties, is unavoidable. Polynomial Chaos Expansion\n(PCE) is widely recognized as a to-go method for constructing stochastic\nsolutions in both intrusive and non-intrusive ways. Its application becomes\nchallenging, however, with complex or high-dimensional processes, as achieving\naccuracy requires higher-order polynomials, which can increase computational\ndemands and or the risk of overfitting. Furthermore, PCE requires specialized\ntreatments to manage random variables that are not independent, and these\ntreatments may be problem-dependent or may fail with increasing complexity. In\nthis work, we adopt the spectral expansion formalism used in PCE; however, we\nreplace the classical polynomial basis functions with neural network (NN) basis\nfunctions to leverage their expressivity. To achieve this, we propose an\nalgorithm that identifies NN-parameterized basis functions in a purely\ndata-driven manner, without any prior assumptions about the joint distribution\nof the random variables involved, whether independent or dependent. The\nproposed algorithm identifies each NN-parameterized basis function\nsequentially, ensuring they are orthogonal with respect to the data\ndistribution. The basis functions are constructed directly on the joint\nstochastic variables without requiring a tensor product structure. This\napproach may offer greater flexibility for complex stochastic models, while\nsimplifying implementation compared to the tensor product structures typically\nused in PCE to handle random vectors. We demonstrate the effectiveness of the\nproposed scheme through several numerical examples of varying complexity and\nprovide comparisons with classical PCE.",
        "Accurate prediction of structural failure modes under seismic excitations is\nessential for seismic risk and resilience assessment. Traditional\nsimulation-based approaches often result in imbalanced datasets dominated by\nnon-failure or frequently observed failure scenarios, limiting the\neffectiveness in machine learning-based prediction. To address this challenge,\nthis study proposes a framework for constructing balanced datasets that include\ndistinct failure modes. The framework consists of three key steps. First,\ncritical ground motion features (GMFs) are identified to effectively represent\nground motion time histories. Second, an adaptive algorithm is employed to\nestimate the probability densities of various failure domains in the space of\ncritical GMFs and structural parameters. Third, samples generated from these\nprobability densities are transformed into ground motion time histories by\nusing a scaling factor optimization process. A balanced dataset is constructed\nby performing nonlinear response history analyses on structural systems with\nparameters matching the generated samples, subjected to corresponding\ntransformed ground motion time histories. Deep neural network models are\ntrained on balanced and imbalanced datasets to highlight the importance of\ndataset balancing. To further evaluate the framework's applicability, numerical\ninvestigations are conducted using two different structural models subjected to\nrecorded and synthetic ground motions. The results demonstrate the framework's\nrobustness and effectiveness in addressing dataset imbalance and improving\nmachine learning performance in seismic failure mode prediction.",
        "A graph is \\textit{rigid} if it only admits the identity endomorphism. We\nshow that for every $d\\ge 3$ there exist infinitely many mutually rigid\n$d$-regular graphs of arbitrary odd girth $g\\geq 7$. Moreover, we determine the\nminimum order of a rigid $d$-regular graph for every $d\\ge 3$. This provides\nstrong positive answers to a question of van der Zypen\n[https:\/\/mathoverflow.net\/q\/296483, https:\/\/mathoverflow.net\/q\/321108].\nFurther, we use our construction to show that every finite monoid is isomorphic\nto the endomorphism monoid of a regular graph. This solves a problem of Babai\nand Pultr [J. Comb.~Theory, Ser.~B, 1980].",
        "Let $p$ be a prime number and $\\mathcal{S}_p$ the $p$-solenoid. For\n$\\alpha\\in \\mathbb{R}\\times \\mathbb{Q}_p$ we consider in this paper a naturally\nassociated action groupoid $S_\\alpha:=\\mathbb{Z} [1\/p]\\ltimes_\\alpha\n\\mathcal{S}_p \\rightrightarrows \\mathcal{S}_p$ whose $C^*-$algebra is a model\nfor the noncommutative solenoid $\\mathcal{A}_\\alpha^\\mathscr{S}$ studied by\nLatremoli\\`ere and Packer. Following the geometric ideas of Connes and Rieffel\nto describe the Morita equivalences of noncommutative torus using the Kronecker\nfoliation on the torus, we give an explicit description of the\ngeometric\/topologic equivalence bibundle for groupoids $S_\\alpha$ and $S_\\beta$\nwhenever $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ are in the same orbit\nof the $GL_2(\\mathbb{Z}[1\/p])$ action by linear fractional transformations. As\na corollary, for $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ as above we\nget an explicit description of the imprimitivity bimodules for the associated\nnoncommutative solenoids.",
        "This paper presents the AI Enabled Individual Entrepreneurship Theory (AIET),\na theoretical framework explaining how artificial intelligence technologies\ntransform individual entrepreneurial capability. The theory identifies two\nfoundational premises: knowledge democratization and resource requirements\nevolution. Through three core mechanisms skill augmentation, capital structure\ntransformation, and risk profile modification AIET explains how individuals can\nnow undertake entrepreneurial activities at scales previously requiring\nsignificant organizational infrastructure. The theory presents five testable\npropositions addressing the changing relationship between organizational size\nand competitive advantage, the expansion of individual entrepreneurial\ncapacity, the transformation of market entry barriers, the evolution of\ntraditional firm advantages, and the modification of entrepreneurial risk\nprofiles. Boundary conditions related to task characteristics and market\nconditions define the theory's scope and applicability. The framework suggests\nsignificant implications for entrepreneurship theory, organizational design,\nand market structure as AI capabilities continue to advance. This theory\nprovides a foundation for understanding the evolving landscape of\nentrepreneurship in an AI-enabled world.",
        "Relationships between moduli spaces of curves and sheaves on 3-folds are\npresented starting with the Gromov-Witten\/Donaldson-Thomas correspondence\nproposed more than 20 years ago with D. Maulik, N. Nekrasov, and A. Okounkov.\nThe descendent and relative correspondences as developed with A. Pixton in the\ncontext of stable pairs led to the proof of the correspondence for the\nCalabi-Yau quintic 3-fold. More recently, the study of correspondences in\nfamilies has played an important role in connection with other basic moduli\nproblems in algebraic geometry. The full conjectural framework is presented\nhere in the context of families of 3-folds. This article accompanies my lecture\nat the ICBS in July 2024.",
        "The Doyle-Fuller-Newman model is arguably the most ubiquitous electrochemical\nmodel in lithium-ion battery research. Since it is a highly nonlinear model,\nits input-output relations are still poorly understood. Researchers therefore\noften employ sensitivity analyses to elucidate relative parametric importance\nfor certain use cases. However, some methods are ill-suited for the complexity\nof the model and appropriate methods often face the downside of only being\napplicable to scalar quantities of interest. We implement a novel framework for\nglobal sensitivity analysis of time-dependent model outputs and apply it to a\ndrive cycle simulation. We conduct a full and a subgroup sensitivity analysis\nto resolve lowly sensitive parameters and explore the model error when\nunimportant parameters are set to arbitrary values. Our findings suggest that\nthe method identifies insensitive parameters whose variations cause only small\ndeviations in the voltage response of the model. By providing the methodology,\nwe hope research questions related to parametric sensitivity for time-dependent\nquantities of interest, such as voltage responses, can be addressed more easily\nand adequately in simulative battery research and beyond.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "During brain function, groups of neurons fire synchronously. When these\ngroups are large enough, the resulting electrical signals can be measured on\nthe scalp using Electroencephalography (EEG). The amplitude of these signals\ncan be significant depending on the size and synchronization of the neural\nactivity. EEG waves exhibit distinct patterns based on the brain's state, such\nas whether it is asleep, awake, engaged in mental calculations, or performing\nother cognitive functions. Additionally, these patterns can be modified by\nexternal factors, such as transcranial magnetic stimulation (TMS). TMS involves\nbringing an antenna that generates variable electromagnetic fields close to\nspecific areas of the skull to treat certain pathologies. Given that the human\nbody naturally generates magnetic fields, a question arises: Can these fields\ninfluence the EEG by modulating neuronal function, causing a resonance effect,\nor through some unknown interaction? This study investigated whether\napproaching the palm of the hand to the top of the head (Intervention) could\ninduce effects in the EEG. Power Spectral Density (PSD) was obtained for the 30\nseconds preceding the intervention (PSD_pre) and the final 30 seconds of the\nintervention (PSD_last). The exact Wilcoxon signed-rank test suggests that the\nmedian of PSD_pre is greater than the median of PSD_last at the 95% confidence\nlevel (p-value = 0.004353). In contrast, in the control group, the test\nindicates that at the 95% confidence level (p-value = 0.7667), the median of\nPSD_pre is not greater than the median of PSD_last.",
        "Charge-to-spin and spin-to-charge conversion mechanisms in high spin-orbit\nmaterials are the new frontier of memory devices. They operate via spin-orbit\ntorque (SOT) switching of a magnetic electrode, driven by an applied charge\ncurrent. In this work, we propose a novel memory device based on the\nsemiconducting two-dimensional centrosymmetric transition metal dichalcogenide\n(TMD) MoS2, that operates as a SOT device in the writing process and a spin\nvalve in the reading process. We demonstrate that stable voltage states at room\ntemperature can be deterministically controlled by a switching current density\nas low as 3.2x10^4 A\/cm^2 even in zero field. An applied field 50-100 Oe can be\nused as a further or alternative control parameter for the state switching. Ab\ninitio calculations of spin Hall effect (SHE) and orbital Hall effect (OHE)\nindicate that the latter is the only one responsible for the generation of the\nSOT in the magnetic electrode. The large value of OHC in bulk MoS2 makes our\ndevice competitive in terms of energetic efficiency and could be integrated in\nTMD heterostructures to design memory devices with multiple magnetization\nstates for non-Boolean computation.",
        "Ergodic optimization for beta-transformations $T_\\beta(x)= \\beta x \\pmod 1$\nis developed. If $\\beta>1$ is a beta-number, or such that the orbit-closure of\n$1$ is not minimal, we show that the Typically Periodic Optimization Conjecture\nholds, establishing that there exists an open dense set of H\\\"{o}lder\ncontinuous functions such that for each function in this set, there exists a\nunique maximizing measure, this measure is supported on a periodic orbit, and\nthe periodic locking property holds. It follows that typical periodic\noptimization is typical among the class of beta-transformations: it holds for a\nset of parameters $\\beta>1$ that is residual, and has full Lebesgue measure.",
        "A novel probabilistic framework for modelling anomalous diffusion is\npresented. The resulting process is Markovian, non-homogeneous, non-stationary,\nnon-ergodic, and state-dependent. The fundamental law governing this process is\ndriven by two opposing forces: one proportional to the current state,\nrepresenting the intensity of autocorrelation or contagion, and another\ninversely proportional to the elapsed time, acting as a damping function. The\ninterplay between these forces determines the diffusion regime, characterized\nby the ratio of their proportionality coefficients. This framework encompasses\nvarious regimes, including subdiffusion, Brownian non-Gaussian, superdiffusion,\nballistic, and hyperballistic behaviours. The hyperballistic regime emerges\nwhen the correlation force dominates over damping, whereas a balance between\nthese mechanisms results in a ballistic regime, which is also stationary.\nCrucially, non-stationarity is shown to be necessary for regimes other than\nballistic. The model's ability to describe hyperballistic phenomena has been\ndemonstrated in applications such as epidemics, software reliability, and\nnetwork traffic. Furthermore, deviations from Gaussianity are explored and\nviolations of the Central Limit Theorem are highlighted, supported by\ntheoretical analysis and simulations. It will also be shown that the model\nexhibits a strong autocorrelation structure due to a position dependent jump\nprobability.",
        "The notion of graph covers (also referred to as locally bijective\nhomomorphisms) plays an important role in topological graph theory and has\nfound its computer science applications in models of local computation. For a\nfixed target graph $H$, the {\\sc $H$-Cover} problem asks if an input graph $G$\nallows a graph covering projection onto $H$. Despite the fact that the quest\nfor characterizing the computational complexity of {\\sc $H$-Cover} had been\nstarted more than 30 years ago, only a handful of general results have been\nknown so far.\n  In this paper, we present a complete characterization of the computational\ncomplexity of covering coloured graphs for the case that every equivalence\nclass in the degree partition of the target graph has at most two vertices. We\nprove this result in a very general form. Following the lines of current\ndevelopment of topological graph theory, we study graphs in the most relaxed\nsense of the definition. In particular, we consider graphs that are mixed (they\nmay have both directed and undirected edges), may have multiple edges, loops,\nand semi-edges. We show that a strong P\/NP-complete dichotomy holds true in the\nsense that for each such fixed target graph $H$, the {\\sc $H$-Cover} problem is\neither polynomial-time solvable for arbitrary inputs, or NP-complete even for\nsimple input graphs.",
        "The coronal heating problem remains one of the most challenging questions in\nsolar physics. The energy driving coronal heating is widely understood to be\nassociated with convective motions below the photosphere. Recent\nhigh-resolution observations reveal that photospheric magnetic fields in the\nquiet Sun undergo complex and rapid evolution. These photospheric dynamics are\nexpected to be reflected in the coronal magnetic field. Motivated by these\ninsights, our research aims to explore the relationship between magnetic energy\nand coronal heating. By combining observations from Solar Orbiter and SDO with\na magnetic field extrapolation technique, we estimate the magnetic free energy\nof multi-scale energy release events in the quiet Sun. Interestingly, our\nresults reveal a strong correlation between the evolution of free energy and\nthe integrated intensity of extreme ultraviolet emission at 171 \\AA~in these\nevents. We quantitatively assess the potential energy flux budget of these\nevents to evaluate their contribution to coronal heating. Our study implies a\nlink between photospheric magnetic field evolution and coronal temperature\nvariations, paving the way for further research into similar phenomena."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Trends in Phase II Trials for Cancer Therapies",
    "start_abstract":"Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "E(n) Equivariant Graph Neural Networks"
      ],
      "abstract":[
        "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Beyond the Hype: Benchmarking LLM-Evolved Heuristics for Bin Packing",
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Estimation of total body fat using symbolic regression and evolutionary\n  algorithms",
        "QGAIC: Quantum Inspired Genetic Algorithm for Image Classification",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "Runtime Analysis of the Compact Genetic Algorithm on the LeadingOnes\n  Benchmark",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "Efficient Event-based Delay Learning in Spiking Neural Networks",
        "Cascading CMA-ES Instances for Generating Input-diverse Solution Batches",
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights\n  LLMs",
        "Contextual bandits with entropy-based human feedback",
        "Rational Functions on the Projective Line from a Computational Viewpoint",
        "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and\n  Earth Surface Analysis",
        "Solar flares as electron accelerators: toward a resolution of the\n  acceleration efficiency issue",
        "Connectivity of Coxeter group Morse boundaries",
        "Passive Heart Rate Monitoring During Smartphone Use in Everyday Life",
        "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
        "Enhancing Transformers for Generalizable First-Order Logical Entailment",
        "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm\n  Perception in Large Vision-Language Models",
        "Monotonicity results in half spaces for quasilinear elliptic equations\n  involving a singular term",
        "On Fair Ordering and Differential Privacy",
        "Comparative Analysis of Efficient Adapter-Based Fine-Tuning of\n  State-of-the-Art Transformer Models",
        "Feedback cooling of fermionic atoms in optical lattices",
        "A Constraint-Preserving Neural Network Approach for Solving Mean-Field\n  Games Equilibrium"
      ],
      "abstract":[
        "Coupling Large Language Models (LLMs) with Evolutionary Algorithms has\nrecently shown significant promise as a technique to design new heuristics that\noutperform existing methods, particularly in the field of combinatorial\noptimisation. An escalating arms race is both rapidly producing new heuristics\nand improving the efficiency of the processes evolving them. However, driven by\nthe desire to quickly demonstrate the superiority of new approaches, evaluation\nof the new heuristics produced for a specific domain is often cursory: testing\non very few datasets in which instances all belong to a specific class from the\ndomain, and on few instances per class. Taking bin-packing as an example, to\nthe best of our knowledge we conduct the first rigorous benchmarking study of\nnew LLM-generated heuristics, comparing them to well-known existing heuristics\nacross a large suite of benchmark instances using three performance metrics.\nFor each heuristic, we then evolve new instances won by the heuristic and\nperform an instance space analysis to understand where in the feature space\neach heuristic performs well. We show that most of the LLM heuristics do not\ngeneralise well when evaluated across a broad range of benchmarks in contrast\nto existing simple heuristics, and suggest that any gains from generating very\nspecialist heuristics that only work in small areas of the instance space need\nto be weighed carefully against the considerable cost of generating these\nheuristics.",
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "Body fat percentage is an increasingly popular alternative to Body Mass Index\nto measure overweight and obesity, offering a more accurate representation of\nbody composition. In this work, we evaluate three evolutionary computation\ntechniques, Grammatical Evolution, Context-Free Grammar Genetic Programming,\nand Dynamic Structured Grammatical Evolution, to derive an interpretable\nmathematical expression to estimate the percentage of body fat that are also\naccurate. Our primary objective is to obtain a model that balances accuracy\nwith explainability, making it useful for clinical and health applications. We\ncompare the performance of the three variants on a public anthropometric\ndataset and compare the results obtained with the QLattice framework.\nExperimental results show that grammatical evolution techniques can obtain\ncompetitive results in performance and interpretability.",
        "This study uses two meta-heuristics methodologies to introduce two novel\nquantum-inspired meta heuristic approaches: quantum-inspired genetic algorithm\n(QIGA1) and quantum-inspired genetic algorithm with dynamic approach (QIGA2).\nThe two suggested methods combine a classical and quantum genetic algorithm\napproach. Both approaches use The correlation coefficient as an assessment\nfunction to identify the best (optimal) values for binary image. In quantum\ncomputing, they use simple ideas like qubits and state superposition. Due to\nthese characteristics, parallelism which uses the time discreteness of quantum\nmechanical systems, is exhibited. For five distinct MNIST datasets, the\nperformance of all participating algorithms has been assessed by comparing the\nsuggested approaches first with their traditional approach counterparts and\nthen with the proposed methods QIGA1 and QIGA2. Each method's ideal threshold\nvalue, associated fitness value (best and average), loss, and accuracy for each\nMNIST dataset have all been published. The outcomes demonstrate the superior\nefficiency of the suggested approaches over their traditional equivalents.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "The compact genetic algorithm (cGA) is one of the simplest\nestimation-of-distribution algorithms (EDAs). Next to the univariate marginal\ndistribution algorithm (UMDA) -- another simple EDA -- , the cGA has been\nsubject to extensive mathematical runtime analyses, often showcasing a similar\nor even superior performance to competing approaches. Surprisingly though, up\nto date and in contrast to the UMDA and many other heuristics, we lack a\nrigorous runtime analysis of the cGA on the LeadingOnes benchmark -- one of the\nmost studied theory benchmarks in the domain of evolutionary computation.\n  We fill this gap in the literature by conducting a formal runtime analysis of\nthe cGA on LeadingOnes. For the cGA's single parameter -- called the\nhypothetical population size -- at least polylogarithmically larger than the\nproblem size, we prove that the cGA samples the optimum of LeadingOnes with\nhigh probability within a number of function evaluations quasi-linear in the\nproblem size and linear in the hypothetical population size. For the best\nhypothetical population size, our result matches, up to polylogarithmic\nfactors, the typical quadratic runtime that many randomized search heuristics\nexhibit on LeadingOnes. Our analysis exhibits some noteworthy differences in\nthe working principles of the two algorithms which were not visible in previous\nworks.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "Spiking Neural Networks (SNNs) are attracting increased attention as a more\nenergy-efficient alternative to traditional Artificial Neural Networks. Spiking\nneurons are stateful and intrinsically recurrent, making them well-suited for\nspatio-temporal tasks. However, this intrinsic memory is limited by synaptic\nand membrane time constants. A powerful additional mechanism are delays. In\nthis paper, we propose a novel event-based training method for SNNs with\ndelays, grounded in the EventProp formalism and enabling the calculation of\nexact gradients with respect to weights and delays. Our method supports\nmultiple spikes per neuron and, to our best knowledge, is the first delay\nlearning algorithm to be applied to recurrent SNNs. We evaluate our method on a\nsimple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits and\nSpiking Speech Commands datasets, demonstrating that our algorithm can optimize\ndelays from suboptimal initial conditions and enhance classification accuracy\ncompared to architectures without delays. Finally, we show that our approach\nuses less than half the memory of the current state-of-the-art delay-learning\nmethod and is up to 26x faster.",
        "Rather than obtaining a single good solution for a given optimization\nproblem, users often seek alternative design choices, because the best-found\nsolution may perform poorly with respect to additional objectives or\nconstraints that are difficult to capture into the modeling process.\n  Aiming for batches of diverse solutions of high quality is often desirable,\nas it provides flexibility to accommodate post-hoc user preferences. At the\nsame time, it is crucial that the quality of the best solution found is not\ncompromised.\n  One particular problem setting balancing high quality and diversity is fixing\nthe required minimum distance between solutions while simultaneously obtaining\nthe best possible fitness. Recent work by Santoni et al. [arXiv 2024] revealed\nthat this setting is not well addressed by state-of-the-art algorithms,\nperforming in par or worse than pure random sampling.\n  Driven by this important limitation, we propose a new approach, where\nparallel runs of the covariance matrix adaptation evolution strategy (CMA-ES)\ninherit tabu regions in a cascading fashion. We empirically demonstrate that\nour CMA-ES-Diversity Search (CMA-ES-DS) algorithm generates trajectories that\nallow to extract high-quality solution batches that respect a given minimum\ndistance requirement, clearly outperforming those obtained from off-the-shelf\nrandom sampling, multi-modal optimization algorithms, and standard CMA-ES.",
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Large Language Models (LLMs) have demonstrated great potential as evaluators\nof NLG systems, allowing for high-quality, reference-free, and multi-aspect\nassessments. However, existing LLM-based metrics suffer from two major\ndrawbacks: reliance on proprietary models to generate training data or perform\nevaluations, and a lack of fine-grained, explanatory feedback. In this paper,\nwe introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation\nmetric that provides accurate explanations based on error spans. OpeNLGauge is\navailable as a two-stage ensemble of larger open-weight LLMs, or as a small\nfine-tuned evaluation model, with confirmed generalizability to unseen tasks,\ndomains and aspects. Our extensive meta-evaluation shows that OpeNLGauge\nachieves competitive correlation with human judgments, outperforming\nstate-of-the-art models on certain tasks while maintaining full reproducibility\nand providing explanations more than twice as accurate.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "We explore the moduli space $\\mathcal{M}_d^1$ of degree $d \\geq 3$ rational\nfunctions on the projective line using a machine learning approach, focusing on\nautomorphism group classification. For $d = 3$, where $\\mathcal{M}_3^1 =\n{\\mathbb P}_{\\mathbf{w}}^5 ({\\mathbb Q})$ with weights $\\mathbf{w} = (2, 2, 3,\n3, 4, 6)$, we generate a dataset of 2,078,697 rational functions over $\\Q$ with\nnaive height $\\leq 4$. Initial coefficient-based models achieved high overall\naccuracy but struggled with minority classes due to extreme class imbalance. By\nusing invariants $\\xi_0, \\ldots, \\xi_5$ as features in a Random Forest\nclassifier, we achieved approximately 99.992\\% accuracy, mirroring successes in\ngenus 2 curves \\cite{2024-03}. This highlights the transformative role of\ninvariants in arithmetic dynamics, yet for $d > 3$, unknown generators of\n$\\mathcal{R}_{(d+1, d-1)}$ pose scalability challenges. Our framework bridges\ndata-driven and algebraic methods, with potential extensions to higher degrees\nand $\\mathcal{M}_d^2$.",
        "Surficial geologic mapping is essential for understanding Earth surface\nprocesses, addressing modern challenges such as climate change and national\nsecurity, and supporting common applications in engineering and resource\nmanagement. However, traditional mapping methods are labor-intensive, limiting\nspatial coverage and introducing potential biases. To address these\nlimitations, we introduce EarthScape, a novel, AI-ready multimodal dataset\nspecifically designed for surficial geologic mapping and Earth surface\nanalysis. EarthScape integrates high-resolution aerial RGB and near-infrared\n(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain\nfeatures, and hydrologic and infrastructure vector data. The dataset provides\ndetailed annotations for seven distinct surficial geologic classes encompassing\nvarious geological processes. We present a comprehensive data processing\npipeline using open-sourced raw data and establish baseline benchmarks using\ndifferent spatial modalities to demonstrate the utility of EarthScape. As a\nliving dataset with a vision for expansion, EarthScape bridges the gap between\ncomputer vision and Earth sciences, offering a valuable resource for advancing\nresearch in multimodal learning, geospatial analysis, and geological mapping.\nOur code is available at https:\/\/github.com\/masseygeo\/earthscape.",
        "A major open issue concerning the active Sun is the effectiveness with which\nmagnetic reconnection accelerates electrons in flares. A paper published by\n{\\em{Nature}} in 2022 used microwave observations to conclude that the Sun is\nan almost ideal accelerator, energizing nearly all electrons within a coronal\nvolume to nonthermal energies. Shortly thereafter, a paper published in\n{\\em{Astrophysical Journal Letters}} used hard X-ray measurements \\emph{of the\nsame event} to reach the contradictory conclusion that less than 1\\% of the\navailable electrons were accelerated. Here we address this controversy by using\nspatially resolved observations of hard X-ray emission and a spectral inversion\nmethod to determine the evolution of the electron spectrum throughout the\nflare. So we estimated the density of the medium where electrons accelerate\nand, from this, the ratio of accelerated to ambient electron densities. Results\nshow that this ratio never exceeds a percent or so in the cases analyzed.",
        "We study the connectivity of Morse boundaries of Coxeter groups. We define\ntwo conditions on the defining graph of a Coxeter group: wide-avoidant and\nwide-spherical-avoidant. We show that wide-spherical-avoidant, one-ended,\naffine-free Coxeter groups have connected and locally connected Morse\nboundaries. On the other hand, one-ended Coxeter groups that are not\nwide-avoidant and not wide have disconnected Morse boundary. For the\nright-angled case, we get a full characterization: a one-ended right-angled\nCoxeter group has connected, non-empty Morse boundary if and only if it is\nwide-avoidant. Along the way we characterize Morse geodesic rays in affine-free\nCoxeter groups as those that spend uniformly bounded time in cosets of wide\nspecial subgroups.",
        "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
        "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
        "Transformers, as a fundamental deep learning architecture, have demonstrated\nremarkable capabilities in reasoning. This paper investigates the generalizable\nfirst-order logical reasoning ability of transformers with their parameterized\nknowledge and explores ways to improve it. The first-order reasoning capability\nof transformers is assessed through their ability to perform first-order\nlogical entailment, which is quantitatively measured by their performance in\nanswering knowledge graph queries. We establish connections between (1) two\ntypes of distribution shifts studied in out-of-distribution generalization and\n(2) the unseen knowledge and query settings discussed in the task of knowledge\ngraph query answering, enabling a characterization of fine-grained\ngeneralizability. Results on our comprehensive dataset show that transformers\noutperform previous methods specifically designed for this task and provide\ndetailed empirical evidence on the impact of input query syntax, token\nembedding, and transformer architectures on the reasoning capability of\ntransformers. Interestingly, our findings reveal a mismatch between positional\nencoding and other design choices in transformer architectures employed in\nprior practices. This discovery motivates us to propose a more sophisticated,\nlogic-aware architecture, TEGA, to enhance the capability for generalizable\nfirst-order logical entailment in transformers.",
        "With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps:\/\/github.com\/CoderChen01\/LVLMSarcasmAnalysis",
        "We consider positive solutions to $\\displaystyle -\\Delta_p\nu=\\frac{1}{u^\\gamma}+f(u)$ under zero Dirichlet condition in the half space.\nExploiting a prio-ri estimates and the moving plane technique, we prove that\nany solution is monotone increasing in the direction orthogonal to the\nboundary.",
        "In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.",
        "In this work, we investigate the efficacy of various adapter architectures on\nsupervised binary classification tasks from the SuperGLUE benchmark as well as\na supervised multi-class news category classification task from Kaggle.\nSpecifically, we compare classification performance and time complexity of\nthree transformer models, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art (SoTA) adapter\narchitectures. Our analysis reveals performance differences across adapter\narchitectures, highlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the training time. Similar\nresults are observed on the new classification task, further supporting our\nfindings and demonstrating adapters as efficient and flexible alternatives to\nfine-tuning. This study provides valuable insights and guidelines for selecting\nand implementing adapters in diverse natural language processing (NLP)\napplications.",
        "We discuss the preparation of topological insulator states with fermionic\nultracold atoms in optical lattices by means of measurement-based Markovian\nfeedback control. The designed measurement and feedback operators induce an\neffective dissipative channel that stabilizes the desired insulator state,\neither in an exact way or approximately in the case where additional\nexperimental constraints are assumed. Successful state preparation is\ndemonstrated in one-dimensional insulators as well as for Haldane's Chern\ninsulator, by calculating the fidelity between the target ground state and the\nsteady state of the feedback-modified master equation. The fidelity is obtained\nvia time evolution of the system with moderate sizes. For larger 2D systems, we\ncompare the mean occupation of the single-particle eigenstates for the ground\nand steady state computed through mean-field kinetic equations.",
        "Neural network-based methods have demonstrated effectiveness in solving\nhigh-dimensional Mean-Field Games (MFG) equilibria, yet ensuring mathematically\nconsistent density-coupled evolution remains a major challenge. This paper\nproposes the NF-MKV Net, a neural network approach that integrates\nprocess-regularized normalizing flow (NF) with state-policy-connected\ntime-series neural networks to solve MKV FBSDEs and their associated\nfixed-point formulations of MFG equilibria. The method first reformulates MFG\nequilibria as MKV FBSDEs, embedding density evolution into equation\ncoefficients within a probabilistic framework. Neural networks are then\nemployed to approximate value functions and their gradients. To enforce\nvolumetric invariance and temporal continuity, NF architectures impose loss\nconstraints on each density transfer function."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"E(n) Equivariant Graph Neural Networks",
    "start_abstract":"This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Trends in Phase II Trials for Cancer Therapies"
      ],
      "abstract":[
        "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Stability of difference equations with interspecific density dependence,\n  competition, and maturation delays",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "MorphoITH: A Framework for Deconvolving Intra-Tumor Heterogeneity Using\n  Tissue Morphology",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "R3F: An R package for evolutionary dates, rates, and priors using the\n  relative rate framework",
        "Multicellular self-organization in Escherichia coli",
        "Characterizing the Conformational States of G Protein Coupled Receptors\n  Generated with AlphaFold",
        "The time-dependent reproduction number for epidemics in heterogeneous\n  populations",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Mechanoreceptive A$\\beta$ primary afferents discriminate naturalistic\n  social touch inputs at a functionally relevant time scale",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "On the eternal non-Markovianity of qubit maps",
        "Micro Blossom: Accelerated Minimum-Weight Perfect Matching Decoding for\n  Quantum Error Correction",
        "Search for continuous gravitational wave signals from luminous dark\n  photon superradiance clouds with LVK O3 observations",
        "Asymptotics for multiple $q$-orthogonal polynomials from the RHP",
        "Scalable First-order Method for Certifying Optimal k-Sparse GLMs",
        "Towards Transparent and Accurate Plasma State Monitoring at JET",
        "Quark number susceptibility and conserved charge fluctuation for\n  (2+1)-flavor QCD with M\\\"obius domain wall fermions",
        "Galaxy-cluster-stacked Fermi-LAT III: substructure and radio-relic\n  counterparts",
        "Erosion of a dense molecular core by a strong outflow from a massive\n  protostar",
        "Reporting on pTP sublimation during evaporation deposition",
        "Parameter Invariance Analysis of Moment Equations Using\n  Dulmage-Mendelsohn Decomposition",
        "Singularity-Based Consistent QML Estimation of Multiple Breakpoints in\n  High-Dimensional Factor Models",
        "Spectral Analysis and Invariant Measure in Studies of the Dynamics of\n  the Hemostasis of a Blood Vessel",
        "The Evolution of Hypervelocity Supernova Survivors and the Outcomes of\n  Interacting Double White Dwarf Binaries",
        "$K$-theoretic pullbacks for Lagrangians on derived critical loci"
      ],
      "abstract":[
        "A general system of difference equations is presented for multispecies\ncommunities with density dependent population growth and delayed maturity.\nInterspecific competition, mutualism, predation, commensalism, and amensalism\nare accommodated. A sufficient condition for the local asymptotic stability of\na coexistence equilibrium in this system is then proven. Using this system, the\ngeneralisation of the Beverton-Holt and Leslie-Gower models of competition to\nmultispecies systems with possible maturation delays is presented and shown to\nyield interesting stability properties. The stability of coexistence depends on\nthe relative abundances of the species at the unique interior equilibrium. A\nsufficient condition for local stability is derived that only requires\nintraspecific competition to outweigh interspecific competition. The condition\ndoes not depend on maturation delays. The derived stability properties are used\nto develop a novel estimation approach for the coefficients of interspecific\ncompetition. This approach finds an optimal configuration given the conjecture\nthat coexisting species strive to outcompete competitors and are most likely\nobserved near the stable interior equilibrium with the greatest dampening of\nperturbations. The optimal solution is compared to estimates of niche overlap\nusing an empirical example of malaria mosquito vectors with delayed maturity in\nthe Anopheles gambiae sensu lato species complex.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "The ability of tumors to evolve and adapt by developing subclones in\ndifferent genetic and epigenetic states is a major challenge in oncology.\nTraditional tools like multi-regional sequencing used to study tumor evolution\nand the resultant intra-tumor heterogeneity (ITH) are often impractical because\nof their resource-intensiveness and limited scalability. Here, we present\nMorphoITH, a novel framework that leverages histopathology slides to deconvolve\nmolecular ITH through tissue morphology. MorphoITH integrates a self-supervised\ndeep learning similarity measure to capture phenotypic variation across\nmultiple dimensions (cytology, architecture, and microenvironment) with\nrigorous methods to eliminate spurious sources of variation. Using a prototype\nof ITH, clear cell renal cell carcinoma (ccRCC), we show that MorphoITH\ncaptures clinically-significant biological features, such as vascular\narchitecture and nuclear grades. Furthermore, we find that MorphoITH recognizes\ndifferential biological states corresponding to subclonal changes in key driver\ngenes (BAP1\/PBRM1\/SETD2). Finally, by applying MorphoITH to a multi-regional\nsequencing experiment, we postulate evolutionary trajectories that largely\nrecapitulate genetic evolution. In summary, MorphoITH provides a scalable\nphenotypic lens that bridges the gap between histopathology and genomics,\nadvancing precision oncology.",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "The relative rate framework (RRF) can estimate divergence times from branch\nlengths in a phylogeny, which is the theoretical basis of the RelTime method\nfrequently applied, a relaxed clock approach for molecular dating that scales\nwell for large phylogenies. The use of RRF has also enabled the development of\ncomputationally efficient and accurate methods for testing the autocorrelation\nof lineage rates in a phylogeny (CorrTest) and selecting data-driven parameters\nof the birth-death speciation model (ddBD), which can be used to specify priors\nin Bayesian molecular dating. We have developed R3F, an R package implementing\nRRF to estimate divergence times, infer lineage rates, conduct CorrTest, and\nbuild a ddBD tree prior for Bayesian dating in molecular phylogenies. Here, we\ndescribe R3F functionality and explain how to interpret and use its outputs in\nother visualization software and packages, such as MEGA, ggtree, and FigTree.\nUltimately, R3F is intended to enable the dating of the Tree of Life with\ngreater accuracy and precision, which would have important implications for\nstudies of organism evolution, diversification dynamics, phylogeography, and\nbiogeography. Availability and Implementation: The source codes and related\ninstructions for installing and implementing R3F are available from GitHub\n(https:\/\/github.com\/cathyqqtao\/R3F).",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "G-Protein Coupled Receptors (GPCRs) are integral to numerous physiological\nprocesses and are the target of approximately one-third of FDA-approved\ntherapeutics. Despite their significance, only a limited subset of GPCRs has\nbeen successfully targeted, primarily due to challenges in accurately modeling\ntheir structures. AlphaFold, a state-of-the-art deep learning model, has\ndemonstrated remarkable capability in predicting protein structures with high\naccuracy. This study conducts an evaluation of AlphaFold performance in\npredicting GPCR structures and their conformational states by comparing its\npredictions to experimentally determined structures using metrics such as\naverage deformation between alpha carbon atoms and the Helix 3 - Helix 6\n(H3-H6) distance. Our analysis reveals that both AlphaFold 2 (AF2) and\nAlphaFold 3 (AF3) produce more accurate predictions for GPCRs in inactive\nconformations, with lower activity levels correlating with smaller\ndeformations. Conversely, higher activity levels are associated with increased\nvariability in AlphaFold performance due to difficulties with accurately\npredicting conformational changes upon GPCR activation and ligand binding.\nAdditionally, AlphaFold performance varies across different GPCR classes,\ninfluenced by the availability and quality of training data as well as the\nstructural complexity and diversity of the receptors. These findings\ndemonstrate the potential of AlphaFold in advancing drug discovery efforts,\nwhile also highlighting the necessity for continued refinement to enhance\npredictive accuracy for active conformations.",
        "The time-dependent reproduction number Rt can be used to track pathogen\ntransmission and to assess the efficacy of interventions. This quantity can be\nestimated by fitting renewal equation models to time series of infectious\ndisease case counts. These models almost invariably assume a homogeneous\npopulation. Individuals are assumed not to differ systematically in the rates\nat which they come into contact with others. It is also assumed that the\ntypical time that elapses between one case and those it causes (known as the\ngeneration time distribution) does not differ across groups. But contact\npatterns are known to widely differ by age and according to other demographic\ngroupings, and infection risk and transmission rates have been shown to vary\nacross groups for a range of directly transmitted diseases. Here, we derive\nfrom first principles a renewal equation framework which accounts for these\ndifferences in transmission across groups. We use a generalisation of the\nclassic McKendrick-von Foerster equation to handle populations structured into\ninteracting groups. This system of partial differential equations allows us to\nderive a simple analytical expression for Rt which involves only group-level\ncontact patterns and infection risks. We show that the same expression emerges\nfrom both deterministic and stochastic discrete-time versions of the model and\ndemonstrate via simulations that our Rt expression governs the long-run fate of\nepidemics. Our renewal equation model provides a basis from which to account\nfor more realistic, diverse populations in epidemiological models and opens the\ndoor to inferential approaches which use known group characteristics to\nestimate Rt.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Interpersonal touch is an important channel of social emotional interaction.\nHow these physical skin-to-skin touch expressions are processed in the\nperipheral nervous system is not well understood. From microneurography\nrecordings in humans, we evaluated the capacity of six subtypes of cutaneous\nmechanoreceptive afferents to differentiate human-delivered social touch\nexpressions. Leveraging statistical and classification analyses, we found that\nsingle units of multiple mechanoreceptive A$\\beta$ subtypes, especially slowly\nadapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can\nreliably differentiate social touch expressions at accuracies similar to human\nrecognition. We then identified the most informative firing patterns of SA-II\nand HFA afferents, which indicate that average durations of 3-4 s of firing\nprovide sufficient discriminative information. Those two subtypes also exhibit\nrobust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch\nexpressions due to their specific firing properties. Greater shifts in\nspike-timing, however, can change a firing pattern's envelope to resemble that\nof another expression and drastically compromise an afferent's discrimination\ncapacity. Altogether, the findings indicate that SA-II and HFA afferents\ndifferentiate the skin contact of social touch at time scales relevant for such\ninteractions, which are 1-2 orders of magnitude longer than those for\nnon-social touch.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "As is well known, unital Pauli maps can be eternally non-CP-divisible. In\ncontrast, here we show that in the case of non-unital maps, eternal\nnon-Markovianity in the non-unital part is ruled out. In the unital case, the\neternal non-Markovianity can be obtained by a convex combination of two\ndephasing semigroups, but not all three of them. We study these results and the\nramifications arising from them.",
        "Minimum-Weight Perfect Matching (MWPM) decoding is important to quantum error\ncorrection decoding because of its accuracy. However, many believe that it is\ndifficult, if possible at all, to achieve the microsecond latency requirement\nposed by superconducting qubits. This work presents the first publicly known\nMWPM decoder, called Micro Blossom, that achieves sub-microsecond decoding\nlatency. Micro Blossom employs a heterogeneous architecture that carefully\npartitions a state-of-the-art MWPM decoder between software and a programmable\naccelerator with parallel processing units, one of each vertex\/edge of the\ndecoding graph. On a surface code with code distance $d$ and a circuit-level\nnoise model with physical error rate $p$, Micro Blossom's accelerator employs\n$O(d^3)$ parallel processing units to reduce the worst-case latency from\n$O(d^{12})$ to $O(d^9)$ and reduce the average latency from $O(p d^3+1)$ to\n$O(p^2 d^2+1)$ when $p \\ll 1$.\n  We report a prototype implementation of Micro Blossom using FPGA. Measured at\n$d=13$ and $p=0.1\\%$, the prototype achieves an average decoding latency of\n$0.8 \\mu s$ at a moderate clock frequency of $62 MHz$. Micro Blossom is the\nfirst publicly known hardware-accelerated exact MWPM decoder, and the decoding\nlatency of $0.8 \\mu s$ is 8 times shorter than the best latency of MWPM decoder\nimplementations reported in the literature.",
        "Superradiance clouds of kinetically-mixed dark photons around spinning black\nholes can produce observable multi-messenger electromagnetic and gravitational\nwave signals. The cloud generates electric fields of up to a\nTeravolt-per-meter, which lead to a cascade production of charged particles,\nyielding a turbulent quasi-equilibrium plasma around the black hole, and\nresulting in electromagnetic fluxes ranging from supernova to pulsar-like\nluminosities. For stellar mass black holes, such systems resemble millisecond\npulsars and are expected to emit pulsating radio waves and continuous\ngravitational waves (CWs) within the LIGO-Virgo-KAGRA (LVK) sensitivity band.\nWe select 44 sources with approximately coincident frequencies or positive\nfrequency drifts from existing pulsar catalogs as potential candidates of\nlong-lasting superradiance clouds around old galactic black holes. For a subset\nof 34 sources that are well measured and have not been previously targeted, we\nperform the first search for CW emission in LVK data from the third observing\nrun. We find no evidence of a CW signal and place 95% confidence level upper\nlimits on the emitted strain amplitude. We interpret these results, together\nwith limits from previous searches, in terms of the underlying dark photon\ntheory by performing an analysis of the expected signals from superradiance\nclouds from galactic black holes. We find that, even for moderately spinning\nblack holes, the absence of an observed CW signal disfavors a discrete set of\ndark photon masses between about $10^{-13}$ $\\rm{eV}\/c^2$ and $10^{-12}$\n$\\rm{eV}\/c^2$ and kinetic mixing couplings in the range of $10^{-9}$-$10^{-7}$,\nsubject to assumptions about the properties of the black hole population and\nthe cloud's electromagnetic emission.",
        "We deduce the asymptotic behaviour of a broad class of multiple\n$q$-orthogonal polynomials as their degree tends to infinity. We achieve this\nby rephrasing multiple $q$-orthogonal polynomials as part of a solution to a\nRiemann Hilbert Problem (RHP). In particular, we study multiple $q$-orthogonal\npolynomials of the first kind (see [12]), which are Type II orthogonal\npolynomials with weights given by\n  \\begin{equation}\n  w_1(x) = x^\\alpha \\omega(x)d_qx,\\qquad w_2(x) = x^\\beta \\omega(x)d_qx,\n\\nonumber \\end{equation} which satisfy the constraint \\begin{equation}\\nonumber\n  |\\omega(q^{2n})-1| = \\mathcal{O}(q^{2n}), \\end{equation} as $n\\to \\infty$.\nUsing $q$-calculus we obtain detailed asymptotics for these polynomials from\nthe RHP. This class of polynomials studied was chosen in part to their\nconnection to the work of [11,12], concerning the irrationality of $\\zeta_q(1)$\nand $\\zeta_q(2)$. To conduct our asymptotic analysis we will require the\nfollowing added restrictions on $w_1(x)$ and $w_2(x)$: $\\alpha \\notin\n\\mathbb{Z}$, $\\beta \\notin \\mathbb{Z}$ and $\\alpha \\neq \\beta \\mod \\mathbb{Z}$.\nThese restrictions are necessary for the asymptotic analysis but not the\nstatement of multiple $q$-orthogonal polynomials as solutions to a RHP.\n  The author wishes to extend special thanks to Prof. Walter Van Assche, who\nmotivated this studied and provided valuable discussion.",
        "This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.",
        "Controlling and monitoring plasma within a tokamak device is complex and\nchallenging. Plasma off-normal events, such as disruptions, are hindering\nsteady-state operation. For large devices, they can even endanger the machine's\nintegrity and it represents in general one of the most serious concerns for the\nexploitation of the tokamak concept for future power plants. Effective plasma\nstate monitoring carries the potential to enable an understanding of such\nphenomena and their evolution which is crucial for the successful operation of\ntokamaks. This paper presents the application of a transparent and data-driven\nmethodology to monitor the plasma state in a tokamak. Compared to previous\nstudies in the field, supervised and unsupervised learning techniques are\ncombined. The dataset consisted of 520 expert-validated discharges from JET.\nThe goal was to provide an interpretable plasma state representation for the\nJET operational space by leveraging multi-task learning for the first time in\nthe context of plasma state monitoring. When evaluated as disruption\npredictors, a sequence-based approach showed significant improvements compared\nto the state-based models. The best resulting network achieved a promising\ncross-validated success rate when combined with a physical indicator and\naccounting for nearby instabilities. Qualitative evaluations of the learned\nlatent space uncovered operational and disruptive regions as well as patterns\nrelated to learned dynamics and global feature importance. The applied\nmethodology provides novel possibilities for the definition of triggers to\nswitch between different control scenarios, data analysis, and learning as well\nas exploring latent dynamics for plasma state monitoring. It also showed\npromising quantitative and qualitative results with warning times suitable for\navoidance purposes and distributions that are consistent with known physical\nmechanisms.",
        "We present quark number susceptibilities and conserved charge fluctuations\nfor (2+1)-flavor QCD using M\\\"obius Domain Wall fermions with a pion mass of\n\\(135~\\rm{MeV}\\). Our results are compared with hadron resonance gas models\nbelow the QCD transition temperature and with \\(\\mathcal{O}(g^2)\\) perturbation\ntheory at high temperatures. Additionally, we compare our findings with results\nfrom staggered fermion discretizations. Furthermore, we also present results of\nleading order Kurtosis of electric charge and strangeness fluctuations.",
        "Faint $\\gamma$-ray signatures emerge in Fermi-LAT data stacked scaled to the\ncharacteristic $R_{500}$ radii of MCXC galaxy clusters. This third paper in a\nseries shows a $4.3\\sigma$ excess of discrete 4FGL-DR4 catalog $\\gamma$-ray\nsources at the $r<1.5R_{500}$ radii of 205 clusters, coincident with an $r\\sim\nR_{500}$ diffuse $2.6\\sigma$ excess of 1-100 GeV emission from 75 high-latitude\nclusters. The source excess becomes highly ($>5\\sigma$) significant when\nconsidering the substantial ($3.4\\sigma$) and unexpectedly rapid quenching of\n$\\gamma$-ray sources just inside the virial shock. The excess sources show\nradial, spectral, and luminosity distributions better matching radio-relic\ncounterparts or substructure than present tentative classifications as\nblazar-candidates. Their spectral distribution is bimodal: flat-spectrum\nsources are consistent with enhanced hadronic emission behind weak, Mach\n$\\sim2$ shocks, while softer sources may be phoenix counterparts.",
        "We present Atacama Large Millimeter\/submillimeter Array Band 3 observations\nof N$_2$H$^+$ (1-0) and CH$_3$CN (5-4), as well as Band 7 observations of the\nH$_2$CO molecular line emissions from the protostellar system GGD 27-MM2(E).\nThrough position-velocity diagrams along and across the outflow axis, we study\nthe kinematics and structure of the outflow. We also fit extracted spectra of\nthe CH$_3$CN emission to obtain the physical conditions of the gas. We use the\nresults to discuss the impact of the outflow on its surroundings. We find that\nN$_2$H$^+$ emission traces a dense molecular cloud surrounding GGD 27-MM2(E).\nWe estimate that the mass of this cloud is $\\sim$13.3-26.5 M$_\\odot$. The\nmolecular cloud contains an internal cavity aligned with the H$_2$CO-traced\nmolecular outflow. The outflow, also traced by $\\mathrm{CH_3 CN}$, shows\nevidence of a collision with a molecular core (MC), as indicated by the\ndistinctive increases in the distinct physical properties of the gas such as\nexcitation temperature, column density, line width, and velocity. This\ncollision results in an X-shape structure in the northern part of the outflow\naround the position of the MC, which produces spray-shocked material downstream\nin the north of MC as observed in position-velocity diagrams both along and\nacross of the outflow axis. The outflow has a mass of 1.7-2.1 M$_\\odot$, a\nmomentum of 7.8-10.1 M$_\\odot$ km s$^{-1}$, a kinetic energy of 5.0-6.6$\\times\n10^{44}$ erg, and a mass loss rate of 4.9--6.0$\\times10^{-4}$ M$_\\odot$\nyr$^{-1}$. The molecular outflow from GGD 27-MM2(E) significantly perturbs and\nerodes its parent cloud, compressing the gas of sources such as MC and ALMA 12.\nThe feedback from this powerful protostellar outflow contributes to maintain\nthe turbulence in the surrounding area.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Living organisms maintain stable functioning amid environmental fluctuations\nthrough homeostasis, a mechanism that preserves a system's behavior despite\nchanges in environmental conditions. To elucidate homeostasis in stochastic\nbiochemical reactions, theoretical tools for assessing population-level\ninvariance under parameter perturbations are crucial. In this paper, we propose\na systematic method for identifying the stationary moments that remain\ninvariant under parameter perturbations by leveraging the structural properties\nof the stationary moment equations. A key step in this development is\naddressing the underdetermined nature of moment equations, which has\ntraditionally made it difficult to characterize how stationary moments depend\non system parameters. To overcome this, we utilize the Dulmage-Mendelsohn (DM)\ndecomposition of the coefficient matrix to extract welldetermined subequations\nand reveal their hierarchical structure. Leveraging this structure, we identify\nstationary moments whose partial derivatives with respect to parameters are\nstructurally zero, facilitating the exploration of fundamental constraints that\ngovern homeostatic behavior in stochastic biochemical systems.",
        "This paper investigates the estimation of high-dimensional factor models in\nwhich factor loadings undergo an unknown number of structural changes over\ntime. Given that a model with multiple changes in factor loadings can be\nobservationally indistinguishable from one with constant loadings but varying\nfactor variances, this reduces the high-dimensional structural change problem\nto a lower-dimensional one. Due to the presence of multiple breakpoints, the\nfactor space may expand, potentially causing the pseudo factor covariance\nmatrix within some regimes to be singular. We define two types of breakpoints:\n{\\bf a singular change}, where the number of factors in the combined regime\nexceeds the minimum number of factors in the two separate regimes, and {\\bf a\nrotational change}, where the number of factors in the combined regime equals\nthat in each separate regime. Under a singular change, we derive the properties\nof the small eigenvalues and establish the consistency of the QML estimators.\nUnder a rotational change, unlike in the single-breakpoint case, the pseudo\nfactor covariance matrix within each regime can be either full rank or\nsingular, yet the QML estimation error for the breakpoints remains stably\nbounded. We further propose an information criterion (IC) to estimate the\nnumber of breakpoints and show that, with probability approaching one, it\naccurately identifies the true number of structural changes. Monte Carlo\nsimulations confirm strong finite-sample performance. Finally, we apply our\nmethod to the FRED-MD dataset, identifying five structural breaks in factor\nloadings between 1959 and 2024.",
        "A mathematical model of atherosclerosis of a blood vessel is advanced with\nregard for the entry of low-density lipoproteins (LDLs) into blood. For the\nfirst time, the influence of cytokines on the inflammation of a blood vessel at\nthe formation of atherosclerotic plaques is taken into account. With the help\nof the expansion in a Fourier series and the calculation of an invariant\nmeasure, the scenario of the appearance of strange attractors depending on a\nchange in the parameter of the dissipation of cholesterol is studied. The\nconclusion is made about the interconnection of the dynamics of the metabolic\nprocess in a blood vascular system and its physical state.",
        "The recent prediction and discovery of hypervelocity supernova survivors has\nprovided strong evidence that the \"dynamically driven double-degenerate\ndouble-detonation\" (D6) Type Ia supernova scenario occurs in Nature. In this\nmodel, the accretion stream from the secondary white dwarf in a double white\ndwarf binary strikes the primary white dwarf violently enough to trigger a\nhelium shell detonation, which in turn triggers a carbon\/oxygen core\ndetonation. If the secondary white dwarf survives the primary's explosion, it\nwill be flung away as a hypervelocity star. While previous work has shown that\nthe hotter observed D6 stars can be broadly understood as secondaries whose\nouter layers have been heated by their primaries' explosions, the properties of\nthe cooler D6 stars have proven difficult to reproduce. In this paper, we show\nthat the cool D6 stars can be explained by the Kelvin-Helmholtz contraction of\nhelium or carbon\/oxygen white dwarfs that underwent significant mass loss and\ncore heating prior to and during the explosion of their white dwarf companions.\nWe find that the current population of known D6 candidates is consistent with\n~2% of Type Ia supernovae leaving behind a hypervelocity surviving companion.\nWe also calculate the evolution of hot, low-mass oxygen\/neon stars and find\nreasonable agreement with the properties of the LP 40-365 class of\nhypervelocity survivors, suggesting that these stars are the kicked remnants of\nnear-Chandrasekhar-mass oxygen\/neon white dwarfs that were partially disrupted\nby oxygen deflagrations. We use these results as motivation for schematic\ndiagrams showing speculative outcomes of interacting double white dwarf\nbinaries, including long-lived merger remnants, Type Ia supernovae, and several\nkinds of peculiar transients.",
        "Given a regular function $\\phi$ on a smooth stack, and a $(-1)$-shifted\nLagrangian $M$ on the derived critical locus of $\\phi$, under fairly general\nhypotheses, we construct a pullback map from the Grothendieck group of coherent\nmatrix factorizations of $\\phi$ to that of coherent sheaves on $M$. This map\nsatisfies a functoriality property with respect to the composition of\nLagrangian correspondences, as well as the usual bivariance and base-change\nproperties.\n  We provide three applications of the construction, one in the definition of\nquantum $K$-theory of critical loci (Landau-Ginzburg models), paving the way to\ngeneralize works of Okounkov school from Nakajima quiver varieties to quivers\nwith potentials, one in establishing a degeneration formula for $K$-theoretic\nDonaldson-Thomas theory of local Calabi-Yau 4-folds, the other in confirming a\n$K$-theoretic version of Joyce-Safronov conjecture."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Human-Centric Interfaces for Ambient Intelligence",
    "start_abstract":"To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence.",
    "start_categories":[
      "physics.app-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Speaker Diarization with LSTM"
      ],
      "abstract":[
        "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate\n  personalized feedback",
        "From Screens to Scenes: A Survey of Embodied AI in Healthcare",
        "Economic Rationality under Specialization: Evidence of Decision Bias in\n  AI Agents",
        "Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through\n  Multi-Agent Reinforcement Learning",
        "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment",
        "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of\n  Role-Playing Language Agents",
        "A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large\n  Language Models Using Cross-Attention Signals",
        "Narrative-Driven Travel Planning: Geoculturally-Grounded Script\n  Generation with Evolutionary Itinerary Optimization",
        "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language\n  Models for Navigation Applications",
        "Representation Learning to Advance Multi-institutional Studies with\n  Electronic Health Record Data",
        "DeclareAligner: A Leap Towards Efficient Optimal Alignments for\n  Declarative Process Model Conformance Checking",
        "AlgoPilot: Fully Autonomous Program Synthesis Without Human-Written\n  Programs",
        "Multi-modal Speech Enhancement with Limited Electromyography Channels",
        "Spline Quantile Regression",
        "Custom Loss Functions in Fuel Moisture Modeling",
        "Interior control for surfaces with positive scalar curvature and its\n  application",
        "Estimates for short character sums evaluated at homogeneous polynomials",
        "An Explainable Pipeline for Machine Learning with Functional Data",
        "Mamba-Shedder: Post-Transformer Compression for Efficient Selective\n  Structured State Space Models",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Data-driven geometric parameter optimization for PD-GMRES",
        "Diagrammatic Categories which arise from Representation Graphs",
        "Detecting Heel Strike and toe off Events Using Kinematic Methods and\n  LSTM Models",
        "Approximation properties of neural ODEs",
        "Objective Metrics for Human-Subjects Evaluation in Explainable\n  Reinforcement Learning",
        "Unraveling Pedestrian Fatality Patterns: A Comparative Study with\n  Explainable AI",
        "Duoidal R-Matrices"
      ],
      "abstract":[
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Grading student assignments in STEM courses is a laborious and repetitive\ntask for tutors, often requiring a week to assess an entire class. For\nstudents, this delay of feedback prevents iterating on incorrect solutions,\nhampers learning, and increases stress when exercise scores determine admission\nto the final exam. Recent advances in AI-assisted education, such as automated\ngrading and tutoring systems, aim to address these challenges by providing\nimmediate feedback and reducing grading workload. However, existing solutions\noften fall short due to privacy concerns, reliance on proprietary closed-source\nmodels, lack of support for combining Markdown, LaTeX and Python code, or\nexcluding course tutors from the grading process. To overcome these\nlimitations, we introduce PyEvalAI, an AI-assisted evaluation system, which\nautomatically scores Jupyter notebooks using a combination of unit tests and a\nlocally hosted language model to preserve privacy. Our approach is free,\nopen-source, and ensures tutors maintain full control over the grading process.\nA case study demonstrates its effectiveness in improving feedback speed and\ngrading efficiency for exercises in a university-level course on numerics.",
        "Healthcare systems worldwide face persistent challenges in efficiency,\naccessibility, and personalization. Powered by modern AI technologies such as\nmultimodal large language models and world models, Embodied AI (EmAI)\nrepresents a transformative frontier, offering enhanced autonomy and the\nability to interact with the physical world to address these challenges. As an\ninterdisciplinary and rapidly evolving research domain, \"EmAI in healthcare\"\nspans diverse fields such as algorithms, robotics, and biomedicine. This\ncomplexity underscores the importance of timely reviews and analyses to track\nadvancements, address challenges, and foster cross-disciplinary collaboration.\nIn this paper, we provide a comprehensive overview of the \"brain\" of EmAI for\nhealthcare, wherein we introduce foundational AI algorithms for perception,\nactuation, planning, and memory, and focus on presenting the healthcare\napplications spanning clinical interventions, daily care & companionship,\ninfrastructure support, and biomedical research. Despite its promise, the\ndevelopment of EmAI for healthcare is hindered by critical challenges such as\nsafety concerns, gaps between simulation platforms and real-world applications,\nthe absence of standardized benchmarks, and uneven progress across\ninterdisciplinary domains. We discuss the technical barriers and explore\nethical considerations, offering a forward-looking perspective on the future of\nEmAI in healthcare. A hierarchical framework of intelligent levels for EmAI\nsystems is also introduced to guide further development. By providing\nsystematic insights, this work aims to inspire innovation and practical\napplications, paving the way for a new era of intelligent, patient-centered\nhealthcare.",
        "In the study by Chen et al. (2023) [01], the large language model GPT\ndemonstrated economic rationality comparable to or exceeding the average human\nlevel in tasks such as budget allocation and risk preference. Building on this\nfinding, this paper further incorporates specialized agents, such as\nbiotechnology experts and economists, for a horizontal comparison to explore\nwhether specialization can enhance or maintain economic rationality equivalent\nto that of GPT in similar decision-making scenarios. The results indicate that\nwhen agents invest more effort in specialized fields, their decision-making\nbehavior is more prone to 'rationality shift,' specifically manifested as\nincreased violations of GARP (Generalized Axiom of Revealed Preference),\ndecreased CCEI (Critical Cost Efficiency Index), and more significant decision\ndeviations under high-risk conditions. In contrast, GPT and more generalized\nbasic agents maintain a more stable and consistent level of rationality across\nmultiple tasks. This study reveals the inherent conflict between specialization\nand economic rationality, providing new insights for constructing AI\ndecision-making systems that balance specialization and generalization across\nvarious scenarios.",
        "The effective design of patrol strategies is a difficult and complex problem,\nespecially in medium and large areas. The objective is to plan, in a\ncoordinated manner, the optimal routes for a set of patrols in a given area, in\norder to achieve maximum coverage of the area, while also trying to minimize\nthe number of patrols. In this paper, we propose a multi-agent reinforcement\nlearning (MARL) model, based on a decentralized partially observable Markov\ndecision process, to plan unpredictable patrol routes within an urban\nenvironment represented as an undirected graph. The model attempts to maximize\na target function that characterizes the environment within a given time frame.\nOur model has been tested to optimize police patrol routes in three\nmedium-sized districts of the city of Malaga. The aim was to maximize\nsurveillance coverage of the most crime-prone areas, based on actual crime data\nin the city. To address this problem, several MARL algorithms have been\nstudied, and among these the Value Decomposition Proximal Policy Optimization\n(VDPPO) algorithm exhibited the best performance. We also introduce a novel\nmetric, the coverage index, for the evaluation of the coverage performance of\nthe routes generated by our model. This metric is inspired by the predictive\naccuracy index (PAI), which is commonly used in criminology to detect hotspots.\nUsing this metric, we have evaluated the model under various scenarios in which\nthe number of agents (or patrols), their starting positions, and the level of\ninformation they can observe in the environment have been modified. Results\nshow that the coordinated routes generated by our model achieve a coverage of\nmore than $90\\%$ of the $3\\%$ of graph nodes with the highest crime incidence,\nand $65\\%$ for $20\\%$ of these nodes; $3\\%$ and $20\\%$ represent the coverage\nstandards for police resource allocation.",
        "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
        "Recent advances in LLM-based role-playing language agents (RPLAs) have\nattracted broad attention in various applications. While chain-of-thought\nreasoning has shown importance in many tasks for LLMs, the internal thinking\nprocesses of RPLAs remain unexplored. Understanding characters' inner thoughts\nis crucial for developing advanced RPLAs. In this paper, we introduce\nROLETHINK, a novel benchmark constructed from literature for evaluating\ncharacter thought generation. We propose the task of inner thought reasoning,\nwhich includes two sets: the gold set that compares generated thoughts with\noriginal character monologues, and the silver set that uses expert synthesized\ncharacter analyses as references. To address this challenge, we propose MIRROR,\na chain-of-thought approach that generates character thoughts by retrieving\nmemories, predicting character reactions, and synthesizing motivations. Through\nextensive experiments, we demonstrate the importance of inner thought reasoning\nfor RPLAs, and MIRROR consistently outperforms existing methods. Resources are\navailable at https:\/\/github.com\/airaer1998\/RPA_Thought.",
        "We propose a novel reinforcement learning framework for post training large\nlanguage models that does not rely on human in the loop feedback. Instead, our\napproach uses cross attention signals within the model itself to derive a self\nsupervised reward, thereby guiding iterative fine tuning of the model policy.\nBy analyzing how the model attends to the input prompt during generation, we\nconstruct measures of prompt coverage, focus, and coherence. We then use these\nmeasures to rank or score candidate responses, providing a reward signal that\nencourages the model to produce well aligned, on topic text. In empirical\ncomparisons against standard policy gradient methods and RL fine tuning with\nsynthetic preference models, our method shows significant gains in prompt\nrelevance and consistency over a non RL baseline. While it does not yet match\nthe performance of fully human supervised RLHF systems, it highlights an\nimportant direction for scaling alignment with minimal human labeling. We\nprovide a detailed analysis, discuss potential limitations, and outline future\nwork for combining cross-attention based signals with smaller amounts of human\nfeedback.",
        "To enhance tourists' experiences and immersion, this paper proposes a\nnarrative-driven travel planning framework called NarrativeGuide, which\ngenerates a geoculturally-grounded narrative script for travelers, offering a\nnovel, role-playing experience for their journey. In the initial stage,\nNarrativeGuide constructs a knowledge graph for attractions within a city, then\nconfigures the worldview, character setting, and exposition based on the\nknowledge graph. Using this foundation, the knowledge graph is combined to\ngenerate an independent scene unit for each attraction. During the itinerary\nplanning stage, NarrativeGuide models narrative-driven travel planning as an\noptimization problem, utilizing a genetic algorithm (GA) to refine the\nitinerary. Before evaluating the candidate itinerary, transition scripts are\ngenerated for each pair of adjacent attractions, which, along with the scene\nunits, form a complete script. The weighted sum of script coherence, travel\ntime, and attraction scores is then used as the fitness value to update the\ncandidate solution set. Experimental results across four cities, i.e., Nanjing\nand Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate\nsignificant improvements in narrative coherence and cultural fit, alongside a\nnotable reduction in travel time and an increase in the quality of visited\nattractions. Our study highlights that incorporating external evolutionary\noptimization effectively addresses the limitations of large language models in\ntravel planning.",
        "Existing navigation decision support systems often perform poorly when\nhandling non-predefined navigation scenarios. Leveraging the generalization\ncapabilities of large language model (LLM) in handling unknown scenarios, this\nresearch proposes a dual-core framework for LLM applications to address this\nissue. Firstly, through ReAct-based prompt engineering, a larger LLM core\ndecomposes intricate navigation tasks into manageable sub-tasks, which\nautonomously invoke corresponding external tools to gather relevant\ninformation, using this feedback to mitigate the risk of LLM hallucinations.\nSubsequently, a fine-tuned and compact LLM core, acting like a first-mate is\ndesigned to process such information and unstructured external data, then to\ngenerates context-aware recommendations, ultimately delivering lookout insights\nand navigation hints that adhere to the International Regulations for\nPreventing Collisions at Sea (COLREGs) and other rules. Extensive experiments\ndemonstrate the proposed framework not only excels in traditional ship\ncollision avoidance tasks but also adapts effectively to unstructured,\nnon-predefined, and unpredictable scenarios. A comparative analysis with\nDeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and\nrationality of the proposed framework. This research bridges the gap between\nconventional navigation systems and LLMs, offering a framework to enhance\nsafety and operational efficiency across diverse navigation applications.",
        "The adoption of EHRs has expanded opportunities to leverage data-driven\nalgorithms in clinical care and research. A major bottleneck in effectively\nconducting multi-institutional EHR studies is the data heterogeneity across\nsystems with numerous codes that either do not exist or represent different\nclinical concepts across institutions. The need for data privacy further limits\nthe feasibility of including multi-institutional patient-level data required to\nstudy similarities and differences across patient subgroups. To address these\nchallenges, we developed the GAME algorithm. Tested and validated across 7\ninstitutions and 2 languages, GAME integrates data in several levels: (1) at\nthe institutional level with knowledge graphs to establish relationships\nbetween codes and existing knowledge sources, providing the medical context for\nstandard codes and their relationship to each other; (2) between institutions,\nleveraging language models to determine the relationships between\ninstitution-specific codes with established standard codes; and (3) quantifying\nthe strength of the relationships between codes using a graph attention\nnetwork. Jointly trained embeddings are created using transfer and federated\nlearning to preserve data privacy. In this study, we demonstrate the\napplicability of GAME in selecting relevant features as inputs for AI-driven\nalgorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.\nWe then highlight the application of GAME harmonized multi-institutional EHR\ndata in a study of Alzheimer's disease outcomes and suicide risk among patients\nwith mental health disorders, without sharing patient-level data outside\nindividual institutions.",
        "In many engineering applications, processes must be followed precisely,\nmaking conformance checking between event logs and declarative process models\ncrucial for ensuring adherence to desired behaviors. This is a critical area\nwhere Artificial Intelligence (AI) plays a pivotal role in driving effective\nprocess improvement. However, computing optimal alignments poses significant\ncomputational challenges due to the vast search space inherent in these models.\nConsequently, existing approaches often struggle with scalability and\nefficiency, limiting their applicability in real-world settings. This paper\nintroduces DeclareAligner, a novel algorithm that uses the A* search algorithm,\nan established AI pathfinding technique, to tackle the problem from a fresh\nperspective leveraging the flexibility of declarative models. Key features of\nDeclareAligner include only performing actions that actively contribute to\nfixing constraint violations, utilizing a tailored heuristic to navigate\ntowards optimal solutions, and employing early pruning to eliminate\nunproductive branches, while also streamlining the process through\npreprocessing and consolidating multiple fixes into unified actions. The\nproposed method is evaluated using 8,054 synthetic and real-life alignment\nproblems, demonstrating its ability to efficiently compute optimal alignments\nby significantly outperforming the current state of the art. By enabling\nprocess analysts to more effectively identify and understand conformance\nissues, DeclareAligner has the potential to drive meaningful process\nimprovement and management.",
        "Program synthesis has traditionally relied on human-provided specifications,\nexamples, or prior knowledge to generate functional algorithms. Existing\nmethods either emulate human-written algorithms or solve specific tasks without\ngenerating reusable programmatic logic, limiting their ability to create novel\nalgorithms. We introduce AlgoPilot, a groundbreaking approach for fully\nautomated program synthesis without human-written programs or trajectories.\nAlgoPilot leverages reinforcement learning (RL) guided by a Trajectory Language\nModel (TLM) to synthesize algorithms from scratch. The TLM, trained on\ntrajectories generated by random Python functions, serves as a soft constraint\nduring the RL process, aligning generated sequences with patterns likely to\nrepresent valid algorithms. Using sorting as a test case, AlgoPilot\ndemonstrates its ability to generate trajectories that are interpretable as\nclassical algorithms, such as Bubble Sort, while operating without prior\nalgorithmic knowledge. This work establishes a new paradigm for algorithm\ndiscovery and lays the groundwork for future advancements in autonomous program\nsynthesis.",
        "Speech enhancement (SE) aims to improve the clarity, intelligibility, and\nquality of speech signals for various speech enabled applications. However,\nair-conducted (AC) speech is highly susceptible to ambient noise, particularly\nin low signal-to-noise ratio (SNR) and non-stationary noise environments.\nIncorporating multi-modal information has shown promise in enhancing speech in\nsuch challenging scenarios. Electromyography (EMG) signals, which capture\nmuscle activity during speech production, offer noise-resistant properties\nbeneficial for SE in adverse conditions. Most previous EMG-based SE methods\nrequired 35 EMG channels, limiting their practicality. To address this, we\npropose a novel method that considers only 8-channel EMG signals with acoustic\nsignals using a modified SEMamba network with added cross-modality modules. Our\nexperiments demonstrate substantial improvements in speech quality and\nintelligibility over traditional approaches, especially in extremely low SNR\nsettings. Notably, compared to the SE (AC) approach, our method achieves a\nsignificant PESQ gain of 0.235 under matched low SNR conditions and 0.527 under\nmismatched conditions, highlighting its robustness.",
        "Quantile regression is a powerful tool capable of offering a richer view of\nthe data as compared to linear-squares regression. Quantile regression is\ntypically performed individually on a few quantiles or a grid of quantiles\nwithout considering the similarity of the underlying regression coefficients at\nnearby quantiles. When needed, an ad hoc post-processing procedure such as\nkernel smoothing is employed to smooth the estimated coefficients across\nquantiles and thereby improve the performance of these estimates. This paper\nintroduces a new method, called spline quantile regression (SQR), that unifies\nquantile regression with quantile smoothing and jointly estimates the\nregression coefficients across quantiles as smoothing splines. We discuss the\ncomputation of the SQR solution as a linear program (LP) using an\ninterior-point algorithm. We also experiment with some gradient algorithms that\nrequire less memory than the LP algorithm. The performance of the SQR method\nand these algorithms is evaluated using simulated and real-world data.",
        "Fuel moisture content (FMC) is a key predictor for wildfire rate of spread\n(ROS). Machine learning models of FMC are being used more in recent years,\naugmenting or replacing traditional physics-based approaches. Wildfire rate of\nspread (ROS) has a highly nonlinear relationship with FMC, where small\ndifferences in dry fuels lead to large differences in ROS. In this study,\ncustom loss functions that place more weight on dry fuels were examined with a\nvariety of machine learning models of FMC. The models were evaluated with a\nspatiotemporal cross-validation procedure to examine whether the custom loss\nfunctions led to more accurate forecasts of ROS. Results show that the custom\nloss functions improved accuracy for ROS forecasts by a small amount. Further\nresearch would be needed to establish whether the improvement in ROS forecasts\nleads to more accurate real-time wildfire simulations.",
        "Let $M^n$, $n\\in\\{3,4,5\\}$, be a closed aspherical $n$-manifold and $S\\subset\nM$ a subset consisting of disjoint incompressible embedded closed aspherical\nsubmanifolds (possibly with different dimensions). When $n =3,4$, we show that\n$M\\setminus S$ cannot admit any complete metric with positive scalar curvature.\nWhen $n=5$, we obtain the same result either when $S$ contains a submanifold of\ncodimension 1 or 2, or when $S$ itself is a connected submaifold of codimension\n$\\ge 3.$ The key ingredient is a new interior control for the extrinsic\ndiameter of surfaces with positive scalar curvature.",
        "Let $p$ be a prime. We prove bounds on short Dirichlet character sums\nevaluated at a class of homogeneous polynomials in arbitrary dimensions. In\nevery dimension, this bound is nontrivial for sums over boxes with side lengths\nas short as $p^{1\/4 + \\kappa}$ for any $\\kappa>0$. Our methods capitalize on\nthe relationship between characters mod $p$ and characters over finite field\nextensions as well as bounds on the multiplicative energy of sets in products\nof finite fields.",
        "Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.",
        "Large pre-trained models have achieved outstanding results in sequence\nmodeling. The Transformer block and its attention mechanism have been the main\ndrivers of the success of these models. Recently, alternative architectures,\nsuch as Selective Structured State Space Models (SSMs), have been proposed to\naddress the inefficiencies of Transformers. This paper explores the compression\nof SSM-based models, particularly Mamba and its hybrids. We study the\nsensitivity of these models to the removal of selected components at different\ngranularities to reduce the model size and computational overhead, thus\nimproving their efficiency while maintaining accuracy. The proposed solutions,\ncollectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x\nduring inference, demonstrating that model efficiency can be improved by\neliminating several redundancies with minimal impact on the overall model\nperformance. The code is available at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "Restarted GMRES is a robust and widely used iterative solver for linear\nsystems. The control of the restart parameter is a key task to accelerate\nconvergence and to prevent the well-known stagnation phenomenon. We focus on\nthe Proportional-Derivative GMRES (PD-GMRES), which has been derived using\ncontrol-theoretic ideas in [Cuevas N\\'u\\~nez, Schaerer, and Bhaya (2018)] as a\nversatile method for modifying the restart parameter. Several variants of a\nquadtree-based geometric optimization approach are proposed to find a best\nchoice of PD-GMRES parameters. We show that the optimized PD-GMRES performs\nwell across a large number of matrix types and we observe superior performance\nas compared to major other GMRES-based iterative solvers. Moreover, we propose\nan extension of the PD-GMRES algorithm to further improve performance by\ncontrolling the range of values for the restart parameter.",
        "The main result of this paper utilizes the representation graph of a group\n$G$, $R(V,G)$, and gives a general construction of a diagrammatic category\n$\\mathbf{Dgrams}_{R(V,G)}$. The proof of the main theorem shows that, given\nexplicit criteria, there is an equivalence of categories between a quotient\ncategory of $\\mathbf{Dgrams}_{R(V,G)}$ and a full subcategory of\n$G-\\textbf{mod}$ with objects being the tensor products of finitely many\nirreducible $G$-modules.",
        "Accurate gait event detection is crucial for gait analysis, rehabilitation,\nand assistive technology, particularly in exoskeleton control, where precise\nidentification of stance and swing phases is essential. This study evaluated\nthe performance of seven kinematics-based methods and a Long Short-Term Memory\n(LSTM) model for detecting heel strike and toe-off events across 4363 gait\ncycles from 588 able-bodied subjects. The results indicated that while the Zeni\net al. method achieved the highest accuracy among kinematics-based approaches,\nother methods exhibited systematic biases or required dataset-specific tuning.\nThe LSTM model performed comparably to Zeni et al., providing a data-driven\nalternative without systematic bias. These findings highlight the potential of\ndeep learning-based approaches for gait event detection while emphasizing the\nneed for further validation in clinical populations and across diverse gait\nconditions. Future research will explore the generalizability of these methods\nin pathological populations, such as individuals with post-stroke conditions\nand knee osteoarthritis, as well as their robustness across varied gait\nconditions and data collection settings to enhance their applicability in\nrehabilitation and exoskeleton control.",
        "We study the approximation properties of shallow neural networks whose\nactivation function is defined as the flow of a neural ordinary differential\nequation (neural ODE) at the final time of the integration interval. We prove\nthe universal approximation property (UAP) of such shallow neural networks in\nthe space of continuous functions. Furthermore, we investigate the\napproximation properties of shallow neural networks whose parameters are\nrequired to satisfy some constraints. In particular, we constrain the Lipschitz\nconstant of the flow of the neural ODE to increase the stability of the shallow\nneural network, and we restrict the norm of the weight matrices of the linear\nlayers to one to make sure that the restricted expansivity of the flow is not\ncompensated by the increased expansivity of the linear layers. For this\nsetting, we prove approximation bounds that tell us the accuracy to which we\ncan approximate a continuous function with a shallow neural network with such\nconstraints. We prove that the UAP holds if we consider only the constraint on\nthe Lipschitz constant of the flow or the unit norm constraint on the weight\nmatrices of the linear layers.",
        "Explanation is a fundamentally human process. Understanding the goal and\naudience of the explanation is vital, yet existing work on explainable\nreinforcement learning (XRL) routinely does not consult humans in their\nevaluations. Even when they do, they routinely resort to subjective metrics,\nsuch as confidence or understanding, that can only inform researchers of users'\nopinions, not their practical effectiveness for a given problem. This paper\ncalls on researchers to use objective human metrics for explanation evaluations\nbased on observable and actionable behaviour to build more reproducible,\ncomparable, and epistemically grounded research. To this end, we curate,\ndescribe, and compare several objective evaluation methodologies for applying\nexplanations to debugging agent behaviour and supporting human-agent teaming,\nillustrating our proposed methods using a novel grid-based environment. We\ndiscuss how subjective and objective metrics complement each other to provide\nholistic validation and how future work needs to utilise standardised\nbenchmarks for testing to enable greater comparisons between research.",
        "Road fatalities pose significant public safety and health challenges\nworldwide, with pedestrians being particularly vulnerable in vehicle-pedestrian\ncrashes due to disparities in physical and performance characteristics. This\nstudy employs explainable artificial intelligence (XAI) to identify key factors\ncontributing to pedestrian fatalities across the five U.S. states with the\nhighest crash rates (2018-2022). It compares them to the five states with the\nlowest fatality rates. Using data from the Fatality Analysis Reporting System\n(FARS), the study applies machine learning techniques-including Decision Trees,\nGradient Boosting Trees, Random Forests, and XGBoost-to predict contributing\nfactors to pedestrian fatalities. To address data imbalance, the Synthetic\nMinority Over-sampling Technique (SMOTE) is utilized, while SHapley Additive\nExplanations (SHAP) values enhance model interpretability. The results indicate\nthat age, alcohol and drug use, location, and environmental conditions are\nsignificant predictors of pedestrian fatalities. The XGBoost model outperformed\nothers, achieving a balanced accuracy of 98 %, accuracy of 90 %, precision of\n92 %, recall of 90 %, and an F1 score of 91 %. Findings reveal that pedestrian\nfatalities are more common in mid-block locations and areas with poor\nvisibility, with older adults and substance-impaired individuals at higher\nrisk. These insights can inform policymakers and urban planners in implementing\ntargeted safety measures, such as improved lighting, enhanced pedestrian\ninfrastructure, and stricter traffic law enforcement, to reduce fatalities and\nimprove public safety.",
        "In this note, we define an analogue of R-matrices for bialgebras in the\nsetting of a monad that is opmonoidal over two tensor products. Analogous to\nthe classical case, such structures bijectively correspond to duoidal\nstructures on the Eilenberg--Moore category of the monad. Further, we\ninvestigate how a cocommutative version of this lifts the linearly distributive\nstructure of a normal duoidal category."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Speaker Diarization with LSTM",
    "start_abstract":"For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Human-Centric Interfaces for Ambient Intelligence"
      ],
      "abstract":[
        "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
      ],
      "categories":[
        "physics.app-ph"
      ]
    },
    "list":{
      "title":[
        "Ultrawhite structural starch film for sustainable cooling",
        "Multi-time scale and high performance in-material reservoir computing\n  using graphene-based ion-gating reservoir",
        "Detection and Ranging Beyond the Canonical Resolution Limit",
        "Transfer ABCD Matrix for Time-Varying Media and Time Crystals",
        "Novel crossbar array of silicon nitride resistive memories on SOI\n  enables memristor rationed logic",
        "Practical implementation of a chiral phononic crystal demonstrator with\n  ultra-low frequency bandgap",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "Stochastic reconstruction of multiphase composite microstructures using\n  statistics-encoded neural network for poro\/micro-mechanical modelling",
        "Analytical modeling of laminated composite rings on nonreciprocal\n  elastic foundations under non-axisymmetric loading",
        "Tracking the creation of single photon emitters in AlN by implantation\n  and annealing",
        "Node-to-node contact-friction problems using run-time parameter updates\n  on a conventional force-deformation finite element",
        "Ultra-flexible silicon foils with seamless detachability: the effect of\n  porous multilayered structures prepared through modulated electrolyte\n  composition",
        "Experimental and Theoretical Study of Thin-covered Composite Dowels\n  considering Multiple Load Conditions",
        "Einstein-Maxwell-Dilaton Wormholes that meet the Energy Conditions",
        "On the comparison principle for a nonlocal infinity Laplacian",
        "Freeze-and-release direct optimization method for variational\n  calculations of excited electronic states",
        "$e$-product of distributions, with applications",
        "A nonlinear model of shearable elastic rod from an origami-like\n  microstructure displaying folding and faulting",
        "Galaxy infall models for arbitrary velocity directions",
        "Analysis of $q_\\mathrm{rec}^2$-distribution for $B\\to K M_X$ and $B\\to\n  K^* M_X$ decays in a scalar-mediator dark-matter scenario",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Geometric Gauss Sums and Gross-Koblitz Formulas over Function Fields",
        "Solar irradiance statistical analysis in Mexico City from 2018 to 2021",
        "On the axially symmetric solutions to the spatially homogeneous Landau\n  equation",
        "Electroweak baryogenesis from charged current anomalies in $B$ meson\n  decays",
        "Ferri- and Ferro-Electric Switching in Spontaneously Chiral Polar Liquid\n  Crystals",
        "A general quasilinear elliptic problem with variable exponents and\n  Neumann boundary conditions for image processing",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case"
      ],
      "abstract":[
        "Reducing human reliance on high-electricity-consuming cooling technologies\nlike air conditioning is crucial for reshaping the global energy paradigm.\nThrough utilizing natural starch gelatinization, freezedrying and densification\nprocesses, we fabricated an ultrawhite cooling starch film with an ultrahigh\nsolar reflectance of 0.96 and strong infrared emittance of 0.94. The porous\nstructure of the cooling starch film, systematically controlled by the\nmechanical pressing processing, allows for effective scattering of solar\nradiation while emitting strongly during the atmospheric transparency window,\nthereby contributing to high-efficiency daytime radiative cooling capacity.\nFurthermore, the cooling starch film exhibits excellent mechanical tensile\nstrength, measuring at up to 38.5 megapascals, which is more than twice the\nstrength of natural wood. The ultrawhite radiative cooling starch film holds\nsignificant promise for optimizing cooling energy usage, especially in hot and\narid climates.",
        "The rising energy demands of conventional AI systems underscore the need for\nefficient computing technologies like brain-inspired computing. Physical\nreservoir computing (PRC), leveraging the nonlinear dynamics of physical\nsystems for information processing, has emerged as a promising approach for\nneuromorphic computing. However, current PRC systems are constrained by narrow\noperating timescales and limited performance. To address these challenges, an\nion-gel\/graphene electric double layer transistor-based ion-gating reservoir\n(IGR) was developed, offering adaptability across multi-time scales with an\nexceptionally wide operating range from 1 MHz to 20 Hz and high information\nprocessing capacity. The IGR achieved deep learning (DL)-level accuracy in\nchaotic time series prediction tasks while reducing computational resource\nrequirements to 1\/100 of those needed by DL. Principal component analysis\nreveals the IGR's superior performance stems from its high-dimensionality,\ndriven by the ambipolar behavior of graphene and multiple relaxation processes.\nThe proposed IGR represents a significant step forward in providing low-power,\nhigh-performance computing solutions, particularly for resource-constrained\nedge environments.",
        "The canonical range resolution limit in radar, sonar, and lidar systems is\nfound to be a special case of a more general resolution limit. The general\nlimit indicates that it is possible to surpass the canonical limit in moderate\n(of order unity) signal-to-noise ratio (SNR) environments by using the signal\namplitude and phase information. The canonical limit only considers the\nbandwidth of the received signal without considering how SNR affects the range\nresolution. Details present in the signal amplitude, such as attenuation and\ngeometric spreading, can act as additional sources of range information.\nPrevious studies have taken advantage of the relationship between target\ndistance and signal amplitude or phase to achieve higher resolution ranging,\nand often employ unusual transmit waveforms for this purpose. These methods\neach provide distinct bounds on range resolution, rather than a unified bound\napplicable across different systems and applications. We apply ideas from\ninformation theory to determine a general lower bound to the smallest\nresolvable range bin size and corresponding target strength measurements.",
        "This paper introduces a formal definition of the transfer ABCD parameters in\ntime-varying electromagnetic systems. The formal definition comes after the\nrearrangement of the fields $D$ and $B$ at the inputs and outputs of the\ntemporal system based on the time-varying boundary conditions. Then, we derive\nthe ABCD parameters of a temporal transmission line, i.e., a temporal slab, and\ncompute the associated scattering parameters (reflection and transmission\ncoefficients). The results presented here open up an alternative way, based on\nnetwork theory, to analyze multilayer temporal configurations. Moreover, we\nshow that the ABCD parameters can be used to compute the dispersion diagram\n($\\omega$ vs $k$) of time crystals.",
        "In this work, the fabrication of crossbar arrays of silicon nitride resistive\nmemories on silicon-on-insulator substrate and their utilization to realize\nmulti-rationed logic circuits are presented. Typical electrical\ncharacterization of the memristors revealed their ability of multi-state\noperation by the presence of 12 well separated resistance levels. Through a\ndedicated modeling and fitting procedure of these levels, a reconfigurable\nlogic based on memristor rationed logic scheme is designed and a crossbar\nintegration methodology was proposed. Finally, several circuitry aspects were\nsimulated in SPICE with a silicon nitride SOI crossbar array calibrated model\nand power optimization prospects were discussed.",
        "The use of phononic crystals for vibration attenuation and isolation has been\nwidely studied, showing that the attenuation frequency range depends on their\nmass and stiffness. The concepts of chirality and tacticity have been\nintroduced into classical phononic crystals to enrich the dynamics of the mass\nelements and thereby achieve lower frequency ranges with high vibration\nattenuation. Although these concepts have demonstrated their effectiveness on\nlab-scale crystals, their implementation in industrial applications is still\nrare. Chiral phononic crystals require a complex geometry that complicates\ntheir manufacturing. Existing examples require to be fabricated by 3D printing,\nmaking them expensive to build on a large scale for demonstration purposes or\nin-situ applications. In this study, we redefine a chiral phononic crystal\ndesign for translational-rotational coupling in order to enable its\nmanufacturability using exclusively conventional processes. We then investigate\nthe design space of these newly designed phononic crystals, using a simplified\nunit cell FEM model that minimizes computation time. A parametric study is\nconducted to investigate the crystal's tunability by modifying the dimensions\nof the chiral links between the masses. A large crystal with ultra-low\nfrequency range attenuation -- starting at 60~Hz -- is then designed, with the\naim to demonstrate the influence of the crystal's tacticity on the vibration\nisolation by hand sensing. A crystal composed of 2 unit cells is manufactured\nand its measured transfer function is compared with numerical predictions, thus\nhighlighting the disparities between the behavior of the structure under\nreal-life and ideal excitation conditions.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "Understanding microstructure-property relationships (MPRs) is essential for\noptimising the performance of multiphase composites. Image-based\nporo\/micro-mechanical modelling provides a non-invasive approach to exploring\nMPRs, but the randomness of multiphase composites often necessitates extensive\n3D microstructure datasets for statistical reliability. This study introduces a\ncost-effective machine learning framework to reconstruct numerous virtual 3D\nmicrostructures from limited 2D exemplars, circumventing the high costs of\nvolumetric microscopy. Using feedforward neural networks, termed the\nstatistics-encoded neural network (SENN), the framework encodes 2D\nmorphological statistics and infers 3D morphological statistics via a 2D-to-3D\nintegration scheme. Statistically equivalent 3D microstructures are synthesised\nusing Gibbs sampling. Hierarchical characterisation enables seamless capture of\nfeatures across multiple scales. Validation on three composites demonstrates\nstrong statistical equivalence between reconstructed and reference\nmicrostructures, confirmed by morphological descriptors and simulated\nmacroscopic properties (e.g., stiffness, permeability). The SENN-based\nframework is a high-fidelity tool for efficiently and accurately reconstructing\nmultiphase microstructures.",
        "A mechanical model of a laminated composite ring on a nonreciprocal elastic\nfoundation is a valuable engineering tool during the early design stages of\nvarious applications, such as non-pneumatic wheels, flexible bearings,\nexpandable tubulars in oil wells, and vascular stents interacting with blood\nvessel linings, especially under non-axisymmetric loadings. Despite its\nimportance, limited research has focused on the interaction between laminated\ncomposite rings and nonreciprocal elastic foundations. Moreover, no\nquantitative studies have yet explored the influence of foundation stiffness on\nthe ring deformation. This work aims to develop an analytical framework for a\nlaminated composite ring supported by a nonreciprocal elastic foundation under\nnon-axisymmetric loading conditions. The model generates a design map that\ncorrelates the foundation stiffness with the ring deformation, accounting for\nring dimensions, laminate layup architecture, and lamina anisotropy. The\nclosed-form solution provides an efficient design tool for analyzing\nnon-axisymmetric and nonuniform loadings at a low computational cost. The\nresulting design map provides a valuable resource for exploring the interaction\nbetween the nonreciprocal foundation and the laminated ring. The proposed\nanalytical framework and design map hold broad potential applications in\nautomotive, mechanical, civil, and biomedical engineering fields.",
        "In this study, we inspect and analyze the effect of Al implantation into AlN\nby conducting confocal microscopy on the ion implanted regions, before and\nafter implantation, followed by an annealing step. The independent effect of\nannealing is studied in an unimplanted control region, which showed that\nannealing alone does not produce new emitters. We observed that point-like\nemitters are created in the implanted regions after annealing by tracking\nindividual locations in a lithographically patterned sample. The newly created\nquantum emitters show anti-bunching under ambient conditions and are spectrally\nsimilar to the previously discovered emitters in as-grown AlN.",
        "A novel implementation of the traditional node-to-node Coulomb\ncontact-friction problem is presented that utilizes run-time parameter updates\non conventional elasto-plastic elements. The two-noded elements are defined by\nan independent uniaxial force-deformation (or constitutive) relation in each\ndegree of freedom. The location of the two nodes may or may not be coincident.\nA parameter is a pointer to a value (nodal geometry, element property, material\nproperty, etc.) in the finite element domain that can be controlled by the\nuser. Parameters that control the frictional interface normal and tangential\nresponses are updated based on contact detection, and eliminate the need for\nadding new source code to the finite element library of a given software. The\nrun-time algorithm for updating both an elastic and elasto-plastic\nforce-deformation element to achieve a penalty-based contact-friction model is\npresented. Static and dynamic cases were investigated for a two-noded unit\nprocess and compared with the results obtained from closed-form solutions.\nMultiple interface elements were then used for the sliding, tipping, and\nrocking responses of a rigid block on rigid foundation. Finally, several case\nstudies were investigated, and the findings were compared with the experimental\nresults or solutions from the literature. The proposed friction-contact\nimplementation can be deployed in larger finite element models, and parameter\nupdates facilitate implementation of a wider array of friction models by\nchanging only the constitutive model.",
        "A comprehensive evaluation of the effect and limitations of variable current\ndensity and electrolyte composition on layer porosity and microstructure\nchanges of porous silicon (pSi) multilayer stacks is reported. Following these\nresults, the development and optimization of a four-layer stack architecture is\nreported through addition of super-low porosity layers (SLPL) on a low\/high\nporosity layer (LPL\/HLP) stack. Thermal treatment of these structures achieved\nexcellent top and bottom surface reconstruction to form sintered porous silicon\n(SPS) detachable foils, enabling direct foil separation using cello tape from a\nsmooth and specular parent substrate without the need to cut or damage it",
        "With the widespread application of composite structures in the fields of\nbuilding and bridge constructions, thin-covered composite dowels are\nincreasingly adopted in various engineering scenarios. This paper presents a\ndesign methodology for thin-covered composite dowels, supported by both\nexperimental and theoretical investigations. In the experiment, a novel test\nrig and specimens are designed to facilitate tensile-shear coupling loading.\nThe study identifies a new failure mode: Restricted Cone Failure (RCF) in\nthin-covered composite dowels under tensile-shear coupling load, which distinct\nfrom conventional composite dowels. This RCF mode is attributed to the thin\nthickness of the side concrete cover, which restricts the development of the\nfailure cone in the thickness direction. Additionally, a parametric analysis is\nconducted to evaluate the effects of key factors--such as steel dowel\nthickness, effective embedment depth, and the tensile strength of steel fiber\nreinforced concrete--on the bearing capacity and ductility of thin-covered\ncomposite dowels. Based on the theoretical findings, comprehensive tensile,\nshear, and tensile-shear coupling capacity models along with an engineering\ndesign model are developed to aid in the practical application of thin-covered\ncomposite dowels.",
        "One of the latest predictions of Einstein's theory is the existence of\nWormholes (WH). In this work, we present exact solutions of the\nEinstein-Maxwell-Dilaton equations representing traversable Wormholes. These\nsolutions satisfy the energy conditions and have a ring singularity satisfying\nthe cosmic censorship of WHs, i.e. we show that, as in previous solutions,\ngeodesics cannot touch the singularity. We find that the most optimal input\nregions for the first class of solutions traversing these wormholes are near\nthe poles and near the equatorial plane for the second class. We also find that\nthe solution associated with the first class is physically feasible, while for\nthe second class it presents the problem of not being asymptotically flat when\nconsidering a dilatonic-type scalar field. Finally, we give examples of\nrealistic astrophysical objects that could fulfill these conditions.",
        "In this article, we prove the uniqueness of viscosity solutions to\n$\\mathcal{L}_{\\infty} u =f$ in $\\Omega$, where $\\mathcal{L}_{\\infty}$ denotes\nthe nonlocal infinity Laplace operator, $\\Omega$ a bounded domain, and $f$ a\ncontinuous functions such that $f \\leq 0$. Uniqueness is established through a\ncomparison principle.",
        "Time-independent, orbital-optimized density functional approaches outperform\ntime-dependent density functional theory (TDDFT) in calculations of excited\nelectronic states involving a large rearrangement of the electron density, such\nas charge transfer excitations. However, optimizing orbitals for excited states\nremains challenging, as the latter typically correspond to saddle points on the\nelectronic energy surface. A simple and robust strategy for variational orbital\noptimization of excited states is presented. The approach involves two steps:\n(1) a constrained energy minimization, where a subset of orbitals changed by\nthe excitation are frozen, followed by (2) a fully unconstrained saddle point\noptimization. The constrained minimization step makes it possible to identify\nthe electronic degrees of freedom along which the energy needs to be maximized,\npreventing variational collapse. Both steps of this freeze-and-release strategy\nare carried out using direct optimization algorithms with a computational\nscaling comparable to ground state calculations. Numerical tests using a\nsemilocal functional are performed on intramolecular charge transfer states of\norganic molecules and intermolecular charge transfer states of molecular\ndimers. It is shown that the freeze-and-release direct optimization (FR-DO)\napproach can successfully converge challenging charge transfer states,\novercoming limitations of conventional algorithms based on the maximum overlap\nmethod, which either collapse to lower energy, charge-delocalized solutions or\nfail to converge. While FR-DO requires more iterations on average, the overall\nincrease in computational cost is small. For the NH3-F2 dimer, it is found that\nunlike TDDFT, orbital-optimized calculations reproduce the correct long-range\ndependency of the energy with respect to the donor-acceptor separation without\nthe need to include exact exchange in the long range.",
        "We consider and reformulate a recent definition of multiplication between\ndistributions. We show that this definition can be adopted, in particular, to\nprove biorthonormality of some distributions arising when looking to the\n(generalized) eigenvalues of a specific non self-adjoint number-like operator,\nconsidered in connection with the recently introduced {\\em weak pseudo-bosons}.\nSeveral examples are discussed in details.",
        "A new continuous model of shearable rod, subject to large elastic\ndeformation, is derived from nonlinear homogenization of a one-dimensional\nperiodic microstructured chain. As particular cases, the governing equations\nreduce to the Euler elastica and to the shearable elastica known as 'Engesser',\nthat has been scarcely analysed so far. The microstructure that is homogenized\nis made up of elastic hinges and four-bar linkages, which may be realized in\npractice using origami joints. The equivalent continuous rod is governed by a\nDifferential-Algebraic system of nonlinear Equations (DAE), containing an\ninternal length ratio, and showing a surprisingly rich mechanical landscape,\nwhich involves a twin sequence of bifurcation loads, separated by a\n'transition' mode. The latter occurs, for simply supported and cantilever rods\nin a 'bookshelf-like' mode and in a mode involving faulting (formation of a\nstep in displacement), respectively. The postcritical response of the simply\nsupported rod exhibits the emergence of folding, an infinite curvature\noccurring at a point of the rod axis, developing into a curvature jump at\nincreasing load. Faulting and folding, excluded for both Euler and Reissner\nmodels and so far unknown in the rod theory, represent 'signatures' revealing\nthe origami design of the microstructure. These two features are shown to be\nassociated with bifurcations and, in particular folding, with a secondary\nbifurcation of the corresponding discrete chain when the number of elements is\nodd. Beside the intrinsic theoretical relevance to the field of structural\nmechanics, our results can be applied to various technological contexts\ninvolving highly compliant mechanisms, such as the achievement of objective\ntrajectories with soft robot arms through folding and localized displacement of\norigami-inspired or multi-material mechanisms.",
        "For most galaxies in the cosmos, our knowledge of their motion is limited to\nline-of-sight velocities from redshift observations. Peculiar motions on the\nsky are only measured for a few cases. With increasingly detailed observations,\nthe assumption that line-of-sight velocities suffice for an accurate and\nprecise reconstruction of galaxy kinematics needs to be re-investigated and the\nimpact of perpendicular velocities to be quantified. We analyse the motion of\ntwo galaxies with arbitrary velocities, determine their mutual velocity on an\narbitrary background, and compare this general relative velocity to the one\nfrom line-of-sight components only. The latter are known as ``minor and major\ninfall models'' established by Karachentsev and Kashibadze (2006). Our\nderivations reveal that the infall models approximate the radial velocity\nbetween two galaxies by two different projections employing different\ninformation about the system. For galaxies with small angular separations, all\ninfall models agree that the radial velocity is the difference of their\nline-of-sight velocities. For larger angles, the minor infall model is mostly\nsuitable when perpendicular velocity components are negligible and there is no\ninformation about the tangential velocity of the binary. The major infall model\nis best suitable when the motion is mainly radial and symmetry assumptions\ncancel the tangential and one perpendicular component. The latter often\nrequires to transition from galaxy binaries to groups or clusters, as we show\nquantitatively. We give an encompassing overview how the infall models over-\nand under-estimate general binary or $N$-body motions. We quantify the impact\nof perpendicular velocity components, sparse sampling, and deviations of the\ntracer-galaxies from the motion in an embedding gravitational potential which\nare related to the angular momentum of the structure. (abridged)",
        "We demonstrate that the scalar-mediator dark-matter scenario is consistent\nwith the experimental data on the decay $B\\to K M_X$ and provides a good\ndescription of the shape of the observed excess. Within this scenario, the\ninteraction with dark-matter particles leads to approximately the same excess\nin $\\Gamma(B\\to K^* M_X)$ and $\\Gamma(B\\to K M_X)$ compared to the Standard\nModel; also the differential distributions of the excess events are similar in\nshape in the variable $q_\\mathrm{rec}^2$ measured by experiment.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "In this paper, we prove the Gross-Koblitz-Thakur formulas relating special\n$v$-adic gamma values to the newly introduced geometric Gauss sums in the\nfunction field setting. These are analogous to those for the $p$-adic gamma\nfunction in the classical setting due to Gross-Koblitz and the $v$-adic\narithmetic gamma function over function fields due to Thakur. For these new\nGauss sums, we establish their key arithmetic properties, including the\nuniformity of absolute values and prime factorizations. We also determine their\nsigns at infinite places, and derive two analogs of the Hasse-Davenport\nrelations.",
        "Solar radiation is made up of three components of electromagnetic waves:\ninfrared, visible and ultraviolet. The infrared component is the cause of\nthermal energy, the visible spectrum allows to see through the eyes and the\nultraviolet component is the most energetic and damaging. Solar radiation has\nseveral benefits, such as helping to synthesize vitamin D in the skin, favors\nblood circulation, among others benefits for the human body. In the Earth, it\nis the main source of energy for agriculture, also used as an alternative\nsource of energy to hydrocarbons, through solar cells. The solar irradiance\nrepresents the surface power density with units W\/m$^2$ in SI. Too much\nexposure can cause damage and an increase in value over the time can be can be\nalso damaging. In this work it was used an open data base provided by\nSecretar\\'ia del Medio Ambiente, from which a statistical analysis was\nperformed of the solar irradiance values measured at various meteorological\nstations in Mexico City and the so-called metropolitan area, from 2018 to 2021.\nThis analysis was carried out per years, months and days. From the solar\nirradiance values distributions, it was obtained the averages, maximums and\nmeans were it was found there was no variation in the solar irradiance values\nover this period of years.",
        "In this paper, we consider the spatially homogeneous Landau equation, which\nis a variation of the Boltzmann equation in the grazing collision limit. For\nthe Landau equation for hard potentials in the style of Desvillettes-Villani\n(Comm. Partial Differential Equations, 2000), we provide the proof of the\nexistence of axisymmetric measure-valued solution for any axisymmetric\n$\\mathcal{P}_p(\\mathbb{R}^3)$ initial profile for any $p\\ge 2$. Moreover, we\nprove that if the initial data is not a single Dirac mass, then the solution\ninstantaneously becomes analytic for any time $t>0$ in the hard potential case.\nIn the soft potential and the Maxwellian molecule cases, we show that there are\nno solutions whose support is contained in a fixed line even for any given\nline-concentrated data.",
        "We demonstrate for the first time that new physics explaining the long\nstanding charged $B$ meson anomalies, $R(D^{(*)})$, can be the source of CP\nviolation that explains the observed baryon asymmetry of the universe (BAU). We\nconsider the general two Higgs doublet model with complex Yukawa couplings and\ncompute the BAU in the semiclassical formalism, using a novel analytic\napproximation for the latter. After imposing constraints from both flavor\nobservables and the electron electric dipole moment (eEDM), we find that a\nsignificant BAU can still be generated for a variety of benchmark points in the\nparameter space, assuming the occurrence of a sufficiently strong first order\nelectroweak phase transition. These scenarios, which explain both the\n$R(D^{(*)})$ flavor anomalies and the BAU, can be probed with future eEDM\nexperiments and Higgs factories measurements.",
        "The recent discovery of spontaneous chiral symmetry breaking has demonstrated\nthe possibility of discovering the exotic textures of ferromagnetic systems in\nliquid crystalline fluid ferro-electrics. We show that the polar smectic\nmesophase exhibited by the first molecule discovered to exhibit a spontaneously\nchiral ferroelectric nematic phase is also helical has a strongly varied\ntextural morphology depending in its thermal history and phase ordering.\nElectro-optic studies demonstrate that the two spontaneously chiral phases\nexhibit field induced phase transitions. For the nematic variant, this process\nis threshold-less and has no hysteresis while for the smectic it has a clear\nthreshold and shows hysteresis meaning this phase exhibits pseudo-ferrielectric\nswitching, the first of its kind for ferroelectric nematic like phases. We show\nthat helix formation can be both 1st and 2nd order but when it is 1st it is\naccompanied by pre-transitional helix formation in the preceding ferroelectric\nnematic phase.",
        "The aim of this paper is to state and prove existence and uniqueness results\nfor a general elliptic problem with homogeneous Neumann boundary conditions,\noften associated with image processing tasks like denoising. The novelty is\nthat we surpass the lack of coercivity of the Euler-Lagrange functional with an\ninnovative technique that has at its core the idea of showing that the minimum\nof the energy functional over a subset of the space $W^{1,p(x)}(\\Omega)$\ncoincides with the global minimum. The obtained existence result applies to\nmultiple-phase elliptic problems under remarkably weak assumptions.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift."
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Turning single-molecule localization microscopy into a quantitative bioanalytical tool",
    "start_abstract":"Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "A framework for evaluating the performance of SMLM cluster analysis algorithms"
      ],
      "abstract":[
        "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Accelerated Cardiac Parametric Mapping using Deep Learning-Refined\n  Subspace Models",
        "Technical report of a DMD-based Characterization Method for Vision\n  Sensors",
        "Transferring between sparse and dense matching via probabilistic\n  reweighting",
        "Color Correction Meets Cross-Spectral Refinement: A Distribution-Aware\n  Diffusion for Underwater Image Restoration",
        "Balanced Opto-electronic Joint Transform Correlator for Enhanced\n  Real-Time Pattern Recognition",
        "Observation-only learning of neural mapping schemes for gappy\n  satellite-derived ocean colour parameters",
        "Gain-MLP: Improving HDR Gain Map Encoding via a Lightweight MLP",
        "ILACS-LGOT: A Multi-Layer Contrast Enhancement Approach for Palm-Vein\n  Images",
        "Deep Task-Based Beamforming and Channel Data Augmentations for Enhanced\n  Ultrasound Imaging",
        "A Synergy Scoring Filter for Unsupervised Anomaly Detection with Noisy\n  Data",
        "Skeletonisation Scale-Spaces",
        "Overview of Variable Rate Coding in JPEG AI",
        "DeepNuParc: A Novel Deep Clustering Framework for Fine-scale\n  Parcellation of Brain Nuclei Using Diffusion MRI Tractography",
        "MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length\n  Video Generation",
        "Out-of-distribution generalisation for learning quantum channels with\n  low-energy coherent states",
        "Formalising the intentional stance 2: a coinductive approach",
        "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep\n  Reinforcement Learning",
        "MastermindEval: A Simple But Scalable Reasoning Benchmark",
        "Critical Unstable Qubits: an Application to $B^0\\bar{B}^0$-Meson System",
        "MobileSteward: Integrating Multiple App-Oriented Agents with\n  Self-Evolution to Automate Cross-App Instructions",
        "Simple Hamiltonians for Matrix Product State models",
        "A Reinforcement Learning Approach to Quiet and Safe UAM Traffic\n  Management",
        "Learning Visual Proxy for Compositional Zero-Shot Learning",
        "k-LLMmeans: Summaries as Centroids for Interpretable and Scalable\n  LLM-Based Text Clustering",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "An overview of regularity results for the Laplacian and $p$-Laplacian in\n  metric spaces",
        "Temporal Feature Weaving for Neonatal Echocardiographic Viewpoint Video\n  Classification"
      ],
      "abstract":[
        "Cardiac parametric mapping is useful for evaluating cardiac fibrosis and\nedema. Parametric mapping relies on single-shot heartbeat-by-heartbeat imaging,\nwhich is susceptible to intra-shot motion during the imaging window. However,\nreducing the imaging window requires undersampled reconstruction techniques to\npreserve image fidelity and spatial resolution. The proposed approach is based\non a low-rank tensor model of the multi-dimensional data, which jointly\nestimates spatial basis images and temporal basis time-courses from an\nauxiliary parallel imaging reconstruction. The tensor-estimated spatial basis\nis then further refined using a deep neural network, trained in a fully\nsupervised fashion, improving the fidelity of the spatial basis using learned\nrepresentations of cardiac basis functions. This two-stage spatial basis\nestimation will be compared against Fourier-based reconstructions and parallel\nimaging alone to demonstrate the sharpening and denoising properties of the\ndeep learning-based subspace analysis.",
        "This technical report presents a novel DMD-based characterization method for\nvision sensors, particularly neuromorphic sensors such as event-based vision\nsensors (EVS) and Tianmouc, a complementary vision sensor. Traditional image\nsensor characterization standards, such as EMVA1288, are unsuitable for BVS due\nto their dynamic response characteristics. To address this, we propose a\nhigh-speed, high-precision testing system using a Digital Micromirror Device\n(DMD) to modulate spatial and temporal light intensity. This approach enables\nquantitative analysis of key parameters such as event latency, signal-to-noise\nratio (SNR), and dynamic range (DR) under controlled conditions. Our method\nprovides a standardized and reproducible testing framework, overcoming the\nlimitations of existing evaluation techniques for neuromorphic sensors.\nFurthermore, we discuss the potential of this method for large-scale BVS\ndataset generation and conversion, paving the way for more consistent\nbenchmarking of bio-inspired vision technologies.",
        "Detector-based and detector-free matchers are only applicable within their\nrespective sparsity ranges. To improve adaptability of existing matchers, this\npaper introduces a novel probabilistic reweighting method. Our method is\napplicable to Transformer-based matching networks and adapts them to different\nsparsity levels without altering network parameters. The reweighting approach\nadjusts attention weights and matching scores using detection probabilities of\nfeatures. And we prove that the reweighted matching network is the asymptotic\nlimit of detector-based matching network. Furthermore, we propose a sparse\ntraining and pruning pipeline for detector-free networks based on reweighting.\nReweighted versions of SuperGlue, LightGlue, and LoFTR are implemented and\nevaluated across different levels of sparsity. Experiments show that the\nreweighting method improves pose accuracy of detector-based matchers on dense\nfeatures. And the performance of reweighted sparse LoFTR is comparable to\ndetector-based matchers, demonstrating good flexibility in balancing accuracy\nand computational complexity.",
        "Underwater imaging often suffers from significant visual degradation, which\nlimits its suitability for subsequent applications. While recent underwater\nimage enhancement (UIE) methods rely on the current advances in deep neural\nnetwork architecture designs, there is still considerable room for improvement\nin terms of cross-scene robustness and computational efficiency. Diffusion\nmodels have shown great success in image generation, prompting us to consider\ntheir application to UIE tasks. However, directly applying them to UIE tasks\nwill pose two challenges, \\textit{i.e.}, high computational budget and color\nunbalanced perturbations. To tackle these issues, we propose DiffColor, a\ndistribution-aware diffusion and cross-spectral refinement model for efficient\nUIE. Instead of diffusing in the raw pixel space, we transfer the image into\nthe wavelet domain to obtain such low-frequency and high-frequency spectra, it\ninherently reduces the image spatial dimensions by half after each\ntransformation. Unlike single-noise image restoration tasks, underwater imaging\nexhibits unbalanced channel distributions due to the selective absorption of\nlight by water. To address this, we design the Global Color Correction (GCC)\nmodule to handle the diverse color shifts, thereby avoiding potential global\ndegradation disturbances during the denoising process. For the sacrificed image\ndetails caused by underwater scattering, we further present the Cross-Spectral\nDetail Refinement (CSDR) to enhance the high-frequency details, which are\nintegrated with the low-frequency signal as input conditions for guiding the\ndiffusion. This way not only ensures the high-fidelity of sampled content but\nalso compensates for the sacrificed details. Comprehensive experiments\ndemonstrate the superior performance of DiffColor over state-of-the-art methods\nin both quantitative and qualitative evaluations.",
        "Opto-electronic joint transform correlators (JTCs) use a focal plane array\n(FPA) to detect the joint power spectrum (JPS) of two input images, projecting\nit onto a spatial light modulator (SLM) to be optically Fourier transformed.\nThe JPS is composed of two self-intensities and two conjugate-products, where\nonly the latter produce the cross-correlation. However, the self-intensity\nterms are typically much stronger than the conjugate-products, consuming most\nof the available bit-depth on the FPA and SLM. Here we propose and demonstrate,\nthrough simulation and experiment, a balanced opto-electronic JTC that\nelectronically processes the JPS to remove the self-intensity terms, thereby\nenhancing the quality of the cross-correlation result.",
        "Monitoring optical properties of coastal and open ocean waters is crucial to\nassessing the health of marine ecosystems. Deep learning offers a promising\napproach to address these ecosystem dynamics, especially in scenarios where\ngap-free ground-truth data is lacking, which poses a challenge for designing\neffective training frameworks. Using an advanced neural variational data\nassimilation scheme (called 4DVarNet), we introduce a comprehensive training\nframework designed to effectively train directly on gappy data sets. Using the\nMediterranean Sea as a case study, our experiments not only highlight the high\nperformance of the chosen neural network in reconstructing gap-free images from\ngappy datasets but also demonstrate its superior performance over\nstate-of-the-art algorithms such as DInEOF and Direct Inversion, whether using\nCNN or UNet architectures.",
        "While most images shared on the web and social media platforms are encoded in\nstandard dynamic range (SDR), many displays now can accommodate high dynamic\nrange (HDR) content. Additionally, modern cameras can capture images in an HDR\nformat but convert them to SDR to ensure maximum compatibility with existing\nworkflows and legacy displays. To support both SDR and HDR, new encoding\nformats are emerging that store additional metadata in SDR images in the form\nof a gain map. When applied to the SDR image, the gain map recovers the HDR\nversion of the image as needed. These gain maps, however, are typically\ndown-sampled and encoded using standard image compression, such as JPEG and\nHEIC, which can result in unwanted artifacts. In this paper, we propose to use\na lightweight multi-layer perceptron (MLP) network to encode the gain map. The\nMLP is optimized using the SDR image information as input and provides superior\nperformance in terms of HDR reconstruction. Moreover, the MLP-based approach\nuses a fixed memory footprint (10 KB) and requires no additional adjustments to\naccommodate different image sizes or encoding parameters. We conduct extensive\nexperiments on various MLP based HDR embedding strategies and demonstrate that\nour approach outperforms the current state-of-the-art.",
        "This article presents an extended author's version based on our previous\nwork, where we introduced the Multiple Overlapping Tiles (MOT) method for palm\nvein image enhancement. To better reflect the specific operations involved, we\nrename MOT to ILACS-LGOT (Intensity-Limited Adaptive Contrast Stretching with\nLayered Gaussian-weighted Overlapping Tiles). This revised terminology more\naccurately represents the method's approach to contrast enhancement and blocky\neffect mitigation. Additionally, this article provides a more detailed\nanalysis, including expanded evaluations, graphical representations, and\nsample-based comparisons, demonstrating the effectiveness of ILACS-LGOT over\nexisting methods.",
        "This paper introduces a deep learning (DL)-based framework for task-based\nultrasound (US) beamforming, aiming to enhance clinical outcomes by integrating\nspecific clinical tasks directly into the beamforming process. Task-based\nbeamforming optimizes the beamformer not only for image quality but also for\nperformance on a particular clinical task, such as lesion classification. The\nproposed framework explores two approaches: (1) a Joint Beamformer and\nClassifier (JBC) that classifies the US images generated by the beamformer to\nprovide feedback for image quality improvement; and (2) a Channel Data\nClassifier Beamformer (CDCB) that incorporates classification directly at the\nchannel data representation within the beamformer's bottleneck layer.\nAdditionally, we introduce channel data augmentations to address challenges\nposed by noisy and limited in-vivo data. Numerical evaluations demonstrate that\ntraining with channel data augmentations significantly improves image quality.\nThe proposed methods were evaluated against conventional Delay-and-Sum (DAS)\nand Minimum Variance (MV) beamforming techniques, demonstrating superior\nperformance in terms of both image contrast and clinical relevance. Among all\nmethods, the CDCB approach achieves the best results, outperforming others in\nterms of image quality and clinical relevance. These approaches exhibit\nsignificant potential for improving clinical relevance and image quality in\nultrasound imaging.",
        "Noise-inclusive fully unsupervised anomaly detection (FUAD) holds significant\npractical relevance. Although various methods exist to address this problem,\nthey are limited in both performance and scalability. Our work seeks to\novercome these obstacles, enabling broader adaptability of unsupervised anomaly\ndetection (UAD) models to FUAD. To achieve this, we introduce the Synergy\nScoring Filter (SSFilter), the first fully unsupervised anomaly detection\napproach to leverage sample-level filtering. SSFilter facilitates end-to-end\nrobust training and applies filtering to the complete training set\npost-training, offering a model-agnostic solution for FUAD. Specifically,\nSSFilter integrates a batch-level anomaly scoring mechanism based on mutual\npatch comparison and utilizes regression errors in anomalous regions, alongside\nprediction uncertainty, to estimate sample-level uncertainty scores that\ncalibrate the anomaly scoring mechanism. This design produces a synergistic,\nrobust filtering approach. Furthermore, we propose a realistic anomaly\nsynthesis method and an integrity enhancement strategy to improve model\ntraining and mitigate missed noisy samples. Our method establishes\nstate-of-the-art performance on the FUAD benchmark of the recent large-scale\nindustrial anomaly detection dataset, Real-IAD. Additionally, dataset-level\nfiltering enhances the performance of various UAD methods on the FUAD\nbenchmark, and the high scalability of our approach significantly boosts its\npractical applicability.",
        "The medial axis transform is a well-known tool for shape recognition. Instead\nof the object contour, it equivalently describes a binary object in terms of a\nskeleton containing all centres of maximal inscribed discs. While this shape\ndescriptor is useful for many applications, it is also sensitive to noise:\nSmall boundary perturbations can result in large unwanted expansions of the\nskeleton. Pruning offers a remedy by removing unwanted skeleton parts. In our\ncontribution, we generalise this principle to skeleton sparsification: We show\nthat subsequently removing parts of the skeleton simplifies the associated\nshape in a hierarchical manner that obeys scale-space properties.\n  To this end, we provide both a continuous and discrete theory that\nincorporates architectural and simplification statements as well as\ninvariances. We illustrate how our skeletonisation scale-spaces can be employed\nfor practical applications with two proof-of-concept implementations for\npruning and compression.",
        "Empirical evidence has demonstrated that learning-based image compression can\noutperform classical compression frameworks. This has led to the ongoing\nstandardization of learned-based image codecs, namely Joint Photographic\nExperts Group (JPEG) AI. The objective of JPEG AI is to enhance compression\nefficiency and provide a software and hardwarefriendly solution. Based on our\nresearch, JPEG AI represents the first standardization that can facilitate the\nimplementation of a learned image codec on a mobile device. This article\npresents an overview of the variable rate coding functionality in JPEG AI,\nwhich includes three variable rate adaptations: a threedimensional quality map,\na fast bit rate matching algorithm, and a training strategy. The variable rate\nadaptations offer a continuous rate function up to 2.0 bpp, exhibiting a high\nlevel of performance, a flexible bit allocation between different color\ncomponents, and a region of interest function for the specified use case. The\nevaluation of performance encompasses both objective and subjective results.\nWith regard to the objective bit rate matching, the main profile with low\ncomplexity yielded a 13.1% BD-rate gain over VVC intra, while the high profile\nwith high complexity achieved a 19.2% BD-rate gain over VVC intra. The BD-rate\nresult is calculated as the mean of the seven perceptual metrics defined in the\nJPEG AI common test conditions. With respect to subjective results, the example\nof improving the quality of the region of interest is illustrated.",
        "Brain nuclei are clusters of anatomically distinct neurons that serve as\nimportant hubs for processing and relaying information in various neural\ncircuits. Fine-scale parcellation of the brain nuclei is vital for a\ncomprehensive understanding of its anatomico-functional correlations. Diffusion\nMRI tractography is an advanced imaging technique that can estimate the brain's\nwhite matter structural connectivity to potentially reveal the topography of\nthe nuclei of interest for studying its subdivisions. In this work, we present\na deep clustering pipeline, namely DeepNuParc, to perform automated, fine-scale\nparcellation of brain nuclei using diffusion MRI tractography. First, we\nincorporate a newly proposed deep learning approach to enable accurate\nsegmentation of the nuclei of interest directly on the dMRI data. Next, we\ndesign a novel streamline clustering-based structural connectivity feature for\na robust representation of voxels within the nuclei. Finally, we improve the\npopular joint dimensionality reduction and k-means clustering approach to\nenable nuclei parcellation at a finer scale. We demonstrate DeepNuParc on two\nimportant brain structures, i.e. the amygdala and the thalamus, that are known\nto have multiple anatomically and functionally distinct nuclei subdivisions.\nExperimental results show that DeepNuParc enables consistent parcellation of\nthe nuclei into multiple parcels across multiple subjects and achieves good\ncorrespondence with the widely used coarse-scale atlases. Our codes are\navailable at https:\/\/github.com\/HarlandZZC\/deep_nuclei_parcellation.",
        "Diffusion models are successful for synthesizing high-quality videos but are\nlimited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained\nfootage (e.g. over minutes) still remains an open research question. In this\npaper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers),\na new diffusion model specialized for long video generation. MALT Diffusion (or\njust MALT) handles long videos by subdividing them into short segments and\ndoing segment-level autoregressive generation. To achieve this, we first\npropose recurrent attention layers that encode multiple segments into a compact\nmemory latent vector; by maintaining this memory vector over time, MALT is able\nto condition on it and continuously generate new footage based on a long\ntemporal context. We also present several training techniques that enable the\nmodel to generate frames over a long horizon with consistent quality and\nminimal degradation. We validate the effectiveness of MALT through experiments\non long video benchmarks. We first perform extensive analysis of MALT in\nlong-contextual understanding capability and stability using popular long video\nbenchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video\ngeneration on UCF-101, outperforming the previous state-of-the-art of 648.4.\nFinally, we explore MALT's capabilities in a text-to-video generation setting\nand show that it can produce long videos compared with recent techniques for\nlong text-to-video generation.",
        "When experimentally learning the action of a continuous variable quantum\nprocess by probing it with inputs, there will often be some restriction on the\ninput states used. One experimentally simple way to probe the channel is using\nlow-energy coherent states. Learning a quantum channel in this way presents\ndifficulties, due to the fact that two channels may act similarly on low energy\ninputs but very differently for high energy inputs. They may also act similarly\non coherent state inputs but differently on non-classical inputs. Extrapolating\nthe behaviour of a channel for more general input states from its action on the\nfar more limited set of low energy coherent states is a case of\nout-of-distribution generalisation. To be sure that such generalisation gives\nmeaningful results, one needs to relate error bounds for the training set to\nbounds that are valid for all inputs. We show that for any pair of channels\nthat act sufficiently similarly on low energy coherent state inputs, one can\nbound how different the input-output relations are for any (high energy or\nhighly non-classical) input. This proves out-of-distribution generalisation is\nalways possible for learning quantum channels using low energy coherent states.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single\/multiple agents,\nsingle\/multiple backdoors, discrete\/continuous action spaces, and sparse\/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps:\/\/github.com\/maoubo\/UNIDOOR.",
        "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
        "We extend our previous work on a novel class of unstable qubits which we have\nidentified recently and called them Critical Unstable Qubits (CUQs). The\ncharacteristic property of CUQs is that the energy-level and decay-width\nvectors, ${\\bf E}$ and ${\\bf \\Gamma}$, are orthogonal to one another, and the\nkey parameter $r = |{\\bf \\Gamma}|\/|2{\\bf E}|$ is less than 1. Most remarkably,\nCUQs exhibit two atypical behaviours: (i) they display coherence-decoherence\noscillations in a co-decaying frame of the system described by a unit Bloch\nvector ${\\bf b}$, and (ii) the unit Bloch vector ${\\bf b}$ describing a pure\nCUQ sweeps out unequal areas during equal intervals of time, while rotating\nabout the vector ${\\bf E}$. The latter anharmonic phenomenon emerges beyond the\nusual oscillatory pattern due to the energy-level difference of the two-level\nquantum system, which governs an ordinary qubit. By making use of a Fourier\nseries decomposition, we define anharmonicity observables that quantify the\ndegree of non-sinusoidal oscillation of a CUQ. We apply the results of our\nformalism to the $B^0\\bar{B}^0$-meson system and derive, for the first time,\ngeneric upper limits on these new observables.",
        "Mobile phone agents can assist people in automating daily tasks on their\nphones, which have emerged as a pivotal research spotlight. However, existing\nprocedure-oriented agents struggle with cross-app instructions, due to the\nfollowing challenges: (1) complex task relationships, (2) diverse app\nenvironment, and (3) error propagation and information loss in multi-step\nexecution. Drawing inspiration from object-oriented programming principles, we\nrecognize that object-oriented solutions is more suitable for cross-app\ninstruction. To address these challenges, we propose a self-evolving\nmulti-agent framework named MobileSteward, which integrates multiple\napp-oriented StaffAgents coordinated by a centralized StewardAgent. We design\nthree specialized modules in MobileSteward: (1) Dynamic Recruitment generates a\nscheduling graph guided by information flow to explicitly associate tasks among\napps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each\nequipped with app-specialized expertise to address the diversity between apps.\n(3) Adjusted Evaluation conducts evaluation to provide reflection tips or\ndeliver key information, which alleviates error propagation and information\nloss during multi-step execution. To continuously improve the performance of\nMobileSteward, we develop a Memory-based Self-evolution mechanism, which\nsummarizes the experience from successful execution, to improve the performance\nof MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench)\nin the real-world environment to evaluate the agents' capabilities of solving\ncomplex cross-app instructions. Experimental results demonstrate that\nMobileSteward achieves the best performance compared to both single-agent and\nmulti-agent frameworks, highlighting the superiority of MobileSteward in better\nhandling user instructions with diverse complexity.",
        "Matrix Product States (MPS) and Tensor Networks provide a general framework\nfor the construction of solvable models. The best-known example is the\nAffleck-Kennedy-Lieb-Tasaki (AKLT) model, which is the ground state of a 2-body\nnearest-neighbor parent Hamiltonian. We show that such simple parent\nHamiltonians for MPS models are, in fact, much more prevalent than hitherto\nknown: The existence of a single example with a simple Hamiltonian for a given\nchoice of dimensions already implies that any generic MPS with those dimensions\npossesses an equally simple Hamiltonian. We illustrate our finding by\ndiscussing a number of models with nearest-neighbor parent Hamiltonians, which\ngeneralize the AKLT model on various levels.",
        "Urban air mobility (UAM) is a transformative system that operates various\nsmall aerial vehicles in urban environments to reshape urban transportation.\nHowever, integrating UAM into existing urban environments presents a variety of\ncomplex challenges. Recent analyses of UAM's operational constraints highlight\naircraft noise and system safety as key hurdles to UAM system implementation.\nFuture UAM air traffic management schemes must ensure that the system is both\nquiet and safe. We propose a multi-agent reinforcement learning approach to\nmanage UAM traffic, aiming at both vertical separation assurance and noise\nmitigation. Through extensive training, the reinforcement learning agent learns\nto balance the two primary objectives by employing altitude adjustments in a\nmulti-layer UAM network. The results reveal the tradeoffs among noise impact,\ntraffic congestion, and separation. Overall, our findings demonstrate the\npotential of reinforcement learning in mitigating UAM's noise impact while\nmaintaining safe separation using altitude adjustments",
        "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.",
        "We introduce k-LLMmeans, a novel modification of the k-means clustering\nalgorithm that utilizes LLMs to generate textual summaries as cluster\ncentroids, thereby capturing contextual and semantic nuances often lost when\nrelying on purely numerical means of document embeddings. This modification\npreserves the properties of k-means while offering greater interpretability:\nthe cluster centroid is represented by an LLM-generated summary, whose\nembedding guides cluster assignments. We also propose a mini-batch variant,\nenabling efficient online clustering for streaming text data and providing\nreal-time interpretability of evolving cluster centroids. Through extensive\nsimulations, we show that our methods outperform vanilla k-means on multiple\nmetrics while incurring only modest LLM usage that does not scale with dataset\nsize. Finally, We present a case study showcasing the interpretability of\nevolving cluster centroids in sequential text streams. As part of our\nevaluation, we compile a new dataset from StackExchange, offering a benchmark\nfor text-stream clustering.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "We review some regularity results for the Laplacian and $p$-Laplacian in\nmetric measure spaces. The focus is mainly on interior H\\\"older, Lipschitz and\nsecond-regularity estimates and on spaces supporting a Poincar\\'e inequality or\nhaving Ricci curvature bounded below.",
        "Automated viewpoint classification in echocardiograms can help\nunder-resourced clinics and hospitals in providing faster diagnosis and\nscreening when expert technicians may not be available. We propose a novel\napproach towards echocardiographic viewpoint classification. We show that\ntreating viewpoint classification as video classification rather than image\nclassification yields advantage. We propose a CNN-GRU architecture with a novel\ntemporal feature weaving method, which leverages both spatial and temporal\ninformation to yield a 4.33\\% increase in accuracy over baseline image\nclassification while using only four consecutive frames. The proposed approach\nincurs minimal computational overhead. Additionally, we publish the Neonatal\nEchocardiogram Dataset (NED), a professionally-annotated dataset providing\nsixteen viewpoints and associated echocardipgraphy videos to encourage future\nwork and development in this field. Code available at:\nhttps:\/\/github.com\/satchelfrench\/NED"
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"A framework for evaluating the performance of SMLM cluster analysis algorithms",
    "start_abstract":"This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
      ],
      "abstract":[
        "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Progress of the anti-obesity of Berberine",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Silicon is the next frontier in plant synthetic biology",
        "An Evaluation on the Role of Non-Coding RNA in HIV Transcription and\n  Latency: A Review",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "A Comprehensive Review of Protein Language Models",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "Mechanism of Shape Symmetry Breaking in Surfactant Mediated Crystal\n  Growth",
        "A New Interpretation for the Hot Corona in Active Galactic Nuclei",
        "Dynamics of Quantum Correlations within the double Caldeira-Leggett\n  formalism",
        "Comparative Time-Series Analysis of Hip and Shoulder Rotation in\n  Baseball Bat Swings",
        "Nearly tight weighted 2-designs in complex projective spaces of every\n  dimension",
        "On the viability of higher order theories",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Exceptional-point-controlled mode interaction in three-dimensional\n  microcavities represented by generalized Husimi functions",
        "Proxy Control Barrier Functions: Integrating Barrier-Based and\n  Lyapunov-Based Safety-Critical Control Design",
        "New exact spatially localized solutions of the (3 + 1) -dimensional\n  nonlinear non-dissipative quasi-geostrophic potential vorticity equation for\n  an exponential atmosphere",
        "Hierarchies from deterministic non-locality in theory space Anderson\n  localisation",
        "Efficient detection of entanglement by stimulated disentanglement",
        "Fractional Sobolev spaces related to an ultraparabolic operator",
        "Geography of irreducible 4-manifolds with order two fundamental group",
        "Evaluation codes arising from symmetric polynomials"
      ],
      "abstract":[
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "Obesity is defined as the excessive accumulation or abnormal distribution of\nbody fat. According to data from World Obesity Atlas 2024, the increase in\nprevalence of obesity has become a major worldwide health problem in adults as\nwell as among children and adolescents. Although an increasing number of drugs\nhave been approved for the treatment of obesity in recent years, many of these\ndrugs have inevitable side effects which have increased the demand for new\nsafe, accessible and effective drugs for obesity and prompt interest in natural\nproducts. Berberine (BBR) and its metabolites, known for their multiple\npharmacological effects. Recent studies have emphatically highlighted the\nanti-obesity benefits of BBR and the underlying mechanisms have been gradually\nelucidated. However, its clinical application is limited by poor oral\nabsorption and low bioavailability. Based on this, this review summarizes\ncurrent research on the anti-obesity effects of BBR and its metabolites,\nincluding advancements in clinical trail results, understanding potential\nmolecular mechanisms and absorption and bioavailability. As a natural compound\nderived from plants, BBR holds potential as an alternative approach for\nmanaging obesity.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "The existence of latent cellular reservoirs is recognized as the major\nbarrier to an HIV cure. Reactivating and eliminating \"shock and kill\" or\npermanently silencing \"block and lock\" the latent HIV reservoir, as well as\ngene editing, remain promising approaches, but so far have proven to be only\npartially successful. Moreover, using latency reversing agents or \"block and\nlock\" drugs pose additional considerations, including the ability to cause\ncellular toxicity, a potential lack of specificity for HIV, or low potency when\neach agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long\nnon-coding RNAs (lncRNAs) are becoming increasingly recognized as important\nregulators of gene expression. RNA-based approaches for combatting HIV latency\nrepresent a promising strategy since both miRNAs and lncRNAs are more cell-type\nand tissue specific than protein coding genes. Thus, a higher specificity of\ntargeting the latent HIV reservoir with less overall cellular toxicity can\nlikely be achieved. In this review, we summarize current knowledge about HIV\ngene expression regulation by miRNAs and lncRNAs encoded in the human genome,\nas well as regulatory molecules encoded in the HIV genome. We discuss both the\ntranscriptional and post-transcriptional regulation of HIV gene expression to\nalign with the current definition of latency, and describe RNA molecules that\neither promote HIV latency or have anti-latency properties. Finally, we provide\nperspectives on using each class of RNAs as potential targets for combatting\nHIV latency, and describe the complexity of the interactions between different\nRNA molecules, their protein targets, and HIV.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "At the intersection of the rapidly growing biological data landscape and\nadvancements in Natural Language Processing (NLP), protein language models\n(PLMs) have emerged as a transformative force in modern research. These models\nhave achieved remarkable progress, highlighting the need for timely and\ncomprehensive overviews. However, much of the existing literature focuses\nnarrowly on specific domains, often missing a broader analysis of PLMs. This\nstudy provides a systematic review of PLMs from a macro perspective, covering\nkey historical milestones and current mainstream trends. We focus on the models\nthemselves and their evaluation metrics, exploring aspects such as model\narchitectures, positional encoding, scaling laws, and datasets. In the\nevaluation section, we discuss benchmarks and downstream applications. To\nfurther support ongoing research, we introduce relevant mainstream tools.\nLastly, we critically examine the key challenges and limitations in this\nrapidly evolving field.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "We present a dynamical model of crystal growth, in which it is possible to\nreliably achieve asymmetric products, beginning from symmetric initial\nconditions and growing within an isotropic environment. The asymmetric growth\nis the result of a positive feedback mechanism that amplifies the effect of\nthermal fluctuations in the coverage of surfactants on the growing crystalline\nfacets. Within our simple model, we are able to understand the kinetic and\nthermodynamic factors involved in both the onset of symmetry breaking and the\npersistence of anisotropic growth. We demonstrate that the mechanism is general\nby studying models with increasing complexity. We argue that this mechanism of\nsymmetry breaking underpins observations of colloidal, seed-mediated syntheses\nof single crystalline metal nanorods capped with strongly interacting\nsurfactants. The parameters within our model are related to experimental\nobservables such as the concentration, hydrophobicity, and binding strength of\nthe surfactants, which suggests a potential route to optimize the yield of\nasymmetric products in colloidal nanoparticle syntheses.",
        "This work attempts to provide a new interpretation for the hot corona in\nactive galactic nuclei (AGNs). A thin\n  parabolic magnetic reconnection layer, anchored at the innermost disk and\nextending along the boundary of the\n  magnetic tower for a few tens of gravitational radii, serves as a hard-X-ray\nsource above the disk. Within this\n  reconnection layer, the tearing instability leads to the formation of a chain\nof plasmoids, which contain relativistic\n  electrons that generate X-ray radiation through inverse-Compton (IC)\nscattering of soft photons emitted by the\n  accretion disk. Based on previous theoretical works and numerical\nsimulations, we develop a heuristic framework\n  to parameterize the geometry and magnetization of the reconnection layer, as\nwell as to compute both the power of\n  the IC-scattering radiation and the height of the reconnection layer. Our\nmodel allows for a quantitative\n  investigation of the relation between the height of the corona and the X-ray\nradiation luminosity, which can be\n  directly compared against the observed relation from X-ray reverberation\nmapping of individual AGNs. The\n  theoretical results are in good agreement with the observations of IRAS\n13224-3809, indicating the validation of\n  our model.",
        "This study investigates the effects of decoherence and squeezing on the\ndynamics of various kinds of quantum features--local quantum coherence, local\nentropy, EPR correlations, and entanglement--in the high-temperature limit of\nthe double Caldeira-Leggett model, focusing on initially squeezed states. We\ncompare two scenarios: (1) particles interacting with distinct environments and\n(2) particles coupled to a common environment. Our analysis reveals that common\nenvironments better preserve local coherence over time, whereas distinct\nenvironments accelerate decoherence. Temperature enhances decoherence and\nsuppresses coherence revivals, while squeezing affects transient dynamics but\nnot long-term coherence saturation. Local entropy increases with temperature\nand squeezing, though their underlying physical mechanisms differ. EPR\ncorrelations degrade due to environmental interactions, with squeezing\ninitially enhancing them but failing to prevent their eventual loss.\nEntanglement exhibits distinct behaviors: in separate environments, it\nundergoes sudden death, whereas in common environments, it experiences a dark\nperiod whose duration shortens with stronger squeezing. These findings provide\na comprehensive understanding of how decoherence and squeezing influence\nquantum correlations in open quantum systems.",
        "This study focuses on the rotation of the hips and shoulders during a\nbaseball bat swing, analyzing the time-series changes in rotational angles,\nrotational velocities, and axes using marker position data obtained from a\nmotion capture system with 12 infrared cameras. Previous studies have examined\nfactors such as ground reaction forces, muscle activation patterns, rotational\nenergy, angular velocity, and angles during a swing. However, to the best of\nour knowledge, the hip and shoulder rotational motions have not been adequately\nvisualized or compared. In particular, there is a lack of analysis regarding\nthe coordination and timing differences between hip and shoulder movements\nduring the swing. Therefore, this study aims to quantitatively compare the hip\nand shoulder rotational movements during the swing between skilled and\nunskilled players and visualizes the differences between them. Based on the\nobtained data, the study aims to improve the understanding of bat swing\nmechanics by visualizing the coordinated body movements during the swing.",
        "We use dense Sidon sets to construct small weighted projective 2-designs.\nThis represents quantitative progress on Zauner's conjecture.",
        "In physics, all dynamical equations that describe fundamental interactions\nare second order ordinary differential equations in the time derivatives. In\nthe literature, this property is traced back to a result obtained by\nOstrogradski in the mid 19th century, which is the technical basis of a 'no-go'\ntheorem for higher order theories. In this work, we review the connection of\nsymmetry properties with the order of dynamical equations, before reconsidering\nOstrogradski's result. Then, we show how Ostrogradski's conclusion is reached\nby applying to higher order theories concepts and method that have been\nspecifically developed for second order theories. We discuss a potential lack\nof consistency in this approach, to support the claim that Ostrogradski's\nresult applies to a class of higher order theories that is nowhere\nrepresentative of generic ones: we support this claim by giving an example of a\nhigher-order Lagrangian that is asymptotically stable, but that would be\nunstable under Ostrogradski's criterion. We also conclude that, when\nconsidering higher order theories as fundamental, we may need to reconsider and\nextend the conceptual framework on which our standard treatment of second order\ntheories is based.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Non-Hermitian photonics has attracted significant interest and influences\nseveral key areas such as optical metamaterials, laser physics, and nonlinear\noptics. While non-Hermitian effects have been widely addressed in\ntwo-dimensional systems, we focus on realistic three-dimensional devices. To\nthis end we generalize established phase space methods from mesoscopic optics\nand introduce Husimi functions for three-dimensional systems that deepen the\ninsight and access to the mode morphology and their dynamics. We illustrate\nthat four-dimensional Husimi functions can be represented using a specific\nprojection in two dimensions and illustrate it for (conical) cylindrical\ncavities. The non-Hermitian character of the intrinsically open photonic\nsystems is in particular revealed when examining the TE and TM polarization\ncharacter of the resonance modes. Unlike the 2D case, polarization is not\nconserved in three-dimensional cavities, and we use generalized Husimi function\nto represent the interaction of polarization modes. We find their dynamics to\nbe ruled by a network of exceptional points in the parameter space spanned by\nthe refractive index and the cavity geometry tilt angle. This approach not only\nenhances our understanding of cavity modes but also aids in the design of more\nefficient photonic devices and systems.",
        "This work introduces a novel Proxy Control Barrier Function (PCBF) scheme\nthat integrates barrier-based and Lyapunov-based safety-critical control\nstrategies for strict-feedback systems with potentially unknown dynamics. The\nproposed method employs a modular design procedure, decomposing the original\nsystem into a proxy subsystem and a virtual tracking subsystem that are\ncontrolled by the control barrier function (CBF)-based and Lyapunov-based\ncontrollers, respectively. By integrating these separately designed\ncontrollers, the overall system's safety is ensured. Moreover, a new\nfilter-based disturbance observer is utilized to design a PCBF-based safe\ncontroller for strict-feedback systems subject to mismatched disturbances. This\napproach broadens the class of systems to which CBF-based methods can be\napplied and significantly simplifies CBF construction by requiring only the\nmodel of the proxy subsystem. The effectiveness of the proposed method is\ndemonstrated through numerical simulations.",
        "New exact spatially localized stationary solutions against the background of\na zonal flow are found for the (3+1)-dimensional nonlinear non-dissipative\nquasi-geostrophic potential vorticity equation, which describes Rossby waves\nand vortices in an exponential atmosphere. In total, three solutions are\npresented. The nonlinear boundary conditions with a flat bottom and a rigid lid\ngenerate an infinite discrete set of baroclinic modes for each solution. The\nsolutions show the possibility of existence of baroclinic dipoles in the\nexponential atmosphere, similar to baroclinic dipoles in the ocean. It is shown\nthat: a) a pair of vortices in the baroclinic dipole appears and disappears\nwhen the velocity of stationary motion changes; b) the baroclinic dipoles show\nthe ability to transfer warm or cold air depending on the polarity of the\nvortices in the dipole.",
        "The nearest-neighbour or local mass terms in theory space among quantum\nfields, with their generic disordered values, are known to lead to the\nlocalisation of mass eigenstates, analogous to Anderson localisation in a\none-dimensional lattice. This mechanism can be used to create an exponential\nhierarchy in the coupling between two fields by placing them at opposite ends\nof the lattice chain. Extending this mechanism, we show that when copies of\nsuch fields are appropriately attached to the lattice chain, it leads to the\nemergence of multiple massless modes. These vanishing masses are a direct\nconsequence of the locality of interactions in theory space. The latter may\nbreak down in an ordered and deterministic manner through quantum effects if\nadditional interactions exist among the chain fields. Such non-locality can\ninduce small masses for the otherwise massless modes without necessarily\ndelocalising the mass eigenstates. We provide examples of interactions that\npreserve or even enhance localisation. Applications to flavour hierarchies,\nneutrino mass, and the $\\mu$-problem in supersymmetric theories are discussed.",
        "Standard detection of entanglement relies on local measurements of the\nindividual particles, evaluating their correlations in post-processing. For\ntime-energy entangled photons, either times $(t_{1},t_{2})$ or energies\n$(E_{1},E_{2})$ are measured, but not both due to the mutual quantum\nuncertainty, providing only partial information of the entanglement. In\nprinciple, a global detector could recover the complete information of\nentanglement in a single shot if it could measure the combined correlated\nvariables $(t_{1}-t_{2})$ and $(E_{1}+E_{2})$ without measuring the individual\nenergies or times. Such a global measurement is possible using the reverse\ndisentangling interaction, like sum-frequency generation (SFG), but nonlinear\ninteractions at the single-photon level are exceedingly inefficient. We\novercome this barrier by stimulating the nonlinear SFG interaction with a\nstrong pump, thereby measuring both the energy-sum (SFG spectrum) and the\ntime-difference (response to group delay\/dispersion) simultaneously and\nefficiently. We generate bi-photons with extreme time-energy entanglement\n(octave-spanning spectrum of 113THz) and measure a relative uncertainty between\ntime-difference and energy-sum of\n$\\Delta(t_1-t_2)\\Delta(E_1+E_2)\\approx\\!2\\!\\cdot\\!10^{-13}h$, violating the\nclassical bound by more than 12 orders of magnitude. The presented coherent SFG\ndramatically enhances the detection SNR compared to standard methods since it\nideally rejects erroneous coincidences in both time and energy, paving the way\nfor sensing applications, such as quantum illumination (radar) and more.",
        "We propose a functional framework of fractional Sobolev spaces for a class of\nultra-parabolic Kolmogorov type operators satisfying the weak H\\\"ormander\ncondition. We characterize these spaces as real interpolation of natural order\nintrinic Sobolev spaces recently introduced in [27], and prove continuous\nembeddings into $L^p$ and intrinsic H\\\"older spaces from [24]. These embeddings\nnaturally extend the standard Euclidean ones, coherently with the homogeneous\nstructure of the associated Kolmogorov group. Our approach to interpolation is\nbased on approximation of intrinsically regular functions, the latter heavily\nrelying on integral estimates of the intrinsic Taylor remainder. The embeddings\nexploit the aforementioned interpolation property and the corresponding\nembeddings of natural order intrinsic spaces.",
        "Let $R$ be a closed, oriented topological 4-manifold whose Euler\ncharacteristic and signature are denoted by $e$ and $\\sigma$. We show that if\n$R$ has order two $\\pi_1$, odd intersection form, and $2e + 3\\sigma \\geq 0$,\nthen for all but seven $(e, \\sigma)$ coordinates, $R$ admits an irreducible\nsmooth structure. We accomplish this by performing a variety of operations on\nirreducible simply-connected 4-manifolds to build 4-manifolds with order two\n$\\pi_1$. These techniques include torus surgeries, symplectic fiber sums,\nrational blow-downs, and numerous constructions of Lefschetz fibrations,\nincluding a new approach to equivariant fiber summing.",
        "Datta and Johnsen (Des. Codes and Cryptogr., {\\bf{91}} (2023), 747-761)\nintroduced a new family of evalutation codes in an affine space of dimension\n$\\ge 2$ over a finite field $\\mathbb{F}_q$ where linear combinations of\nelementary symmetric polynomials are evaluated on the set of all points with\npairwise distinct coordinates. In this paper, we propose a generalization by\ntaking low dimensional linear systems of symmetric polynomials. Computation for\nsmall values of $q=7,9$ shows that carefully chosen generalized Datta-Johnsen\ncodes $\\left[\\frac{1}{2}q(q-1),3,d\\right]$ have minimum distance $d$ equal to\nthe optimal value minus 1."
      ]
    }
  },
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "start_abstract":"It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products.",
    "start_categories":[
      "physics.gen-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Auto-Encoding Variational Bayes"
      ],
      "abstract":[
        "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Theorem Prover as a Judge for Synthetic Data Generation",
        "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!",
        "Empirical Evaluation of the Implicit Hitting Set Approach for Weighted\n  CSPs",
        "NS-Gym: Open-Source Simulation Environments and Benchmarks for\n  Non-Stationary Markov Decision Processes",
        "A3: Android Agent Arena for Mobile GUI Agents",
        "Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the\n  ARC Task",
        "Seeding for Success: Skill and Stochasticity in Tabletop Games",
        "The Internet of Large Language Models: An Orchestration Framework for\n  LLM Training and Knowledge Exchange Toward Artificial General Intelligence",
        "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol",
        "Knowledge is Power: Harnessing Large Language Models for Enhanced\n  Cognitive Diagnosis",
        "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning",
        "GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular\n  Representation and Diffusion Generation",
        "Intelligence Sequencing and the Path-Dependence of Intelligence\n  Evolution: AGI-First vs. DCI-First as Irreversible Attractors",
        "ALPET: Active Few-shot Learning for Citation Worthiness Detection in\n  Low-Resource Wikipedia Languages",
        "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity",
        "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource\n  Awareness",
        "D3PO: Preference-Based Alignment of Discrete Diffusion Models",
        "Pre-Equalization Aided Grant-Free Massive Access in Massive MIMO System",
        "MSV-Mamba: A Multiscale Vision Mamba Network for Echocardiography\n  Segmentation",
        "A stochastic programming approach for the scheduling of medical\n  interpreting service under uncertainty",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "A dynamical proof of Matui's absorption theorem",
        "Semantic Communications Services within Generalist Operated Networks",
        "Unified Approaches in Self-Supervised Event Stream Modeling: Progress\n  and Prospects",
        "A note on finiteness properties of vertex stabilisers",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "GameFactory: Creating New Games with Generative Interactive Videos",
        "Handling Uncertainty in Health Data using Generative Algorithms"
      ],
      "abstract":[
        "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
        "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https:\/\/github.com\/NovaSky-AI\/SkyThought.",
        "SAT technology has proven to be surprisingly effective in a large variety of\ndomains. However, for the Weighted CSP problem dedicated algorithms have always\nbeen superior. One approach not well-studied so far is the use of SAT in\nconjunction with the Implicit Hitting Set approach. In this work, we explore\nsome alternatives to the existing algorithm of reference. The alternatives,\nmostly borrowed from related boolean frameworks, consider trade-offs for the\ntwo main components of the IHS approach: the computation of low-cost hitting\nvectors, and their transformation into high-cost cores. For each one, we\npropose 4 levels of intensity. Since we also test the usefulness of cost\nfunction merging, our experiments consider 32 different implementations. Our\nempirical study shows that for WCSP it is not easy to identify the best\nalternative. Nevertheless, the cost-function merging encoding and extracting\nmaximal cores seems to be a robust approach.",
        "In many real-world applications, agents must make sequential decisions in\nenvironments where conditions are subject to change due to various exogenous\nfactors. These non-stationary environments pose significant challenges to\ntraditional decision-making models, which typically assume stationary dynamics.\nNon-stationary Markov decision processes (NS-MDPs) offer a framework to model\nand solve decision problems under such changing conditions. However, the lack\nof standardized benchmarks and simulation tools has hindered systematic\nevaluation and advance in this field. We present NS-Gym, the first simulation\ntoolkit designed explicitly for NS-MDPs, integrated within the popular\nGymnasium framework. In NS-Gym, we segregate the evolution of the environmental\nparameters that characterize non-stationarity from the agent's decision-making\nmodule, allowing for modular and flexible adaptations to dynamic environments.\nWe review prior work in this domain and present a toolkit encapsulating key\nproblem characteristics and types in NS-MDPs. This toolkit is the first effort\nto develop a set of standardized interfaces and benchmark problems to enable\nconsistent and reproducible evaluation of algorithms under non-stationary\nconditions. We also benchmark six algorithmic approaches from prior work on\nNS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to\nassess the adaptability and robustness of their decision-making algorithms to\nnon-stationary conditions.",
        "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at https:\/\/yuxiangchai.github.io\/Android-Agent-Arena\/.",
        "While LLMs have exhibited strong performance on various NLP tasks, it is\nnoteworthy that most of these tasks rely on utilizing the vast amount of\nknowledge encoded in LLMs' parameters, rather than solving new problems without\nprior knowledge. In cognitive research, the latter ability is referred to as\nfluid intelligence, which is considered to be critical for assessing human\nintelligence. Recent research on fluid intelligence assessments has highlighted\nsignificant deficiencies in LLMs' abilities. In this paper, we analyze the\nchallenges LLMs face in demonstrating fluid intelligence through controlled\nexperiments, using the most representative ARC task as an example. Our study\nrevealed three major limitations in existing LLMs: limited ability for skill\ncomposition, unfamiliarity with abstract input formats, and the intrinsic\ndeficiency of left-to-right decoding. Our data and code can be found in\nhttps:\/\/wujunjie1998.github.io\/araoc-benchmark.github.io\/.",
        "Games often incorporate random elements in the form of dice or shuffled card\ndecks. This randomness is a key contributor to the player experience and the\nvariety of game situations encountered. There is a tension between a level of\nrandomness that makes the game interesting and contributes to the player\nenjoyment of a game, and a level at which the outcome itself is effectively\nrandom and the game becomes dull. The optimal level for a game will depend on\nthe design goals and target audience. We introduce a new technique to quantify\nthe level of randomness in game outcome and use it to compare 15 tabletop games\nand disentangle the different contributions to the overall randomness from\nspecific parts of some games. We further explore the interaction between game\nrandomness and player skill, and how this innate randomness can affect error\nanalysis in common game experiments.",
        "This paper explores the multi-dimensional challenges faced during the\ndevelopment of Large Language Models (LLMs), including the massive scale of\nmodel parameters and file sizes, the complexity of development environment\nconfiguration, the singularity of model functionality, and the high costs of\ncomputational resources. To address these challenges, this paper proposes three\ncore technical solutions: LLM sharing protocol, LLM universal environment\nframework, and Agent optimal path module. To solve the computational resource\nconstraints in the early stages of research, we further innovatively propose a\njoint mining mechanism, achieving bilateral value sharing between computing\npower providers and model designers, including breakthrough rewards for optimal\nmodel paths and long-term profit distribution, thereby providing researchers\nwith cost-optimized computational resource support and promoting the continuous\ndevelopment of LLM research and applications.",
        "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings.",
        "Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive\nstates by analyzing their performance across a series of exercises. However,\nexisting CDMs often struggle with diagnosing infrequent students and exercises\ndue to a lack of rich prior knowledge. With the advancement in large language\nmodels (LLMs), which possess extensive domain knowledge, their integration into\ncognitive diagnosis presents a promising opportunity. Despite this potential,\nintegrating LLMs with CDMs poses significant challenges. LLMs are not\nwell-suited for capturing the fine-grained collaborative interactions between\nstudents and exercises, and the disparity between the semantic space of LLMs\nand the behavioral space of CDMs hinders effective integration. To address\nthese issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD)\nframework, which is a model-agnostic framework utilizing LLMs to enhance CDMs\nand compatible with various CDM architectures. The KCD framework operates in\ntwo stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis\nstage, both students and exercises are diagnosed to achieve comprehensive and\ndetailed modeling. In the Cognitive Level Alignment stage, we bridge the gap\nbetween the CDMs' behavioral space and the LLMs' semantic space using\ncontrastive learning and mask-reconstruction approaches. Experiments on several\nreal-world datasets demonstrate the effectiveness of our proposed framework.",
        "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.",
        "Retrosynthesis prediction focuses on identifying reactants capable of\nsynthesizing a target product. Typically, the retrosynthesis prediction\ninvolves two phases: Reaction Center Identification and Reactant Generation.\nHowever, we argue that most existing methods suffer from two limitations in the\ntwo phases: (i) Existing models do not adequately capture the ``face''\ninformation in molecular graphs for the reaction center identification. (ii)\nCurrent approaches for the reactant generation predominantly use sequence\ngeneration in a 2D space, which lacks versatility in generating reasonable\ndistributions for completed reactive groups and overlooks molecules' inherent\n3D properties. To overcome the above limitations, we propose GDiffRetro. For\nthe reaction center identification, GDiffRetro uniquely integrates the original\ngraph with its corresponding dual graph to represent molecular structures,\nwhich helps guide the model to focus more on the faces in the graph. For the\nreactant generation, GDiffRetro employs a conditional diffusion model in 3D to\nfurther transform the obtained synthon into a complete reactant. Our\nexperimental findings reveal that GDiffRetro outperforms state-of-the-art\nsemi-template models across various evaluative metrics.",
        "The trajectory of intelligence evolution is often framed around the emergence\nof artificial general intelligence (AGI) and its alignment with human values.\nThis paper challenges that framing by introducing the concept of intelligence\nsequencing: the idea that the order in which AGI and decentralized collective\nintelligence (DCI) emerge determines the long-term attractor basin of\nintelligence. Using insights from dynamical systems, evolutionary game theory,\nand network models, it argues that intelligence follows a path-dependent,\nirreversible trajectory. Once development enters a centralized (AGI-first) or\ndecentralized (DCI-first) regime, transitions become structurally infeasible\ndue to feedback loops and resource lock-in. Intelligence attractors are modeled\nin functional state space as the co-navigation of conceptual and adaptive\nfitness spaces. Early-phase structuring constrains later dynamics, much like\nrenormalization in physics. This has major implications for AI safety:\ntraditional alignment assumes AGI will emerge and must be controlled after the\nfact, but this paper argues that intelligence sequencing is more foundational.\nIf AGI-first architectures dominate before DCI reaches critical mass,\nhierarchical monopolization and existential risk become locked in. If DCI-first\nemerges, intelligence stabilizes around decentralized cooperative equilibrium.\nThe paper further explores whether intelligence structurally biases itself\ntoward an attractor based on its self-modeling method -- externally imposed\naxioms (favoring AGI) vs. recursive internal visualization (favoring DCI).\nFinally, it proposes methods to test this theory via simulations, historical\nlock-in case studies, and intelligence network analysis. The findings suggest\nthat intelligence sequencing is a civilizational tipping point: determining\nwhether the future is shaped by unbounded competition or unbounded cooperation.",
        "Citation Worthiness Detection (CWD) consists in determining which sentences,\nwithin an article or collection, should be backed up with a citation to\nvalidate the information it provides. This study, introduces ALPET, a framework\ncombining Active Learning (AL) and Pattern-Exploiting Training (PET), to\nenhance CWD for languages with limited data resources. Applied to Catalan,\nBasque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW\nbaseline while reducing the amount of labeled data in some cases above 80\\%.\nALPET's performance plateaus after 300 labeled samples, showing it suitability\nfor low-resource scenarios where large, labeled datasets are not common. While\nspecific active learning query strategies, like those employing K-Means\nclustering, can offer advantages, their effectiveness is not universal and\noften yields marginal gains over random sampling, particularly with smaller\ndatasets. This suggests that random sampling, despite its simplicity, remains a\nstrong baseline for CWD in constraint resource environments. Overall, ALPET's\nability to achieve high performance with fewer labeled samples makes it a\npromising tool for enhancing the verifiability of online content in\nlow-resource language settings.",
        "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.",
        "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
        "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D3PO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D3PO on a structured binary sequence generation task,\ndemonstrating that the method effectively aligns model outputs with preferences\nwhile maintaining structural validity. Our results highlight that D3PO enables\ncontrolled fine-tuning without requiring explicit reward models, making it a\npractical alternative to reinforcement learning-based approaches. Future\nresearch will explore extending D3PO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
        "The spatial diversity and multiplexing advantages of massive\nmulti-input-multi-output (mMIMO) can significantly improve the capacity of\nmassive non-orthogonal multiple access (NOMA) in machine type communications.\nHowever, state-of-the-art grant-free massive NOMA schemes for mMIMO systems\nrequire accurate estimation of random access channels to perform activity\ndetection and the following coherent data demodulation, which suffers from\nexcessive pilot overhead and access latency. To address this, we propose a\npre-equalization aided grant-free massive access scheme for mMIMO systems,\nwhere an iterative detection scheme is conceived. Specifically, the base\nstation (BS) firstly activates one of its antennas (i.e., beacon antenna) to\nbroadcast a beacon signal, which facilitates the user equipment (UEs) to\nperform downlink channel estimation and pre-equalize the uplink random access\nsignal with respect to the channels associated with the beacon antenna. During\nthe uplink transmission stage, the BS detects UEs' activity and data by using\nthe proposed iterative detection algorithm, which consists of three modules:\ncoarse data detection (DD), data-aided channel estimation (CE), and fine DD. In\nthe proposed algorithm, the joint activity and DD is firstly performed based on\nthe signals received by the beacon antenna. Subsequently, the DD is further\nrefined by iteratively performing data-aided CE module and fine DD module using\nsignals received by all BS antennas. Our simulation results demonstrate that\nthe proposed scheme outperforms state-of-the-art mMIMO-based grant-free massive\nNOMA schemes with the same access latency. Simulation codes are provided to\nreproduce the results in this article: https:\/\/github.com\/owenwang517\/tvt-2025.",
        "Ultrasound imaging frequently encounters challenges, such as those related to\nelevated noise levels, diminished spatiotemporal resolution, and the complexity\nof anatomical structures. These factors significantly hinder the model's\nability to accurately capture and analyze structural relationships and dynamic\npatterns across various regions of the heart. Mamba, an emerging model, is one\nof the most cutting-edge approaches that is widely applied to diverse vision\nand language tasks. To this end, this paper introduces a U-shaped deep learning\nmodel incorporating a large-window Mamba scale (LMS) module and a hierarchical\nfeature fusion approach for echocardiographic segmentation. First, a cascaded\nresidual block serves as an encoder and is employed to incrementally extract\nmultiscale detailed features. Second, a large-window multiscale mamba module is\nintegrated into the decoder to capture global dependencies across regions and\nenhance the segmentation capability for complex anatomical structures.\nFurthermore, our model introduces auxiliary losses at each decoder layer and\nemploys a dual attention mechanism to fuse multilayer features both spatially\nand across channels. This approach enhances segmentation performance and\naccuracy in delineating complex anatomical structures. Finally, the\nexperimental results using the EchoNet-Dynamic and CAMUS datasets demonstrate\nthat the model outperforms other methods in terms of both accuracy and\nrobustness. For the segmentation of the left ventricular endocardium\n(${LV}_{endo}$), the model achieved optimal values of 95.01 and 93.36,\nrespectively, while for the left ventricular epicardium (${LV}_{epi}$), values\nof 87.35 and 87.80, respectively, were achieved. This represents an improvement\nranging between 0.54 and 1.11 compared with the best-performing model.",
        "Limited English Proficiency (LEP) patients face higher risks of adverse\nhealth outcomes due to communication barriers, making timely medical\ninterpreting services essential for mitigating those risks. This paper\naddresses the scheduling of medical interpreting services under uncertainty.\nThe problem is formulated as a two-stage stochastic programming model that\naccounts for uncertainties in emergency patients' arrival and service time. The\nmodel handles the hiring decisions of part-time interpreters and the assignment\nof full-time and hired part-time interpreters. The objective is to minimize the\ntotal cost, which encompasses full-time interpreters' overtime cost, the fixed\nand variable costs of part-time interpreters, and the penalty cost for not\nserving LEP patients on time. The model is solved using the Sample Average\nApproximation (SAA) algorithm. To overcome the computational burden of the SAA\nalgorithm, a Tabu Search (TS) algorithm was used to solve the model. A\nreal-life case study is used to validate and evaluate the proposed solution\nalgorithms. The results demonstrate the effectiveness of the proposed\nstochastic programming-based solutions in concurrently reducing both the total\ncost and the waiting time. Further, sensitivity analysis reveals how the\nincrease in some key parameters, such as the arrival rate of emergency patients\nwith LEP, impacts scheduling outcomes.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "We give a dynamical, relatively elementary proof of an \"absorption theorem\"\nwhich is closely related to a well-known result due to Matui. The construction\nis in the spirit of an earlier joint work of the author and S. Robert. In an\nappendix we explain how to use this result to correct the dynamical proof given\nby Melleray--Robert of a classification theorem for orbit equivalence of\nminimal ample groups due to Giordano, Putnam and Skau (the original argument\nhad a gap).",
        "This paper addresses the challenge of integrating semantic communication\nprinciples into operated networks, traditionally optimized based on\nnetwork-centric metrics rather than application-specific needs. Operated\nnetworks strongly adhere to the principle of ``separation of concerns\", which\nemphasizes a clear distinction between network operation and application.\nDespite the initial perceived incompatibility between semantic communication\nand the principles of operated networks, this paper provides solutions to\nreconcile them. The foundations of these solutions include the adoption of\nnon-arbitrary semantic representations as a standard encoding for\ncommunications, the establishment of a standard interface between the\napplication and network, and a dedicated network control plane. These enable\nthe application to describe the data typology and the nature of the task, and\nto agree upon a transmission scheme tailored to the supported task. Through\nthree scenarios involving an application transmitting text representations, we\nillustrate the implementation of the proposal and demonstrate the potential of\nthe approach.",
        "The proliferation of digital interactions across diverse domains, such as\nhealthcare, e-commerce, gaming, and finance, has resulted in the generation of\nvast volumes of event stream (ES) data. ES data comprises continuous sequences\nof timestamped events that encapsulate detailed contextual information relevant\nto each domain. While ES data holds significant potential for extracting\nactionable insights and enhancing decision-making, its effective utilization is\nhindered by challenges such as the scarcity of labeled data and the fragmented\nnature of existing research efforts. Self-Supervised Learning (SSL) has emerged\nas a promising paradigm to address these challenges by enabling the extraction\nof meaningful representations from unlabeled ES data. In this survey, we\nsystematically review and synthesize SSL methodologies tailored for ES modeling\nacross multiple domains, bridging the gaps between domain-specific approaches\nthat have traditionally operated in isolation. We present a comprehensive\ntaxonomy of SSL techniques, encompassing both predictive and contrastive\nparadigms, and analyze their applicability and effectiveness within different\napplication contexts. Furthermore, we identify critical gaps in current\nresearch and propose a future research agenda aimed at developing scalable,\ndomain-agnostic SSL frameworks for ES modeling. By unifying disparate research\nefforts and highlighting cross-domain synergies, this survey aims to accelerate\ninnovation, improve reproducibility, and expand the applicability of SSL to\ndiverse real-world ES challenges.",
        "We prove a criterion for the geometric and algebraic finiteness properties of\nvertex stabilisers of $G$-CW-complexes, given the finiteness properties of the\ngroup $G$ and of the stabilisers of positive dimensional cells. This\ngeneralises a result of Haglund--Wise for groups acting on trees to higher\ndimensions. As an application, for $n\\ge 2$, we deduce the existence of\nuncountably many quasi-isometry classes of one-ended groups that are of type\n$\\mathsf{FP}_n$ and not of type $\\mathsf{FP}_{n+1}$.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "Generative game engines have the potential to revolutionize game development\nby autonomously creating new content and reducing manual workload. However,\nexisting video-based game generation methods fail to address the critical\nchallenge of scene generalization, limiting their applicability to existing\ngames with fixed styles and scenes. In this paper, we present GameFactory, a\nframework focused on exploring scene generalization in game video generation.\nTo enable the creation of entirely new and diverse games, we leverage\npre-trained video diffusion models trained on open-domain video data. To bridge\nthe domain gap between open-domain priors and small-scale game dataset, we\npropose a multi-phase training strategy that decouples game style learning from\naction control, preserving open-domain generalization while achieving action\ncontrollability. Using Minecraft as our data source, we release GF-Minecraft, a\nhigh-quality and diversity action-annotated video dataset for research.\nFurthermore, we extend our framework to enable autoregressive\naction-controllable game video generation, allowing the production of\nunlimited-length interactive game videos. Experimental results demonstrate that\nGameFactory effectively generates open-domain, diverse, and action-controllable\ngame videos, representing a significant step forward in AI-driven game\ngeneration. Our dataset and project page are publicly available at\n\\url{https:\/\/vvictoryuki.github.io\/gamefactory\/}.",
        "Understanding and managing uncertainty is crucial in machine learning,\nespecially in high-stakes domains like healthcare, where class imbalance can\nimpact predictions. This paper introduces RIGA, a novel pipeline that mitigates\nclass imbalance using generative AI. By converting tabular healthcare data into\nimages, RIGA leverages models like cGAN, VQVAE, and VQGAN to generate balanced\nsamples, improving classification performance. These representations are\nprocessed by CNNs and later transformed back into tabular format for seamless\nintegration. This approach enhances traditional classifiers like XGBoost,\nimproves Bayesian structure learning, and strengthens ML model robustness by\ngenerating realistic synthetic data for underrepresented classes."
      ]
    }
  }
]