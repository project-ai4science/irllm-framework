[
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms",
    "start_abstract":"Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b1"
      ],
      "title":[
        "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
        "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
      ],
      "abstract":[
        "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
        "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
      ],
      "categories":[
        "q-bio.CB",
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Self-supervised Conformal Prediction for Uncertainty Quantification in\n  Imaging Problems",
        "Gradient-enhanced PINN with residual unit for studying forward-inverse\n  problems of variable coefficient equations",
        "Witnessing Magic with Bell inequalities",
        "Boundary conditions of general black hole perturbations",
        "The superadiabatic projectors method applied to the spectral theory of\n  magnetic operators",
        "Bosonic Amplitude-Damping Codes Beyond Binomial Schemes",
        "Differentiating the acceleration mechanisms in the slow and Alfv\\'enic\n  slow solar wind",
        "CIBER 4th flight fluctuation analysis: Measurements of near-IR auto- and\n  cross-power spectra on arcminute to sub-degree scales",
        "Towords the Near Optimal Sampling Complexity via Generalized Exponential\n  Spectral Pursuit",
        "Counting spinal phylogenetic networks",
        "On the characteristic structure of the adjoint Euler equations with\n  application to supersonic flows",
        "Approximation of the generalized principal eigenvalue of cooperative\n  nonlocal dispersal systems and applications",
        "Multimode fiber based high-dimensional light analyzer",
        "The White Dwarf Pareto: Tracing Mass Loss in Binary Systems",
        "Securing Integrated Sensing and Communication Against a Mobile\n  Adversary: A Stackelberg Game with Deep Reinforcement Learning",
        "Tip-Enhanced Raman Spectroscopy of Cell Wall Heterogeneity for\n  Aspergillus Fumigatus",
        "Impact of eccentricity and mean anomaly in numerical relativity mergers",
        "Thermal and thermoelectric transport in flat bands with non-trivial\n  quantum geometry",
        "Designing Flat Bands, Localized and Itinerant States in TaS2 Trilayer\n  Heterostructures",
        "Accurate Estimates of Ultimate 100-Meter Records",
        "Proportional asymptotics of piecewise exponential proportional hazards\n  models",
        "Mutual Regression Distance",
        "Pink-Beam Dark Field X-ray Microscopy: Expanding 3D\/4D Imaging for\n  Complex and Deformed Microstructures",
        "Critical String Theory in a $D=4$ Robertson-Walker Background and the\n  Large Scale Structure of Spacetime",
        "Finding Quasars Behind the Galactic Plane: Spectroscopic Identifications\n  of ~1300 New Quasars at |b|<=20 degree from LAMOST DR10",
        "Percolation of Domain Walls in the Two-Higgs Doublet Model",
        "d-plane transform: unique and non-unique continuation",
        "Searching for compact pentaquark state within the bag model framework"
      ],
      "abstract":[
        "Most image restoration problems are ill-conditioned or ill-posed and hence\ninvolve significant uncertainty. Quantifying this uncertainty is crucial for\nreliably interpreting experimental results, particularly when reconstructed\nimages inform critical decisions and science. However, most existing image\nrestoration methods either fail to quantify uncertainty or provide estimates\nthat are highly inaccurate. Conformal prediction has recently emerged as a\nflexible framework to equip any estimator with uncertainty quantification\ncapabilities that, by construction, have nearly exact marginal coverage. To\nachieve this, conformal prediction relies on abundant ground truth data for\ncalibration. However, in image restoration problems, reliable ground truth data\nis often expensive or not possible to acquire. Also, reliance on ground truth\ndata can introduce large biases in situations of distribution shift between\ncalibration and deployment. This paper seeks to develop a more robust approach\nto conformal prediction for image restoration problems by proposing a\nself-supervised conformal prediction method that leverages Stein's Unbiased\nRisk Estimator (SURE) to self-calibrate itself directly from the observed noisy\nmeasurements, bypassing the need for ground truth. The method is suitable for\nany linear imaging inverse problem that is ill-conditioned, and it is\nespecially powerful when used with modern self-supervised image restoration\ntechniques that can also be trained directly from measurement data. The\nproposed approach is demonstrated through numerical experiments on image\ndenoising and deblurring, where it delivers results that are remarkably\naccurate and comparable to those obtained by supervised conformal prediction\nwith ground truth data.",
        "Physics-informed neural network (PINN) is a powerful emerging method for\nstudying forward-inverse problems of partial differential equations (PDEs),\neven from limited sample data. Variable coefficient PDEs, which model\nreal-world phenomena, are of considerable physical significance and research\nvalue. This study proposes a gradient-enhanced PINN with residual unit\n(R-gPINN) method to solve the data-driven solution and function discovery for\nvariable coefficient PDEs. On the one hand, the proposed method incorporates\nresidual units into the neural networks to mitigate gradient vanishing and\nnetwork degradation, unify linear and nonlinear coefficient problem. We present\ntwo types of residual unit structures in this work to offer more flexible\nsolutions in problem-solving. On the other hand, by including gradient terms of\nvariable coefficients, the method penalizes collocation points that fail to\nsatisfy physical properties. This enhancement improves the network's adherence\nto physical constraints and aligns the prediction function more closely with\nthe objective function. Numerical experiments including solve the\nforward-inverse problems of variable coefficient Burgers equation, variable\ncoefficient KdV equation, variable coefficient Sine-Gordon equation, and\nhigh-dimensional variable coefficient Kadomtsev-Petviashvili equation. The\nresults show that using R-gPINN method can greatly improve the accuracy of\npredict solution and predict variable coefficient in solving variable\ncoefficient equations.",
        "Non-stabilizerness, or magic, is a fundamental resource for quantum\ncomputation, enabling quantum algorithms to surpass classical capabilities.\nDespite its importance, characterizing magic remains challenging due to the\nintricate geometry of stabilizer polytopes and the difficulty of simulating\nnon-stabilizer states. In this work, we reveal an unexpected connection between\nmagic and Bell inequalities. Although maximally entangled stabilizer states can\nviolate Bell inequalities and magic is deeply tied to the algebraic structure\nof observables, we show that tailored Bell inequalities can act as witnesses of\nmagic. This result bridges two key quantum resources, uncovering a novel\nrelationship between the device-independent framework and resource-theoretic\nproperties of quantum computation.",
        "Recently, significant progress has been made in the study of black hole (BH)\nperturbations, both within the framework of general modified gravity theories\nand in complex environments. However, a well-established conclusion regarding\nthe boundary conditions of the perturbed fields remains elusive. In this paper,\nwe investigate the boundary conditions for a general perturbation at spatial\ninfinity and the event horizon of a black hole (BH) described by a generic\nmetric that is stationary, axisymmetric, asymptotically flat, and respects the\ncondition of circularity. Our analysis is independent of any specific BH model\nor the nature of the perturbed field. In particular, by extending the\nformulation introduced by Teukolsky and utilizing purely geometric principles,\nwe derive a universal expression for the boundary condition at the horizon.\nThis expression is elegantly formulated in terms of the physical quantities at\nthe event horizon, specifically the BH's surface gravity and angular velocity.\nThe results presented in this work may provide valuable insights for the\ncalculation of quasinormal modes and the gravitational waves generated by\nextreme-mass-ratio inspirals, extending beyond the standard Kerr case.",
        "This article deals with a generalization of the superadiabatic projectors\nmethod. In a general framework, the well-known superadiabatic projectors are\nconstructed and accurately described in the case of rank one, when a remarkable\nfactorization occurs. We apply these ideas to spectral theory and we explain\nhow our abstract results allow to recover or improve recent results about the\nsemiclassical magnetic Laplacian.",
        "We introduce two new families of bosonic quantum error correction (QEC) codes\nto address collective coherent and amplitude-damping errors, building upon our\nprevious multi-qubit QEC codes. These new bosonic codes enhance existing\nbinomial codes for oscillators and permutation-invariant codes for qubits by\nreducing the required excitations per input qubit from linear to sub-linear\ngrowth. The mappings from multi-qubit stabilizer codes to bosonic codes\nestablish a bridge between QEC code construction for qubits and oscillators,\noffering a unified approach to error correction across different quantum\nsystems.",
        "In the corona, plasma is accelerated to hundreds of kilometers per second,\nand heated to temperatures hundreds of times hotter than the Sun's surface,\nbefore it escapes to form the solar wind. Decades of space-based experiments\nhave shown that the energization process does not stop after it escapes.\nInstead, the solar wind continues to accelerate and it cools far more slowly\nthan a freely-expanding adiabatic gas. Recent work suggests that fast solar\nwind requires additional momentum beyond what can be provided by the observed\nthermal pressure gradients alone whereas it is sufficient for the slowest wind.\nThe additional acceleration for fast wind can be provided through an Alfv\\'en\nwave pressure gradient. Beyond this fast-slow categorization, however, a subset\nof slow solar wind exhibits high Alfv\\'enicity that suggest Alfv\\'en waves\ncould play a larger role in its acceleration compared to conventional slow wind\noutflows. Through a well-timed conjunction between Solar Orbiter and Parker\nSolar Probe, we trace the energetics of slow wind to compare with a neighboring\nAlfv\\'enic slow solar wind stream. An analysis that integrates remote and\nheliospheric properties and modeling of the two distinct solar wind streams\nfinds Alfv\\'enic slow solar wind behaves like fast wind, where a wave pressure\ngradient is required to reconcile its full acceleration, while non-Alfv\\'enic\nslow wind can be driven by its non-adiabatic electron and proton thermal\npressure gradients. Derived coronal conditions of the source region indicate\ngood model compatibility but extended coronal observations are required to\neffectively trace solar wind energetics below Parker's orbit.",
        "We present new anisotropy measurements in the near-infrared (NIR) for angular\nmultipoles $300<\\ell<10^5$ using imaging data at 1.1 $\\mu$m and 1.8 $\\mu$m from\nthe fourth flight of the Cosmic Infrared Background ExpeRiment (CIBER). Using\nimproved analysis methods and higher quality fourth flight data, we detect\nsurface brightness fluctuations on scales $\\ell<2000$ with CIBER auto-power\nspectra at $\\sim14\\sigma$ and 18$\\sigma$ for 1.1 and 1.8 $\\mu$m, respectively,\nand at $\\sim10\\sigma$ in cross-power spectra. The CIBER measurements pass\ninternal consistency tests and represent a $5-10\\times$ improvement in power\nspectrum sensitivity on several-arcminute scales relative to that of existing\nstudies. Through cross-correlations with tracers of diffuse galactic light\n(DGL), we determine that scattered DGL contributes $<10\\%$ to the observed\nfluctuation power at high confidence. On scales $\\theta > 5'$, the CIBER auto-\nand cross-power spectra exceed predictions for integrated galactic light (IGL)\nand integrated stellar light (ISL) by over an order of magnitude, and are\ninconsistent with our baseline IGL+ISL+DGL model at high significance. We\ncross-correlate two of the CIBER fields with 3.6 $\\mu$m and 4.5 $\\mu$m mosaics\nfrom the Spitzer Deep Wide-Field Survey and find similar evidence for\ndepartures from Poisson noise in Spitzer-internal power spectra and CIBER\n$\\times$ Spitzer cross-power spectra. A multi-wavelength analysis indicates\nthat the auto-power of the fluctuations at low-$\\ell$ is bluer than the Poisson\nnoise from IGL and ISL; however, for $1' <\\theta < 10'$, the cross-correlation\ncoefficient $r_{\\ell}$ of nearly all band combinations decreases with\nincreasing $\\theta$, disfavoring astrophysical explanations that invoke a\nsingle correlated sky component.",
        "Sparse phase retrieval aims to recover a $k$-sparse signal from $m$ phaseless\nobservations, raising the fundamental question of the minimum number of samples\nrequired for accurate recovery. As a classical non-convex optimization problem,\nsparse phase retrieval algorithms are typically composed of two stages:\ninitialization and refinement. Existing studies reveal that the sampling\ncomplexity under Gaussian measurements is largely determined by the\ninitialization stage.\n  In this paper, we identify commonalities among widely used initialization\nalgorithms and introduce key extensions to improve their performance. Building\non this analysis, we propose a novel algorithm, termed Generalized Exponential\nSpectral Pursuit (GESP). Theoretically, our results not only align with\nexisting conclusions but also demonstrate enhanced generalizability. In\nparticular, our theoretical findings coincide with prior results under certain\nconditions while surpassing them in others by providing more comprehensive\nguarantees. Furthermore, extensive simulation experiments validate the\npractical effectiveness and robustness of GESP, showcasing its superiority in\nrecovering sparse signals with reduced sampling requirements.",
        "Phylogenetic networks are an important way to represent evolutionary\nhistories that involve reticulations such as hybridization or horizontal gene\ntransfer, yet fundamental questions such as how many networks there are that\nsatisfy certain properties are very difficult. A new way to encode a large\nclass of networks, using expanding covers, may provide a way to approach such\nproblems. Expanding covers encode a large class of phylogenetic networks,\ncalled labellable networks. This class does not include all networks, but does\ninclude many familiar classes, including orchard, normal, tree-child and\ntree-sibling networks. As expanding covers are a combinatorial structure, it is\npossible that they can be used as a tool for counting such classes for a fixed\nnumber of leaves and reticulations, for which, in many cases, a closed formula\nhas not yet been found. More recently, a new class of networks was introduced,\ncalled spinal networks, which are analogous to caterpillar trees for\nphylogenetic trees and can be fully described using covers. In the present\narticle, we describe a method for counting networks that are both spinal and\nbelong to some more familiar class, with the hope that these form a base case\nfrom which to attack the more general classes.",
        "We review the characteristic structure of the two-dimensional adjoint Euler\nequations. We derive the compatibility and jump conditions along\ncharacteristics and show that the characteristic information can be used to\nobtain exact predictions for the adjoint variables in certain supersonic flows.",
        "It is well known that, in the study of the dynamical properties of nonlinear\nevolution system with nonlocal dispersals, the principal eigenvalue of\nlinearized system play an important role. However, due to lack of compactness,\nin order to obtain the existence of principal eigenvalue, certain additional\nconditions must be attached to the coefficients. In this paper, we approximate\nthe generalized principal eigenvalue of nonlocal dispersal cooperative and\nirreducible system, which admits the Collatz-Wielandt characterization, by\nconstructing the monotonic upper and lower control systems with principal\neigenvalues; and show that the generalized principal eigenvalue plays the same\nrole as the usual principal eigenvalue.",
        "The wavelength and state of polarization (SOP) are fundamental properties of\nan optical field which are essential for applications in optical\ncommunications, imaging and other fields. However, it is challenging for\nexisting spectrometers and polarimeters to measure these parameters\nsimultaneously, resulting in reduced spatial and temporal efficiency. To\novercome this limitation, we propose and demonstrate a compact multimode fiber\n(MMF)-based high-dimensional light analyzer capable of simultaneously\nperforming high-precision measurements of both wavelength and SOP. Core-offset\nlaunching is introduced in the MMF to reshuffle the mode coupling. A neural\nnetwork named WP-Net has been designed dedicated to wavelength and SOP\nsynchronization measurements. Physics-informed loss function based on optical\nprior knowledge is used to optimize the learning process. These advancements\nhave enhanced the sensitivity, achieving a wavelength resolution of 0.045 pm\nand an SOP resolution of 0.0088.",
        "The white dwarf mass distribution has been studied primarily at two extremes:\nobjects that presumably evolved as single stars and members of close binaries\nthat likely underwent substantial interaction. This work considers the\nintermediate separation regime of ~1 au and demonstrates how binary interaction\naffects white dwarf masses. The binary mass ratio distribution is utilized for\nthis purpose. Modeled as a truncated Pareto profile, this distribution provides\ninsights into the populations' properties and evolutionary history. When\napplied to homogeneous samples of binaries with giant primaries of similar age,\nthe distribution's shape constrains the fraction of white dwarf companions, the\nwhite dwarf mass distribution, and the properties of their progenitors. As a\ntest case, this method is applied to a small spectroscopic sample of binaries\nin open clusters with red giant primaries and orbital periods between 0.5 and\n20 years. The analysis reveals that white dwarfs in these systems are ~20% less\nmassive than their isolated counterparts, with a typical mass of ~0.55 Msun.\nTheir progenitors likely lost 80-85% of their mass, with binary interactions\nenhancing mass loss by an additional ~0.2 Msun. These findings highlight the\nutility of this approach for studying binary evolution and improving population\nmodels, particularly with future datasets from Gaia and other large-scale\nsurveys.",
        "In this paper, we study a secure integrated sensing and communication (ISAC)\nsystem employing a full-duplex base station with sensing capabilities against a\nmobile proactive adversarial target$\\unicode{x2014}$a malicious unmanned aerial\nvehicle (M-UAV). We develop a game-theoretic model to enhance communication\nsecurity, radar sensing accuracy, and power efficiency. The interaction between\nthe legitimate network and the mobile adversary is formulated as a\nnon-cooperative Stackelberg game (NSG), where the M-UAV acts as the leader and\nstrategically adjusts its trajectory to improve its eavesdropping ability while\nconserving power and avoiding obstacles. In response, the legitimate network,\nacting as the follower, dynamically allocates resources to minimize network\npower usage while ensuring required secrecy rates and sensing performance. To\naddress this challenging problem, we propose a low-complexity successive convex\napproximation (SCA) method for network resource optimization combined with a\ndeep reinforcement learning (DRL) algorithm for adaptive M-UAV trajectory\nplanning through sequential interactions and learning. Simulation results\ndemonstrate the efficacy of the proposed method in addressing security\nchallenges of dynamic ISAC systems in 6G, i.e., achieving a Stackelberg\nequilibrium with robust performance while mitigating the adversary's ability to\nintercept network signals.",
        "Tip-enhanced Raman spectroscopy (TERS) enables nanoscale chemical mapping of\nbiological structures, providing high-resolution, high-signal-to-noise ratio\nimaging into molecular distribution and interactions beyond the capabilities of\nconventional Raman imaging. However, challenges such as the deformation of\nfragile biological cells and the complexity of signal interpretation would\nincrease the difficulty in investigating biological samples with TERS. Here, we\ndemonstrate using TERS to investigate the cell wall heterogeneity of\nAspergillus fumigatus spores. Using TERS imaging and spectral analysis, we map\nthe chemical components including melanin within the fungal cell wall. The\nresults reveal distinct spectral features associated with polysaccharides,\nlipids, and proteins. Furthermore, by comparing the wild-type and albino mutant\nspores, we illuminate the biochemical characteristics of Dihydroxynaphthalene\nmelanin (DHN-melanin) in the fungal cell wall.",
        "Accurate modelling of black hole binaries is critical to achieve the science\ngoals of gravitational-wave detectors. Modelling such configurations relies\nstrongly on calibration to numerical-relativity (NR) simulations. Binaries on\nquasi-circular orbits have been widely explored in NR, however, coverage of the\nbroader 9-dimensional parameter space, including orbital eccentricity, remains\nsparse. This article develops a new procedure to control orbital eccentricity\nof binary black hole simulations that enables choosing initial data parameters\nwith precise control over eccentricity and mean anomaly of the subsequent\nevolution, as well as the coalescence time. We then calculate several sequences\nof NR simulations that nearly uniformly cover the 2-dimensional\neccentricity--mean anomaly space for equal mass, non-spinning binary black\nholes. We demonstrate that, for fixed eccentricity, many quantities related to\nthe merger dynamics of binary black holes show an oscillatory dependence on\nmean anomaly. The amplitude of these oscillations scales nearly linearly with\nthe eccentricity of the system. We find that for the eccentricities explored in\nthis work, deviations in various quantities such as the merger amplitude and\npeak luminosity can approach $\\sim5\\%$ of their quasi-circular value. We use\nour findings to explain eccentric phenomena reported in other studies. We also\nshow that methods for estimating the remnant mass employed in the\neffective-one-body approach exhibit similar deviations, roughly matching the\namplitude of the oscillations we find in NR simulations. This work is an\nimportant step towards a complete description of eccentric binary black hole\nmergers, and demonstrates the importance of considering the entire\n2-dimensional parameter subspace related to eccentricity.",
        "Although quasiparticles in flat bands have zero group velocity, they can\ndisplay an anomalous velocity due to the quantum geometry. We address the\nthermal and thermoelectric transport in flat bands in the clean limit with a\nsmall amount of broadening due to inelastic scattering. We derive general Kubo\nformulas for flat bands in the DC limit up to linear order in the broadening\nand extract expressions for the thermal conductivity, the Seebeck and Nernst\ncoefficients. We show that the Seebeck coefficient for flat Chern bands is\ntopological up to second order corrections in the broadening. We identify\nthermal and thermoelectric transport signatures for two generic flat Chern\nbands and also for the generalized flattened Lieb model, which describes a\nfamily of three equally spaced flat Chern bands where the middle one is\ntopologically trivial. Finally, we address the saturation of the quantum metric\nlower bound for a general family of Hamiltonians with an arbitrary number of\nflat Chern bands corresponding to SU(2) coherent states. We find that only the\nextremal bands in this class of Hamiltonians saturate the bound, provided that\nthe momentum dependence of their Hamiltonians is described by a meromorphic\nfunction.",
        "Stacking and twisting van der Waals materials provide a powerful tool to\ndesign quantum matter and engineer electron correlation. For instance,\nmonolayers of 1T- and 1H-TaS2 are Mott insulating and metallic (also\nsuperconducting), respectively, and thus, the T\/H bilayer systems have been\nextensively investigated in the context of heavy fermions and unconventional\nsuperconductivity, which are expected phases from localized spins (1T)\ncoexisting with itinerant electrons (1H). However, recent studies revealed that\nsignificant charge transfer from the 1T to 1H layers removes the 1T Mottness\nand renders the above scenario elusive. In this work, we propose a T\/T\/H\ntrilayer heterostructure by combining a T\/T bilayer -- which is a band\ninsulator with flat dispersion -- with a 1H layer. After charge redistribution,\nthis trilayer heterostructure shows localized spins in the Mott flat band of\nthe T\/T bilayer and weak spin polarization in the metallic H layer. We argue\nthat by varying the stacking configurations of the T\/T bilayer in the T\/T\/H\ntrilayer, a crossover from a doped Mott insulator to a Kondo insulator can be\nachieved. The T\/T\/H trilayer provides therefore a rich novel heterostructure\nplatform to study strong correlation phenomena and unconventional\nsuperconductivity.",
        "We employ the novel theory of heterogeneous extreme value statistics to\naccurately estimate the ultimate world records for the 100-m running race, for\nmen and for women. For this aim we collected data from 1991 through 2023 from\nthousands of top athletes, using multiple fast times per athlete. We consider\nthe left endpoint of the probability distribution of the running times of a top\nathlete and define the ultimate world record as the minimum, over all top\nathletes, of all these endpoints. For men we estimate the ultimate world record\nto be 9.56 seconds. More prudently, employing this heterogeneous extreme value\ntheory we construct an accurate asymptotic 95% lower confidence bound on the\nultimate world record of 9.49 seconds, still quite close to the present world\nrecord of 9.58. For the women's 100-meter dash our point estimate of the\nultimate world record is 10.34 seconds, somewhat lower than the world record of\n10.49. The more prudent 95% lower confidence bound on the women's ultimate\nworld record is 10.20.",
        "We study the flexible piecewise exponential model in a high dimensional\nsetting where the number of covariates $p$ grows proportionally to the number\nof observations $n$ and under the hypothesis of random uncorrelated Gaussian\ndesigns. We prove rigorously that the optimal ridge penalized log-likelihood of\nthe model converges in probability to the saddle point of a surrogate objective\nfunction. The technique of proof is the Convex Gaussian Min-Max theorem of\nThrampoulidis, Oymak and Hassibi. An important consequence of this result, is\nthat we can study the impact of the ridge regularization on the estimates of\nthe parameter of the model and the prediction error as a function of the ratio\n$p\/n > 0$. Furthermore, these results represent a first step toward rigorously\nproving the (conjectured) correctness of several results obtained with the\nheuristic replica method for the Cox semi-parametric model.",
        "The maximum mean discrepancy and Wasserstein distance are popular distance\nmeasures between distributions and play important roles in many machine\nlearning problems such as metric learning, generative modeling, domain\nadaption, and clustering. However, since they are functions of pair-wise\ndistances between data points in two distributions, they do not exploit the\npotential manifold properties of data such as smoothness and hence are not\neffective in measuring the dissimilarity between the two distributions in the\nform of manifolds. In this paper, different from existing measures, we propose\na novel distance called Mutual Regression Distance (MRD) induced by a\nconstrained mutual regression problem, which can exploit the manifold property\nof data. We prove that MRD is a pseudometric that satisfies almost all the\naxioms of a metric. Since the optimization of the original MRD is costly, we\nprovide a tight MRD and a simplified MRD, based on which a heuristic algorithm\nis established. We also provide kernel variants of MRDs that are more effective\nin handling nonlinear data. Our MRDs especially the simplified MRDs have much\nlower computational complexity than the Wasserstein distance. We provide\ntheoretical guarantees, such as robustness, for MRDs. Finally, we apply MRDs to\ndistribution clustering, generative models, and domain adaptation. The\nnumerical results demonstrate the effectiveness and superiority of MRDs\ncompared to the baselines.",
        "Dark Field X-ray Microscopy (DFXM) has advanced 3D non-destructive,\nhigh-resolution imaging of strain and orientation in crystalline materials,\nenabling the study of embedded structures in bulk. However, the\nphoton-intensive nature of monochromatic DFXM limits its applicability to\nhighly deformed or weakly crystalline structures and constrains time-resolved\nstudies in industrially relevant materials. We present pink-beam DFXM (\\pDFXM)\nat the ID03 beamline of ESRF, achieving a 27-fold increase in diffracted\nintensity while maintaining 100 nm spatial resolution. We validate \\pDFXM{} by\nimaging a partially recrystallized aluminum grain, confirming sufficient\nangular resolution for microstructure mapping. The increased flux significantly\nenhances the diffracted signal, enabling the resolution of subgrain structures.\nAdditionally, we image a highly deformed ferritic iron grain, previously\ninaccessible in monochromatic mode without focusing optics. Beyond static\nimaging, \\pDFXM{} enables real-time tracking of grain growth during annealing,\nachieving hundred-millisecond temporal resolution. By combining high photon\nflux with non-destructive, high-resolution 3D mapping, \\pDFXM{} expands\ndiffraction-contrast imaging to poorly diffracting crystals, unlocking new\nopportunities for studying grain growth, fatigue, and corrosion in bulk\nmaterials.",
        "We show that $4$-dimensional Robertson-Walker spacetimes can be constructed\nfor which all of the beta functions vanish to leading order, yielding\nconsistent string theory without extra dimensions. We find that there is a\nunique static solution, which we refer to as an anti-Einstein static universe.\nThe associated stress energy tensor can be interpreted as a perfect fluid with\na negative energy density. Interestingly, a fluid with a negative energy\ndensity was proposed in [2] as an ad hoc hypothesis to serve as a possible\nexplanation for dark energy and dark matter. Here, such a fluid spontaneously\nappears by trying to fit string theory into only $4$ dimensions. We can look at\nperturbations away from the anti-Einstein static universe. This has to be done\nnumerically and we only do it for a few choices of initial conditions. We find\nthat these solutions are very sensitive to the initial conditions and yield a\nvariety of behaviors. We hope that this behavior is rich enough to match\ncosmological observations by appropriately choosing the initial conditions. One\nof these solutions that we found is particularly interesting. It has a hubble\nparameter which is negative in the past (meaning a contracting universe), then\na point at which the hubble parameter changes sign, so the universe starts\nexpanding which could be identified with a big bang, after which the hubble\nparameter continues growing. Throughout all of this time, the acceleration is\npositive since the Hubble parameter is increasing. Finally, we comment that our\nresult does not contradict [1] as it seems that the authors overlooked the\npossibility of a purely complex axion field which allows for the\nRobertson-Walker metrics to make a contribution $c_{RW}\\geq 4$ to the central\ncharge. Thus, we can interpret dark matter and dark energy as being parts of a\nmechanism needed to keep $4$-dimensional string theory consistent.",
        "Quasars behind the Galactic plane (GPQs) are excellent tracers to probe the\nchemistry and kinematics of the interstellar\/intergalactic medium (ISM\/IGM) of\nthe Milky Way along sight lines via absorption line spectroscopy. Moreover, the\nquasars located at low Galactic latitudes will fill the gap in the spatial\ndistribution of known quasars near the Galactic plane, and can be used to\nconstruct an astrometric reference frame for accurate measurements of proper\nmotions (PMs) of stars, and substructures of the Milky Way. We started a survey\nof background quasars in the low Galactic latitude region since the LAMOST\nphase II survey in 2017. Quasar candidates have been selected from the optical\nand infrared photometric data of Pan-STARRS1 and WISE surveys based on their\nvariability and color properties. In this paper, we present a sample of 1982\nspectroscopically confirmed GPQs with |b| <= 20 degree based on LAMOST Data\nRelease 10 (DR10). Among them, 1338 are newly discovered. Most GPQs are located\naround 240<l<90 degree, and the spatial distributions are non-uniform. These\nGPQs have a magnitude distribution with a peak at i-mag 19.0, and mostly around\n18.0-19.5mag. The peak of redshift distributions is around ~1.5, and most GPQs\nhave redshifts between 0.3 and 2.5. Our finding demonstrates the potential\ndiscovery space for the GPQs from the spectroscopic surveys and the promising\napplications for future research.",
        "Domain walls formed during a phase transition in a simple field theory model\nwith $\\mathbb{Z}_2$ symmetry in a periodic box have been demonstrated to\nannihilate as fast as causality allows and their area density scales $\\propto\nt^{-1}$. We have performed numerical simulations of the dynamics of domain\nwalls in the Two-Higgs Doublet Model (2HDM) where the potential has\n$\\mathbb{Z}_2$ symmetry in two spatial dimensions. We observed significant\ndifferences with the standard case. Although the extreme long-time limit is the\nsame for the $\\approx 10^{5}$ sets of random initial configurations analysed,\nthe percolation process is much slower due to the formation of long-lived\nloops. We suggest that this is due to the build up of superconducting currents\non the walls which could lead ultimately to stationary configurations known as\nKinky Vortons. We discuss the relevance of these findings for the production of\nVortons in three spatial dimensions.",
        "The $d$-plane transform maps functions to their integrals over $d$-planes in\n$\\mathbb{R}^n$. We study the following question: if a function vanishes in a\nbounded open set, and its $d$-plane transform vanishes on all $d$-planes\nintersecting the same set, does the function vanish identically? For $d$ an\neven integer, we show by producing an explicit counterexample, that neither the\n$d$-plane transform, nor its normal operator has this property. On the other\nhand, an even stronger property holds when $d$ is odd, where the normal\noperator vanishing to infinite order at a point, along with the function\nvanishing on an open set containing that point, is sufficient to conclude that\nthe function vanishes identically.",
        "The search for the compact limit of multi-quark states is a challenging\nissue. Within the framework of the MIT bag model, we propose an effective limit\nbag radius of $R_{c} = 5.615 \\, \\text{GeV}^{-1}$(or $1.11 \\, \\text{fm}$) for\nbound states . When the bag radius of a hadron falls below this value, the bag\nbinding energy satisfies $E_{B} < 0$, indicating that the system has a compact\nintention. We consider various combinations of different numbers and ratios of\nheavy and light quarks, indicating that the bag radius of hadrons depending on\nthe number of quarks is suppressed by the presence of heavy quarks. Focusing on\nfive-quark combinations, we find that the average bag radius of $nncc\\bar{c}$\nis below the threshold $R_{c}$. We take into account color-magnetic\ninteractions and calculate the mass, magnetic moment, binding energy, and\nrelative strong decay width for the $nnQQ\\bar{Q}$ system. We show that the\nbinding energies of most states in the flavor combination $nncc\\bar{c}$ are\napproximately $-20 \\, \\text{MeV}$, whereas states involving bottom quarks have\nbinding energies around $-120 \\, \\text{MeV}$, with some decay widths suppressed\nby the decay constant. Additionally, the $nQQQ\\bar{Q}$ system exhibits even\ndeeper binding. Our results support the compact intention of $nnQQ\\bar{Q}$ and\nsuggest that the $nQQQ\\bar{Q}$ configuration demonstrates even stronger\ncompactness."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Screening for lung cancer: 2023 guideline update from the American Cancer Society",
    "start_abstract":"Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies",
        "The Electric Location-Routing Problem: Improved Formulations and Effects\n  of Nonlinear Charging",
        "DiffVSR: Revealing an Effective Recipe for Taming Robust Video\n  Super-Resolution Against Complex Degradations",
        "Entanglement dynamics of many-body quantum states with evolving system\n  conditions",
        "An Accurate Computational Approach for Partial Likelihood Using\n  Poisson-Binomial Distributions",
        "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel\n  Tool Invocation",
        "Two-sided Remotely Almost Periodic Solutions of Ordinary Differential\n  Equations in Banach Spaces",
        "Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising",
        "Comparison of Bending-Energy Discretization Methods for Anisotropic\n  Meshes in Morphogenetic Simulations",
        "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
        "Counting for rigidity under projective transformations in the plane",
        "Green points in the reals",
        "Fault-tolerance of [[6, 1, 3]] non-CSS code family generated using\n  measurements on graph states",
        "Design of resilient structures by randomization and bistability",
        "Getting SMARTER for Motion Planning in Autonomous Driving Systems"
      ],
      "abstract":[
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones.",
        "Electric Location-Routing models (ELRP) can contribute to the effective\nplanning of electric vehicles (EVs) fleets and charging infrastructure within\nEV logistic networks because it simultaneously combines routing and location\ndecisions to find optimal solutions to the network design. This study\nintroduces ELRP models that incorporate nonlinear charging process, multiple\ncharging station types and develop new improved formulations to the problem.\nExisting ELRP models commonly assume a linear charging process and employ a\nnode-based formulation for tracking EV energy and time consumption. In\ncontrast, we propose novel formulations offering alternative approaches for\nmodeling EV energy, time consumption, and nonlinear charging. Through extensive\ncomputational experiments, our analysis demonstrates the effectiveness of the\nnew formulations, reducing the average gap from 29.1% to 11.9%, yielding\nimproved solutions for 28 out of 74 instances compared to the node-based\nformulation. Moreover, our findings provide valuable insights into the\nstrategic implications of nonlinear charging in ELRP decision-making, offering\nnew perspectives for planning charging infrastructure in EV logistic networks.",
        "Diffusion models have demonstrated exceptional capabilities in image\nrestoration, yet their application to video super-resolution (VSR) faces\nsignificant challenges in balancing fidelity with temporal consistency. Our\nevaluation reveals a critical gap: existing approaches consistently fail on\nseverely degraded videos--precisely where diffusion models' generative\ncapabilities are most needed. We identify that existing diffusion-based VSR\nmethods struggle primarily because they face an overwhelming learning burden:\nsimultaneously modeling complex degradation distributions, content\nrepresentations, and temporal relationships with limited high-quality training\ndata. To address this fundamental challenge, we present DiffVSR, featuring a\nProgressive Learning Strategy (PLS) that systematically decomposes this\nlearning burden through staged training, enabling superior performance on\ncomplex degradations. Our framework additionally incorporates an Interweaved\nLatent Transition (ILT) technique that maintains competitive temporal\nconsistency without additional training overhead. Experiments demonstrate that\nour approach excels in scenarios where competing methods struggle, particularly\non severely degraded videos. Our work reveals that addressing the learning\nstrategy, rather than focusing solely on architectural complexity, is the\ncritical path toward robust real-world video super-resolution with diffusion\nmodels.",
        "The entanglement analysis of a pure state of a many-body quantum system\nrequires a prior information about its density matrix\/ state matrix, obtained\nin principle by solving the Hamiltonian matrix. The missing information due to\ncomplexity of the many-body interactions however renders it necessary to\nconsider an ensemble of Hamiltonians and thereby an ensemble of pure states.\nThis in turn leaves a statistical description of the entanglement measures as\nthe only option.\n  We consider physical Hamiltonians that can be modelled by a multiparametric\nGaussian ensembles, theoretically derive the state ensembles for its\neigenstates and analyze the effect of varying system conditions on its\nentanglement statistics. Our approach leads to a single parametric based common\nmathematical formulation for the evolution of the statistics of different state\nensembles. The parameter turns out to be a single functional of the system\nparameters and thereby reveals a deep web of connection underlying different\nquantum states.",
        "In a Cox model, the partial likelihood, as the product of a series of\nconditional probabilities, is used to estimate the regression coefficients. In\npractice, those conditional probabilities are approximated by risk score ratios\nbased on a continuous time model, and thus result in parameter estimates from\nonly an approximate partial likelihood. Through a revisit to the original\npartial likelihood idea, an accurate partial likelihood computing method for\nthe Cox model is proposed, which calculates the exact conditional probability\nusing the Poisson-binomial distribution. New estimating and inference\nprocedures are developed, and theoretical results are established for the\nproposed computational procedure. Although ties are common in real studies,\ncurrent theories for the Cox model mostly do not consider cases for tied data.\nIn contrast, the new approach includes the theory for grouped data, which\nallows ties, and also includes the theory for continuous data without ties,\nproviding a unified framework for computing partial likelihood for data with or\nwithout ties. Numerical results show that the proposed method outperforms\ncurrent methods in reducing bias and mean squared error, while achieving\nimproved confidence interval coverage rates, especially when there are many\nties or when the variability in risk scores is large. Comparisons between\nmethods in real applications have been made.",
        "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT\/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess\/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https:\/\/corn0205.github.io\/",
        "The aim of this paper is studying the two-sided remotely almost periodic\nsolutions of ordinary differential equations in Banach spaces of the form\n$x'=A(t)x+f(t)+F(t,x)$ with two-sided remotely almost periodic coefficients if\nthe linear equation $x'=A(t)x$ satisfies the condition of exponential\ntrichotomy and nonlinearity $F$ is \"small\".",
        "Recent advances in diffusion models have greatly improved text-driven video\ngeneration. However, training models for long video generation demands\nsignificant computational power and extensive data, leading most video\ndiffusion models to be limited to a small number of frames. Existing\ntraining-free methods that attempt to generate long videos using pre-trained\nshort video diffusion models often struggle with issues such as insufficient\nmotion dynamics and degraded video fidelity. In this paper, we present\nBrick-Diffusion, a novel, training-free approach capable of generating long\nvideos of arbitrary length. Our method introduces a brick-to-wall denoising\nstrategy, where the latent is denoised in segments, with a stride applied in\nsubsequent iterations. This process mimics the construction of a staggered\nbrick wall, where each brick represents a denoised segment, enabling\ncommunication between frames and improving overall video quality. Through\nquantitative and qualitative evaluations, we demonstrate that Brick-Diffusion\noutperforms existing baseline methods in generating high-fidelity videos.",
        "Accurately modeling bending energy in morphogenetic simulations is crucial,\nespecially when dealing with anisotropic meshes where remeshing is infeasible\ndue to the biologically meaningful entities of vertex positions (e.g., cells).\nThis study addresses the underexplored question of which bending-energy\ndiscretization methods are most accurate and suitable for such simulations.\n  The evaluation consists of two stages: First, the accuracy of each method is\ntested by comparing predicted bending energy and force against theoretical\nvalues for two benchmark cases--a wrinkled planar sheet and a smooth spherical\nsheet. Second, we simulate the formation of wrinkles in a planar sheet caused\nby anisotropic cell division, analyzing the resulting wavenumber patterns for\ntwo division orientations: uniaxial and random.\n  The results highlight that the choice of the optimal discretization method\ndepends on the application. For simulations requiring precise quantitative\npredictions, the Hamann model demonstrates superior accuracy. Conversely, for\nsimulations where qualitative trends in morphology are of primary interest, the\nJ\\\"ulicher model provides a computationally efficient alternative. These\nfindings provide guidance for selecting appropriate bending-energy\ndiscretization methods in morphogenetic simulations, ultimately leading to more\naccurate and efficient modeling of complex biological forms.",
        "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
        "Let $P$ be a set of points and $L$ a set of lines in the (extended) Euclidean\nplane, and $I \\subseteq P\\times L$, where $i =(p,l) \\in I$ means that point $p$\nand line $l$ are incident. The incidences can be interpreted as quadratic\nconstraints on the homogeneous coordinates of the points and lines. We study\nthe space of incidence preserving motions of the given incidence structure by\nlinearizing the system of quadratic equations. The Jacobian of the quadratic\nsystem, our projective rigidity matrix, leads to the notion of\nindependence\/dependence of incidences. Column dependencies correspond to\ninfinitesimal motions. Row dependencies or self-stresses allow for new\ninterpretations of classical geometric incidence theorems. We show that\nself-stresses are characterized by a 3-fold balance. As expected, infinitesimal\n(first order) projective rigidity as well as second order projective rigidity\nimply projective rigidity but not conversely. Several open problems and\npossible generalizations are indicated.",
        "We construct an expansion of a real closed field by a multiplicative subgroup\nadapting Poizat's theory of green points. Its theory is strongly dependent, and\nevery open set definable in a model of this theory is semialgebraic. We prove\nthat the real field with a dense family of logarithmic spirals, proposed by\nZilber, satisfies our theory.",
        "We construct and analyze the fault tolerance of $[[6,1,3]]$ non-CSS quantum\nerror correcting code under the anisotropic and depolarizing noise models. This\nrate-optimized code achieves fault-tolerance using a single ancilla qubit for\nsyndrome measurement under anisotropic noise conditions. This method was called\nfault-tolerance using bare ancilla by Brown \\emph{et al.} We give explicit\nconstruction of the code using measurements on non-planar graph states. We also\nargue that using our approach, we can construct a family of such fault-tolerant\ncodes. This method fills a notable gap in constructing fault-tolerant non-CSS\ncode families.",
        "This paper examines various ways of improving the impact resilience of\nprotective structures. Such structures' purpose is to dissipate an impact's\nenergy while avoiding cracking and failure. We have tested the reaction of\nplane elastic-brittle lattices to an impulse. Four topologies are compared:\nperiodic triangular, square, and hexagonal topologies, and aperiodic Penrose\ntopology. Then, structures with random variations of the links' stiffness, node\npositions, and random holes are compared. Combinations of these random factors\nare also considered, as well as the resilience of bistable elastic-brittle\nlattices with sacrificial links. Several parameters are introduced to measure\nthe structural resilience of the compared designs: (i) the amount of dissipated\nimpact energy, (ii) the size of broken clusters of links, and (iii) the spread\nof damage. The results suggest new routes for rationally designing protective\nstructures using nonperiodic topology, bistability, and structural randomness.\nIn particular, we find that some quantities of interest can be maximized by\ntuning the randomized design appropriately -- for example, randomly removing\n8\\% of links maximizes energy dissipation. We also find that randomization of\nbistable lattices can offer superior energy dissipation while reducing the\nconnectivity between broken clusters of links.",
        "Motion planning is a fundamental problem in autonomous driving and perhaps\nthe most challenging to comprehensively evaluate because of the associated\nrisks and expenses of real-world deployment. Therefore, simulations play an\nimportant role in efficient development of planning algorithms. To be\neffective, simulations must be accurate and realistic, both in terms of\ndynamics and behavior modeling, and also highly customizable in order to\naccommodate a broad spectrum of research frameworks. In this paper, we\nintroduce SMARTS 2.0, the second generation of our motion planning simulator\nwhich, in addition to being highly optimized for large-scale simulation,\nprovides many new features, such as realistic map integration,\nvehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and\na broad variety of sensor models.\n  Moreover, we present a novel benchmark suite for evaluating planning\nalgorithms in various highly challenging scenarios, including interactive\ndriving, such as turning at intersections, and adaptive driving, in which the\ntask is to closely follow a lead vehicle without any explicit knowledge of its\nintention. Each scenario is characterized by a variety of traffic patterns and\nroad structures. We further propose a series of common and task-specific\nmetrics to effectively evaluate the performance of the planning algorithms. At\nthe end, we evaluate common motion planning algorithms using the proposed\nbenchmark and highlight the challenges the proposed scenarios impose. The new\nSMARTS 2.0 features and the benchmark are publicly available at\ngithub.com\/huawei-noah\/SMARTS."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects",
    "start_abstract":"Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies",
        "The Electric Location-Routing Problem: Improved Formulations and Effects\n  of Nonlinear Charging",
        "DiffVSR: Revealing an Effective Recipe for Taming Robust Video\n  Super-Resolution Against Complex Degradations",
        "Entanglement dynamics of many-body quantum states with evolving system\n  conditions",
        "An Accurate Computational Approach for Partial Likelihood Using\n  Poisson-Binomial Distributions",
        "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel\n  Tool Invocation",
        "Two-sided Remotely Almost Periodic Solutions of Ordinary Differential\n  Equations in Banach Spaces",
        "Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising",
        "Comparison of Bending-Energy Discretization Methods for Anisotropic\n  Meshes in Morphogenetic Simulations",
        "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
        "Counting for rigidity under projective transformations in the plane",
        "Green points in the reals",
        "Fault-tolerance of [[6, 1, 3]] non-CSS code family generated using\n  measurements on graph states",
        "Design of resilient structures by randomization and bistability",
        "Getting SMARTER for Motion Planning in Autonomous Driving Systems"
      ],
      "abstract":[
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones.",
        "Electric Location-Routing models (ELRP) can contribute to the effective\nplanning of electric vehicles (EVs) fleets and charging infrastructure within\nEV logistic networks because it simultaneously combines routing and location\ndecisions to find optimal solutions to the network design. This study\nintroduces ELRP models that incorporate nonlinear charging process, multiple\ncharging station types and develop new improved formulations to the problem.\nExisting ELRP models commonly assume a linear charging process and employ a\nnode-based formulation for tracking EV energy and time consumption. In\ncontrast, we propose novel formulations offering alternative approaches for\nmodeling EV energy, time consumption, and nonlinear charging. Through extensive\ncomputational experiments, our analysis demonstrates the effectiveness of the\nnew formulations, reducing the average gap from 29.1% to 11.9%, yielding\nimproved solutions for 28 out of 74 instances compared to the node-based\nformulation. Moreover, our findings provide valuable insights into the\nstrategic implications of nonlinear charging in ELRP decision-making, offering\nnew perspectives for planning charging infrastructure in EV logistic networks.",
        "Diffusion models have demonstrated exceptional capabilities in image\nrestoration, yet their application to video super-resolution (VSR) faces\nsignificant challenges in balancing fidelity with temporal consistency. Our\nevaluation reveals a critical gap: existing approaches consistently fail on\nseverely degraded videos--precisely where diffusion models' generative\ncapabilities are most needed. We identify that existing diffusion-based VSR\nmethods struggle primarily because they face an overwhelming learning burden:\nsimultaneously modeling complex degradation distributions, content\nrepresentations, and temporal relationships with limited high-quality training\ndata. To address this fundamental challenge, we present DiffVSR, featuring a\nProgressive Learning Strategy (PLS) that systematically decomposes this\nlearning burden through staged training, enabling superior performance on\ncomplex degradations. Our framework additionally incorporates an Interweaved\nLatent Transition (ILT) technique that maintains competitive temporal\nconsistency without additional training overhead. Experiments demonstrate that\nour approach excels in scenarios where competing methods struggle, particularly\non severely degraded videos. Our work reveals that addressing the learning\nstrategy, rather than focusing solely on architectural complexity, is the\ncritical path toward robust real-world video super-resolution with diffusion\nmodels.",
        "The entanglement analysis of a pure state of a many-body quantum system\nrequires a prior information about its density matrix\/ state matrix, obtained\nin principle by solving the Hamiltonian matrix. The missing information due to\ncomplexity of the many-body interactions however renders it necessary to\nconsider an ensemble of Hamiltonians and thereby an ensemble of pure states.\nThis in turn leaves a statistical description of the entanglement measures as\nthe only option.\n  We consider physical Hamiltonians that can be modelled by a multiparametric\nGaussian ensembles, theoretically derive the state ensembles for its\neigenstates and analyze the effect of varying system conditions on its\nentanglement statistics. Our approach leads to a single parametric based common\nmathematical formulation for the evolution of the statistics of different state\nensembles. The parameter turns out to be a single functional of the system\nparameters and thereby reveals a deep web of connection underlying different\nquantum states.",
        "In a Cox model, the partial likelihood, as the product of a series of\nconditional probabilities, is used to estimate the regression coefficients. In\npractice, those conditional probabilities are approximated by risk score ratios\nbased on a continuous time model, and thus result in parameter estimates from\nonly an approximate partial likelihood. Through a revisit to the original\npartial likelihood idea, an accurate partial likelihood computing method for\nthe Cox model is proposed, which calculates the exact conditional probability\nusing the Poisson-binomial distribution. New estimating and inference\nprocedures are developed, and theoretical results are established for the\nproposed computational procedure. Although ties are common in real studies,\ncurrent theories for the Cox model mostly do not consider cases for tied data.\nIn contrast, the new approach includes the theory for grouped data, which\nallows ties, and also includes the theory for continuous data without ties,\nproviding a unified framework for computing partial likelihood for data with or\nwithout ties. Numerical results show that the proposed method outperforms\ncurrent methods in reducing bias and mean squared error, while achieving\nimproved confidence interval coverage rates, especially when there are many\nties or when the variability in risk scores is large. Comparisons between\nmethods in real applications have been made.",
        "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT\/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess\/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https:\/\/corn0205.github.io\/",
        "The aim of this paper is studying the two-sided remotely almost periodic\nsolutions of ordinary differential equations in Banach spaces of the form\n$x'=A(t)x+f(t)+F(t,x)$ with two-sided remotely almost periodic coefficients if\nthe linear equation $x'=A(t)x$ satisfies the condition of exponential\ntrichotomy and nonlinearity $F$ is \"small\".",
        "Recent advances in diffusion models have greatly improved text-driven video\ngeneration. However, training models for long video generation demands\nsignificant computational power and extensive data, leading most video\ndiffusion models to be limited to a small number of frames. Existing\ntraining-free methods that attempt to generate long videos using pre-trained\nshort video diffusion models often struggle with issues such as insufficient\nmotion dynamics and degraded video fidelity. In this paper, we present\nBrick-Diffusion, a novel, training-free approach capable of generating long\nvideos of arbitrary length. Our method introduces a brick-to-wall denoising\nstrategy, where the latent is denoised in segments, with a stride applied in\nsubsequent iterations. This process mimics the construction of a staggered\nbrick wall, where each brick represents a denoised segment, enabling\ncommunication between frames and improving overall video quality. Through\nquantitative and qualitative evaluations, we demonstrate that Brick-Diffusion\noutperforms existing baseline methods in generating high-fidelity videos.",
        "Accurately modeling bending energy in morphogenetic simulations is crucial,\nespecially when dealing with anisotropic meshes where remeshing is infeasible\ndue to the biologically meaningful entities of vertex positions (e.g., cells).\nThis study addresses the underexplored question of which bending-energy\ndiscretization methods are most accurate and suitable for such simulations.\n  The evaluation consists of two stages: First, the accuracy of each method is\ntested by comparing predicted bending energy and force against theoretical\nvalues for two benchmark cases--a wrinkled planar sheet and a smooth spherical\nsheet. Second, we simulate the formation of wrinkles in a planar sheet caused\nby anisotropic cell division, analyzing the resulting wavenumber patterns for\ntwo division orientations: uniaxial and random.\n  The results highlight that the choice of the optimal discretization method\ndepends on the application. For simulations requiring precise quantitative\npredictions, the Hamann model demonstrates superior accuracy. Conversely, for\nsimulations where qualitative trends in morphology are of primary interest, the\nJ\\\"ulicher model provides a computationally efficient alternative. These\nfindings provide guidance for selecting appropriate bending-energy\ndiscretization methods in morphogenetic simulations, ultimately leading to more\naccurate and efficient modeling of complex biological forms.",
        "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
        "Let $P$ be a set of points and $L$ a set of lines in the (extended) Euclidean\nplane, and $I \\subseteq P\\times L$, where $i =(p,l) \\in I$ means that point $p$\nand line $l$ are incident. The incidences can be interpreted as quadratic\nconstraints on the homogeneous coordinates of the points and lines. We study\nthe space of incidence preserving motions of the given incidence structure by\nlinearizing the system of quadratic equations. The Jacobian of the quadratic\nsystem, our projective rigidity matrix, leads to the notion of\nindependence\/dependence of incidences. Column dependencies correspond to\ninfinitesimal motions. Row dependencies or self-stresses allow for new\ninterpretations of classical geometric incidence theorems. We show that\nself-stresses are characterized by a 3-fold balance. As expected, infinitesimal\n(first order) projective rigidity as well as second order projective rigidity\nimply projective rigidity but not conversely. Several open problems and\npossible generalizations are indicated.",
        "We construct an expansion of a real closed field by a multiplicative subgroup\nadapting Poizat's theory of green points. Its theory is strongly dependent, and\nevery open set definable in a model of this theory is semialgebraic. We prove\nthat the real field with a dense family of logarithmic spirals, proposed by\nZilber, satisfies our theory.",
        "We construct and analyze the fault tolerance of $[[6,1,3]]$ non-CSS quantum\nerror correcting code under the anisotropic and depolarizing noise models. This\nrate-optimized code achieves fault-tolerance using a single ancilla qubit for\nsyndrome measurement under anisotropic noise conditions. This method was called\nfault-tolerance using bare ancilla by Brown \\emph{et al.} We give explicit\nconstruction of the code using measurements on non-planar graph states. We also\nargue that using our approach, we can construct a family of such fault-tolerant\ncodes. This method fills a notable gap in constructing fault-tolerant non-CSS\ncode families.",
        "This paper examines various ways of improving the impact resilience of\nprotective structures. Such structures' purpose is to dissipate an impact's\nenergy while avoiding cracking and failure. We have tested the reaction of\nplane elastic-brittle lattices to an impulse. Four topologies are compared:\nperiodic triangular, square, and hexagonal topologies, and aperiodic Penrose\ntopology. Then, structures with random variations of the links' stiffness, node\npositions, and random holes are compared. Combinations of these random factors\nare also considered, as well as the resilience of bistable elastic-brittle\nlattices with sacrificial links. Several parameters are introduced to measure\nthe structural resilience of the compared designs: (i) the amount of dissipated\nimpact energy, (ii) the size of broken clusters of links, and (iii) the spread\nof damage. The results suggest new routes for rationally designing protective\nstructures using nonperiodic topology, bistability, and structural randomness.\nIn particular, we find that some quantities of interest can be maximized by\ntuning the randomized design appropriately -- for example, randomly removing\n8\\% of links maximizes energy dissipation. We also find that randomization of\nbistable lattices can offer superior energy dissipation while reducing the\nconnectivity between broken clusters of links.",
        "Motion planning is a fundamental problem in autonomous driving and perhaps\nthe most challenging to comprehensively evaluate because of the associated\nrisks and expenses of real-world deployment. Therefore, simulations play an\nimportant role in efficient development of planning algorithms. To be\neffective, simulations must be accurate and realistic, both in terms of\ndynamics and behavior modeling, and also highly customizable in order to\naccommodate a broad spectrum of research frameworks. In this paper, we\nintroduce SMARTS 2.0, the second generation of our motion planning simulator\nwhich, in addition to being highly optimized for large-scale simulation,\nprovides many new features, such as realistic map integration,\nvehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and\na broad variety of sensor models.\n  Moreover, we present a novel benchmark suite for evaluating planning\nalgorithms in various highly challenging scenarios, including interactive\ndriving, such as turning at intersections, and adaptive driving, in which the\ntask is to closely follow a lead vehicle without any explicit knowledge of its\nintention. Each scenario is characterized by a variety of traffic patterns and\nroad structures. We further propose a series of common and task-specific\nmetrics to effectively evaluate the performance of the planning algorithms. At\nthe end, we evaluate common motion planning algorithms using the proposed\nbenchmark and highlight the challenges the proposed scenarios impose. The new\nSMARTS 2.0 features and the benchmark are publicly available at\ngithub.com\/huawei-noah\/SMARTS."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Biological identifications through DNA barcodes",
    "start_abstract":"Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "BarcodeBERT: Transformers for Biodiversity Analysis"
      ],
      "abstract":[
        "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Complexity of Finite Semigroups: History and Decidability",
        "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
        "Two-component nonlinear wave solutions of the sixth-order generalised\n  Boussinesq-type equations",
        "Anyon Theory and Topological Frustration of High-Efficiency Quantum LDPC\n  Codes",
        "A central limit theorem for the giant in a stochastic block model",
        "Faster Machine Translation Ensembling with Reinforcement Learning and\n  Competitive Correction",
        "Unified Native Spaces in Kernel Methods",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction",
        "Pressure-induced structural and superconducting transitions in black\n  arsenic",
        "A healthier stochastic semiclassical gravity: world without\n  Schr\\\"{o}dinger cats",
        "From Target Tracking to Targeting Track -- Part II: Regularized\n  Polynomial Trajectory Optimization",
        "Open-Ended and Knowledge-Intensive Video Question Answering",
        "General Relativity and Geodesy",
        "Evaluating Interpretable Reinforcement Learning by Distilling Policies\n  into Programs",
        "An inherently parallel H2-ULV factorization for solving dense linear\n  systems on GPUs",
        "Current-induced magnetoresistance hysteresis in the kagome\n  superconductor CsV$_3$Sb$_5$",
        "Comment on \"Comment on Attosecond electron microscopy and diffraction\"",
        "The Ophiuchus DIsk Survey Employing ALMA (ODISEA): Complete Size\n  Distributions for the 100 Brightest Disks Across Multiplicity and SED Classes",
        "Comparison of Offset and Ratio Weighted Regressions in Tweedie Models\n  with Application to Mid-Term Cancellations",
        "High-Throughput Computational Screening and Interpretable Machine\n  Learning of Metal-organic Frameworks for Iodine Capture",
        "A Comprehensive Survey on Cross-Domain Recommendation: Taxonomy,\n  Progress, and Prospects",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Vibrational properties of photochromic yttrium oxyhydride and\n  oxydeuteride thin films",
        "Empirical Calibration and Metric Differential Privacy in Language Models",
        "Argument-Based Comparative Question Answering Evaluation Benchmark",
        "Bifurcation and multiplicity results for critical problems involving the\n  $p$-Grushin operator",
        "InsTALL: Context-aware Instructional Task Assistance with Multi-modal\n  Large Language Models",
        "Teleportation scheme for the complete state of light at the example of\n  coherent states"
      ],
      "abstract":[
        "In recent papers, Margolis, Rhodes and Schilling proved that the complexity\nof a finite semigroup is computable. This solved a problem that had been open\nfor more than 50 years. The purpose of this paper is to survey the basic\nresults of Krohn-Rhodes complexity of finite semigroups and to outline the\nproof of its computability.",
        "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
        "Two different versions of cubic sixth-order generalised Boussinesq-type wave\nequations are considered in this study. A generalised perturbation reduction\nmethod is used to solve these equations, which allows the reduction of\nconsidered equations to coupled nonlinear Schrodinger equations. Two-component\nnonlinear wave solutions are obtained. The profiles and parameters of these\nsolutions for both nonlinear equations are presented and compared. These\nsolutions coincide with the vector 0 \\pi pulse of self-induced transparency,\nwhich was previously studied in several known nonlinear wave equations.",
        "Quantum low-density parity-check (QLDPC) codes present a promising route to\nlow-overhead fault-tolerant quantum computation, yet systematic strategies for\ntheir exploration remain underdeveloped. In this work, we establish a\ntopological framework for studying the bivariate-bicycle codes, a prominent\nclass of QLDPC codes tailored for real-world quantum hardware. Our framework\nenables the investigation of these codes through universal properties of\ntopological orders. Besides providing efficient characterizations for\ndemonstrations using Gr\\\"obner bases, we also introduce a novel\nalgebraic-geometric approach based on the Bernstein--Khovanskii--Kushnirenko\ntheorem, allowing us to analytically determine how the topological order varies\nwith the generic choice of bivariate-bicycle codes under toric layouts. Novel\nphenomena are unveiled, including topological frustration, where ground-state\ndegeneracy on a torus deviates from the total anyon number, and quasi-fractonic\nmobility, where anyon movement violates energy conservation. We demonstrate\ntheir inherent link to symmetry-enriched topological orders and offer an\nefficient method for searching for finite-size codes. Furthermore, we extend\nthe connection between anyons and logical operators using Koszul complex\ntheory. Our work provides a rigorous theoretical basis for exploring the fault\ntolerance of QLDPC codes and deepens the interplay among topological order,\nquantum error correction, and advanced mathematical structures.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Ensembling neural machine translation (NMT) models to produce higher-quality\ntranslations than the $L$ individual models has been extensively studied.\nRecent methods typically employ a candidate selection block (CSB) and an\nencoder-decoder fusion block (FB), requiring inference across \\textit{all}\ncandidate models, leading to significant computational overhead, generally\n$\\Omega(L)$. This paper introduces \\textbf{SmartGen}, a reinforcement learning\n(RL)-based strategy that improves the CSB by selecting a small, fixed number of\ncandidates and identifying optimal groups to pass to the fusion block for each\ninput sentence. Furthermore, previously, the CSB and FB were trained\nindependently, leading to suboptimal NMT performance. Our DQN-based\n\\textbf{SmartGen} addresses this by using feedback from the FB block as a\nreward during training. We also resolve a key issue in earlier methods, where\ncandidates were passed to the FB without modification, by introducing a\nCompetitive Correction Block (CCB). Finally, we validate our approach with\nextensive experiments on English-Hindi translation tasks in both directions.",
        "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.",
        "We report high-pressure Raman spectra and resistance measurements of black\narsenic (b-As) up to 58 GPa, along with phonon density of states (DOS) and\nenthalpy calculations for four reported arsenic phases up to 50 GPa. It is\nfound that metastable b-As transforms into gray arsenic (g-As) phase at a\ncritical pressure of 1.51 GPa, followed by subsequent transitions to simple\ncubic arsenic (c-As) and incommensurate host-guest arsenic (hg-As) phases at\n25.9 and 44.8 GPa, respectively. Superconductivity emerges above 25 GPa in the\nc-As phase, with the superconducting transition temperature ($T$$\\rm_c$)\nremaining nearly a constant of 3 K. Upon further compression, $T$$\\rm_c$\nsteeply increases to a higher value around 4.5 K in the incommensurate hg-As\nphase above 43 GPa. We use our results to update the structural and\nsuperconducting phase diagrams under pressure for the novel semiconductor,\nblack arsenic.",
        "Semiclassical gravity couples classical gravity to the quantized matter in\nmeanfield approximation. The meanfield coupling is problematic for two reasons.\nFirst, it ignores the quantum fluctuation of matter distribution. Second, it\nviolates the linearity of the quantum dynamics. The first problem can be\nmitigated by allowing stochastic fluctuations of the geometry but the second\nproblem lies deep in quantum foundations. Restoration of quantum linearity\nrequires a conceptual approach to hybrid classical-quantum coupling. Studies of\nthe measurement problem and the quantum-classical transition point to the\nsolution. It is based on a postulated mechanism of spontaneous quantum\nmonitoring plus feedback. This approach eliminates Schr\\\"{o}dinger cat states,\ntakes quantum fluctuations into account, and restores the linearity of quantum\ndynamics. Such a captivating conceptionally `healthier' semiclassical theory\nexists in the Newtonian limit, but its relativistic covariance hits a wall.\nHere we will briefly recapitulate the concept and its realization in the\nnonrelativistic limit. We emphasize that the long-known obstacles to the\nrelativistic extension lie in quantum foundations.",
        "Target tracking entails the estimation of the evolution of the target state\nover time, namely the target trajectory. Different from the classical state\nspace model, our series of studies, including this paper, model the collection\nof the target state as a stochastic process (SP) that is further decomposed\ninto a deterministic part which represents the trend of the trajectory and a\nresidual SP representing the residual fitting error. Subsequently, the tracking\nproblem is formulated as a learning problem regarding the trajectory SP for\nwhich a key part is to estimate a trajectory FoT (T-FoT) best fitting the\nmeasurements in time series. For this purpose, we consider the polynomial T-FoT\nand address the regularized polynomial T-FoT optimization employing two\ndistinct regularization strategies seeking trade-off between the accuracy and\nsimplicity. One limits the order of the polynomial and then the best choice is\ndetermined by grid searching in a narrow, bounded range while the other adopts\n$\\ell_0$ norm regularization for which the hybrid Newton solver is employed.\nSimulation results obtained in both single and multiple maneuvering target\nscenarios demonstrate the effectiveness of our approaches.",
        "Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.",
        "Mass redistribution on Earth due to dynamic processes such as ice melting and\nsea level rise leads to a changing gravitational field, observable by geodetic\ntechniques. Monitoring this change over time allows us to learn more about our\nplanet and its dynamic evolution. In this paper, we highlight the impact of\nGeneral Relativity (GR) on geodesy: it provides corrections essential for the\ninterpretation of high-precision measurements and enables a completely novel\nmeasurement approach using chronometry, i.e., clock-based observations.\nFocusing on the latter, we review the construction of the relativistic gravity\npotential and the corresponding geoid definition as an isochronometric surface\nto elucidate the comparison to the conventional Newtonian geoid. Furthermore,\nwe comment on additional potentials due to the non-Newtonian degrees of freedom\nof the relativistic gravitational field, and assess the feasibility of\nclock-based measurements for Gravity Field Recovery (GFR) from space. Although\nclock observations in space demonstrate technical promise for GFR, achieving\nthe necessary precision for practical applications remains challenging.",
        "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability.",
        "Hierarchical low-rank approximation of dense matrices can reduce the\ncomplexity of their factorization from O(N^3) to O(N). However, the complex\nstructure of such hierarchical matrices makes them difficult to parallelize.\nThe block size and ranks can vary between the sub-blocks, which creates load\nimbalance. The dependency between the sub-blocks during factorization results\nin serialization. Since many sub-blocks are low-rank, their small computational\nload exposes the overhead of runtime systems. The combination of these factors\nmakes it challenging to implement these methods on GPUs. In this work, we show\nthat dense matrices can be factorized with linear complexity, while extracting\nthe potential parallelism of GPUs. This is made possible through the H2-ULV\nfactorization, which removes the dependency on trailing sub-matrices.",
        "We report the observation of current-modulated magnetoresistance hysteresis\nbelow the superconducting transition temperature in the kagome superconductor\nCsV$_3$Sb$_5$. This highly tunable hysteresis behavior is confined to the\nsuperconducting state and vanishes when superconductivity is fully suppressed,\ndirectly linking magnetoresistance hysteresis to the superconducting order in\nCsV$_3$Sb$_5$. Additionally, the superconducting diode effect driven by a small\nmagnetic field is observed, indicating the enhanced electronic magnetochiral\nanisotropy by the chiral domain-wall scattering. Our findings position\nCsV$_3$Sb$_5$ as a promising platform for exploring nontrivial physical\nphenomena, including unconventional pairing mechanisms and topological\nsuperconductivity.",
        "Over the past few decades, following the first demonstration of ultrafast\nelectron microscopy, numerous research groups have focused on achieving\nattosecond temporal resolution in electron microscopy with the goal of imaging\nelectron and atomic motion. Recently, several studies have claimed to achieve\nattosecond temporal resolution in imaging(1-3). These claims are based on the\ngeneration of attosecond electron pulse trains. However, in typical\ntime-resolved measurements used to capture dynamic processes in real-time, the\ntemporal resolution is determined by the envelope of the pulse train. The\nreliance of using attosecond electron pulse trains fails to account for the\ndistinct temporal resolution advantages enabled by our attosecond optical\ngating, which are absent in the case of using a continuous-wave or long laser\npulse. These oversights highlight the limitations of this methodology (1-3) in\nstudying ultrafast phenomena of matter. It is crucial to clarify this\ndistinction to avoid confusion, misinterpretation, and potential miscitations\nwithin the community regarding attosecond temporal resolution in electron\nmicroscopy and the attosecond imaging of matter dynamics. In contrast, Hui et\nal. (4) present the first realistic demonstration of attosecond imaging\nresolution in electron microscopy, enabling the diffraction imaging of electron\nmotion dynamics in graphene. In a commentary by Peter Baum and Claus Ropers,\nthe authors conjecture that the graphene dynamics observed in our time-resolved\ndiffraction experiment (Fig. 5, Hui et al. 2024) (4) is an optical interference\nartifact or light modulation of electrons effects, similar to what was reported\npreviously (1-3), in addition to raising other technical concerns. In this\nreply, we are pleased to address these allegations and provide clarifications\nto resolve the raised technical questions.",
        "The size of a protoplanetary disk is a fundamental property, yet most remain\nunresolved, even in nearby star-forming regions (d $\\sim$ 140-200 pc). We\npresent the complete continuum size distribution for the $105$ brightest\nprotoplanetary disks (M$_{\\text{dust}}$ $\\gtrsim$ 2 M$_{\\oplus}$) in the\nOphiuchus cloud, obtained from ALMA Band 8 (410 GHz) observations at\n0.05$^{\\prime\\prime}$ (7 au) to 0.15$^{\\prime\\prime}$ (21 au) resolution. This\nsample includes 54 Class II and 51 Class I and Flat Spectrum sources, providing\na comprehensive distribution across evolutionary stages. We measure the Half\nWidth at Half Maximum (HWHM) and the radius encircling $68\\%$ of the flux\n($R_{68\\%}$) for most non-binary disks, yielding the largest flux-limited\nsample of resolved disks in any star-forming region. The distribution is\nlog-normal with a median value of $\\sim$14 au and a logarithmic standard\ndeviation $\\sigma_{\\log} = 0.46$ (factor of 2.9 in linear scale). Disks in\nclose binary systems ($<$ 200 au separation) have smaller radii, with median\nvalue of $\\sim$5 au, indicating efficient radial drift as predicted by dust\nevolution models. The size distribution for young embedded objects (SED Class I\nand Flat Spectrum, age $\\lesssim$ 1 Myr) is similar to that of Class II objects\n(age $\\sim$ a few Myr), implying that pressure bumps must be common at early\ndisk stages to prevent mm-sized particle migration at au scales.",
        "In property and casualty insurance, particularly in automobile insurance,\nrisk exposure is traditionally associated with the coverage duration. However,\nfactors such as early contract cancellations demand more precise modelling to\nensure accurate premium pricing. This study introduces and compares two\napproaches for modelling total claims (or loss costs) in insurance portfolios\nwith a high proportion of policies that have partial year exposure: the offset\nand ratio methods. We demonstrate that both approaches can be viewed as\nweighted regressions under the Tweedie distribution framework. Through an\nanalysis based on the financial balance property, we find that the ratio\napproach outperforms the offset method. This comparison is illustrated using an\nautomobile insurance portfolio, where a significant share of policyholders\nterminate their contracts before the coverage period concludes.",
        "The removal of leaked radioactive iodine isotopes in humid environments holds\nsignificant importance in nuclear waste management and nuclear accident\nmitigation. In this study, high-throughput computational screening and machine\nlearning were combined to reveal the iodine capture performance of 1816\nmetal-organic framework (MOF) materials under humid air conditions. Firstly,\nthe relationship between the structural characteristics of MOFs and their\nadsorption properties was explored, with the aim of identifying the optimal\nstructural parameters for iodine capture. Subsequently, two machine learning\nregression algorithms - Random Forest and CatBoost, were employed to predict\nthe iodine adsorption capabilities of MOFs. In addition to 6 structural\nfeatures, 25 molecular features and 8 chemical features were incorporated to\nenhance the prediction accuracy of the machine learning algorithms. Feature\nimportance was assessed to determine the relative influence of various features\non iodine adsorption performance, in which the Henry's coefficient and heat of\nadsorption to iodine were found the two most crucial chemical factors.\nFurthermore, four types of molecular fingerprints were introduced for providing\ncomprehensive and detailed structural information of MOF materials. The top 20\nmost significant MACCS molecular fingerprints were picked out, revealing that\nthe presence of six-membered ring structures and nitrogen atoms in the MOFs\nwere the key structural factors that enhanced iodine adsorption, followed by\nthe existence of oxygen atoms. This work combined high-throughput computation,\nmachine learning, and molecular fingerprints to comprehensively elucidate the\nmultifaceted factors influencing the iodine adsorption performance of MOFs,\noffering profound insightful guidelines for screening and structural design of\nadvanced MOF materials.",
        "Recommender systems (RS) have become crucial tools for information filtering\nin various real world scenarios. And cross domain recommendation (CDR) has been\nwidely explored in recent years in order to provide better recommendation\nresults in the target domain with the help of other domains. The CDR technology\nhas developed rapidly, yet there is a lack of a comprehensive survey\nsummarizing recent works. Therefore, in this paper, we will summarize the\nprogress and prospects based on the main procedure of CDR, including Cross\nDomain Relevance, Cross Domain Interaction, Cross Domain Representation\nEnhancement and Model Optimization. To help researchers better understand and\nengage in this field, we also organize the applications and resources, and\nhighlight several current important challenges and future directions of CDR.\nMore details of the survey articles are available at\nhttps:\/\/github.com\/USTCAGI\/Awesome-Cross-Domain\nRecommendation-Papers-and-Resources.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "A comprehensive study of the vibrational properties of photochromic yttrium\noxyhydride (YHO) and oxydeuteride (YDO) thin films is presented. These films\nare deposited using reactive magnetron sputtering, followed by post-oxidation.\nOur investigation employs vibrational Fourier-transform infrared (FTIR)\nspectroscopy, in conjunction with first-principles Density Functional Theory\n(DFT) calculations. The FTIR spectra of the films reveal broad vibrational\nbands, primarily attributed to the disordered structure containing small\ncrystallites (<10 nm), as confirmed by solid-state nuclear magnetic resonance\nand X-ray diffraction measurements. An isotopic shift from approximately 900 to\n745 cm-1 is observed in the hydrogen\/deuterium-related vibration band, while\nthe lower frequency bands (< 600 cm-1) remain unaffected upon replacement of\nhydrogen with deuterium. These experimental observations are consistent with\nthe DFT theoretical calculations for various stable YHO lattices reported in\nthe literature. Illumination of the films with ultraviolet light at 3.3 eV\nleads to additional absorption not only in the visible light range but also up\nto approximately 2000 cm-1 in the mid-infrared region. However, no phase\ntransformation change or formation of hydroxyl (OH) groups are observed\nfollowing illumination. Our findings provide valuable insight into the\nvibrational and photochromic properties of YH(D)O thin films.",
        "NLP models trained with differential privacy (DP) usually adopt the DP-SGD\nframework, and privacy guarantees are often reported in terms of the privacy\nbudget $\\epsilon$. However, $\\epsilon$ does not have any intrinsic meaning, and\nit is generally not possible to compare across variants of the framework. Work\nin image processing has therefore explored how to empirically calibrate noise\nacross frameworks using Membership Inference Attacks (MIAs). However, this kind\nof calibration has not been established for NLP. In this paper, we show that\nMIAs offer little help in calibrating privacy, whereas reconstruction attacks\nare more useful. As a use case, we define a novel kind of directional privacy\nbased on the von Mises-Fisher (VMF) distribution, a metric DP mechanism that\nperturbs angular distance rather than adding (isotropic) Gaussian noise, and\napply this to NLP architectures. We show that, even though formal guarantees\nare incomparable, empirical privacy calibration reveals that each mechanism has\ndifferent areas of strength with respect to utility-privacy trade-offs.",
        "In this paper, we aim to solve the problems standing in the way of automatic\ncomparative question answering. To this end, we propose an evaluation framework\nto assess the quality of comparative question answering summaries. We formulate\n15 criteria for assessing comparative answers created using manual annotation\nand annotation from 6 large language models and two comparative question\nasnwering datasets. We perform our tests using several LLMs and manual\nannotation under different settings and demonstrate the constituency of both\nevaluations. Our results demonstrate that the Llama-3 70B Instruct model\ndemonstrates the best results for summary evaluation, while GPT-4 is the best\nfor answering comparative questions. All used data, code, and evaluation\nresults are publicly\navailable\\footnote{\\url{https:\/\/anonymous.4open.science\/r\/cqa-evaluation-benchmark-4561\/README.md}}.",
        "In this article we prove a bifurcation and multiplicity result for a critical\nproblem involving a degenerate nonlinear operator $\\Delta_\\gamma^p$. We extend\nto a generic $p>1$ a result which was proved only when $p=2$. When $p\\neq 2$,\nthe nonlinear operator $-\\Delta_\\gamma^p$ has no linear eigenspaces, so our\nextension is nontrivial and requires an abstract critical theorem which is not\nbased on linear subspaces. We also prove a new abstract result based on a\npseudo-index related to the $\\mathbf{Z}_2$-cohomological index that is\napplicable here. We provide a version of the Lions' Concentration-Compactness\nPrinciple for our operator.",
        "The improved competence of generative models can help building multi-modal\nvirtual assistants that leverage modalities beyond language. By observing\nhumans performing multi-step tasks, one can build assistants that have\nsituational awareness of actions and tasks being performed, enabling them to\ncater assistance based on this understanding. In this paper, we develop a\nContext-aware Instructional Task Assistant with Multi-modal Large Language\nModels (InsTALL) that leverages an online visual stream (e.g. a user's screen\nshare or video recording) and responds in real-time to user queries related to\nthe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal\nmodel on task videos and paired textual data, and 2) automatically extracts\ntask graph from video data and leverages it at training and inference time. We\nshow InsTALL achieves state-of-the-art performance across proposed sub-tasks\nconsidered for multimodal activity understanding -- task recognition (TR),\naction recognition (AR), next action prediction (AP), and plan prediction (PP)\n-- and outperforms existing baselines on two novel sub-tasks related to\nautomatic error identification.",
        "We present a scheme to teleport both the spatial and number-of-photons\ndegrees of freedom of light. This is achieved by teleportation of each pixel of\na target image. We take coherent states as the input and demonstrate the scheme\nfor the cases with ideal and realistic entanglement resources."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"BarcodeBERT: Transformers for Biodiversity Analysis",
    "start_abstract":"Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Biological identifications through DNA barcodes"
      ],
      "abstract":[
        "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "A coupled finite and boundary spectral element method for linear\n  water-wave propagation problems",
        "Deep Subspace Learning for Surface Anomaly Classification Based on 3D\n  Point Cloud Data",
        "Branching with selection and mutation II: Mutant fitness of Gumbel type",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "Study of giant radio galaxies using spectroscopic observations from the\n  Himalayan Chandra Telescope",
        "Theory of Cation Solvation in the Helmholtz Layer of Li-ion Battery\n  Electrolytes",
        "Reducing Circuit Depth in Quantum State Preparation for Quantum\n  Simulation Using Measurements and Feedforward",
        "Multi-particle-collision simulation of heat transfer in low-dimensional\n  fluids",
        "Measuring decoherence due to quantum vacuum fluctuations",
        "Impact of Fasteners on the Radar Cross-Section performance of Radar\n  Absorbing Air Intake Duct",
        "Semilinear Dynamic Programming: Analysis, Algorithms, and Certainty\n  Equivalence Properties",
        "Bayesian calculus and predictive characterizations of extended feature\n  allocation models",
        "Conversion of photon temporal shape using single gradient metasurface",
        "Well-Posedness of the R13 Equations Using Tensor-Valued Korn\n  Inequalities",
        "Non-self-adjoint Dirac operators on graphs",
        "The highest energy cosmic rays from superheavy dark matter particles",
        "A refined functorial universal tangle invariant",
        "Kitaoka's Conjecture for quadratic fields",
        "Advancing the heralded photon number-state characterization by\n  understanding the interplay of experimental imperfections",
        "Quantifying the degree of hydrodynamic behaviour in heavy-ion collisions",
        "Bayesian estimation of a multivariate TAR model when the noise process\n  distribution belongs to the class of Gaussian variance mixtures",
        "Cookie cutters: Bisections with fixed shapes",
        "Wasserstein-based Kernels for Clustering: Application to Power\n  Distribution Graphs",
        "Quantum Synchronizing Words: Resetting and Preparing Qutrit States",
        "Cross-Validation in Penalized Linear Mixed Models: Addressing Common\n  Implementation Pitfalls",
        "A Bourgain-Gromov problem on non-compact Sobolev-Lorentz embeddings",
        "$3d$ flat bands and coupled $4f$ moments in the kagome-honeycomb\n  permanent magnet Sm$_{2}$Co$_{17}$",
        "D-shell mixing in light baryons and its effect on the orbital motion",
        "Visible-range excitons and electronic structure in $d^0$ double\n  perovskite oxides"
      ],
      "abstract":[
        "A coupled boundary spectral element method (BSEM) and spectral element method\n(SEM) formulation for the propagation of small-amplitude water waves over\nvariable bathymetries is presented in this work. The wave model is based on the\nmild-slope equation (MSE), which provides a good approximation of the\npropagation of water waves over irregular bottom surfaces with slopes up to\n1:3. In unbounded domains or infinite regions, space can be divided into two\ndifferent areas: a central region of interest, where an irregular bathymetry is\nincluded, and an exterior infinite region with straight and parallel\nbathymetric lines. The SEM allows us to model the central region, where any\nvariation of the bathymetry can be considered, while the exterior infinite\nregion is modelled by the BSEM which, combined with the fundamental solution\npresented by Cerrato et al. [A. Cerrato, J. A. Gonz\\'alez, L.\nRodr\\'iguez-Tembleque, Boundary element formulation of the mild-slope equation\nfor harmonic water waves propagating over unidirectional variable bathymetries,\nEng. Anal. Boundary Elem. 62 (2016) 22-34.] can include bathymetries with\nstraight and parallel contour lines. This coupled model combines important\nadvantages of both methods; it benefits from the flexibility of the SEM for the\ninterior region and, at the same time, includes the fulfilment of the\nSommerfeld's radiation condition for the exterior problem, that is provided by\nthe BSEM. The solution approximation inside the elements is constructed by high\norder Legendre polynomials associated with Legendre-Gauss-Lobatto quadrature\npoints, providing a spectral convergence for both methods. The proposed\nformulation has been validated in three different benchmark cases with\ndifferent shapes of the bottom surface. The solutions exhibit the typical\np-convergence of spectral methods.",
        "Surface anomaly classification is critical for manufacturing system fault\ndiagnosis and quality control. However, the following challenges always hinder\naccurate anomaly classification in practice: (i) Anomaly patterns exhibit\nintra-class variation and inter-class similarity, presenting challenges in the\naccurate classification of each sample. (ii) Despite the predefined classes,\nnew types of anomalies can occur during production that require to be detected\naccurately. (iii) Anomalous data is rare in manufacturing processes, leading to\nlimited data for model learning. To tackle the above challenges simultaneously,\nthis paper proposes a novel deep subspace learning-based 3D anomaly\nclassification model. Specifically, starting from a lightweight encoder to\nextract the latent representations, we model each class as a subspace to\naccount for the intra-class variation, while promoting distinct subspaces of\ndifferent classes to tackle the inter-class similarity. Moreover, the explicit\nmodeling of subspaces offers the capability to detect out-of-distribution\nsamples, i.e., new types of anomalies, and the regularization effect with much\nfewer learnable parameters of our proposed subspace classifier, compared to the\npopular Multi-Layer Perceptions (MLPs). Extensive numerical experiments\ndemonstrate our method achieves better anomaly classification results than\nbenchmark methods, and can effectively identify the new types of anomalies.",
        "We study a model of a branching process subject to selection, modeled by\ngiving each family an individual fitness acting as a branching rate, and\nmutation, modeled by resampling the fitness of a proportion of offspring in\neach generation. For two large classes of fitness distributions of Gumbel type\nwe determine the growth of the population, almost surely on survival. We then\nstudy the empirical fitness distribution in a simplified model, which is\nnumerically indistinguishable from the original model, and show the emergence\nof a Gaussian travelling wave.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "We present the results of spectroscopic observations of host galaxies of\neleven candidate giant radio galaxies (GRGs), powered by active galactic nuclei\n(AGNs), conducted with the 2-m Himalayan Chandra Telescope (HCT). The primary\naim of these observations, performed with the Hanle Faint Object Spectrograph\nCamera (HFOSC), was to secure accurate spectroscopic redshifts, enabling\nprecise calculations of their projected linear sizes. Based on these\nmeasurements, we confirm all eleven sources as giants, with linear sizes\nranging from 0.7 to 2.9 Mpc, including ten GRGs and one giant radio quasar\n(GRQ). One of the GRGs shows evidence of a potential AGN jet-driven ionized\noutflow, extending up to $\\sim$12 kpc, which, if confirmed, would represent a\nrarely observed feature. Two of the confirmed GRGs exceed 2 Mpc in size, which\nare relatively rare examples of GRG. The redshifts of the host galaxies span\n0.09323 $\\leq$ z $\\leq$ 0.41134. Using the obtained spectroscopic data, we\ncharacterised their AGN states based on the optical emission line properties.\nTo complement these observations, archival radio and optical survey data were\nutilised to characterise their large-scale radio morphology and estimate\nprojected linear sizes, arm-length ratios, flux densities, luminosities, and\ncore dominance factors. These results provide new insights into the properties\nof GRSs and form a critical foundation for further detailed studies of their\nenvironments, AGN activity, and evolution using future high-sensitivity optical\nand radio datasets.",
        "The solvation environments of Li$^+$ in conventional non-aqueous battery\nelectrolytes, such as LiPF$_6$ in mixtures of ethylene carbaronate (EC) and\nethyl methyl carbonate (EMC), are often used to rationalize the transport\nproperties of electrolytes and solid electrolyte interphase (SEI) formation. In\nthe SEI, the solvation environments in the compact electrical double layer\n(EDL) next to the electrode, also known as the Helmholtz layer, determine\n(partially) what species can react to form the SEI, with bulk solvation\nenvironments often being used as a proxy. Here we develop and test a theory of\ncation solvation in the Helmholtz layer of non-aqueous Li-ion battery\nelectrolytes. First, we validate the theory against bulk and diffuse EDL\natomistic molecular dynamics (MD) simulations of LiPF$_6$ EC\/EMC mixtures as a\nfunction of surface charge, where we find the theory can capture the solvation\nenvironments well. Next we turn to the Helmholtz layer, where we find that the\nmain effect of the solvation structures next to the electrode is an apparent\nreduction in the number of binding sites between Li$^+$ and the solvents, again\nwhere we find good agreement with our developed theory. Finally, by solving a\nsimplified version of the theory, we find that the probability of Li$^+$\nbinding to each solvent remains equal to the bulk probability, suggesting that\nthe bulk solvation environments are a reasonable place to start when\nunderstanding new battery electrolytes. Our developed formalism can be\nparameterized from bulk MD simulations and used to predict the solvation\nenvironments in the Helmholtz layer, which can be used to determine what could\nreact and form the SEI.",
        "Reducing circuit depth and identifying an optimal trade-off between circuit\ndepth and width is crucial for successful quantum computation. In this context,\nmid-circuit measurement and feedforward have been shown to significantly reduce\nthe depth of quantum circuits, particularly in implementing logical gates. By\nleveraging these techniques, we propose several parallelization strategies that\nreduce quantum circuit depth at the expense of increasing width in preparing\nvarious quantum states relevant to quantum simulation. With measurements and\nfeedforward, we demonstrate that utilizing unary encoding as a bridge between\ntwo quantum states substantially reduces the circuit depth required for\npreparing quantum states, such as sparse quantum states and sums of Slater\ndeterminants within the first quantization framework, while maintaining an\nefficient circuit width. Additionally, we show that a coordinate Bethe ansatz,\ncharacterized by its high degree of freedom in its phase, can be\nprobabilistically prepared in a constant-depth quantum circuit using\nmeasurements and feedforward. We anticipate that our study will contribute to\nthe reduction of circuit depth in initial state preparation, particularly for\nquantum simulation, which is a critical step toward achieving quantum\nadvantage.",
        "Simulation of transport properties of confined, low-dimensional fluids can be\nperformed efficiently by means of Multi-Particle Collision (MPC) dynamics with\nsuitable thermal-wall boundary conditions. We illustrate the effectiveness of\nthe method by studying dimensionality effects and size-dependence of thermal\nconduction, properties of crucial importance for understanding heat transfer at\nthe micro-nanoscale. We provide a sound numerical evidence that the simple MPC\nfluid displays the features previously predicted from hydrodynamics of lattice\nsystems: (1) in 1D, the thermal conductivity $\\kappa$ diverges with the system\nsize $L$ as $\\kappa\\sim L^{1\/3}$ and its total heat current autocorrelation\nfunction $C(t)$ decays with the time $t$ as $C(t)\\sim t^{-2\/3}$; (2) in 2D,\n$\\kappa$ diverges with $L$ as $\\kappa\\sim \\mathrm{ln} (L)$ and its $C(t)$\ndecays with $t$ as $C(t)\\sim t^{-1}$; (3) in 3D, its $\\kappa$ is independent\nwith $L$ and its $C(t)$ decays with $t$ as $C(t)\\sim t^{-3\/2}$. For weak\ninteraction (the nearly integrable case) in 1D and 2D, there exists an\nintermediate regime of sizes where kinetic effects dominate and transport is\ndiffusive before crossing over to expected anomalous regime. The crossover can\nbe studied by decomposing the heat current in two contributions, which allows\nfor a very accurate test of the predictions. In addition, we also show that\nupon increasing the aspect ratio of the system, there exists a dimensional\ncrossover from 2D or 3D dimensional behavior to the 1D one. Finally, we show\nthat an applied magnetic field renders the transport normal, indicating that\npseudomomentum conservation is not sufficient for the anomalous heat conduction\nbehavior to occur.",
        "The interaction of a particle with vacuum fluctuations -- which theoretically\nexist even in the complete absence of matter -- can lead to observable\nirreversible decoherence, if it were possible to switch on and off the particle\ncharge suddenly. We compute the leading order decoherence effect for such a\nscenario and propose an experimental setup for its detection. Such a\nmeasurement might provide further insights into the nature of vacuum\nfluctuations and a novel precision test for the decoherence theory.",
        "An aircraft consists of various cavities including air intake ducts, cockpit,\nradome, inlet and exhaust of heat exchangers, passage for engine bay\/other bay\ncooling etc. These cavities are prime radar cross-section (RCS) contributors of\naircraft. The major such cavity is air intake duct, and it contributes\nsignificantly to frontal sector RCS of an aircraft. The RCS reductions of air\nintake duct is very important to achieve a low RCS (or stealthy) aircraft\nconfiguration. In general, radar absorbing materials (RAM) are getting utilized\nfor RCS reduction of air intake duct. It can also be noticed that a large\nnumber of fasteners are used for integration of air intake duct with the\naircraft structures. The installation of fasteners on RAS may lead to\ndegradation of RCS performance of air intake. However, no such studies are\nreported in the literature on the impact of rivets on the RCS performance of\nRAS air intake duct. In this paper, radar absorbing material of thickness 6.25\nmm is designed which givens more than -10 dB reflection loss from 4 to 18GHz of\nfrequencies. Next, the effect of rivet installation on these RAS is carried out\nusing three different rivet configurations. The RCS performance of RAS is\nevaluated for duct of different lengths from 1 to 18GHz of frequencies. In\norder to see the RCS performance, five different air intake cases are\nconsidered The RCS performance with increase in percentage surface area of\nrivet heads to RAS is reported in detail. At the last, an open-source aircraft\nCAD model is considered and the RCS performance of RAS air intake with and\nwithout rivets is evaluated.",
        "We consider a broad class of dynamic programming (DP) problems that involve a\npartially linear structure and some positivity properties in their system\nequation and cost function. We address deterministic and stochastic problems,\npossibly with Markov jump parameters. We focus primarily on infinite horizon\nproblems and prove that under our assumptions, the optimal cost function is\nlinear, and that an optimal policy can be computed efficiently with standard DP\nalgorithms. Moreover, we show that forms of certainty equivalence hold for our\nstochastic problems, in analogy with the classical linear quadratic optimal\ncontrol problems.",
        "We introduce and study a unified Bayesian framework for extended feature\nallocations which flexibly captures interactions -- such as repulsion or\nattraction -- among features and their associated weights. We provide a\ncomplete Bayesian analysis of the proposed model and specialize our general\ntheory to noteworthy classes of priors. This includes a novel prior based on\ndeterminantal point processes, for which we show promising results in a spatial\nstatistics application. Within the general class of extended feature\nallocations, we further characterize those priors that yield predictive\nprobabilities of discovering new features depending either solely on the sample\nsize or on both the sample size and the distinct number of observed features.\nThese predictive characterizations, known as \"sufficientness\" postulates, have\nbeen extensively studied in the literature on species sampling models starting\nfrom the seminal contribution of the English philosopher W.E. Johnson for the\nDirichlet distribution. Within the feature allocation setting, existing\npredictive characterizations are limited to very specific examples; in\ncontrast, our results are general, providing practical guidance for prior\nselection. Additionally, our approach, based on Palm calculus, is analytical in\nnature and yields a novel characterization of the Poisson point process through\nits reduced Palm kernel.",
        "By applying phase modulation across different frequencies, metasurfaces\npossess the ability to manipulate the temporal dimension of photons at the\nfemtosecond scale. However, there remains a fundamental challenge to shape the\nsingle wavepacket at the nanosecond scale by using of metasurfaces. Here, we\npropose that the single photon temporal shape can be converted through the\nmulti-photon wavepacket interference on a single metasurface. By selecting\nappropriate input single-photon temporal shapes and metasurfaces beam splitting\nratio, controllable photon shape conversion can be achieved with high fidelity.\nFor examples, photons with an exponentially decaying profile can be shaped into\na Gaussian profile; by tuning the relative time delays of input photons,\nGaussian-shaped photons can be transformed into exponentially decaying or\nrising profiles through the same metasurface. The proposed mechanism provides a\ncompact way for solving the temporal shape mismatch issues in quantum networks,\nfacilitating the realization of high-fidelity on-chip quantum information\nprocessing.",
        "In this paper, we finally catch up with proving the well-posedness of the\nlinearized R13 moment model, which describes, e.g., rarefied gas flows. As an\nextension of the classical fluid equations, moment models are robust and have\nbeen frequently used, yet they are challenging to analyze due to their\nadditional equations. By effectively grouping variables, we identify a 2-by-2\nblock structure, allowing the analysis of the well-posedness within the\nabstract LBB framework of saddle point problems. Due to the unique tensorial\nstructure of the equations, in addition to an interesting combination of tools\nfrom Stokes' and linear elasticity theory, we also need new coercivity\nestimates for tensor fields. These Korn-type inequalities are established by\nanalyzing the symbol map of the symmetric and trace-free part of tensor\nderivative fields. Together with the corresponding right inverse of the\ntensorial divergence, we obtain the existence and uniqueness of weak solutions.",
        "In this paper we introduce and study generally non-self-adjoint realizations\nof the Dirac operator on an arbitrary finite metric graph. Employing the robust\nboundary triple framework, we derive, in particular, a variant of the Birman\nSchwinger principle for its eigenvalues, and with an example of a star shaped\ngraph we show that the point spectrum may exhibit diverse behaviour.\nSubsequently, we find sufficient and necessary conditions on transmission\nconditions at the graph's vertices under which the Dirac operator on the graph\nis symmetric with respect to the parity, the time reversal, or the charge\nconjugation transformation.",
        "It is commonly accepted that high energy cosmic rays up to $10^{19}$ eV can\nbe produced in catastrophic astrophysical processes. However the source of a\nfew observed events with higher energies remains mysterious. We propose that\nthey may originate from decay or annihilation of ultra heavy particles of dark\nmatter. Such particles naturally appear in some models of modified gravity\nrelated to Starobinsky inflation.",
        "The universal invariant with respect to a given ribbon Hopf algebra is a\ntangle invariant that dominates all the Reshetikhin-Turaev invariants built\nfrom the representation theory of the algebra. We construct a canonical strict\nmonoidal functor that encodes the universal invariant of upwards tangles and\nrefines the Kerler-Kauffman-Radford functorial invariant. Moreover, this\nfunctor preserves the braiding, twist and the open trace, the latter being a\nmild modification of Joyal-Street-Verity's notion of trace in a balanced\ncategory. We construct this functor using the more flexible XC-algebras, a\nclass which contains both ribbon Hopf algebras and endomorphism algebras of\nrepresentation of these.",
        "We prove that there are at most 13 real quadratic fields that admit a ternary\nuniversal quadratic lattice, thus establishing a strong version of Kitaoka's\nConjecture for quadratic fields. More generally, we obtain explicit upper\nbounds on the discriminants of real quadratic fields with a quadratic lattice\nof rank at most 7 that represents all totally positive multiples of a fixed\ninteger.",
        "We theoretically explore the properties of heralded number states including\nup to three photons that are generated from single-mode twin beams. We\ninvestigate the effects of different parameters involved in the state\npreparation by using the fidelity, normalized second-order factorial moment of\nphoton number for the heralded state $(g^{(2)}_h)$, and photon-number parity as\nfigures of merit. Especially, the photon-number parity offers a practical and\nrobust tool for inferring the target state quality by capturing the\ncontamination of all undesired photon-number contributions. We focus on\nexpressing our results in terms of experimentally easily accessible parameters\nsuch as the coincidences-to-accidentals ratio and the detection efficiencies.\nOur results identify the optimal parameter regions for generating high quality\nphoton-number states and provide useful insights for advancing their use in\nquantum technologies.",
        "Exploiting the first measurements of the same ion species in O+O collisons at\nRHIC and LHC, we propose an experimentally accessible observable to distinguish\nwhether collective behaviour builds up through a hydrodynamic expansion of a\nstrongly interacting QGP or through few rescatterings in a non-equilibrated\ndilute medium. Our procedure allows to disentangle the effects of the initial\nstate geometry and the dynamical response mechanism on the total resulting\nanisotropic flow. We validate the ability of our proposed observable to\ndiscriminate between systems with different interaction rates using results\nfrom event-by-event simulations in kinetic theory in the Relaxation Time\nApproximation (RTA). As a proof of concept, we extract the degree of\nhydrodynamization for Pb+Pb collisions at LHC from experimental data.",
        "A threshold autoregressive (TAR) model is a powerful tool for analyzing\nnonlinear multivariate time series, which includes special cases like\nself-exciting threshold autoregressive (SETAR) models and vector autoregressive\n(VAR) models. In this paper, estimation, inference, and forecasting using the\nBayesian approach are developed for multivariate TAR (MTAR) models considering\na flexible setup, under which the noise process behavior can be described using\nnot only the Gaussian distribution but also other distributions that belong to\nthe class of Gaussian variance mixtures, which includes Student-t, Slash,\nsymmetric hyperbolic, and contaminated normal distributions, which are also\nsymmetric but are more flexible and with heavier tails than the Gaussian one.\nInferences from MTAR models based on that kind of distribution may be less\naffected by extreme or outlying observations than those based on the Gaussian\none. All parameters in the MTAR model are included in the proposed MCMC-type\nalgorithm, except the number of regimes and the autoregressive orders, which\ncan be chosen using the Deviance Information Criterion (DIC) and\/or the\nWatanabe-Akaike Information Criterion (WAIC). A library for the language and\nenvironment for statistical computing R was also developed to assess the\neffectiveness of the proposed methodology using simulation studies and analysis\nof two real multivariate time series.",
        "In a mass partition problem, we are interested in finding equitable\npartitions of smooth measures in $\\mathbb{R}^d$. In this manuscript, we study\nthe problem of finding simultaneous bisections of measures using scaled copies\nof a prescribed set $K$. We distinguish the problem when we are allowed to use\nscaled and translated copies of $K$ and the problem when we are allowed to use\nscaled isometric copies of $K$. These problems have only previously been\nstudied if $K$ is a half-space or a Euclidean ball. We obtain positive results\nfor simultaneous bisection of any $d+1$ masses for star-shaped compact sets $K$\nwith non-empty interior, where the conditions on the problem depend on the\nsmoothness of the boundary of $K$. Additional proofs are included for\nparticular instances of $K$, such as hypercubes and cylinders, answering\npositively a conjecture of Sober\\'on and Takahashi. The proof methods are\ntopological and involve new Borsuk--Ulam-type theorems.",
        "Many data clustering applications must handle objects that cannot be\nrepresented as vector data. In this context, the bag-of-vectors representation\ncan be leveraged to describe complex objects through discrete distributions,\nand the Wasserstein distance can effectively measure the dissimilarity between\nthem. Additionally, kernel methods can be used to embed data into feature\nspaces that are easier to analyze. Despite significant progress in data\nclustering, a method that simultaneously accounts for distributional and\nvectorial dissimilarity measures is still lacking. To tackle this gap, this\nwork explores kernel methods and Wasserstein distance metrics to develop a\ncomputationally tractable clustering framework. The compositional properties of\nkernels allow the simultaneous handling of different metrics, enabling the\nintegration of both vectors and discrete distributions for object\nrepresentation. This approach is flexible enough to be applied in various\ndomains, such as graph analysis and image processing. The framework consists of\nthree main components. First, we efficiently approximate pairwise Wasserstein\ndistances using multiple reference distributions. Second, we employ kernel\nfunctions based on Wasserstein distances and present ways of composing kernels\nto express different types of information. Finally, we use the kernels to\ncluster data and evaluate the quality of the results using scalable and\ndistance-agnostic validity indices. A case study involving two datasets of 879\nand 34,920 power distribution graphs demonstrates the framework's effectiveness\nand efficiency.",
        "Synchronizing words in classical automata theory provide a mechanism to reset\nany state of a deterministic automaton to a specific target state via a\ncarefully chosen finite sequence of transition rules. In this work, we extend\nthe concept of synchronizing words to quantum information theory. Specifically,\nwe show that with only two quantum channels, it is possible to bring an\narbitrary qutrit state close to a designated target state. Furthermore, we\ndemonstrate that following this reset, any pure real qutrit state can be\nclosely approximated using the same two channels. These findings establish a\nquantum analogue of synchronizing words, highlighting their potential\napplications in constructing minimal sets of universal quantum gates capable of\nboth resetting and preparing arbitrary states.",
        "In this paper, we develop an implementation of cross-validation for penalized\nlinear mixed models. While these models have been proposed for correlated\nhigh-dimensional data, the current literature implicitly assumes that tuning\nparameter selection procedures developed for independent data will also work\nwell in this context. We argue that such naive assumptions make analysis prone\nto pitfalls, several of which we will describe. Here we present a correct\nimplementation of cross-validation for penalized linear mixed models,\naddressing these common pitfalls. We support our methods with mathematical\nproof, simulation study, and real data analysis.",
        "We study the non-compact Sobolev embeddings into the optimal scale of Lorentz\nspaces, $W_0^mL^{p,q}(\\Omega) \\to L^{\\frac{dp}{d - mp},r}(\\Omega)$, where\n$\\Omega \\subseteq \\mathbb{R}^d$, $1 \\le m \\le d$ and $0<q<r\\le\\infty$ with\n$1<p<\\frac dm$ or $p=q=1$. We show that these embeddings are finitely strictly\nsingular with certain upper bounds on the decay rate of the Bernstein numbers.\nWe reduce the Sobolev embeddings to embeddings of Besov spaces and sequence\nspaces, which simplifies the previous methods by Bourgain-Gromov and\nLang-Mihula.",
        "Rare earth permanent magnets (REPMs) with both localized moments and\nitinerant conduction bands are not only important for fundamental research but\nalso have significant technological applications. In particular, Sm$_{\\rm\n2}$Co$_{\\rm 17}$ is a prototypical high-temperture REPM, where the Co atoms\nform a kagome-honeycomb stacked lattice. Here we report synthesis of epitaxial\nSm$_{\\rm 2}$Co$_{\\rm 17}$ films using molecular beam epitaxy and measurements\nof their momentum-resolved electronic structure from \\textit{in-situ}\nangle-resolved photoemission spectroscopy. Our results unveil two flat bands\nfrom Co $3d$ orbitals near the Fermi level ($E_F$), one at $\\sim$-300meV and\nanother right at $E_F$, which arise from orbital-selective destructive\ninterference and strong electron correlations, respectively. In addition, our\nresults reveal that Sm $4f$ states are far away from $E_F$ (hence mostly\nlocalized) and exhibit an anomalous temperature dependence, caused by the\n$3d$-$4f$ magnetic coupling. Our findings provide direct spectroscopic insights\nto understand the strong uniaxial ferromagnetism in Sm$_{\\rm 2}$Co$_{\\rm 17}$\n(and REPMs alike). Our work also opens avenues to explore flat-band physics\nnear $E_F$ and emergent phenomena in correlated kagome-honeycomb lattices.",
        "The standard description of the nucleon in the non-relativistic quark model\nis an $1S,L=0$ state without orbital motion. Yet, there are several indications\nfrom phenomenology that an admixture of states with nonzero orbital motion\nmaybe substantial. In this paper we focus on the ``second shell\" of the nucleon\nexcitations (D-shell), for which we give a modern description of the wave\nfunctions. We follow it by investigating what we call a ``maximal mixing\"\nscenario, assuming a hypothetical long-range tensor force.\n  We give the explicit wave functions for all states, before and after mixing,\nand re-assess many predictions such as the magnetic moments, the standard and\ntransitional form-factors from the nucleon to $N^*$. Unexpectedly, in this\nscenario we can reproduce the long-puzzling features of the Roper resonance\n$N^*( 1440)$. But even in this extreme case, the\n  admixture of the $1D,L=2$ state to a nucleon remains significantly smaller\n  than expected from phenomenology.",
        "Presence of excitons significantly influence the optoelectronic properties\nand potential applications of materials. Using combined theoretical and\nexperimental tools, we investigate the absorption spectra of $d^0$ double\nperovskite oxides Ba$_{2}$Y$B'$O$_6$ ($B'$ = Nb, Ta, Sb), Ba$_{2}$Sc$B'$O$_6$\n($B'$ = Ta, Sb) and $A_{2}$ScSbO$_6$ ($A$=Ca, Sr, Ba), allowing for a\nsystematic variation of composition. We not only show that low-energy excitons\nare present in the visible range in {\\it all} the considered so-called wide-gap\ninsulators, but also that the nature and properties of these exciton modes\ndiffer from those in double perovskite halides as well as perovskite oxides. We\nprovide insights on the origin of such differences by a detailed and\ncomparative analysis of the electronic structure. Further, our findings\nelucidate possible correlations between the exciton properties and the\ncomposition, via the electronic structure, towards a comprehensive\nunderstanding of correlation effects and rational design principles."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"On the Distribution of the Two-Sample Cramer-von Mises Criterion",
    "start_abstract":"The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",
    "start_categories":[
      "q-fin.GN"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Quant GANs: deep generation of financial time series"
      ],
      "abstract":[
        "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "SuperPC: A Single Diffusion Model for Point Cloud Completion,\n  Upsampling, Denoising, and Colorization",
        "Adaptive H&E-IHC information fusion staining framework based on feature\n  extra",
        "Siren Song: Manipulating Pose Estimation in XR Headsets Using Acoustic\n  Attacks",
        "Illuminating Spaces: Deep Reinforcement Learning and Laser-Wall\n  Partitioning for Architectural Layout Generation",
        "GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\n  Benchmarking",
        "Meta-Instance Selection. Instance Selection as a Classification Problem\n  with Meta-Features",
        "Adjustable picometer-stable interferometers for testing space-based\n  gravitational wave detectors",
        "Asymmetric Dark Matter in SUSY with approximate $R-$symmetry",
        "The Solar System's passage through the Radcliffe wave during the middle\n  Miocene",
        "Improving SAM for Camouflaged Object Detection via Dual Stream Adapters",
        "Addressing Popularity Bias in Third-Party Library Recommendations Using\n  LLMs",
        "The thermal index of neutron-star matter in the virial approximation",
        "Ditto: Accelerating Diffusion Model via Temporal Value Similarity",
        "Symmetry Topological Field Theory for Flavor Symmetry",
        "Cost-Effective Single-Antenna RSSI Positioning Through Dynamic Radiation\n  Pattern Analysis",
        "Who Writes What: Unveiling the Impact of Author Roles on AI-generated\n  Text Detection",
        "Binary Bosonic Mixtures with Pair Hopping in Synthetic Dimension: Phase\n  Transitions and Demixing Effects",
        "SN1987A bounds on neutrino quantum decoherence",
        "Optimal Brain Iterative Merging: Mitigating Interference in LLM Merging",
        "Generalizing Semi-$n$-Potent Rings",
        "Neutron versus proton scattering on exotic nuclei: the $^9$He example",
        "Freidlin-Wentzell type exit-time estimates for time-inhomogeneous\n  diffusions and their applications",
        "AutoDeduct: A Tool for Automated Deductive Verification of C Code",
        "Climate And Resource Awareness is Imperative to Achieving Sustainable AI\n  (and Preventing a Global AI Arms Race)",
        "Crossover-BPSO Driven Multi-Agent Technology for Managing Local Energy\n  Systems",
        "Reevaluating Policy Gradient Methods for Imperfect-Information Games",
        "Unconditional foundations for supersingular isogeny-based cryptography",
        "Multi-Transmission Node DER Aggregation: Chance-Constrained Unit\n  Commitment with Bounded Hetero-Dimensional Mixture Model for Uncertain\n  Distribution Factors",
        "On the time complexity of finding a well-spread perfect matching in\n  bridgeless cubic graphs"
      ],
      "abstract":[
        "Point cloud (PC) processing tasks-such as completion, upsampling, denoising,\nand colorization-are crucial in applications like autonomous driving and 3D\nreconstruction. Despite substantial advancements, prior approaches often\naddress each of these tasks independently, with separate models focused on\nindividual issues. However, this isolated approach fails to account for the\nfact that defects like incompleteness, low resolution, noise, and lack of color\nfrequently coexist, with each defect influencing and correlating with the\nothers. Simply applying these models sequentially can lead to error\naccumulation from each model, along with increased computational costs. To\naddress these challenges, we introduce SuperPC, the first unified diffusion\nmodel capable of concurrently handling all four tasks. Our approach employs a\nthree-level-conditioned diffusion framework, enhanced by a novel\nspatial-mix-fusion strategy, to leverage the correlations among these four\ndefects for simultaneous, efficient processing. We show that SuperPC\noutperforms the state-of-the-art specialized models as well as their\ncombination on all four individual tasks.",
        "Immunohistochemistry (IHC) staining plays a significant role in the\nevaluation of diseases such as breast cancer. The H&E-to-IHC transformation\nbased on generative models provides a simple and cost-effective method for\nobtaining IHC images. Although previous models can perform digital coloring\nwell, they still suffer from (i) coloring only through the pixel features that\nare not prominent in HE, which is easy to cause information loss in the\ncoloring process; (ii) The lack of pixel-perfect H&E-IHC groundtruth pairs\nposes a challenge to the classical L1 loss.To address the above challenges, we\npropose an adaptive information enhanced coloring framework based on feature\nextractors. We first propose the VMFE module to effectively extract the color\ninformation features using multi-scale feature extraction and wavelet transform\nconvolution, while combining the shared decoder for feature fusion. The\nhigh-performance dual feature extractor of H&E-IHC is trained by contrastive\nlearning, which can effectively perform feature alignment of HE-IHC in high\nlatitude space. At the same time, the trained feature encoder is used to\nenhance the features and adaptively adjust the loss in the HE section staining\nprocess to solve the problems related to unclear and asymmetric information. We\nhave tested on different datasets and achieved excellent performance.Our code\nis available at https:\/\/github.com\/babyinsunshine\/CEFF",
        "Extended Reality (XR) experiences involve interactions between users, the\nreal world, and virtual content. A key step to enable these experiences is the\nXR headset sensing and estimating the user's pose in order to accurately place\nand render virtual content in the real world. XR headsets use multiple sensors\n(e.g., cameras, inertial measurement unit) to perform pose estimation and\nimprove its robustness, but this provides an attack surface for adversaries to\ninterfere with the pose estimation process. In this paper, we create and study\nthe effects of acoustic attacks that create false signals in the inertial\nmeasurement unit (IMU) on XR headsets, leading to adverse downstream effects on\nXR applications. We generate resonant acoustic signals on a HoloLens 2 and\nmeasure the resulting perturbations in the IMU readings, and also demonstrate\nboth fine-grained and coarse attacks on the popular ORB-SLAM3 and an\nopen-source XR system (ILLIXR). With the knowledge gleaned from attacking these\nopen-source frameworks, we demonstrate four end-to-end proof-of-concept attacks\non a HoloLens 2: manipulating user input, clickjacking, zone invasion, and\ndenial of user interaction. Our experiments show that current commercial XR\nheadsets are susceptible to acoustic attacks, raising concerns for their\nsecurity.",
        "Space layout design (SLD), occurring in the early stages of the design\nprocess, nonetheless influences both the functionality and aesthetics of the\nultimate architectural outcome. The complexity of SLD necessitates innovative\napproaches to efficiently explore vast solution spaces. While image-based\ngenerative AI has emerged as a potential solution, they often rely on\npixel-based space composition methods that lack intuitive representation of\narchitectural processes. This paper leverages deep Reinforcement Learning (RL),\nas it offers a procedural approach that intuitively mimics the process of human\ndesigners. Effectively using RL for SLD requires an explorative space composing\nmethod to generate desirable design solutions. We introduce \"laser-wall\", a\nnovel space partitioning method that conceptualizes walls as emitters of\nimaginary light beams to partition spaces. This approach bridges vector-based\nand pixel-based partitioning methods, offering both flexibility and exploratory\npower in generating diverse layouts. We present two planning strategies:\none-shot planning, which generates entire layouts in a single pass, and dynamic\nplanning, which allows for adaptive refinement by continuously transforming\nlaser-walls. Additionally, we introduce on-light and off-light wall\ntransformations for smooth and fast layout refinement, as well as identity-less\nand identity-full walls for versatile room assignment. We developed\nSpaceLayoutGym, an open-source OpenAI Gym compatible simulator for generating\nand evaluating space layouts. The RL agent processes the input design scenarios\nand generates solutions following a reward function that balances geometrical\nand topological requirements. Our results demonstrate that the RL-based\nlaser-wall approach can generate diverse and functional space layouts that\nsatisfy both geometric constraints and topological requirements and is\narchitecturally intuitive.",
        "Large Vision-Language Models (LVLMs) have recently gained attention due to\ntheir distinctive performance and broad applicability. While it has been\npreviously shown that their efficacy in usage scenarios involving non-Western\ncontexts falls short, existing studies are limited in scope, covering just a\nnarrow range of cultures, focusing exclusively on a small number of cultural\naspects, or evaluating a limited selection of models on a single task only.\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\nacross 144 countries representing six global macro-regions. GIMMICK comprises\nsix tasks built upon three new datasets that span 728 unique cultural events or\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\nand 26 open-weight models of all sizes. We systematically examine (1) regional\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\nexternal cues. Our analyses reveal strong biases toward Western cultures across\nmodels and tasks and highlight strong correlations between model size and\nperformance, as well as the effectiveness of multimodal input and external\ngeographic cues. We further find that models have more knowledge of tangible\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\nrecognizing broad cultural origins but struggle with a more nuanced\nunderstanding.",
        "Data pruning, or instance selection, is an important problem in machine\nlearning especially in terms of nearest neighbour classifier. However, in data\npruning which speeds up the prediction phase, there is an issue related to the\nspeed and efficiency of the process itself. In response, the study proposes an\napproach involving transforming the instance selection process into a\nclassification task conducted in a unified meta-feature space where each\ninstance can be classified and assigned to either the \"to keep\" or \"to remove\"\nclass. This approach requires training an appropriate meta-classifier, which\ncan be developed based on historical instance selection results from other\ndatasets using reference instance selection methods as a labeling tool. This\nwork proposes constructing the meta-feature space based on properties extracted\nfrom the nearest neighbor graph. Experiments conducted on 17 datasets of\nvarying sizes and five reference instance selection methods (ENN, Drop3, ICF,\nHMN-EI, and CCIS) demonstrate that the proposed solution achieves results\ncomparable to reference instance selection methods while significantly reducing\ncomputational complexity. In the proposed approach, the computational\ncomplexity of the system depends only on identifying the k-nearest neighbors\nfor each data sample and running the meta-classifier. Additionally, the study\ndiscusses the choice of meta-classifier, recommending the use of Balanced\nRandom Forest.",
        "Space-based gravitational wave detectors, such as the Laser Interferometer\nSpace Antenna (LISA), use picometer-precision laser interferometry to detect\ngravitational waves at frequencies from 1 Hz down to below 0.1 mHz. Laser\ninterferometers used for on-ground prototyping and testing of such instruments\nare typically constructed by permanently bonding or gluing optics onto an\nultra-stable bench made of low-expansion glass ceramic. This design minimizes\ntemperature coupling to length and tilt, which dominates the noise at low\nfrequencies due to finite temperature stability achievable in laboratories and\nvacuum environments. Here, we present the study of an alternative\nopto-mechanical concept where optical components are placed with adjustable and\nfreely positionable mounts on an ultra-stable bench, while maintaining\npicometer length stability. With this concept, a given interferometer\nconfiguration can be realised very quickly due to a simplified and speed-up\nassembly process, reducing the realisation time from weeks or months to a\nmatter of hours. We built a corresponding test facility and verified the length\nstability of our concept by measuring the length change in an optical cavity\nthat was probed with two different locking schemes, heterodyne laser frequency\nstabilisation and Pound-Drever-Hall locking. We studied the limitations of both\nlocking schemes and verified that the cavity length noise is below 1\npm\/sqrt(Hz) for frequencies down to 3 mHz. We thereby demonstrate that our\nconcept can simplify the testing of interferometer configurations and\nopto-mechanical components and is suitable to realise flexible optical ground\nsupport equipment for space missions that use laser interferometry, such as\nfuture space-based gravitational wave detectors and satellite geodesy missions.",
        "We implement the asymmetric dark matter framework, linking the ordinary and\ndark matter abundances, within a supersymmetric context. We consider a\nsupersymmetric model that respects an approximate $U(1)_R$ symmetry, which is\nbroken in such a way that at high temperature the $R$ breaking sector mediate\nprocesses in equilibrium, but at the SUSY mass scale, the sparticles asymmetry\nis frozen. In this framework, the gravitino serves as the dark matter\ncandidate, and its mass is predicted to be $\\sim10$ GeV to match the observed\nrelic abundance. We identify several realistic spectra; however, the\nrequirement for the Next-to-Lightest Supersymmetric Particle (NLSP) to decay\ninto the gravitino before Big Bang Nucleosynthesis constrains the viable\nspectrum to masses above 2 TeV.",
        "Context. As the Solar System orbits the Milky Way, it encounters various\nGalactic environments, including dense regions of the interstellar medium\n(ISM). These encounters can compress the heliosphere, exposing parts of the\nSolar System to the ISM, while also increasing the influx of interstellar dust\ninto the Solar System and Earth's atmosphere. The discovery of new Galactic\nstructures, such as the Radcliffe wave, raises the question of whether the Sun\nhas encountered any of them. Aims. The present study investigates the potential\npassage of the Solar System through the Radcliffe wave gas structure over the\npast 30 million years (Myr). Methods. We used a sample of 56 high-quality,\nyoung ($\\leq$ 30 Myr) open clusters associated with a region of interest of the\nRadcliffe wave to trace its motion back and investigate a potential crossing\nwith the Solar System's past orbit. Results. We find that the Solar System's\ntrajectory intersected the Radcliffe wave in the Orion region. We have\nconstrained the timing of this event to between 18.2 and 11.5 Myr ago, with the\nclosest approach occurring between 14.8 and 12.4 Myr ago. Notably, this period\ncoincides with the Middle Miocene climate transition on Earth, providing an\ninterdisciplinary link with paleoclimatology. The potential impact of the\ncrossing of the Radcliffe wave on the climate on Earth is estimated. This\ncrossing could also lead to anomalies in radionuclide abundances, which is an\nimportant research topic in the field of geology and nuclear astrophysics.",
        "Segment anything model (SAM) has shown impressive general-purpose\nsegmentation performance on natural images, but its performance on camouflaged\nobject detection (COD) is unsatisfactory. In this paper, we propose SAM-COD\nthat performs camouflaged object detection for RGB-D inputs. While keeping the\nSAM architecture intact, dual stream adapters are expanded on the image encoder\nto learn potential complementary information from RGB images and depth images,\nand fine-tune the mask decoder and its depth replica to perform dual-stream\nmask prediction. In practice, the dual stream adapters are embedded into the\nattention block of the image encoder in a parallel manner to facilitate the\nrefinement and correction of the two types of image embeddings. To mitigate\nchannel discrepancies arising from dual stream embeddings that do not directly\ninteract with each other, we augment the association of dual stream embeddings\nusing bidirectional knowledge distillation including a model distiller and a\nmodal distiller. In addition, to predict the masks for RGB and depth attention\nmaps, we hybridize the two types of image embeddings which are jointly learned\nwith the prompt embeddings to update the initial prompt, and then feed them\ninto the mask decoders to synchronize the consistency of image embeddings and\nprompt embeddings. Experimental results on four COD benchmarks show that our\nSAM-COD achieves excellent detection performance gains over SAM and achieves\nstate-of-the-art results with a given fine-tuning paradigm.",
        "Recommender systems for software engineering (RSSE) play a crucial role in\nautomating development tasks by providing relevant suggestions according to the\ndeveloper's context. However, they suffer from the so-called popularity bias,\ni.e., the phenomenon of recommending popular items that might be irrelevant to\nthe current task. In particular, the long-tail effect can hamper the system's\nperformance in terms of accuracy, thus leading to false positives in the\nprovided recommendations. Foundation models are the most advanced generative\nAI-based models that achieve relevant results in several SE tasks.\n  This paper aims to investigate the capability of large language models (LLMs)\nto address the popularity bias in recommender systems of third-party libraries\n(TPLs). We conduct an ablation study experimenting with state-of-the-art\ntechniques to mitigate the popularity bias, including fine-tuning and\npopularity penalty mechanisms. Our findings reveal that the considered LLMs\ncannot address the popularity bias in TPL recommenders, even though fine-tuning\nand post-processing penalty mechanism contributes to increasing the overall\ndiversity of the provided recommendations. In addition, we discuss the\nlimitations of LLMs in this context and suggest potential improvements to\naddress the popularity bias in TPL recommenders, thus paving the way for\nadditional experiments in this direction.",
        "Motivated by gravitational wave observations of binary neutron-star mergers,\nwe study the thermal index of low-density, high-temperature dense matter. We\nuse the virial expansion to account for nuclear interaction effects. We focus\non the region of validity of the expansion, which reaches $10^{-3}$ fm$^{-3}$\nat $T=5$ MeV up to almost saturation density at $T=50$ MeV. In pure neutron\nmatter, we find an analytical expression for the thermal index, and show that\nit is nearly density- and temperature-independent, within a fraction of a\npercent of the non-interacting, non-relativistic value of $\\Gamma_\\text{th}\n\\approx 5\/3$. When we incorporate protons, electrons and photons, we find that\nthe density and temperature dependence of the thermal index changes\nsignificantly. We predict a smooth transition between an electron-dominated\nregime with $\\Gamma_\\text{th} \\approx 4\/3$ at low densities to a\nneutron-dominated region with $\\Gamma_\\text{th} \\approx 5\/3$ at high densities.\nThis behavior is by and large independent of proton fraction and is not\naffected by nuclear interactions in the region where the virial expansion\nconverges. We model this smooth transition analytically and provide a simple\nbut accurate parametrization of the inflection point between these regimes.\nWhen compared to tabulated realistic models of the thermal index, we find an\noverall agreement at high temperatures that weakens for colder matter. The\ndiscrepancies can be attributed to the missing contributions of nuclear\nclusters. The virial approximation provides a clear and physically intuitive\nframework for understanding the thermal properties of dense matter, offering a\ncomputationally efficient solution that makes it particularly well-suited for\nthe regimes relevant to neutron star binary remnants.",
        "Diffusion models achieve superior performance in image generation tasks.\nHowever, it incurs significant computation overheads due to its iterative\nstructure. To address these overheads, we analyze this iterative structure and\nobserve that adjacent time steps in diffusion models exhibit high value\nsimilarity, leading to narrower differences between consecutive time steps. We\nadapt these characteristics to a quantized diffusion model and reveal that the\nmajority of these differences can be represented with reduced bit-width, and\neven zero. Based on our observations, we propose the Ditto algorithm, a\ndifference processing algorithm that leverages temporal similarity with\nquantization to enhance the efficiency of diffusion models. By exploiting the\nnarrower differences and the distributive property of layer operations, it\nperforms full bit-width operations for the initial time step and processes\nsubsequent steps with temporal differences. In addition, Ditto execution flow\noptimization is designed to mitigate the memory overhead of temporal difference\nprocessing, further boosting the efficiency of the Ditto algorithm. We also\ndesign the Ditto hardware, a specialized hardware accelerator, fully exploiting\nthe dynamic characteristics of the proposed algorithm. As a result, the Ditto\nhardware achieves up to 1.5x speedup and 17.74% energy saving compared to other\naccelerators.",
        "In this Letter, we demonstrate that the Symmetry Topological Field Theory\n(SymTFT) associated to a Quantum Field Theory (QFT) with continuous non-abelian\n$G$-flavor symmetry is a $BF$-theory with gauge group $G$. We show that gauging\n$G$-symmetry with a flat connection yields a theory with global symmetry\ncharacterized by exchanging the conjugate variables in the quantization of\n$BF$-theory. We construct the extended operators that generate the $G$-flavor\nsymmetry and the $(d-2)$-form $\\text{Rep}(G)$-symmetry of the gauged QFT. We\ndemonstrate that $BF$-theory arises as the theory characterizing $G$-flavor\nsymmetry of a QFT in the AdS\/CFT setup. 't Hooft anomalies of the $G$-flavor\nsymmetry are realized as extra terms in the action.",
        "This paper presents a novel indoor positioning approach that leverages\nantenna radiation pattern characteristics through Received Signal Strength\nIndication (RSSI) measurements in a single-antenna system. By rotating the\nantenna or reconfiguring its radiation pattern, we derive a maximum likelihood\nestimation (MLE) algorithm that achieves near-optimal positioning accuracy\napproaching the Cramer-Rao lower bound (CRLB). Through theoretical analysis, we\nestablish three fundamental theorems characterizing the estimation accuracy\nbounds and demonstrating how performance improves with increased\nsignal-to-noise ratio, antenna rotation count, and radiation pattern\nvariations. Additionally, we propose a two-position measurement strategy that\neliminates dependence on receiving antenna patterns. Simulation results\nvalidate that our approach provides an effective solution for indoor robot\ntracking applications where both accuracy and system simplicity are essential\nconsiderations.",
        "The rise of Large Language Models (LLMs) necessitates accurate AI-generated\ntext detection. However, current approaches largely overlook the influence of\nauthor characteristics. We investigate how sociolinguistic attributes-gender,\nCEFR proficiency, academic field, and language environment-impact\nstate-of-the-art AI text detectors. Using the ICNALE corpus of human-authored\ntexts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous\nevaluation employing multi-factor ANOVA and weighted least squares (WLS). Our\nresults reveal significant biases: CEFR proficiency and language environment\nconsistently affected detector accuracy, while gender and academic field showed\ndetector-dependent effects. These findings highlight the crucial need for\nsocially aware AI text detection to avoid unfairly penalizing specific\ndemographic groups. We offer novel empirical evidence, a robust statistical\nframework, and actionable insights for developing more equitable and reliable\ndetection systems in real-world, out-of-domain contexts. This work paves the\nway for future research on bias mitigation, inclusive evaluation benchmarks,\nand socially responsible LLM detectors.",
        "We employ the cluster Gutzwiller mean-field method to investigate the\nground-state phase diagrams and demixing effects in binary boson mixtures with\npair hopping in synthetic dimensions. Our study reveals two novel interspecies\npaired superfluid phases: the paired super-counter-fluid (PSCF) phase,\nfeaturing pairs of two particles of one species and two holes of the other, and\nthe SCF* phase, which combines PSCF and super-counter-fluid (SCF) orders. These\nphases provide new insights into XY ferromagnet states from a pseudo-spin\nperspective, with SCF* and PSCF states corresponding to different XY\nferromagnet phases depending on particle filling. We also identify a quantum\nquadruple critical point in the interexchange asymmetric case. Importantly, we\ndemonstrate that the mixed-demixed critical point is phase-dependent due to\npairing hopping, differing from normal two-component bosonic systems.\nExperimental schemes to observe these novel phases are proposed.",
        "We obtain stringent bounds on neutrino quantum decoherence from the analysis\nof SN1987A data. We show that for the decoherence model considered here, which\nallows for neutrino-loss along the trajectory, the bounds are many orders of\nmagnitude stronger than the ones that can be obtained from the analysis of data\nfrom reactor neutrino oscillation experiments or neutrino telescopes.",
        "Large Language Models (LLMs) have demonstrated impressive capabilities, but\ntheir high computational costs pose challenges for customization. Model merging\noffers a cost-effective alternative, yet existing methods suffer from\ninterference among parameters, leading to performance degradation. In this\nwork, we propose Optimal Brain Iterative Merging (OBIM), a novel method\ndesigned to mitigate both intra-model and inter-model interference. OBIM\nconsists of two key components: (1) A saliency measurement mechanism that\nevaluates parameter importance based on loss changes induced by individual\nweight alterations, reducing intra-model interference by preserving only\nhigh-saliency parameters. (2) A mutually exclusive iterative merging framework,\nwhich incrementally integrates models using a binary mask to avoid direct\nparameter averaging, thereby mitigating inter-model interference. We validate\nOBIM through experiments on both Supervised Fine-Tuned (SFT) models and\npost-pretrained checkpoints. The results show that OBIM significantly\noutperforms existing merging techniques. Overall, OBIM provides an effective\nand practical solution for enhancing LLM merging.",
        "We define and explore the class of rings $R$ for which each element in $R$ is\na sum of a tripotent element from $R$ and an element from the subring\n$\\Delta(R)$ of $R$ which commute each other. Succeeding to obtain a complete\ndescription of these rings modulo their Jacobson radical as the direct product\nof a Boolean ring and a Yaqub ring, our results somewhat generalize those\nestablished by Ko\\c{s}an-Yildirim-Zhou in Can. Math. Bull. (2019).",
        "Neutron scattering on exotic nuclides is a class of processes which can not\nbe studied directly now and in any observable future. Resonance proton\nscattering of exotic nuclide on a thick target in inverse kinematics can be\nused to infer the properties of the low-energy neutron scattering of this\nnuclide assuming the isobaric symmetry. However, the results of such resonance\nproton scattering reactions are so far analyzed in theoretical approaches\n(optical, R-matrix models), which are missing important aspects of isospin\ndynamics, isospin violation in continuum and threshold dynamics. The isospin\nconserving coupled-channel model (ICM) is proposed, which provides a more\nreliable basis for understanding of such experimental studies. Qualitatively\ndifferent phase shifts for the $^{8}$He+$p$ $T=5\/2$ and $T=3\/2$ resonances are\npredicted by ICM with quite unusual profile for the $T=5\/2$ states. Alternative\ninterpretation of the existing $^{8}$He+$p$ data is proposed. The observable\nproperties of the $T=5\/2$ resonances may be strongly affected by the\nisobaric-partner $T=3\/2$ states. Crucial importance of studies of the\nneutron-emission channel for disentangling this possible influence is\ndemonstrated.",
        "This paper investigates the exit-time problem for time-inhomogeneous\ndiffusion processes. The focus is on the small-noise behavior of the exit time\nfrom a bounded positively invariant domain. We demonstrate that, when the drift\nand diffusion terms are uniformly close to some time-independent functions, the\nexit time grows exponentially both in probability and in $L_1$ as a parameter\nthat controls the noise tends to zero. We also characterize the exit position\nof the time-inhomogeneous process. Additionally, we investigate the impact of\nrelaxing the uniform closeness condition on the exit-time behavior. As an\napplication, we extend these results to the McKean-Vlasov process. Our findings\nimprove upon existing results in the literature for the exit-time problem for\nthis class of processes.",
        "Deductive verification has become a mature paradigm for the verification of\nindustrial software. Applying deductive verification, however, requires that\nevery function in the code base is annotated with a function contract\nspecifying its behaviour. This introduces a large overhead of manual work. To\naddress this challenge, we introduce the AutoDeduct toolchain, built on top of\nthe Frama-C framework. It implements a combination of techniques to\nautomatically infer contracts for functions in C programs, in the syntax of\nACSL, the specification language of Frama-C. Contract inference in AutoDecuct\nis implemented as two plugins for Frama-C, each inferring different types of\nannotations. We assume that programs have an entry-point function already\nequipped with a contract, which is used in conjunction with the program source\ncode to infer contracts for the helper functions, so that the entry-point\ncontract can be verified. The current release of AutoDeduct is the first public\nprototype, which we evaluate on an example adapted from industrial software.",
        "Sustainability encompasses three key facets: economic, environmental, and\nsocial. However, the nascent discourse that is emerging on sustainable\nartificial intelligence (AI) has predominantly focused on the environmental\nsustainability of AI, often neglecting the economic and social aspects.\nAchieving truly sustainable AI necessitates addressing the tension between its\nclimate awareness and its social sustainability, which hinges on equitable\naccess to AI development resources. The concept of resource awareness advocates\nfor broader access to the infrastructure required to develop AI, fostering\nequity in AI innovation. Yet, this push for improving accessibility often\noverlooks the environmental costs of expanding such resource usage. In this\nposition paper, we argue that reconciling climate and resource awareness is\nessential to realizing the full potential of sustainable AI. We use the\nframework of base-superstructure to analyze how the material conditions are\ninfluencing the current AI discourse. We also introduce the Climate and\nResource Aware Machine Learning (CARAML) framework to address this conflict and\npropose actionable recommendations spanning individual, community, industry,\ngovernment, and global levels to achieve sustainable AI.",
        "This article presents a new hybrid algorithm, crossover binary particle swarm\noptimization (crBPSO), for allocating resources in local energy systems via\nmulti-agent (MA) technology. Initially, a hierarchical MA-based architecture in\na grid-connected local energy setup is presented. In this architecture, task\nspecific agents operate in a master-slave manner. Where, the master runs a\nwell-formulated optimization routine aiming at minimizing costs of energy\nprocurement, battery degradation, and load scheduling delay. The slaves update\nthe master on their current status and receive optimal action plans\naccordingly. Simulation results demonstrate that the proposed algorithm\noutperforms selected existing ones by 21\\% in terms average energy system costs\nwhile satisfying customers' energy demand and maintaining the required quality\nof service.",
        "In the past decade, motivated by the putative failure of naive self-play deep\nreinforcement learning (DRL) in adversarial imperfect-information games,\nresearchers have developed numerous DRL algorithms based on fictitious play\n(FP), double oracle (DO), and counterfactual regret minimization (CFR). In\nlight of recent results of the magnetic mirror descent algorithm, we\nhypothesize that simpler generic policy gradient methods like PPO are\ncompetitive with or superior to these FP, DO, and CFR-based DRL approaches. To\nfacilitate the resolution of this hypothesis, we implement and release the\nfirst broadly accessible exact exploitability computations for four large\ngames. Using these games, we conduct the largest-ever exploitability comparison\nof DRL algorithms for imperfect-information games. Over 5600 training runs, FP,\nDO, and CFR-based approaches fail to outperform generic policy gradient\nmethods. Code is available at https:\/\/github.com\/nathanlct\/IIG-RL-Benchmark and\nhttps:\/\/github.com\/gabrfarina\/exp-a-spiel .",
        "In this paper, we prove that the supersingular isogeny problem (Isogeny),\nendomorphism ring problem (EndRing) and maximal order problem (MaxOrder) are\nequivalent under probabilistic polynomial time reductions,\nunconditionally.Isogeny-based cryptography is founded on the presumed hardness\nof these problems, and their interconnection is at the heart of the design and\nanalysis of cryptosystems like the SQIsign digital signature scheme. Previously\nknown reductions relied on unproven assumptions such as the generalized Riemann\nhypothesis. In this work, we present unconditional reductions, and extend this\nnetwork of equivalences to the problem of computing the lattice of all\nisogenies between two supersingular elliptic curves (HomModule).For\ncryptographic applications, one requires computational problems to be hard on\naverage for random instances. It is well-known that if Isogeny is hard (in the\nworst case), then it is hard for random instances. We extend this result by\nproving that if any of the above-mentionned classical problems is hard in the\nworst case, then all of them are hard on average. In particular, if there exist\nhard instances of Isogeny, then all of Isogeny, EndRing, MaxOrder and HomModule\nare hard on average.",
        "To facilitate the integration of distributed energy resources (DERs) into the\nwholesale market while maintaining the tractability of associated market\noperation tools such as unit commitment (UC), existing DER aggregation (DERA)\nstudies usually consider that each DERA is presented on a single node of the\ntransmission network. Nevertheless, the increasing scale and geographical\ndistribution of DERs spur the emergence of DERAs covering multiple transmission\nnodes, posing new challenges in modeling such multi-transmission-node DERAs\n(M-DERAs). Indeed, assessing the aggregated impact of an M-DERA on power flows\nis a non-trivial task, because the sensitivities of each transmission line to\nDERs at different transmission nodes are not identical. Inspired by the\ndistribution factor (DF) based shift factor (SF) aggregation strategy in\nindustry practice, this paper proposes a novel DF-based chance-constrained UC\n(CCUC) model to determine system optimal operation plans with M-DERAs. DFs,\ntreated as uncertain parameters to describe possible responses of DERs against\naggregated dispatch instructions from regional transmission organizations, are\nmodeled via a bounded hetero-dimensional mixture model (BHMM) by leveraging\nhistorical DF records distributed on multiple hyperplanes in a bounded space.\nWith this, power flow limits are modeled as chance constraints in CCUC, which\nis reformulated into a scenarios-based stochastic form and solved by Benders\ndecomposition. The proposed method is tested on an IEEE 24-bus system to\nillustrate its effectiveness in managing M-DERA integration while ensuring\noperational economics and mitigating the overloading of transmission lines.",
        "We present an algorithm for finding a perfect matching in a\n$3$-edge-connected cubic graph that intersects every $3$-edge cut in exactly\none edge. Specifically, we propose an algorithm with a time complexity of $O(n\n\\log^4 n)$, which significantly improves upon the previously known\n$O(n^3)$-time algorithms for the same problem. The technique we use for the\nimprovement is efficient use of cactus model of 3-edge cuts. As an application,\nwe use our algorithm to compute embeddings of $3$-edge-connected cubic graphs\nwith limited number of singular edges (i.e., edges that are twice in the\nboundary of one face) in $O(n \\log^4 n)$ time; this application contributes to\nthe study of the well-known Cycle Double Cover conjecture."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Quant GANs: deep generation of financial time series",
    "start_abstract":"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
      ],
      "abstract":[
        "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
      ],
      "categories":[
        "q-fin.GN"
      ]
    },
    "list":{
      "title":[
        "Membrane phononic crystals for high-Qm mechanical defect modes in\n  piezoelectric aluminum nitride",
        "Self-induced Josephson oscillations and self-trapping in a supersolid\n  dipolar quantum gas",
        "An 'Experimental Mathematics' Approach to Stolarsky Interspersions via\n  Automata Theory",
        "A New Type of MPd5 Kagome Superconductors",
        "Observation of a zero-energy excitation mode in the open Dicke model",
        "Intra-neuronal attention within language models Relationships between\n  activation and semantics",
        "Quantifying the Impact of the Dust Torque on the Migration of Low-mass\n  Planets II: The Role of Pebble Accretion in Planet Growth within a Global\n  Planet Formation Model",
        "Terahertz Cavity Phonon Polaritons in Lead Telluride in the Deep-Strong\n  Coupling Regime",
        "Short-Pulse Driven Radiofrequency X-Band Photoinjector: Electromagnetic\n  Properties and Beam Dynamics in the Transient Regime",
        "Vote Delegation in DeFi Governance",
        "Partial Quantum Shadow Tomography for Structured Operators and its\n  Experimental Demonstration using NMR",
        "Empirical Constraints on Tidal Dissipation in Exoplanet Host Stars",
        "K-theoretic Tate-Poitou duality at prime 2",
        "Observation of the nonlinear chiral thermoelectric Hall effect in\n  tellurium",
        "Inf-sup condition for Stokes with outflow condition",
        "Statistical Uncertainties of Limit Cycle Systems in Langevin Bath",
        "Fabrication of self-powered photodetector materials based on Ni-doped\n  ZnO\/p-Si heterojunctions",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry",
        "A CFL condition for the finite cell method",
        "ASD Classification on Dynamic Brain Connectome using Temporal Random\n  Walk with Transformer-based Dynamic Network Embedding",
        "Birational geometry of the twofold symmetric product of a Hirzebruch\n  surface via secant maps",
        "An unstructured block-based adaptive mesh refinement approach for\n  explicit discontinuous Galerkin method",
        "Four-Qubit Variational Algorithms in Silicon Photonics with Integrated\n  Entangled Photon Sources",
        "Testing the bloated star hypothesis in the massive young stellar object\n  IRAS 19520+2759 through optical and infrared variability",
        "Ultrasensitive Higher-Order Exceptional Points via Non-Hermitian\n  Zero-Index Materials",
        "Electrically active defects in 3C, 4H and 6H silicon carbide polytypes:\n  A review",
        "Construction and Properties of the Ground State of Natural Phenomena",
        "New duality in choices of feature spaces via kernel analysis",
        "CosmoLink: Portable Coincidence Detector for On-Site Muon Flux\n  Measurement"
      ],
      "abstract":[
        "Nanomechanical resonators with exceptionally low dissipation are advancing\nmechanics-based sensors and quantum technologies. The key for these advances is\nthe engineering of localized phononic modes that are well-isolated from the\nenvironment, i.e., that exhibit a high mechanical quality factor, Qm. Membrane\nphononic crystals fabricated from strained thin films can realize high-Qm\nsingle or multiple localized phononic defect modes. These defect modes can be\nefficiently interfaced with out-of-plane light or capacitively coupled to a\nmicrowave quantum circuit, enabling readout and control of their motion. When\nmembrane phononic crystals are fabricated from a crystalline film, they could\noffer built-in functionality. We demonstrate a membrane phononic crystal\nrealized in a strained 90 nm-thin film of aluminum nitride (AlN), which is a\ncrystalline piezoelectric material. We engineer a high-Qm localized phononic\ndefect mode at 1.8 MHz with a Qxf-product of 1.5 10^13 Hz at room temperature.\nIn future devices, the built-in piezoelectricity of AlN can be utilized for\nin-situ tuning of mechanical mode frequencies, defect mode couplings, or\nacoustic bandgaps, which can be used as building blocks of tunable phononic\ncircuits.",
        "The Josephson effect characterizes superfluids and superconductors separated\nby a weak link, the so-called Josephson junction. A recent experiment has shown\nthat Josephson oscillations can be observed also in a supersolid, where the\nweak link is not due to an external barrier, but is self-induced by\ninterparticle interactions. Here we show theoretically that supersolids --\ndespite their self-induced character -- feature all the standard properties of\nbosonic Josephson junction arrays, including macroscopic quantum self-trapping.\nWe focus on the harmonically trapped dipolar supersolids of interest for\ncurrent experiments, and show that they can be described with a generalized\nJosephson model that takes into account spatial inhomogeneities. Our work\nshades new light on the dynamics of supersolids and opens the way to the study\nof a novel class of Josephson junctions.",
        "We look at the Stolarsky interspersions (such as the Wythoff array) one more\ntime, this time using tools from automata theory. These tools allow easy\nverification of many of the published results on these arrays, as well as\nproofs of new results.",
        "Kagome materials, which are composed of hexagons tiled with a shared\ntriangle, have inspired enormous interest due to their unique structures and\nrich physical properties, but exploring superconducting material systems with\nnew kagome structures is still an important research direction. Here, we\npredict a new type of kagome superconductors, MPd5 (M is a group IIA metal\nelement), and identify that they exhibit coexistence of superconductivity and\nnon-trivial topological properties. We uncover their phonon-mediated\nsuperconductivity by the density functional theory for superconductors (SCDFT),\npredicting the superconducting transition temperatures (Tc) of 2.64, 2.03, and\n1.50 K for CaPd5, SrPd5, and BaPd5, respectively, which can be effectively\ntuned by external pressure and electron doping. The present results also\ndemonstrate that MPd5 have various topological properties such as, CaPd5 shows\ntopological non-trivial intersection near the Fermi level (EF). Our results\nindicate that the MPd5 materials can be an emerging material platform with rich\nexotic physics in their kagome structures, and render themselves excellent\ncandidates for superconducting and advanced functional materials that could be\nutilized in topological quantum computing and information technology.",
        "Approaching phase boundaries in many-body systems can give rise to intriguing\nsignatures in their excitation spectra. Here, we explore the excitation\nspectrum of a Bose-Einstein condensate strongly coupled to an optical cavity\nand pumped by an optical standing wave, which simulates the famous\nDicke-Hepp-Lieb phase transition of the open Dicke model with dissipation\narising due to photon leakage from the cavity. For weak dissipation, the\nexcitation spectrum displays two strongly polaritonic modes. Close to the phase\nboundary, we observe an intriguing regime where the lower-energetic of these\nmodes, instead of showing the expected roton-type mode softening, is found to\napproach and persist at zero energy, well before the critical pump strength for\nthe Dicke-Hepp-Lieb transition boundary is reached. Hence, a peculiar situation\narises, where an excitation is possible at zero energy cost, but nevertheless\nno instability of the system is created.",
        "This study investigates the ability of perceptron-type neurons in language\nmodels to perform intra-neuronal attention; that is, to identify different\nhomogeneous categorical segments within the synthetic thought category they\nencode, based on a segmentation of specific activation zones for the tokens to\nwhich they are particularly responsive. The objective of this work is therefore\nto determine to what extent formal neurons can establish a homomorphic\nrelationship between activation-based and categorical segmentations. The\nresults suggest the existence of such a relationship, albeit tenuous, only at\nthe level of tokens with very high activation levels. This intra-neuronal\nattention subsequently enables categorical restructuring processes at the level\nof neurons in the following layer, thereby contributing to the progressive\nformation of high-level categorical abstractions.",
        "Although dust constitutes only about 1% of the mass of a protoplanetary disk,\nrecent studies demonstrate that it can exert a significant torque on low- and\nintermediate-mass planetary cores. We compute and quantify for the first time\nthe influence of the dust torque on the evolution of growing planetary embryos\nas they move in a protoplanetary disk while growing via gas and pebble\naccretion. Our global model evolves the gaseous disk via viscous accretion and\nX-ray photoevaporation, while accounting for dust growth and evolution\nincluding coagulation, drift, and fragmentation. Our research indicates that\ndust torque significantly influences planetary migration, particularly driving\nsubstantial outward migration for planets forming within the water ice-line.\nThis effect occurs due to an increased dust-to-gas mass ratio in the inner\ndisk, resulting from inward pebble drift from outer regions. In contrast, for\nplanets initially located beyond the water ice-line, the dust torque mitigates\ninward migration but does not significantly alter their paths, as the\ndust-to-gas ratio diminishes rapidly due to rapid pebble drift and the brief\ntimescales of planet formation in these areas. These findings underscore the\npivotal role of dust torque in shaping the migration patterns of low- and\nintermediate-mass planets, especially when enhanced dust concentrations in the\ninner disk amplify its effects",
        "Lead telluride is an important thermoelectric material due to its large\nSeebeck coefficient combined with its unusually low thermal conductivity that\nis related to the strong anharmonicity of phonons in this material. Here, we\nhave studied the resonant and nonperturbative coupling of transverse optical\nphonons in lead telluride with cavity photons inside small-mode-volume metallic\nmetasurface cavities that have photonic modes with terahertz frequencies. We\nobserved a giant vacuum Rabi splitting on the order of the bare phonon and\ncavity frequencies. Through terahertz time-domain spectroscopy experiments, we\nsystematically studied the vacuum Rabi splitting as a function of sample\nthickness, temperature, and cavity length. Under the strongest light-matter\ncoupling conditions, the strength of coupling exceeded the bare phonon and\ncavity frequencies, putting the system into the deep-strong coupling regime.\nThese results demonstrate that this uniquely tunable platform is promising for\nrealizing and understanding predicted cavity-vacuum-induced ferroelectric\ninstabilities and exploring applications of light-matter coupling in the\nultrastrong and deep-strong coupling regimes in quantum technology.",
        "This paper presents a study of the radiofrequency (RF) characteristics and\nbeam dynamics of an X-band photogun (Xgun) operating in the transient state.\nThe photoinjector is designed to operate with short RF pulses (9 ns) to achieve\nhigh accelerating gradients. Short-pulse operation potentially reduces\nbreakdown risks, as experimentally demonstrated by achieving a gradient\nexceeding 350 MV\/m. However, the short pulse duration causes the cavity to\noperate in a transient regime where the electromagnetic field deviates from the\nconventional steady-state condition. To investigate the effects of deviations\nfrom steady-state operation, the time-dependent spatial evolution of the cavity\nfields was examined using both 9-ns and 50-ns RF pulses. The 50-ns pulse served\nas a reference to characterize the cavity behavior under a fully filled\nsteady-state condition. Beam dynamics simulations were conducted to explore the\nimpact of transient RF effects on beam kinetic energy, transverse emittance,\nand bunch length; these simulations employed a new dynamic field mapping\napproach to model transient RF fields.",
        "We investigate the drivers of vote delegation in Decentralized Autonomous\nOrganizations (DAOs), using the Uniswap governance DAO as a laboratory. We show\nthat parties with fewer self-owned votes and those affiliated with the\ncontrolling venture capital firm, Andreesen Horowitz (a16z), receive more vote\ndelegations. These patterns suggest that while the Uniswap ecosystem values\ndecentralization, a16z may engage in window-dressing around it. Moreover, we\nfind that an active and successful track record in submitting improvement\nproposals, especially in the final stage, leads to more vote delegations,\nindicating that delegation in DAOs is at least partly reputation- or\nmerit-based. Combined, our findings provide new insights into how governance\nand decentralization operate in DeFi.",
        "Quantum shadow tomography based on the classical shadow representation\nprovides an efficient way to estimate properties of an unknown quantum state\nwithout performing a full quantum state tomography. In scenarios where\nestimating the expectation values for only certain classes of observables is\nrequired, obtaining information about the entire density matrix is unnecessary.\nWe propose a partial quantum shadow tomography protocol, which allows\nestimation of a subset of density matrix elements contributing to the\nexpectation values of certain classes of structured observables. This method\nutilizes tomographically incomplete subsets of single qubit Pauli basis\nmeasurements to perform partial shadow tomography, making it experimentally\nmore efficient. We demonstrate the advantage over unitary $k$-designs such as\nClifford, full Pauli basis, and methods utilizing mutually unbiased bases by\nnumerically analyzing the protocol for structured density matrices and\nobservables. We experimentally demonstrate the partial shadow estimation scheme\nfor a wide class of two-qubit states (pure, entangled, and mixed) in the\nnuclear magnetic resonance (NMR) platform, which relies on ensemble-based\nmeasurements. The full density matrix experimentally reconstructed by combining\ndifferent partial estimators produces fidelities exceeding 97%.",
        "The orbits of short-period exoplanets are sculpted by tidal dissipation.\nHowever, the mechanisms and associated efficiencies of these tidal interactions\nare poorly constrained. We present robust constraints on the tidal quality\nfactors of short-period exoplanetary host stars through the usage of a novel\nempirical technique. The method is based on analyzing structures in the\npopulation-level distribution of tidal decay times, defined as the time\nremaining before a planet spirals into its host star due to stellar tides.\nUsing simple synthetic planet population simulations and analytic theory, we\nshow that there exists a steady-state portion of the decay time distribution\nwith an approximately power-law form. This steady-state feature is clearly\nevident in the decay time distribution of the observed short-period planet\npopulation. We use this to constrain both the magnitude and frequency\ndependence of the stellar tidal quality factor and show that it must decrease\nsharply with planetary orbital period. Specifically, with $Q_{\\star}' = Q_0\n(P\/2 \\ \\mathrm{days})^{\\alpha}$, we find $10^{5.5} \\lesssim Q_0 \\lesssim 10^7$\nand $-4.33 \\lesssim \\alpha \\lesssim -2$. Our results are most consistent with\npredictions from tidal resonance locking, in which the planets are locked into\nresonance with a tidally excited gravity mode in their host stars.",
        "We extend the result of Blumberg and Mandell on K-theoretic Tate-Poitou\nduality at odd primes which serves as a spectral refinement of the classical\narithmetic Tate-Poitou duality. The duality is formulated for the\n$K(1)$-localized algebraic K-theory of the ring of $p$-integers in a number\nfield and its completion using the $\\bZ_p$-Anderson duality. This paper\ncompletes the picture by addressing the prime 2, where the real embeddings of\nnumber fields introduce extra complexities. As an application, we identify the\nhomotopy type at prime 2 of the homotopy fiber of the cyclotomic trace for the\nsphere spectrum in terms of the algebraic K-theory of the integers.",
        "The nonlinear thermoelectric effect is a key factor for realising\nunconventional thermoelectric phenomena, such as heat rectification and power\ngeneration using thermal fluctuations. Recent theoretical advances have\nindicated that chiral materials can host a variety of exotic nonlinear\nthermoelectric transport arising from inversion-symmetry breaking. However,\nexperimental demonstration has yet to be achieved. Here, we report the\nobservation of the nonlinear chiral thermoelectric Hall effect in chiral\ntellurium at room temperature, where a voltage is generated as a cross product\nof the temperature gradient and electric field. The resulting thermoelectric\nHall voltage is on the order of {\\mu}V, consistent with the prediction from the\nab initio calculation. Furthermore, the sign of the thermoelectric Hall voltage\nis reversed depending on the crystal chirality, demonstrating a novel\nfunctionality of sign control of the thermoelectric effect by the chirality\ndegrees of freedom. Our findings reveal the potential of chiral systems as\nnonlinear thermoelectric materials for advanced thermal management and energy\nharvesting.",
        "The inf-sup condition is one of the essential tools in the analysis of the\nStokes equations and especially in numerical analysis. In its usual form, the\ncondition states that for every pressure $p\\in L^2(\\Omega)\\setminus\n\\mathbb{R}$, (i.e. with mean value zero) a velocity $u\\in H^1_0(\\Omega)^d$ can\nbe found, so that $(div\\,u,p)=\\|p\\|^2$ and $\\|\\nabla u\\|\\le c \\|p\\|$ applies,\nwhere $c>0$ does not depend on $u$ and $p$. However, if we consider domains\nthat have a Neumann-type outflow condition on part of the boundary\n$\\Gamma_N\\subset\\partial\\Omega$, the inf-sup condition cannot be used in this\nform, since the pressure here comes from $L^2(\\Omega)$ and does not necessarily\nhave zero mean value. In this note, we derive the inf-sup condition for the\ncase of outflow boundaries.",
        "We show that limit cycle systems in Langevin bath exhibit uncertainty in\nobservables that define the limit-cycle plane, and maintain a positive lower\nbound. The uncertainty-bound depends on the parameters that determine the shape\nand periodicity of the limit cycle. In one dimension, we use the framework of\ncanonical dissipative systems to construct the limit cycle, whereas in two\ndimensions, particle in central potentials with radial-dissipation provide us\nnatural examples. We show that, the position-momenta uncertainty of particle in\na central potential is larger than half the magnitude of the angular momentum\n(conserved) of the particle. We also investigate how uncertainties, which are\nabsent in deterministic systems, increase with time when the systems are\nattached to a bath and eventually cross the lower bound before reaching the\nsteady state.",
        "In this paper, Ni-doped ZnO films were grown on a p-type silicon substrate\nvia spray pyrolysis. The Ni dopant concentrations were varied by adjusting the\nweight ratio between Zinc Acetate Dehydrate (ZAD) and Nickel Chloride\nHexahydrate (NCH), resulting in the ZnO, ZnO:Ni1%, and ZnO:Ni3% samples.\nField-effect scanning electron microscopy (FESEM) images revealed that Ni\ndoping significantly reduced the nanostructure size from 326 nm (ZnO) to 146 nm\n(ZnO:Ni3%). Similarly, X-ray diffraction (XRD) analysis also shows the decrease\nof the crystallite size with increasing Ni doping, from 44 nm (ZnO) to 35 nm\n(ZnO:Ni3%). Current-voltage (I-V) measurements were conducted at a bias voltage\nof 0 and 5 V to examine electrical and self-powered photodetection properties.\nAll samples demonstrate self-powered photodetector performance. At the bias of\n0 V, the undoped ZnO exhibited a higher photo-to-dark-current ratio (162) as\ncompared to ZnO:Ni1% (18) and ZnO:Ni3% (16). The ZnO:Ni3% samples displayed\nfaster rise (0.4 s) and fall times (1.7 s) as compared to the pure ZnO (10.8 s\nfor rise time and 9.1 s for fall time), highlighting their potential for\napplications requiring rapid photoresponse. The findings provide valuable\ninsights into optimizing the performance of ZnO-based photodetectors through\ncontrolled Ni doping, enabling advancements in self-powered photodetection\ntechnology for energy-efficient optoelectronic devices.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.",
        "Immersed boundary finite element methods allow the user to bypass the\npotentially troublesome task of boundary-conforming mesh generation. However,\nthey suffer from the influence of cut elements, i.e., elements that are\nintersected by the physical domain boundaries. When combined with explicit time\nintegration, poorly cut elements with little support in the physical domain\nhave a detrimental effect on the critical time step size, thereby hampering the\napplication of immersed boundary methods to wave propagation simulations. In\nthis paper, we investigate the stabilizing effect of the finite cell method\nconcerning explicit time integration. Starting with an analytical solution of\nan example with one degree of freedom, we systematically study the influence of\n$\\alpha$-stabilization on the maximum eigenvalue and thus on the critical time\nstep size. The analysis is then complemented by a numerical study of an example\nwith one element and an increasing polynomial degree. We demonstrate that the\ncritical time step size does not decrease below a certain limit, even when\nfurther reducing the cut fraction of the element. This minimum critical time\nstep size is controlled by the chosen $\\alpha$ value and becomes less severe\nfor higher dimensions. Increasing the polynomial degree has little effect on\nthe degradation of the minimum critical time step size. Finally, we provide an\nestimate of the minimum critical time step size depending on the chosen\nstabilization parameter $\\alpha$ and the dimension of the problem. Based on\nthis estimate, we propose a modified CFL condition for the finite cell method,\nthe validity of which we demonstrate on a numerical example of a perforated\nplate.",
        "Autism Spectrum Disorder (ASD) is a complex neurological condition\ncharacterized by varied developmental impairments, especially in communication\nand social interaction. Accurate and early diagnosis of ASD is crucial for\neffective intervention, which is enhanced by richer representations of brain\nactivity. The brain functional connectome, which refers to the statistical\nrelationships between different brain regions measured through neuroimaging,\nprovides crucial insights into brain function. Traditional static methods often\nfail to capture the dynamic nature of brain activity, in contrast, dynamic\nbrain connectome analysis provides a more comprehensive view by capturing the\ntemporal variations in the brain. We propose BrainTWT, a novel dynamic network\nembedding approach that captures temporal evolution of the brain connectivity\nover time and considers also the dynamics between different temporal network\nsnapshots. BrainTWT employs temporal random walks to capture dynamics across\ndifferent temporal network snapshots and leverages the Transformer's ability to\nmodel long term dependencies in sequential data to learn the discriminative\nembeddings from these temporal sequences using temporal structure prediction\ntasks. The experimental evaluation, utilizing the Autism Brain Imaging Data\nExchange (ABIDE) dataset, demonstrates that BrainTWT outperforms baseline\nmethods in ASD classification.",
        "In this paper, extending some ideas of Fano, we study the birational geometry\nof the Hilbert scheme of 0-dimensional subschemes of length 2 of a rational\nnormal scroll. This fourfold has three elementary contractions associated to\nthe three faces of its nef cone. We study natural projective realizations of\nthese contractions. In particular, given a smooth rational normal scroll\n$S_{a,b}$ of degree $r$ in ${\\mathbb P}^{r+1}$ with $1 \\leq a \\leq b$ and\na+b=r, i.e., $S_{a,b}$ is the relative Proj of the vector bundle $O_{{\\mathbb\nP}^1}(a)\\oplus O_{{\\mathbb P}^1}(b)$ embedded in ${\\mathbb P}^{r+1}$ with its\nO(1) line bundle (from an abstract viewpoint $S_{a,b}\\cong {\\mathbb F}_{b-a}$),\nwe consider the subvariety $X_{a,b}$ of the Grassmannian $G(1,r+1)$ described\nby all lines that are secant or tangent to $S_{a,b}$. The variety $X_{a,b}$ is\nthe image of some of the aforementioned contractions, it is smooth if a>1, and\nit is singular at a unique point if a=1. We compute the degree of $X_{a,b}$ and\nthe local structure of the singularity of $X_{a,b}$ when a=1. Finally we\ndiscuss in some detail the case r=4, originally considered by Fano, because the\nsmooth hyperplane sections of $X_{2,2}$ and $X_{1,3}$ are the Fano 3-folds that\nappear as number 16 in the Mori-Mukai list of Fano 3-folds with Picard number\n2. We prove that any smooth hyperplane section of $X_{2,2}$ is also a\nhyperplane section of $X_{1,3}$, and we discuss the GIT-stability of the smooth\nhyperplane sections of $X_{1,3}$ where $G$ is the subgroup of the projective\nautomorphisms of $X_{1,3}$ coming from the ones of $S_{1,3}.$",
        "In the present paper, we present an adaptive mesh refinement(AMR) approach\ndesigned for the discontinuous Galerkin method for conservation laws. The\nblock-based AMR is adopted to ensure the local data structure simplicity and\nthe efficiency, while the unstructured topology of the initial blocks is\nsupported by the forest concept such that the complex geometry of the\ncomputational domain can be easily treated. The inter-block communication\nthrough guardcells is introduced to avoid the direct treatment of flux\ncomputing between cells at different refinement levels. The sharp corners and\ncreases generated during direct refinement can be avoided by projecting the\nboundary nodes to either the user-defined boundary surface function or the\nauto-generated NURBs. High-level MPI parallelization is implemented with\ndynamic load balancing through a space curve filling procedure. Some test cases\nare presented. As a result, ideal accuracy order and versatility in tracing and\ncontrolling the dynamic refinement are observed. Also, good parallelization\nefficiency is demonstrated.",
        "Variational quantum algorithms are hybrid quantum-classical approaches\nextensively studied for their potential to leverage near-term quantum hardware\nfor computational advantages. In this work, we successfully execute two\nvariational quantum algorithms on a silicon photonic integrated circuit at room\ntemperature: a variational quantum eigensolver for the Hydrogen molecule and a\nvariational quantum factorization for semi-prime numbers. In our reconfigurable\nsilicon photonic circuit, four identical spontaneous-four-wave-mixing-based\nintegrated photon pair sources are used to prepare two path-entangled ququarts,\nwhose correlation gives rise to the resource for generic trial states'\npreparation. This marks a first demonstration of variational quantum algorithms\non a photonic quantum simulator with integrated photon pair sources.",
        "Using optical time series with Telescopi Joan Or\\'o (TJO), Gaia, TESS, and\nNEOWISE archival data, we performed a variability study on the candidate\nbloated massive young stellar object (MYSO) IRAS 19520+2759. This is the first\ntime that a bloated star candidate has been tested for the theoretically\npredicted periodic variability. The source is found to be variable at optical\nand mid-infrared wavelengths and classified as a long-period variable MYSO. The\nobserved TJO data gives a period of the source of $\\sim$ 270$\\pm$40 days (in\nthe Rc band) and $\\sim$ 270$\\pm$50 days (in the Ic band), which is very close\nto the value predicted by the theoretical Period-Luminosity relation for a\nbloated young star of $\\sim 10^5 L\\odot$. Additionally, a large period of\n$\\sim$ 460$\\pm$80 days (in the G band) and $\\sim$ 440$\\pm$70 (in the Rp band)\nis also visible in the Gaia light curve. The physical parameters of the source,\nsuch as mass, radius, and accretion rate, based on the theoretical predictions\nfor the spherical accretion case and corresponding to a period of 270--460\ndays, are $\\sim 24$--28$\\,M\\odot$, $\\sim 650$--900$\\,R\\odot$ and $\\sim\n(6$--$9)\\times10^{-3}\\,M\\odot yr^{-1}$. However, these numbers are very\nsensitive to the effective temperatures assumed in the models. Additionally,\nthese values strongly depend on the geometry of accretion and could\nsignificantly decrease for the case of a MYSO accreting through a disc. The\nobserved periodic variability, the observed colour trend, and the nature of the\nvariability are found to be consistent with the pulsational model for a bloated\nMYSO.",
        "Higher-order exceptional points (EPs) in optical structures enable\nultra-sensitive responses to perturbations. However, previous investigations on\nhigher-order EPs have predominantly focused on coupled systems, leaving their\nfundamental physics in open scattering systems largely unexplored. Here, we\nharness wave interference to realize higher-order EPs in non-Hermitian\nzero-index materials connected to multiple open channels. Specifically, we\ndemonstrate that a three-channel model can give rise to three interesting types\nof third-order EPs: lasing EP, reflecting EP, and absorbing EP. Notably, near\nthe third-order absorbing EP, we observe ultrasensitivity--a drastic change in\noutput power in response to perturbations at the operating frequency--in a\npurely lossy system. These findings pave the way for achieving higher-order and\neven arbitrary-order EPs in open scattering systems, offering significant\npotential for advanced sensing applications.",
        "This paper aims to critically review electrically active defects studied by\njunction spectroscopy techniques (deep-level transient spectroscopy and\nminority carrier transient spectroscopy) in the three most commonly used\nsilicon carbide (SiC) polytypes: 3C-SiC, 4H-SiC, and 6H-SiC.",
        "We construct an $\\infty$-category $\\mathcal{G}$ as a model for the Ground\nState of physical phenomena and we provide properties of its manifestations\n$\\chi = \\text{Fun}(\\mathcal{G}, \\text{Cat}_{\\infty})$ in $\\text{Cat}_{\\infty}$\nas well as of its $\\infty$-category of spectra $\\text{Sp}(\\chi)$.",
        "We present a systematic study of the family of positive definite (p.d.)\nkernels with the use of their associated feature maps and feature spaces. For a\nfixed set $X$, generalizing Loewner, we make precise the corresponding\npartially ordered set $Pos\\left(X\\right)$ of all p.d. kernels on $X$, as well\nas a study of its global properties. This new analysis includes both results\ndealing with applications and concrete examples, including such general notions\nfor $Pos\\left(X\\right)$ as the structure of its partial order, its products,\nsums, and limits; as well as their Hilbert space-theoretic counterparts. For\nthis purpose, we introduce a new duality for feature spaces, feature\nselections, and feature mappings. For our analysis, we further introduce a\ngeneral notion of dual pairs of p.d. kernels. Three special classes of kernels\nare studied in detail: (a) the case when the reproducing kernel Hilbert spaces\n(RKHSs) may be chosen as Hilbert spaces of analytic functions, (b) when they\nare realized in spaces of Schwartz-distributions, and (c) arise as fractal\nlimits. We further prove inverse theorems in which we derive results for the\nanalysis of $Pos\\left(X\\right)$ from the operator theory of specified\ncounterpart-feature spaces. We present constructions of new p.d. kernels in two\nways: (i) as limits of monotone families in $Pos\\left(X\\right)$, and (ii) as\np.d. kernels which model fractal limits, i.e., are invariant with respect to\ncertain iterated function systems (IFS)-transformations.",
        "The CosmoLink project aims to develop low cost, low power, long range and\nportable form factor particle detectors for experimental and educational\nresearch on Cosmic Rays. Use of these detectors distributed across locations\nwill be used for setup small scale telescope array experiments. Each detector\nunit in CosmoLink project consists of two plastic scintillators kept in stacked\ngeometry to look for coincidence induced by cosmic ray particles. The\nscintillators are directly coupled with Silicon photomultipliers (SiPMs) for\nefficient light guiding. Each readout channel is equipped with an amplifier,\ndiscriminator and a peak-detect-and-hold circuit. Upon successful coincidence,\nthe peak amplitude is digitized using Analog to Digital Converter (ADC). The\ncoincidence count and ADC data are acquired using a low cost microcontroller.\nMultiple detectors can be grouped together like swarms and arranged in\ndifferent geometries to collect data jointly. The acquired data is wirelessly\ntransmitted to a central server with GPS Coordinates and other sensor data."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Quantifying Variance in Evaluation Benchmarks",
    "start_abstract":"Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The Llama 3 Herd of Models"
      ],
      "abstract":[
        "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Financial Wind Tunnel: A Retrieval-Augmented Market Simulator",
        "Functionalized Cr$_2$C MXenes: Novel Magnetic Semiconductors",
        "Constructing Sobolev orthonormal rational functions via an updating\n  procedure",
        "Deep Reinforcement Learning Enabled Persistent Surveillance with\n  Energy-Aware UAV-UGV Systems for Disaster Management Applications",
        "Event Soliton Formation in Mixed Energy-Momentum Gaps of Nonlinear\n  Spacetime Crystals",
        "RAPTOR: Refined Approach for Product Table Object Recognition",
        "Spin-flip Scattering at a Chiral Interface of Helical Chains",
        "Tutorial: Hong-Ou-Mandel interference with Structured Photons",
        "Following the state of an evaporating charged black hole into the\n  quantum gravity regime",
        "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning\n  Attacks",
        "Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal\n  Dependencies in Complex Data",
        "Quantum entanglement correlations in double quark PDFs",
        "Modelling Soil as a Living System: Feedback between Microbial Activity\n  and Spatial Structure",
        "Towards Understanding Text Hallucination of Diffusion Models via Local\n  Generation Bias",
        "Language Modelling for Speaker Diarization in Telephonic Interviews",
        "Grounding Creativity in Physics: A Brief Survey of Physical Priors in\n  AIGC",
        "Big Atomics",
        "Measurement-Device-Independent Certification of Schmidt Number",
        "UrbanSAM: Learning Invariance-Inspired Adapters for Segment Anything\n  Models in Urban Construction",
        "Does Time Have Its Place? Temporal Heads: Where Language Models Recall\n  Time-specific Information",
        "Generation of Frequency-Tunable Shaped Single Microwave Photons Using a\n  Fixed-Frequency Superconducting Qubit",
        "LSM Trees in Adversarial Environments",
        "Multi-beam-energy control unit based on triple bend achromats",
        "Human Body Restoration with One-Step Diffusion Model and A New Benchmark",
        "Dual view of the Z$_2$-Gauged XY Model in 3D",
        "Deep Muscle EMG construction using A Physics-Integrated Deep Learning\n  approach",
        "Forecasting Open-Weight AI Model Growth on HuggingFace",
        "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
        "DGSSA: Domain generalization with structural and stylistic augmentation\n  for retinal vessel segmentation"
      ],
      "abstract":[
        "Market simulator tries to create high-quality synthetic financial data that\nmimics real-world market dynamics, which is crucial for model development and\nrobust assessment. Despite continuous advancements in simulation methodologies,\nmarket fluctuations vary in terms of scale and sources, but existing frameworks\noften excel in only specific tasks. To address this challenge, we propose\nFinancial Wind Tunnel (FWT), a retrieval-augmented market simulator designed to\ngenerate controllable, reasonable, and adaptable market dynamics for model\ntesting. FWT offers a more comprehensive and systematic generative capability\nacross different data frequencies. By leveraging a retrieval method to discover\ncross-sectional information as the augmented condition, our diffusion-based\nsimulator seamlessly integrates both macro- and micro-level market patterns.\nFurthermore, our framework allows the simulation to be controlled with wide\napplicability, including causal generation through \"what-if\" prompts or\nunprecedented cross-market trend synthesis. Additionally, we develop an\nautomated optimizer for downstream quantitative models, using stress testing of\nsimulated scenarios via FWT to enhance returns while controlling risks.\nExperimental results demonstrate that our approach enables the generalizable\nand reliable market simulation, significantly improve the performance and\nadaptability of downstream models, particularly in highly complex and volatile\nmarket conditions. Our code and data sample is available at\nhttps:\/\/anonymous.4open.science\/r\/fwt_-E852",
        "We report an \\textit{ab initio} investigation of functionalized and\n3$d$-electrons doped Cr$_2$C MXenes. Upon functionalization, the Cr$_2$C\nbecomes chemically, dynamically, and mechanically stable, and it exhibits\nmagnetic semiconducting behavior. Cr$_2$CF$_2$ stands out as a wide band gap\nsemiconductor, possessing super exchange interaction mediated by F atoms within\nthe layer, however, the applied strain transforms it from an indirect to a\ndirect band gap semiconductor. Strong spin-phonon coupling found in\nCr$_2$CH$_2$ is supported by the distorted Cr spin density due to hydrogen\nenvironment. Two magnon branches, associated with two sub-lattice spins, are\nfound in the ferromagnetic Cr$_2$CO$_2$ and antiferromagnetic Cr$_2$CF$_2$.\nDepending on the types of 3$d$-electron dopants and functionalization, Cr$_2$C\nMXenes (except for Cr$_2$CO$_2$) change from the indirect band gap magnetic\nsemiconductor to different states of electronic and magnetic matter including\nexotic direct band gap magnetic bipolar semiconductor. In addition, we reveal a\nband inversion between the two highest valence bands in the Fe-doped\nCr$_2$CCl$_2$.",
        "In this paper, we generate the recursion coefficients for rational functions\nwith prescribed poles that are orthonormal with respect to a continuous Sobolev\ninner product. Using a rational Gauss quadrature rule, the inner product can be\ndiscretized, thus allowing a linear algebraic approach. The presented approach\ninvolves reformulating the problem as an inverse eigenvalue problem involving a\nHessenberg pencil, where the pencil will contain the recursion coefficients\nthat generate the sequence of Sobolev orthogonal rational functions. This\nreformulation is based on the connection between Sobolev orthonormal rational\nfunctions and the orthonormal bases for rational Krylov subspaces generated by\na Jordan-like matrix. An updating procedure, introducing the nodes of the inner\nproduct one after the other, is proposed and the performance is examined\nthrough some numerical examples.",
        "Integrating Unmanned Aerial Vehicles (UAVs) with Unmanned Ground Vehicles\n(UGVs) provides an effective solution for persistent surveillance in disaster\nmanagement. UAVs excel at covering large areas rapidly, but their range is\nlimited by battery capacity. UGVs, though slower, can carry larger batteries\nfor extended missions. By using UGVs as mobile recharging stations, UAVs can\nextend mission duration through periodic refueling, leveraging the\ncomplementary strengths of both systems. To optimize this energy-aware UAV-UGV\ncooperative routing problem, we propose a planning framework that determines\noptimal routes and recharging points between a UAV and a UGV. Our solution\nemploys a deep reinforcement learning (DRL) framework built on an\nencoder-decoder transformer architecture with multi-head attention mechanisms.\nThis architecture enables the model to sequentially select actions for visiting\nmission points and coordinating recharging rendezvous between the UAV and UGV.\nThe DRL model is trained to minimize the age periods (the time gap between\nconsecutive visits) of mission points, ensuring effective surveillance. We\nevaluate the framework across various problem sizes and distributions,\ncomparing its performance against heuristic methods and an existing\nlearning-based model. Results show that our approach consistently outperforms\nthese baselines in both solution quality and runtime. Additionally, we\ndemonstrate the DRL policy's applicability in a real-world disaster scenario as\na case study and explore its potential for online mission planning to handle\ndynamic changes. Adapting the DRL policy for priority-driven surveillance\nhighlights the model's generalizability for real-time disaster response.",
        "We report the formation of a novel soliton, termed event soliton, in\nnonlinear photonic spacetime crystals (STCs). In these media, simultaneous\nspatiotemporal periodic modulation of the dielectric constant generates mixed\nfrequency (${\\omega}$) and wavevector (k) gaps. Under Kerr nonlinearity, the\nevent solitons emerge as fully localized entities in both spacetime and\nenergy-momentum domains, providing a tangible demonstration of the concept of\nevent in relativity. The ${\\omega}$k-gap mixture arises from the coexistence\nand competition between time reflected and Bragg reflected waves due to the\nspatiotemporal modulation. We propose a new partial differential equation to\ncapture various spatiotemporal patterns and present numerical simulations to\nvalidate our theoretical predictions, reflecting a three-way balance among\nk-gap opening, ${\\omega}$-gap opening, and nonlinearity. Our work opens avenues\nfor fundamental studies and fosters experimental prospects for implementing\nspacetime crystals in both time-varying photonics and periodically driven\ncondensed matter systems.",
        "Extracting tables from documents is a critical task across various\nindustries, especially on business documents like invoices and reports.\nExisting systems based on DEtection TRansformer (DETR) such as TAble\nTRansformer (TATR), offer solutions for Table Detection (TD) and Table\nStructure Recognition (TSR) but face challenges with diverse table formats and\ncommon errors like incorrect area detection and overlapping columns. This\nresearch introduces RAPTOR, a modular post-processing system designed to\nenhance state-of-the-art models for improved table extraction, particularly for\nproduct tables. RAPTOR addresses recurrent TD and TSR issues, improving both\nprecision and structural predictions. For TD, we use DETR (trained on ICDAR\n2019) and TATR (trained on PubTables-1M and FinTabNet), while TSR only relies\non TATR. A Genetic Algorithm is incorporated to optimize RAPTOR's module\nparameters, using a private dataset of product tables to align with industrial\nneeds. We evaluate our method on two private datasets of product tables, the\npublic DOCILE dataset (which contains tables similar to our target product\ntables), and the ICDAR 2013 and ICDAR 2019 datasets. The results demonstrate\nthat while our approach excels at product tables, it also maintains reasonable\nperformance across diverse table formats. An ablation study further validates\nthe contribution of each module in our system.",
        "We investigate spin-flip scattering processes of electrons when they pass a\nchiral interface, which is the boundary between right- and left-handed\none-dimensional chain. We construct a minimal $p$-orbital model consisting of\nthe right- and left-handed one-dimensional threefold helical chains connected\nat $z=0$ with the nearest neighbor hopping and the spin-orbit coupling. The\ndynamics of spin-polarized wave packet passing through the interface, the\nGreen's functions, and electronic states near the interface are analyzed\nnumerically. We find that the microscopic structure of the interface is\nimportant and this strongly affects the local electronic orbital state. This in\naddition to the spin-orbit coupling determines whether the spin flip occurs or\nnot at the chiral interface and suggests a possible spin transport control by\nthe orbital configuration at the chiral interface.",
        "The Hong-Ou-Mandel (HOM) effect, an effective two-photon interference\nphenomenon, is a cornerstone of quantum optics and a key tool for linear\noptical quantum information processing. While the HOM effect has been\nextensively studied both theoretically and experimentally for various photonic\nquantum states, particularly in the spectral domain, detailed overviews of its\nbehaviour for structured photons -- those with complex spatial profiles --\nunder arbitrary spatial mode measurement schemes are still lacking. This\ntutorial aims to fill this gap by providing a comprehensive theoretical\nanalysis of the HOM effect for structured photons, including an arbitrary mode\nprojection on quantum interference outcomes. The tutorial also provides\nanalytical, closed-form expressions of the HOM visibility under different\nmeasurement conditions, which is a crucial contribution for its application in\ncomputational and artificial-intelligence-driven discovery of new quantum\nexperiments exploiting the power of photons with complex spatial modes.",
        "We study the energy probability density function of an evaporating\nnear-extremal charged black hole. At sufficiently low energies, such black\nholes experience large quantum metric fluctuations in the $AdS_{2}$ throat\nwhich are governed by a Schwarzian action. These fluctuations modify Hawking\nevaporation rates, and therefore also affect how the black hole state evolves\nover time. In previous work on Schwarzian-corrected Hawking radiation, the\nblack hole was taken to be in the microcanonical or canonical ensemble\n[arXiv:2411.03447]. However, we find that an initially fixed-energy or\nfixed-temperature state does not remain so in the regime where Schwarzian\ncorrections are important. We consider three decay channels: the emission of\nmassless scalars, photons, and entangled pairs of photons in angular momentum\nsinglet states. In each of the three cases, we find that in the very low\nenergy, quantum dominated regime, the probability distribution of the black\nhole energy level occupation tends toward a particular attractor function that\neffectively depends on only one combination of time and energy. This function\nis independent of the initial state and gives new predictions for the energy\nfluxes and Hawking emission spectra of near-extremal charged black holes.",
        "Multimodal large language models (MLLMs) equipped with Retrieval Augmented\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\nexternal knowledge to excel in tasks such as Question Answering. While RAG\nenhances MLLMs by grounding responses in query-relevant external knowledge,\nthis reliance poses a critical yet underexplored safety risk: knowledge\npoisoning attacks, where misinformation or irrelevant knowledge is\nintentionally injected into external knowledge bases to manipulate model\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\ninjects query-specific misinformation in both text and images for targeted\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\nduring MLLM generation to elicit nonsensical responses across all queries. We\nevaluate our attacks across multiple tasks, models, and access settings,\ndemonstrating that LPA successfully manipulates the MLLM to generate\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\nsingle irrelevant knowledge injection. Our results highlight the urgent need\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\nframeworks.",
        "Causality in time series can be difficult to determine, especially in the\npresence of non-linear dependencies. The concept of Granger causality helps\nanalyze potential relationships between variables, thereby offering a method to\ndetermine whether one time series can predict-Granger cause-future values of\nanother. Although successful, Granger causal methods still struggle with\ncapturing long-range relations between variables. To this end, we leverage the\nrecently successful Extended Long Short-Term Memory (xLSTM) architecture and\npropose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between\nthe time series components by using a novel dynamic lass penalty on the initial\nprojection. Specifically, we adaptively improve the model and identify sparsity\ncandidates. Our joint optimization procedure then ensures that the Granger\ncausal relations are recovered in a robust fashion. Our experimental\nevaluations on three datasets demonstrate the overall efficacy of our proposed\nGC-xLSTM model.",
        "Methods from Quantum Information Theory are used to scrutinize quantum\ncorrelations encoded in the two-quark density matrix over light-cone momentum\nfractions $x_1$ and $x_2$. A non-perturbative three quark model light-cone\nwavefunction predicts significant non-classical correlations associated with\nthe \"entanglement negativity\" measure for asymmetric and small quark momentum\nfractions. We perform one step of QCD scale evolution of the entire density\nmatrix, not just its diagonal (dPDF), by computing collinearly divergent\ncorrections due to the emission of a gluon. Finally, we present first\nqualitative numerical results for single-step scale evolution of quantum\nentanglement correlations in double quark PDFs. At a higher $Q^2$ scale, the\nnon-classical correlations manifest in the dPDF for nearly symmetric momentum\nfractions.",
        "Soil is a complex, dynamic material, with physical properties that depend on\nits biological content. We propose a cellular automaton model for\nself-organizing soil structure, where soil aggregates and serves as food for\nmicrobial species. These, in turn, produce nutrients that facilitate\nself-amplification, establishing a cyclical dynamic of consumption and\nregeneration. Our model explores the spatial interactions between these\ncomponents and their role in sustaining a balanced ecosystem. The main results\ndemonstrate that (1) spatial structure supports a stable living state,\npreventing population collapse or uncontrolled growth; (2) the spatial model\nallows for the coexistence of parasitic species, which exploit parts of the\nsystem without driving it to extinction; and (3) optimal growth conditions for\nmicrobes are associated to diverse length scales in the soil structure,\nsuggesting that heterogeneity is key to ecosystem resilience. These findings\nhighlight the importance of spatio-temporal dynamics of life in soil ecology.",
        "Score-based diffusion models have achieved incredible performance in\ngenerating realistic images, audio, and video data. While these models produce\nhigh-quality samples with impressive details, they often introduce unrealistic\nartifacts, such as distorted fingers or hallucinated texts with no meaning.\nThis paper focuses on textual hallucinations, where diffusion models correctly\ngenerate individual symbols but assemble them in a nonsensical manner. Through\nexperimental probing, we consistently observe that such phenomenon is\nattributed it to the network's local generation bias. Denoising networks tend\nto produce outputs that rely heavily on highly correlated local regions,\nparticularly when different dimensions of the data distribution are nearly\npairwise independent. This behavior leads to a generation process that\ndecomposes the global distribution into separate, independent distributions for\neach symbol, ultimately failing to capture the global structure, including\nunderlying grammar. Intriguingly, this bias persists across various denoising\nnetwork architectures including MLP and transformers which have the structure\nto model global dependency. These findings also provide insights into\nunderstanding other types of hallucinations, extending beyond text, as a result\nof implicit biases in the denoising models. Additionally, we theoretically\nanalyze the training dynamics for a specific case involving a two-layer MLP\nlearning parity points on a hypercube, offering an explanation of its\nunderlying mechanism.",
        "The aim of this paper is to investigate the benefit of combining both\nlanguage and acoustic modelling for speaker diarization. Although conventional\nsystems only use acoustic features, in some scenarios linguistic data contain\nhigh discriminative speaker information, even more reliable than the acoustic\nones. In this study we analyze how an appropriate fusion of both kind of\nfeatures is able to obtain good results in these cases. The proposed system is\nbased on an iterative algorithm where a LSTM network is used as a speaker\nclassifier. The network is fed with character-level word embeddings and a GMM\nbased acoustic score created with the output labels from previous iterations.\nThe presented algorithm has been evaluated in a Call-Center database, which is\ncomposed of telephone interview audios. The combination of acoustic features\nand linguistic content shows a 84.29% improvement in terms of a word-level DER\nas compared to a HMM\/VB baseline system. The results of this study confirms\nthat linguistic content can be efficiently used for some speaker recognition\ntasks.",
        "Recent advancements in AI-generated content have significantly improved the\nrealism of 3D and 4D generation. However, most existing methods prioritize\nappearance consistency while neglecting underlying physical principles, leading\nto artifacts such as unrealistic deformations, unstable dynamics, and\nimplausible objects interactions. Incorporating physics priors into generative\nmodels has become a crucial research direction to enhance structural integrity\nand motion realism. This survey provides a review of physics-aware generative\nmethods, systematically analyzing how physical constraints are integrated into\n3D and 4D generation. First, we examine recent works in incorporating physical\npriors into static and dynamic 3D generation, categorizing methods based on\nrepresentation types, including vision-based, NeRF-based, and Gaussian\nSplatting-based approaches. Second, we explore emerging techniques in 4D\ngeneration, focusing on methods that model temporal dynamics with physical\nsimulations. Finally, we conduct a comparative analysis of major methods,\nhighlighting their strengths, limitations, and suitability for different\nmaterials and motion dynamics. By presenting an in-depth analysis of\nphysics-grounded AIGC, this survey aims to bridge the gap between generative\nmodels and physical realism, providing insights that inspire future research in\nphysically consistent content generation.",
        "In this paper, we give theoretically and practically efficient\nimplementations of Big Atomics, i.e., $k$-word linearizable registers that\nsupport the load, store, and compare-and-swap (CAS) operations. While modern\nhardware supports $k = 1$ and sometimes $k = 2$ (e.g., double-width\ncompare-and-swap in x86), our implementations support arbitrary $k$. Big\nAtomics are useful in many applications, including atomic manipulation of\ntuples, version lists, and implementing load-linked\/store-conditional (LL\/SC).\nWe design fast, lock-free implementations of big atomics based on a novel\nfast-path-slow-path approach we develop. We then use them to develop an\nefficient concurrent hash table, as evidence of their utility.\n  We experimentally validate the approach by comparing a variety of\nimplementations of big atomics under a variety of workloads (thread counts,\nload\/store ratios, contention, oversubscription, and number of atomics). The\nexperiments compare two of our lock-free variants with C++ std::atomic, a\nlock-based version, a version using sequence locks, and an indirect version.\nThe results show that our approach is close to the fastest under all conditions\nand far outperforms others under oversubscription. We also compare our big\natomics based concurrent hash table to a variety of other state-of-the-art hash\ntables that support arbitrary length keys and values, including implementations\nfrom Intel's TBB, Facebook's Folly, libcuckoo, and a recent release from Boost.\nThe results show that our approach of using big atomics in the design of hash\ntables is a promising direction.",
        "Bipartite quantum states with higher Schmidt numbers have been shown to\noutperform those with lower Schmidt numbers in various information processing\ntasks. Therefore, to ensure the efficient use of resources in these tasks, it\nis essential to certify the Schmidt number of the resource states. Ideally,\nthis certification should rely as little as possible on the certifying devices,\nensuring robustness against potential imperfections. In this work, we explore\nthe scope of fully and partially device-independent Schmidt number\ncertification methods. We demonstrate the general impossibility of fully\ndevice-independent certification for all states. Specifically, in a restricted\nsetting, we present a class of states with Schmidt number 3, for which it is\nimpossible to certify that their Schmidt number is greater than 2. However, we\nshow that the Schmidt number of all states can be certified in a\nmeasurement-device-independent manner via semi-quantum nonlocal games, which\nassume trust in the preparation devices. Finally, we present an explicit\nconstruction of a semi-quantum game for the measurement-device-independent\ncertification of a class of states.",
        "Object extraction and segmentation from remote sensing (RS) images is a\ncritical yet challenging task in urban environment monitoring. Urban morphology\nis inherently complex, with irregular objects of diverse shapes and varying\nscales. These challenges are amplified by heterogeneity and scale disparities\nacross RS data sources, including sensors, platforms, and modalities, making\naccurate object segmentation particularly demanding. While the Segment Anything\nModel (SAM) has shown significant potential in segmenting complex scenes, its\nperformance in handling form-varying objects remains limited due to\nmanual-interactive prompting. To this end, we propose UrbanSAM, a customized\nversion of SAM specifically designed to analyze complex urban environments\nwhile tackling scaling effects from remotely sensed observations. Inspired by\nmulti-resolution analysis (MRA) theory, UrbanSAM incorporates a novel learnable\nprompter equipped with a Uscaling-Adapter that adheres to the invariance\ncriterion, enabling the model to capture multiscale contextual information of\nobjects and adapt to arbitrary scale variations with theoretical guarantees.\nFurthermore, features from the Uscaling-Adapter and the trunk encoder are\naligned through a masked cross-attention operation, allowing the trunk encoder\nto inherit the adapter's multiscale aggregation capability. This synergy\nenhances the segmentation performance, resulting in more powerful and accurate\noutputs, supported by the learned adapter. Extensive experimental results\ndemonstrate the flexibility and superior segmentation performance of the\nproposed UrbanSAM on a global-scale dataset, encompassing scale-varying urban\nobjects such as buildings, roads, and water.",
        "While the ability of language models to elicit facts has been widely\ninvestigated, how they handle temporally changing facts remains underexplored.\nWe discover Temporal Heads, specific attention heads primarily responsible for\nprocessing temporal knowledge through circuit analysis. We confirm that these\nheads are present across multiple models, though their specific locations may\nvary, and their responses differ depending on the type of knowledge and its\ncorresponding years. Disabling these heads degrades the model's ability to\nrecall time-specific knowledge while maintaining its general capabilities\nwithout compromising time-invariant and question-answering performances.\nMoreover, the heads are activated not only numeric conditions (\"In 2004\") but\nalso textual aliases (\"In the year ...\"), indicating that they encode a\ntemporal dimension beyond simple numerical representation. Furthermore, we\nexpand the potential of our findings by demonstrating how temporal knowledge\ncan be edited by adjusting the values of these heads.",
        "Scaling up a superconducting quantum computer will likely require quantum\ncommunication between remote chips, which can be implemented using an itinerant\nmicrowave photon in a transmission line. To realize high-fidelity\ncommunication, it is essential to control the frequency and temporal shape of\nthe microwave photon. In this work, we demonstrate the generation of\nfrequency-tunable shaped microwave photons without resorting to any\nfrequency-tunable circuit element. We develop a framework which treats a\nmicrowave resonator as a band-pass filter mediating the interaction between a\nsuperconducting qubit and the modes in the transmission line. This\ninterpretation allows us to stimulate the photon emission by an off-resonant\ndrive signal. We characterize how the frequency and temporal shape of the\ngenerated photon depends on the frequency and amplitude of the drive signal. By\nmodulating the drive amplitude and frequency, we achieve a frequency tunability\nof 40 MHz while maintaining the photon mode shape time-symmetric.Through\nmeasurements of the quadrature amplitudes of the emitted photons, we\ndemonstrate consistently high state and process fidelities around 95\\% across\nthe tunable frequency range. Our hardware-efficient approach eliminates the\nneed for additional biasing lines typically required for frequency tuning,\noffering a simplified architecture for scalable quantum communication.",
        "The Log Structured Merge (LSM) Tree is a popular choice for key-value stores\nthat focus on optimized write throughput while maintaining performant,\nproduction-ready read latencies. To optimize read performance, LSM stores rely\non a probabilistic data structure called the Bloom Filter (BF). In this paper,\nwe focus on adversarial workloads that lead to a sharp degradation in read\nperformance by impacting the accuracy of BFs used within the LSM store. Our\nevaluation shows up to $800\\%$ increase in the read latency of lookups for\npopular LSM stores. We define adversarial models and security definitions for\nLSM stores. We implement adversary resilience into two popular LSM stores,\nLevelDB and RocksDB. We use our implementations to demonstrate how performance\ndegradation under adversarial workloads can be mitigated.",
        "X-ray free electron lasers (XFELs) are the new generation of particle\naccelerator-based light sources, capable of producing tunable, high-power X-ray\npulses that are increasingly vital across various scientific disciplines.\nRecently, continuous-wave (CW) XFELs driven by superconducting linear\naccelerators have garnered significant attention due to their ability to\nenhance availability by supporting multiple undulator lines simultaneously.\nHowever, different undulator lines typically require distinct electron beam\nqualities, particularly varying electron beam energy to achieve a wide range of\nphoton energy tunability. Consequently, precise bunch-to-bunch control of\nelectron beam energy is essential. A double-bend achromat based electron beam\ndelay system has been proposed to enable multi-beam energy operations in\nCW-XFELs. In this paper, we introduce a novel delay system comprising four\ntriple-bend achromats (TBAs). Based on parameters of the Shanghai\nHigh-Repetition-Rate XFEL and Extreme Light Facility, start-to-end simulations\ndemonstrate that the TBA-based delay system achieves better electron beam\nqualities while providing a wide beam energy tuning range.",
        "Human body restoration, as a specific application of image restoration, is\nwidely applied in practice and plays a vital role across diverse fields.\nHowever, thorough research remains difficult, particularly due to the lack of\nbenchmark datasets. In this study, we propose a high-quality dataset automated\ncropping and filtering (HQ-ACF) pipeline. This pipeline leverages existing\nobject detection datasets and other unlabeled images to automatically crop and\nfilter high-quality human images. Using this pipeline, we constructed a\nperson-based restoration with sophisticated objects and natural activities\n(\\emph{PERSONA}) dataset, which includes training, validation, and test sets.\nThe dataset significantly surpasses other human-related datasets in both\nquality and content richness. Finally, we propose \\emph{OSDHuman}, a novel\none-step diffusion model for human body restoration. Specifically, we propose a\nhigh-fidelity image embedder (HFIE) as the prompt generator to better guide the\nmodel with low-quality human image information, effectively avoiding misleading\nprompts. Experimental results show that OSDHuman outperforms existing methods\nin both visual quality and quantitative metrics. The dataset and code will at\nhttps:\/\/github.com\/gobunu\/OSDHuman.",
        "The $Z_2$ gauged neutral XY model is of long-standing interest both in the\ncontext of nematic order, and the study of fractionalization and\nsuperconductivity. This paper presents heuristic arguments that no\ndeconfinement of the XY field occurs in this model and presents results of a\nlarge-scale Monte Carlo simulations on a cubic lattice which are consistent\nwith this conclusion. The correlation radius determining the confinement is\nfound to be growing rapidly as a function of the parameters in the phase\nfeaturing the nematic order. Thus, mesoscopic properties of the system can\nmimic deconfinement with high accuracy in some part of the phase diagram.",
        "Electromyography (EMG)--based computational musculoskeletal modeling is a\nnon-invasive method for studying musculotendon function, human movement, and\nneuromuscular control, providing estimates of internal variables like muscle\nforces and joint torques. However, EMG signals from deeper muscles are often\nchallenging to measure by placing the surface EMG electrodes and unfeasible to\nmeasure directly using invasive methods. The restriction to the access of EMG\ndata from deeper muscles poses a considerable obstacle to the broad adoption of\nEMG-driven modeling techniques. A strategic alternative is to use an estimation\nalgorithm to approximate the missing EMG signals from deeper muscle. A similar\nstrategy is used in physics-informed deep learning, where the features of\nphysical systems are learned without labeled data. In this work, we propose a\nhybrid deep learning algorithm, namely the neural musculoskeletal model (NMM),\nthat integrates physics-informed and data-driven deep learning to approximate\nthe EMG signals from the deeper muscles. While data-driven modeling is used to\npredict the missing EMG signals, physics-based modeling engraves the\nsubject-specific information into the predictions. Experimental verifications\non five test subjects are carried out to investigate the performance of the\nproposed hybrid framework. The proposed NMM is validated against the joint\ntorque computed from 'OpenSim' software. The predicted deep EMG signals are\nalso compared against the state-of-the-art muscle synergy extrapolation (MSE)\napproach, where the proposed NMM completely outperforms the existing MSE\nframework by a significant margin.",
        "As the open-weight AI landscape continues to proliferate-with model\ndevelopment, significant investment, and user interest-it becomes increasingly\nimportant to predict which models will ultimately drive innovation and shape AI\necosystems. Building on parallels with citation dynamics in scientific\nliterature, we propose a framework to quantify how an open-weight model's\ninfluence evolves. Specifically, we adapt the model introduced by Wang et al.\nfor scientific citations, using three key parameters-immediacy, longevity, and\nrelative fitness-to track the cumulative number of fine-tuned models of an\nopen-weight model. Our findings reveal that this citation-style approach can\neffectively capture the diverse trajectories of open-weight model adoption,\nwith most models fitting well and outliers indicating unique patterns or abrupt\njumps in usage.",
        "LLMs have immense potential for generating plans, transforming an initial\nworld state into a desired goal state. A large body of research has explored\nthe use of LLMs for various planning tasks, from web navigation to travel\nplanning and database querying. However, many of these systems are tailored to\nspecific problems, making it challenging to compare them or determine the best\napproach for new tasks. There is also a lack of clear and consistent evaluation\ncriteria. Our survey aims to offer a comprehensive overview of current LLM\nplanners to fill this gap. It builds on foundational work by Kartam and Wilkins\n(1990) and examines six key performance criteria: completeness, executability,\noptimality, representation, generalization, and efficiency. For each, we\nprovide a thorough analysis of representative works and highlight their\nstrengths and weaknesses. Our paper also identifies crucial future directions,\nmaking it a valuable resource for both practitioners and newcomers interested\nin leveraging LLM planning to support agentic workflows.",
        "Retinal vascular morphology is crucial for diagnosing diseases such as\ndiabetes, glaucoma, and hypertension, making accurate segmentation of retinal\nvessels essential for early intervention. Traditional segmentation methods\nassume that training and testing data share similar distributions, which can\nlead to poor performance on unseen domains due to domain shifts caused by\nvariations in imaging devices and patient demographics. This paper presents a\nnovel approach, DGSSA, for retinal vessel image segmentation that enhances\nmodel generalization by combining structural and style augmentation strategies.\nWe utilize a space colonization algorithm to generate diverse vascular-like\nstructures that closely mimic actual retinal vessels, which are then used to\ngenerate pseudo-retinal images with an improved Pix2Pix model, allowing the\nsegmentation model to learn a broader range of structure distributions.\nAdditionally, we utilize PixMix to implement random photometric augmentations\nand introduce uncertainty perturbations, thereby enriching stylistic diversity\nand significantly enhancing the model's adaptability to varying imaging\nconditions. Our framework has been rigorously evaluated on four challenging\ndatasets-DRIVE, CHASEDB, HRF, and STARE-demonstrating state-of-the-art\nperformance that surpasses existing methods. This validates the effectiveness\nof our proposed approach, highlighting its potential for clinical application\nin automated retinal vessel analysis."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The Llama 3 Herd of Models",
    "start_abstract":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Quantifying Variance in Evaluation Benchmarks"
      ],
      "abstract":[
        "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "The Dust Polarisation and Magnetic Field Structure in the Centre of\n  NGC253 with ALMA",
        "Mind the Gap: Evaluating Patch Embeddings from General-Purpose and\n  Histopathology Foundation Models for Cell Segmentation and Classification",
        "Prospects for measuring neutrino mass with 21-cm forest",
        "Stochastic conformal integrators for linearly damped stochastic Poisson\n  systems",
        "Enhanced superconducting correlations in the Emery model and its\n  connections to strange metallic transport and normal state coherence",
        "FF7: A Code Package for High-throughput Calculations and Constructing\n  Materials Database",
        "A three-body form factor at sub-leading power in the high-energy limit:\n  planar contributions",
        "Constraints on flavor-dependent long-range interactions of high-energy\n  astrophysical neutrinos",
        "Variational formulation of planar linearized elasticity with\n  incompatible kinematics",
        "A Hierarchical Shock Model of Ultra-High-Energy Cosmic Rays",
        "Extending Israel-Stewart theory: Causal bulk viscosity at large\n  gradients",
        "DUNE-PRISM: Reducing neutrino interaction model dependence with a\n  movable neutrino detector",
        "Tailoring the Electronic Structure of Ni(111) by Alloying with Sb\n  Ad-Atoms",
        "On the Implementation of a Bayesian Optimization Framework for\n  Interconnected Systems",
        "Uniqueness of the strong positive solution for a general quasilinear\n  elliptic problem with variable exponents and homogeneous Neumann boundary\n  conditions using a generalization of the $p(x)$-D\\'{i}az-Saa inequality",
        "Entanglement entropy by tensor renormalization group approach",
        "TuMag: the tunable magnetograph for the Sunrise III mission",
        "Inelastic neutrino-nucleus scattering off $^{203\/205}$Tl in terms of the\n  nuclear recoil energy using a hybrid nuclear model",
        "Long-range nonstabilizerness and phases of matter",
        "Causality constraints on radiative transfer",
        "Core Collapse Supernova Gravitational Wave Sourcing and Characterization\n  based on Three-Dimensional Models",
        "A Low-Rank QTT-based Finite Element Method for Elasticity Problems",
        "Photoexcitation-induced Stacking Transition Assisted by Intralayer\n  Reconstruction in Charge-Density-Wave Materials",
        "Transient and steady convection in two dimensions",
        "Flow and thermal modelling of the argon volume in the DarkSide-20k TPC",
        "Extreme Events of Quantum Walks on Graphs",
        "On free subgroups in Leavitt path algebras",
        "Quantifying the imaginarity via different distance measures",
        "Long-time behavior of Ricci flow on some complex surfaces"
      ],
      "abstract":[
        "Magnetic fields have an impact on galaxy evolution at multiple scales. They\nare particularly important for starburst galaxies, where they play a crucial\nrole in shaping the interstellar medium (ISM), influencing star formation\nprocesses and interacting with galactic outflows. The primary aim of this study\nis to obtain a parsec scale map of dust polarisation and B-field structure\nwithin the central starburst region of NGC253. This includes examining the\nrelationship between the morphology of B-fields, galactic outflows and the\nspatial distribution of super star clusters (SSC), to understand their combined\neffects on the galaxy's star formation and ISM. We used ALMA full polarisation\ndata in Bands 4 (145 GHz) and 7 (345 GHz) with resolution of 25 and 5 pc scale,\nrespectively. According to our SED fitting analysis, the observed Band 4\nemission is a combination of dust, synchrotron and free-free, while Band 7\ntraces only dust. The polarisation fraction (PF) of the synchrotron component\nis 2%, while that of the dust component is 0.3%. The B-fields orientation maps\nin both bands at common resolution show that the same B-fields structure is\ntraced by dust and synchrotron emission at scales of 25 pc. The B-field\nmorphology suggests a coupling with the multiphase outflow, while the\ndistribution of PF in Band 7 showed to be correlated with the presence of SSC.\nWe observed a significant anti-correlation between polarisation fraction and\ncolumn density in both Bands 4 and 7. A negative correlation between PF and\ndispersion angle function was observed in Band 4 but was nearly absent in Band\n7 at native resolution, suggesting that the tangling of B-field geometry along\nthe plane of the sky is the main cause of depolarisation at 25 pc scales, while\nother factors play a role at 5 pc scales.",
        "Recent advancements in foundation models have transformed computer vision,\ndriving significant performance improvements across diverse domains, including\ndigital histopathology. However, the advantages of domain-specific\nhistopathology foundation models over general-purpose models for specialized\ntasks such as cell analysis remain underexplored. This study investigates the\nrepresentation learning gap between these two categories by analyzing\nmulti-level patch embeddings applied to cell instance segmentation and\nclassification. We implement an encoder-decoder architecture with a consistent\ndecoder and various encoders. These include convolutional, vision transformer\n(ViT), and hybrid encoders pre-trained on ImageNet-22K or LVD-142M,\nrepresenting general-purpose foundation models. These are compared against ViT\nencoders from the recently released UNI, Virchow2, and Prov-GigaPath foundation\nmodels, trained on patches extracted from hundreds of thousands of\nhistopathology whole-slide images. The decoder integrates patch embeddings from\ndifferent encoder depths via skip connections to generate semantic and distance\nmaps. These maps are then post-processed to create instance segmentation masks\nwhere each label corresponds to an individual cell and to perform cell-type\nclassification. All encoders remain frozen during training to assess their\npre-trained feature extraction capabilities. Using the PanNuke and CoNIC\nhistopathology datasets, and the newly introduced Nissl-stained CytoDArk0\ndataset for brain cytoarchitecture studies, we evaluate instance-level\ndetection, segmentation accuracy, and cell-type classification. This study\nprovides insights into the comparative strengths and limitations of\ngeneral-purpose vs. histopathology foundation models, offering guidance for\nmodel selection in cell-focused histopathology and brain cytoarchitecture\nanalysis workflows.",
        "Both particle physics experiments and cosmological observations have been\nused to explore neutrino properties. Cosmological researches of neutrinos often\nrely on the early-universe cosmic microwave background observations or other\nlate-universe probes, which mostly focus on large-scale structures. We\nintroduce a distinct probe, the 21-cm forest, that differs from other probes in\nboth time and scale. Actually, the 21-cm forest is a unique tool for studying\nsmall-scale structures in the early universe. Below the free-streaming scale,\nmassive neutrinos suppress the matter power spectrum, influencing small-scale\nfluctuations in the distribution of matter. The one-dimensional (1D) power\nspectrum of the 21-cm forest can track these fluctuations across different\nscales, similar to the matter power spectrum, providing an effective method to\nconstrain neutrino mass. Although heating effects in the early universe can\nalso impact the 1D power spectrum of the 21-cm forest, we assess the potential\nof the 21-cm forest as a tool for measuring neutrino mass, given that the\ntemperature of the intergalactic medium can be constrained using other methods\nwithin a certain range. In the ideal scenario, the 21-cm forest observation\nwill have the ability to constrain the total neutrino mass to around 0.1 eV.\nWith the accumulation of observational data and advancements in observational\ntechnology, the 21-cm forest holds great promise as an emerging and potent tool\nfor measuring neutrino mass.",
        "We propose and study conformal integrators for linearly damped stochastic\nPoisson systems. We analyse the qualitative and quantitative properties of\nthese numerical integrators: preservation of dynamics of certain Casimir and\nHamiltonian functions, almost sure bounds of the numerical solutions, and\nstrong and weak rates of convergence under appropriate conditions. These\ntheoretical results are illustrated with several numerical experiments on, for\nexample, the linearly damped free rigid body with random inertia tensor or the\nlinearly damped stochastic Lotka--Volterra system.",
        "Numerical evidence for superconductivity in the single-band Hubbard model is\nelusive or ambiguous despite extensive study, raising the question of whether\nthe single-band Hubbard model is a faithful low energy effective model for\ncuprates, and whether explicitly including the oxygen ions will recover the\nproperties necessary for superconducting transition. Here we show, by using\nnumerically exact determinant quantum Monte Carlo (DQMC) simulations of the\ndoped two-dimensional three-band Emery model, that while the single-band model\nexhibits strikingly T-linear transport behavior, the three-band model shows a\nlow temperature resistivity curvature indicating a crossover to a more metallic\ntransport regime. Evidence has also been found in thermodynamic and\nsuperconducting measurements, which suggests that some degree of coherence in\ntransport might be necessary for the high-temperature superconductivity in\ncuprates, further implying a possible connection between superconducting and\ntransport behaviors.",
        "Decades accumulation of theory simulations lead to boom in material database,\nwhich combined with machine learning methods has been a valuable driver for the\ndata-intensive material discovery, i.e., the fourth research paradigm. However,\nconstruction of segmented databases and data reuse in generic databases with\nuniform parameters still lack easy-to-use code tools. We herein develop a code\npackage named FF7 (Fast Funnel with 7 modules) to provide command-line based\ninteractive interface for performing customized high-throughput calculations\nand building your own handy databases. Data correlation studies and material\nproperty prediction can progress by built-in installation-free artificial\nneural network module and various post processing functions are also supported\nby auxiliary module. This paper shows the usage of FF7 code package and\ndemonstrates its usefulness by example of database driven thermodynamic\nstability high-throughput calculation and machine learning model for predicting\nthe superconducting critical temperature of clathrate hydrides.",
        "We consider two-loop planar contributions to a three-body form factor at the\nnext-to-leading power in the high-energy limit, where the masses of external\nparticles are much smaller than their energies. The calculation is performed by\nexploiting the differential equations of the expansion coefficients, both for\nfacilitating the linear relations among them, and for deriving their analytic\nexpressions. The result is written in terms of generalized polylogarithms\ninvolving a few simple symbol letters. Our method can be readily applied to the\ncalculation of non-planar contributions as well. The result provides crucial\ninformation for establishing sub-leading factorization theorems for massive\nscattering amplitudes in the high-energy limit.",
        "Astrophysical neutrinos with energy in the TeV-PeV range traverse megaparsecs\n(Mpc) to gigaparsecs (Gpc) scale distances before they reach the Earth. Tiny\nphysics effects that get accumulated over these large propagation paths during\ntheir journey may become observable at the detector. If there is some new\ninteraction between neutrinos and the background matter, that can potentially\naffect the propagation of the astrophysical neutrinos. One such possible case\nis the flavor-dependent long-range interaction of neutrinos, which can affect\nthe standard neutrino flavor transition, modifying the flavor composition of\nthe astrophysical neutrinos at Earth. Using the present-day and future\nprojection of the flavor-composition measurements of IceCube and IceCube-Gen2\nalong with the present and future measurement of the oscillation parameters, we\nexplore the sensitivity of these experiments to probe long-range neutrino\ninteraction with matter.",
        "We present a variational characterization of mechanical equilibrium in the\nplanar strain regime for systems with incompatible kinematics. For non-simply\nconnected domains, we show that the equilibrium problem for a non-liftable\nstrain-stress pair can be reformulated as a well-posed minimization problem for\nthe Airy potential of the system. We characterize kinematic incompatibilities\non internal boundaries as rotational or translational mismatches, in agreement\nwith Volterra's modeling of disclinations and dislocations. Finally, we\nestablish that the minimization problem for the Airy potential can be reduced\nto a finite-dimensional optimization involving cell formulas.",
        "We propose that a hierarchical shock model$\\unicode{x2014}$including\nsupernova remnant shocks, galactic wind termination shocks, and accretion\nshocks around cosmic filaments and galaxy clusters$\\unicode{x2014}$can\nnaturally explain the cosmic ray spectrum from ~1 GeV up to ~200 EeV. While\nthis framework applies to the entire cosmic ray spectrum, in this work, we\nfocus on its implications for ultra-high-energy cosmic rays (UHECRs). We\nperform a hydrodynamic cosmological simulation to investigate the power\nprocessed at shocks around clusters and filaments. The downstream flux from\nnearby shocks around the local filament accounts for the softer, lower-energy\nextragalactic component around the ankle, and the upstream escaping flux from\nnearby clusters accounts for the transition to a hard spectral component at the\nhighest energies. This interpretation is in agreement with UHECR observations.\nWe suggest that a combination of early-Universe galactic outflows, cosmic ray\nstreaming instabilities, and a small-scale turbulent dynamo can increase\nmagnetic fields enough to attain the required rigidities. Our simulation\nsuggests that the available volume-averaged power density of accretion shocks\nexceeds the required UHECR luminosity density by three orders of magnitude. We\nshow that microgauss magnetic fields at these shocks could explain both the\norigin of UHECRs and the as-yet unidentified source of the diffuse radio\nsynchrotron background below 10 GHz. The shock-accelerated electrons produce a\nhard radio background without overproducing diffuse inverse Compton emission.\nThese results motivate further observational tests with upcoming facilities to\nhelp distinguish accretion shocks from other UHECR sources.",
        "We present a class of relativistic fluid models for cold and dense matter\nwith bulk viscosity, whose equilibrium equation of state is polytropic. These\nmodels reduce to Israel-Stewart theory for small values of the viscous stress\n$\\Pi$. However, when $\\Pi$ becomes comparable to the equilibrium pressure $P$,\nthe evolution equations \"adjust\" to prevent the onset of far-from-equilibrium\npathologies that would otherwise plague Israel-Stewart. Specifically, the\nequations of motion remain symmetric hyperbolic and causal at all times along\nany continuously differentiable flow, and across the whole thermodynamic state\nspace. This means that, no matter how fast the fluid expands or contracts, the\nhydrodynamic equations are always well-behaved (away from singularities). The\nsecond law of thermodynamics is enforced exactly. Near equilibrium, these\nmodels can accommodate an arbitrarily complicated dependence of the bulk\nviscosity coefficient $\\zeta$ on both density and temperature.",
        "The Deep Underground Neutrino Experiment (DUNE) is a next-generation\nlong-baseline neutrino oscillation experiment designed to make precision\nmeasurements in the world's most powerful neutrino beam. Neutrinos are measured\nat two detector facilities: a near detector (ND) located at Fermilab close to\nwhere the beam is produced and a far detector (FD) at SURF. The Precision\nReaction Independent Spectrum Measurement (PRISM) system allows for the\nmeasurement of different neutrino energy spectra by moving the near detector\naway from the central axis of the neutrino beam. These off-axis neutrino energy\nspectra provide a new degree of freedom that can be used to develop a deeper\nunderstanding of the relationship between the observable energy deposits in the\ndetector and the energy of the interacting neutrino. This can benefit DUNE by\nsignificantly reducing the impact of systematic uncertainties in the neutrino\ninteraction model. One possible use of the PRISM system is to perform a novel\nneutrino oscillation analysis that linearly combines off-axis neutrino energy\nspectra at the near detector to produce data-driven predictions of the far\ndetector energy spectrum.",
        "Surface alloying can alter surface electronic and magnetic properties, which\nare key parameters when developing new materials tailored for specific\napplications. A magnetic surface alloy was formed by depositing Sb on Ni(111)\nat elevated temperatures, yielding new electronic states at the Fermi level and\nmodifying the Ni-derived bandstructure. In particular, it changed the electron\noccupancy of the spin-polarized surface resonance bands, which may affect the\nmagnetic properties of the surface and its associated many-body effects. By\nfitting a finite element model to angle-dependent core level measurements,\nsimilar amounts of Sb and Ni were found within the first few atomic layers to\nindicate a near-surface composition similar to the bulk alloy NiSb. Annealing\nto higher temperatures post-growth further improved the crystalline quality of\nthe surface. Our investigation of the surface alloy's crystallinity, chemical\ncomposition, and layer structure lays the basis for future studies of how its\nelectronic and magnetic properties can be modified.",
        "Bayesian optimization (BO) is an effective paradigm for the optimization of\nexpensive-to-sample systems. Standard BO learns the performance of a system\n$f(x)$ by using a Gaussian Process (GP) model; this treats the system as a\nblack-box and limits its ability to exploit available structural knowledge\n(e.g., physics and sparse interconnections in a complex system). Grey-box\nmodeling, wherein the performance function is treated as a composition of known\nand unknown intermediate functions $f(x, y(x))$ (where $y(x)$ is a GP model)\noffers a solution to this limitation; however, generating an analytical\nprobability density for $f$ from the Gaussian density of $y(x)$ is often an\nintractable problem (e.g., when $f$ is nonlinear). Previous work has handled\nthis issue by using sampling techniques or by solving an auxiliary problem over\nan augmented space where the values of $y(x)$ are constrained by confidence\nintervals derived from the GP models; such solutions are computationally\nintensive. In this work, we provide a detailed implementation of a recently\nproposed grey-box BO paradigm, BOIS, that uses adaptive linearizations of $f$\nto obtain analytical expressions for the statistical moments of the composite\nfunction. We show that the BOIS approach enables the exploitation of structural\nknowledge, such as that arising in interconnected systems as well as systems\nthat embed multiple GP models and combinations of physics and GP models. We\nbenchmark the effectiveness of BOIS against standard BO and existing grey-box\nBO algorithms using a pair of case studies focused on chemical process\noptimization and design. Our results indicate that BOIS performs as well as or\nbetter than existing grey-box methods, while also being less computationally\nintensive.",
        "In this paper, we study a generalization of the D\\'iaz-Saa inequality and its\napplications to nonlinear elliptic problems. We first present the necessary\nhypotheses and preliminary results before introducing an improved version of\nthe inequality, which holds in a broader functional setting and allows\napplications to problems with homogeneous Neumann boundary conditions. The\nsignificance of cases where the inequality becomes an equality is also\nanalyzed, leading to uniqueness results for certain classes of partial\ndifferential equations. Furthermore, we provide a detailed proof of a\nuniqueness theorem for strong positive solutions and illustrate our findings\nwith two concrete applications: a multiple-phase problem and an elliptic\nquasilinear equation relevant to image processing. The paper concludes with\npossible directions for future research.",
        "We report on tensor renormalization group calculations of entanglement\nentropy in one-dimensional quantum systems. The reduced density matrix of a\nGibbs state can be represented as a $1 + 1$-dimensional tensor network, which\nis analogous to the tensor network representation of the partition function.\nThe HOTRG method is used to approximate the reduced density matrix for\narbitrary subsystem sizes, from which we obtain the entanglement entropy. We\ntest our method in the quantum Ising model and obtain the entanglement entropy\nof the ground state by taking the size of time direction to infinity. The\ncentral charge $c$ is obtained as $c = 0.49997(8)$ for a bond dimension $D=96$,\nwhich agrees with the theoretical value $c=1\/2$ within the error.",
        "One of the instruments aboard the Sunrise III mission, the Tunable\nMagnetograph (TuMag), is a tunable imaging spectropolarimeter in visible\nwavelengths. It is designed to probe the vector magnetic field and the\nline-of-sight velocity of the photosphere and the lower chromosphere. The\nquasi-simultaneous observation of two spectral lines provides excellent\ndiagnostic measurements of the magnetic and dynamic coupling in these layers.\n  The key technologies employed for TuMag are an LCVR-based polarimeter and a\nsolid, LiNbO3 Fabry-P\\'erot etalon as a spectrometer. However, it also\nincorporates several innovative features, such as home-made high-sensitivity\nscientific cameras and a double filter wheel. TuMag can sequentially observe\nany two out of the three spectral lines of Fe I at 525.02 and 525.06 nm and of\nMg I at 517.3 nm.\n  Laboratory measurements have demonstrated outstanding performance, including\na wavefront root-mean-square error better than {\\lambda}\/13 for image quality,\na full-width-at-half-maximum of 8.7 pm for the filtergraph transmission\nprofile, and polarimetric efficiencies > 0.54. Here we report on the concept,\ndesign, calibration, and integration phases of the instrument, as well as on\nthe data reduction pipeline.",
        "Nuclear structure calculations in the context of a novel hybrid nuclear\nmodel, combining the nuclear shell model and the microscopic\nquasiparticle-phonon model are presented. The predictivity of the hybrid model\nis tested by computing inelastic neutral-current neutrino-nucleus scattering\ncross sections off the stable thallium isotopes. The cross sections are\npresented in terms of the incoming neutrino energy, taking also into account\nthe effect of nuclear recoil energy. Also reported are the expected event rates\nassuming neutrinos emerging from pion-decay at rest and the diffuse supernova\nneutrino background. Regarding solar neutrino rates, new results are presented\nin the context of the hybrid model and compared with previously reported\nresults based solely on nuclear shell model calculations, demonstrating the\nimproved accuracy of the adopted hybrid model at higher neutrino energies.",
        "Long-range nonstabilizerness can be defined as the amount of\nnonstabilizerness which cannot be removed by shallow local quantum circuits. In\nthis work, we study long-range nonstabilizerness in the context of many-body\nquantum physics, a task with possible implications for quantum-state\npreparation protocols and implementation of quantum-error correcting codes.\nAfter presenting a simple argument showing that long-range nonstabilizerness is\na generic property of many-body states, we restrict to the class of ground\nstates of gapped local Hamiltonians. We focus on one-dimensional systems and\npresent rigorous results in the context of translation-invariant matrix product\nstates (MPSs). By analyzing the fixed points of the MPS renormalization-group\nflow, we provide a sufficient condition for long-range nonstabilizerness, which\ndepends entirely on the local MPS tensors. Physically, our condition captures\nthe fact that the mutual information between distant regions of stabilizer\nfixed points is quantized, and this fact is not changed after applying shallow\nquantum circuits. We also discuss possible ramifications in the classification\nof phases of matter and quantum error correction.",
        "The standard formula, due to Spiegel, for the smoothing of temperature\nfluctuations by radiative transfer is unstable in relativity. This is due to\nthe fact that Spiegel neglected the transit time of light, thereby allowing the\ntransport coefficients to move outside the convex geometry compatible with\ncausality (the \"hydrohedron\"). Here, we fix this pathology. First, we prove\nthat the linearized radiative transfer equations are causal and covariantly\nstable by construction. Then, we repeat Spiegel's calculation accounting for\nthe finite speed of photons. We find that the full transfer problem can be\nsolved analytically. All the infinite (exact) transport coefficients arising\nfrom it fall inside the hydrohedron. Our analysis also accounts for isotropic\nscattering.",
        "We present for the first time an analysis of high-frequency gravitational\nwave (GW) emission from proto-neutron stars (PNS) in core collapse supernovae\n(CCSNe) that combines spatial decomposition and modal decomposition to both\nsource and characterize the emission. Our analysis is based on\nthree-dimensional CCSN simulations initiated from two progenitors with\ndiffering mass and metallicity. We spatially decompose GW strains into five\nregions and show they are initially largest in the PNS surface layers from\naccretion and later largest from the Ledoux convective and convective overshoot\nregions within the PNS. We compute the fractional GW luminosity and observe\nthat the majority of the luminosity moves from the same outer layers to deep\nwithin the PNS at comparable times. Using a self-consistent perturbative\nanalysis, we investigate the evolution of the oscillation modes of the PNS. We\nfind that the frequency of the evolving high-frequency component of the GW\nsignal is well matched to the ${}^2g_2$-mode, the ${}^2g_1$-mode, and the\n${}^2f$-mode over time. We show that the ${}^2g$-modes emit most of their power\nin GWs initially from the PNS surface region, but within a few 100 ms after\nbounce, it is the convective overshoot region of the PNS that emits the most GW\npower for the ${}^2g_1$-mode. Eventually, the ${}^2f$-mode is the dominant mode\nproducing GWs, and they are emitted primarily from the convective overshoot\nregion. Thus, with three interconnected analyses, we show that, while the GW\nemission is global, stemming from multiple regions in and around the PNS, we\nare able to source the dominant contributions to it. We find that\nhigh-frequency GW emission from the PNS in CCSNe is more complex than assessed\nby other methods, and dependent, first emitted mainly by ${}^2g$-modes driven\nby accretion onto the PNS and later emitted by the ${}^2f$-mode driven by\nsustained Ledoux convection.",
        "We present an efficient and robust numerical algorithm for solving the\ntwo-dimensional linear elasticity problem that combines the Quantized Tensor\nTrain format and a domain partitioning strategy. This approach makes it\npossible to solve the linear elasticity problem on a computational domain that\nis more general than a square. Our method substantially decreases memory usage\nand achieves a notable reduction in rank compared to established Finite Element\nimplementations like the FEniCS platform. This performance gain, however,\nrequires a fundamental rethinking of how core finite element operations are\nimplemented, which includes changes to mesh discretization, node and degree of\nfreedom ordering, stiffness matrix and internal nodal force assembly, and the\nexecution of algebraic matrix-vector operations. In this work, we discuss all\nthese aspects in detail and assess the method's performance in the numerical\napproximation of three representative test cases.",
        "Laser excitation has emerged as an effective tool for probing microscopic\ninteractions and manipulating phases of matter. Among charge density wave (CDW)\nmaterials, 1T-TaS2 has garnered significant attention due to its diverse\nstacking orders and photoexcited responses. However, the mechanisms driving\ntransitions among different stacking orders and the microscopic\nout-of-equilibrium dynamics remain unclear. We elucidate that photoexcitation\ncan introduce interlayer stacking order transitions facilitated by\nlaser-induced intralayer reconstruction in 1T-TaS2. Importantly, our finding\nreveals a novel pathway to introduce different phases through laser\nexcitations, apparently distinct from thermally-induced phase transitions via\ninterlayer sliding. In particular, photoexcitation is able to considerably\nchange potential energy surfaces and evoke collective lattice dynamics.\nConsequently, the laser-induced intralayer reconstruction plays a crucial role\nin interlayer stacking-order transition, offering a new method to create exotic\nstackings and quantum phases. The exploration opens up great opportunities for\nmanipulating CDW phases and electronic properties on the femtosecond timescale.",
        "We simulate thermal convection in a two-dimensional square box using the\nno-slip condition on all boundaries, and isothermal bottom and top walls and\nadiabatic sidewalls. We choose 0.1 and 1 for the Prandtl number and vary the\nRayleigh number between $10^6$ and $10^{12}$. We particularly study the\ntemporal evolution of integral transport quantities towards their steady\nstates. Perhaps not surprisingly, the velocity field evolves more slowly than\nthe thermal field. Its steady state is nominal in the sense that\nlarge-amplitude low-frequency oscillations persist around plausible averages.\nWe study these oscillation characteristics.",
        "The DarkSide-20k dark matter experiment, currently under construction at\nLNGS, features a dual-phase time projection chamber (TPC) with a ~50 t argon\ntarget from an underground well. At this scale, it is crucial to optimise the\nargon flow pattern for efficient target purification and for fast distribution\nof internal gaseous calibration sources with lifetimes of the order of hours.\nTo this end, we have performed computational fluid dynamics simulations and\nheat transfer calculations. The residence time distribution shows that the\ndetector is well-mixed on time-scales of the turnover time (~40 d). Notably,\nsimulations show that despite a two-order-of-magnitude difference between the\nturnover time and the half-life of $^{83\\text{m}}$Kr of 1.83 h, source atoms\nhave the highest probability to reach the centre of the TPC 13 min after their\ninjection, allowing for a homogeneous distribution before undergoing\nradioactive decay. We further analyse the thermal aspects of dual-phase\noperation and define the requirements for the formation of a stable gas pocket\non top of the liquid. We find a best-estimate value for the heat transfer rate\nat the liquid-gas interface of 62 W with an upper limit of 144 W and a minimum\ngas pocket inlet temperature of 89 K to avoid condensation on the acrylic\nanode. This study also informs the placement of liquid inlets and outlets in\nthe TPC. The presented techniques are widely applicable to other large-scale,\nnoble-liquid detectors.",
        "Due to the unitary evolution, quantum walks display different dynamical\nfeatures from that of classical random walks. In contrast to this expectation,\nin this work, we show that extreme events can arise in unitary dynamics and its\nproperties are qualitatively similar to that of random walks. We consider\nquantum walks on a ring lattice and a scale-free graph. Firstly, we obtain\nquantum version of flux-fluctuation relation and use this to define to extreme\nevents on vertices of a graph as exceedences above the mean flux. The\noccurrence probability for extreme events on scale-free graphs displays a\npower-law with the degree of vertices, in qualitative agreement with\ncorresponding classical random walk result. For both classical and quantum\nwalks, the extreme event probability is larger for small degree nodes compared\nto hubs on the graph. Further, it is shown that extreme event probability\nscales with threshold used to define extreme events.",
        "Let $E$ be a graph and $K$ a field. In this paper we prove that the\nmultiplicative group of a unital noncommutative Leavitt path algebra $L_K(E)$\ncontains non-cyclic free subgroups provided $K$ is of characteristic $0$.\nFurther, we provide a description of the generators of such free subgroups in\nterm of the graph $E$.",
        "The recently introduced resource theory of imaginarity facilitates a\nsystematic investigation into the role of complex numbers in quantum mechanics\nand quantum information theory. In this work, we propose well-defined measures\nof imaginarity using various distance metrics, drawing inspiration from recent\nadvancements in quantum entanglement and coherence. Specifically, we focus on\nquantitatively evaluating imaginarity through measures such as Tsallis relative\n$\\alpha$-entropy, Sandwiched R\\'{e}nyi relative entropy, and Tsallis relative\noperator entropy. Additionally, we analyze the decay rates of these measures.\nOur findings reveal that the Tsallis relative $\\alpha$-entropy of imaginarity\nexhibits higher decay rate under quantum channels compared to other measures.\nFinally, we examine the ordering of single-qubit states under these imaginarity\nmeasures, demonstrating that the order remains invariant under the bit-flip\nchannel for specific parameter ranges. This study enhances our understanding of\nimaginarity as a quantum resource and its potential applications in quantum\ninformation theory.",
        "We give biLipschitz models for the Ricci flow on some 4-manifolds (minimal\nsurfaces of general type), exhibiting a combination of expanding and static\nbehavior."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Pediatric low-grade glioma: State-of-the-art and ongoing challenges",
    "start_abstract":"Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
      ],
      "abstract":[
        "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings",
        "Separate surface and bulk topological Anderson localization transitions\n  in disordered axion insulators",
        "Harmonic And Transposition Constraints Arising From The Use Of The\n  Roland TR-808 Bass Drum",
        "Quantifying Point Contributions: A Lightweight Framework for Efficient\n  and Effective Query-Driven Trajectory Simplification",
        "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf",
        "A Hybrid Model\/Data-Driven Solution to Channel, Position and Orientation\n  Tracking in mmWave Vehicular Systems",
        "Exploring the Potential of QEEGNet for Cross-Task and Cross-Dataset\n  Electroencephalography Encoding with Quantum Machine Learning",
        "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic\n  Data Generation and Fairness Algorithms",
        "Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation",
        "Dango: A Mixed-Initiative Data Wrangling System using Large Language\n  Model",
        "6GStarLab -- A CubeSat Mission to support the development and\n  standardization of Non-Terrestrial Networks towards 6G",
        "Hints of Primordial Magnetic Fields at Recombination and Implications\n  for the Hubble Tension",
        "EMK-KEN: A High-Performance Approach for Assessing Knowledge Value in\n  Citation Network",
        "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention",
        "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models",
        "Grammar and Gameplay-aligned RL for Game Description Generation with\n  LLMs",
        "Classical elasticity meets quantum complexity: A connection from the\n  holographic lens",
        "Contact value theorem for electric double layers with modulated surface\n  charge density",
        "Leader-follower formation enabled by pressure sensing in free-swimming\n  undulatory robotic fish",
        "Distribution and Moments of a Normalized Dissimilarity Ratio for two\n  Correlated Gamma Variables",
        "Three-stage dynamics of nonlinear pulse amplification in ultrafast\n  mid-infrared fiber amplifier with anomalous dispersion",
        "DNN-Powered MLOps Pipeline Optimization for Large Language Models: A\n  Framework for Automated Deployment and Resource Management",
        "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration",
        "Singular leaning coefficients and efficiency in learning theory",
        "Topological derivative approach for deep neural network architecture\n  adaptation",
        "Robust Moving-horizon Estimation for Nonlinear Systems: From Perfect to\n  Imperfect Optimization",
        "Brown dwarf number density in the JWST COSMOS-Web field",
        "Elemental and angular fragmentation cross section measurements with the\n  FOOT experiment",
        "RUSSO: Robust Underwater SLAM with Sonar Optimization against Visual\n  Degradation"
      ],
      "abstract":[
        "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps:\/\/github.com\/ZeroNLP\/SEA.",
        "In topological phases of matter for which the bulk and boundary support\ndistinct electronic gaps, there exists the possibility of decoupled mobility\ngaps in the presence of disorder. This is in analogy with the well-studied\nproblem of realizing separate or concomitant bulk-boundary criticality in\nconventional Landau theory. Using a three-dimensional axion insulator having\nclean, gapped surfaces with $e^2\/2h$ quantized Hall conductance, we show the\nbulk and surface mobility gap evolve differently in the presence of disorder.\nThe decoupling of the bulk and surface topology yields a regime that realizes a\ntwo-dimensional, unquantized anomalous Hall metal in the Gaussian unitary\nensemble (GUE), which shares some spectral and response properties akin to the\nsurface states of a conventional three-dimensional (3D) topological insulator.\nThe generality of these results as well as extensions to other insulators and\nsuperconductors is discussed.",
        "The study investigates hip-hop music producer Scott Storch's approach to\ntonality, where the song's key is transposed to fit the Roland TR-808 bass drum\ninstead of tuning the drums to the song's key. This process, involving the\nadjustment of all tracks except the bass drum, suggests significant production\nmotives. The primary constraint stems from the limited usable pitch range of\nthe TR-808 bass drum if its characteristic sound is to be preserved. The\nresearch examines drum tuning practices, the role of the Roland TR-808 in\nmusic, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples\nreveals their characteristics and their integration into modern genres like\ntrap and hip-hop. The study also considers the impact of loudspeaker frequency\nresponse and human ear sensitivity on bass drum perception. The findings\nsuggest that Storch's method prioritizes the spectral properties of the bass\ndrum over traditional pitch values to enhance the bass response. The need to\nmaintain the unique sound of the TR-808 bass drum underscores the importance of\nspectral formants and register in contemporary popular music production.",
        "As large volumes of trajectory data accumulate, simplifying trajectories to\nreduce storage and querying costs is increasingly studied. Existing proposals\nface three main problems. First, they require numerous iterations to decide\nwhich GPS points to delete. Second, they focus only on the relationships\nbetween neighboring points (local information) while neglecting the overall\nstructure (global information), reducing the global similarity between the\nsimplified and original trajectories and making it difficult to maintain\nconsistency in query results, especially for similarity-based queries. Finally,\nthey fail to differentiate the importance of points with similar features,\nleading to suboptimal selection of points to retain the original trajectory\ninformation.\n  We propose MLSimp, a novel Mutual Learning query-driven trajectory\nsimplification framework that integrates two distinct models: GNN-TS, based on\ngraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluates\nthe importance of a point according to its globality, capturing its correlation\nwith the entire trajectory, and its uniqueness, capturing its differences from\nneighboring points. It also incorporates attention mechanisms in the GNN\nlayers, enabling simultaneous data integration from all points within the same\ntrajectory and refining representations, thus avoiding iterative processes.\nDiff-TS generates amplified signals to enable the retention of the most\nimportant points at low compression rates. Experiments involving eight\nbaselines on three databases show that MLSimp reduces the simplification time\nby 42%--70% and improves query accuracy over simplified trajectories by up to\n34.6%.",
        "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events.",
        "Channel tracking in millimeter wave (mmWave) vehicular systems is crucial for\nmaintaining robust vehicle-to-infrastructure (V2I) communication links, which\ncan be leveraged to achieve high accuracy vehicle position and orientation\ntracking as a byproduct of communication. While prior work tends to simplify\nthe system model by omitting critical system factors such as clock offsets,\nfiltering effects, antenna array orientation offsets, and channel estimation\nerrors, we address the challenges of a practical mmWave multiple-input\nmultiple-output (MIMO) communication system between a single base station (BS)\nand a vehicle while tracking the vehicle's position and orientation (PO)\nconsidering realistic driving behaviors. We first develop a channel tracking\nalgorithm based on multidimensional orthogonal matching pursuit (MOMP) with\nfactoring (F-MOMP) to reduce computational complexity and enable\nhigh-resolution channel estimates during the tracking stage, suitable for PO\nestimation. Then, we develop a network called VO-ChAT (Vehicle\nOrientation-Channel Attention for orientation Tracking), which processes the\nchannel estimate sequence for orientation prediction. Afterward, a weighted\nleast squares (WLS) problem that exploits the channel geometry is formulated to\ncreate an initial estimate of the vehicle's 2D position. A second network named\nVP-ChAT (Vehicle Position-Channel Attention for position Tracking) refines the\ngeometric position estimate. VP-ChAT is a Transformer inspired network\nprocessing the historical channel and position estimates to provide the\ncorrection for the initial geometric position estimate. The proposed solution\nis evaluated using raytracing generated channels in an urban canyon\nenvironment. For 80% of the cases it achieves a 2D position tracking accuracy\nof 26 cm while orientation errors are kept below 0.5 degree.",
        "Electroencephalography (EEG) is widely used in neuroscience and clinical\nresearch for analyzing brain activity. While deep learning models such as\nEEGNet have shown success in decoding EEG signals, they often struggle with\ndata complexity, inter-subject variability, and noise robustness. Recent\nadvancements in quantum machine learning (QML) offer new opportunities to\nenhance EEG analysis by leveraging quantum computing's unique properties. In\nthis study, we extend the previously proposed Quantum-EEGNet (QEEGNet), a\nhybrid neural network incorporating quantum layers into EEGNet, to investigate\nits generalization ability across multiple EEG datasets. Our evaluation spans a\ndiverse set of cognitive and motor task datasets, assessing QEEGNet's\nperformance in different learning scenarios. Experimental results reveal that\nwhile QEEGNet demonstrates competitive performance and maintains robustness in\ncertain datasets, its improvements over traditional deep learning methods\nremain inconsistent. These findings suggest that hybrid quantum-classical\narchitectures require further optimization to fully leverage quantum advantages\nin EEG processing. Despite these limitations, our study provides new insights\ninto the applicability of QML in EEG research and highlights challenges that\nmust be addressed for future advancements.",
        "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
        "Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.",
        "Data wrangling is a time-consuming and challenging task in a data science\npipeline. While many tools have been proposed to automate or facilitate data\nwrangling, they often misinterpret user intent, especially in complex tasks. We\npropose Dango, a mixed-initiative multi-agent system for data wrangling.\nCompared to existing tools, Dango enhances user communication of intent by\nallowing users to demonstrate on multiple tables and use natural language\nprompts in a conversation interface, enabling users to clarify their intent by\nanswering LLM-posed multiple-choice clarification questions, and providing\nmultiple forms of feedback such as step-by-step natural language explanations\nand data provenance to help users evaluate the data wrangling scripts. We\nconducted a within-subjects user study with 38 participants and demonstrated\nthat Dango's features can significantly improve intent clarification, accuracy,\nand efficiency in data wrangling. Furthermore, we demonstrated the\ngeneralizability of Dango by applying it to a broader set of data wrangling\ntasks.",
        "The emergence of the Non-Terrestrial Network (NTN) concept in the last years\nhas revolutionized the space industry. This novel network architecture composed\nof aircraft and spacecraft is currently being standardized by the 3GPP. This\nstandardization process follows dedicated phases in which experimentation of\nthe technology is needed. Although some missions have been conducted to\ndemonstrate specific and service-centric technologies, a open flexible in-orbit\ninfrastructure is demanded to support this standardization process. This work\npresents the 6GStarLab mission, which aims to address this gap. Specifically,\nthis mission envisions to provide a 6U CubeSat as the main in-orbit\ninfrastructure in which multiple technology validations can be uploaded. The\nconcept of this mission is depicted. Additionally, this work presents the\ndetails of the satellite platform and the payload. This last one is designed to\nenable the experimentation in multiple radio-frequency bands (i.e. UHF, S-, X-,\nand Ka-bands) and an optical terminal. The launch of the satellite is scheduled\nfor Q2 2025, and it will contribute to the standardization of future NTN\narchitectures.",
        "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
        "With the explosive growth of academic literature, effectively evaluating the\nknowledge value of literature has become quite essential. However, most of the\nexisting methods focus on modeling the entire citation network, which is\nstructurally complex and often suffers from long sequence dependencies when\ndealing with text embeddings. Thus, they might have low efficiency and poor\nrobustness in different fields. To address these issues, a novel knowledge\nevaluation method is proposed, called EMK-KEN. The model consists of two\nmodules. Specifically, the first module utilizes MetaFP and Mamba to capture\nsemantic features of node metadata and text embeddings to learn contextual\nrepresentations of each paper. The second module utilizes KAN to further\ncapture the structural information of citation networks in order to learn the\ndifferences in different fields of networks. Extensive experiments based on ten\nbenchmark datasets show that our method outperforms the state-of-the-art\ncompetitors in effectiveness and robustness.",
        "Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and\/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.",
        "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms.",
        "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.",
        "In this work, we explore the effects of shear deformations in a wide class of\nholographic amorphous solids. It is found that both the shear stress and the\ncomplexity of formation grow with the increase of the shear strain. Notably, in\nthe regime of very large shear, they exhibit coordinated behavior and adhere to\na universal scaling relation, uncovering a surprising connection between two\nseemingly unrelated aspects of amorphous systems. Furthermore, our findings\nalso provide a counterexample to the previous understanding that the complexity\nscales linearly with the Bekenstein-Hawking entropy for large static black\nholes.",
        "The contact value theorem was originally derived for Coulomb fluids of mobile\ncharged particles in thermal equilibrium, in the presence of interfaces\ncarrying a {\\em uniform} surface charge density and in the absence of\ndielectric discontinuities. It relates the pressure (the effective force)\nbetween two parallel electric double layers to the particle number density and\nthe surface charge density at the interface, separately for each of the two\nelectric double layers. In this paper, we generalise the contact value theorem\nto electric double layers with interfaces carrying a {\\em modulated} surface\ncharge density. The derivation is based on balance of forces exerted on\ninterfaces. The relevance of particular terms of the contact value theorem is\ntested on an exactly solvable two-dimensional Coulomb system with counterions\nonly at the coupling constant $\\Gamma=2$.",
        "Fish use their lateral lines to sense flows and pressure gradients, enabling\nthem to detect nearby objects and organisms. Towards replicating this\ncapability, we demonstrated successful leader-follower formation swimming using\nflow pressure sensing in our undulatory robotic fish ($\\mu$Bot\/MUBot). The\nfollower $\\mu$Bot is equipped at its head with bilateral pressure sensors to\ndetect signals excited by both its own and the leader's movements. First, using\nexperiments with static formations between an undulating leader and a\nstationary follower, we determined the formation that resulted in strong\npressure variations measured by the follower. This formation was then selected\nas the desired formation in free swimming for obtaining an expert policy. Next,\na long short-term memory neural network was used as the control policy that\nmaps the pressure signals along with the robot motor commands and the Euler\nangles (measured by the onboard IMU) to the steering command. The policy was\ntrained to imitate the expert policy using behavior cloning and Dataset\nAggregation (DAgger). The results show that with merely two bilateral pressure\nsensors and less than one hour of training data, the follower effectively\ntracked the leader within distances of up to 200 mm (= 1 body length) while\nswimming at speeds of 155 mm\/s (= 0.8 body lengths\/s). This work highlights the\npotential of fish-inspired robots to effectively navigate fluid environments\nand achieve formation swimming through the use of flow pressure feedback.",
        "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
        "Nonlinear pulse amplification in optical fiber, with capability of breaking\nthe gain-bandwidth limitation, is a key technique for high-energy, ultrafast\npulse generation. In the longer wavelength region (including 1.55 {\\mu}m, 2\n{\\mu}m and 2.8 {\\mu}m) where the gain fiber has normally strong anomalous\ndispersion, the nonlinear amplification process over fiber exhibits more\ncomplicated dynamics than that of its 1-{\\mu}m counterpart, and the underlying\nmechanism of the nonlinear pulse propagation process in high-gain anomalous\nfiber is still elusive so far. Here, we demonstrate an in-depth study on the\nnonlinear amplification process in high-gain ultrafast mid-infrared fiber,\nproviding clear physical understanding on the debate of adiabatic soliton\ncompression. We unveil that under the high-gain condition, the ultrafast pulse\nlaunched into the anomalous gain fiber experiences successively three distinct\nstages, named as the balance between linear and nonlinear chirp,\nhigh-order-soliton-like pulse compression and pulse splitting due to high-order\neffects. While a relatively-clean ultrafast pulse can be obtained immediately\nafter the high-order-soliton-like compression stage, excessive gain fiber\nlength could hardly enhance further the pulse peak power due to soliton\nsplitting. Our findings can provide several critical guidelines for designing\nhigh-power ultrafast fiber amplifiers at near- and mid-infrared wavelengths.",
        "The exponential growth in the size and complexity of Large Language Models\n(LLMs) has introduced unprecedented challenges in their deployment and\noperational management. Traditional MLOps approaches often fail to efficiently\nhandle the scale, resource requirements, and dynamic nature of these models.\nThis research presents a novel framework that leverages Deep Neural Networks\n(DNNs) to optimize MLOps pipelines specifically for LLMs. Our approach\nintroduces an intelligent system that automates deployment decisions, resource\nallocation, and pipeline optimization while maintaining optimal performance and\ncost efficiency. Through extensive experimentation across multiple cloud\nenvironments and deployment scenarios, we demonstrate significant improvements:\n40% enhancement in resource utilization, 35% reduction in deployment latency,\nand 30% decrease in operational costs compared to traditional MLOps approaches.\nThe framework's ability to adapt to varying workloads and automatically\noptimize deployment strategies represents a significant advancement in\nautomated MLOps management for large-scale language models. Our framework\nintroduces several novel components including a multi-stream neural\narchitecture for processing heterogeneous operational metrics, an adaptive\nresource allocation system that continuously learns from deployment patterns,\nand a sophisticated deployment orchestration mechanism that automatically\nselects optimal strategies based on model characteristics and environmental\nconditions. The system demonstrates robust performance across various\ndeployment scenarios, including multi-cloud environments, high-throughput\nproduction systems, and cost-sensitive deployments. Through rigorous evaluation\nusing production workloads from multiple organizations, we validate our\napproach's effectiveness in reducing operational complexity while improving\nsystem reliability and cost efficiency.",
        "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582\/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61\/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks.",
        "Singular learning models with non-positive Fisher information matrices\ninclude neural networks, reduced-rank regression, Boltzmann machines, normal\nmixture models, and others. These models have been widely used in the\ndevelopment of learning machines. However, theoretical analysis is still in its\nearly stages. In this paper, we examine learning coefficients, which indicate\nthe general learning efficiency of deep linear learning models and three-layer\nneural network models with ReLU units. Finally, we extend the results to\ninclude the case of the Softmax function.",
        "This work presents a novel algorithm for progressively adapting neural\nnetwork architecture along the depth. In particular, we attempt to address the\nfollowing questions in a mathematically principled way: i) Where to add a new\ncapacity (layer) during the training process? ii) How to initialize the new\ncapacity? At the heart of our approach are two key ingredients: i) the\nintroduction of a ``shape functional\" to be minimized, which depends on neural\nnetwork topology, and ii) the introduction of a topological derivative of the\nshape functional with respect to the neural network topology. Using an optimal\ncontrol viewpoint, we show that the network topological derivative exists under\ncertain conditions, and its closed-form expression is derived. In particular,\nwe explore, for the first time, the connection between the topological\nderivative from a topology optimization framework with the Hamiltonian from\noptimal control theory. Further, we show that the optimality condition for the\nshape functional leads to an eigenvalue problem for deep neural architecture\nadaptation. Our approach thus determines the most sensitive location along the\ndepth where a new layer needs to be inserted during the training phase and the\nassociated parametric initialization for the newly added layer. We also\ndemonstrate that our layer insertion strategy can be derived from an optimal\ntransport viewpoint as a solution to maximizing a topological derivative in\n$p$-Wasserstein space, where $p>= 1$. Numerical investigations with fully\nconnected network, convolutional neural network, and vision transformer on\nvarious regression and classification problems demonstrate that our proposed\napproach can outperform an ad-hoc baseline network and other architecture\nadaptation strategies. Further, we also demonstrate other applications of\ntopological derivative in fields such as transfer learning.",
        "Robust stability of moving-horizon estimators is investigated for nonlinear\ndiscrete-time systems that are detectable in the sense of incremental\ninput\/output-to-state stability and are affected by disturbances. The estimate\nof a moving-horizon estimator stems from the on-line solution of a\nleast-squares minimization problem at each time instant. The resulting\nstability guarantees depend on the optimization tolerance in solving such\nminimization problems. Specifically, two main contributions are established:\n(i) the robust stability of the estimation error, while supposing to solve\nexactly the on-line minimization problem; (ii) the practical robust stability\nof the estimation error with state estimates obtained by an imperfect\nminimization. Finally, the construction of such robust moving-horizon\nestimators and the performances resulting from the design based on the\ntheoretical findings are showcased with two numerical examples.",
        "Brown dwarfs are failed stars with very low mass (13 to 75 $M_J$), and an\neffective temperature lower than 2500 K. Thus, they play a key role in\nunderstanding the gap in the mass function between stars and planets. However,\ndue to their faint nature, previous searches are inevitably limited to the\nsolar neighbourhood (20 pc). To improve our knowledge of the low mass part of\nthe initial stellar mass function and the star formation history of the Milky\nWay, it is crucial to find more distant brown dwarfs. Using James Webb Space\nTelescope (JWST) COSMOS-Web data, this study seeks to enhance our comprehension\nof the physical characteristics of brown dwarfs situated at a distance of kpc\nscale. The exceptional sensitivity of the JWST enables the detection of brown\ndwarfs that are up to 100 times more distant than those discovered in the\nearlier all-sky infrared surveys. The large area coverage of the JWST\nCOSMOS-Web survey allows us to find more distant brown dwarfs than earlier JWST\nstudies with smaller area coverages. To capture prominent water absorption\nfeatures around 2.7 $\\mu$m, we apply two colour criteria,\nF115W-F277W+1<F277W-F444W and F277W-F444W>0.9. We then select point sources by\nCLASS_STAR, FLUX_RADIUS, and SPREAD_MODEL criteria. Faint sources are visually\nchecked to exclude possibly extended sources. We conduct SED fitting and MCMC\nsimulations to determine their physical properties and associated\nuncertainties. Our search reveals 25 T-dwarf and 2 Y-dwarf candidates, more\nthan any previous JWST brown dwarf searches. They are located from 0.3 kpc to 4\nkpc away from the Earth. The cumulative number count of our brown dwarf\ncandidates is consistent with the prediction from a standard double exponential\nmodel. Three of our brown dwarf candidates were detected by HST, with\ntransverse velocities $12\\pm5$ km s$^{-1}$, $12\\pm4$ km s$^{-1}$, and $17\\pm6$\nkm s$^{-1}$.",
        "The FOOT (FragmentatiOn Of Target) experiment was proposed to measure double\ndifferential nuclear fragmentation cross sections in angle and kinetic energy\nof the produced fragments in beam-target settings, interesting for\nhadrontherapy and space radioprotection applications. In particular, FOOT\nmeasures projectile and target fragmentations in the kinetic energy range\nbetween $200 \\text{MeV\/u}$ and $800 \\text{MeV\/u}$. In this contribution,\ndifferential cross section measurements of a $400 \\text{MeV\/u}$ $^{16}$O beam\non a Carbon and a polyethylene target with data acquired at GSI (Darmstadt,\nGermany) beam accelerator facility are presented, along with the extraction of\nthe first total fragmentation cross section for a Hydrogen target within the\nFOOT experiment.",
        "Visual degradation in underwater environments poses unique and significant\nchallenges, which distinguishes underwater SLAM from popular vision-based SLAM\non the ground. In this paper, we propose RUSSO, a robust underwater SLAM system\nwhich fuses stereo camera, inertial measurement unit (IMU), and imaging sonar\nto achieve robust and accurate localization in challenging underwater\nenvironments for 6 degrees of freedom (DoF) estimation. During visual\ndegradation, the system is reduced to a sonar-inertial system estimating 3-DoF\nposes. The sonar pose estimation serves as a strong prior for IMU propagation,\nthereby enhancing the reliability of pose estimation with IMU propagation.\nAdditionally, we propose a SLAM initialization method that leverages the\nimaging sonar to counteract the lack of visual features during the\ninitialization stage of SLAM. We extensively validate RUSSO through experiments\nin simulator, pool, and sea scenarios. The results demonstrate that RUSSO\nachieves better robustness and localization accuracy compared to the\nstate-of-the-art visual-inertial SLAM systems, especially in visually\nchallenging scenarios. To the best of our knowledge, this is the first time\nfusing stereo camera, IMU, and imaging sonar to realize robust underwater SLAM\nagainst visual degradation."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks",
    "start_abstract":"Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
      ],
      "abstract":[
        "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Anomalous bulk-edge correspondence of nonlinear Rice-Mele model",
        "Optical stabilization for laser communication satellite systems through\n  proportional-integral-derivative (PID) control and reinforcement learning\n  approach",
        "Mono-lepton Signature of a Neutrino-philic Dark Fermion at Hadron\n  Colliders",
        "Effect of thickness on the maximum potential drop of current collectors",
        "PPO-Q: Proximal Policy Optimization with Parametrized Quantum Policies\n  or Values",
        "Block structures of graphs and quantum isomorphism",
        "Comparison principles and asymptotic behavior of delayed age-structured\n  neuron models",
        "Regular Schwarzschild black holes and cosmological models",
        "A hidden Condorcet domain in Loday's realisation of the associahedron",
        "On the Wave Kinetic Equation in the presence of forcing and dissipation",
        "Influence of the growth temperature and annealing on the optical\n  properties of {CdO\/ZnO}30 superlattices",
        "Proceedings of the Erice Workshop: A new baseline for the hybrid,\n  asymmetric, linear Higgs factory HALHF",
        "Study of Null Geodesics and their Stability in Horndeski Black Holes",
        "Measuring decoherence due to quantum vacuum fluctuations",
        "Rank conditions for exactness of semidefinite relaxations in polynomial\n  optimization",
        "On the robustness of exoplanet atmospheric detections: insights from\n  extensive simulations",
        "Gravitational physics in the context of Indian astronomy: A vision\n  document",
        "LZMidi: Compression-Based Symbolic Music Generation",
        "A gap between two approaches of dimensional reduction for a\n  six-dimensional Kaluza-Klein theory",
        "Merging-Based Quantum Repeater",
        "AI-Driven Hybrid Ecological Model for Predicting Oncolytic Viral Therapy\n  Dynamics",
        "Intrinsic width of the flux tube in 2+1 dimensional Yang-Mills therories",
        "Convergence analysis of Wirtinger Flow for Poisson phase retrieval",
        "Concentration via Metastable Mixing, with Applications to the\n  Supercritical Exponential Random Graph Model",
        "Affective Polarization Amongst Swedish Politicians",
        "Transformations of predictions and realizations in consistent scoring\n  functions",
        "Stein Discrepancy for Unsupervised Domain Adaptation",
        "A Data-Driven Real-Time Optimal Power Flow Algorithm Using Local\n  Feedback",
        "Enhancing DUNE Physics Sensitivity with Light and Charge Calorimetry"
      ],
      "abstract":[
        "Bulk-edge correspondence (BEC) constitutes a fundamental concept within the\ndomain of topological physics, elucidating the profound interplay between the\ntopological invariants that characterize the bulk states and the emergent edge\nstates. A recent highlight along this research line consists of establishing\nBEC under the eigenvalue's nonlinearity in a linear Hamiltonian by introducing\nauxiliary eigenvalues [\\href{https:\/\/doi.org\/10.1103\/PhysRevLett.132.126601}{\nT. Isobe {\\it et al.,} Phys. Rev. Lett. 132, 126601 (2024)}]. The purpose of\nthis work aims to extend Isobe's analysis to uncover BEC of eigenvalue's\nnonlinearity in intrinsic nonlinear Hamiltonians. To achieve this, we\nnumerically solve the nonlinear Rice-Mele (RM) model and identify two distinct\ntypes of nonlinear eigenvalues: the intrinsically nonlinear eigenvalues and the\neigenvalue's nonlinearity introduced through the incorporation of auxiliary\neigenvalues. Furthermore, we establish a novel form of BEC based on these\nauxiliary nonlinear eigenvalues, which we term the anomalous BEC of a nonlinear\nphysical system. The concept of the anomalous BEC defined herein provides a\nnovel perspective on the intricate interplay between topology and nonlinearity\nin the context of BEC.",
        "One of the main issues of the satellite-to-ground optical communication,\nincluding free-space satellite quantum key distribution (QKD), is an\nachievement of the reasonable accuracy of positioning, navigation and optical\nstabilization. Proportional-integral-derivative (PID) controllers can handle\nwith various control tasks in optical systems. Recent research shows the\npromising results in the area of composite control systems including classical\ncontrol via PID controllers and reinforcement learning (RL) approach. In this\nwork we apply RL agent to an experimental stand of the optical stabilization\nsystem of QKD terminal. We find via agent control history more precise PID\nparameters and also provide effective combined RL-PID dynamic control approach\nfor the optical stabilization of satellite-to-ground communication system.",
        "Searching for dark matter at high-energy colliders and direct detection\nexperiments can effectively cover nearly the entire mass range from the MeV to\nthe TeV scale. In this paper, we focus on four-fermion contact interactions\nformulated within the framework of Effective Field Theory. Specifically, we\npresent a detailed analysis of mono-lepton production at the LHC. Our results\ndemonstrate that tensor operators exhibit superior sensitivity in the\nmono-lepton channel, constraining energy scales up to 3\\,TeV for a nearly\nmassless dark fermion using current LHC data. Moreover, these operators mediate\nboth spin-independent and spin-dependent absorption processes in nuclear\ntargets. A systematic comparison of constraints between direct detection\nexperiments and collider measurements reveals the LHC's distinct advantage in\nexploring sub-GeV dark matter candidates while maintaining competitive\nsensitivity at the TeV scale. Notably, direct detection experiments such as\nSuper-Kamiokande and Borexino achieve complementary constraints in the\n10-100\\,TeV mass range through their unique capabilities: utilization of light\nnuclei targets, large exposure volumes, and distinctive features of the recoil\nenergy spectra.",
        "The basic principle for achieving high-power capability on an electrochemical\nenergy storage cell is minimizing the overall resistance. The resistance due to\ncurrent collecting systems has not received sufficient attention in the past,\npresumably because it was not considered of significance for low-power\nbatteries and supercapacitors. However, the necessity of high-power cells has\nreduced other sources of the inner resistance, and the current collector\npotential drop has become more important. Moreover, the miniaturization of\nenergy storage devices could increase the ohmic loses in current collectors. In\nthis work, we have developed an electrical model to assess the effect of the\ncurrent collector thickness on the maximum potential drop. We have found that\nthe thickness of current collectors is a critical parameter that can increase\nthe maximum potential drop drastically. Indeed, the maximum potential drop of\ncurrent collectors remains almost constant for thicknesses greater than 500 lm,\nbut below this value, there is an inverse relationship between the maximum\npotential drop and the thickness. We have also analyzed the effect of the\nmaterial and tab position in the maximum potential drop.",
        "Quantum machine learning (QML), which combines quantum computing with machine\nlearning, is widely believed to hold the potential to outperform traditional\nmachine learning in the era of noisy intermediate-scale quantum (NISQ). As one\nof the most important types of QML, quantum reinforcement learning (QRL) with\nparameterized quantum circuits as agents has received extensive attention in\nthe past few years. Various algorithms and techniques have been introduced,\ndemonstrating the effectiveness of QRL in solving some popular benchmark\nenvironments such as CartPole, FrozenLake, and MountainCar. However, tackling\nmore complex environments with continuous action spaces and high-dimensional\nstate spaces remains challenging within the existing QRL framework. Here we\npresent PPO-Q, which, by integrating hybrid quantum-classical networks into the\nactor or critic part of the proximal policy optimization (PPO) algorithm,\nachieves state-of-the-art performance in a range of complex environments with\nsignificantly reduced training parameters. The hybrid quantum-classical\nnetworks in the PPO-Q incorporate two additional traditional neural networks to\naid the parameterized quantum circuits in managing high-dimensional state\nencoding and action selection. When evaluated on 8 diverse environments,\nincluding four with continuous action space, the PPO-Q achieved comparable\nperformance with the PPO algorithm but with significantly reduced training\nparameters. Especially, we accomplished the BipedalWalker environment, with a\nhigh-dimensional state and continuous action space simultaneously, which has\nnot previously been reported in the QRL. More importantly, the PPO-Q is very\nfriendly to the current NISQ hardware. We successfully trained two\nrepresentative environments on the real superconducting quantum devices via the\nQuafu quantum cloud service.",
        "We prove that for every pair of quantum isomorphic graphs, their block trees\nand their block graphs are isomorphic and that such an isomorphism can be\nchosen so that the corresponding blocks are quantum isomorphic. As a corollary\nof this result, we obtain that a minimal pair of quantum isomorphic graphs\nwhich are not isomorphic consists of 2-connected graphs.",
        "In the context of neuroscience the elapsed-time model is an age-structured\nequation that describes the behavior of interconnected spiking neurons through\nthe time since the last discharge, with many interesting dynamics depending on\nthe type of interactions between neurons. We investigate the asymptotic\nbehavior of this equation in the case of both discrete and distributed delays\nthat account for the time needed to transmit a nerve impulse from one neuron to\nthe rest the ensemble. To prove the convergence to the equilibrium, we follow\nan approach based on comparison principles for Volterra equations involving the\ntotal activity, which provides a simpler and more straightforward alternative\ntechnique than those in the existing literature on the elapsed-time model.",
        "We study regular Schwarzschild black holes in General Relativity as an\nalternative to the singular counterpart. We analyze two types of solutions\nwhich are completely parameterised by the ADM mass alone. We find that both\nfamilies of regular solutions contain a de Sitter condensate at the core and\nadmit (quasi) extremal black hole configurations in which the two horizons are\narbitrarily close. Cosmological models based on these regular configurations\nare also analyzed, finding that they describe non-trivial Kantowski-Sachs\nuniverses free of singularities.",
        "We prove that Loday's polytopal realisation of the nth Tamari lattice T_n,\ncalled associahedron, has 2^{n-1} common points with the permutohedron, which\nform a maximal never-middle (symmetric) Condorcet domain.",
        "The wave kinetic equation has become an important tool in different fields of\nphysics. In particular, for surface gravity waves, it is the backbone of wave\nforecasting models. Its derivation is based on the Hamiltonian dynamics of\nsurface gravity waves. Only at the end of the derivation are the\nnon-conservative effects, such as forcing and dissipation, included as\nadditional terms to the collision integral. In this paper, we present a first\nattempt to derive the wave kinetic equation when the dissipation\/forcing is\nincluded in the deterministic dynamics. If, in the dynamical equations, the\ndissipation\/forcing is one order of magnitude smaller than the nonlinear\neffect, then the classical wave action balance equation is obtained and the\nkinetic time scale corresponds to the dissipation\/forcing time scale. However,\nif we assume that the nonlinearity and the dissipation\/forcing act on the same\ndynamical time scale, we find that the dissipation\/forcing dominates the\ndynamics and the resulting collision integral appears in a modified form, at a\nhigher order.",
        "Optical properties of the short period {CdO\/ZnO} superlattices grown by\nplasma assisted MBE were analyzed. The superlattice (SLs) structures were\nsuccessfully obtained at different growth temperatures from 360 to 550 {\\deg}C.\nInterestingly, the growth temperature of the SLs influences quality of\nmultilayers and also optical properties of these structures. After annealing at\n900{\\deg}C by rapid thermal method various defect luminescence located at\ndifferent energetic positions , were detected, and intensity of luminescence\nstrongly depends on applied growth temperature.",
        "The HALHF collaboration has discussed a new baseline for the project, taking\ninto account comments from the accelerator community on various aspects of the\noriginal design. In particular, these concerned the practicality of the\ndual-purpose linac to accelerate both colliding positron bunches and the drive\nbeams required for the plasma linac. In addition, many other aspects of the\nproject were also considered; the discussion and conclusions are documented in\nthis paper. Finally, a new baseline is outlined that has been optimised and\naddresses several weaknesses in the original design, has higher luminosity,\nreduced centre-of-mass energy boost and additional features such as positron\npolarization as well as electron polarization. Although HALHF has become longer\nand more expensive, it remains significantly smaller and cheaper than other\nmature Higgs factory designs currently under discussion.",
        "We study the motion of particles in the background of a scalar-tensor theory\nof gravity in which the scalar field is kinetically coupled to the Einstein\ntensor and we present the null geodesic structure for asymptotically flat, AdS,\nand dS Horndeski black holes, studying the effect of the cosmological constant\non the orbits. Also, we consider three classical test of the gravity in the\nsolar system, such as, the bending of the light, the gravitational redshift,\nand the Shapiro time delay in order to constraint the coupling parameters of\nthe scalar field to gravity. Calculating the Lyapunov exponent we explore the\nstability of these geodesics for various values of the cosmological constant.",
        "The interaction of a particle with vacuum fluctuations -- which theoretically\nexist even in the complete absence of matter -- can lead to observable\nirreversible decoherence, if it were possible to switch on and off the particle\ncharge suddenly. We compute the leading order decoherence effect for such a\nscenario and propose an experimental setup for its detection. Such a\nmeasurement might provide further insights into the nature of vacuum\nfluctuations and a novel precision test for the decoherence theory.",
        "We consider the Moment-SOS hierarchy in polynomial optimization. We first\nprovide a sufficient condition to solve the truncated K-moment problem\nassociated with a given degree-$2n$ pseudo-moment sequence $\\phi$ n and a\nsemi-algebraic set $K \\subset \\mathbb{R}^d$. Namely, let $2v$ be the maximum\ndegree of the polynomials that describe $K$. If the rank $r$ of its associated\nmoment matrix is less than $nv + 1$, then $\\phi^n$ has an atomic representing\nmeasure supported on at most $r$ points of $K$. When used at step-$n$ of the\nMoment-SOS hierarchy, it provides a sufficient condition to guarantee its\nfinite convergence (i.e., the optimal value of the corresponding degree-n\nsemidefinite relaxation of the hierarchy is the global minimum). For Quadratic\nConstrained Quadratic Problems (QCQPs) one may also recover global minimizers\nfrom the optimal pseudo-moment sequence. Our condition is in the spirit of\nBlekherman's rank condition and while on the one-hand it is more restrictive,\non the other hand it applies to constrained POPs as it provides a localization\non $K$ for the representing measure.",
        "The classical picture of our Solar System being the archetypal outcome of\nplanet formation has been rendered obsolete by the astonishing diversity of\nextrasolar-system architectures. From rare hot-Jupiters to abundant\nsuper-Earths and sub-Neptunes, most detected exoplanets have no analogs in our\nsystem, and their interior and atmospheric compositions remain largely unknown.\nFortunately, new methodologies enable us to analyze exoplanet atmospheres,\ninferring their compositions, temperatures, dynamics, and even formation\npathways. Specifically, ground-based high-resolution Doppler spectroscopy\n(HRDS) can disentangle spectral-line profiles of weak exo-atmospheric signals\nfrom the dominating features of Earth's atmosphere in the observed flux. For\nover a decade, HRDS has focused on hot Jupiters (close-orbiting gas giants) due\nto their high signal-to-noise ratio, which makes them ideal laboratories for\nadvancing our knowledge. However, there have been concerns regarding potential\nbiases in exo-atmospheric-detection methods, hindering comparative planetology.\nHere we propose a modeling framework based on extensive simulations of HRDS\nexo-atmospheric observations to systematically explore in-silico underlying\nbiases in commonly-used pipelines, particularly under the presence of\nobservational noise. Our findings show that exo-atmospheric\ndetection-significances are highly contingent on details of the\nanalysis-pipeline used, with different techniques responding differently to\nnoise: A given technique may fail to recover a true-signal that is detected by\nanother. Noise effects in the computed significances are non-trivial and\npipeline-dependent. Statistical analyses provide a complementary tool to\ncontextualize signal-significances, which will gain in relevance as we move\ntowards studying the atmospheres of smaller, potentially habitable exoplanets\nwith even weaker signals.",
        "Contributions from the Indian gravity community have played a significant\nrole in shaping several branches of astronomy and astrophysics. This document\nreviews some of the most important contributions and presents a vision for\ngravity research in the context of astronomy \\& astrophysics in India. This is\nan expanded version of one of the chapters in the recently released Vision\nDocument of the Astronomical Society of India.",
        "Recent advances in symbolic music generation primarily rely on deep learning\nmodels such as Transformers, GANs, and diffusion models. While these approaches\nachieve high-quality results, they require substantial computational resources,\nlimiting their scalability. We introduce LZMidi, a lightweight symbolic music\ngeneration framework based on a Lempel-Ziv (LZ78)-induced sequential\nprobability assignment (SPA). By leveraging the discrete and sequential\nstructure of MIDI data, our approach enables efficient music generation on\nstandard CPUs with minimal training and inference costs. Theoretically, we\nestablish universal convergence guarantees for our approach, underscoring its\nreliability and robustness. Compared to state-of-the-art diffusion models,\nLZMidi achieves competitive Frechet Audio Distance (FAD), Wasserstein Distance\n(WD), and Kullback-Leibler (KL) scores, while significantly reducing\ncomputational overhead - up to 30x faster training and 300x faster generation.\nOur results position LZMidi as a significant advancement in compression-based\nlearning, highlighting how universal compression techniques can efficiently\nmodel and generate structured sequential data, such as symbolic music, with\npractical scalability and theoretical rigor.",
        "Inspired by the five-dimensional Kaluza-Klein theory, we would like to study\nthe dimensional reduction issue of six-dimensional Kaluza-Klein extension in\nthis paper. In particular, we will examine two possible approaches of\ndimensional reduction from six-dimensional spacetimes to four-dimensional ones.\nThe first one is a direct dimensional reduction, i.e., from six-dimensional\nspacetimes directly to four-dimensional ones, via a $T^2\\equiv S^1 \\times S^1$\ncompactification, while the second one is an indirect dimensional reduction,\ni.e., from six-dimensional spacetimes to five-dimensional ones then\nfour-dimensional ones, via two separated $S^1$ compactifications.\nInterestingly, we show that these two approaches lead to different\nfour-dimensional effective actions although using the same six-dimensional\nmetric. It could therefore address an important question of which approach is\nmore reliable than the other.",
        "We introduce an alternative approach for the design of quantum repeaters\nbased on generating entangled states of growing size. The scheme utilizes\nquantum merging operations, also known as fusion type-I operations, that allow\nthe reintegration and reuse of entanglement. Unlike conventional swapping-based\nprotocols, our method preserves entanglement after failed operations, thereby\nreducing waiting times, enabling higher rates, and introducing enhanced\nflexibility in the communication requests. Through proof-of-principle analysis,\nwe demonstrate the advantages of this approach over standard repeater\nprotocols, highlighting its potential for practical quantum communication\nscenarios.",
        "Oncolytic viral therapy (OVT) is an emerging precision therapy for aggressive\nand recurrent cancers. However, its clinical efficacy is hindered by the\ncomplexity of tumor-virus-immune interactions and the lack of predictive models\nfor personalized treatment. This study develops a data-driven, AI-powered\ncomputational model combining time-delayed Generalized Lotka-Volterra equations\nwith advanced optimization algorithms, including Genetic Algorithms,\nDifferential Evolution, and Reinforcement Learning, to optimize OVT\noscillations' growth and damping. We hypothesize that the model can provide\naccurate, real-time predictions of OVT responses while identifying key\nbiomarkers to enhance therapeutic efficacy. The model demonstrates strong\npredictive accuracy, achieving mean squared error (MSE) < 0.02 and R-squared >\n0.82. It also identifies experimentally validated biomarkers such as TNF, NFkB,\nCD81, TRAF2, IL18, and BID, among other inflammatory cytokines and\nextracellular matrix reconstruction factors, despite being causally agnostic\nand unaware of specific experimental conditions or therapeutic combinations.\nGene set enrichment analysis confirmed these biosignatures as critical\npredictors of tumor progression and indicated that photodynamic therapy\nactivates immune responses similar to those elicited by combined OVT and immune\ncheckpoint inhibitors. This hybrid model represents a significant step toward\nprecision oncology and computational medicine, enabling longitudinal, adaptive\ntreatment regimens and developing targeted immunotherapies based on molecular\nsignatures, potentially improving patient outcomes.",
        "We study the shape of the flux tube in lattice Yang-Mills theories and in\nparticular its intrinsic width. In the framework of the Effective String Theory\ndescription of the confining flux tube this intrinsic width has no measurable\neffects on the inter-quark static potential, but it can be precisely detected\nlooking at the profile of the flux tube. We address this problem with a set of\nhigh precision simulations in the (2+1) dimensional SU(2) model. We find two\ndifferent behaviours as a function of the temperature. In the low temperature\nregime ($T \\ll T_c$) we find a good agreement with an expression inspired by\nthe dual superconductive model of confinement. In the high temperature regime\n($T \\lesssim T_c$) our data agree with a model based on the Svetitsky-Yaffe\nmapping. All our data in this regime can be described in terms of only one\nlength scale, the intrinsic width, which turns out to be the same scale\nappearing in the confining inter-quark static potential.",
        "This paper presents a rigorous theoretical convergence analysis of the\nWirtinger Flow (WF) algorithm for Poisson phase retrieval, a fundamental\nproblem in imaging applications. Unlike prior analyses that rely on truncation\nor additional adjustments to handle outliers, our framework avoids eliminating\nmeasurements or introducing extra computational steps, thereby reducing overall\ncomplexity. We prove that WF achieves linear convergence to the true signal\nunder noiseless conditions and remains robust and stable in the presence of\nbounded noise for Poisson phase retrieval. Additionally, we propose an\nincremental variant of WF, which significantly improves computational\nefficiency and guarantees convergence to the true signal with high probability\nunder suitable conditions.",
        "It is a folklore belief that metastable wells in low-temperature statistical\nmechanics models exhibit high-temperature behavior. We prove a rigorous version\nof this phenomenon in the setting of the exponential random graph model (ERGM)\nthrough the lens of concentration of measure. To do this, we first present a\nnew general result deriving concentration inequalities in a metastable well\nfrom the metastable mixing of a Markov chain with the appropriate stationary\ndistribution, extending a result of Chatterjee [Cha05] which is suited for more\ntraditional forms of global mixing. We then apply this result to the\nsupercritical (low-temperature) ERGM which was recently proven to exhibit\nmetastable mixing by Bresler, Nagaraj, and Nichani [BNN24], and obtain a novel\nconcentration inequality for Lipschitz observables of the supercritical ERGM\nconditioned on a large metastable well, answering a question posed by [BNN24].\nThis extends a result of Ganguly and Nam [GN24] from the subcritical\n(high-temperature) regime to a metastable well in the supercritical regime, and\nwe are also able to extend the applications of their concentration inequality\nto these metastable wells. Namely, we obtain an upper bound on the Wasserstein\ndistance between the ERGM conditioned on a metastable well and an appropriate\nErd\\H{o}s-R\\'enyi model, as well as derive a central limit theorem for the\ncount of edges in certain small subcollections of possible edges. Finally, to\nsupplement the mathematical content of the article, we also discuss the results\nof what appears to be the first simulation study of a metastable well in the\nsupercritical ERGM.",
        "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research.",
        "Scoring functions constructed by transforming the realization and prediction\nvariables of (strictly) consistent scoring functions have been widely studied\nempirically, yet their theoretical foundations remain unexplored. To address\nthis gap, we establish formal characterizations of (strict) consistency for\nthese transformed scoring functions and their elicitable functionals. Our\nanalysis focuses on two interrelated cases: (a) transformations applied\nexclusively to the realization variable, and (b) bijective transformations\napplied jointly to both realization and prediction variables. We formulate\nanalogous characterizations for (strict) identification functions. The\nresulting theoretical framework is broadly applicable to statistical and\nmachine learning methodologies. When applied to Bregman and expectile scoring\nfunctions, our framework shows how it enables two critical advances: (a)\nrigorous interpretation of prior empirical findings from models trained with\ntransformed scoring functions, and (b) systematic construction of novel\nidentifiable and elicitable functionals, specifically the g-transformed\nexpectation and g-transformed expectile. By unifying theoretical insights with\npractical applications, this work advances principled methodologies for\ndesigning scoring functions in complex predictive tasks.",
        "Unsupervised domain adaptation (UDA) leverages information from a labeled\nsource dataset to improve accuracy on a related but unlabeled target dataset. A\ncommon approach to UDA is aligning representations from the source and target\ndomains by minimizing the distance between their data distributions. Previous\nmethods have employed distances such as Wasserstein distance and maximum mean\ndiscrepancy. However, these approaches are less effective when the target data\nis significantly scarcer than the source data. Stein discrepancy is an\nasymmetric distance between distributions that relies on one distribution only\nthrough its score function. In this paper, we propose a novel UDA method that\nuses Stein discrepancy to measure the distance between source and target\ndomains. We develop a learning framework using both non-kernelized and\nkernelized Stein discrepancy. Theoretically, we derive an upper bound for the\ngeneralization error. Numerical experiments show that our method outperforms\nexisting methods using other domain discrepancy measures when only small\namounts of target data are available.",
        "The increasing penetration of distributed energy resources (DERs) adds\nvariability as well as fast control capabilities to power networks. Dispatching\nthe DERs based on local information to provide real-time optimal network\noperation is the desideratum. In this paper, we propose a data-driven real-time\nalgorithm that uses only the local measurements to solve time-varying AC\noptimal power flow (OPF). Specifically, we design a learnable function that\ntakes the local feedback as input in the algorithm. The learnable function,\nunder certain conditions, will result in a unique stationary point of the\nalgorithm, which in turn transfers the OPF problems to be optimized over the\nparameters of the function. We then develop a stochastic primal-dual update to\nsolve the variant of the OPF problems based on a deep neural network (DNN)\nparametrization of the learnable function, which is referred to as the training\nstage. We also design a gradient-free alternative to bypass the cumbersome\ngradient calculation of the nonlinear power flow model. The OPF\nsolution-tracking error bound is established in the sense of universal\napproximation of DNN. Numerical results on the IEEE 37-bus test feeder show\nthat the proposed method can track the time-varying OPF solutions with higher\naccuracy and faster computation compared to benchmark methods.",
        "We investigate the potential of light calorimetry in liquid argon time\nprojection chambers and its intrinsic self compensation properties, emphasizing\nits advantages alongside conventional charge calorimetry. Previous studies have\ndemonstrated that light calorimetry can achieve energy resolution comparable to\nadvanced charge based techniques, particularly for GeV scale neutrinos. In this\nwork, we explore the complementarity of light calorimetry with charge\ncalorimetry for precision measurements of key physics parameters in the DUNE,\nincluding CP violation (CPV) and mass hierarchy determination. While charge\ncalorimetry provides superior resolution in CP phase measurements, light\ncalorimetry independently offers significant insights into CPV and mass\nhierarchy sensitivities. Furthermore, our exposure versus CPV sensitivity\nstudies indicate that the $5\\sigma$ discovery potential is reached faster using\nlight and charge calorimetry than with the traditional TDR based reconstruction\nmethods. These findings highlight the promising role of light calorimetry as a\nsimple yet effective reconstruction method, serving as a complementary approach\nto enhance the physics capabilities of DUNE."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema",
    "start_abstract":"This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity\n  Grouping",
        "Coded Deep Learning: Framework and Algorithm",
        "Universal programmable and self-configuring optical filter",
        "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Development of Application-Specific Large Language Models to Facilitate\n  Research Ethics Review",
        "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
        "Contracting low degree points on curves",
        "Domain-conditioned and Temporal-guided Diffusion Modeling for\n  Accelerated Dynamic MRI Reconstruction",
        "A study of the Antlion Random Walk",
        "Evolving Skeletons: Motion Dynamics in Action Recognition",
        "B-Call: Integrating Ideological Position and Political Cohesion in\n  Legislative Voting Models",
        "Foliated Plateau problems, geometric rigidity and equidistribution of\n  closed $k$-surfaces",
        "Evidence for J\/$\\psi$ suppression in incoherent photonuclear production",
        "A Machine Learning Approach for Design of Frequency Selective Surface\n  based Radar Absorbing Material via Image Prediction",
        "Detection of chiral spin fluctuations driven by frustration in Mott\n  insulators",
        "Norms in equivariant homotopy theory",
        "MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language\n  Models for Biomedical In-Context Learning",
        "Robust Conformal Outlier Detection under Contaminated Reference Data",
        "Local damage detection in rolling element bearings based on a Single\n  Ensemble Empirical Mode Decomposition",
        "Single-crystalline CrSb(0001) thin films grown by dc magnetron\n  co-sputtering",
        "Parental Guidance: Efficient Lifelong Learning through Evolutionary\n  Distillation",
        "A Fully Self-Synchronized Control for Hybrid Series-Parallel\n  Electronized Power Networks",
        "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond",
        "Navigating Gender Disparities in Communication Research Leadership:\n  Academic Recognition, Career Development, and Compensation",
        "ASKAP and VLASS search for a radio-continuum counterpart of\n  ultra-high-energy neutrino event KM3-230213A",
        "Effective textures from a $[SU(3)]^3$ flavored scalar sector",
        "MAUCell: An Adaptive Multi-Attention Framework for Video Frame\n  Prediction",
        "$q$-numerical radius of rank-one operators and the generalized Buzano\n  inequality"
      ],
      "abstract":[
        "Class Incremental Learning (CIL) aims to enable models to learn new classes\nsequentially while retaining knowledge of previous ones. Although current\nmethods have alleviated catastrophic forgetting (CF), recent studies highlight\nthat the performance of CIL models is highly sensitive to the order of class\narrival, particularly when sequentially introduced classes exhibit high\ninter-class similarity. To address this critical yet understudied challenge of\nclass order sensitivity, we first extend existing CIL frameworks through\ntheoretical analysis, proving that grouping classes with lower pairwise\nsimilarity during incremental phases significantly improves model robustness to\norder variations. Building on this insight, we propose Graph-Driven Dynamic\nSimilarity Grouping (GDDSG), a novel method that employs graph coloring\nalgorithms to dynamically partition classes into similarity-constrained groups.\nEach group trains an isolated CIL sub-model and constructs meta-features for\nclass group identification. Experimental results demonstrate that our method\neffectively addresses the issue of class order sensitivity while achieving\noptimal performance in both model accuracy and anti-forgetting capability. Our\ncode is available at https:\/\/github.com\/AIGNLAI\/GDDSG.",
        "The success of deep learning (DL) is often achieved with large models and\nhigh complexity during both training and post-training inferences, hindering\ntraining in resource-limited settings. To alleviate these issues, this paper\nintroduces a new framework dubbed ``coded deep learning'' (CDL), which\nintegrates information-theoretic coding concepts into the inner workings of DL,\nto significantly compress model weights and activations, reduce computational\ncomplexity at both training and post-training inference stages, and enable\nefficient model\/data parallelism. Specifically, within CDL, (i) we first\npropose a novel probabilistic method for quantizing both model weights and\nactivations, and its soft differentiable variant which offers an analytic\nformula for gradient calculation during training; (ii) both the forward and\nbackward passes during training are executed over quantized weights and\nactivations, eliminating most floating-point operations and reducing training\ncomplexity; (iii) during training, both weights and activations are entropy\nconstrained so that they are compressible in an information-theoretic sense\nthroughout training, thus reducing communication costs in model\/data\nparallelism; and (iv) the trained model in CDL is by default in a quantized\nformat with compressible quantized weights, reducing post-training inference\nand storage complexity. Additionally, a variant of CDL, namely relaxed CDL\n(R-CDL), is presented to further improve the trade-off between validation\naccuracy and compression though requiring full precision in training with other\nadvantageous features of CDL intact. Extensive empirical results show that CDL\nand R-CDL outperform the state-of-the-art algorithms in DNN compression in the\nliterature.",
        "We propose an approach to integrated optical spectral filtering that allows\narbitrary programmability, can compensate automatically for imperfections in\nfilter fabrication, allows multiple simultaneous and separately programmable\nfilter functions on the same input, and can configure itself automatically to\nthe problem of interest, for example to filter or reject multiple arbitrarily\nchosen frequencies. The approach exploits splitting the input light into an\narray of multiple waveguides of different lengths that then feed a programmable\ninterferometer array that can also self-configure. It can give spectral\nresponse similar to arrayed waveguide gratings but offers many other filtering\nfunctions, as well as supporting other structures based on non-redundant arrays\nfor precise spectral filtering. Simultaneous filtering also allows, for the\nfirst time to our knowledge, an automatic measurement of the temporal coherency\nmatrix and physical separation into the Karhunen-Lo\\`eve expansion of\ntemporally partially coherent light fields.",
        "In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1\/\\sqrt{n}$.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.",
        "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https:\/\/huggingface.co\/datasets\/bytedance-research\/ToolHop.",
        "The main result of this article is that all but finitely many points of small\nenough degree on a curve can be written as a pullback of a smaller degree\npoint. The main theorem has several corollaries that yield improvements on\nresults of Kadets and Vogt, Khawaja and Siksek, and Vojta under a slightly\nstronger assumption on the degree of the points.",
        "Purpose: To propose a domain-conditioned and temporal-guided diffusion\nmodeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated\ndynamic MRI reconstruction, enabling diffusion process to characterize\nspatiotemporal information for time-resolved multi-coil Cartesian and\nnon-Cartesian data. Methods: The dDiMo framework integrates temporal\ninformation from time-resolved dimensions, allowing for the concurrent capture\nof intra-frame spatial features and inter-frame temporal dynamics in diffusion\nmodeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent\nfrequency-temporal ($k$-$t$) priors to guide the diffusion process. This\napproach ensures precise temporal alignment and enhances the recovery of fine\nimage details. To facilitate a smooth diffusion process, the nonlinear\nconjugate gradient algorithm is utilized during the reverse diffusion steps.\nThe proposed model was tested on two types of MRI data: Cartesian-acquired\nmulti-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil\nfree-breathing lung MRI, across various undersampling rates. Results: dDiMo\nachieved high-quality reconstructions at various acceleration factors,\ndemonstrating improved temporal alignment and structural recovery compared to\nother competitive reconstruction methods, both qualitatively and\nquantitatively. This proposed diffusion framework exhibited robust performance\nin handling both Cartesian and non-Cartesian acquisitions, effectively\nreconstructing dynamic datasets in cardiac and lung MRI under different imaging\nconditions. Conclusion: This study introduces a novel diffusion modeling method\nfor dynamic MRI reconstruction.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "Skeleton-based action recognition has gained significant attention for its\nability to efficiently represent spatiotemporal information in a lightweight\nformat. Most existing approaches use graph-based models to process skeleton\nsequences, where each pose is represented as a skeletal graph structured around\nhuman physical connectivity. Among these, the Spatiotemporal Graph\nConvolutional Network (ST-GCN) has become a widely used framework.\nAlternatively, hypergraph-based models, such as the Hyperformer, capture\nhigher-order correlations, offering a more expressive representation of complex\njoint interactions. A recent advancement, termed Taylor Videos, introduces\nmotion-enhanced skeleton sequences by embedding motion concepts, providing a\nfresh perspective on interpreting human actions in skeleton-based action\nrecognition. In this paper, we conduct a comprehensive evaluation of both\ntraditional skeleton sequences and Taylor-transformed skeletons using ST-GCN\nand Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal\ngraph and hypergraph representations, analyzing static poses against\nmotion-injected poses. Our findings highlight the strengths and limitations of\nTaylor-transformed skeletons, demonstrating their potential to enhance motion\ndynamics while exposing current challenges in fully using their benefits. This\nstudy underscores the need for innovative skeletal modelling techniques to\neffectively handle motion-rich data and advance the field of action\nrecognition.",
        "This paper combines two significant areas of political science research:\nmeasuring individual ideological position and cohesion. Although both\napproaches help analyze legislative behaviors, no unified model currently\nintegrates these dimensions. To fill this gap, the paper proposes a methodology\ncalled B-Call that combines ideological positioning with voting cohesion,\ntreating votes as random variables. The model is empirically validated using\nroll-call data from the United States, Brazil, and Chile legislatures, which\nrepresent diverse legislative dynamics. The analysis aims to capture the\ncomplexities of voting and legislative behaviors, resulting in a\ntwo-dimensional indicator. This study addresses gaps in current legislative\nvoting models, particularly in contexts with limited party control.",
        "In this note, we survey recent advances in the study of dynamical properties\nof the space of surfaces with constant curvature in three-dimensional manifolds\nof negative sectional curvature. We interpret this space as a two-dimensional\nanalogue of the geodesic flow and explore the extent to which the thermodynamic\nproperties of the latter can be generalized to the surface setting.\nAdditionally, we apply this theory to derive geometric rigidity results,\nincluding the rigidity of the hyperbolic marked area spectrum.",
        "According to quantum chromodynamics, at sufficiently high energy, the\nstructure of hadrons reveals a dynamic equilibrium between gluon splitting and\ngluon recombination -- a phenomenon known as saturation. The process of\ndiffractive photonuclear production of a J\/$\\psi$ vector meson provides a\ndirect insight into the gluon composition of hadrons. The J\/$\\psi$ production\nas a function of momentum transferred in the interaction, quantified by the\nMandelstam-$t$ variable, serves as an excellent probe for studying the\nstructure of hadrons within the impact-parameter plane, because different\nranges in $t$ are sensitive to the dynamics of the gluon field at varying\nspatial size scales. The ALICE collaboration has measured the energy dependence\nof incoherent photonuclear production of J\/$\\psi$ mesons off lead ions, at\n$\\sqrt{s_{\\rm NN}} = 5.02$ TeV, for three Mandelstam-$t$ intervals. The energy\ndependence of the photonuclear cross section at the highest $|t|$ range\nmeasured, $(0.81< |t| <1.44)$ GeV$^2$, is sensitive to subnucleonic structures\nof the Pb target. The increase of the cross section with energy at large $|t|$\nshows evidence of suppression with respect to the increase seen at low $|t|$.\nThe observed pattern of the energy evolution in data is similar to that of\ngluon saturation models.",
        "The paper presents an innovative methodology for designing frequency\nselective surface (FSS) based radar absorbing materials using machine learning\n(ML) technique. In conventional electromagnetic design, unit cell dimensions of\nFSS are used as input and absorption coefficient is then predicted for a given\ndesign. In this paper, absorption coefficient is considered as input to ML\nmodel and image of FSS unit cell is predicted. Later, this image is used for\ngenerating the FSS unit cell parameters. Eleven different ML models are studied\nover a wide frequency band of 1GHz to 30GHz. Out of which six ML models (i.e.\n(a) Random Forest classification, (b) K- Neighbors Classification, (c) Grid\nsearch regression, (d) Random Forest regression, (e) Decision tree\nclassification, and (f) Decision tree regression) show training accuracy more\nthan 90%. The absorption coefficients with varying frequencies of these\npredicted images are subsequently evaluated using commercial electromagnetic\nsolver. The performance of these ML models is encouraging, and it can be used\nfor accelerating design and optimization of high performance FSS based radar\nabsorbing material for advanced electromagnetic applications in future.",
        "Topologically ordered states, such as chiral spin liquids, have been proposed\nas candidates that host fractionalized excitations. However, detecting chiral\ncharacter or proximity to these non-trivial states remains a challenge.\nResonant Raman scattering can be a powerful tool for detecting chiral\nfluctuations, as the $A_{2g}$ channel probes excitations with broken\ntime-reversal symmetry and local chiral order. Here, we use exact\ndiagonalization to characterize the resonant $A_{2g}$ channel, alongside\ntwo-magnon scattering in $B_{1g}$ and $E_g$ channels, for the Hubbard model on\nlattices with increasing levels of geometric spin frustration, where tuning the\nincident energy near the Mott gap reveals strong chiral spin excitation\nintensity. Increased spin frustration in the Mott insulator results in an\noverall softening of the Raman $A_{2g}$ response, indicating a tendency toward\nlow energy chiral-chiral fluctuations in Mott insulators with magnetic\nfrustration and proximity to chiral spin liquid states that can potentially be\ntuned by external perturbations.",
        "We show that the $\\infty$-category of normed algebras in genuine $G$-spectra,\nas introduced by Bachmann-Hoyois, is modelled by strictly commutative algebras\nin $G$-symmetric spectra for any finite group $G$. We moreover provide an\nanalogous description of Schwede's ultra-commutative global ring spectra in\nhigher categorical terms.\n  Using these new descriptions, we exhibit the $\\infty$-category of\nultra-commutative global ring spectra as a partially lax limit of the\n$\\infty$-categories of genuine $G$-spectra for varying $G$, in analogy with the\nnon-multiplicative comparison of Nardin, Pol, and the second author.\n  Along the way, we establish various new results in parametrized higher\nalgebra, which we hope to be of independent interest.",
        "Objective: To optimize in-context learning in biomedical natural language\nprocessing by improving example selection. Methods: We introduce a novel\nmulti-mode retrieval-augmented generation (MMRAG) framework, which integrates\nfour retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2)\nTop Mode, retrieving the most relevant examples based on similarity; (3)\nDiversity Mode, ensuring variation in selected examples; and (4) Class Mode,\nselecting category-representative examples. This study evaluates MMRAG on three\ncore biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction\n(RE), and Text Classification (TC). The datasets used include BC2GM for gene\nand protein mention recognition (NER), DDI for drug-drug interaction extraction\n(RE), GIT for general biomedical information extraction (RE), and HealthAdvice\nfor health-related text classification (TC). The framework is tested with two\nlarge language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever,\nMedCPT, BGE-Large) to assess performance across different retrieval strategies.\nResults: The results from the Random mode indicate that providing more examples\nin the prompt improves the model's generation performance. Meanwhile, Top mode\nand Diversity mode significantly outperform Random mode on the RE (DDI) task,\nachieving an F1 score of 0.9669, a 26.4% improvement. Among the three\nretrievers tested, Contriever outperformed the other two in a greater number of\nexperiments. Additionally, Llama 2 and Llama 3 demonstrated varying\ncapabilities across different tasks, with Llama 3 showing a clear advantage in\nhandling NER tasks. Conclusion: MMRAG effectively enhances biomedical\nin-context learning by refining example selection, mitigating data scarcity\nissues, and demonstrating superior adaptability for NLP-driven healthcare\napplications.",
        "Conformal prediction is a flexible framework for calibrating machine learning\npredictions, providing distribution-free statistical guarantees. In outlier\ndetection, this calibration relies on a reference set of labeled inlier data to\ncontrol the type-I error rate. However, obtaining a perfectly labeled inlier\nreference set is often unrealistic, and a more practical scenario involves\naccess to a contaminated reference set containing a small fraction of outliers.\nThis paper analyzes the impact of such contamination on the validity of\nconformal methods. We prove that under realistic, non-adversarial settings,\ncalibration on contaminated data yields conservative type-I error control,\nshedding light on the inherent robustness of conformal methods. This\nconservativeness, however, typically results in a loss of power. To alleviate\nthis limitation, we propose a novel, active data-cleaning framework that\nleverages a limited labeling budget and an outlier detection model to\nselectively annotate data points in the contaminated reference set that are\nsuspected as outliers. By removing only the annotated outliers in this\n``suspicious'' subset, we can effectively enhance power while mitigating the\nrisk of inflating the type-I error rate, as supported by our theoretical\nanalysis. Experiments on real datasets validate the conservative behavior of\nconformal methods under contamination and show that the proposed data-cleaning\nstrategy improves power without sacrificing validity.",
        "A Single Ensemble Empirical Mode Decomposition (SEEMD) is proposed for\nlocating the damage in rolling element bearings. The SEEMD does not require a\nnumber of ensembles from the addition or subtraction of noise every time while\nprocessing the signals. The SEEMD requires just a single sifting process of a\nmodified raw signal to reduce the computation time significantly. The other\nadvantage of the SEEMD method is its success in dealing with non-Gaussian or\nnon-stationary perturbing signals. In SEEMD, initially, a fractional Gaussian\nnoise (FGN) is added to the raw signal to emphasize on high frequencies of the\nsignal. Then, a convoluted white Gaussian noise is multiplied to the resulting\nsignal which changes the spectral content of the signal which helps in\nextraction of the weak periodic signal. Finally, the obtained signal is\ndecomposed by using a single sifting process. The proposed methodology is\napplied to the raw signals obtained from the mining industry. These signals are\ndifficult to analyze since cyclic impulsive components are obscured by noise\nand other interference. Based on the results, the proposed method can\neffectively detect the fault where the signal of interest (SOI) has been\nextracted with good quality.",
        "The recent discovery of altermagnetism has sparked renewed interest in the\ngrowth of epitaxial films of the NiAs-phase polymorph of CrSb. This paper\ndescribes the magnetron sputtering-based fabrication and characterization of\nhigh-quality single crystalline CrSb(0001) thin films supported by an\nisostructural non-magnetic PtSb buffer. X-ray diffraction and scanning\ntransmission electron microscopy show that the films are phase-pure and possess\na very high crystalline quality (mosaicity ~0.05 deg), while also being free of\nextended crystallographic defects. Both scanning electron microscopy and atomic\nforce microscopy confirm their smooth and homogeneous topography. Additionally,\nthe elemental composition of our films was found to be close to stoichiometric\nvia electron probe microanalysis and X-ray fluorescence. Thus, the developed\nsamples represent an ideal platform for further investigation of the material\nproperties of CrSb.",
        "Developing robotic agents that can perform well in diverse environments while\nshowing a variety of behaviors is a key challenge in AI and robotics.\nTraditional reinforcement learning (RL) methods often create agents that\nspecialize in narrow tasks, limiting their adaptability and diversity. To\novercome this, we propose a preliminary, evolution-inspired framework that\nincludes a reproduction module, similar to natural species reproduction,\nbalancing diversity and specialization. By integrating RL, imitation learning\n(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents\ncontinuously through complex tasks. This approach promotes adaptability,\ninheritance of useful traits, and continual learning. Agents not only refine\ninherited skills but also surpass their predecessors. Our initial experiments\nshow that this method improves exploration efficiency and supports open-ended\nlearning, offering a scalable solution where sparse reward coupled with diverse\nterrain environments induces a multi-task setting.",
        "The hybrid series-parallel system is the final form of the power\nelectronics-enabled power system, which combines the advantages of both series\nand parallel connections. Although self-synchronization of parallel-type and\nseries-type systems is well known, self-synchronization of hybrid systems\nremains unrevealed. To fill in this gap, a fully self-synchronized control for\nhybrid series-parallel system is proposed in this paper. Based on the\nself-synchronization mechanism of power angle in parallel-type system and power\nfactor angle in series-type system, a decentralized control strategy by\nintegration of power droop and power factor angle droop can realize\nself-synchronization and power balancing of each module in the hybrid system.",
        "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.",
        "This study examines gender disparities in communication research through\ncitation metrics, authorship patterns, team composition, and faculty salaries.\nUsing data from 62,359 papers across 121 communication journals, we find that\nwhile female authors are increasingly represented, citation gaps persist, with\nsole-authored papers by women receiving fewer citations than those by men,\nespecially in smaller teams. Team composition analysis reveals a tendency\ntoward gender homophily, with single-gender teams being more common. In top\nU.S. communication journals, female authors face underrepresentation and\ncitation disparities favoring male authors. Salary analysis from leading U.S.\npublic universities shows that female faculty earn lower salaries at the\nAssistant Professor level, though disparities lessen at higher ranks. These\nfindings highlight the need for greater efforts to promote gender equity\nthrough inclusive collaboration, equitable citation practices, and fair\ncompensation.",
        "We present the results of an Australian Square Kilometre Array Pathfinder\n(ASKAP) 944 MHz and Very Large Array Sky Survey (VLASS) 3~GHz search for a\nradio-continuum counterpart of the recent ultra-high-energy (UHE) neutrino\nevent, KM3-230213A. Using (ASKAP), we catalog 1052 radio sources within the\n1.5$^\\circ$ radius search area (68% certainty region) around the particle's\ncalculated origin, 10 of which we classify as blazar candidates based on their\nradio spectra. The most prominent radio source in the search area is the nearby\nspiral galaxy UGCA 127 (nicknamed Phaedra, From Greek: $\\phi\\alpha\ni\\delta\\rho\\alpha$, a Cretan princess of Greek Mythology, derived from\nPhaidros, Greek: ${\\phi}{\\alpha}{\\iota}{\\delta}{\\rho}o{\\varsigma}$, meaning\n'bright'.). Its non-thermal radio spectrum classifies it as a non-blazar active\ngalactic nucleus (AGN). We also present an extended radio source, WISEA\nJ061715.89-075455.4 (nicknamed Hebe, From Greek: $H{\\beta}{\\eta}$, the Greek\ngoddess of youth.), located only ~7' from the geometric center of the search\narea, with a very unusual highly polarized compact component. Finally, we\npresent a strong radio source, EMU J062248-072246 (nicknamed Narcissus, From\nGreek $N{\\alpha}{\\rho}{\\kappa}{\\iota}{\\sigma}{\\sigma}o{\\zeta}$ was a\nself-absorbed hunter from Thespiae in Boeotia.), which has a maximum\nself-absorption spectral slope of +2.5 at low frequencies, and exhibits ~25%\nflux density variability over the ~5-year VLASS 3~GHz survey.",
        "Current constraints on flavor-changing neutral currents (FCNCs) strongly\nindicate that any new physics emerging at the 1-10 TeV scale must adhere to the\nMinimal Flavor Violation (MFV) principle, where Yukawa couplings are the sole\nsources of flavor violation. In this work, we present a model inspired by a\ngauged $SU(3)$ flavor symmetry that dynamically generates leptonic Yukawa\nmatrices through effective operators. The model incorporates a scalar sector\nwith two sets of flavons, characterized by their vacuum expectation values\n(VEVs), which govern the suppression scale of the Yukawa couplings and the\nhierarchy of neutrino masses. By leveraging phenomenologically viable Yukawa\ntextures, we derive restrictions on the flavon VEVs and demonstrate the\ncompatibility of the model with experimental neutrino oscillation data.\nFurthermore, the model predicts at least one neutrino mass to be strongly\nsuppressed, consistent with the normal mass ordering and experimental upper\nbounds. This framework provides a robust mechanism for dynamically generating\nneutrino masses and mixing while addressing key challenges in leptonic flavor\nphysics, such as FCNC suppression and CP-violating phases.",
        "Temporal sequence modeling stands as the fundamental foundation for video\nprediction systems and real-time forecasting operations as well as anomaly\ndetection applications. The achievement of accurate predictions through\nefficient resource consumption remains an ongoing issue in contemporary\ntemporal sequence modeling. We introduce the Multi-Attention Unit (MAUCell)\nwhich combines Generative Adversarial Networks (GANs) and spatio-temporal\nattention mechanisms to improve video frame prediction capabilities. Our\napproach implements three types of attention models to capture intricate motion\nsequences. A dynamic combination of these attention outputs allows the model to\nreach both advanced decision accuracy along with superior quality while\nremaining computationally efficient. The integration of GAN elements makes\ngenerated frames appear more true to life therefore the framework creates\noutput sequences which mimic real-world footage. The new design system\nmaintains equilibrium between temporal continuity and spatial accuracy to\ndeliver reliable video prediction. Through a comprehensive evaluation\nmethodology which merged the perceptual LPIPS measurement together with classic\ntests MSE, MAE, SSIM and PSNR exhibited enhancing capabilities than\ncontemporary approaches based on direct benchmark tests of Moving MNIST, KTH\nAction, and CASIA-B (Preprocessed) datasets. Our examination indicates that\nMAUCell shows promise for operational time requirements. The research findings\ndemonstrate how GANs work best with attention mechanisms to create better\napplications for predicting video sequences.",
        "Here, we study the $q$-numerical radius of rank-one operators on a Hilbert\nspace $\\mathcal{H}$. More precisely, for $q \\in [0,1]$ and $a, b \\in\n\\mathcal{H}$, we establish the formula \\[ \\omega_q(a \\otimes b) =\n\\frac{1}{2}\\left(\\|a\\|\\|b\\| + q|\\langle a, b \\rangle| +\n\\sqrt{1-q^2}\\sqrt{\\|a\\|^2\\|b\\|^2 - |\\langle a, b \\rangle|^2}\\right), \\] which\nrepresents a generalization of the well-known formula for the numerical radius\nof a rank-one operator in a Hilbert space, obtained by setting $q = 1$. As a\ncorollary, we also derive a generalization of the classical Buzano inequality."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
      ],
      "abstract":[
        "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Social dynamics can delay or prevent climate tipping points by speeding\n  the adoption of climate change mitigation",
        "Greedy Stein Variational Gradient Descent: An algorithmic approach for\n  wave prospection problems",
        "Uncertainty-permitting machine learning reveals sources of dynamic sea\n  level predictability across daily-to-seasonal timescales",
        "Leveraging statistical models to improve pre-season forecasting and\n  in-season management of a recreational fishery",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Discrete Level Set Persistence for Finite Discrete Functions",
        "Accurate myocardial T1 mapping at 5T using an improved MOLLI method: A\n  validation study",
        "Sparse Identification for bifurcating phenomena in Computational Fluid\n  Dynamics",
        "The $p$-adic limits of iterated $p$-power cyclic resultants of\n  multivariable polynomials",
        "An exactly solvable tight-binding billiard in graphene",
        "What Kind of Morphisms Induces Covering Maps over a Real Closed Field?",
        "Learning Robot Safety from Sparse Human Feedback using Conformal\n  Prediction",
        "Semiclassical resolvent estimates for the magnetic Schr \\\"odinger\n  operator",
        "On Construction, Properties and Simulation of Haar-Based Multifractional\n  Processes",
        "TRANSIT your events into a new mass: Fast background interpolation for\n  weakly-supervised anomaly searches",
        "Multiplayer Federated Learning: Reaching Equilibrium with Less\n  Communication",
        "Self-Refinement of Auxiliary-Field Quantum Monte Carlo via\n  Non-Orthogonal Configuration Interaction",
        "A reduction theorem for non-vanishing of Hochschild cohomology of block\n  algebras and Happel's property",
        "Multipliers, $W$-algebras and the growth of generalized polynomial\n  identities",
        "High-velocity outflows in [OIII] emitters at z=2.5-9 from JWST NIRSpec\n  medium-resolution spectroscopy",
        "Exploring symbolic regression and genetic algorithms for astronomical\n  object classification",
        "SDEs with subcritical Lebesgue--H\\\"{o}lder drifts and driven by\n  $\\alpha$-stable processes",
        "Monogenic Reciprocal Quartic Polynomials And Their Galois Groups",
        "Emergent fractals in hBN-encapsulated graphene based supermoir\\'e\n  structures and their experimental signatures",
        "Benchmarking ANN extrapolations of the ground-state energies and radii\n  of Li isotopes",
        "Fiducial Confidence Intervals for Agreement Measures Among Raters Under\n  a Generalized Linear Mixed Effects Model",
        "Navigating permanent underdetermination in dark energy and inflationary\n  cosmology",
        "Measurement-Device-Independent Certification of Schmidt Number",
        "Automorphisms of bounded growth"
      ],
      "abstract":[
        "Social behaviour models are increasingly integrated into climate change\nstudies, and the significance of climate tipping points for `runaway' climate\nchange is well recognised. However, there has been insufficient focus on\ntipping points in social-climate dynamics. We developed a coupled\nsocial-climate model consisting of an Earth system model and a social behaviour\nmodel, both with tipping elements. The social model explores opinion formation\nby analysing social learning rates, the net cost of mitigation, and the\nstrength of social norms. Our results indicate that the net cost of mitigation\nand social norms have minimal impact on tipping points when social norms are\nweak. As social norms strengthen, the climate tipping point can trigger a\ntipping element in the social model. However, faster social learning can delay\nor prevent the climate tipping point: sufficiently fast social learning means\ngrowing climate change mitigation can outpace the oncoming climate tipping\npoint, despite social-climate feedback. By comparing high- and low-risk\nscenarios, we demonstrated high-risk scenarios increase the likelihood of\ntipping points. We also illustrate the role of a critical temperature anomaly\nin triggering tipping points. In conclusion, understanding social behaviour\ndynamics is vital for predicting climate tipping points and mitigating their\nimpacts.",
        "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
        "Reliable dynamic sea level forecasts are hindered by numerous sources of\nuncertainty on daily-to-seasonal timescales (1-180 days) due to atmospheric\nboundary conditions and internal ocean variability. Studies have demonstrated\nthat certain initial states can extend predictability horizons; thus,\nidentifying these initial conditions may help improve forecast skill. Here, we\nidentify sources of dynamic sea level predictability on daily-to-seasonal\ntimescales using neural networks trained on CESM2 large ensemble data to\nforecast dynamic sea level. The forecasts yield not only a point estimate for\nsea level but also a standard deviation to quantify forecast uncertainty based\non the initial conditions. Forecasted uncertainties can be leveraged to\nidentify state-dependent sources of predictability at most locations and\nforecast leads. Network forecasts, particularly in the low-latitude\nIndo-Pacific, exhibit skillful deterministic predictions and skillfully\nforecast exceedance probabilities relative to local linear baselines. For\nnetworks trained at Guam and in the western Indian Ocean, the transfer of\nsources of predictability from local sources to remote sources is presented by\nthe deteriorating utility of initial condition information for predicting\nexceedance events. Propagating Rossby waves are identified as a potential\nsource of predictability for dynamic sea level at Guam. In the Indian Ocean,\npersistence of thermosteric sea level anomalies from the Indian Ocean Dipole\nmay be a source of predictability on subseasonal timescales, but El Ni\\~no\ndrives predictability on seasonal timescales. This work shows how\nuncertainty-quantifying machine learning can help identify changes in sources\nof state-dependent predictability over a range of forecast leads.",
        "Effective management of recreational fisheries requires accurate forecasting\nof future harvests and real-time monitoring of ongoing harvests. Traditional\nmethods that rely on historical catch data to predict short-term harvests can\nbe unreliable, particularly if changes in management regulations alter angler\nbehavior. In contrast, statistical modeling approaches can provide faster, more\nflexible, and potentially more accurate predictions, enhancing management\noutcomes. In this study, we developed and tested models to improve predictions\nof Gulf of Mexico gag harvests for both pre-season planning and in-season\nmonitoring. Our best-fitting model outperformed traditional methods (i.e.,\nestimates derived from historical average harvest) for both cumulative\npre-season projections and in-season monitoring. Notably, our modeling\nframework appeared to be more accurate in more recent, shorter seasons due to\nits ability to account for effort compression. A key advantage of our framework\nis its ability to explicitly quantify the probability of exceeding harvest\nquotas for any given season duration. This feature enables managers to evaluate\ntrade-offs between season duration and conservation goals. This is especially\ncritical for vulnerable, highly targeted stocks. Our findings also underscore\nthe value of statistical models to complement and advance traditional fisheries\nmanagement approaches.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "We study sublevel set and superlevel set persistent homology on discrete\nfunctions through the perspective of finite ordered sets of both linearly\nordered and cyclically ordered domains. Finite ordered sets also serve as the\ncodomain of our functions making all arguments finite and discrete. We prove\nduality of filtrations of sublevel sets and superlevel sets that undergirths a\nrange of duality results of sublevel set persistent homology without the need\nto invoke complications of continuous functions or classical Morse theory. We\nshow that Morse-like behavior can be achieved for flat extrema without assuming\ngenericity. Additionally, we show that with inversion of order, one can compute\nsublevel set persistence from superlevel set persistence, and vice versa via a\nduality result that does not require the boundary to be treated as a special\ncase. Furthermore, we discuss aspects of barcode construction rules, surgery of\ncircular and linearly ordered sets, as well as surgery on auxiliary structures\nsuch as box snakes, which segment the ordered set by extrema and monotones.",
        "Purpose: To develop 5T-SRIS, an improved 5T myocardial T1 mapping method\nbased on MOLLI, which addresses limitations in inversion efficiency, readout\nperturbations, and imperfect magnetization recovery. Methods: The proposed\n5T-SRIS method is based on a modified 5-(3)-3 MOLLI sequence with ECG gating\nand gradient echo readout. To improve inversion efficiency at 5T, the inversion\npulse was redesigned using adiabatic hyperbolic secant (HSn) and\ntangent\/hyperbolic tangent (Tan\/Tanh) pulses. Signal evolution was modeled\nrecursively with inversion efficiency and a correction factor (C) to correct\ninversion imperfections, and T1 values were estimated via nonlinear\noptimization. The method was validated in phantom studies, as well as in 21\nhealthy volunteers and 9 patients at 5T. Results: The optimized IR pulse based\non the tangent\/hyperbolic tangent pulse was found to outperform the\nconventional hyperbolic secant IR pulse at the 5T scanner. This optimized IR\npulse achieves an average inversion factor of 0.9014within a B0 range of 250Hz\nand a B1 range of -50% to 20%. Phantom studies show that the 5T-SRIS achieved\nhigh accuracy with errors within 5%. In vivo studies with 21 healthy\nvolunteers, the native myocardial T1 values were 1468 ms (apex), 1514 ms\n(middle), and 1545 ms (base). In vivo studies with 9 heart patients, the native\nmyocardial T1 values were 1484 ms (apex), 1532 ms (middle), and 1581 ms (base).\nAnd the post myocardial T1 values were 669 ms (apex), 698 ms (middle), and 675\nms (base). Conclusion: The 5T-SRIS technique is robust and suitable for\nclinical cardiac imaging. This study demonstrates its feasibility for accurate\nmyocardial T1 mapping at 5T, despite challenges related to magnetic field\ninhomogeneity. Keywords: Myocardial T1 mapping, 5T, improved MOLLI, 5T-SRIS",
        "This work investigates model reduction techniques for nonlinear parameterized\nand time-dependent PDEs, specifically focusing on bifurcating phenomena in\nComputational Fluid Dynamics (CFD). We develop interpretable and non-intrusive\nReduced Order Models (ROMs) capable of capturing dynamics associated with\nbifurcations by identifying a minimal set of coordinates. Our methodology\ncombines the Sparse Identification of Nonlinear Dynamics (SINDy) method with a\ndeep learning framework based on Autoencoder (AE) architectures. To enhance\ndimensionality reduction, we integrate a nested Proper Orthogonal Decomposition\n(POD) with the SINDy-AE architecture. This novel combination enables a sparse\ndiscovery of system dynamics while maintaining efficiency of the reduced model.\nWe demonstrate our approach via two challenging test cases defined on\nsudden-expansion channel geometries: a symmetry-breaking bifurcation and a Hopf\nbifurcation. Starting from a comprehensive analysis of their high-fidelity\nbehavior, i.e. symmetry-breaking phenomena and the rise of unsteady periodic\nsolutions, we validate the accuracy and computational efficiency of our ROMs.\nThe results show successful reconstruction of the bifurcations, accurate\nprediction of system evolution for unseen parameter values, and significant\nspeed-up compared to full-order methods.",
        "Let $p$ be a prime number. The $p$-power cyclic resultant of a polynomial is\nthe determinant of the Sylvester matrix of $t^{p^n}-1$ and the polynomial. It\nis known that the sequence of $p$-power cyclic resultants and its non-$p$-parts\nconverge in $\\mathbb{Z}_p$. This article shows the $p$-adic convergence of the\niterated $p$-power cyclic resultants of multivariable polynomials. As an\napplication, we show the $p$-adic convergence of the torsion numbers of\n$\\mathbb{Z}_p^d$-coverings of links. We also explicitly calculate the $p$-adic\nlimits for the twisted Whitehead links as concrete examples. Moreover, in a\nspecific case, we show that our $p$-adic limit of torsion numbers coincides\nwith the $p$-adic torsion, which is a homotopy invariant of a CW-complex\nintroduced by S. Kionke.",
        "A triangular graphenic billiard is defined as a planar carbon polymer in the\nH\\\"uckeloid approximation of $\\pi-$band electrons. It is shown that the\nequilateral triangle of arbitrary size and zig-zag edges allows for exact\nsolutions of the associated spectral problem. This is done by a construction of\nwave superpositions similar to the Lam\\'e solution of the Helmholtz equation in\na triangular cavity, revisited by Pinsky. Exact wave functions, eigenvalues,\ndegeneracies, and edge states are provided. The edge states are also obtained\nby a non-periodic construction of waves with vanishing energy. A comment on its\nconnection with recent molecular models, such as triangulene, is given.",
        "In this article, we show that a flat morphism of $k$-varieties\n($\\mathop{\\mathrm{char}} k=0$) with locally constant geometric fibers becomes\nfinite \\'etale after reduction. When $k$ is a real closed field, we prove that\nsuch a morphism induces a covering map on the rational points. We further give\na triviality result different from Hardt's and a new interpretation of the\nconstruction of cylindrical algebraic decomposition as applications.",
        "Ensuring robot safety can be challenging; user-defined constraints can miss\nedge cases, policies can become unsafe even when trained from safe data, and\nsafety can be subjective. Thus, we learn about robot safety by showing policy\ntrajectories to a human who flags unsafe behavior. From this binary feedback,\nwe use the statistical method of conformal prediction to identify a region of\nstates, potentially in learned latent space, guaranteed to contain a\nuser-specified fraction of future policy errors. Our method is\nsample-efficient, as it builds on nearest neighbor classification and avoids\nwithholding data as is common with conformal prediction. By alerting if the\nrobot reaches the suspected unsafe region, we obtain a warning system that\nmimics the human's safety preferences with guaranteed miss rate. From video\nlabeling, our system can detect when a quadcopter visuomotor policy will fail\nto steer through a designated gate. We present an approach for policy\nimprovement by avoiding the suspected unsafe region. With it we improve a model\npredictive controller's safety, as shown in experimental testing with 30\nquadcopter flights across 6 navigation tasks. Code and videos are provided.",
        "We obtain semiclassical resolvent estimates for the Schr{\\\"o}dinger operator\n(ih$\\nabla$ + b)^2 + V in R^d , d $\\ge$ 3, where h is a semiclassical\nparameter, V and b are real-valued electric and magnetic potentials independent\nof h.Under quite general assumptions, we prove that the norm of the weighted\nresolvent is bounded by exp(Ch^{-2} log(h^{ -1} )) . We get better resolvent\nbounds for electric potentials which are H{\\\"o}lder with respect to the radial\nvariable and magnetic potentials which are H{\\\"o}lder with respect to the space\nvariable. For long-range electric potentials which are Lipschitz with respect\nto the radial variable and long-range magnetic potentials which are Lipschitz\nwith respect to the space variable we obtain a resolvent bound of the form\nexp(Ch^{-1}) .",
        "Multifractional processes extend the concept of fractional Brownian motion by\nreplacing the constant Hurst parameter with a time-varying Hurst function. This\nextension allows for modulation of the roughness of sample paths over time. The\npaper introduces a new class of multifractional processes, the Gaussian\nHaar-based multifractional processes (GHBMP), which is based on the Haar\nwavelet series representations. The resulting processes cover a significantly\nbroader set of Hurst functions compared to the existing literature, enhancing\ntheir suitability for both practical applications and theoretical studies. The\ntheoretical properties of these processes are investigated. Simulation studies\nconducted for various Hurst functions validate the proposed model and\ndemonstrate its applicability, even for Hurst functions exhibiting\ndiscontinuous behaviour.",
        "We introduce a new model for conditional and continuous data morphing called\nTRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it\nto create a background data template for weakly-supervised searches at the LHC.\nThe method smoothly transforms sideband events to match signal region mass\ndistributions. We demonstrate the performance of TRANSIT using the LHC Olympics\nR\\&D dataset. The model captures non-linear mass correlations of features and\nproduces a template that offers a competitive anomaly sensitivity compared to\nstate-of-the-art transport-based template generators. Moreover, the\ncomputational training time required for TRANSIT is an order of magnitude lower\nthan that of competing deep learning methods. This makes it ideal for analyses\nthat iterate over many signal regions and signal models. Unlike generative\nmodels, which must learn a full probability density distribution, i.e., the\ncorrelations between all the variables, the proposed transport model only has\nto learn a smooth conditional shift of the distribution. This allows for a\nsimpler, more efficient residual architecture, enabling mass uncorrelated\nfeatures to pass the network unchanged while the mass correlated features are\nadjusted accordingly. Furthermore, we show that the latent space of the model\nprovides a set of mass decorrelated features useful for anomaly detection\nwithout background sculpting.",
        "Traditional Federated Learning (FL) approaches assume collaborative clients\nwith aligned objectives working towards a shared global model. However, in many\nreal-world scenarios, clients act as rational players with individual\nobjectives and strategic behaviors, a concept that existing FL frameworks are\nnot equipped to adequately address. To bridge this gap, we introduce\nMultiplayer Federated Learning (MpFL), a novel framework that models the\nclients in the FL environment as players in a game-theoretic context, aiming to\nreach an equilibrium. In this scenario, each player tries to optimize their own\nutility function, which may not align with the collective goal. Within MpFL, we\npropose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm\nin which each player\/client performs local updates independently and\nperiodically communicates with other players. We theoretically analyze\nPEARL-SGD and prove that it reaches a neighborhood of equilibrium with less\ncommunication in the stochastic setup compared to its non-local counterpart.\nFinally, we verify our theoretical findings through numerical experiments.",
        "For optimal accuracy, auxiliary-field quantum Monte Carlo (AFQMC) requires\ntrial states consisting of multiple Slater determinants. We develop an\nefficient algorithm to select the determinants from an AFQMC random walk\neliminating the need for other methods. When determinants contribute\nsignificantly to the non-orthogonal configuration interaction energy, we\ninclude them in the trial state. These refined trial wave functions\nsignificantly reduce the phaseless bias and sampling variance of the local\nenergy estimator. With 100 to 200 determinants, we lower the error of AFQMC by\nup to a factor of 10 for second row elements that are not accurately described\nwith a Hartree-Fock trial wave function. For the HEAT set, we improve the\naverage error to within the chemical accuracy. For benzene, the largest studied\nsystem, we reduce AFQMC error by 80% with 214 Slater determinants and find a\n10-fold increase of the time to solution. We show that the remaining error of\nthe method prevails in systems with static correlation or strong spin\ncontamination.",
        "We show that any $p$-block algebra ($p$ is a prime) of a finite group with\nrealizable fusion system satisfies Happel's property. We obtain a reduction\ntheorem for the non-vanishing of the first Hochschild cohomology of block\nalgebras with non-trivial defect groups. Along the way we investigate this\nproblem for the blocks of some simple finite groups.",
        "Let $A$ be a $W$-algebra over a field $F$ of characteristic zero, where $W$\nis any $F$-algebra. We develop a comprehensive theory of generalized identities\nindependently on the algebraic structure of $W$, using the multiplier algebra\nof $A.$ We also characterize the generalized varieties of almost polynomial\ngrowth generated by finite dimensional $W$-algebras. Finally, we provide a\ncounter-example to the Specht property of generalized $T_W$-ideals in\ncharacteristic zero.",
        "We identify galaxies hosting ionised-gas high-velocity outflows from the\ncomplete sample of medium resolution (R1000) JWST\/NIRSpec MSA spectroscopy\ntaken as part of the JWST Advanced Deep Extragalactic Survey (JADES). From a\ntotal sample of 1087 [OIII]5007 emitters we identify 40 galaxies with a\nblue-side broadened [OIII]5007 line at z=2.5-9. Of these, 34 are strong outflow\ncandidates whilst 6 sources have broadening potentially driven by rotating\nclumps. Our outflow candidate sample is mainly composed of star-forming\ngalaxies, including ~65% starbursts, which span the stellar mass range\nlog10(M*\/Msun)=7.5-11.0. It also includes two candidate type-2 active galactic\nnuclei (AGN) and a 'little red dot' (LRD). We report a median outflow velocity\nof 531(+146)(-159) km\/s and an overall incidence rate of 3.4%. These values are\nsignificantly higher and lower respectively than recent similar works, which we\naccredit to the limiting resolution of the R1000 spectroscopy and a stricter\noutflow selection criterion. We find no correlation between the outflow\nvelocity and the galaxy stellar mass or star-formation rate. The median ratio\nbetween outflow velocity and escape velocity is 0.77(+0.36)(-0.32), indicating\nthat most outflows cannot escape the galaxy gravitational potentials. We do\nfind an anti-correlation between mass loading factor and stellar mass up to M*\n~(10^10)Msun, with most lowest stellar-mass M*<(10^9)Msun galaxies reaching\nvalues well above unity, as is the case for local starburst galaxies.",
        "This study explores the use of symbolic regression (SR) combined with genetic\nalgorithms (GA) to classify astronomical objects. Using the SDSS17 dataset from\nKaggle, which includes 100,000 observations of stars, galaxies, and quasars, we\napplied SR to 10\\% of the data to derive a mathematical expression capable of\ndistinguishing these classes. A genetic algorithm was then employed to optimize\nthe hyperparameters of the expression, refining the model's performance. The\nfinal model achieved a Cohen's kappa value of 0.81, indicating a strong\nagreement with true classifications. Our results demonstrate that the SR+GA\napproach can produce interpretable and accurate models for the classification\nof astronomical objects, offering a promising alternative to traditional\nblack-box machine learning methods.",
        "We obtain the unique weak and strong solvability for time inhomogeneous\nstochastic differential equations with the drifts in subcritical\nLebesgue--H\\\"{o}lder spaces $L^p([0,T];{\\mathcal C}_b^{\\beta}({\\mathbb\nR}^d;{\\mathbb R}^d))$ and driven by $\\alpha$-stable processes for $\\alpha\\in\n(0,2)$. The weak well-posedness is derived for $\\beta\\in (0,1)$,\n$\\alpha+\\beta>1$ and $p>\\alpha\/(\\alpha+\\beta-1)$ through the Prohorov theorem,\nSkorohod representation and the regularity estimates of solutions for a class\nof fractional parabolic partial differential equations. The pathwise uniqueness\nand Davie's type uniqueness are proved for $\\beta>1- \\alpha\/2$ by using\nIt\\^{o}--Tanaka's trick. Moreover, we give a counterexample to the pathwise\nuniqueness for the supercritical Lebesgue--H\\\"{o}lder drifts to explain the\npresent result is sharp.",
        "Suppose that $f(x)=x^4+Ax^3+Bx^2+Ax+1\\in {\\mathbb Z}[x]$. We say that $f(x)$\nis monogenic if $f(x)$ is irreducible over ${\\mathbb Q}$ and\n$\\{1,\\theta,\\theta^2,\\theta^3\\}$ is a basis for the ring of integers of\n${\\mathbb Q}(\\theta)$, where $f(\\theta)=0$. For each possible Galois group $G$\nthat can occur in the two cases of $A\\ne 0$ with $B=0$, and $AB\\ne 0$, we\ndetermine all monogenic polynomials $f(x)$ with Galois group $G$.",
        "Supermoir\\'e structures (SMS), formed by overlapping moir\\'e-patterns in van\nder Waals heterostructures, display complex behaviour that lacks a\ncomprehensive low-energy theoretical description. We demonstrate that these\nstructures can form emergent fractals under specific conditions and identify\nthe parameter space where this occurs in hexagonal trilateral SMS. This\nfractality enables a reliable calculation of low-energy band counts, which are\ncrucial for understanding both single-particle and correlation effects. Using\nan effective Hamiltonian that includes in- and out-of-plane lattice relaxation,\nwe analyze SMS in hBN-encapsulated single and bilayer graphene. We prescribe\nmethods to experimentally verify these fractals and extract their fractal\ndimension through angle-resolved photoemission spectroscopy (ARPES) and\nscanning tunneling microscopy (STM).",
        "We present a comparison of model-space extrapolation methods for No-Core\nShell Model calculations of ground-state energies and root-mean-square radii in\nLi isotopes. In particular, we benchmark the latest machine learning tools\nagainst widely used exponential and infrared extrapolations for energies and\ncrossing point estimates for radii. Our findings demonstrate that machine\nlearning-based approaches provide reliable predictions with robust statistical\nuncertainties for both observables even in small model spaces. These\npredictions are compatible with established exponential and IR extrapolations\nof energies and mark a notable improvement over conventional radius estimates.",
        "A generalization of the classical concordance correlation coefficient (CCC)\nis considered under a three-level design where multiple raters rate every\nsubject over time, and each rater is rating every subject multiple times at\neach measuring time point. The ratings can be discrete or continuous. A\nmethodology is developed for the interval estimation of the CCC based on a\nsuitable linearization of the model along with an adaptation of the fiducial\ninference approach. The resulting confidence intervals have satisfactory\ncoverage probabilities and shorter expected widths compared to the interval\nbased on Fisher Z-transformation, even under moderate sample sizes. Two real\napplications available in the literature are discussed. The first application\nis based on a clinical trial to determine if various treatments are more\neffective than a placebo for treating knee pain associated with osteoarthritis.\nThe CCC was used to assess agreement among the manual measurements of the joint\nspace widths on plain radiographs by two raters, and the computer-generated\nmeasurements of digitalized radiographs. The second example is on a\ncorticospinal tractography, and the CCC was once again applied in order to\nevaluate the agreement between a well-trained technologist and a\nneuroradiologist regarding the measurements of fiber number in both the right\nand left corticospinal tracts. Other relevant applications of our general\napproach are highlighted in many areas including artificial intelligence.",
        "We identify troubling cases of so-called `permanent underdetermination' in\nboth dark energy and inflationary cosmology. We bring to bear (a) a taxonomy of\npossible responses to underdetermination, and (b) an understanding of both dark\nenergy and inflationary cosmology from an effective field point of view. We\nargue that, under certain conditions, there are available viable responses\nwhich can alleviate at least some of the concerns about underdetermination in\nthe dark energy and inflationary sectors. However, outside of these specific\nscenarios, the epistemic threat of permanent underdetermination will persist.",
        "Bipartite quantum states with higher Schmidt numbers have been shown to\noutperform those with lower Schmidt numbers in various information processing\ntasks. Therefore, to ensure the efficient use of resources in these tasks, it\nis essential to certify the Schmidt number of the resource states. Ideally,\nthis certification should rely as little as possible on the certifying devices,\nensuring robustness against potential imperfections. In this work, we explore\nthe scope of fully and partially device-independent Schmidt number\ncertification methods. We demonstrate the general impossibility of fully\ndevice-independent certification for all states. Specifically, in a restricted\nsetting, we present a class of states with Schmidt number 3, for which it is\nimpossible to certify that their Schmidt number is greater than 2. However, we\nshow that the Schmidt number of all states can be certified in a\nmeasurement-device-independent manner via semi-quantum nonlocal games, which\nassume trust in the preparation devices. Finally, we present an explicit\nconstruction of a semi-quantum game for the measurement-device-independent\ncertification of a class of states.",
        "We study birational automorphisms of algebraic varieties of bounded growth,\ni.e. such that the norms of the inverse images ${(f^n)}^* \\colon\n\\mathrm{NS}(X)\\to \\mathrm{NS}(X)$ of the powers of the automorphism\n$f\\in\\mathrm{Bir}(X)$ are bounded above for $n\\geqslant 0$. We prove that some\npower of an infinite order automorphism of a variety $X$ with such property\nfactors either through an infinite order translation on the Albanese variety of\n$X$ or through an infinite order regular automorphism of $\\mathbb{P}^m$ for\n$m\\geqslant 1$. We deduce from this that if a rationally connected threefold\nadmits an infinite order automorphism whose growth is bounded then the\nthreefold is rational and an iterate of the automorphism is birationally\nconjugate to a regular automorphism of $\\mathbb{P}^3$, a generalization of\nBlanc and Deserti's result."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel",
    "start_abstract":"Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2",
        "b0"
      ],
      "title":[
        "Shannon capacity of signal transduction for multiple independent receptors",
        "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
      ],
      "abstract":[
        "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
        "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
      ],
      "categories":[
        "eess.SP",
        "cs.SY"
      ]
    },
    "list":{
      "title":[
        "Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based\n  Tiling Generation",
        "A Unifying View of Linear Function Approximation in Off-Policy RL\n  Through Matrix Splitting and Preconditioning",
        "Cup Products on Hochschild Cohomology of Hopf-Galois Extensions.pdf",
        "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
        "Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization",
        "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "Parallelizing Multi-objective A* Search",
        "The Role of Artificial Intelligence in Enhancing Insulin Recommendations\n  and Therapy Outcomes",
        "Algorithmical Aspects of Some Bio Inspired Operations",
        "Proximal Flow Inspired Multi-Step Methods",
        "E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization",
        "Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized\n  Attention Vision Transformer for Autonomous Driving",
        "One-Loop QCD Corrections to $\\bar{u}d \\rightarrow t\\bar{t}W$ at\n  $\\mathcal{O}(\\varepsilon^2)$",
        "Unconstrained Body Recognition at Altitude and Range: Comparing Four\n  Approaches",
        "Realization of Two-dimensional Discrete Time Crystals with Anisotropic\n  Heisenberg Coupling",
        "Dissipative quantum phase transitions monitored by current fluctuations",
        "High-frequency coronal Alfv\\'enic waves observed with DKIST\/Cryo-NIRSP",
        "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
        "Controlling Large Language Models Through Concept Activation Vectors",
        "Publish on Ping: A Better Way to Publish Reservations in Memory\n  Reclamation for Concurrent Data Structures",
        "Design and Benchmarks for Emulating Kondo Dynamics on a Quantum Chip",
        "A note on the Sauvageot density principle",
        "Web Phishing Net (WPN): A scalable machine learning approach for\n  real-time phishing campaign detection",
        "Prompting in the Dark: Assessing Human Performance in Prompt Engineering\n  for Data Labeling When Gold Labels Are Absent",
        "An Analysis for Reasoning Bias of Language Models with Small\n  Initialization",
        "Disordered Weyl semimetal as an array of coupled Hubbard chains",
        "Process-based Self-Rewarding Language Models",
        "Isogeny graphs with level structures arrising from the Verschiebung map"
      ],
      "abstract":[
        "As an artistic aid in tiled level design, Constraint Based Tiling Generation\n(CBTG) algorithms can help to automatically create level realizations from a\nset of tiles and placement constraints. Merrell's Modify in Blocks Model\nSynthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as\nConstraint Based Tiling Generation (CBTG) algorithms that work well for many\nscenarios but have limitations in problem size, problem setup and solution\nbiasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling\nGeneration algorithm, that can handle large problem sizes, requires minimal\nassumptions for setup and can help mitigate solution biasing. POMS attempts to\nresolve indeterminate grid regions by trying to progressively realize\nsub-blocks, performing a stochastic boundary erosion on previously resolved\nregions should sub-block resolution fail. We highlight the results of running a\nreference implementation on different tile sets and discuss a tile correlation\nlength, implied by the tile constraints, and its role in choosing an\nappropriate block size to aid POMS in successfully finding grid realizations.",
        "Traditionally, TD and FQI are viewed as differing in the number of updates\ntoward the target value function: TD makes one update, FQI makes an infinite\nnumber, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as\nthe use of a target network in Deep Q-Networks (DQN) in the OPE setting. This\nperspective, however, fails to capture the convergence connections between\nthese algorithms and may lead to incorrect conclusions, for example, that the\nconvergence of TD implies the convergence of FQI. In this paper, we focus on\nlinear value function approximation and offer a new perspective, unifying TD,\nFQI, and PFQI as the same iterative method for solving the Least Squares\nTemporal Difference (LSTD) system, but using different preconditioners and\nmatrix splitting schemes. TD uses a constant preconditioner, FQI employs a\ndata-feature adaptive preconditioner, and PFQI transitions between the two.\nThen, we reveal that in the context of linear function approximation,\nincreasing the number of updates under the same target value function\nessentially represents a transition from using a constant preconditioner to\ndata-feature adaptive preconditioner. This unifying perspective also simplifies\nthe analyses of the convergence conditions for these algorithms and clarifies\nmany issues. Consequently, we fully characterize the convergence of each\nalgorithm without assuming specific properties of the chosen features (e.g.,\nlinear independence). We also examine how common assumptions about feature\nrepresentations affect convergence, and discover new conditions on features\nthat are important for convergence. These convergence conditions allow us to\nestablish the convergence connections between these algorithms and to address\nimportant questions.",
        "In this paper, we give an explicit chain map, which induces the algebra\nisomorphism between the Hochschild cohomology ${\\bf HH}^{\\bullet}(B)$ and the\n$H$-invariant subalgebra ${\\bf H}^{\\bullet}(A, B)^{H}$ under two mild\nhypotheses, where $H$ is a finite dimensional semisimple Hopf algebra and $B$\nis an $H$-Galois extension of $A$. In particular, the smash product $B=A\\#H$\nalways satisfies the mild hypotheses. The isomorphism between ${\\bf\nHH}^{\\bullet}(A\\#H)$ and ${\\bf H}^{\\bullet}(A, A\\#H)^{H}$ generalizes the\nclassical result of group actions. As an application, Hochschild cohomology and\ncup product of the smash product of the quantum $(-1)$-plane and Kac--Paljutkin\nHopf algebra are computed.",
        "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries.",
        "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.",
        "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities.",
        "The Multi-objective Shortest Path (MOSP) problem is a classic network\noptimization problem that aims to find all Pareto-optimal paths between two\npoints in a graph with multiple edge costs. Recent studies on multi-objective\nsearch with A* (MOA*) have demonstrated superior performance in solving\ndifficult MOSP instances. This paper presents a novel search framework that\nallows efficient parallelization of MOA* with different objective orders. The\nframework incorporates a unique upper bounding strategy that helps the search\nreduce the problem's dimensionality to one in certain cases. Experimental\nresults demonstrate that the proposed framework can enhance the performance of\nrecent A*-based solutions, with the speed-up proportional to the problem\ndimension.",
        "The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.",
        "This thesis investigates three biologically inspired operations:\nprefix-suffix duplication, bounded prefix-suffix duplication, and\nprefix-suffix-square completion. Duplication, a common genetic mutation,\ninvolves repeating DNA sequences and is modeled here as formal operations on\nwords. The prefix-suffix duplication generates non-context-free languages, even\nfrom simple initial words. To better reflect biological processes, we propose a\nbounded variant that limits duplication length, resolving unsolved problems and\naligning with biochemical realities.\n  We also introduce the prefix-suffix-square completion operation, which\ngenerates squares at sequence ends. This operation enables the generation of\ninfinite words such as Fibonacci, Period-doubling, and Thue-Morse, which\ncontain squares but avoid higher exponent repetitions, highlighting unique\nstructural properties. In contrast, prefix-suffix duplication cannot generate\ncertain infinite words, such as Thue-Morse, but can produce cube-free words.\n  Additionally, we address the detection of gapped repeats and\npalindromes-structures important in DNA and RNA analysis. These involve\nrepeating or reversed factors flanking a central gap. Previous studies imposed\nconstraints on gap length or arm-gap relationships; we extend this by solving\nthe problem in three novel settings. This work advances theoretical insights\ninto biologically inspired operations and their computational applications in\ngenetic modeling.",
        "We investigate a family of approximate multi-step proximal point methods,\nframed as implicit linear discretizations of gradient flow. The resulting\nmethods are multi-step proximal point methods, with similar computational cost\nin each update as the proximal point method. We explore several optimization\nmethods where applying an approximate multistep proximal points method results\nin improved convergence behavior. We also include convergence analysis for the\nproposed method in several problem settings: quadratic problems, general\nproblems that are strongly or weakly (non)convex, and accelerated results for\nalternating projections.",
        "We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.",
        "The perception system is a a critical role of an autonomous driving system\nfor ensuring safety. The driving scene perception system fundamentally\nrepresents an object detection task that requires achieving a balance between\naccuracy and processing speed. Many contemporary methods focus on improving\ndetection accuracy but often overlook the importance of real-time detection\ncapabilities when computational resources are limited. Thus, it is vital to\ninvestigate efficient object detection strategies for driving scenes. This\npaper introduces Fast-COS, a novel single-stage object detection framework\ncrafted specifically for driving scene applications. The research initiates\nwith an analysis of the backbone, considering both macro and micro\narchitectural designs, yielding the Reparameterized Attention Vision\nTransformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise\nConvolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance\ncomputational efficiency and feature extraction. In extensive tests across GPU,\nedge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the\nImageNet-1K dataset, demonstrating significant throughput improvements over\ncomparable backbone models such as ResNet, FastViT, RepViT, and\nEfficientFormer. Additionally, integrating RepMSDW into a feature pyramid\nnetwork forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS\nenhances object detection in driving scenes, attaining an AP50 score of 57.2%\non the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses\nleading models in efficiency, delivering up to 75.9% faster GPU inference and\n1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet.\nThese findings establish Fast-COS as a highly scalable and reliable solution\nsuitable for real-time applications, especially in resource-limited\nenvironments like autonomous driving systems",
        "We present a computation of the one-loop QCD corrections to top-quark pair\nproduction in association with a $W$ boson, including terms up to order\n$\\varepsilon^2$ in dimensional regularization. Providing a first glimpse into\nthe complexity of the corresponding two-loop amplitude, this result is a first\nstep towards a description of this process at next-to-next-to-leading order\n(NNLO) in QCD. We perform a tensor decomposition and express the corresponding\nform factors in terms of a basis of independent special functions with compact\nrational coefficients, providing a structured framework for future\ndevelopments. In addition, we derive an explicit analytic representation of the\nform factors, valid up to order $\\varepsilon^0$, expressed in terms of\nlogarithms and dilogarithms. For the complete set of special functions\nrequired, we obtain a semi-numerical solution based on generalized power series\nexpansion.",
        "This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.",
        "A discrete time crystal (DTC) is the paradigmatic example of a phase of\nmatter that occurs exclusively in systems out of equilibrium. This phenomenon\nis characterized by the spontaneous symmetry breaking of discrete\ntime-translation and provides a rich playground to study a fundamental question\nin statistical physics: what mechanism allows for driven quantum systems to\nexhibit emergent behavior that deviates from their counterparts with\ntime-independent evolution? Unlike equilibrium phases, DTCs exhibit macroscopic\nmanifestations of coherent quantum dynamics, challenging the conventional\nnarrative that thermodynamic behavior universally erases quantum signatures.\nHowever, due to the difficulty of simulating these systems with either\nclassical or quantum computers, previous studies have been limited to a set of\nmodels with Ising-like couplings -- and mostly only in one dimension -- thus\nprecluding our understanding of the existence (or not) of DTCs in models with\ninteractions that closely align with what occurs in nature. In this work, by\ncombining the latest generation of IBM quantum processors with state-of-the-art\ntensor network methods, we are able to demonstrate the existence of a DTC in a\ntwo-dimensional system governed by anisotropic Heisenberg interactions. Our\ncomprehensive analysis reveals a rich phase diagram encompassing spin-glass,\nergodic, and time-crystalline phases, highlighting the tunability of these\nphases through multiple control parameters. Crucially, our results emphasize\nthe interplay of initialization, interaction anisotropy, and driving protocols\nin stabilizing the DTC phase. By extending the study of Floquet matter beyond\nsimplified models, we lay the groundwork for exploring how driven systems\nbridge the gap between quantum coherence and emergent non-equilibrium\nthermodynamics.",
        "Dissipative phase transitions (DPT) are defined by sudden changes in the\nphysical properties of nonequilibrium open quantum systems and they present\ncharacteristics that have no analogue in closed and thermal systems. Several\nmethods to detect and characterize DPT have been suggested in the literature,\nthe most famous of which -- the $\\textit{Liouvillian gap}$ -- can be derived\nfrom a spectral analysis of the Liouvillian super-operator that governs the\ncomplex interplay between coherent and dissipative dynamics. Here, we consider\nthe $\\textit{output current}$, defined as the average total quantum jumps per\nunit time between the open quantum system and the environment. We propose that\noutput current fluctuations, and in particular their dynamical correlations,\ntheir power spectrum, and their characteristic timescale can provide valuable\ninformation about DPT, confirming a dramatic change of behavior at the critical\npoint. We validate our proposal using the dissipative XYZ model and the\nnonlinear driven-dissipative Kerr model, showing good agreement with previous\nestimates of the location of the critical point. Compared to previous\napproaches, our proposal could be already experimentally tested in optical\nsystems, providing a practical method to detect criticality in quantum open\nsystems.",
        "The presence and nature of low-frequency (0.1-10~mHz) Alfv\\'enic waves in the\ncorona has been established over the last decade, with many of these results\ncoming from coronagraphic observations of the infrared Fe XIII line. The\nCryo-NIRSP instrument situated at DKIST has recently begun acquiring science\nquality data of the same Fe XIII line, with at least a factor of 9 improvement\nin spatial resolution, a factor 30 increase in temporal resolution and an\nincrease in signal-to-noise, when compared to the majority of previously\navailable data. Here we present an analysis of 1~s cadence sit-and-stare data\nfrom Cryo-NIRSP, examining the Doppler velocity fluctuations associated with\nthe Fe XIII 1074~nm coronal line. We are able to confirm previous results of\nAlfv\\'enic waves in the corona as well as explore a new frequency regime. The\ndata reveals that the power law behaviour of the Doppler velocity power\nspectrum extends to higher frequencies. This result appears to challenge some\nmodels of photospheric-driven Alfv\\'enic waves that predict a lack of high\nfrequency wave power in the corona due to strong chromospheric damping.\nMoreover, the high-frequency waves do not transport as much energy as their\nlow-frequency counterparts, with less time-averaged energy per frequency\ninterval. We are also able to confirm the incompressible nature of the\nfluctuations with little coherence between the line amplitude and Doppler\nvelocity time-series.",
        "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
        "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.",
        "Safe memory reclamation techniques that utilize per read reservations, such\nas hazard pointers, often cause significant overhead in traversals of linked\nconcurrent data structures. This is primarily due to the need to announce a\nreservation, and fence to enforce appropriate ordering, before each read. In\nread-intensive workloads, this overhead is amplified because, even if\nrelatively little memory reclamation actually occurs, the full overhead of\nreserving records is still incurred while traversing data structures.\n  In this paper, we propose a novel memory reclamation technique by combining\nPOSIX signals and delayed reclamation, introducing a publish-on-ping approach.\nThis method eliminates the need to make reservations globally visible before\nuse. Instead, threads privately track which records they are accessing, and\nshare this information on demand with threads that intend to reclaim memory.\nThe approach can serve as a drop-in replacement for hazard pointers and hazard\neras. Furthermore, the capability to retain reservations during traversals in\ndata structure operations and publish them on demand facilitates the\nconstruction of a variant of hazard pointers (EpochPOP). This variant uses\nepochs to approach the performance of epoch-based reclamation in the common\ncase where threads are not frequently delayed (while retaining the robustness\nof hazard pointers).\n  Our publish-on-ping implementations based on hazard pointers (HP) and hazard\neras, when applied to various data structures, exhibit significant performance\nimprovements. The improvements across various workloads and data structures\nrange from 1.2X to 4X over the original HP, up to 20% compared to a heavily\noptimized HP implementation similar to the one in the Folly open-source\nlibrary, and up to 3X faster than hazard eras. EpochPOP delivers performance\nsimilar to epoch-based reclamation while providing stronger guarantees.",
        "Motivated by recent advances in digital quantum simulation and the overall\nprospective of solving correlated many-electron problems using quantum\nalgorithms, we design a gate-based quantum circuit that emulates the dynamics\nof the Kondo impurity model. We numerically determine the impurity\nmagnetization, entanglement between impurity and fermionic sites and energy as\na function of time (i.e.~circuit depth) for various initial states and find\nuniversal long-time dynamics. We complement the numerical simulations for\nmoderate system size with an asymptotically exact analytical solution that is\neffective in the limit of large system sizes and for starting states\ncorresponding to a filled Fermi sea. This work opens up the perspective of\nstudying the dynamics of electronic quantum many-body states on quantum chips\nof the NISQ era.",
        "In this short note, we address a gap in the proof of Sauvageot's density\nprinciple, which was pointed out in a paper by Nelson-Venkatesh.",
        "Phishing is the most prevalent type of cyber-attack today and is recognized\nas the leading source of data breaches with significant consequences for both\nindividuals and corporations. Web-based phishing attacks are the most frequent\nwith vectors such as social media posts and emails containing links to phishing\nURLs that once clicked on render host systems vulnerable to more sinister\nattacks. Research efforts to detect phishing URLs have involved the use of\nsupervised learning techniques that use large amounts of data to train models\nand have high computational requirements. They also involve analysis of\nfeatures derived from vectors including email contents thus affecting user\nprivacy. Additionally, they suffer from a lack of resilience against evolution\nof threats especially with the advent of generative AI techniques to bypass\nthese systems as with AI-generated phishing URLs. Unsupervised methods such as\nclustering techniques have also been used in phishing detection in the past,\nhowever, they are at times unscalable due to the use of pair-wise comparisons.\nThey also lack high detection rates while detecting phishing campaigns. In this\npaper, we propose an unsupervised learning approach that is not only fast but\nscalable, as it does not involve pair-wise comparisons. It is able to detect\nentire campaigns at a time with a high detection rate while preserving user\nprivacy; this includes the recent surge of campaigns with targeted phishing\nURLs generated by malicious entities using generative AI techniques.",
        "Millions of users prompt large language models (LLMs) for various tasks, but\nhow good are people at prompt engineering? Do users actually get closer to\ntheir desired outcome over multiple iterations of their prompts? These\nquestions are crucial when no gold-standard labels are available to measure\nprogress. This paper investigates a scenario in LLM-powered data labeling,\n\"prompting in the dark,\" where users iteratively prompt LLMs to label data\nwithout using manually-labeled benchmarks. We developed PromptingSheet, a\nGoogle Sheets add-on that enables users to compose, revise, and iteratively\nlabel data through spreadsheets. Through a study with 20 participants, we found\nthat prompting in the dark was highly unreliable-only 9 participants improved\nlabeling accuracy after four or more iterations. Automated prompt optimization\ntools like DSPy also struggled when few gold labels were available. Our\nfindings highlight the importance of gold labels and the needs, as well as the\nrisks, of automated support in human prompt engineering, providing insights for\nfuture tool design.",
        "Transformer-based Large Language Models (LLMs) have revolutionized Natural\nLanguage Processing by demonstrating exceptional performance across diverse\ntasks. This study investigates the impact of the parameter initialization scale\non the training behavior and task preferences of LLMs. We discover that smaller\ninitialization scales encourage models to favor reasoning tasks, whereas larger\ninitialization scales lead to a preference for memorization tasks. We validate\nthis reasoning bias via real datasets and meticulously designed anchor\nfunctions. Further analysis of initial training dynamics suggests that specific\nmodel components, particularly the embedding space and self-attention\nmechanisms, play pivotal roles in shaping these learning biases. We provide a\ntheoretical framework from the perspective of model training dynamics to\nexplain these phenomena. Additionally, experiments on real-world language tasks\ncorroborate our theoretical insights. This work enhances our understanding of\nhow initialization strategies influence LLM performance on reasoning tasks and\noffers valuable guidelines for training models.",
        "We demonstrate that a disordered magnetic Weyl semimetal may be mapped onto a\ntwo-dimensional array of coupled replicated Hubbard chains, where the Hubbard\n$U$ is directly related to the variance of the disorder potential. This is a\nthree-dimensional generalization of a similar mapping of the two-dimensional\nquantum Hall plateau transition to a one-dimensional Hubbard chain. We\ndemonstrate that this mapping leads to the conclusion that the Weyl semimetal\nbecomes a diffusive metal with a nonzero density of states at arbitrarily weak\ndisorder, in agreement with recent work. We also discuss the absence of\nlocalization in strongly disordered Weyl semimetals from the viewpoint of this\nmapping.",
        "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.",
        "We enhance an isogeny graph of elliptic curves by incorporating level\nstructures defined by bases of the kernels of iterates of the Verschiebung map.\nWe extend several previous results on isogeny graphs with level structures\ndefined by geometric points to these graphs. Firstly, we prove that these\ngraphs form $\\mathbb{Z}_p$-towers of graph coverings as the power of the\nVerschiebung map varies. Secondly, we prove that the connected components of\nthese graphs display a volcanic structure."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Shannon capacity of signal transduction for multiple independent receptors",
    "start_abstract":"Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Vacuum Polarization, Geodesic Equation and Sachs-Wolfe Effect",
        "Production of doubly heavy baryon at the Muon-Ion Collider",
        "Mangnon-Phonon-Photon Interactions in Cubic Ferromagnets in the Vicinity\n  of Orientational Phase Transition: Retrospective and Phenomenological Study",
        "Arithmetic properties of Cantor sets involving non-diagonal forms",
        "Fully comprehensive diagnostic of galaxy activity using principal\n  components of visible spectra: implementation on nearby S0s",
        "A Discontinuous Galerkin Method for H(curl)-Elliptic Hemivariational\n  Inequalities",
        "Ca II K Polar Network Index of the Sun: A Proxy for Historical Polar\n  Magnetic Field",
        "Strong law of large numbers for random walks in weakly dependent random\n  scenery",
        "Using Co-Located Range and Doppler Radars for Initial Orbit\n  Determination",
        "Polymer-based solid-state electrolytes for lithium sulfur batteries",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Euclid preparation: Extracting physical parameters from galaxies with\n  machine learning",
        "Information Consistent Pruning: How to Efficiently Search for Sparse\n  Networks?",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "The SARAO MeerKAT Galactic Plane Survey extended source catalogue",
        "A Semi-Parametric Bayesian Spatial Model for Rainfall Events in\n  Geographically Complex Domains",
        "Geodesic vortex detection on curved surfaces: Analyzing the 2002 austral\n  stratospheric polar vortex warming event",
        "$(2,4)$-Colorability of Planar Graphs Excluding $3$-, $4$-, and\n  $6$-Cycles",
        "The effect of self-induced Marangoni flow on polar-nematic waves in\n  active-matter systems",
        "Radiation of a particle performing helical motion in a multilayer\n  cylindrical waveguide",
        "Algorithms for min-buying in networks",
        "Interacting free boundaries in obstacle problems",
        "Rashba effect modulation in two-dimensional A2B2Te6 (A = Sb, Bi; B = Si,\n  Ge) materials via charge transfer",
        "Non-Markovianity increases transition path probability",
        "A state sum for four-dimensional Lorentzian quantum geometry in terms of\n  edge vectors",
        "Optimal control on a brain tumor growth model with lactate metabolism,\n  viscoelastic effects, and tissue damage",
        "Acceleration temperature for quantum fields without event horizon",
        "Understanding the role of autoencoders for stiff dynamical systems using\n  information theory",
        "Comparing $\\mathrm{Add}(M)$ with $\\mathrm{Prod}(M)$"
      ],
      "abstract":[
        "We show that the null geodesic equation for photons is modified in the\npresence of a charged scalar field, with quantum fluctuations acting as an\neffective mass term that changes the null paths to timelike curves. This effect\ncan be interpreted as a vacuum polarization phenomenon in curved spacetime. The\nresulting contribution to the Sachs-Wolfe effect varies with photon frequency,\nleading to frequency-dependent corrections to the cosmic microwave background\n(CMB) blackbody spectrum in the form of a $\\mu$-distortion, as well as\nmodifications to the CMB power spectrum. We estimate these within a standard\ninflationary scenario and find that while the correction to the CMB power\nspectrum is significant when the scalar field is light, the magnitude of the\n$\\mu$-distortion depends strongly on the regularization prescription.",
        "This study forecasts the production of doubly heavy baryons, $\\Xi_{cc}$,\n$\\Xi_{bc}$, and $\\Xi_{bb}$, within the nonrelativistic QCD framework at the\nMuon-Ion Collider (MuIC). It examines two production mechanisms: photon-gluon\nfusion ($\\gamma + g \\to (QQ')[n] +\\bar{Q} +\\bar{Q'}$) and extrinsic heavy quark\nchannels ($\\gamma + Q \\to (QQ')[n] + \\bar{Q'}$), where $Q$ and $Q'$ denote\nheavy quarks ($c$ or $b$) and $(QQ')[n]$ represents a diquark in specific\nspin-color configurations. The diquark fragments into $\\Xi_{QQ'}$ baryons with\nhigh probability. For $\\Xi_{cc}$ and $\\Xi_{bb}$, the relevant configurations\nare $[^1S_0]_{\\textbf{6}}$ (spin-singlet and color-sextuplet) and\n$[^3S_1]_{\\bar{\\textbf{3}}}$ (spin-triplet and color-antitriplet). For\n$\\Xi_{bc}$, the configurations are $[^1S_0]_{\\bar{\\textbf{3}}}$,\n$[^1S_0]_{\\textbf{6}}$, $[^3S_1]_{\\bar{\\textbf{3}}}$, and\n$[^3S_1]_{\\textbf{6}}$. The study compares total and differential\ncross-sections for these channels, highlighting their uncertainties. The\nresults indicate that the extrinsic heavy quark channel, particularly the\n$[^3S_1]_{\\bar{\\textbf{3}}}$ configuration, dominates $\\Xi_{QQ'}$ production,\nthough other diquark states also contribute significantly. Using quark masses\n$m_c = 1.80 \\pm 0.10$ GeV and $m_b = 5.1 \\pm 0.20$ GeV, the study estimates\nannual event yields at MuIC ($\\sqrt{s} = 1$ TeV, luminosity ${\\mathcal L}\\simeq\n40$ ${\\rm fb}^{-1}$) of $(3.67^{+1.29}_{-0.91}) \\times 10^9$ for $\\Xi_{cc}$,\n$(2.24^{+0.28}_{-0.20}) \\times 10^8$ for $\\Xi_{bc}$, and\n$(3.00^{+0.64}_{-0.56}) \\times 10^6$ for $\\Xi_{bb}$. These findings suggest\nthat MuIC will significantly enhance our understanding of doubly heavy baryons.",
        "This work presents a comprehensive theoretical investigation of\nmagnon-phonon-photon interactions in cubic ferromagnets near orientational\nphase transitions (OPT). The study focuses on the interplay of magnetoelastic\n(ME), electromagnetic-spin (EMS), and acousticelectromagnetic (AEM)\ninteractions in ferromagnetic dielectrics and metals. By deriving dispersion\nequations for coupled waves, the research reveals how the dynamic properties of\nmagnetically ordered crystals evolve in response to these interactions under\nvarying external magnetic fields and near OPT points. The ME interaction\nprominently influences spin and elastic wave dynamics, giving rise to coupled\nmodes such as quasimagnon and quasi-acoustic waves. Near the OPT, these\ninteractions become dominant, leading to phenomena like the magnetoelastic gap\nand softening of vibrational modes. The EMS interaction significantly alters\nthe activation energy and dispersion of quasi-spin and quasi-electromagnetic\nwaves. In ferromagnetic metals, helicons (weakly damped electromagnetic waves)\nexhibit strong coupling with spin and elastic waves, particularly in the OPT\nregion. The study identifies conditions for triple resonance among spin,\nelastic, and electromagnetic waves. Additionally, it explores the\nfrequency-dependent rotation of polarization planes in electromagnetic and\nmagnetoelastic waves, which sharpens near OPT points. These results provide a\ndeeper understanding of the coupled dynamics in ferromagnetic materials, paving\nthe way for new technological applications in spintronics, signal processing,\nand advanced magneto-optical devices. The theoretical framework developed here\nemphasizes the critical role of ME, EMS, and AEM interactions in tailoring wave\nproperties for specific applications, particularly in designing next-generation\nmagnetic and electronic systems.",
        "We show conditions on $k$ such that any number $x$ in the interval $[0, k\/2]$\ncan be represented in the form $x_1^{a_1} x_2^{a_2} + x_3^{a_3} x_4^{a_4} +\n\\cdots + x_{k-1}^{a_{k-1}} x_k^{a_k}$, where the exponents $a_{2i-1}$ and\n$a_{2i}$ are positive integers satisfying $a_{2i-1} + a_{2i} = s$ for $i = 1,\n2, \\dots, k\/2$, and each $x_i$ belongs to the generalized Cantor set. Moreover,\nwe discuss different types of non-diagonal polynomials and clarify the optimal\nresults in low-dimensional cases.",
        "We introduce a novel galaxy classification methodology based on the visible\nspectra of a sample of over 68,000 nearby ($z\\leq 0.1$) Sloan Digital Sky\nSurvey lenticular (S0) galaxies. Unlike traditional diagnostic diagrams, which\nrely on a limited set of emission lines and class dividers to identify ionizing\nsources, our approach provides a comprehensive framework for characterizing\ngalaxies regardless of their activity level. By projecting galaxies into the 2D\nlatent space defined by the first three principal components (PCs) of their\nentire visible spectra, our method remains robust even when data from\nindividual emission lines are missing. We employ Gaussian kernel density\nestimates of the classical Baldwin-Phillips-Terlevich (BPT) activity classes in\nthe new classification subspace, adjusted according to their relative abundance\nin our S0 sample, to generate probability maps for star-forming, Seyfert,\ncomposite, and LINER galaxies. These maps closely mirror the canonical\ndistribution of BPT classes shown by the entire galaxy population,\ndemonstrating that our PC-based taxonomy effectively predicts the dominant\nionizing mechanisms through a probabilistic approach that provides a realistic\nreflection of galaxy activity and allows for refined class membership. Our\nanalysis further reveals that flux-limited BPT-like diagrams are inherently\nbiased against composite and star-forming galaxies due to their weaker [OIII]\nemission. Besides, it suggests that although most low-activity galaxies\nexcluded from these diagnostics exhibit visual spectra with LINER-like\ncharacteristics, their remaining activity is likely driven by mechanisms\nunrelated to either star formation or supermassive black hole accretion. A\nmachine-readable catalogue listing BPT-class probabilities for the galaxies\nanalysed is available online at the CDS website.",
        "In this paper, we develop a Discontinuous Galerkin (DG) method for solving\nH(curl)-elliptic hemivariational inequalities. By selecting an appropriate\nnumerical flux, we construct an Interior Penalty Discontinuous Galerkin (IPDG)\nscheme. A comprehensive numerical analysis of the IPDG method is conducted,\naddressing key aspects such as consistency, boundedness, stability, and the\nexistence, uniqueness, uniform boundedness of the numerical solutions. Building\non these properties, we establish a priori error estimates, demonstrating the\noptimal convergence order of the numerical solutions under suitable solution\nregularity assumptions. Finally, a numerical example is presented to illustrate\nthe theoretically predicted convergence order and to show the effectiveness of\nthe proposed method.",
        "The Sun's polar magnetic field is pivotal in understanding solar dynamo\nprocesses and forecasting future solar cycles. However, direct measurements of\nthe polar field is only available since the 1970s. The chromospheric Ca II K\npolar network index (PNI; the fractional area of the chromospheric network\nregions above a certain latitude) has recently emerged as a reliable proxy for\npolar magnetic fields. In this study, we derive PNI estimates from newly\ncalibrated, rotation-corrected Ca II K observations from the Kodaikanal Solar\nObservatory (1904-2007) and modern data from the Rome Precision Solar\nPhotometric Telescope (2000-2022). We use both of those Ca II K archives to\nidentify polar network regions with an automatic adaptive threshold\nsegmentation technique and calculate the PNI. The PNI obtained from both the\narchives shows a significant correlation with the measured polar field from WSO\n(Pearson correlation coefficient r > 0.93) and the derived polar field based on\nan Advective Flux Transport Model (r > 0.91). The PNI series also shows a\nsignificant correlation with faculae counts derived from Mount Wilson\nObservatory observations (r > 0.87) for both KoSO and Rome-PSPT data. Finally,\nwe use the PNI series from both archives to reconstruct the polar magnetic\nfield over a 119-year-long period, which includes last 11 solar cycles (Cycle\n14-24). We also obtain a relationship between the amplitude of solar cycles (in\n13-month smoothed sunspot number) and the strength of the reconstructed polar\nfield at the preceding solar cycle minimum to validate the prediction of the\nongoing solar cycle, Cycle 25.",
        "In this brief note, we study the strong law of large numbers for random walks\nin random scenery. Under the assumptions that the random scenery is\nnon-stationary and satisfies weakly dependent condition with an appropriate\nrate, we establish strong law of large numbers for random walks in random\nscenery. Our results extend the known results in the literature.",
        "With debris larger than 1 cm in size estimated to be over one million,\nprecise cataloging efforts are essential to ensure space operations' safety.\nCompounding this challenge is the oversubscribed problem, where the sheer\nvolume of space objects surpasses ground-based observatories' observational\ncapacity. This results in sparse, brief observations and extended intervals\nbefore image acquisition. LeoLabs' network of phased-array radars addresses\nthis need by reliably tracking 10 cm objects and larger in low Earth orbit with\n10 independent radars across six sites. While LeoLabs tracklets are extremely\nshort, they hold much more information than typical radar observations.\nFurthermore, two tracklets are generally available, separated by a couple of\nminutes. Thus, this paper develops a tailored approach to initialize state and\nuncertainty from a single or pair of tracklets. Through differential algebra,\nthe initial orbit determination provides the state space compatible with the\navailable measurements, namely an orbit set. This practice, widely used in\nprevious research, allows for efficient data association of different\ntracklets, thus enabling the addition of accurate tracks to the catalog\nfollowing their independent initialization. The algorithm's efficacy is tested\nusing real measurements, evaluating the IOD solution's accuracy and ability to\npredict the next passage from a single or a pair of tracklets.",
        "Lithium-sulfur (Li-S) batteries offer substantial theoretical energy density\ngains over Li-ion bat-teries, a crucial factor for transportation\nelectrification. In addition, sulfur is an earth-abundant, inexpensive material\nobtainable from multiple resources; thus, Li-S batteries are envisioned to\nprovide environmentally sustainable solutions to the growing demand for energy\nstorage. A crit-ical roadblock to the realization of commercial Li-S batteries\nis the formation of polysulfides and their secondary reactions with liquid\norganic electrolytes, resulting in low coulombic efficiency for charging and\nfast self-discharge rates. The realization of solid-state electrolytes for Li-S\nbat-teries provides potential pathways to address the safety concerns of liquid\nelectrolytes and inhib-it the formation of polysulfides and\/or prevent their\ndiffusion into the anode electrode. However, current solid-state electrolytes\nare limited by low ionic conductivity, inadequate electrode inter-facial\ncompatibility, and restricted electrochemical windows. This review discusses\nthe status of polymer-based electrolytes for Li-S batteries, and outlines\ncurrent methods for their fabrication, their transport characteristics and\nongoing research aimed at overcoming material properties hindering the\ndevelopment of all-solid-state Li-S batteries.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "The Euclid mission is generating a vast amount of imaging data in four\nbroadband filters at high angular resolution. This will allow the detailed\nstudy of mass, metallicity, and stellar populations across galaxies, which will\nconstrain their formation and evolutionary pathways. Transforming the Euclid\nimaging for large samples of galaxies into maps of physical parameters in an\nefficient and reliable manner is an outstanding challenge. We investigate the\npower and reliability of machine learning techniques to extract the\ndistribution of physical parameters within well-resolved galaxies. We focus on\nestimating stellar mass surface density, mass-averaged stellar metallicity and\nage. We generate noise-free, synthetic high-resolution imaging data in the\nEuclid photometric bands for a set of 1154 galaxies from the TNG50 cosmological\nsimulation. The images are generated with the SKIRT radiative transfer code,\ntaking into account the complex 3D distribution of stellar populations and\ninterstellar dust attenuation. We use a machine learning framework to map the\nidealised mock observational data to the physical parameters on a\npixel-by-pixel basis. We find that stellar mass surface density can be\naccurately recovered with a $\\leq 0.130 {\\rm \\,dex}$ scatter. Conversely,\nstellar metallicity and age estimates are, as expected, less robust, but still\ncontain significant information which originates from underlying correlations\nat a sub-kpc scale between stellar mass surface density and stellar population\nproperties.",
        "Iterative magnitude pruning methods (IMPs), proven to be successful in\nreducing the number of insignificant nodes in over-parameterized deep neural\nnetworks (DNNs), have been getting an enormous amount of attention with the\nrapid deployment of DNNs into cutting-edge technologies with computation and\nmemory constraints. Despite IMPs popularity in pruning networks, a fundamental\nlimitation of existing IMP algorithms is the significant training time required\nfor each pruning iteration. Our paper introduces a novel \\textit{stopping\ncriterion} for IMPs that monitors information and gradient flows between\nnetworks layers and minimizes the training time. Information Consistent Pruning\n(\\ourmethod{}) eliminates the need to retrain the network to its original\nperformance during intermediate steps while maintaining overall performance at\nthe end of the pruning process. Through our experiments, we demonstrate that\nour algorithm is more efficient than current IMPs across multiple dataset-DNN\ncombinations. We also provide theoretical insights into the core idea of our\nalgorithm alongside mathematical explanations of flow-based IMP. Our code is\navailable at \\url{https:\/\/github.com\/Sekeh-Lab\/InfCoP}.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "We present a catalogue of extended radio sources from the SARAO MeerKAT\nGalactic Plane Survey (SMGPS). Compiled from 56 survey tiles and covering\napproximately 500 deg$^2$ across the first, third, and fourth Galactic\nquadrants, the catalogue includes 16534 extended and diffuse sources with areas\nlarger than 5 synthesised beams. Of them, 3891 (24\\% of the total) are\nconfidently associated with known Galactic radio-emitting objects in the\nliterature, such as HII regions, supernova remnants, planetary nebulae,\nluminous blue variables, and Wolf-Rayet stars. A significant fraction of the\nremaining sources, 5462 (33\\%), are candidate extragalactic sources, while 7181\n(43\\%) remain unclassified. Isolated radio filaments are excluded from the\ncatalogue. The diversity of extended sources underscores MeerKAT's contribution\nto the completeness of censuses of Galactic radio emitters, and its potential\nfor new scientific discoveries. For the catalogued sources, we derived basic\npositional and morphological parameters, as well as flux density estimates,\nusing standard aperture photometry. This paper describes the methods followed\nto generate the catalogue from the original SMGPS tiles, detailing the source\nextraction, characterisation, and crossmatching procedures. Additionally, we\nanalyse the statistical properties of the catalogued populations",
        "Environmental phenomena are influenced by complex interactions among various\nfactors. For instance, the amount of rainfall measured at different stations\nwithin a given area is shaped by atmospheric conditions, orography, and physics\nof water processes. Motivated by the need to analyze rainfall across complex\nspatial locations, we propose a flexible Bayesian semi-parametric model for\nspatially distributed data. This method effectively accounts for spatial\ncorrelation while incorporating dependencies on geographical characteristics in\na highly flexible manner. Indeed, using latent Gaussian processes, indexed by\nspatial coordinates and topographical features, the model integrates spatial\ndependencies and environmental characteristics within a nonparametric\nframework. Posterior inference is conducted using an efficient rejection-free\nMarkov Chain Monte Carlo algorithm, which eliminates the need for tuning\nparameter calibration, ensuring smoother and more reliable estimation. The\nmodel's flexibility is evaluated through a series of simulation studies,\ninvolving different rainfall and spatial correlation scenarios, to demonstrate\nits robustness across various conditions. We then apply the model to a large\ndataset of rainfall events collected from the Italian regions of Veneto and\nTrentino-Alto Adige, these areas are known for their complex orography and\ndiverse meteorological drivers. By analyzing this data, we generate detailed\nmaps that illustrate the mean and variance of rainfall and rainy days. The\nmethod is implemented in a new R package available on GitHub.",
        "Geodesic vortex detection on curved surfaces: Analyzing the 2002 austral\nstratospheric polar vortex warming event",
        "A defective $k$-coloring is a coloring on the vertices of a graph using\ncolors $1,2, \\dots, k$ such that adjacent vertices may share the same color. A\n$(d_1,d_2)$-\\emph{coloring} of a graph $G$ is a defective $2$-coloring of $G$\nsuch that any vertex colored by color $i$ has at most $d_i$ adjacent vertices\nof the same color, where $i\\in\\{1,2\\}$. A graph $G$ is said to be\n$(d_1,d_2)$-\\emph{colorable} if it admits a $(d_1,d_2)$-coloring.\n  Defective $2$-coloring in planar graphs without $3$-cycles, $4$-cycles, and\n$6$-cycles has been investigated by Dross and Ochem, as well as Sittitrai and\nPimpasalee. They showed that such graphs are $(0,6)$-colorable and\n$(3,3)$-colorable, respectively. In this paper, we proved that these graphs are\nalso $(2,4)$-colorable.",
        "We study the formation of propagating large-scale density waves of mixed\npolar-nematic symmetry in a colony of self-propelled agents that are bound to\nmove along the planar surface of a thin viscous film. The agents act as an\ninsoluble surfactant, i.e. the surface tension of the liquid depends on their\ndensity. Therefore, density gradients generate a Marangoni flow. We demonstrate\nthat for active matter in the form of self-propelled surfactants with local\n(nematic) aligning interactions such a Marangoni flow nontrivially influences\nthe propagation of the density waves. Upon gradually increasing the Marangoni\nparameter, which characterises the relative strength of the Marangoni flow as\ncompared to the self-propulsion speed, the density waves broaden while their\nspeed may either increase or decrease depending on wavelength and overall mean\ndensity. A further increase of the Marangoni parameter eventually results in\nthe disappearance of the density waves. This may occur either discontinuously\nat finite wave amplitude via a saddle-node bifurcation or continuously with\nvanishing wave amplitude at a wave bifurcation, i.e. a finite-wavelength Hopf\nbifurcation.",
        "- An algorithm for calculating the radiation field of a charged point\nparticle performing a spiral motion in an infinite cylindrical waveguide with a\nmultilayer side wall is found. The number of layers and their filling is\narbitrary. The axis of the spiral is aligned with the axis of the waveguide, so\nthat the geometry of the problem has cylindrical symmetry. Explicit expressions\nfor modal frequency distributions and equations for resonant frequencies for\nsingle-layer and double-layer waveguides are given. Examples of graphical\nconstructions of modal frequency distributions of modes for single-layer\n(resistive), double-layer (metal-dielectric) and triple-layer (metal-dielectric\nwith internal NEG coating) waveguides are presented.",
        "The paper is motivated by pricing decisions faced by forecourt fuel retailers\nacross their outlets on a road network. Through our modelling approach we are\nable adapt the network structure to a bipartite graph with demand nodes\nrepresenting volumes of fuel from customers using a specific route that\nconnects to the seller's outlet nodes that intersect that route on the network.\nCustomers may have their demand satisfied at the lowest priced competitor on\ntheir route. However, the seller can satisfy some or all of this demand by\nmatching or beating this price via one of their outlets intersecting the route.\nWe give a practical extension to min-pricing by considering a binary logit\nvariant for buyers evaluating the choice between two sellers. We derive two MIP\nformulations for min-buying in the case of general demand. We also propose\nseveral constructive heuristics, based on insertion and selection operations,\nsuitable for problem instances beyond the scope of the exact methods. The\nperformance of models and algorithms are evaluated in a numerical study and\ndevelop insights from the results. Importantly, we are able to highlight the\nvalue of price-matching decisions under buyer demand sensitivity.",
        "We study obstacle problems governed by two distinct types of diffusion\noperators involving interacting free boundaries. We obtain a somewhat\nsurprising coupling property, leading to a comprehensive analysis of the free\nboundary. More precisely, we show that near regular points of a coordinate\nfunction, the free boundary is analytic, whereas singular points lie on a\nsmooth manifold. Additionally, we prove that uncoupled free boundary points are\nsingular, indicating that regular points lie exclusively on the coupled free\nboundary. Furthermore, optimal regularity, non-degeneracy, and lower\ndimensional Hausdorff measure estimates are obtained. Explicit examples\nillustrate the sharpness of assumptions.",
        "Designing two-dimensional (2D) Rashba semiconductors, exploring the\nunderlying mechanism of Rashba effect, and further proposing efficient and\ncontrollable approaches are crucial for the development of spintronics. On the\nbasis of first-principles calculations, we here theoretically design all\npossible types (common, inverse, and composite) of Janus structures and\nsuccessfully achieve numerous ideal 2D Rashba semiconductors from a series of\nfive atomic-layer A2B2Te6 (A = Sb, Bi; B = Si, Ge) materials. Considering the\ndifferent Rashba constant {\\alpha}R and its modulation trend under external\nelectric field, we comprehensively analyze the intrinsic electric field Ein in\nterms of work function, electrostatic potential, dipole moment, and inner\ncharge transfer. Inspired by the quantitative relationship between charge\ntransfer and the strength of Ein and even the {\\alpha}R, we propose a\nstraightforward strategy of introducing a single adatom onto the surface of 2D\nmonolayer to introduce and modulate the Rashba effect. Lastly, we also examine\nthe growth feasibility and electronic structures of the Janus Sb2Ge2Se3Te3\nsystem and Janus-adsorbed systems on a 2D BN substrate. Our work not only\nconducts a detailed analysis of A2B2Te6-based Rashba systems, but also proposes\na new strategy for efficiently and controllably modulating the {\\alpha}R\nthrough the reconfiguration of charge transfer.",
        "Defining low-dimensional reaction coordinates is crucial for analyzing the\ndynamics of complex systems and for comparison with experiments. The maximal\nvalue of the transition-path probability along the reaction coordinate $x$,\n$p(\\mathrm{TP}|x)$, is a common estimator for reaction-coordinate quality by\ncomparing to the theoretical maximal value of 1\/2 in the overdamped Markovian\nlimit. We show by analytical arguments and simulations that for non-Markovian\ndynamics $p(\\mathrm{TP}|x)$ is non-monotonic as a function of the memory time\nand exceeds 1\/2 for long memory time. This disqualifies $p(\\mathrm{TP}|x)$ as a\ncriterion for reaction coordinate quality.",
        "We present the construction of a new state sum model for $4d$ Lorentzian\nquantum gravity based on the description of quantum simplicial geometry in\nterms of edge vectors. Quantum states and amplitudes for simplicial geometry\nare built from irreducible representations of the translation group, then\nrelated to the representations of the Lorentz group via expansors, leading to\ninteresting (and intricate) non-commutative structures. We also show how the\nnew model connects to the Lorentzian Barrett-Crane spin foam model, formulated\nin terms of quantized triangle bivectors.",
        "In this paper, we study an optimal control problem for a brain tumor growth\nmodel that incorporates lactate metabolism, viscoelastic effects, and tissue\ndamage. The PDE system, introduced in [G. Cavalleri, P. Colli, A. Miranville,\nE. Rocca, On a Brain Tumor Growth Model with Lactate Metabolism, Viscoelastic\nEffects, and Tissue Damage (2025)], couples a Fisher-Kolmogorov type equation\nfor tumor cell density with a reaction-diffusion equation for the lactate, a\nquasi-static force balance governing the displacement, and a nonlinear\ndifferential inclusion for tissue damage. The control variables, representing\nchemotherapy and a lactate-targeting drug, influence tumor progression and\ntreatment response. Starting from well-posedness, regularity, and continuous\ndependence results already established, we define a suitable cost functional\nand prove the existence of optimal controls. Then, we analyze the\ndifferentiability of the control-to-state operator and establish a necessary\nfirst-order condition for treatment optimality.",
        "We investigate unitarily inequivalent representations of the algebra of\noperators in quantum field theory. In those cases that exist a Fock\nrepresentation of the commutation relations, we have a unique no-quanta state.\nWe examine more closely the operational definition of a measuring device which\ndetect a quantum of an Hermitian scalar field. The Unruh-DeWitt and Glauber\nmodel of quanta detectors are discussed. In Minkowski spacetime, the transition\nprobability per unit proper time of both detectors is studied in two different\nnon-inertial frames of reference. The first one, a detector with a constant\nproper acceleration, travelling in a stationary worldline, i.e., an hyperbolic\nmotion, interacting with the field prepared in the Poincar\\'e invariant Fock\nvacuum state. Next, we studied the Unruh-DeWitt detector at rest in a\nnon-uniformly accelerated frame, with a time dependent acceleration. We\nevaluate the positive frequency Wightman function for the non-uniformly\naccelerated frame in a finite time interval and obtain the same two-point\ncorrelation function of a system in equilibrium with a thermal bath. Therefore\nthe non-uniformly accelerated Unruh-DeWitt detector interacting with the field\nin the Poincar\\'e invariant Fock vacuum state measures a thermal state. This\nresult shows that the existence of an acceleration temperature without the\npresence of an event horizon.",
        "Using the information theory, this study provides insights into how the\nconstruction of latent space of autoencoder (AE) using deep neural network\n(DNN) training finds a smooth low-dimensional manifold in the stiff dynamical\nsystem. Our recent study [1] reported that an autoencoder (AE) combined with\nneural ODE (NODE) as a surrogate reduced order model (ROM) for the integration\nof stiff chemically reacting systems led to a significant reduction in the\ntemporal stiffness, and the behavior was attributed to the identification of a\nslow invariant manifold by the nonlinear projection of the AE. The present work\noffers fundamental understanding of the mechanism by employing concepts from\ninformation theory and better mixing. The learning mechanism of both the\nencoder and decoder are explained by plotting the evolution of mutual\ninformation and identifying two different phases. Subsequently, the density\ndistribution is plotted for the physical and latent variables, which shows the\ntransformation of the \\emph{rare event} in the physical space to a \\emph{highly\nlikely} (more probable) event in the latent space provided by the nonlinear\nautoencoder. Finally, the nonlinear transformation leading to density\nredistribution is explained using concepts from information theory and\nprobability.",
        "We present characterizations for the inclusions $\\mathrm{Add}(M)\\subseteq\n\\mathrm{Prod}(M)$ and $\\mathrm{Prod}(M)\\subseteq \\mathrm{Add}(M)$ in locally\nfinitely presented categories and in compactly generated triangulated\ncategories. As applications, we describe the situations when the classes of the\nform $\\mathrm{Prod}(M)$ and $\\mathrm{Add}(M)$ are (pre)covering, respectively\n(pre)enveloping."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT",
    "start_abstract":"Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Vacuum Polarization, Geodesic Equation and Sachs-Wolfe Effect",
        "Production of doubly heavy baryon at the Muon-Ion Collider",
        "Mangnon-Phonon-Photon Interactions in Cubic Ferromagnets in the Vicinity\n  of Orientational Phase Transition: Retrospective and Phenomenological Study",
        "Arithmetic properties of Cantor sets involving non-diagonal forms",
        "Fully comprehensive diagnostic of galaxy activity using principal\n  components of visible spectra: implementation on nearby S0s",
        "A Discontinuous Galerkin Method for H(curl)-Elliptic Hemivariational\n  Inequalities",
        "Ca II K Polar Network Index of the Sun: A Proxy for Historical Polar\n  Magnetic Field",
        "Strong law of large numbers for random walks in weakly dependent random\n  scenery",
        "Using Co-Located Range and Doppler Radars for Initial Orbit\n  Determination",
        "Polymer-based solid-state electrolytes for lithium sulfur batteries",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Euclid preparation: Extracting physical parameters from galaxies with\n  machine learning",
        "Information Consistent Pruning: How to Efficiently Search for Sparse\n  Networks?",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "The SARAO MeerKAT Galactic Plane Survey extended source catalogue",
        "A Semi-Parametric Bayesian Spatial Model for Rainfall Events in\n  Geographically Complex Domains",
        "Geodesic vortex detection on curved surfaces: Analyzing the 2002 austral\n  stratospheric polar vortex warming event",
        "$(2,4)$-Colorability of Planar Graphs Excluding $3$-, $4$-, and\n  $6$-Cycles",
        "The effect of self-induced Marangoni flow on polar-nematic waves in\n  active-matter systems",
        "Radiation of a particle performing helical motion in a multilayer\n  cylindrical waveguide",
        "Algorithms for min-buying in networks",
        "Interacting free boundaries in obstacle problems",
        "Rashba effect modulation in two-dimensional A2B2Te6 (A = Sb, Bi; B = Si,\n  Ge) materials via charge transfer",
        "Non-Markovianity increases transition path probability",
        "A state sum for four-dimensional Lorentzian quantum geometry in terms of\n  edge vectors",
        "Optimal control on a brain tumor growth model with lactate metabolism,\n  viscoelastic effects, and tissue damage",
        "Acceleration temperature for quantum fields without event horizon",
        "Understanding the role of autoencoders for stiff dynamical systems using\n  information theory",
        "Comparing $\\mathrm{Add}(M)$ with $\\mathrm{Prod}(M)$"
      ],
      "abstract":[
        "We show that the null geodesic equation for photons is modified in the\npresence of a charged scalar field, with quantum fluctuations acting as an\neffective mass term that changes the null paths to timelike curves. This effect\ncan be interpreted as a vacuum polarization phenomenon in curved spacetime. The\nresulting contribution to the Sachs-Wolfe effect varies with photon frequency,\nleading to frequency-dependent corrections to the cosmic microwave background\n(CMB) blackbody spectrum in the form of a $\\mu$-distortion, as well as\nmodifications to the CMB power spectrum. We estimate these within a standard\ninflationary scenario and find that while the correction to the CMB power\nspectrum is significant when the scalar field is light, the magnitude of the\n$\\mu$-distortion depends strongly on the regularization prescription.",
        "This study forecasts the production of doubly heavy baryons, $\\Xi_{cc}$,\n$\\Xi_{bc}$, and $\\Xi_{bb}$, within the nonrelativistic QCD framework at the\nMuon-Ion Collider (MuIC). It examines two production mechanisms: photon-gluon\nfusion ($\\gamma + g \\to (QQ')[n] +\\bar{Q} +\\bar{Q'}$) and extrinsic heavy quark\nchannels ($\\gamma + Q \\to (QQ')[n] + \\bar{Q'}$), where $Q$ and $Q'$ denote\nheavy quarks ($c$ or $b$) and $(QQ')[n]$ represents a diquark in specific\nspin-color configurations. The diquark fragments into $\\Xi_{QQ'}$ baryons with\nhigh probability. For $\\Xi_{cc}$ and $\\Xi_{bb}$, the relevant configurations\nare $[^1S_0]_{\\textbf{6}}$ (spin-singlet and color-sextuplet) and\n$[^3S_1]_{\\bar{\\textbf{3}}}$ (spin-triplet and color-antitriplet). For\n$\\Xi_{bc}$, the configurations are $[^1S_0]_{\\bar{\\textbf{3}}}$,\n$[^1S_0]_{\\textbf{6}}$, $[^3S_1]_{\\bar{\\textbf{3}}}$, and\n$[^3S_1]_{\\textbf{6}}$. The study compares total and differential\ncross-sections for these channels, highlighting their uncertainties. The\nresults indicate that the extrinsic heavy quark channel, particularly the\n$[^3S_1]_{\\bar{\\textbf{3}}}$ configuration, dominates $\\Xi_{QQ'}$ production,\nthough other diquark states also contribute significantly. Using quark masses\n$m_c = 1.80 \\pm 0.10$ GeV and $m_b = 5.1 \\pm 0.20$ GeV, the study estimates\nannual event yields at MuIC ($\\sqrt{s} = 1$ TeV, luminosity ${\\mathcal L}\\simeq\n40$ ${\\rm fb}^{-1}$) of $(3.67^{+1.29}_{-0.91}) \\times 10^9$ for $\\Xi_{cc}$,\n$(2.24^{+0.28}_{-0.20}) \\times 10^8$ for $\\Xi_{bc}$, and\n$(3.00^{+0.64}_{-0.56}) \\times 10^6$ for $\\Xi_{bb}$. These findings suggest\nthat MuIC will significantly enhance our understanding of doubly heavy baryons.",
        "This work presents a comprehensive theoretical investigation of\nmagnon-phonon-photon interactions in cubic ferromagnets near orientational\nphase transitions (OPT). The study focuses on the interplay of magnetoelastic\n(ME), electromagnetic-spin (EMS), and acousticelectromagnetic (AEM)\ninteractions in ferromagnetic dielectrics and metals. By deriving dispersion\nequations for coupled waves, the research reveals how the dynamic properties of\nmagnetically ordered crystals evolve in response to these interactions under\nvarying external magnetic fields and near OPT points. The ME interaction\nprominently influences spin and elastic wave dynamics, giving rise to coupled\nmodes such as quasimagnon and quasi-acoustic waves. Near the OPT, these\ninteractions become dominant, leading to phenomena like the magnetoelastic gap\nand softening of vibrational modes. The EMS interaction significantly alters\nthe activation energy and dispersion of quasi-spin and quasi-electromagnetic\nwaves. In ferromagnetic metals, helicons (weakly damped electromagnetic waves)\nexhibit strong coupling with spin and elastic waves, particularly in the OPT\nregion. The study identifies conditions for triple resonance among spin,\nelastic, and electromagnetic waves. Additionally, it explores the\nfrequency-dependent rotation of polarization planes in electromagnetic and\nmagnetoelastic waves, which sharpens near OPT points. These results provide a\ndeeper understanding of the coupled dynamics in ferromagnetic materials, paving\nthe way for new technological applications in spintronics, signal processing,\nand advanced magneto-optical devices. The theoretical framework developed here\nemphasizes the critical role of ME, EMS, and AEM interactions in tailoring wave\nproperties for specific applications, particularly in designing next-generation\nmagnetic and electronic systems.",
        "We show conditions on $k$ such that any number $x$ in the interval $[0, k\/2]$\ncan be represented in the form $x_1^{a_1} x_2^{a_2} + x_3^{a_3} x_4^{a_4} +\n\\cdots + x_{k-1}^{a_{k-1}} x_k^{a_k}$, where the exponents $a_{2i-1}$ and\n$a_{2i}$ are positive integers satisfying $a_{2i-1} + a_{2i} = s$ for $i = 1,\n2, \\dots, k\/2$, and each $x_i$ belongs to the generalized Cantor set. Moreover,\nwe discuss different types of non-diagonal polynomials and clarify the optimal\nresults in low-dimensional cases.",
        "We introduce a novel galaxy classification methodology based on the visible\nspectra of a sample of over 68,000 nearby ($z\\leq 0.1$) Sloan Digital Sky\nSurvey lenticular (S0) galaxies. Unlike traditional diagnostic diagrams, which\nrely on a limited set of emission lines and class dividers to identify ionizing\nsources, our approach provides a comprehensive framework for characterizing\ngalaxies regardless of their activity level. By projecting galaxies into the 2D\nlatent space defined by the first three principal components (PCs) of their\nentire visible spectra, our method remains robust even when data from\nindividual emission lines are missing. We employ Gaussian kernel density\nestimates of the classical Baldwin-Phillips-Terlevich (BPT) activity classes in\nthe new classification subspace, adjusted according to their relative abundance\nin our S0 sample, to generate probability maps for star-forming, Seyfert,\ncomposite, and LINER galaxies. These maps closely mirror the canonical\ndistribution of BPT classes shown by the entire galaxy population,\ndemonstrating that our PC-based taxonomy effectively predicts the dominant\nionizing mechanisms through a probabilistic approach that provides a realistic\nreflection of galaxy activity and allows for refined class membership. Our\nanalysis further reveals that flux-limited BPT-like diagrams are inherently\nbiased against composite and star-forming galaxies due to their weaker [OIII]\nemission. Besides, it suggests that although most low-activity galaxies\nexcluded from these diagnostics exhibit visual spectra with LINER-like\ncharacteristics, their remaining activity is likely driven by mechanisms\nunrelated to either star formation or supermassive black hole accretion. A\nmachine-readable catalogue listing BPT-class probabilities for the galaxies\nanalysed is available online at the CDS website.",
        "In this paper, we develop a Discontinuous Galerkin (DG) method for solving\nH(curl)-elliptic hemivariational inequalities. By selecting an appropriate\nnumerical flux, we construct an Interior Penalty Discontinuous Galerkin (IPDG)\nscheme. A comprehensive numerical analysis of the IPDG method is conducted,\naddressing key aspects such as consistency, boundedness, stability, and the\nexistence, uniqueness, uniform boundedness of the numerical solutions. Building\non these properties, we establish a priori error estimates, demonstrating the\noptimal convergence order of the numerical solutions under suitable solution\nregularity assumptions. Finally, a numerical example is presented to illustrate\nthe theoretically predicted convergence order and to show the effectiveness of\nthe proposed method.",
        "The Sun's polar magnetic field is pivotal in understanding solar dynamo\nprocesses and forecasting future solar cycles. However, direct measurements of\nthe polar field is only available since the 1970s. The chromospheric Ca II K\npolar network index (PNI; the fractional area of the chromospheric network\nregions above a certain latitude) has recently emerged as a reliable proxy for\npolar magnetic fields. In this study, we derive PNI estimates from newly\ncalibrated, rotation-corrected Ca II K observations from the Kodaikanal Solar\nObservatory (1904-2007) and modern data from the Rome Precision Solar\nPhotometric Telescope (2000-2022). We use both of those Ca II K archives to\nidentify polar network regions with an automatic adaptive threshold\nsegmentation technique and calculate the PNI. The PNI obtained from both the\narchives shows a significant correlation with the measured polar field from WSO\n(Pearson correlation coefficient r > 0.93) and the derived polar field based on\nan Advective Flux Transport Model (r > 0.91). The PNI series also shows a\nsignificant correlation with faculae counts derived from Mount Wilson\nObservatory observations (r > 0.87) for both KoSO and Rome-PSPT data. Finally,\nwe use the PNI series from both archives to reconstruct the polar magnetic\nfield over a 119-year-long period, which includes last 11 solar cycles (Cycle\n14-24). We also obtain a relationship between the amplitude of solar cycles (in\n13-month smoothed sunspot number) and the strength of the reconstructed polar\nfield at the preceding solar cycle minimum to validate the prediction of the\nongoing solar cycle, Cycle 25.",
        "In this brief note, we study the strong law of large numbers for random walks\nin random scenery. Under the assumptions that the random scenery is\nnon-stationary and satisfies weakly dependent condition with an appropriate\nrate, we establish strong law of large numbers for random walks in random\nscenery. Our results extend the known results in the literature.",
        "With debris larger than 1 cm in size estimated to be over one million,\nprecise cataloging efforts are essential to ensure space operations' safety.\nCompounding this challenge is the oversubscribed problem, where the sheer\nvolume of space objects surpasses ground-based observatories' observational\ncapacity. This results in sparse, brief observations and extended intervals\nbefore image acquisition. LeoLabs' network of phased-array radars addresses\nthis need by reliably tracking 10 cm objects and larger in low Earth orbit with\n10 independent radars across six sites. While LeoLabs tracklets are extremely\nshort, they hold much more information than typical radar observations.\nFurthermore, two tracklets are generally available, separated by a couple of\nminutes. Thus, this paper develops a tailored approach to initialize state and\nuncertainty from a single or pair of tracklets. Through differential algebra,\nthe initial orbit determination provides the state space compatible with the\navailable measurements, namely an orbit set. This practice, widely used in\nprevious research, allows for efficient data association of different\ntracklets, thus enabling the addition of accurate tracks to the catalog\nfollowing their independent initialization. The algorithm's efficacy is tested\nusing real measurements, evaluating the IOD solution's accuracy and ability to\npredict the next passage from a single or a pair of tracklets.",
        "Lithium-sulfur (Li-S) batteries offer substantial theoretical energy density\ngains over Li-ion bat-teries, a crucial factor for transportation\nelectrification. In addition, sulfur is an earth-abundant, inexpensive material\nobtainable from multiple resources; thus, Li-S batteries are envisioned to\nprovide environmentally sustainable solutions to the growing demand for energy\nstorage. A crit-ical roadblock to the realization of commercial Li-S batteries\nis the formation of polysulfides and their secondary reactions with liquid\norganic electrolytes, resulting in low coulombic efficiency for charging and\nfast self-discharge rates. The realization of solid-state electrolytes for Li-S\nbat-teries provides potential pathways to address the safety concerns of liquid\nelectrolytes and inhib-it the formation of polysulfides and\/or prevent their\ndiffusion into the anode electrode. However, current solid-state electrolytes\nare limited by low ionic conductivity, inadequate electrode inter-facial\ncompatibility, and restricted electrochemical windows. This review discusses\nthe status of polymer-based electrolytes for Li-S batteries, and outlines\ncurrent methods for their fabrication, their transport characteristics and\nongoing research aimed at overcoming material properties hindering the\ndevelopment of all-solid-state Li-S batteries.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "The Euclid mission is generating a vast amount of imaging data in four\nbroadband filters at high angular resolution. This will allow the detailed\nstudy of mass, metallicity, and stellar populations across galaxies, which will\nconstrain their formation and evolutionary pathways. Transforming the Euclid\nimaging for large samples of galaxies into maps of physical parameters in an\nefficient and reliable manner is an outstanding challenge. We investigate the\npower and reliability of machine learning techniques to extract the\ndistribution of physical parameters within well-resolved galaxies. We focus on\nestimating stellar mass surface density, mass-averaged stellar metallicity and\nage. We generate noise-free, synthetic high-resolution imaging data in the\nEuclid photometric bands for a set of 1154 galaxies from the TNG50 cosmological\nsimulation. The images are generated with the SKIRT radiative transfer code,\ntaking into account the complex 3D distribution of stellar populations and\ninterstellar dust attenuation. We use a machine learning framework to map the\nidealised mock observational data to the physical parameters on a\npixel-by-pixel basis. We find that stellar mass surface density can be\naccurately recovered with a $\\leq 0.130 {\\rm \\,dex}$ scatter. Conversely,\nstellar metallicity and age estimates are, as expected, less robust, but still\ncontain significant information which originates from underlying correlations\nat a sub-kpc scale between stellar mass surface density and stellar population\nproperties.",
        "Iterative magnitude pruning methods (IMPs), proven to be successful in\nreducing the number of insignificant nodes in over-parameterized deep neural\nnetworks (DNNs), have been getting an enormous amount of attention with the\nrapid deployment of DNNs into cutting-edge technologies with computation and\nmemory constraints. Despite IMPs popularity in pruning networks, a fundamental\nlimitation of existing IMP algorithms is the significant training time required\nfor each pruning iteration. Our paper introduces a novel \\textit{stopping\ncriterion} for IMPs that monitors information and gradient flows between\nnetworks layers and minimizes the training time. Information Consistent Pruning\n(\\ourmethod{}) eliminates the need to retrain the network to its original\nperformance during intermediate steps while maintaining overall performance at\nthe end of the pruning process. Through our experiments, we demonstrate that\nour algorithm is more efficient than current IMPs across multiple dataset-DNN\ncombinations. We also provide theoretical insights into the core idea of our\nalgorithm alongside mathematical explanations of flow-based IMP. Our code is\navailable at \\url{https:\/\/github.com\/Sekeh-Lab\/InfCoP}.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "We present a catalogue of extended radio sources from the SARAO MeerKAT\nGalactic Plane Survey (SMGPS). Compiled from 56 survey tiles and covering\napproximately 500 deg$^2$ across the first, third, and fourth Galactic\nquadrants, the catalogue includes 16534 extended and diffuse sources with areas\nlarger than 5 synthesised beams. Of them, 3891 (24\\% of the total) are\nconfidently associated with known Galactic radio-emitting objects in the\nliterature, such as HII regions, supernova remnants, planetary nebulae,\nluminous blue variables, and Wolf-Rayet stars. A significant fraction of the\nremaining sources, 5462 (33\\%), are candidate extragalactic sources, while 7181\n(43\\%) remain unclassified. Isolated radio filaments are excluded from the\ncatalogue. The diversity of extended sources underscores MeerKAT's contribution\nto the completeness of censuses of Galactic radio emitters, and its potential\nfor new scientific discoveries. For the catalogued sources, we derived basic\npositional and morphological parameters, as well as flux density estimates,\nusing standard aperture photometry. This paper describes the methods followed\nto generate the catalogue from the original SMGPS tiles, detailing the source\nextraction, characterisation, and crossmatching procedures. Additionally, we\nanalyse the statistical properties of the catalogued populations",
        "Environmental phenomena are influenced by complex interactions among various\nfactors. For instance, the amount of rainfall measured at different stations\nwithin a given area is shaped by atmospheric conditions, orography, and physics\nof water processes. Motivated by the need to analyze rainfall across complex\nspatial locations, we propose a flexible Bayesian semi-parametric model for\nspatially distributed data. This method effectively accounts for spatial\ncorrelation while incorporating dependencies on geographical characteristics in\na highly flexible manner. Indeed, using latent Gaussian processes, indexed by\nspatial coordinates and topographical features, the model integrates spatial\ndependencies and environmental characteristics within a nonparametric\nframework. Posterior inference is conducted using an efficient rejection-free\nMarkov Chain Monte Carlo algorithm, which eliminates the need for tuning\nparameter calibration, ensuring smoother and more reliable estimation. The\nmodel's flexibility is evaluated through a series of simulation studies,\ninvolving different rainfall and spatial correlation scenarios, to demonstrate\nits robustness across various conditions. We then apply the model to a large\ndataset of rainfall events collected from the Italian regions of Veneto and\nTrentino-Alto Adige, these areas are known for their complex orography and\ndiverse meteorological drivers. By analyzing this data, we generate detailed\nmaps that illustrate the mean and variance of rainfall and rainy days. The\nmethod is implemented in a new R package available on GitHub.",
        "Geodesic vortex detection on curved surfaces: Analyzing the 2002 austral\nstratospheric polar vortex warming event",
        "A defective $k$-coloring is a coloring on the vertices of a graph using\ncolors $1,2, \\dots, k$ such that adjacent vertices may share the same color. A\n$(d_1,d_2)$-\\emph{coloring} of a graph $G$ is a defective $2$-coloring of $G$\nsuch that any vertex colored by color $i$ has at most $d_i$ adjacent vertices\nof the same color, where $i\\in\\{1,2\\}$. A graph $G$ is said to be\n$(d_1,d_2)$-\\emph{colorable} if it admits a $(d_1,d_2)$-coloring.\n  Defective $2$-coloring in planar graphs without $3$-cycles, $4$-cycles, and\n$6$-cycles has been investigated by Dross and Ochem, as well as Sittitrai and\nPimpasalee. They showed that such graphs are $(0,6)$-colorable and\n$(3,3)$-colorable, respectively. In this paper, we proved that these graphs are\nalso $(2,4)$-colorable.",
        "We study the formation of propagating large-scale density waves of mixed\npolar-nematic symmetry in a colony of self-propelled agents that are bound to\nmove along the planar surface of a thin viscous film. The agents act as an\ninsoluble surfactant, i.e. the surface tension of the liquid depends on their\ndensity. Therefore, density gradients generate a Marangoni flow. We demonstrate\nthat for active matter in the form of self-propelled surfactants with local\n(nematic) aligning interactions such a Marangoni flow nontrivially influences\nthe propagation of the density waves. Upon gradually increasing the Marangoni\nparameter, which characterises the relative strength of the Marangoni flow as\ncompared to the self-propulsion speed, the density waves broaden while their\nspeed may either increase or decrease depending on wavelength and overall mean\ndensity. A further increase of the Marangoni parameter eventually results in\nthe disappearance of the density waves. This may occur either discontinuously\nat finite wave amplitude via a saddle-node bifurcation or continuously with\nvanishing wave amplitude at a wave bifurcation, i.e. a finite-wavelength Hopf\nbifurcation.",
        "- An algorithm for calculating the radiation field of a charged point\nparticle performing a spiral motion in an infinite cylindrical waveguide with a\nmultilayer side wall is found. The number of layers and their filling is\narbitrary. The axis of the spiral is aligned with the axis of the waveguide, so\nthat the geometry of the problem has cylindrical symmetry. Explicit expressions\nfor modal frequency distributions and equations for resonant frequencies for\nsingle-layer and double-layer waveguides are given. Examples of graphical\nconstructions of modal frequency distributions of modes for single-layer\n(resistive), double-layer (metal-dielectric) and triple-layer (metal-dielectric\nwith internal NEG coating) waveguides are presented.",
        "The paper is motivated by pricing decisions faced by forecourt fuel retailers\nacross their outlets on a road network. Through our modelling approach we are\nable adapt the network structure to a bipartite graph with demand nodes\nrepresenting volumes of fuel from customers using a specific route that\nconnects to the seller's outlet nodes that intersect that route on the network.\nCustomers may have their demand satisfied at the lowest priced competitor on\ntheir route. However, the seller can satisfy some or all of this demand by\nmatching or beating this price via one of their outlets intersecting the route.\nWe give a practical extension to min-pricing by considering a binary logit\nvariant for buyers evaluating the choice between two sellers. We derive two MIP\nformulations for min-buying in the case of general demand. We also propose\nseveral constructive heuristics, based on insertion and selection operations,\nsuitable for problem instances beyond the scope of the exact methods. The\nperformance of models and algorithms are evaluated in a numerical study and\ndevelop insights from the results. Importantly, we are able to highlight the\nvalue of price-matching decisions under buyer demand sensitivity.",
        "We study obstacle problems governed by two distinct types of diffusion\noperators involving interacting free boundaries. We obtain a somewhat\nsurprising coupling property, leading to a comprehensive analysis of the free\nboundary. More precisely, we show that near regular points of a coordinate\nfunction, the free boundary is analytic, whereas singular points lie on a\nsmooth manifold. Additionally, we prove that uncoupled free boundary points are\nsingular, indicating that regular points lie exclusively on the coupled free\nboundary. Furthermore, optimal regularity, non-degeneracy, and lower\ndimensional Hausdorff measure estimates are obtained. Explicit examples\nillustrate the sharpness of assumptions.",
        "Designing two-dimensional (2D) Rashba semiconductors, exploring the\nunderlying mechanism of Rashba effect, and further proposing efficient and\ncontrollable approaches are crucial for the development of spintronics. On the\nbasis of first-principles calculations, we here theoretically design all\npossible types (common, inverse, and composite) of Janus structures and\nsuccessfully achieve numerous ideal 2D Rashba semiconductors from a series of\nfive atomic-layer A2B2Te6 (A = Sb, Bi; B = Si, Ge) materials. Considering the\ndifferent Rashba constant {\\alpha}R and its modulation trend under external\nelectric field, we comprehensively analyze the intrinsic electric field Ein in\nterms of work function, electrostatic potential, dipole moment, and inner\ncharge transfer. Inspired by the quantitative relationship between charge\ntransfer and the strength of Ein and even the {\\alpha}R, we propose a\nstraightforward strategy of introducing a single adatom onto the surface of 2D\nmonolayer to introduce and modulate the Rashba effect. Lastly, we also examine\nthe growth feasibility and electronic structures of the Janus Sb2Ge2Se3Te3\nsystem and Janus-adsorbed systems on a 2D BN substrate. Our work not only\nconducts a detailed analysis of A2B2Te6-based Rashba systems, but also proposes\na new strategy for efficiently and controllably modulating the {\\alpha}R\nthrough the reconfiguration of charge transfer.",
        "Defining low-dimensional reaction coordinates is crucial for analyzing the\ndynamics of complex systems and for comparison with experiments. The maximal\nvalue of the transition-path probability along the reaction coordinate $x$,\n$p(\\mathrm{TP}|x)$, is a common estimator for reaction-coordinate quality by\ncomparing to the theoretical maximal value of 1\/2 in the overdamped Markovian\nlimit. We show by analytical arguments and simulations that for non-Markovian\ndynamics $p(\\mathrm{TP}|x)$ is non-monotonic as a function of the memory time\nand exceeds 1\/2 for long memory time. This disqualifies $p(\\mathrm{TP}|x)$ as a\ncriterion for reaction coordinate quality.",
        "We present the construction of a new state sum model for $4d$ Lorentzian\nquantum gravity based on the description of quantum simplicial geometry in\nterms of edge vectors. Quantum states and amplitudes for simplicial geometry\nare built from irreducible representations of the translation group, then\nrelated to the representations of the Lorentz group via expansors, leading to\ninteresting (and intricate) non-commutative structures. We also show how the\nnew model connects to the Lorentzian Barrett-Crane spin foam model, formulated\nin terms of quantized triangle bivectors.",
        "In this paper, we study an optimal control problem for a brain tumor growth\nmodel that incorporates lactate metabolism, viscoelastic effects, and tissue\ndamage. The PDE system, introduced in [G. Cavalleri, P. Colli, A. Miranville,\nE. Rocca, On a Brain Tumor Growth Model with Lactate Metabolism, Viscoelastic\nEffects, and Tissue Damage (2025)], couples a Fisher-Kolmogorov type equation\nfor tumor cell density with a reaction-diffusion equation for the lactate, a\nquasi-static force balance governing the displacement, and a nonlinear\ndifferential inclusion for tissue damage. The control variables, representing\nchemotherapy and a lactate-targeting drug, influence tumor progression and\ntreatment response. Starting from well-posedness, regularity, and continuous\ndependence results already established, we define a suitable cost functional\nand prove the existence of optimal controls. Then, we analyze the\ndifferentiability of the control-to-state operator and establish a necessary\nfirst-order condition for treatment optimality.",
        "We investigate unitarily inequivalent representations of the algebra of\noperators in quantum field theory. In those cases that exist a Fock\nrepresentation of the commutation relations, we have a unique no-quanta state.\nWe examine more closely the operational definition of a measuring device which\ndetect a quantum of an Hermitian scalar field. The Unruh-DeWitt and Glauber\nmodel of quanta detectors are discussed. In Minkowski spacetime, the transition\nprobability per unit proper time of both detectors is studied in two different\nnon-inertial frames of reference. The first one, a detector with a constant\nproper acceleration, travelling in a stationary worldline, i.e., an hyperbolic\nmotion, interacting with the field prepared in the Poincar\\'e invariant Fock\nvacuum state. Next, we studied the Unruh-DeWitt detector at rest in a\nnon-uniformly accelerated frame, with a time dependent acceleration. We\nevaluate the positive frequency Wightman function for the non-uniformly\naccelerated frame in a finite time interval and obtain the same two-point\ncorrelation function of a system in equilibrium with a thermal bath. Therefore\nthe non-uniformly accelerated Unruh-DeWitt detector interacting with the field\nin the Poincar\\'e invariant Fock vacuum state measures a thermal state. This\nresult shows that the existence of an acceleration temperature without the\npresence of an event horizon.",
        "Using the information theory, this study provides insights into how the\nconstruction of latent space of autoencoder (AE) using deep neural network\n(DNN) training finds a smooth low-dimensional manifold in the stiff dynamical\nsystem. Our recent study [1] reported that an autoencoder (AE) combined with\nneural ODE (NODE) as a surrogate reduced order model (ROM) for the integration\nof stiff chemically reacting systems led to a significant reduction in the\ntemporal stiffness, and the behavior was attributed to the identification of a\nslow invariant manifold by the nonlinear projection of the AE. The present work\noffers fundamental understanding of the mechanism by employing concepts from\ninformation theory and better mixing. The learning mechanism of both the\nencoder and decoder are explained by plotting the evolution of mutual\ninformation and identifying two different phases. Subsequently, the density\ndistribution is plotted for the physical and latent variables, which shows the\ntransformation of the \\emph{rare event} in the physical space to a \\emph{highly\nlikely} (more probable) event in the latent space provided by the nonlinear\nautoencoder. Finally, the nonlinear transformation leading to density\nredistribution is explained using concepts from information theory and\nprobability.",
        "We present characterizations for the inclusions $\\mathrm{Add}(M)\\subseteq\n\\mathrm{Prod}(M)$ and $\\mathrm{Prod}(M)\\subseteq \\mathrm{Add}(M)$ in locally\nfinitely presented categories and in compactly generated triangulated\ncategories. As applications, we describe the situations when the classes of the\nform $\\mathrm{Prod}(M)$ and $\\mathrm{Add}(M)$ are (pre)covering, respectively\n(pre)enveloping."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Multimodal Prescriptive Deep Learning",
        "Formation of Frustrated Charge Density Waves in Kagome Metal\n  LuNb$_6$Sn$_6$",
        "Tautological characteristic classes III: the Witt class for PSL(2)",
        "Exploring the Growth-Index ($\\gamma$) Tension with $\\Lambda_{\\rm s}$CDM",
        "Extremal Betti Numbers and Persistence in Flag Complexes",
        "Disk reflection and energetics from the accreting millisecond pulsar\n  SRGA J144459.2-604207",
        "A faithful action of Gal($\\overline{\\mathbb{Q}}\/\\mathbb{Q}$) on Zariski\n  multiplets",
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Curves on Endo-Pajitnov manifolds",
        "Integral gains for non-autonomous Wazewski systems",
        "Removal of excess iron by annealing processes and emergence of bulk\n  superconductivity in sulfur-substituted FeTe",
        "A Response to Recent Critiques of Hainmueller, Mummolo and Xu (2019) on\n  Estimating Conditional Relationships",
        "Electric Power Enhancement using Spin-Polarized Fuel in Fusion Power\n  Plants",
        "Equidistribution of saddle periodic points for H\\'enon-like maps",
        "Determination and evaluation of the critical liquid nitrogen for\n  superconducting levitator based on a novel temperature-weight coupling\n  measurement device",
        "Normalized Solutions for nonlinear Schr\\\"{o}dinger-Poisson equations\n  involving nearly mass-critical exponents",
        "Balance Laws and Transport Theorems for Flows with Singular Interfaces",
        "Line Edge Roughness Effects on the Thermoelectric Properties of Armchair\n  Black Phosphorene Nanoribbons",
        "TT-LSQR For Tensor Least Squares Problems and Application to Data Mining\n  *",
        "Comparison principles and asymptotic behavior of delayed age-structured\n  neuron models",
        "Two-Component Theory of Classical Gravitons with Arbitrary Sources",
        "Transport coefficients of a low-temperature normal Fermi gas with\n  contact interactions: an exact perturbative expansion",
        "Pauli Blocking effects in Nilsson states of weakly bound exotic nuclei",
        "Second order estimates for $\\chi$-semi convex solutions of Hessian\n  equations on Hermitian manifolds",
        "Strong and weak dynamo regimes in Taylor-Couette flows",
        "Generalization of Jamet's test for convergence of number series and its\n  new modifications",
        "Rational Tuning of LLM Cascades via Probabilistic Modeling",
        "Using Space-Filling Curves and Fractals to Reveal Spatial and Temporal\n  Patterns in Neuroimaging Data",
        "Uniform estimates for elliptic equations with Carath\\'eodory\n  nonlinearities at the interior and on the boundary"
      ],
      "abstract":[
        "We introduce a multimodal deep learning framework, Prescriptive Neural\nNetworks (PNNs), that combines ideas from optimization and machine learning,\nand is, to the best of our knowledge, the first prescriptive method to handle\nmultimodal data. The PNN is a feedforward neural network trained on embeddings\nto output an outcome-optimizing prescription. In two real-world multimodal\ndatasets, we demonstrate that PNNs prescribe treatments that are able to\nsignificantly improve estimated outcomes in transcatheter aortic valve\nreplacement (TAVR) procedures by reducing estimated postoperative complication\nrates by 32% and in liver trauma injuries by reducing estimated mortality rates\nby over 40%. In four real-world, unimodal tabular datasets, we demonstrate that\nPNNs outperform or perform comparably to other well-known, state-of-the-art\nprescriptive models; importantly, on tabular datasets, we also recover\ninterpretability through knowledge distillation, fitting interpretable Optimal\nClassification Tree models onto the PNN prescriptions as classification\ntargets, which is critical for many real-world applications. Finally, we\ndemonstrate that our multimodal PNN models achieve stability across randomized\ndata splits comparable to other prescriptive methods and produce realistic\nprescriptions across the different datasets.",
        "The charge density wave (CDW), a translational symmetry breaking electronic\nliquid, plays a pivotal role in correlated quantum materials, such as\nhigh-T$_c$ superconductors and topological semimetals. Recently, CDWs that\npossibly intertwine with superconductivity and magnetism are observed in\nvarious kagome metals. However, the nature of CDWs and the role of the Fermi\nsurface (FS) topology in these materials remain an unresolved challenge. In\nthis letter, we reveal the formation of CDWs in the newly discovered kagome\nmetal LuNb$_6$Sn$_6$. We observe a precursor CDW correlation that features a\n\"yield sign\"-like hollow triangle diffuse scattering pattern and nearly\ncomplete softening of a flat optical phonon band near Q$_H$=(1\/3, 1\/3, 1\/2).\nThe scattering intensity of the precursor CDW displays divergent behavior as\ndecreasing temperature until T$_{CDW}$=70 K, where a competing CDW at\nQ$_{CDW}$=(1\/3, 1\/3, 1\/3) emerges. Using scanning tunneling\nmicroscopy\/spectroscopy, we image the frustrated CDW patterns that show a short\nphase coherence length about ~20 nm in real space. Combined with first\nprinciples calculations, our observations support frustrated lattice\ninteractions that are modulated by the FS topology. These results shed light on\nthe interplay between FS and CDW in quantum materials with nearly degenerate\nstructural deformation patterns.",
        "We explain the relation between the Witt class and the universal\nequicommutative class for PSL(2,K). We discuss an analogue of the Milnor-Wood\ninequality.",
        "Recent observational analyses have revealed a significant tension in the\ngrowth index $\\gamma$, which characterizes the growth rate of cosmic\nstructures. Specifically, when treating $\\gamma$ as a free parameter within\n$\\Lambda$CDM framework, a combination of Planck and $ f\\sigma_8 $ data yields\n$\\gamma \\approx 0.64$, in $\\sim4\\sigma$ tension with the theoretically expected\nvalue $\\gamma \\approx 0.55$ (assuming general relativity). This discrepancy,\nclosely related to the $ S_8 $ tension, poses a new challenge to the standard\ncosmological model by suggesting that it predicts an excessive growth of\nstructure. In this work, we demonstrate that the $\\Lambda_{\\rm s}$CDM framework\n(featuring a rapid sign-switching cosmological constant (mirror AdS-to-dS\ntransition) in the late universe at redshift $ z_\\dagger \\sim 2 $) can\nsimultaneously alleviate the $ \\gamma $, $ H_0 $, and $ S_8 $ tensions. We also\nexamined a scenario with fixed $ z_\\dagger = 1.7 $, previously identified as a\nsweet spot for alleviating multiple major cosmological tensions (including\nthose in $ H_0 $, $ M_B $, and $ S_8 $) finding that it completely eliminates\nboth the $ \\gamma $ and $ H_0 $ tensions, although it is statistically\ndisfavored by our dataset combinations. Our findings suggest that $\\Lambda_{\\rm\ns}$CDM is a promising model, providing a potential unified resolution to\nmultiple major cosmological tensions.",
        "We investigate several problems concerning extremal Betti numbers and\npersistence in filtrations of flag complexes. For graphs on $n$ vertices, we\nshow that $\\beta_k(X(G))$ is maximal when $G=\\mathcal{T}_{n,k+1}$, the Tur\\'an\ngraph on $k+1$ partition classes, where $X(G)$ denotes the flag complex of $G$.\nBuilding on this, we construct an edgewise (one edge at a time) filtration\n$\\mathcal{G}=G_1\\subseteq \\cdots \\subseteq \\mathcal{T}_{n,k+1}$ for which\n$\\beta_k(X(G_i))$ is maximal for all graphs on $n$ vertices and $i$ edges.\nMoreover, the persistence barcode $\\mathcal{B}_k(X(G))$ achieves a maximal\nnumber of intervals, and total persistence, among all edgewise filtrations with\n$|E(\\mathcal{T}_{n,k+1})|$ edges.\n  For $k=1$, we consider edgewise filtrations of the complete graph $K_n$. We\nshow that the maximal number of intervals in the persistence barcode is\nobtained precisely when $G_{\\lceil n\/2\\rceil \\cdot \\lfloor n\/2\n\\rfloor}=\\mathcal{T}_{n,2}$. Among such filtrations, we characterize those\nachieving maximal total persistence. We further show that no filtration can\noptimize $\\beta_1(X(G_i))$ for all $i$, and conjecture that our filtrations\nmaximize the total persistence over all edgewise filtrations of $K_n$.",
        "Accreting millisecond pulsars (AMSPs) are excellent laboratories to study\nreflection spectra and their features from an accretion disk truncated by a\nrapidly rotating magnetosphere near the neutron star surface. These systems\nalso exhibit thermonuclear (type-I) bursts that can provide insights on the\naccretion physics and fuel composition. We explore spectral properties of the\nAMSP SRGA J144459.2-0604207 observed during the outburst that recently led to\nits discovery in February 2024. We aim to characterize the spectral shape of\nthe persistent emission, both its continuum and discrete features, and to\nanalyze type-I bursts properties. We employ XMM and NuSTAR overlapping\nobservations taken during the most recent outburst from SRGA J1444. We perform\nspectral analysis of the persistent (i.e., non-bursting) emission employing a\nsemi-phenomenological continuum model composed of a dominant thermal\nComptonization plus two thermal contributions, and a physical reflection model.\nWe also perform time-resolved spectral analysis of a type-I burst employing a\nblackbody model. We observe a broadened iron emission line, thus suggesting\nrelativistic effects, supported by the physical model accounting for\nrelativistically blurred reflection. The resulting accretion disk extends down\nto 6 gravitational radii, inclined at ~$53^{\\circ}$, and only moderately\nionized (log$\\xi\\simeq2.3$). We observe an absorption edge at ~9.7 keV that can\nbe interpreted as an Fe XXVI edge blueshifted by an ultrafast ($\\simeq0.04$c)\noutflow. Our broadband observations of type-I bursts do not find evidence of\nphotospheric radius expansion. The burst recurrence time shows a dependence on\nthe count rate with the steepest slope ever observed in these systems. We also\nobserve a discrepancy of ~3 between the observed and expected burst recurrence\ntime, which we discuss in the framework of fuel composition and high NS mass\nscenarios.",
        "In this work, we establish two main results in the context of arithmetic and\ngeometric properties of plane curves. First, we construct numerous new examples\nof arithmetic Zariski pairs and multiplets, where only a few ones were\npreviously available. Second, we describe a faithful action of the absolute\nGalois group on the equisingular strata of plane curves, providing insights\ninto the interplay between Galois representations and the geometry of singular\nplane curves. We conclude the paper with very concretes examples of the general\nresults obtained.",
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "Endo--Pajitnov manifolds are generalizations to higher dimensions of the\nInoue surfaces $S^M$. We study the existence of complex submanifolds in\nEndo--Pajitnov manifolds. We identify a class of these manifolds that do\ncontain compact complex submanifolds and establish an algebraic condition under\nwhich an Endo--Pajitnov manifold contains no compact complex curves.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "There are several strategies to discover new superconductors. Growing new\nmaterials and applying high pressures can be the classic ways since\nsuperconductivity was found. Also, chemical processing, such as annealing, is\nanother way to induce superconductivity in a non-superconducting material.\nHere, we show chemical processing effects in the non-superconducting material,\nsulfur-substituted FeTe. It has been known that superconductivity in\nS-substituted FeTe is induced by O$_2$ annealing. We revealed that hydrochloric\nacid etching and vacuum annealing for O$_2$-annealed samples made the quality\nof superconductivity higher by several physical property measurements.\nFurthermore, we visualized the superconducting regions by a magneto-optical\nimaging technique, indicating that the superconductivity in the processed\nsample was bulk. In this sample, we confirmed that the concentration of excess\niron was reduced compared to that in the as-grown state. These results provide\nan important route to bulk superconductivity in S-substituted FeTe and its\nrelated iron-based compounds.",
        "Simonsohn (2024a) and Simonsohn (2024b) critique Hainmueller, Mummolo and Xu\n(2019, HMX), arguing that failing to model nonlinear relationships between the\ntreatment and moderator leads to biased marginal effect estimates and\nuncontrolled Type-I error rates. While these critiques highlight the issue of\nunder-modeling nonlinearity in applied research, they are fundamentally flawed\nin several key ways. First, the causal estimand for interaction effects and the\nnecessary identifying assumptions are not clearly defined in these critiques.\nOnce properly stated, the critiques no longer hold. Second, the kernel\nestimator HMX proposes recovers the true causal effects in the scenarios\npresented in these recent critiques, which compared effects to the wrong\nbenchmark, producing misleading conclusions. Third, while Generalized Additive\nModels (GAM) can be a useful exploratory tool (as acknowledged in HMX), they\nare not designed to estimate marginal effects, and better alternatives exist,\nparticularly in the presence of additional covariates. Our response aims to\nclarify these misconceptions and provide updated recommendations for\nresearchers studying interaction effects through the estimation of conditional\nmarginal effects.",
        "Using a range of fusion power plant (FPP) concepts, we demonstrate that\nspin-polarized fuel (SPF) can significantly enhance net electric power output,\noften by many multiples. Notably, the electric power gain from SPF generally\nexceeds the corresponding increase in thermal fusion power. Plants close to\nengineering breakeven stand to benefit most, where even modest boosts in fusion\npower produce disproportionately larger gains in net electricity. As a\nrepresentative example, a 25% increase in fusion power via SPF could allow an\nITER-like device (with an added turbine to recover thermal fusion power) to\nachieve engineering breakeven. These findings strongly motivate the development\nof spin-polarized fuel for FPPs.",
        "We prove that under the natural assumption over the dynamical degrees, the\nsaddle periodic points of a H\\'enon-like map in any dimension equidistribute\nwith respect to the equilibrium measure. Our work is a generalization of\nresults of Bedford-Lyubich-Smillie, Dujardin and Dinh-Sibony along with\nimprovements of their techniques. We also investigate some fine properties of\nGreen currents associated with the map. On the pluripotential-theory side, in\nour non-compact setting, the wedge product of two positive closed currents of\ncomplementary bi-degrees can be defined using super-potentials and the density\ntheory. We prove that these two definitions are coherent.",
        "Liquid nitrogen (LN2) is the only cooling medium for the high-temperature\nsuperconducting (HTS) bulks in the superconducting levitator, which is the\nheart of the maglev train, to reach working state. The detection and\ndetermination of the critical LN2 content are crucial for reliable operation of\nthe HTS maglev train. However, the related intelligent detection model and\ntechnology is lack in the combination filed of the cryogenic environment and\nmaglev application, and there is no existing method to detect the LN2 content\nin superconducting levitator. This paper proposes to employ multisensor fusion\nframework to fuse and enhance the accuracy of critical LN2 content testing.\nFour temperature sensors were deployed inside superconducting levitator to\nmeasure the temperature change during the LN2 content changing from 100 % to 0.\nIt was first obtained that the critical LN2 content in the superconducting\nlevitator is 4%. To accurately monitor the critical LN2 content in the\nsuperconducting levitator, a matrix-weighted information fusion Kalman filter\nalgorithm was used. Compared with the previous single sensor method, the\ntesting accuracy of the multisensor fusion method can be improved by 5.6%. The\nwork can provide a preliminary research foundation for the online monitoring\nand fault diagnosis of HTS maglev train.",
        "We study the Schr\\\"{o}dinger-Poisson-Slater equation\n\\begin{equation*}\\left\\{\\begin{array}{lll}\n  -\\Delta u + \\lambda u + \\big(|x|^{-1} \\ast |u|^{2}\\big)u = V(x) u^{\np_{\\varepsilon}-1 }, \\, \\text{ in } \\mathbb{R}^{3},\\\\[2mm]\n  \\int_{\\mathbb{R}^3}u^2 \\,dx= a,\\,\\, u > 0,\\,\\, u \\in H^{1}(\\mathbb{R}^{3}),\n  \\end{array}\n  \\right. \\end{equation*} where $\\lambda$ is a Lagrange multiplier, $V(x)$ is a\nreal-valued potential, $a\\in \\mathbb{R}_{+}$ is a constant, $ p_{\\varepsilon} =\n\\frac{10}{3} \\pm \\varepsilon$ and $\\varepsilon>0$ is a small parameter. In this\npaper, we prove that it is the positive critical value of the potential $V$\nthat affects the existence of single-peak solutions for this problem.\nFurthermore, we prove the local uniqueness of the solutions we construct.",
        "This paper gives a concise but rigorous mathematical description of a\nmaterial control volume that is separated into two parts by a singular surface\nat which physical states are discontinuous. The geometrical background material\nis summarized in a unified manner. Transport theorems for use in generic\nbalance laws are given with proofs since they provide some insight into the\nresults. Also the step from integral balances to differential equations is\ngiven in some detail.",
        "This study delves into the thermoelectric properties of armchair black\nphosphorene nanoribbons while considering the presence of line edge roughness.\nEmploying the tight-binding method in conjunction with non-equilibrium Green's\nfunction techniques and Landauer formulas, we explore the impact of various\nparameters on thermoelectric performance. Our findings reveal that the\nelectrical conductivity and, consequently, the power factor exhibit an\nincreasing trend with expanding ribbon length and width. This behavior can be\nattributed to heightened collision rates, particularly in narrow ribbons,\ninduced by line edge roughness as length increases. Remarkably, the Seebeck\ncoefficient at the Fermi energy corresponding to the maximum power factor\nremains nearly constant across different widths, lengths, temperatures and\ntransport regimes. Furthermore, the thermoelectric figure of merit demonstrates\na positive correlation with both ribbon length and width. In narrow widths and\nlengths around 1000 nm, the power factor and figure of merit exhibit an upward\ntrend with ribbon width. However, with further increases in ribbon width, the\ninfluence of line edge roughness on thermal conductivity diminishes.\nConsequently, the figure of merit decreases due to the rise in thermal\nconductivity. Notably, the thermoelectric figure of merit is higher for short\nand narrow ribbons and long and wider ribbons.",
        "We are interested in the numerical solution of the tensor least squares\nproblem \\[\n  \\min_{\\mathcal{X}} \\| \\mathcal{F} - \\sum_{i =1}^{\\ell} \\mathcal{X} \\times_1\nA_1^{(i)} \\times_2 A_2^{(i)} \\cdots \\times_d A_d^{(i)} \\|_F, \\] where\n$\\mathcal{X}\\in\\mathbb{R}^{m_1 \\times m_2 \\times \\cdots \\times m_d}$,\n$\\mathcal{F}\\in\\mathbb{R}^{n_1\\times n_2 \\times \\cdots \\times n_d}$ are tensors\nwith $d$ dimensions, and the coefficients $A_j^{(i)}$ are tall matrices of\nconforming dimensions. We first describe a tensor implementation of the\nclassical LSQR method by Paige and Saunders, using the tensor-train\nrepresentation as key ingredient. We also show how to incorporate sketching to\nlower the computational cost of dealing with the tall matrices $A_j^{(i)}$. We\nthen use this methodology to address a problem in information retrieval, the\nclassification of a new query document among already categorized documents,\naccording to given keywords.",
        "In the context of neuroscience the elapsed-time model is an age-structured\nequation that describes the behavior of interconnected spiking neurons through\nthe time since the last discharge, with many interesting dynamics depending on\nthe type of interactions between neurons. We investigate the asymptotic\nbehavior of this equation in the case of both discrete and distributed delays\nthat account for the time needed to transmit a nerve impulse from one neuron to\nthe rest the ensemble. To prove the convergence to the equilibrium, we follow\nan approach based on comparison principles for Volterra equations involving the\ntotal activity, which provides a simpler and more straightforward alternative\ntechnique than those in the existing literature on the elapsed-time model.",
        "A review of the spin curvatures that arise in the framework of the Infeld-van\nder Waerden {\\epsilon}-formalism for general relativity is presented. The\nsourceful extension formulated in an earlier work of the theory of gravitons in\nvacuum is then reviewed in a concise way. At this stage the\n{\\epsilon}-formalism system of gravitational wave equations obtained thereabout\nis replicated on the basis of the implementation of the same calculational\ntechniques as the ones that had been utilized in the sourceless situation. An\nimportant symmetry property borne by the right-hand sides of those wave\nequations is subsequently brought forward.",
        "We compute the shear viscosity, thermal conductivity and spin diffusivity of\na Fermi gas with short-range interactions in the Fermi liquid regime of the\nnormal phase, that is at temperatures $T$ much lower than the Fermi temperature\n$T_{\\rm F}$ and much larger than the superfluid critical temperature $T_c$.\nGiven recent advances in the precision of cold atom experiments, we provide\nexact results up to second-order in the interaction strength. We extend the\nLandau-Salpeter equation to compute the collision amplitude beyond the\nforward-scattering limit, covering all collisions on the Fermi surface. We\ntreat the collision kernel exactly, leading to significant corrections beyond\nrelaxation time or variational approximations. The transport coefficients, as\nfunctions of the $s$-wave scattering length $a$ and Fermi wavenumber $k_{\\rm\nF}$, follow $(1+\\gamma k_{\\rm F}a)\/a^2$ up to corrections of order $O(a^0)$,\nwith a positive coefficient $\\gamma$ for the viscosity and negative one for the\nthermal conductivity and spin diffusivity. The inclusion of the correction\nlinear in $k_{\\rm F}a$ greatly improves the agreement with the recent\nmeasurement of the viscosity by the Yale group.",
        "The description of weakly bound nuclei using deformed few-body models has\nproven to be crucial in the study of reactions involving certain exotic nuclei.\nHowever, these core+valence models face the challenge of applying the Pauli\nexclusion principle, since the factorisation of the system does not allow\ncomplete antisymmetrization. Therefore, states occupied by core nucleons should\nbe blocked for the valence nucleons. We aim to study $^{17}$C and $^{19}$C,\nwhich are good examples of weakly bound exotic nuclei with significant\ndeformation where the valence shell is partially filled. The structure of\n$^{17}$C and $^{19}$C is described with deformed two-body models where a\nNilsson Hamiltonian is constructed using Antisymmetrized Molecular Dynamic\ncalculations of the cores. Different methods of blocking occupied Nilsson\nstates are considered using the Bardeen$-$Cooper$-$Schrieffer formalism:\nwithout blocking, total blocking and partial blocking. The latter also takes\ninto account pair correlations to some extent. These models are later used to\nstudy $^{16}$C$(d,p)^{17}$C, $^{17}$C$(p,d)^{16}$C and $^{18}$C$(d,p)^{19}$C\ntransfer reactions within the Adiabatic Distorted Wave Approximation. In the\nfirst case, the results are compared with experimental data. A good\nreproduction of the structure of $^{17}$C is found, significantly improving the\nagreement in the $^{16}$C$(d,p)^{17}$C reaction including blocking effects. The\n$^{19}$C spectrum is better reproduced considering blocking, in particular, the\npartial blocking method that considers the pairing interaction provides the\nbest description. Promising results are shown for the study of transfer\nreactions involving weakly bound exotic nuclei, by highlighting the effect of\nblocking occupied Nilsson states. We envision to extend the models to the study\nof breakup reactions and to newly discovered halo nuclei.",
        "In this paper, we establish the modified concavity inequality for complex\nHessian equations\n  under the semi-convexity assumption inspired by Lu \\cite{Lu23} and Zhang\n\\cite{Z24} for real case. Then second order estimates for admissible solutions\nof complex Hessian equations on compact Hermitian manifolds with both sides of\nequations depending on gradient terms are obtained by taking advantage of the\ncrucial inequality.",
        "We reveal a nonlinear magnetic dynamo in a Taylor-Couette flow at small\nmagnetic Prandtl numbers $Pm\\leq 1$, which has been previously believed to\nexist only at higher $Pm\\gtrsim 10$ in this flow. Both the amplitude of initial\nperturbations and $Pm$ play a critical role in its onset and evolution. It is\nshown that this dynamo exists in two main states -- a weak state dominated by\nlarge-scale modes and a strong, more turbulent state with higher amplitude\ndominated by small-scale modes. These findings can be important for dynamo\nprocesses in many astrophysical settings with small $Pm$.",
        "In this article, we present new generalizations of logarithmic convergence\ntests for number series, from which we will derive various new generalizations\nof the Jamet's convergence test. Further, similarly, on the basis of the\ngeneralizations of the Schlomilch's test we found, we will obtain modified\ntests of the convergence of number series.",
        "Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems.",
        "We present a novel method, Fractal Space-Curve Analysis (FSCA), which\ncombines Space-Filling Curve (SFC) mapping for dimensionality reduction with\nfractal Detrended Fluctuation Analysis (DFA). The method is suitable for\nmultidimensional geometrically embedded data, especially for neuroimaging data\nwhich is highly correlated temporally and spatially. We conduct extensive\nfeasibility studies on diverse, artificially generated data with known fractal\ncharacteristics: the fractional Brownian motion, Cantor sets, and Gaussian\nprocesses. We compare the suitability of dimensionality reduction via Hilbert\nSFC and a data-driven alternative. FSCA is then successfully applied to\nreal-world magnetic resonance imaging (MRI) and functional MRI (fMRI) scans.\n  The method utilizing Hilbert curves is optimized for computational\nefficiency, proven robust against boundary effects typical in experimental data\nanalysis, and resistant to data sub-sampling. It is able to correctly quantify\nand discern correlations in both stationary and dynamic two-dimensional images.\nIn MRI Alzheimer's dataset, patients reveal a progression of the disease\nassociated with a systematic decrease of the Hurst exponent. In fMRI recording\nof breath-holding task, the change in the exponent allows distinguishing\ndifferent experimental phases.\n  This study introduces a robust method for fractal characterization of spatial\nand temporal correlations in many types of multidimensional neuroimaging data.\nVery few assumptions allow it to be generalized to more dimensions than typical\nfor neuroimaging and utilized in other scientific fields. The method can be\nparticularly useful in analyzing fMRI experiments to compute markers of\npathological conditions resulting from neurodegeneration. We also showcase its\npotential for providing insights into brain dynamics in task-related\nexperiments.",
        "We establish an explicit uniform a priori estimate for weak solutions to\nslightly subcritical elliptic problems with nonlinearities simultaneously at\nthe interior and on the boundary. Our explicit $L^{\\infty}(\\Omega )$ a priori\nestimates are in terms of powers of their $H^{1}(\\Omega )$ norms. To prove our\nresult, we combine a De Giorgi-Nash-Moser's iteration scheme together with\nelliptic regularity and the Gagliardo-Nirenberg's interpolation inequality."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b33",
    "start_title":"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network",
    "start_abstract":"We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Multimodal Prescriptive Deep Learning",
        "Formation of Frustrated Charge Density Waves in Kagome Metal\n  LuNb$_6$Sn$_6$",
        "Tautological characteristic classes III: the Witt class for PSL(2)",
        "Exploring the Growth-Index ($\\gamma$) Tension with $\\Lambda_{\\rm s}$CDM",
        "Extremal Betti Numbers and Persistence in Flag Complexes",
        "Disk reflection and energetics from the accreting millisecond pulsar\n  SRGA J144459.2-604207",
        "A faithful action of Gal($\\overline{\\mathbb{Q}}\/\\mathbb{Q}$) on Zariski\n  multiplets",
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Curves on Endo-Pajitnov manifolds",
        "Integral gains for non-autonomous Wazewski systems",
        "Removal of excess iron by annealing processes and emergence of bulk\n  superconductivity in sulfur-substituted FeTe",
        "A Response to Recent Critiques of Hainmueller, Mummolo and Xu (2019) on\n  Estimating Conditional Relationships",
        "Electric Power Enhancement using Spin-Polarized Fuel in Fusion Power\n  Plants",
        "Equidistribution of saddle periodic points for H\\'enon-like maps",
        "Determination and evaluation of the critical liquid nitrogen for\n  superconducting levitator based on a novel temperature-weight coupling\n  measurement device",
        "Normalized Solutions for nonlinear Schr\\\"{o}dinger-Poisson equations\n  involving nearly mass-critical exponents",
        "Balance Laws and Transport Theorems for Flows with Singular Interfaces",
        "Line Edge Roughness Effects on the Thermoelectric Properties of Armchair\n  Black Phosphorene Nanoribbons",
        "TT-LSQR For Tensor Least Squares Problems and Application to Data Mining\n  *",
        "Comparison principles and asymptotic behavior of delayed age-structured\n  neuron models",
        "Two-Component Theory of Classical Gravitons with Arbitrary Sources",
        "Transport coefficients of a low-temperature normal Fermi gas with\n  contact interactions: an exact perturbative expansion",
        "Pauli Blocking effects in Nilsson states of weakly bound exotic nuclei",
        "Second order estimates for $\\chi$-semi convex solutions of Hessian\n  equations on Hermitian manifolds",
        "Strong and weak dynamo regimes in Taylor-Couette flows",
        "Generalization of Jamet's test for convergence of number series and its\n  new modifications",
        "Rational Tuning of LLM Cascades via Probabilistic Modeling",
        "Using Space-Filling Curves and Fractals to Reveal Spatial and Temporal\n  Patterns in Neuroimaging Data",
        "Uniform estimates for elliptic equations with Carath\\'eodory\n  nonlinearities at the interior and on the boundary"
      ],
      "abstract":[
        "We introduce a multimodal deep learning framework, Prescriptive Neural\nNetworks (PNNs), that combines ideas from optimization and machine learning,\nand is, to the best of our knowledge, the first prescriptive method to handle\nmultimodal data. The PNN is a feedforward neural network trained on embeddings\nto output an outcome-optimizing prescription. In two real-world multimodal\ndatasets, we demonstrate that PNNs prescribe treatments that are able to\nsignificantly improve estimated outcomes in transcatheter aortic valve\nreplacement (TAVR) procedures by reducing estimated postoperative complication\nrates by 32% and in liver trauma injuries by reducing estimated mortality rates\nby over 40%. In four real-world, unimodal tabular datasets, we demonstrate that\nPNNs outperform or perform comparably to other well-known, state-of-the-art\nprescriptive models; importantly, on tabular datasets, we also recover\ninterpretability through knowledge distillation, fitting interpretable Optimal\nClassification Tree models onto the PNN prescriptions as classification\ntargets, which is critical for many real-world applications. Finally, we\ndemonstrate that our multimodal PNN models achieve stability across randomized\ndata splits comparable to other prescriptive methods and produce realistic\nprescriptions across the different datasets.",
        "The charge density wave (CDW), a translational symmetry breaking electronic\nliquid, plays a pivotal role in correlated quantum materials, such as\nhigh-T$_c$ superconductors and topological semimetals. Recently, CDWs that\npossibly intertwine with superconductivity and magnetism are observed in\nvarious kagome metals. However, the nature of CDWs and the role of the Fermi\nsurface (FS) topology in these materials remain an unresolved challenge. In\nthis letter, we reveal the formation of CDWs in the newly discovered kagome\nmetal LuNb$_6$Sn$_6$. We observe a precursor CDW correlation that features a\n\"yield sign\"-like hollow triangle diffuse scattering pattern and nearly\ncomplete softening of a flat optical phonon band near Q$_H$=(1\/3, 1\/3, 1\/2).\nThe scattering intensity of the precursor CDW displays divergent behavior as\ndecreasing temperature until T$_{CDW}$=70 K, where a competing CDW at\nQ$_{CDW}$=(1\/3, 1\/3, 1\/3) emerges. Using scanning tunneling\nmicroscopy\/spectroscopy, we image the frustrated CDW patterns that show a short\nphase coherence length about ~20 nm in real space. Combined with first\nprinciples calculations, our observations support frustrated lattice\ninteractions that are modulated by the FS topology. These results shed light on\nthe interplay between FS and CDW in quantum materials with nearly degenerate\nstructural deformation patterns.",
        "We explain the relation between the Witt class and the universal\nequicommutative class for PSL(2,K). We discuss an analogue of the Milnor-Wood\ninequality.",
        "Recent observational analyses have revealed a significant tension in the\ngrowth index $\\gamma$, which characterizes the growth rate of cosmic\nstructures. Specifically, when treating $\\gamma$ as a free parameter within\n$\\Lambda$CDM framework, a combination of Planck and $ f\\sigma_8 $ data yields\n$\\gamma \\approx 0.64$, in $\\sim4\\sigma$ tension with the theoretically expected\nvalue $\\gamma \\approx 0.55$ (assuming general relativity). This discrepancy,\nclosely related to the $ S_8 $ tension, poses a new challenge to the standard\ncosmological model by suggesting that it predicts an excessive growth of\nstructure. In this work, we demonstrate that the $\\Lambda_{\\rm s}$CDM framework\n(featuring a rapid sign-switching cosmological constant (mirror AdS-to-dS\ntransition) in the late universe at redshift $ z_\\dagger \\sim 2 $) can\nsimultaneously alleviate the $ \\gamma $, $ H_0 $, and $ S_8 $ tensions. We also\nexamined a scenario with fixed $ z_\\dagger = 1.7 $, previously identified as a\nsweet spot for alleviating multiple major cosmological tensions (including\nthose in $ H_0 $, $ M_B $, and $ S_8 $) finding that it completely eliminates\nboth the $ \\gamma $ and $ H_0 $ tensions, although it is statistically\ndisfavored by our dataset combinations. Our findings suggest that $\\Lambda_{\\rm\ns}$CDM is a promising model, providing a potential unified resolution to\nmultiple major cosmological tensions.",
        "We investigate several problems concerning extremal Betti numbers and\npersistence in filtrations of flag complexes. For graphs on $n$ vertices, we\nshow that $\\beta_k(X(G))$ is maximal when $G=\\mathcal{T}_{n,k+1}$, the Tur\\'an\ngraph on $k+1$ partition classes, where $X(G)$ denotes the flag complex of $G$.\nBuilding on this, we construct an edgewise (one edge at a time) filtration\n$\\mathcal{G}=G_1\\subseteq \\cdots \\subseteq \\mathcal{T}_{n,k+1}$ for which\n$\\beta_k(X(G_i))$ is maximal for all graphs on $n$ vertices and $i$ edges.\nMoreover, the persistence barcode $\\mathcal{B}_k(X(G))$ achieves a maximal\nnumber of intervals, and total persistence, among all edgewise filtrations with\n$|E(\\mathcal{T}_{n,k+1})|$ edges.\n  For $k=1$, we consider edgewise filtrations of the complete graph $K_n$. We\nshow that the maximal number of intervals in the persistence barcode is\nobtained precisely when $G_{\\lceil n\/2\\rceil \\cdot \\lfloor n\/2\n\\rfloor}=\\mathcal{T}_{n,2}$. Among such filtrations, we characterize those\nachieving maximal total persistence. We further show that no filtration can\noptimize $\\beta_1(X(G_i))$ for all $i$, and conjecture that our filtrations\nmaximize the total persistence over all edgewise filtrations of $K_n$.",
        "Accreting millisecond pulsars (AMSPs) are excellent laboratories to study\nreflection spectra and their features from an accretion disk truncated by a\nrapidly rotating magnetosphere near the neutron star surface. These systems\nalso exhibit thermonuclear (type-I) bursts that can provide insights on the\naccretion physics and fuel composition. We explore spectral properties of the\nAMSP SRGA J144459.2-0604207 observed during the outburst that recently led to\nits discovery in February 2024. We aim to characterize the spectral shape of\nthe persistent emission, both its continuum and discrete features, and to\nanalyze type-I bursts properties. We employ XMM and NuSTAR overlapping\nobservations taken during the most recent outburst from SRGA J1444. We perform\nspectral analysis of the persistent (i.e., non-bursting) emission employing a\nsemi-phenomenological continuum model composed of a dominant thermal\nComptonization plus two thermal contributions, and a physical reflection model.\nWe also perform time-resolved spectral analysis of a type-I burst employing a\nblackbody model. We observe a broadened iron emission line, thus suggesting\nrelativistic effects, supported by the physical model accounting for\nrelativistically blurred reflection. The resulting accretion disk extends down\nto 6 gravitational radii, inclined at ~$53^{\\circ}$, and only moderately\nionized (log$\\xi\\simeq2.3$). We observe an absorption edge at ~9.7 keV that can\nbe interpreted as an Fe XXVI edge blueshifted by an ultrafast ($\\simeq0.04$c)\noutflow. Our broadband observations of type-I bursts do not find evidence of\nphotospheric radius expansion. The burst recurrence time shows a dependence on\nthe count rate with the steepest slope ever observed in these systems. We also\nobserve a discrepancy of ~3 between the observed and expected burst recurrence\ntime, which we discuss in the framework of fuel composition and high NS mass\nscenarios.",
        "In this work, we establish two main results in the context of arithmetic and\ngeometric properties of plane curves. First, we construct numerous new examples\nof arithmetic Zariski pairs and multiplets, where only a few ones were\npreviously available. Second, we describe a faithful action of the absolute\nGalois group on the equisingular strata of plane curves, providing insights\ninto the interplay between Galois representations and the geometry of singular\nplane curves. We conclude the paper with very concretes examples of the general\nresults obtained.",
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "Endo--Pajitnov manifolds are generalizations to higher dimensions of the\nInoue surfaces $S^M$. We study the existence of complex submanifolds in\nEndo--Pajitnov manifolds. We identify a class of these manifolds that do\ncontain compact complex submanifolds and establish an algebraic condition under\nwhich an Endo--Pajitnov manifold contains no compact complex curves.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "There are several strategies to discover new superconductors. Growing new\nmaterials and applying high pressures can be the classic ways since\nsuperconductivity was found. Also, chemical processing, such as annealing, is\nanother way to induce superconductivity in a non-superconducting material.\nHere, we show chemical processing effects in the non-superconducting material,\nsulfur-substituted FeTe. It has been known that superconductivity in\nS-substituted FeTe is induced by O$_2$ annealing. We revealed that hydrochloric\nacid etching and vacuum annealing for O$_2$-annealed samples made the quality\nof superconductivity higher by several physical property measurements.\nFurthermore, we visualized the superconducting regions by a magneto-optical\nimaging technique, indicating that the superconductivity in the processed\nsample was bulk. In this sample, we confirmed that the concentration of excess\niron was reduced compared to that in the as-grown state. These results provide\nan important route to bulk superconductivity in S-substituted FeTe and its\nrelated iron-based compounds.",
        "Simonsohn (2024a) and Simonsohn (2024b) critique Hainmueller, Mummolo and Xu\n(2019, HMX), arguing that failing to model nonlinear relationships between the\ntreatment and moderator leads to biased marginal effect estimates and\nuncontrolled Type-I error rates. While these critiques highlight the issue of\nunder-modeling nonlinearity in applied research, they are fundamentally flawed\nin several key ways. First, the causal estimand for interaction effects and the\nnecessary identifying assumptions are not clearly defined in these critiques.\nOnce properly stated, the critiques no longer hold. Second, the kernel\nestimator HMX proposes recovers the true causal effects in the scenarios\npresented in these recent critiques, which compared effects to the wrong\nbenchmark, producing misleading conclusions. Third, while Generalized Additive\nModels (GAM) can be a useful exploratory tool (as acknowledged in HMX), they\nare not designed to estimate marginal effects, and better alternatives exist,\nparticularly in the presence of additional covariates. Our response aims to\nclarify these misconceptions and provide updated recommendations for\nresearchers studying interaction effects through the estimation of conditional\nmarginal effects.",
        "Using a range of fusion power plant (FPP) concepts, we demonstrate that\nspin-polarized fuel (SPF) can significantly enhance net electric power output,\noften by many multiples. Notably, the electric power gain from SPF generally\nexceeds the corresponding increase in thermal fusion power. Plants close to\nengineering breakeven stand to benefit most, where even modest boosts in fusion\npower produce disproportionately larger gains in net electricity. As a\nrepresentative example, a 25% increase in fusion power via SPF could allow an\nITER-like device (with an added turbine to recover thermal fusion power) to\nachieve engineering breakeven. These findings strongly motivate the development\nof spin-polarized fuel for FPPs.",
        "We prove that under the natural assumption over the dynamical degrees, the\nsaddle periodic points of a H\\'enon-like map in any dimension equidistribute\nwith respect to the equilibrium measure. Our work is a generalization of\nresults of Bedford-Lyubich-Smillie, Dujardin and Dinh-Sibony along with\nimprovements of their techniques. We also investigate some fine properties of\nGreen currents associated with the map. On the pluripotential-theory side, in\nour non-compact setting, the wedge product of two positive closed currents of\ncomplementary bi-degrees can be defined using super-potentials and the density\ntheory. We prove that these two definitions are coherent.",
        "Liquid nitrogen (LN2) is the only cooling medium for the high-temperature\nsuperconducting (HTS) bulks in the superconducting levitator, which is the\nheart of the maglev train, to reach working state. The detection and\ndetermination of the critical LN2 content are crucial for reliable operation of\nthe HTS maglev train. However, the related intelligent detection model and\ntechnology is lack in the combination filed of the cryogenic environment and\nmaglev application, and there is no existing method to detect the LN2 content\nin superconducting levitator. This paper proposes to employ multisensor fusion\nframework to fuse and enhance the accuracy of critical LN2 content testing.\nFour temperature sensors were deployed inside superconducting levitator to\nmeasure the temperature change during the LN2 content changing from 100 % to 0.\nIt was first obtained that the critical LN2 content in the superconducting\nlevitator is 4%. To accurately monitor the critical LN2 content in the\nsuperconducting levitator, a matrix-weighted information fusion Kalman filter\nalgorithm was used. Compared with the previous single sensor method, the\ntesting accuracy of the multisensor fusion method can be improved by 5.6%. The\nwork can provide a preliminary research foundation for the online monitoring\nand fault diagnosis of HTS maglev train.",
        "We study the Schr\\\"{o}dinger-Poisson-Slater equation\n\\begin{equation*}\\left\\{\\begin{array}{lll}\n  -\\Delta u + \\lambda u + \\big(|x|^{-1} \\ast |u|^{2}\\big)u = V(x) u^{\np_{\\varepsilon}-1 }, \\, \\text{ in } \\mathbb{R}^{3},\\\\[2mm]\n  \\int_{\\mathbb{R}^3}u^2 \\,dx= a,\\,\\, u > 0,\\,\\, u \\in H^{1}(\\mathbb{R}^{3}),\n  \\end{array}\n  \\right. \\end{equation*} where $\\lambda$ is a Lagrange multiplier, $V(x)$ is a\nreal-valued potential, $a\\in \\mathbb{R}_{+}$ is a constant, $ p_{\\varepsilon} =\n\\frac{10}{3} \\pm \\varepsilon$ and $\\varepsilon>0$ is a small parameter. In this\npaper, we prove that it is the positive critical value of the potential $V$\nthat affects the existence of single-peak solutions for this problem.\nFurthermore, we prove the local uniqueness of the solutions we construct.",
        "This paper gives a concise but rigorous mathematical description of a\nmaterial control volume that is separated into two parts by a singular surface\nat which physical states are discontinuous. The geometrical background material\nis summarized in a unified manner. Transport theorems for use in generic\nbalance laws are given with proofs since they provide some insight into the\nresults. Also the step from integral balances to differential equations is\ngiven in some detail.",
        "This study delves into the thermoelectric properties of armchair black\nphosphorene nanoribbons while considering the presence of line edge roughness.\nEmploying the tight-binding method in conjunction with non-equilibrium Green's\nfunction techniques and Landauer formulas, we explore the impact of various\nparameters on thermoelectric performance. Our findings reveal that the\nelectrical conductivity and, consequently, the power factor exhibit an\nincreasing trend with expanding ribbon length and width. This behavior can be\nattributed to heightened collision rates, particularly in narrow ribbons,\ninduced by line edge roughness as length increases. Remarkably, the Seebeck\ncoefficient at the Fermi energy corresponding to the maximum power factor\nremains nearly constant across different widths, lengths, temperatures and\ntransport regimes. Furthermore, the thermoelectric figure of merit demonstrates\na positive correlation with both ribbon length and width. In narrow widths and\nlengths around 1000 nm, the power factor and figure of merit exhibit an upward\ntrend with ribbon width. However, with further increases in ribbon width, the\ninfluence of line edge roughness on thermal conductivity diminishes.\nConsequently, the figure of merit decreases due to the rise in thermal\nconductivity. Notably, the thermoelectric figure of merit is higher for short\nand narrow ribbons and long and wider ribbons.",
        "We are interested in the numerical solution of the tensor least squares\nproblem \\[\n  \\min_{\\mathcal{X}} \\| \\mathcal{F} - \\sum_{i =1}^{\\ell} \\mathcal{X} \\times_1\nA_1^{(i)} \\times_2 A_2^{(i)} \\cdots \\times_d A_d^{(i)} \\|_F, \\] where\n$\\mathcal{X}\\in\\mathbb{R}^{m_1 \\times m_2 \\times \\cdots \\times m_d}$,\n$\\mathcal{F}\\in\\mathbb{R}^{n_1\\times n_2 \\times \\cdots \\times n_d}$ are tensors\nwith $d$ dimensions, and the coefficients $A_j^{(i)}$ are tall matrices of\nconforming dimensions. We first describe a tensor implementation of the\nclassical LSQR method by Paige and Saunders, using the tensor-train\nrepresentation as key ingredient. We also show how to incorporate sketching to\nlower the computational cost of dealing with the tall matrices $A_j^{(i)}$. We\nthen use this methodology to address a problem in information retrieval, the\nclassification of a new query document among already categorized documents,\naccording to given keywords.",
        "In the context of neuroscience the elapsed-time model is an age-structured\nequation that describes the behavior of interconnected spiking neurons through\nthe time since the last discharge, with many interesting dynamics depending on\nthe type of interactions between neurons. We investigate the asymptotic\nbehavior of this equation in the case of both discrete and distributed delays\nthat account for the time needed to transmit a nerve impulse from one neuron to\nthe rest the ensemble. To prove the convergence to the equilibrium, we follow\nan approach based on comparison principles for Volterra equations involving the\ntotal activity, which provides a simpler and more straightforward alternative\ntechnique than those in the existing literature on the elapsed-time model.",
        "A review of the spin curvatures that arise in the framework of the Infeld-van\nder Waerden {\\epsilon}-formalism for general relativity is presented. The\nsourceful extension formulated in an earlier work of the theory of gravitons in\nvacuum is then reviewed in a concise way. At this stage the\n{\\epsilon}-formalism system of gravitational wave equations obtained thereabout\nis replicated on the basis of the implementation of the same calculational\ntechniques as the ones that had been utilized in the sourceless situation. An\nimportant symmetry property borne by the right-hand sides of those wave\nequations is subsequently brought forward.",
        "We compute the shear viscosity, thermal conductivity and spin diffusivity of\na Fermi gas with short-range interactions in the Fermi liquid regime of the\nnormal phase, that is at temperatures $T$ much lower than the Fermi temperature\n$T_{\\rm F}$ and much larger than the superfluid critical temperature $T_c$.\nGiven recent advances in the precision of cold atom experiments, we provide\nexact results up to second-order in the interaction strength. We extend the\nLandau-Salpeter equation to compute the collision amplitude beyond the\nforward-scattering limit, covering all collisions on the Fermi surface. We\ntreat the collision kernel exactly, leading to significant corrections beyond\nrelaxation time or variational approximations. The transport coefficients, as\nfunctions of the $s$-wave scattering length $a$ and Fermi wavenumber $k_{\\rm\nF}$, follow $(1+\\gamma k_{\\rm F}a)\/a^2$ up to corrections of order $O(a^0)$,\nwith a positive coefficient $\\gamma$ for the viscosity and negative one for the\nthermal conductivity and spin diffusivity. The inclusion of the correction\nlinear in $k_{\\rm F}a$ greatly improves the agreement with the recent\nmeasurement of the viscosity by the Yale group.",
        "The description of weakly bound nuclei using deformed few-body models has\nproven to be crucial in the study of reactions involving certain exotic nuclei.\nHowever, these core+valence models face the challenge of applying the Pauli\nexclusion principle, since the factorisation of the system does not allow\ncomplete antisymmetrization. Therefore, states occupied by core nucleons should\nbe blocked for the valence nucleons. We aim to study $^{17}$C and $^{19}$C,\nwhich are good examples of weakly bound exotic nuclei with significant\ndeformation where the valence shell is partially filled. The structure of\n$^{17}$C and $^{19}$C is described with deformed two-body models where a\nNilsson Hamiltonian is constructed using Antisymmetrized Molecular Dynamic\ncalculations of the cores. Different methods of blocking occupied Nilsson\nstates are considered using the Bardeen$-$Cooper$-$Schrieffer formalism:\nwithout blocking, total blocking and partial blocking. The latter also takes\ninto account pair correlations to some extent. These models are later used to\nstudy $^{16}$C$(d,p)^{17}$C, $^{17}$C$(p,d)^{16}$C and $^{18}$C$(d,p)^{19}$C\ntransfer reactions within the Adiabatic Distorted Wave Approximation. In the\nfirst case, the results are compared with experimental data. A good\nreproduction of the structure of $^{17}$C is found, significantly improving the\nagreement in the $^{16}$C$(d,p)^{17}$C reaction including blocking effects. The\n$^{19}$C spectrum is better reproduced considering blocking, in particular, the\npartial blocking method that considers the pairing interaction provides the\nbest description. Promising results are shown for the study of transfer\nreactions involving weakly bound exotic nuclei, by highlighting the effect of\nblocking occupied Nilsson states. We envision to extend the models to the study\nof breakup reactions and to newly discovered halo nuclei.",
        "In this paper, we establish the modified concavity inequality for complex\nHessian equations\n  under the semi-convexity assumption inspired by Lu \\cite{Lu23} and Zhang\n\\cite{Z24} for real case. Then second order estimates for admissible solutions\nof complex Hessian equations on compact Hermitian manifolds with both sides of\nequations depending on gradient terms are obtained by taking advantage of the\ncrucial inequality.",
        "We reveal a nonlinear magnetic dynamo in a Taylor-Couette flow at small\nmagnetic Prandtl numbers $Pm\\leq 1$, which has been previously believed to\nexist only at higher $Pm\\gtrsim 10$ in this flow. Both the amplitude of initial\nperturbations and $Pm$ play a critical role in its onset and evolution. It is\nshown that this dynamo exists in two main states -- a weak state dominated by\nlarge-scale modes and a strong, more turbulent state with higher amplitude\ndominated by small-scale modes. These findings can be important for dynamo\nprocesses in many astrophysical settings with small $Pm$.",
        "In this article, we present new generalizations of logarithmic convergence\ntests for number series, from which we will derive various new generalizations\nof the Jamet's convergence test. Further, similarly, on the basis of the\ngeneralizations of the Schlomilch's test we found, we will obtain modified\ntests of the convergence of number series.",
        "Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems.",
        "We present a novel method, Fractal Space-Curve Analysis (FSCA), which\ncombines Space-Filling Curve (SFC) mapping for dimensionality reduction with\nfractal Detrended Fluctuation Analysis (DFA). The method is suitable for\nmultidimensional geometrically embedded data, especially for neuroimaging data\nwhich is highly correlated temporally and spatially. We conduct extensive\nfeasibility studies on diverse, artificially generated data with known fractal\ncharacteristics: the fractional Brownian motion, Cantor sets, and Gaussian\nprocesses. We compare the suitability of dimensionality reduction via Hilbert\nSFC and a data-driven alternative. FSCA is then successfully applied to\nreal-world magnetic resonance imaging (MRI) and functional MRI (fMRI) scans.\n  The method utilizing Hilbert curves is optimized for computational\nefficiency, proven robust against boundary effects typical in experimental data\nanalysis, and resistant to data sub-sampling. It is able to correctly quantify\nand discern correlations in both stationary and dynamic two-dimensional images.\nIn MRI Alzheimer's dataset, patients reveal a progression of the disease\nassociated with a systematic decrease of the Hurst exponent. In fMRI recording\nof breath-holding task, the change in the exponent allows distinguishing\ndifferent experimental phases.\n  This study introduces a robust method for fractal characterization of spatial\nand temporal correlations in many types of multidimensional neuroimaging data.\nVery few assumptions allow it to be generalized to more dimensions than typical\nfor neuroimaging and utilized in other scientific fields. The method can be\nparticularly useful in analyzing fMRI experiments to compute markers of\npathological conditions resulting from neurodegeneration. We also showcase its\npotential for providing insights into brain dynamics in task-related\nexperiments.",
        "We establish an explicit uniform a priori estimate for weak solutions to\nslightly subcritical elliptic problems with nonlinearities simultaneously at\nthe interior and on the boundary. Our explicit $L^{\\infty}(\\Omega )$ a priori\nestimates are in terms of powers of their $H^{1}(\\Omega )$ norms. To prove our\nresult, we combine a De Giorgi-Nash-Moser's iteration scheme together with\nelliptic regularity and the Gagliardo-Nirenberg's interpolation inequality."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation",
    "start_abstract":"In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b36",
        "b33"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
        "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
      ],
      "categories":[
        "cs.CV",
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Irreducibility results for equivariant $\\mathcal{D}$-modules on rigid\n  analytic spaces",
        "Benign Overfitting and the Geometry of the Ridge Regression Solution in\n  Binary Classification",
        "A Chain-Driven, Sandwich-Legged Quadruped Robot: Design and Experimental\n  Analysis",
        "TNCSE: Tensor's Norm Constraints for Unsupervised Contrastive Learning\n  of Sentence Embeddings",
        "Bridging Contrastive Learning and Domain Adaptation: Theoretical\n  Perspective and Practical Application",
        "Is excess smoothing of Planck CMB ansiotropy data partially responsible\n  for evidence for dark energy dynamics in other $w(z)$CDM parametrizations?",
        "Generic uniqueness and conjugate points for optimal control problems",
        "Explicit Formulas for the Alexander Polynomial of Pretzel Knots",
        "MLLM4PUE: Toward Universal Embeddings in Digital Pathology through\n  Multimodal LLMs",
        "Dispider: Enabling Video LLMs with Active Real-Time Interaction via\n  Disentangled Perception, Decision, and Reaction",
        "Large Language Models for Video Surveillance Applications",
        "Now you see me! A framework for obtaining class-relevant saliency maps",
        "The two extremal rays of some Hyper-K\\\"ahler fourfolds",
        "Orthogonal projection-based regularization for efficient model\n  augmentation",
        "Can Bayesian Neural Networks Make Confident Predictions?",
        "Uncertainty-Aware Decoding with Minimum Bayes Risk",
        "On Some Hereditary and Super Classes of Directly Finite Abelian Groups",
        "Rule-Based Conflict-Free Decision Framework in Swarm Confrontation",
        "Research on Research Visibility",
        "Point-group symmetry enriched topological orders",
        "Fast Jet Tagging with MLP-Mixers on FPGAs",
        "SAR models with specific spatial coefficients and heteroskedastic\n  innovations",
        "On Min-Max Robust Data-Driven Predictive Control Considering Non-Unique\n  Solutions to Behavioral Representation",
        "User Profile with Large Language Models: Construction, Updating, and\n  Benchmarking",
        "Temperature Dependence of Beam on Plasma Stopping Power in the Resonance\n  Regions of Fusion Reactions",
        "You Do Not Fully Utilize Transformer's Representation Capacity",
        "Introducing JIRIAF: A Virtual Kubelet Integration for Optimizing HPC\n  Resource Provisioning",
        "Discovery and Multi-Wavelength Analysis of a New Dissociative Galaxy\n  Merger: The Champagne Cluster"
      ],
      "abstract":[
        "We prove a general irreducibility result for geometrically induced\ncoadmissible equivariant $\\mathcal{D}$-modules on rigid analytic spaces. As an\napplication, we geometrically reprove the irreducibility of certain locally\nanalytic representations previously constructed by Orlik-Strauch.",
        "In this work, we investigate the behavior of ridge regression in an\noverparameterized binary classification task. We assume examples are drawn from\n(anisotropic) class-conditional cluster distributions with opposing means and\nwe allow for the training labels to have a constant level of label-flipping\nnoise. We characterize the classification error achieved by ridge regression\nunder the assumption that the covariance matrix of the cluster distribution has\na high effective rank in the tail. We show that ridge regression has\nqualitatively different behavior depending on the scale of the cluster mean\nvector and its interaction with the covariance matrix of the cluster\ndistributions. In regimes where the scale is very large, the conditions that\nallow for benign overfitting turn out to be the same as those for the\nregression task. We additionally provide insights into how the introduction of\nlabel noise affects the behavior of the minimum norm interpolator (MNI). The\noptimal classifier in this setting is a linear transformation of the cluster\nmean vector and in the noiseless setting the MNI approximately learns this\ntransformation. On the other hand, the introduction of label noise can\nsignificantly change the geometry of the solution while preserving the same\nqualitative behavior.",
        "This paper introduces a chain-driven, sandwich-legged, mid-size quadruped\nrobot designed as an accessible research platform. The design prioritizes\nenhanced locomotion capabilities, improved reliability and safety of the\nactuation system, and simplified, cost-effective manufacturing processes.\nLocomotion performance is optimized through a sandwiched leg design and a\ndual-motor configuration, reducing leg inertia for agile movements. Reliability\nand safety are achieved by integrating robust cable strain reliefs, efficient\nheat sinks for motor thermal management, and mechanical limits to restrict leg\nmotion. Simplified design considerations include a quasi-direct drive (QDD)\nactuator and the adoption of low-cost fabrication techniques, such as laser\ncutting and 3D printing, to minimize cost and ensure rapid prototyping. The\nrobot weighs approximately 25 kg and is developed at a cost under \\$8000,\nmaking it a scalable and affordable solution for robotics research.\nExperimental validations demonstrate the platform's capability to execute trot\nand crawl gaits on flat terrain and slopes, highlighting its potential as a\nversatile and reliable quadruped research platform.",
        "Unsupervised sentence embedding representation has become a hot research\ntopic in natural language processing. As a tensor, sentence embedding has two\ncritical properties: direction and norm. Existing works have been limited to\nconstraining only the orientation of the samples' representations while\nignoring the features of their module lengths. To address this issue, we\npropose a new training objective that optimizes the training of unsupervised\ncontrastive learning by constraining the module length features between\npositive samples. We combine the training objective of Tensor's Norm\nConstraints with ensemble learning to propose a new Sentence Embedding\nrepresentation framework, TNCSE. We evaluate seven semantic text similarity\ntasks, and the results show that TNCSE and derived models are the current\nstate-of-the-art approach; in addition, we conduct extensive zero-shot\nevaluations, and the results show that TNCSE outperforms other baselines.",
        "This work studies the relationship between Contrastive Learning and Domain\nAdaptation from a theoretical perspective. The two standard contrastive losses,\nNT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to\nthe Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely\nused for Domain Adaptation. Our work shows that minimizing the contrastive\nlosses decreases the CMMD and simultaneously improves class-separability,\nlaying the theoretical groundwork for the use of Contrastive Learning in the\ncontext of Domain Adaptation. Due to the relevance of Domain Adaptation in\nmedical imaging, we focused the experiments on mammography images. Extensive\nexperiments on three mammography datasets - synthetic patches, clinical (real)\npatches, and clinical (real) images - show improved Domain Adaptation,\nclass-separability, and classification performance, when minimizing the\nSupervised Contrastive loss.",
        "We study spatially-flat dynamical dark energy parametrizations, $w(z)$CDM,\nwith redshift-dependent dark energy equation of state parameter $w(z)$\nexpressed using three different quadratic and other polynomial forms (as\nfunctions of $1-a$, where $a$ is the scale factor), without and with a varying\ncosmic microwave background (CMB) lensing consistency parameter $A_L$. We use\nPlanck CMB anisotropy data (P18 and lensing) and a large, mutually-consistent\nnon-CMB data compilation that includes Pantheon+ type Ia supernova, baryon\nacoustic oscillation (BAO), Hubble parameter ($H(z)$), and growth factor\n($f\\sigma_8$) measurements, but not recent DESI BAO data. The six $w(z)$CDM\n($+A_L$) parametrizations show higher consistency between the CMB and non-CMB\ndata constraints compared to the XCDM ($+A_L$) and $w_0 w_a$CDM ($+A_L$) cases.\nConstraints from the most-restrictive P18+lensing+non-CMB data compilation on\nthe six $w(z)$CDM ($+A_L$) parametrizations indicate that dark energy dynamics\nis favored over a cosmological constant by $\\gtrsim 2\\sigma$ when $A_L = 1$,\nbut only by $\\gtrsim 1\\sigma$ when $A_L$ is allowed to vary (and $A_L>1$ at\n$\\sim2\\sigma$ significance). Non-CMB data dominate the P18+lensing+non-CMB\ncompilation at low $z$ and favor quintessence-like dark energy. At high $z$\nP18+lensing data dominate, favoring phantom-like dark energy with significance\nfrom $1.5\\sigma$ to $2.9 \\sigma$ when $A_L = 1$, and from $1.1\\sigma$ to\n$1.8\\sigma$ when $A_L$ varies. These results suggest that the observed excess\nweak lensing smoothing of some of the Planck CMB anistropy multipoles is\npartially responsible for the $A_L = 1$ cases $\\gtrsim 2\\sigma$ evidence for\ndark energy dynamics over a cosmological constant.",
        "The paper is concerned with an optimal control problem on $\\mathbb{R}^n$,\nwhere the dynamics is linear w.r.t.~the control functions. For a terminal cost\n$\\psi$ in a $mathcal{G}_\\delta$ set of $\\mathcal{C}^4(\\mathbb{R}^n)$ (i.e., in\na countable intersection of open dense subsets), two main results are\nproved.Namely: the set $\\Gamma_\\psi\\subset\\mathbb{R}^n$ of conjugate points is\nclosed, with locally bounded $(n-2)$-dimensional Hausdorff measure. Moreover,\nthe set of initial points $y\\in \\mathbb{R}^n\\setminus\\Gamma_\\psi$, which admit\ntwo or more globally optimal trajectories, is contained in the union of a\nlocally finite family of embedded manifolds. In particular, the value function\nis continuously differentiable on an open, dense subset of $\\mathbb{R}^n$.",
        "We provide explicit formulas for the Alexander polynomial of Pretzel knots\nand establish several immediate corollaries, including the characterization of\nPretzel knots with a trivial Alexander polynomial.",
        "Pathology plays a critical role in diagnosing a wide range of diseases, yet\nexisting approaches often rely heavily on task-specific models trained on\nextensive, well-labeled datasets. These methods face sustainability challenges\ndue to the diversity of pathologies and the labor-intensive nature of data\ncollection. To address these limitations, we highlight the need for universal\nmultimodal embeddings that can support multiple downstream tasks. Previous\napproaches involve fine-tuning CLIP-based models, which handle images and texts\nseparately, limiting their ability to capture complex multimodal relationships.\nAdditionally, these models are evaluated across diverse datasets without a\nunified benchmark. In this paper, we explore the possibility of applying\nMultimodal Large Language Models (MLLMs) to generate pathology universal\nembeddings to address these challenges. Our contributions can be summarized in\nthe following aspects: 1) We propose MLLM4PUE, a novel framework that leverages\nMLLMs to generate embeddings for various pathology downstream tasks. 2) We\nfurther introduce the Pathology Multimodal Embedding Benchmark (PMEB), a\ncomprehensive benchmark designed to assess the quality of pathology multimodal\nembeddings, which comprises 16 original tasks drawn from 15 datasets. 3)\nExtensive experimental results demonstrate the superiority of MLLM4PUE,\nillustrating MLLM-based models can effectively support a wide range of\ndownstream tasks and unify the research direction for foundation models in\npathology.",
        "Active Real-time interaction with video LLMs introduces a new paradigm for\nhuman-computer interaction, where the model not only understands user intent\nbut also responds while continuously processing streaming video on the fly.\nUnlike offline video LLMs, which analyze the entire video before answering\nquestions, active real-time interaction requires three capabilities: 1)\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\nraising proactive interaction in proper situations, 3) Reaction: continuous\ninteraction with users. However, inherent conflicts exist among the desired\ncapabilities. The Decision and Reaction require a contrary Perception scale and\ngrain, and the autoregressive decoding blocks the real-time Perception and\nDecision during the Reaction. To unify the conflicted capabilities within a\nharmonious system, we present Dispider, a system that disentangles Perception,\nDecision, and Reaction. Dispider features a lightweight proactive streaming\nvideo processing module that tracks the video stream and identifies optimal\nmoments for interaction. Once the interaction is triggered, an asynchronous\ninteraction module provides detailed responses, while the processing module\ncontinues to monitor the video in the meantime. Our disentangled and\nasynchronous design ensures timely, contextually accurate, and computationally\nefficient responses, making Dispider ideal for active real-time interaction for\nlong-duration video streams. Experiments show that Dispider not only maintains\nstrong performance in conventional video QA tasks, but also significantly\nsurpasses previous online models in streaming scenario responses, thereby\nvalidating the effectiveness of our architecture. The code and model are\nreleased at \\url{https:\/\/github.com\/Mark12Ding\/Dispider}.",
        "The rapid increase in video content production has resulted in enormous data\nvolumes, creating significant challenges for efficient analysis and resource\nmanagement. To address this, robust video analysis tools are essential. This\npaper presents an innovative proof of concept using Generative Artificial\nIntelligence (GenAI) in the form of Vision Language Models to enhance the\ndownstream video analysis process. Our tool generates customized textual\nsummaries based on user-defined queries, providing focused insights within\nextensive video datasets. Unlike traditional methods that offer generic\nsummaries or limited action recognition, our approach utilizes Vision Language\nModels to extract relevant information, improving analysis precision and\nefficiency. The proposed method produces textual summaries from extensive CCTV\nfootage, which can then be stored for an indefinite time in a very small\nstorage space compared to videos, allowing users to quickly navigate and verify\nsignificant events without exhaustive manual review. Qualitative evaluations\nresult in 80% and 70% accuracy in temporal and spatial quality and consistency\nof the pipeline respectively.",
        "Neural networks are part of daily-life decision-making, including in\nhigh-stakes settings where understanding and transparency are key. Saliency\nmaps have been developed to gain understanding into which input features neural\nnetworks use for a specific prediction. Although widely employed, these methods\noften result in overly general saliency maps that fail to identify the specific\ninformation that triggered the classification. In this work, we suggest a\nframework that allows to incorporate attributions across classes to arrive at\nsaliency maps that actually capture the class-relevant information. On\nestablished benchmarks for attribution methods, including the grid-pointing\ngame and randomization-based sanity checks, we show that our framework heavily\nboosts the performance of standard saliency map approaches. It is, by design,\nagnostic to model architectures and attribution methods and now allows to\nidentify the distinguishing and shared features used for a model prediction.",
        "We consider projective Hyper-K\\\"ahler manifolds of dimension four that are\ndeformation equivalent to Hilbert squares of K3 surfaces. In case such a\nmanifold admits a divisorial contraction, the exceptional divisor is a conic\nbundle over a K3 surface. There are five types of such conic bundles. In case\nthe manifold has Picard rank two and has two (birational) divisorial\ncontractions we determine the types of these conic bundles. There are exactly\nseven cases. For the Fano varieties of cubic fourfolds there are only four\ncases and we provide examples of these.",
        "Deep-learning-based nonlinear system identification has shown the ability to\nproduce reliable and highly accurate models in practice. However, these\nblack-box models lack physical interpretability, and often a considerable part\nof the learning effort is spent on capturing already expected\/known behavior\ndue to first-principles-based understanding of some aspects of the system. A\npotential solution is to integrate prior physical knowledge directly into the\nmodel structure, combining the strengths of physics-based modeling and\ndeep-learning-based identification. The most common approach is to use an\nadditive model augmentation structure, where the physics-based and the\nmachine-learning (ML) components are connected in parallel. However, such\nmodels are overparametrized, training them is challenging, potentially causing\nthe physics-based part to lose interpretability. To overcome this challenge,\nthis paper proposes an orthogonal projection-based regularization technique to\nenhance parameter learning, convergence, and even model accuracy in\nlearning-based augmentation of nonlinear baseline models.",
        "Bayesian inference promises a framework for principled uncertainty\nquantification of neural network predictions. Barriers to adoption include the\ndifficulty of fully characterizing posterior distributions on network\nparameters and the interpretability of posterior predictive distributions. We\ndemonstrate that under a discretized prior for the inner layer weights, we can\nexactly characterize the posterior predictive distribution as a Gaussian\nmixture. This setting allows us to define equivalence classes of network\nparameter values which produce the same likelihood (training error) and to\nrelate the elements of these classes to the network's scaling regime -- defined\nvia ratios of the training sample size, the size of each layer, and the number\nof final layer parameters. Of particular interest are distinct parameter\nrealizations that map to low training error and yet correspond to distinct\nmodes in the posterior predictive distribution. We identify settings that\nexhibit such predictive multimodality, and thus provide insight into the\naccuracy of unimodal posterior approximations. We also characterize the\ncapacity of a model to \"learn from data\" by evaluating contraction of the\nposterior predictive in different scaling regimes.",
        "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
        "Continuing recent studies of both the hereditary and super properties of\ncertain classes of Abelian groups, we explore in-depth what is the situation in\nthe quite large class consisting of directly finite Abelian groups. Trying to\nconnect some of these classes, we specifically succeeded to prove the\nsurprising criteria that a relatively Hopfian group is hereditarily only when\nit is extended Bassian, as well as that, a relatively Hopfian group is super\nonly when it is extended Bassian. In this aspect, additional relevant necessary\nand sufficient conditions in a slightly more general context are also proved.",
        "Traditional rule-based decision-making methods with interpretable advantage,\nsuch as finite state machine, suffer from the jitter or deadlock(JoD) problems\nin extremely dynamic scenarios. To realize agent swarm confrontation, decision\nconflicts causing many JoD problems are a key issue to be solved. Here, we\npropose a novel decision-making framework that integrates probabilistic finite\nstate machine, deep convolutional networks, and reinforcement learning to\nimplement interpretable intelligence into agents. Our framework overcomes state\nmachine instability and JoD problems, ensuring reliable and adaptable decisions\nin swarm confrontation. The proposed approach demonstrates effective\nperformance via enhanced human-like cooperation and competitive strategies in\nthe rigorous evaluation of real experiments, outperforming other methods.",
        "This editorial explores the significance of research visibility within the\nevolving landscape of academic communication, mainly focusing on the role of\nsearch engines as online meta-markets shaping the impact of research. With the\nrapid expansion of scientific output and the increasing reliance on\nalgorithm-driven platforms such as Google and Google Scholar, the online\nvisibility of scholarly work has become an essential factor in determining its\nreach and influence. The need for more rigorous research into academic search\nengine optimization (A-SEO), a field still in its infancy despite its growing\nrelevance, is also discussed, highlighting key challenges in the field,\nincluding the lack of robust research methodologies, the skepticism within the\nacademic community regarding the commercialization of science, and the need for\nstandardization in reporting and measurement techniques. This editorial thus\ninvites a multidisciplinary dialogue on the future of research visibility, with\nsignificant implications for academic publishing, science communication,\nresearch evaluation, and the global scientific ecosystem.",
        "We study the classification of two-dimensional (2D) topological orders\nenriched by point-group symmetries, by generalizing the folding appraoch which\nwas previously developed for mirror-symmetry-enriched topological orders. We\nfold the 2D plane hosting the topological order into the foundamental domain of\nthe group group, which is a sector with an angle $2\\pi\/n$ for the cyclic point\ngroup $C_n$ and a sector with an angle $\\pi\/n$ for the dihedral point group\n$D_{2n}$, and the point-group symmetries becomes onsite unitary symmetries on\nthe sector. The enrichment of the point-group symmetries is then fully encoded\nat the boundary of the sector and the apex of the section, which forms a\njunction between the two boundaries. The mirror-symmetry enrichment encoded on\nthe boundaries is analyzed by the classification theory of symmetric gapped\nboundaries, and the point-group-symmetry enrichment encoded on the junction is\nanalyzed by a framework for classifying symmetric gapped junctions between\nboundaries which we develop in this work. We show that at the junction, there\nare two potential obstructions, which we refer to as an $H^1$ obstruction and\nan $H^2$ obstruction, respectively. When the obstruction vanishes, the\njunction, and therefore the point-group-symmetry-enriched topological orders,\nare classified by an $H^0$ cohomology class and an $H^1$ cohomology class,\nwhich can be understood as an additional Abelian anyon and a symmetry charge\nattached to the rotation center, respectively. These results are consistent\nwith the classification of onsite-symmetry-enriched topological orders, where\nthe $H^1$ and $H^2$ obstructions and the junction corresponds to the $H^3$ and\n$H^4$ obstructions for onsite symmetries, respectively.",
        "We explore the innovative use of MLP-Mixer models for real-time jet tagging\nand establish their feasibility on resource-constrained hardware like FPGAs.\nMLP-Mixers excel in processing sequences of jet constituents, achieving\nstate-of-the-art performance on datasets mimicking Large Hadron Collider\nconditions. By using advanced optimization techniques such as High-Granularity\nQuantization and Distributed Arithmetic, we achieve unprecedented efficiency.\nThese models match or surpass the accuracy of previous architectures, reduce\nhardware resource usage by up to 97%, double the throughput, and half the\nlatency. Additionally, non-permutation-invariant architectures enable smart\nfeature prioritization and efficient FPGA deployment, setting a new benchmark\nfor machine learning in real-time data processing at particle colliders.",
        "This paper presents an innovative extension of spatial autoregressive (SAR)\nmodels, introducing spatial coefficients specific to each spatial region that\nevolve over time. The proposed estimation methodology covers both homoscedastic\nand heteroscedastic data, ensuring consistency and efficiency in the estimators\nof the parameters $\\pmb{\\rho}$ and $\\pmb{\\beta}$. The model is based on a\nrobust theoretical framework, supported by the analysis of the asymptotic\nproperties of the estimators, which reinforces its practical implementation. To\nfacilitate its use, an algorithm has been developed in the R software, making\nit a standard tool for the analysis of complex spatial data. The proposed model\nproves to be more effective than other similar techniques, especially when\nmodeling data with normal spatial structures and non-normal distributions, even\nwhen the residuals are not homoscedastic. Finally, the application of the model\nto homicide rates in the United States highlights its advantages in both\nstatistical and social analysis, positioning it as a key tool for the analysis\nof spatial data in various disciplines.",
        "Direct data-driven control methods are known to be vulnerable to uncertainty\nin stochastic systems. In this paper, we propose a new robust data-driven\npredictive control (DDPC) framework to tackle the uncertainty in dynamic\nsystems. By analyzing non-unique solutions to behavioral representation, we\nfirst shed light on the lack of robustness in subspace predictive control (SPC)\nas well as the projection-based regularized DDPC. This inspires us to construct\nan uncertainty set that includes admissible output trajectories deviating to\nsome extent from nominal predictions from the subspace predictor and develop a\nmin-max robust formulation of DDPC that endows control sequences with\nrobustness against such unknown deviations. We establish its performance\nguarantees under bounded additive noise and develop convex reformulations of\nthe min-max problem with moderate complexity. To mitigate the conservatism of\nrobust design, an adaptive robust DDPC scheme is further proposed by\nincorporating an affine feedback policy, with performance guarantees and\ntractable reformulations derived. Simulation studies show that the proposed\nmethod effectively robustifies SPC and outperforms projection-based\nregularizer.",
        "User profile modeling plays a key role in personalized systems, as it\nrequires building accurate profiles and updating them with new information. In\nthis paper, we present two high-quality open-source user profile datasets: one\nfor profile construction and another for profile updating. These datasets offer\na strong basis for evaluating user profile modeling techniques in dynamic\nsettings. We also show a methodology that uses large language models (LLMs) to\ntackle both profile construction and updating. Our method uses a probabilistic\nframework to predict user profiles from input text, allowing for precise and\ncontext-aware profile generation. Our experiments demonstrate that models like\nMistral-7b and Llama2-7b perform strongly in both tasks. LLMs improve the\nprecision and recall of the generated profiles, and high evaluation scores\nconfirm the effectiveness of our approach.",
        "A recent proposal of accelerator based fusion reactor considers a scheme\nwhere an ion beam from the accelerator hits the target plasma on the resonance\nof the fusion reaction so that the reactivity ($\\sigma v$) can be an order of\nmagnitude larger than that of a thermonuclear reactor. One of the important\ninputs is the stopping power which is needed to assess the energy loss of the\nbeam in the plasma. In this work, we shall use the analytic formulation of\nBrown, Preston and Singleton~\\cite{Brown:2005ji} to calculate the temperature\ndependence of the stopping power due to the target $t, {}^3H_e$, and ${}^{11}B$\nplasmas in the resonance regions of their respective fusion reactions, i.e., $\nd + t \\rightarrow n + \\alpha, d + {}^3H_e \\rightarrow p + \\alpha$, and $p +\n{}^{11}B \\rightarrow 3 \\alpha$. It is found that the calculated stopping power,\nespecially when the quantum corrections are included, does not go down with\ntemperature as fast at $T^{-3\/2}$. Instead it decreases slower, more like\n$T^{-x}$ with $x \\le 1$ in the range of T from $\\sim$ 5 to 50 keV for $d$ on\n$t$ and ${}^3H_e$ plasmas around their resonance energies.",
        "In contrast to RNNs, which compress previous tokens into a single hidden\nstate, Transformers can attend to all previous tokens directly. However,\nstandard Transformers only use representations from the immediately preceding\nlayer. In this paper, we show that this design choice causes representation\ncollapse and leads to suboptimal performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that\npreserves the model's overall memory footprint while expanding its\nrepresentational capacity by allowing access to hidden states from earlier\nlayers. Through extensive experiments across various architectures and\ndifferent lookup mechanisms, we demonstrate consistent performance improvements\non a wide range of tasks. Moreover, our analysis of the learned representation\ndynamics and our exploration of depthwise circuits reveal how LIMe integrates\ninformation across layers, pointing to promising directions for future\nresearch.",
        "The JIRIAF (JLab Integrated Research Infrastructure Across Facilities)\nframework is designed to streamline resource management and optimize\nhigh-performance computing (HPC) workloads across heterogeneous environments.\nCentral to JIRIAF is the JIRIAF Resource Manager (JRM), which effectively\nleverages Kubernetes and Virtual Kubelet to manage resources dynamically, even\nin environments with restricted user privileges. By operating in userspace, JRM\nfacilitates the execution of user applications as containers across diverse\ncomputing sites, ensuring unified control and monitoring. The framework's\neffectiveness is demonstrated through a case study involving the deployment of\ndata-stream processing pipelines on the Perlmutter system at NERSC, showcasing\nits capability to manage large-scale HPC applications efficiently.\nAdditionally, we discuss the integration of a digital twin model for a\nsimulated queue system related to a streaming system, using a Dynamic Bayesian\nNetwork (DBN) to enhance real-time monitoring and control, providing valuable\ninsights into system performance and optimization strategies.",
        "We report the discovery of a new binary galaxy cluster merger, the Champagne\nCluster (RM J130558.9+263048.4), using a detection method that identifies\ndynamically active clusters in the redMaPPer SDSS DR8 photometric galaxy\ncluster catalog. The Champagne Cluster exhibits the classic X-ray morphology of\na post-pericenter dissociative galaxy cluster merger: an X-ray peak located\nbetween two galaxy overdensities at the same redshift. We conducted a\nKeck\/DEIMOS survey and obtained redshifts for 103 member galaxies. The redshift\nanalysis indicates a relative velocity of 411 $\\pm$ 180 km\/s between the two\nsubclusters, which suggests that the merger is happening near the plane of the\nsky. We used cosmological simulations to find analogous systems to constrain\nthe time since pericenter (74-250 Myr) and the angle the merger axis makes with\nthe plane of the sky (62$^\\circ$-90$^\\circ$) at the 68$\\%$ confidence level. We\nestimated the bulk temperature (8.20 $\\pm 1.2$ keV) and total X-ray luminosity\n(5.2 $\\pm$ 0.8 $\\times$ $10^{44}$ erg $\\times$ $s^{-1}$) of the intracluster\nmedium using $\\textit{Chandra}$ archival data."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study",
    "start_abstract":"In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "What is a cell type, really? The quest to categorize life's myriad forms."
      ],
      "abstract":[
        "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "A Multiple Transferable Neural Network Method with Domain Decomposition\n  for Elliptic Interface Problems",
        "Transparent Graphene-Superconductor Interfaces: Quantum Hall and Zero\n  Field Regimes",
        "Nuclear level density of ${}^{128}$Te from\n  $(\\mathrm{p},\\mathrm{p}'\\gamma)$ scattering and complementary photonuclear\n  data",
        "Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates\n  and Fundamental Limits",
        "Bounds for within-household encouragement designs with interference",
        "The irreducibility of Hurwitz spaces and Severi varieties on toric\n  surfaces",
        "Electric fields-tuning plasmon and coupled plasmon-phonon modes in\n  monolayer transition metal dichalcogenides",
        "A Global Existence Theorem for a Fourth-Order Crystal Surface Model with\n  Gradient Dependent Mobility",
        "Effect of a polymeric compound layer on jetting dynamics produced by\n  bursting bubbles",
        "Symmetric observations without symmetric causal explanations",
        "Applying computational protein design to therapeutic antibody discovery\n  -- current state and perspectives",
        "Engineering of electronic and magnetic modulations in gradient\n  functional oxide heterostructures",
        "Dual spectroscopy of quantum simulated Fermi-Hubbard systems",
        "On weight and variance uncertainty in neural networks for regression\n  tasks",
        "Steady vortex patches on flat torus with a constant background vorticity",
        "Non-Equilibrium Thermodynamics Framework to Address the Glass Transition",
        "Statistical biases in parameterized searches for gravitational-wave\n  polarizations",
        "Fermi surface origin of the low-temperature magnetoresistance anomaly",
        "Forecasting Local Ionospheric Parameters Using Transformers",
        "The Saturation Spectrum of Berge Stars",
        "Modifying Range-Doppler geometry frameworks to process Spotlight SAR\n  imagery in Polar Format",
        "A shocking outcome: Jet dynamics and polarimetric signatures of the\n  multi-band flare in blazar OJ 248",
        "Optical force and torque on a spinning dielectric sphere",
        "Detecting entanglement in any measurement using quantum networks",
        "A Refined Cometary Fluorescence Model for the $\\nu_{3}$ Vibrational Band\n  of Cyanogen : Application to JWST\/NIRSPEC Spectrum",
        "Genesis of the James Webb Space Telescope Architecture: The Designers'\n  Story",
        "Streaming Self-Corrected Dual-Comb Spectrometer",
        "A life in Mathematical Analysis: a conversation with Luigi Rodino",
        "VSL-Gravity in light of PSR B1913+16 Full Data Set: Upper limits on\n  graviton mass and its theoretical consequences"
      ],
      "abstract":[
        "The transferable neural network (TransNet) is a two-layer shallow neural\nnetwork with pre-determined and uniformly distributed neurons in the hidden\nlayer, and the least-squares solvers can be particularly used to compute the\nparameters of its output layer when applied to the solution of partial\ndifferential equations. In this paper, we integrate the TransNet technique with\nthe nonoverlapping domain decomposition and the interface conditions to develop\na novel multiple transferable neural network (Multi-TransNet) method for\nsolving elliptic interface problems, which typically contain discontinuities in\nboth solutions and their derivatives across interfaces. We first propose an\nempirical formula for the TransNet to characterize the relationship between the\nradius of the domain-covering ball, the number of hidden-layer neurons, and the\noptimal neuron shape. In the Multi-TransNet method, we assign each subdomain\none distinct TransNet with an adaptively determined number of hidden-layer\nneurons to maintain the globally uniform neuron distribution across the entire\ncomputational domain, and then unite all the subdomain TransNets together by\nincorporating the interface condition terms into the loss function. The\nempirical formula is also extended to the Multi-TransNet and further employed\nto estimate appropriate neuron shapes for the subdomain TransNets, greatly\nreducing the parameter tuning cost. Additionally, we propose a normalization\napproach to adaptively select the weighting parameters for the terms in the\nloss function. Ablation studies and extensive experiments with comparison tests\non different types of elliptic interface problems with low to high contrast\ndiffusion coefficients in two and three dimensions are carried out to\nnumerically demonstrate the superior accuracy, efficiency, and robustness of\nthe proposed Multi-TransNet method.",
        "We study clean, edge-contacted graphene\/superconductor interfaces in both the\nquantum Hall (QH) and zero field regimes. We find that Andreev reflection is\nsubstantially stronger than at an interface with a semiconductor\ntwo-dimensional electron gas: the large velocity at graphene's conical Dirac\npoints makes the requirement of current continuity to a metal much less\nrestrictive. In both our tight-binding and continuum models, we find a wide\nrange of parameters for which Andreev reflection is strong. For a transparent\ninterface, we demonstrate the following for graphene in the lowest Landau level\nQH state: (i) Excellent electron-hole hybridization occurs: the electron and\nhole components in graphene are simply related by an exchange of sublattice.\nThe spatial profile for the electron component is predominantly gaussian on one\nsublattice and peaked at the interface, and so very different from the QH edge\nstate of a terminated lattice. (ii) The degree of hybridization is independent\nof the graphene filling: no fine-tuning is needed. (iii) The spectrum is valley\ndegenerate: the dispersion of each chiral Andreev edge mode (CAEM) self-aligns\nto be antisymmetric about the center of each valley, independent of filling.\nAchieving a transparent interface requires the absence of any barrier as well\nas a superconductor that is suitably matched to graphene; we argue that the\nlatter condition is not very stringent. We further consider the effect of\nreduced transparency and Zeeman splitting on both the wavefunctions of the CAEM\nand their dispersion.",
        "We have extracted the nuclear level density of ${}^{128}$Te from a\n$(\\mathrm{p},\\mathrm{p} '\\gamma)$ scattering experiment using the large-volume\n\\labr\\ and \\cebr\\ detectors from ELI-NP at the 9~MV Tandem facilities at\nIFIN-HH. The decay data were normalised using photonuclear data, resulting in\nnuclear level densities without intrinsic model dependencies from the constant\ntemperature or Fermi gas models. The measured nuclear level density follows\nclosely between the expectations from these two models, but we observe a clear\ndivergence from a microscopic model based on the Skyrme force.",
        "One of the most basic problems in reinforcement learning (RL) is policy\nevaluation: estimating the long-term return, i.e., value function,\ncorresponding to a given fixed policy. The celebrated Temporal Difference (TD)\nlearning algorithm addresses this problem, and recent work has investigated\nfinite-time convergence guarantees for this algorithm and variants thereof.\nHowever, these guarantees hinge on the reward observations being always\ngenerated from a well-behaved (e.g., sub-Gaussian) true reward distribution.\nMotivated by harsh, real-world environments where such an idealistic assumption\nmay no longer hold, we revisit the policy evaluation problem from the\nperspective of adversarial robustness. In particular, we consider a\nHuber-contaminated reward model where an adversary can arbitrarily corrupt each\nreward sample with a small probability $\\epsilon$. Under this observation\nmodel, we first show that the adversary can cause the vanilla TD algorithm to\nconverge to any arbitrary value function. We then develop a novel algorithm\ncalled Robust-TD and prove that its finite-time guarantees match that of\nvanilla TD with linear function approximation up to a small $O(\\epsilon)$ term\nthat captures the effect of corruption. We complement this result with a\nminimax lower bound, revealing that such an additive corruption-induced term is\nunavoidable. To our knowledge, these results are the first of their kind in the\ncontext of adversarial robustness of stochastic approximation schemes driven by\nMarkov noise. The key new technical tool that enables our results is an\nanalysis of the Median-of-Means estimator with corrupted, time-correlated data\nthat might be of independent interest to the literature on robust statistics.",
        "We obtain partial identification of direct and spillover effects in settings\nwith strategic interaction and discrete treatments, outcome and independent\ninstruments. We consider a framework with two decision-makers who play\npure-strategy Nash equilibria in treatment take-up, whose outcomes are\ndetermined by their joint take-up decisions. We obtain a latent-type\nrepresentation at the pair level. We enumerate all types that are consistent\nwith pure-strategy Nash equilibria and exclusion restrictions, and then impose\nconditions such as symmetry, strategic complementarity\/substitution, several\nnotions of monotonicity, and homogeneity. Under any combination of the above\nrestrictions, we provide sharp bounds for our parameters of interest via a\nsimple Python optimization routine. Our framework allows the empirical\nresearcher to tailor the above menu of assumptions to their empirical\napplication and to assess their individual and joint identifying power.",
        "In 1969, Fulton introduced classical Hurwitz spaces parametrizing simple\nd-sheeted coverings of the projective line in the algebro-geometric setting. He\nestablished the irreducibility of these spaces under the assumption that the\ncharacteristic of the ground field is greater than d, but the irreducibility\nproblem in smaller characteristics remained open. We resolve this problem in\nthe current paper and prove that the classical Hurwitz spaces are irreducible\nover any algebraically closed field. On the way, we establish the\nirreducibility of Severi varieties in arbitrary characteristic for a rich class\nof toric surfaces, including all classical toric surfaces. Our approach to the\nirreducibility problems comes from tropical geometry, and the paper contains\ntwo more results of independent interest - a lifting result for parametrized\ntropical curves and a strong connectedness property of the moduli spaces of\nparametrized tropical curves.",
        "We theoretically investigate the electric field-tuning plasmons and\nplasmon-phonon couplings of two-dimensional (2D) transition metal\ndichalcogenides (TMDs), such as monolayer MoS2, under the consideration of\nspin-orbit coupling. It is revealed that the frequencies of plasmons and\ncoupled plasmon-phonon modes originating from electron-electron and\nelectron-phonon interactions can be effectively changed by using applied\ndriving electric fields. Notably, these frequencies exhibit a decreasing trend\nwith an increasing electric field. Moreover, the weak angular dependence of\nthese modes suggests that the driving electric field does not induce\nsignificant anisotropy in the plasmon modes. The outcomes of this work\ndemonstrate that the plasmon and coupled plasmon-phonon modes can be tuned not\nonly by manipulating the electron density via the application of a gate voltage\nbut also by tuning the applied driving electric field. These findings hold\nrelevance for facilitating the application of 2D TMDs in optoelectronic\ndevices.",
        "In this article we study the existence of solutions to a fourth-order\nnonlinear PDE related to crystal surface growth. The key difficulty in the\nequations comes from the mobility matrix, which depends on the gradient of the\nsolution. When the mobility matrix is the identity matrix there are now many\nexistence results, however when it is allowed to depend on the solution we lose\ncrucial estimates in the time direction. In this work we are able to prove the\nglobal existence of weak solutions despite this lack of estimates in the time\ndirection.",
        "Jetting dynamics from bursting bubbles play a key role in mediating mass and\nmomentum transport across the air-liquid interface. In marine environments,\nthis phenomenon has drawn considerable attention due to its role in releasing\nbiochemical contaminants, such as extracellular polymeric substances, into the\natmosphere through aerosol production. These biocontaminants often exhibit\nnon-Newtonian characteristics, yet the physics of bubble bursting with a\nrheologically complex layer at bubble-liquid interfaces remains largely\nunexplored. In this study, we experimentally investigate the jetting dynamics\nof bubble bursting events in the presence of such polymeric compound layers.\nUsing bubbles coated by a polyethylene oxide solution, we document the cavity\ncollapse and jetting dynamics produced by bubble bursting. At a fixed polymer\nconcentration, the jet velocity increases while the jet radius decreases with\nan increasing compound layer fraction, as a result of stronger capillary wave\ndamping due to capillary wave separation at the compound interface as well as\nthe formation of smaller cavity cone angles during bubble cavity collapse.\nThese dynamics produce smaller and more numerous jet drops. Meanwhile, as the\npolymer concentration increases, the jet velocity decreases while the jet\nradius increases for the same compound layer fraction due to the increasing\nviscoelastic stresses. In addition, fewer jet drops are ejected as the jets\nbecome slower and broader with increasing polymer concentration, as\nviscoelastic stresses persist throughout the jet formation and thinning\nprocess. We further obtain a regime map delineating the conditions for jet drop\nejection versus no jet drop ejection in bursting bubbles coated with a\npolymeric compound layer. Our results may provide new insights into the\nmechanisms of mass transport of organic materials in bubble-mediated\naerosolization processes.",
        "Inferring causal models from observed correlations is a challenging task,\ncrucial to many areas of science. In order to alleviate the effort, it is\nimportant to know whether symmetries in the observations correspond to\nsymmetries in the underlying realization. Via an explicit example, we answer\nthis question in the negative. We use a tripartite probability distribution\nover binary events that is realized by using three (different) independent\nsources of classical randomness. We prove that even removing the condition that\nthe sources distribute systems described by classical physics, the requirements\nthat i) the sources distribute the same physical systems, ii) these physical\nsystems respect relativistic causality, and iii) the correlations are the\nobserved ones, are incompatible.",
        "Machine learning applications in protein sciences have ushered in a new era\nfor designing molecules in silico. Antibodies, which currently form the largest\ngroup of biologics in clinical use, stand to benefit greatly from this shift.\nDespite the proliferation of these protein design tools, their direct\napplication to antibodies is often limited by the unique structural biology of\nthese molecules. Here, we review the current computational methods for antibody\ndesign, highlighting their role in advancing computational drug discovery.",
        "Advanced interface engineering provides a way to control the ground state of\ncorrelated oxide heterostructures, which enables the shaping of future\nelectronic and magnetic nanodevices with enhanced performance. An especially\npromising and rather new avenue is to find and explore low-dimensional phases\nof structural, ferroic and superconducting origin. In this multimodal study, we\npresent a novel dynamic growth control method that enables synthesizing\ncompositionally graded superlattices (SLs) of (LaMnO_3)_10\/(SrMnO_3)_10\n(LMO\/SMO), in which the layers gradually change their composition between LMO\nand SMO with gradient G values ranging from 0 to 100 %. This leads to strong\nmodulations in the material's electronic properties and of the two-phase\nferromagnetic (FM) behavior. In particular, we observe that G has almost no\nimpact on the emergent high-temperature FM phase; in contrast, the\nlow-temperature volume-like FM phase increases drastically with higher\nG-factors and thus can serve as a precise marker for chemical composition on a\nnanoscale. Focusing on the interfacial charge transfer found at sharp SMO\/LMO\ninterfaces (G=0), we observe that for higher G-factors a long-range charge\nmodulation develops, which is accompanied by an insulator-to-metal transition.\nThese findings showcase G as a crucial control parameter that can shape the\nsuperlattice's intrinsic properties and provide a perspective for designing\nfunctional oxide heterostructures with artificially disordered interfaces.",
        "Quantum gas microscopy with atoms in optical lattices provides remarkable\ninsights into the real space properties of many-body systems, but does not\ndirectly reveal the nature of their fundamental excitation spectrum. Here, we\ndemonstrate that radio-frequency spectroscopy can reveal the quasi-particle\nnature of doped quantum many-body systems, crucial for our understanding of,\ne.g., high-temperature superconductors. In particular, we showcase how the\nexistence and energy of magnetic polaron quasi-particles in doped Fermi-Hubbard\nsystems may be probed, revealed by hallmark peaks in the spectroscopic\nspectrum. In combination with fundamental dualities of the Fermi-Hubbard model,\nwe describe how these findings may be tested using several experimental\nplatforms.",
        "We consider the problem of weight uncertainty proposed by [Blundell et al.\n(2015). Weight uncertainty in neural network. In International conference on\nmachine learning, 1613-1622, PMLR.] in neural networks {(NNs)} specialized for\nregression tasks. {We further} investigate the effect of variance uncertainty\nin {their model}. We show that including the variance uncertainty can improve\nthe prediction performance of the Bayesian {NN}. Variance uncertainty enhances\nthe generalization of the model {by} considering the posterior distribution\nover the variance parameter. { We examine the generalization ability of the\nproposed model using a function approximation} example and {further illustrate\nit with} the riboflavin genetic data set. {We explore fully connected dense\nnetworks and dropout NNs with} Gaussian and spike-and-slab priors,\nrespectively, for the network weights.",
        "We construct a series of vortex patch solutions in a doubly-periodic\nrectangular domain (flat torus), which is accomplished by studying the contour\ndynamic equation for patch boundaries. We will illustrate our key idea by\ndiscussing the single-layered patches as the most fundamental configuration,\nand then investigate the general construction for $N$ patches near a point\nvortex equilibrium. Different with the case of bounded domains in $\\mathbb\nR^2$, a constant background vorticity will arise from the compact nature of\nflat torus, and the $2$-dimensional translational invariance will bring\ntroubles on determining patch locations. To overcome these two difficulties, we\nwill add additional terms for background vorticity and introduce a centralized\ncondition for location vector. By utilizing the regularity difference of terms\nin contour dynamic equations, we also obtain the $C^\\infty$ regularity and\nconvexity of boundary curves.",
        "When the center of fluctuations, i.e., the nonequilibrium eigenphase,\nundergoes transformation, there emerge critical parameters that demonstrate\ninsensitivity to fluctuation perturbations and even independence from the\nmolecular physical properties of the system, while exhibiting pronounced\nefficacy in governing phase transition dynamics.In the context of polymer glass\ntransitions, Flory's conjecture (or C1 in the WLF equation) represents such a\nlongstanding yet unresolved critical parameter. To address this issue, we\nreplace entropy variation with a sequence of microstates and provide an\nanalytically tractable statistical description of non-ergodicity. Our theory\nrigorously demonstrates that reaching the critical parameter is a necessary\ncondition for non-equilibrium transitions to occur. Due to correlations between\nthe eigenphase's entropy and energy in non-equilibrium systems, any system with\na given intrinsic structure inherently possesses a critical parameter that\nrepresents the limiting deviation from equilibrium at any temperature. Flory's\nconjecture exemplifies this, with our new theoretical critical void ratio at\nthe glass transition boundary calculated to be 2.6%, which closely matches\nexperimental observations of 2.5%-2.6% over the past 50 years.",
        "In tests of gravity using gravitational waves (GWs), GW events analyzed are\noften selected based on specific criteria, particularly the signal-to-noise\nratio (SNR). However, such event selection can introduce bias into parameter\nestimation unless the selection effect is appropriately taken into account in\nthe analysis. In this paper, we investigate how event selection with certain\nprior information affects parameter inference within the scalar-tensor\npolarization framework, focusing on the measurement of the scalar mode\namplitude parameters. We find that for the Tensor+Scalar(dipole) model, the\namplitude of the scalar dipole radiation is overestimated when its true value\nis nonzero while there is no false deviation in the absence of the scalar mode.\nThe same bias is expected to occur also for the Tensor+Scalar(quadrupole)\nmodel. However, error typically exceeds the bias as the scalar quadrupole mode\nis difficult to be distinguished from the tensor mode.",
        "A magnetoresistance (MR) anomaly at low temperatures has been observed in a\nvariety of systems, ranging from low-dimensional chalcogenides to spin and\ncharge density wave (SDW\/CDW) metals and, most recently, topological\nsemimetals. In some systems parabolic magnetoresistance can rise to hundreds of\nthousands of times its low-temperature, zero-field value. While the origin of\nsuch a dramatic effect remains unresolved, these systems are often\nlow-carrier-density compensated metals, and the physics is expected to be\nquasi-classical. Here we demonstrate that this MR anomaly in temperature also\nexists in high conductivity good metals with large Fermi surfaces, namely Cr,\nMo, and W, for both linear and quadratic field-dependent regimes with their\nnon-saturation attributed to open orbit and electron-hole compensation,\nrespectively. We provide evidence that quantum transport across sharp Fermi\nsurface arcs, but not necessarily the full cyclotron orbit, governs this\nlow-temperature MR anomaly. In Cr, extremely sharp curvatures are induced by\nsuperposed lattice and SDW band structures. One observes an overlay of the\ntemperature dependence of three phenomena: namely, MR at a constant high field,\nlinear MR in the low-field limit, and Shubnikov-de Haas (SdH) oscillations of\nthe lightest orbit. In Mo, the temperature dependence of low-T MR anomaly\nextends beyond those of its SdH oscillations but disappears at temperatures\nwhere Kohler's scaling reemerges. In the low-temperature and high-field limit,\nlarge magnetoresistance from carriers circling quantum orbits is the\nthree-dimensional analogy to the zero-conductance state of carrier localization\nin the integer quantum Hall effect, especially with regard to the adverse\neffect of disorder.",
        "We present a novel method for forecasting key ionospheric parameters using\ntransformer-based neural networks. The model provides accurate forecasts and\nuncertainty quantification of the F2-layer peak plasma frequency (foF2), the\nF2-layer peak density height (hmF2), and total electron content (TEC) for a\ngiven geographic location. It supports a number of exogenous variables,\nincluding F10.7cm solar flux and disturbance storm time (Dst). We demonstrate\nhow transformers can be trained in a data assimilation-like fashion that use\nthese exogenous variables along with na\\\"ive predictions from climatology to\ngenerate 24-hour forecasts with non-parametric uncertainty bounds. We call this\nmethod the Local Ionospheric Forecast Transformer (LIFT). We demonstrate that\nthe trained model can generalize to new geographic locations and time periods\nnot seen during training, and we compare its performance to that of the\nInternational Reference Ionosphere (IRI).",
        "The forbidden subgraph problem is among the oldest in extremal combinatorics\n-- how many edges can an $n$-vertex $F$-free graph have? The answer to this\nquestion is the well-studied extremal number of $F$. Observing that every\nextremal example must be maximally $F$-free, a natural minimization problem is\nalso studied -- how few edges can an $n$-vertex maximal $F$-free graph have?\nThis leads to the saturation number of $F$. Both of these problems are\nnotoriously difficult to extend to $k$-uniform hypergraphs for any $k\\ge 3$.\n  Barefoot et al., in the case of forbidding triangles in graphs, asked a\nbeautiful question -- which numbers of edges, between the saturation number and\nthe extremal number, are actually realized by an $n$-vertex maximal $F$-free\ngraph? Hence named the saturation spectrum of $F$, this has since been\ndetermined precisely for several classes of graphs through a large number of\npapers over the past two decades.\n  In this paper, we extend the notion of the saturation spectrum to the\nhypergraph context. Given a graph $F$ and a hypergraph $G$ embedded on the same\nvertex set, we say $G$ is a {\\bf{Berge-$F$}} if there exists a bijection\n$\\phi:E(F)\\to E(G)$ such that $e\\subseteq \\phi(e)$ for all $e\\in E(F)$. We\ncompletely determine the saturation spectrum for $3$-uniform Berge-$K_{1,\\ell}$\nfor $1\\leq \\ell\\leq 4$, and for $\\ell=5$ when $5\\mid n$. We also determine all\nbut a constant number of values in the spectrum for $3$-uniform\nBerge-$K_{1,\\ell}$ for all $\\ell\\geq 5$. We note that this is the first result\ndetermining the saturation spectrum for any non-trivial hypergraph.",
        "We present a simple method to enable processing of Spotlight Synthetic\nAperture Radar (SAR) imagery distributed in Polar Format (PFA) using standard\nRange-Doppler (RDA) geometry algorithms. Our approach is applicable to PFA SAR\nimages characterized by a constant value of the Center of Aperture (COA) time.\nWe present simplified expressions for forward (image-to-ground) and inverse\n(ground-to-image) geometry mapping using Sensor Independent Complex Data (SICD)\nconventions. We discuss simple changes needed to current open source SAR\nsoftware that implement Range-Doppler algorithms, to enable support within them\nfor Spotlight data distributed in SICD format. We include a proof-of-concept\nscript that utilizes the Python packages sarpy and isce3 to demonstrate the\ncorrectness of the proposed approach.",
        "The connection between $\\gamma$-ray flares and blazars is a topic of active\nresearch, with few sources exhibiting distinct enough such outbursts to be able\nto conclusively connect them to features in their jet morphology. Here we\npresent an investigation of the sole $\\gamma$-ray flare of the blazar OJ 248\nthus far, in association with its jet structure, as revealed by very long\nbaseline interferometry (VLBI). We find that throughout the course of the\n$\\gamma$-ray flare, the fractional linear polarisation increases in the jet of\nOJ 248, and the VLBI electric vector position angles (EVPAs) turn perpendicular\nto the bulk jet flow. We interpret this behaviour as a moving shock, travelling\nthrough a recollimation shock and upscattering photons via the inverse Compton\nscattering process, producing a $\\gamma$-ray flare; we discuss possible\nmechanisms. Our hypothesised shock-shock interaction scenario is a viable\nmechanism to induce such EVPA rotations in both optical and radio bands.",
        "Optical force can enable precise manipulations of small particles for various\napplications. It is well known that an isotropic lossless dielectric sphere is\nonly subject to forward optical force under the illumination of an\nelectromagnetic plane wave. By using rigorous full-wave simulations, we show\nthat such a sphere can experience a lateral optical force and an optical torque\nbesides the conventional longitudinal force, if it spins with a constant\nangular velocity. The emergence of the unusual optical force and torque is\nattributed to the breaking of mirror and cylindrical symmetries by the spinning\nmotion. Using the multipole expansion in source representation, we illustrate\nhow the spinning-induced effective bi-anisotropy generates the lateral force\nand torque on the sphere through the interference of electric and magnetic\nmultipoles. We also uncover the effect of Sagnac frequency splitting on the\noptical force and torque. The results contribute to the understanding of the\noptical force and torque in moving media and can be applied to realize\nunconventional optical manipulations of small particles.",
        "Entanglement is a key resource to demonstrate quantum advantage over\nclassical strategies. Entanglement in quantum states is one of the most\nwell-explored areas in quantum physics. However, a rigorous approach to\nunderstanding and detecting entanglement in composite quantum measurements is\nlacking. In this work, we focus on composite quantum measurements and classify\nthem into two classes: entangled and separable measurements. As done for\nquantum states, we define analogously a notion of witness that can be used to\ndetect entanglement in composite quantum measurements. Here, one does not need\nto trust the measurement to witness its entanglement but must trust the quantum\nstates. We then further extend this approach to show that any entangled\nmeasurement provides an advantage in network quantum steering without inputs,\nalso known as swap steering. Consequently, this provides a way to witness\nentanglement in any quantum measurement in a one-sided device-independent way.\nFinally, we consider the star network scenario and show that any rank-one\nprojective entangled quantum measurement gives a quantum advantage. Thus, one\ncan detect the entanglement in any rank-one projective measurement in a\ndevice-independent way.",
        "Cyanogen ($C_2N_2$) was among the many molecules identified in the coma of\n67P\/Churyumov-Gerasimenko during the Rosetta mission. As a potential parent\nspecies of the CN radical, its abundance relative to other species such as HCN\nshould be generalized to comets observed from ground-based facilities. To\ninvestigate its presence from infrared spectra in other comets, we developed a\nnew fluorescence model for the $\\nu_3$ fundamental band. From new\nhigh-resolution infrared spectra of cyanogen, we analyzed the region of the\n$\\nu_3$ band of $C_2N_2$, centered around 4.63 $\\mu$m 2158 cm$^{-1}$). In\naddition to line positions and intensities, ground and excited molecular\nparameters were obtained. The spectroscopic analysis allowed us to develop a\nnew fluorescence model for cyanogen. Excitation rates of the $\\nu_3$ band of\ncyanogen are presented. An attempt to detect cyanogen in a high-resolution\nspectrum of comet C\/2022 E3 (ZTF) is discussed.",
        "The James Webb Space Telescope, launched in 2021, is an infrared observatory\nof novel design: deployable, with active optics, fully open to space for\nradiative cooling and orbiting the Lagrange point no. 2. This article explains\nthe rationale leading to this specific design and describes the various other\narchitectures that were considered along the way: from a monolithic 10-meter\ntelescope in geosynchronous orbit to a 6-meter one in High Earth Orbit, then a\n16-meter observatory on the Moon, a 4- or 6-meter one in an elliptical\nheliocentric orbit, and a segmented 8-meter one passively cooled to 50 K at L2,\nwhich was finally descoped to 6.6 meters. It also addresses the optimization\nfor scientific performance, the challenge of dealing with such an ultra-low\noperating temperature, cost issues, supporting technology, modifications made\nduring final design and, finally, how the architecture performs on orbit.",
        "Here, we radically simplify coherently averaged dual-comb spectroscopy by\nintroducing a real-time self-correction system: a radio frequency\nsystem-on-chip computes each incoming dual-comb interferogram's phase,\nfrequency, and arrival time, calculates changes in the combs' carrier-envelope\noffset frequencies and repetition rates, and immediately phase-corrects the\nincoming interferogram data stream. The algorithm supports up to 0.3 GHz\ninterferogram frequency bandwidth and thus combines fast measurement times\n(corresponding to high detunings) with broadband optical detection. Using the\nsystem, we achieve comb-resolved spectroscopy with Fourier-limited linewidth,\ncoherent averaging over arbitrarily long durations, and a signal-to-noise ratio\nof up to 2100. Iodine and acetylene spectroscopy yields excellent agreement\nwith literature over an optical bandwidth of more than 10 THz in the visible\nand near-infrared. Our approach only requires three optical and three\nelectronic components and makes instantaneous dual-comb spectroscopy available\nto everyday applications.",
        "This note is the transcription of an interview with Professor Luigi Rodino,\non the occasion of the ISAAC-ICMAM Conference of Analysis in Developing\nCountries (December 2, 2024 - Bogot\\`a), that was dedicated to him. Luigi\nRodino is at present Emeritus Professor at the University of Turin, and a\nmember of the Accademia delle Scienze di Torino.",
        "Very Special Linear Gravity (VSL-Gravity) is an alternative model of\nlinearized gravity that incorporates massive gravitons while retaining only two\nphysical degrees of freedom thanks to gauge invariance. Recently, the\ngravitational period-decay dynamics of the model has been determined using\neffective field theory techniques. In this study, we conduct a comprehensive\nBayesian analysis of the PSR B1913+16 binary pulsar dataset to test the\npredictions of VSL-Gravity. Our results place a 95\\% confidence level upper\nbound on the graviton mass at $m_g \\lesssim 10^{-19} \\, \\text{eV}\/c^2$.\nAdditionally, we observe a significant discrepancy in the predicted mass of one\nof the binary's companion stars. Lastly, we discuss the broader implications of\na non-zero graviton mass, from astrophysical consequences to potential\ncosmological effects."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"What is a cell type, really? The quest to categorize life's myriad forms.",
    "start_abstract":"The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
      ],
      "abstract":[
        "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "HI-MaNGA: Results from (21cm-HI) single-dish observations of MaNGA\n  Survey Galaxies",
        "\\'El\\'ements de comptage sur les g\\'en\\'erateurs du groupe modulaire et\n  les $\\lambda$-quiddit\\'es",
        "Dark Energy Survey Year 6 Results: Synthetic-source Injection Across the\n  Full Survey Using Balrog",
        "On the Commuting Problem of Toeplitz Operators on the Harmonic Bergman\n  Space",
        "Generalization Performance of Hypergraph Neural Networks",
        "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
        "Properties of the one-component Coulomb gas on a sphere with two\n  macroscopic external charges",
        "Norm-one points in convex combinations of relatively weakly open subsets\n  of the unit ball in the spaces $L_1(\\mu,X)$",
        "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "Deriving motivic coactions and single-valued maps at genus zero from\n  zeta generators",
        "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to\n  Counteract Alpha Decay",
        "Relative knot probabilities in confined lattice polygons",
        "Global branching of solutions to ODEs and integrability",
        "Kink dynamics for the Yang-Mills field in an extremal\n  Reissner-Nordstr\\\"om black hole",
        "Multimodal Emotion Recognition and Sentiment Analysis in Multi-Party\n  Conversation Contexts",
        "70 MW-level picosecond mid-infrared radiation generation by difference\n  frequency generation in AgGaS2, BaGa4Se7, LiGaSe2, and LiGaS2",
        "A life in Mathematical Analysis: a conversation with Luigi Rodino",
        "Higher Riemann-Hilbert correspondence for foliations",
        "Performance of Practical Quantum Oblivious Key Distribution",
        "Human-Like Robot Impedance Regulation Skill Learning from Human-Human\n  Demonstrations",
        "Transient Chirality in the Gelation of Adhesive Spinner Monolayers",
        "Normal and inverse magnetocaloric effects in structurally disordered\n  Laves phase Y$_{1-x}$Gd$_{x}$Co$_{2}$ (0 $\\leq$ x $\\leq$ 1) compounds",
        "Keeping up with dynamic attackers: Certifying robustness to adaptive\n  online data poisoning",
        "Unified Multivariate Ordinal Model for analysis of sensory attributes",
        "Analog QAOA with Bayesian Optimisation on a neutral atom QPU",
        "Can one size fit all?: Measuring Failure in Multi-Document Summarization\n  Domain Transfer",
        "$S$, $T$, $U$ Parameters in The B-LSSM",
        "IRIS: An Immersive Robot Interaction System",
        "A Label-Free High-Precision Residual Moveout Picking Method for Travel\n  Time Tomography based on Deep Learning"
      ],
      "abstract":[
        "In a poster presentation for IAU Symposium 392: \"Neutral hydrogen in and\naround galaxies in the SKA era\", we gave an overview of the HI-MaNGA project\nwhich is working to obtain complementary information about the cold gas\n(neutral hydrogen traced by the radio 21cm line) content of Mapping Nearby\nGalaxies at Apache Point Observatory (MaNGA) sample galaxies. MaNGA, part of\nthe fourth incarnation of the Sloan Digital Sky Surveys (SDSS-IV), obtained\nspatially resolved spectral maps for 10,000 nearby galaxies selected to create\na representative sample out of the SDSS Main Galaxy Sample. MaNGA data have\nprovided a census of the stellar and ionized gas content of these galaxies, as\nwell as kinematics of both stars and gas. Adding HI information via the\nHI-MaNGA program, which has observed or collected 21cm line data for 70% of the\nfull MaNGA sample, has been crucial for a number of applications, but\nespecially understanding the physical mechanisms that regulate gas accretion,\nand through that star formation and quenching of star formation. This\nconference proceedings article accompanies the release of the DR3 version of\nHI-MaNGA data.",
        "The aim of this article is to count the $n$-tuples of positive integers\n$(a_{1},\\ldots,a_{n})$ solutions of the equation $\\begin{pmatrix} a_{n} & -1\n\\\\[4pt] 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_{n-1} & -1 \\\\[4pt] 1 & 0\n\\end{pmatrix} \\cdots \\begin{pmatrix} a_{1} & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}=\\pm\nM$ when $M$ is equal to the generators of the modular group $S=\\begin{pmatrix}\n0 & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}$ and $T=\\begin{pmatrix} 1 & 1 \\\\[4pt] 0 & 1\n\\end{pmatrix}$. To count these elements, we will study the\n$\\lambda$-quiddities, which are the solutions of the equation in the case\n$M=Id$ (related to Coxeter's friezes), whose last component is fixed.",
        "Synthetic source injection (SSI), the insertion of sources into pixel-level\non-sky images, is a powerful method for characterizing object detection and\nmeasurement in wide-field, astronomical imaging surveys. Within the Dark Energy\nSurvey (DES), SSI plays a critical role in characterizing all necessary\nalgorithms used in converting images to catalogs, and in deriving quantities\nneeded for the cosmology analysis, such as object detection rates, galaxy\nredshift estimation, galaxy magnification, star-galaxy classification, and\nphotometric performance. We present here a source injection catalog of $146$\nmillion injections spanning the entire 5000 deg$^2$ DES footprint, generated\nusing the Balrog SSI pipeline. Through this sample, we demonstrate that the DES\nYear 6 (Y6) image processing pipeline provides accurate estimates of the object\nproperties, for both galaxies and stars, at the percent-level, and we highlight\nspecific regimes where the accuracy is reduced. We then show the consistency\nbetween SSI and data catalogs, for all galaxy samples developed within the weak\nlensing and galaxy clustering analyses of DES Y6. The consistency between the\ntwo catalogs also extends to their correlations with survey observing\nproperties (seeing, airmass, depth, extinction, etc.). Finally, we highlight a\nnumber of applications of this catalog to the DES Y6 cosmology analysis. This\ndataset is the largest SSI catalog produced at this fidelity and will serve as\na key testing ground for exploring the utility of SSI catalogs in upcoming\nsurveys such as the Vera C. Rubin Observatory Legacy Survey of Space and Time.",
        "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
        "Hypergraph neural networks have been promising tools for handling learning\ntasks involving higher-order data, with notable applications in web graphs,\nsuch as modeling multi-way hyperlink structures and complex user interactions.\nYet, their generalization abilities in theory are less clear to us. In this\npaper, we seek to develop margin-based generalization bounds for four\nrepresentative classes of hypergraph neural networks, including\nconvolutional-based methods (UniGCN), set-based aggregation (AllDeepSets),\ninvariant and equivariant transformations (M-IGN), and tensor-based approaches\n(T-MPHN). Through the PAC-Bayes framework, our results reveal the manner in\nwhich hypergraph structure and spectral norms of the learned weights can affect\nthe generalization bounds, where the key technical challenge lies in developing\nnew perturbation analysis for hypergraph neural networks, which offers a\nrigorous understanding of how variations in the model's weights and hypergraph\nstructure impact its generalization behavior. Our empirical study examines the\nrelationship between the practical performance and theoretical bounds of the\nmodels over synthetic and real-world datasets. One of our primary observations\nis the strong correlation between the theoretical bounds and empirical loss,\nwith statistically significant consistency in most cases.",
        "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
        "The one-component Coulomb gas on the sphere, consisting on $N$ unit charges\ninteracting via a logarithmic potential, and in the presence of two external\ncharges each of strength proportional to $N$, is considered. There are two\nspherical caps naturally associated with the external charges, giving rise to\ntwo distinct phases depending on them not overlapping (post-critical) or\noverlapping (pre-critical). The equilibrium measure in the post-critical phase\nis known from earlier work. We determine the equilibrium measure in the\npre-critical phase using a particular conformal map, with the parameters\ntherein specified in terms of a root of a certain fourth order polynomial. This\nis used to determine the exact form of the electrostatic energy for the\npre-critical phase. Using a duality relation from random matrix theory, the\npartition function for the Coulomb gas at the inverse temperature $\\beta = 2$\ncan be expanded for large $N$ in the post-critical phase, and in a scaling\nregion of the post and pre-critical boundary. For the pre-critical phase, the\nduality identity implies a relation between two electrostatic energies, one for\nthe present sphere system, and the other for a certain constrained log-gas\nrelating to the Jacobi unitary ensemble.",
        "In a paper published in 2020 in Studia Mathematica, Abrahamsen et al. proved\nthat in the real space $L_1(\\mu)$, where $\\mu$ is a non-zero $\\sigma$-finite\n(countably additive non-negative) measure, norm-one elements in finite convex\ncombinations of relatively weakly open subsets of the unit ball are interior\npoints of these convex combinations in the relative weak topology. In this\npaper that result is generalised by proving that the same is true in the (real\nor complex) Lebesgue--Bochner spaces $L_1(\\mu,X)$ where $X$ is a weakly\nuniformly rotund Banach space.",
        "We present a novel approach to formalise and solve search-based problems\nusing large language models, which significantly improves upon previous\nstate-of-the-art results. We demonstrate the efficacy of this approach on the\nlogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to\ndirectly solve the puzzles, our method prompts the model to formalise the\nproblem in a logic-focused domain-specific language (DSL) called Logic.py. This\nformalised representation is then solved using a constraint solver, leveraging\nthe strengths of both the language model and the solver. Our approach achieves\na remarkable 65% absolute improvement over the baseline performance of Llama\n3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of\nover 90%. This significant advancement demonstrates the potential of combining\nlanguage models with domain-specific languages and auxiliary tools on\ntraditionally challenging tasks for LLMs.",
        "Multiple polylogarithms are equipped with rich algebraic structures including\nthe motivic coaction and the single-valued map which both found fruitful\napplications in high-energy physics. In recent work arXiv:2312.00697, the\ncurrent authors presented a conjectural reformulation of the motivic coaction\nand the single-valued map via zeta generators, certain operations on\nnon-commuting variables in suitable generating series of multiple\npolylogarithms. In this work, the conjectures of the reference will be proven\nfor multiple polylogarithms that depend on any number of variables on the\nRiemann sphere.",
        "Alpha mining, a critical component in quantitative investment, focuses on\ndiscovering predictive signals for future asset returns in increasingly complex\nfinancial markets. However, the pervasive issue of alpha decay, where factors\nlose their predictive power over time, poses a significant challenge for alpha\nmining. Traditional methods like genetic programming face rapid alpha decay\nfrom overfitting and complexity, while approaches driven by Large Language\nModels (LLMs), despite their promise, often rely too heavily on existing\nknowledge, creating homogeneous factors that worsen crowding and accelerate\ndecay. To address this challenge, we propose AlphaAgent, an autonomous\nframework that effectively integrates LLM agents with ad hoc regularizations\nfor mining decay-resistant alpha factors. AlphaAgent employs three key\nmechanisms: (i) originality enforcement through a similarity measure based on\nabstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor\nalignment via LLM-evaluated semantic consistency between market hypotheses and\ngenerated factors, and (iii) complexity control via AST-based structural\nconstraints, preventing over-engineered constructions that are prone to\noverfitting. These mechanisms collectively guide the alpha generation process\nto balance originality, financial rationale, and adaptability to evolving\nmarket conditions, mitigating the risk of alpha decay. Extensive evaluations\nshow that AlphaAgent outperforms traditional and LLM-based methods in\nmitigating alpha decay across bull and bear markets, consistently delivering\nsignificant alpha in Chinese CSI 500 and US S&P 500 markets over the past four\nyears. Notably, AlphaAgent showcases remarkable resistance to alpha decay,\nelevating the potential for yielding powerful factors.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "We consider a natural generalisation of the Painlev\\'e property and use it to\nidentify the known integrable cases of the Lane-Emden equation with a real\npositive index. We classify certain first-order ordinary differential equations\nwith this property and find necessary conditions for a large family of\nsecond-order equations. We consider ODEs such that, given any simply connected\ndomain $\\Omega$ not containing fixed singularities of the equation, the Riemann\nsurface of any solution obtained by analytic continuation along curves in\n$\\Omega$ has a finite number of sheets over $\\Omega$.",
        "Considered in this work is the Yang-Mills field in an extremal\nReissner-Nordstr\\\"om black hole, a physically motivated mathematical model\nintroduced by Bizo\\'n and Kahl. The kink is a fundamental, strongly unstable\nstationary solution in this non-perturbative, variable coefficients model, with\na polynomial tail and no explicit form. In this paper, we introduce and extend\nseveral virial techniques, adapt them to the inhomogeneous medium setting, and\nconstruct a finite codimensional manifold of the energy space where the kink is\nasymptotically stable. In particular, we handle, using virial techniques, the\nemergence of a weak threshold resonance in the description of the stable\nmanifold.",
        "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities\/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis.",
        "Comparative study of nonlinear crystals for picosecond difference frequency\ngeneration in mid-IR is presented. Nonlinear crystals of AgGaS$_2$,\nBaGa$_4$Se$_7$, LiGaSe$_2$, and LiGaS$_2$ were studied. Samples of AgGaS$_2$,\nBaGa$_4$Se$_7$, LiGaSe$_2$, and LiGaS$_2$ were tested in thee sets having\nlengths of 2, 4, or 8 mm. In order to investigate the dependence of efficiency\non the crystal length, three sets of crystals with lengths of 2, 4, or 8 mm\nwere tested. The developed tunable DFG system was driven by the 1.03 $\\mu$m,\n1.8 ps, Yb:YAG thin-disk laser system operated at the repetition rate of 10 or\n100 Hz. As the best result, picosecond mid-IR pulses at a wavelength of $\\sim$7\n$\\mu$m with the energy up to 130 $\\mu$J corresponding to the peak power of\n$\\sim$72 MW were generated using the 8 mm long LiGaS$_2$ crystal. Using the\nBaGa$_4$Se$_7$ crystal, DFG tunability in the wavelength range from 6 up to 13\n$\\mu$m was achieved.",
        "This note is the transcription of an interview with Professor Luigi Rodino,\non the occasion of the ISAAC-ICMAM Conference of Analysis in Developing\nCountries (December 2, 2024 - Bogot\\`a), that was dedicated to him. Luigi\nRodino is at present Emeritus Professor at the University of Turin, and a\nmember of the Accademia delle Scienze di Torino.",
        "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
        "Motivated by the applications of secure multiparty computation as a\nprivacy-protecting data analysis tool, and identifying oblivious transfer as\none of its main practical enablers, we propose a practical realization of\nrandomized quantum oblivious transfer. By using only symmetric cryptography\nprimitives to implement commitments, we construct computationally-secure\nrandomized oblivious transfer without the need for public-key cryptography or\nassumptions imposing limitations on the adversarial devices. We show that the\nprotocol is secure under an indistinguishability-based notion of security and\ndemonstrate an experimental implementation to test its real-world performance.\nIts security and performance are then compared to both quantum and classical\nalternatives, showing potential advantages over existing solutions based on the\nnoisy storage model and public-key cryptography.",
        "Humans are experts in collaborating with others physically by regulating\ncompliance behaviors based on the perception of their partner states and the\ntask requirements. Enabling robots to develop proficiency in human\ncollaboration skills can facilitate more efficient human-robot collaboration\n(HRC). This paper introduces an innovative impedance regulation skill learning\nframework for achieving HRC in multiple physical collaborative tasks. The\nframework is designed to adjust the robot compliance to the human partner\nstates while adhering to reference trajectories provided by human-human\ndemonstrations. Specifically, electromyography (EMG) signals from human muscles\nare collected and analyzed to extract limb impedance, representing compliance\nbehaviors during demonstrations. Human endpoint motions are captured and\nrepresented using a probabilistic learning method to create reference\ntrajectories and corresponding impedance profiles. Meanwhile, an LSTMbased\nmodule is implemented to develop task-oriented impedance regulation policies by\nmapping the muscle synergistic contributions between two demonstrators.\nFinally, we propose a wholebody impedance controller for a human-like robot,\ncoordinating joint outputs to achieve the desired impedance and reference\ntrajectory during task execution. Experimental validation was conducted through\na collaborative transportation task and two interactive Tai Chi pushing hands\ntasks, demonstrating superior performance from the perspective of interactive\nforces compared to a constant impedance control method.",
        "Active systems of self-rotating elements inherently exhibit chirality, making\nthem of fundamental interest due to parity violation. Using large-scale\nhydrodynamic simulations, we investigate the gelation of adhesive spinners\nconfined to quasi-2D monolayers at low Reynolds numbers. Unlike the coarsening\ndynamics of passive colloids, spinner gelation follows a different pathway,\ndisplaying structural chirality during the early stages of aggregation.\nHowever, this chirality dissipates upon dynamical arrest, resulting in a final\ngel structure that resembles a conventional colloidal gel. As a result, we find\nno sign of odd mechanical responses. Nonetheless, the elastic modulus and\ngelation time remain tunable through spinning activity, providing a new avenue\nfor the bottom-up design of programmable soft materials.",
        "Magnetic and magnetocaloric properties of Y$_{1-x}$Gd$_{x}$Co$_{2}$\ncompounds, where x = 0.2, 0.4, 0.6, 0.8 and 1.0, were investigated\nexperimentally and theoretically. Crystal structures were characterized by\nX-ray diffraction (Rietveld analysis) and investigated samples possess the\nMgCu$_{2}$-type single phase with Fd-3m space group. Melt-spinning process\nintroduced a chemical and topological disorder, which directly affected the\nmagnetic properties. Refrigerant capacity (RC), strictly connected to the full\nwidth at half maximum $\\delta$TFWHM of the $\\Delta$S$_M$(T) curve and the\nmaximum of magnetic entropy changes $\\Delta$S$_{Mpk}$(T)(T,$\\Delta$H),\nincreases from 29 to 148 J\/kg with replacement of Y by Gd atoms from x = 0.2 to\nx = 0.8. RC and $\\delta$TFWHM indicate the presence of disorder. Temperature\ndependences of magnetic entropy change $\\Delta$S$_M$(T,$\\Delta$H) and RC were\nmeasured in as-quenched and annealed state for Y$_{0.4}$Gd$_{0.6}$Co$_{2}$.\nThis particular composition was chosen for detailed investigation mainly due to\nits Curie point (T$_C$ = 282 K), which is close to the room temperature. After\nisothermal annealing ($\\tau_a$ = 60 min, Ta = 700$^o$C) RC decreased from 122\nto 104 J\/kg, which clearly indicates the homogenization of the heat treated\nsample. Furthermore, observed inverse magnetocaloric effect is associated with\nthe presence of antiferromagnetically coupled Gd and Co magnetic moments. The\nphase transition temperature increases with increasing Gd content from 74 to\n407 K for Y$_{0.8}$Gd$_{0.2}$Co$_{2}$ and GdCo2, respectively. Within the\nFPLO-LDA DFT method, the non-magnetic ground state for YCo$_{2}$ and the\nmagnetic ground state for GdCo$_{2}$ are predicted in agreement with\nexperiment. The dependence of calculated total and species-resolved magnetic\nmoments on Gd concentration reasonably agrees with available experimental data.",
        "The rise of foundation models fine-tuned on human feedback from potentially\nuntrusted users has increased the risk of adversarial data poisoning,\nnecessitating the study of robustness of learning algorithms against such\nattacks. Existing research on provable certified robustness against data\npoisoning attacks primarily focuses on certifying robustness for static\nadversaries who modify a fraction of the dataset used to train the model before\nthe training algorithm is applied. In practice, particularly when learning from\nhuman feedback in an online sense, adversaries can observe and react to the\nlearning process and inject poisoned samples that optimize adversarial\nobjectives better than when they are restricted to poisoning a static dataset\nonce, before the learning algorithm is applied. Indeed, it has been shown in\nprior work that online dynamic adversaries can be significantly more powerful\nthan static ones. We present a novel framework for computing certified bounds\non the impact of dynamic poisoning, and use these certificates to design robust\nlearning algorithms. We give an illustration of the framework for the mean\nestimation and binary classification problems and outline directions for\nextending this in further work. The code to implement our certificates and\nreplicate our results is available at\nhttps:\/\/github.com\/Avinandan22\/Certified-Robustness.",
        "Experiments involving sensory analysis of foods and beverages are beneficial\nfor selecting healthy products and assessing the preferences of potential\nconsumers. They are generally planned in incomplete blocks, and their\nattributes, such as aroma, colour, and flavour, are evaluated using a 9-point\nhedonic scale, characterising an ordinal variable response. Also, the\ngeneralised logit model with random effects for panellists is one of the\nappropriate models to relate the multivariate response to the covariates. This\nstudy aims to present a method for analysing sensory attributes through a\nunified multivariate model. Due to the nature of the variable, each separate\nmodel already corresponds to a multivariate analysis, so our proposal would\nincorporate a complete analysis with solely one model. This proposal is based\non multivariate methods for categorical data and maximum likelihood theory. Our\nmethod was evaluated through a simulation study, in which we consider three\ndistinct formulations with two attributes to represent various formulation\nselection scenarios via mixed discrete models. The simulated results\ndemonstrated overall concordance rates exceeding 80\\% for the unified model\ncompared to the separate models. Moreover, as motivation is presented, a study\nof 13 prebiotic beverages based on cashew nut almonds added to grape juice,\nwith 130 potential consumers. The attributes evaluated were overall impression,\naroma, Body, sweetness and flavour, using a 9-point hedonic scale. The selected\nunified model considering all attributes was the non-proportional odds\nmixed-effect model. According to this model, the prebiotic beverage\nformulations most likely to be accepted were: 8\\% sugar and 40\\% grape juice\n($F_4$), 6\\% sugar and 44\\% grape juice ($F_6$), and 9\\% sugar and 30\\% grape\njuice ($F_{13}$). The unified analysis and computational time showed the\nadvantages of this proposal.",
        "This study explores the implementation of the Quantum Approximate\nOptimisation Algorithm (QAOA) in its analog form using a neutral atom quantum\nprocessing unit to solve the Maximum Independent Set problem. The analog QAOA\nleverages the natural encoding of problem Hamiltonians by Rydberg atom\ninteractions, while employing Bayesian Optimisation to navigate the\nquantum-classical parameter space effectively under the constraints of hardware\nnoise and resource limitations. We evaluate the approach through a combination\nof simulations and experimental runs on Pasqal's first commercial quantum\nprocessing unit, Orion Alpha, demonstrating effective parameter optimisation\nand noise mitigation strategies, such as selective bitstring discarding and\ndetection error corrections. Results show that a limited number of measurements\nstill allows for a quick convergence to a solution, making it a viable solution\nfor resource-efficient scenarios.",
        "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.",
        "Using the pinch technique, we compute the one-loop vertices of weak\ninteractions in the B-LSSM and incorporate their pinch contributions into the\ngauge boson self-energies. Compared to the definitions of the $S$, $T$, and $U$\nparameters in the Standard Model based on the $SU(2)_L\\otimes U(1)_Y$ group,\nthe corresponding parameters in the B-LSSM are modified. We provide these\nredefined $S$, $T$, and $U$ parameters and demonstrate the convergence of the\nresults. In the framework of the low-energy effective Lagrangian for weak\ninteractions, the $S$, $T$, and $U$ parameters can be expressed as functions of\ncertain parameters in the B-LSSM. The updated experimental and fitting results\nconstrain the parameter space of the B-LSSM strongly.",
        "This paper introduces IRIS, an immersive Robot Interaction System leveraging\nExtended Reality (XR), designed for robot data collection and interaction\nacross multiple simulators, benchmarks, and real-world scenarios. While\nexisting XR-based data collection systems provide efficient and intuitive\nsolutions for large-scale data collection, they are often challenging to\nreproduce and reuse. This limitation arises because current systems are highly\ntailored to simulator-specific use cases and environments. IRIS is a novel,\neasily extendable framework that already supports multiple simulators,\nbenchmarks, and even headsets. Furthermore, IRIS is able to include additional\ninformation from real-world sensors, such as point clouds captured through\ndepth cameras. A unified scene specification is generated directly from\nsimulators or real-world sensors and transmitted to XR headsets, creating\nidentical scenes in XR. This specification allows IRIS to support any of the\nobjects, assets, and robots provided by the simulators. In addition, IRIS\nintroduces shared spatial anchors and a robust communication protocol that\nlinks simulations between multiple XR headsets. This feature enables multiple\nXR headsets to share a synchronized scene, facilitating collaborative and\nmulti-user data collection. IRIS can be deployed on any device that supports\nthe Unity Framework, encompassing the vast majority of commercially available\nheadsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and\nthe HoloLens 2. IRIS showcased its versatility across a wide range of\nreal-world and simulated scenarios, using current popular robot simulators such\nas MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study\nevaluates IRIS on a data collection task for the LIBERO benchmark. The study\nshows that IRIS significantly outperforms the baseline in both objective and\nsubjective metrics.",
        "Residual moveout (RMO) provides critical information for travel time\ntomography. The current industry-standard method for fitting RMO involves\nscanning high-order polynomial equations. However, this analytical approach\ndoes not accurately capture local saltation, leading to low iteration\nefficiency in tomographic inversion. Supervised learning-based image\nsegmentation methods for picking can effectively capture local variations;\nhowever, they encounter challenges such as a scarcity of reliable training\nsamples and the high complexity of post-processing. To address these issues,\nthis study proposes a deep learning-based cascade picking method. It\ndistinguishes accurate and robust RMOs using a segmentation network and a\npost-processing technique based on trend regression. Additionally, a data\nsynthesis method is introduced, enabling the segmentation network to be trained\non synthetic datasets for effective picking in field data. Furthermore, a set\nof metrics is proposed to quantify the quality of automatically picked RMOs.\nExperimental results based on both model and real data demonstrate that,\ncompared to semblance-based methods, our approach achieves greater picking\ndensity and accuracy."
      ]
    }
  },
  {
    "id":2411.01144,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images.",
    "start_abstract":"Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods.",
    "start_categories":[
      "Lung Ultrasound"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A Simple Framework for Contrastive Learning of Visual Representations"
      ],
      "abstract":[
        "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed self-supervised algorithms without requiring specialized architectures or memory bank. In order to understand what enables the prediction tasks learn useful representations, we systematically study major components our framework. show that (1) composition data augmentations plays critical role in defining effective predictive tasks, (2) introducing learnable nonlinear transformation between representation and loss substantially improves quality learned (3) benefits from larger batch sizes more training steps compared supervised learning. By combining these findings, are able considerably outperform previous methods semi-supervised on ImageNet. A linear classifier trained representations by SimCLR achieves 76.5% top-1 accuracy, which is 7% relative improvement over state-of-the-art, matching performance ResNet-50. When fine-tuned only 1% labels, achieve 85.8% top-5 outperforming AlexNet with 100X fewer labels."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "DIRIGENt: End-To-End Robotic Imitation of Human Demonstrations Based on\n  a Diffusion Model",
        "Highly multi-mode anti-resonant hollow core fibres",
        "Fluctuations of the linear functionals for supercritical non-local\n  branching superprocesses",
        "Interpretable Droplet Digital PCR Assay for Trustworthy Molecular\n  Diagnostics",
        "Magneto-Optics of Anisotropic Exciton Polaritons in Two-Dimensional\n  Perovskites",
        "Solving Situation Puzzles with Large Language Model and External\n  Reformulation",
        "Can synchrotron radiation reveal the presence of {\\it dark sector}\n  around black hole?",
        "Non-Gaussianity of invariant measures to SPDEs in Da Prato-Debussche\n  regime",
        "MAP: Multi-user Personalization with Collaborative LLM-powered Agents",
        "A Lightweight Deep Exclusion Unfolding Network for Single Image\n  Reflection Removal",
        "Fast But Accurate: A Real-Time Hyperelastic Simulator with Robust\n  Frictional Contact",
        "An improved nonparametric test and sample size procedures for the\n  randomized complete block designs",
        "Local limits of high energy eigenfunctions on integrable billiards",
        "Logarithmic double phase problems with generalized critical growth",
        "Sebra: Debiasing Through Self-Guided Bias Ranking",
        "Unraveling Reverse Annealing: A Study of D-Wave Quantum Annealers",
        "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept\n  Representations",
        "Hierarchical Lexical Manifold Projection in Large Language Models: A\n  Novel Mechanism for Multi-Scale Semantic Representation",
        "SMTL: A Stratified Logic for Expressive Multi-Level Temporal\n  Specifications",
        "Sharply k-transitive actions on ultrahomogeneous structures",
        "Task-Aware Virtual Training: Enhancing Generalization in\n  Meta-Reinforcement Learning for Out-of-Distribution Tasks",
        "Categorification of Rainbow Paths on Odd Reflection Graphs through\n  Homomorphisms between Verma Supermodules",
        "A Systematic Evaluation of Generative Models on Tabular Transportation\n  Data",
        "Shots and variance on noisy quantum circuits",
        "Neural Posterior Estimation for Cataloging Astronomical Images with\n  Spatially Varying Backgrounds and Point Spread Functions",
        "spike: A tool to drizzle HST, JWST, and Roman PSFs for improved analyses",
        "bi-Lipschitz versus Analytic equivalence of two variable complex\n  quasihomogeneous function-germs",
        "Flavor-Dependent Long-Range Neutrino Interactions in DUNE and T2HK:\n  Synergy Breeds Power",
        "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks"
      ],
      "abstract":[
        "There has been substantial progress in humanoid robots, with new skills\ncontinuously being taught, ranging from navigation to manipulation. While these\nabilities may seem impressive, the teaching methods often remain inefficient.\nTo enhance the process of teaching robots, we propose leveraging a mechanism\neffectively used by humans: teaching by demonstrating. In this paper, we\nintroduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel\nend-to-end diffusion approach that directly generates joint values from\nobserving human demonstrations, enabling a robot to imitate these actions\nwithout any existing mapping between it and humans. We create a dataset in\nwhich humans imitate a robot and then use this collected data to train a\ndiffusion model that enables a robot to imitate humans. The following three\naspects are the core of our contribution. First is our novel dataset with\nnatural pairs between human and robot poses, allowing our approach to imitate\nhumans accurately despite the gap between their anatomies. Second, the\ndiffusion input to our model alleviates the challenge of redundant joint\nconfigurations, limiting the search space. And finally, our end-to-end\narchitecture from perception to action leads to an improved learning\ncapability. Through our experimental analysis, we show that combining these\nthree aspects allows DIRIGENt to outperform existing state-of-the-art\napproaches in the field of generating joint values from RGB images.",
        "We report the characterisation of anti-resonant hollow core optical fibres\nguiding at least 50 spatial modes in the infrared. Their propagation losses\nwere measured to be between 0.1 and 0.2 dB\/m from 1000 to 1500 nm wavelength,\nwith bend losses of less than 3 dB\/turn for bend radii of 7.5 cm despite core\nradii greater than 60 times the guided wavelengths.",
        "Suppose $\\{X_{t}:t\\ge 0\\}$ is a supercritical superprocess on a Luzin space\n$E$, with a non-local branching mechanism and probabilities\n$\\mathbb{P}_{\\delta_{x}}$, when initiated from a unit mass at $x\\in E$. By\n``supercritical\", we mean that the first moment semigroup of $X_{t}$ exhibits a\nPerron-Frobenius type behaviour characterized by an eigentriple\n$(\\lambda_{1},\\varphi,\\widetilde{\\varphi})$, where the principal eigenvalue\n$\\lambda_{1}$ is greater than $0$. Under a second moment condition, we prove\nthat $X_{t}$ satisfies a law of large numbers. The main purpose of this paper\nis to further investigate the fluctuations of the linear functional\n$\\mathrm{e}^{-\\lambda_{1}t}\\langle f,X_{t}\\rangle$ around the limit given by\nthe law of large numbers. To this end, we introduce a parameter $\\epsilon(f)$\nfor a bounded measurable function $f$, which determines the exponent term of\nthe decay rate for the first moment of the fluctuation. Qualitatively, the\nsecond-order behaviour of $\\langle f,X_{t}\\rangle$ depends on the sign of\n$\\epsilon(f)-\\lambda_{1}\/2$. We prove that, for a suitable test function $f$,\nthe fluctuation of the associated linear functional exhibits distinct\nasymptotic behaviours depending on the magnitude of $\\epsilon(f)$: If\n$\\epsilon(f)\\ge \\lambda_{1}\/2$, the fluctuation converges in distribution to a\nGaussian limit under appropriate normalization; If $\\epsilon(f)<\\lambda_{1}\/2$,\nthe fluctuation converges to an $L^{2}$ limit with a larger normalization\nfactor. In particular, when the test function is chosen as the right\neigenfunction $\\varphi$, we establish a functional central limit theorem. As an\napplication, we consider a multitype superdiffusion in a bounded domain. For\nthis model, we derive limit theorems for the fluctuations of arbitrary linear\nfunctionals.",
        "Accurate molecular quantification is essential for advancing research and\ndiagnostics in fields such as infectious diseases, cancer biology, and genetic\ndisorders. Droplet digital PCR (ddPCR) has emerged as a gold standard for\nachieving absolute quantification. While computational ddPCR technologies have\nadvanced significantly, achieving automatic interpretation and consistent\nadaptability across diverse operational environments remains a challenge. To\naddress these limitations, we introduce the intelligent interpretable droplet\ndigital PCR (I2ddPCR) assay, a comprehensive framework integrating front-end\npredictive models (for droplet segmentation and classification) with GPT-4o\nmultimodal large language model (MLLM, for context-aware explanations and\nrecommendations) to automate and enhance ddPCR image analysis. This approach\nsurpasses the state-of-the-art models, affording 99.05% accuracy in processing\ncomplex ddPCR images containing over 300 droplets per image with varying\nsignal-to-noise ratios (SNRs). By combining specialized neural networks and\nlarge language models, the I2ddPCR assay offers a robust and adaptable solution\nfor absolute molecular quantification, achieving a sensitivity capable of\ndetecting low-abundance targets as low as 90.32 copies\/{\\mu}L. Furthermore, it\nimproves model's transparency through detailed explanation and troubleshooting\nguidance, empowering users to make informed decisions. This innovative\nframework has the potential to benefit molecular diagnostics, disease research,\nand clinical applications, especially in resource-constrained settings.",
        "Layered 2D organic-inorganic perovskite semiconductors support strongly\nconfined excitons that offer significant potential for ultrathin polaritonic\ndevices due to their tunability and huge oscillator strength. The application\nof a magnetic field has proven to be an invaluable tool for investigating the\nexciton fine structure observed in these materials. Yet, the combination of an\nin-plane magnetic field and the strong coupling regime has remained largely\nunexplored. In this work, we combine microscopic theory with a rigorous\nsolution of Maxwell's equations to model the magneto-optics of exciton\npolaritons in 2D perovskites. We predict that the brightened dark exciton state\ncan enter the strong coupling regime. Furthermore, the magnetic-field-induced\nmixing of polarization selection rules and the breaking of in-plane symmetry\nlead to highly anisotropic polariton branches. This study contributes to a\nbetter understanding of the exciton fine structure in 2D perovskites and\ndemonstrates the cavity control of highly anisotropic and\npolarization-sensitive exciton polaritons.",
        "In recent years, large language models (LLMs) have shown an impressive\nability to perform arithmetic and symbolic reasoning tasks. However, we found\nthat LLMs (e.g., ChatGPT) cannot perform well on reasoning that requires\nmultiple rounds of dialogue, especially when solving situation puzzles.\nSpecifically, LLMs intend to ask very detailed questions focusing on a specific\naspect or same\/similar questions after several rounds of Q&As. To help LLMs get\nout of the above dilemma, we propose a novel external reformulation\nmethodology, where the situation puzzle will be reformulated after several\nrounds of Q&A or when the LLMs raise an incorrect guess. Experiments show\nsuperior performance (e.g., win rate, number of question\/guess attempts) of our\nmethod than directly using LLMs for solving situation puzzles, highlighting the\npotential of strategic problem reformulation to enhance the reasoning\ncapabilities of LLMs in complex interactive scenarios.",
        "We studied synchrotron radiation of a massive charged under visible and\nhidden sector groups, moving in equatorial plane around spherically symmetric\nweakly magnetized black hole. As a model of dark matter we choose the one, in\nwhich Maxwell field is coupled to the additional $U(1)$-gauge field envisaging\nthe dark sector, the so-called dark photon model. Magnetization of a black hole\nalso stems from Maxwell-dark photon electrodynamics. One found the radiation\npower and energy loss of the particle and looked for the imprints of dark\nmatter on those phenomena.",
        "We propose an elementary method to show non-Gaussianity of invariant measures\nof parabolic stochastic partial differential equations with polynomial\nnon-linearities in the Da Prato--Debussche regime. The approach is essentially\nalgebraic and involves using the generator equation of the SPDE at\nstationarity. Our results in particular cover the $\\Phi^4_\\delta$ measures in\ndimensions $\\delta<\\frac{14}{5}$, which includes cases where the invariant\nmeasure is singular with respect to the invariant measure of the linear\nsolution.",
        "The widespread adoption of Large Language Models (LLMs) and LLM-powered\nagents in multi-user settings underscores the need for reliable, usable methods\nto accommodate diverse preferences and resolve conflicting directives. Drawing\non conflict resolution theory, we introduce a user-centered workflow for\nmulti-user personalization comprising three stages: Reflection, Analysis, and\nFeedback. We then present MAP -- a \\textbf{M}ulti-\\textbf{A}gent system for\nmulti-user \\textbf{P}ersonalization -- to operationalize this workflow. By\ndelegating subtasks to specialized agents, MAP (1) retrieves and reflects on\nrelevant user information, while enhancing reliability through agent-to-agent\ninteractions, (2) provides detailed analysis for improved transparency and\nusability, and (3) integrates user feedback to iteratively refine results. Our\nuser study findings (n=12) highlight MAP's effectiveness and usability for\nconflict resolution while emphasizing the importance of user involvement in\nresolution verification and failure management. This work highlights the\npotential of multi-agent systems to implement user-centered, multi-user\npersonalization workflows and concludes by offering insights for\npersonalization in multi-user contexts.",
        "Single Image Reflection Removal (SIRR) is a canonical blind source separation\nproblem and refers to the issue of separating a reflection-contaminated image\ninto a transmission and a reflection image. The core challenge lies in\nminimizing the commonalities among different sources. Existing deep learning\napproaches either neglect the significance of feature interactions or rely on\nheuristically designed architectures. In this paper, we propose a novel Deep\nExclusion unfolding Network (DExNet), a lightweight, interpretable, and\neffective network architecture for SIRR. DExNet is principally constructed by\nunfolding and parameterizing a simple iterative Sparse and Auxiliary Feature\nUpdate (i-SAFU) algorithm, which is specifically designed to solve a new\nmodel-based SIRR optimization formulation incorporating a general exclusion\nprior. This general exclusion prior enables the unfolded SAFU module to\ninherently identify and penalize commonalities between the transmission and\nreflection features, ensuring more accurate separation. The principled design\nof DExNet not only enhances its interpretability but also significantly\nimproves its performance. Comprehensive experiments on four benchmark datasets\ndemonstrate that DExNet achieves state-of-the-art visual and quantitative\nresults while utilizing only approximately 8\\% of the parameters required by\nleading methods.",
        "We present a GPU-friendly framework for real-time implicit simulation of\nelastic material in the presence of frictional contacts. The integration of\nhyperelasticity, non-interpenetration contact, and friction in real-time\nsimulations presents formidable nonlinear and non-smooth problems, which are\nhighly challenging to solve. By incorporating nonlinear complementarity\nconditions within the local-global framework, we achieve rapid convergence in\naddressing these challenges. While the structure of local-global methods is not\nfully GPU-friendly, our proposal of a simple yet efficient solver with sparse\npresentation of the system inverse enables highly parallel computing while\nmaintaining a fast convergence rate. Moreover, our novel splitting strategy for\nnon-smooth indicators not only amplifies overall performance but also refines\nthe complementarity preconditioner, enhancing the accuracy of frictional\nbehavior modeling. Through extensive experimentation, the robustness of our\nframework in managing real-time contact scenarios, ranging from large-scale\nsystems and extreme deformations to non-smooth contacts and precise friction\ninteractions, has been validated. Compatible with a wide range of hyperelastic\nmodels, our approach maintains efficiency across both low and high stiffness\nmaterials. Despite its remarkable efficiency, robustness, and generality, our\nmethod is elegantly simple, with its core contributions grounded solely on\nstandard matrix operations.",
        "The Friedman test has been extensively applied as a nonparametric alternative\nto the conventional F procedure for comparing treatment effects in randomized\ncomplete block designs. A chi-square distribution provides a convenient\napproximation to determining the critical values for the Friedman procedure in\nhypothesis testing. However, the chi-square approximation is generally\nconservative and the accuracy declines with increasing number of treatments.\nThis paper describes an alternative transformation of the Friedman statistic\nalong with an approximate F distribution that has the same numerator degrees of\nfreedom as the ANOVA F test. Moreover, two approximate noncentral F\ndistributions are presented for the proposed F-transformation under the\nalternative hypothesis of heterogeneous location shifts. Explicit power\nfunctions are derived when the underlying populations have the uniform, normal,\nLaplace, and exponential distributions. Theoretical examination and empirical\nassessment are presented to validate the advantages of the proposed approaches\nover the existing methods of the Friedman test. The developed test and power\nprocedures are recommended due to their consistently acceptable Type I error\nrates and accurate power calculations for the location shift structures and\npopulation distributions considered here.",
        "Berry's random wave conjecture posits that high energy eigenfunctions of\nchaotic systems resemble random monochromatic waves at the Planck scale. One\nimportant consequence is that, at the Planck scale around \"many\" points in the\nmanifold, any solution to the Helmholtz equation $\\Delta\\varphi+\\varphi =0$ can\nbe approximated by high energy eigenfunctions. This property, sometimes called\ninverse localization, has useful applications to the study of the nodal sets of\neigenfunctions. Alas, the only manifold for which the local limits of a\nsequence of high energy eigenfunctions are rigorously known to be given by\nrandom waves is the flat torus $(\\mathbf{R}\/\\mathbf{Z})^2$, which is certainly\nnot chaotic.\n  Our objective in this paper is to study the validity of this \"inverse\nlocalization\" property in the class of integrable billiards, exploiting the\nfact that integrable polygonal billiards are classified and that Birkhoff\nconjectured that ellipses are the only smooth integrable billiards. Our main\nresults show that, while there are infinitely many integrable polygons\nexhibiting good inverse localization properties, for \"most\" integrable polygons\nand ellipses, this property fails dramatically. We thus conclude that, in a\ngeneric integrable billiard, the local limits of Dirichlet and Neumann\neigenfunctions do not match random waves, as one might expect in view of\nBerry's conjecture. Extensions to higher dimensions and nearly integrable\npolygons are discussed too.",
        "In this paper we study logarithmic double phase problems with variable\nexponents involving nonlinearities that have generalized critical growth. We\nfirst prove new continuous and compact embedding results in order to guarantee\nthe well-definedness by studying the Sobolev conjugate function of our\ngeneralized $N$-function. In the second part we prove the concentration\ncompactness principle for Musielak-Orlicz Sobolev spaces having logarithmic\ndouble phase modular function structure. Based on this we are going to show\nmultiplicity results for the problem under consideration for superlinear and\nsublinear growth, respectively.",
        "Ranking samples by fine-grained estimates of spuriosity (the degree to which\nspurious cues are present) has recently been shown to significantly benefit\nbias mitigation, over the traditional binary biased-\\textit{vs}-unbiased\npartitioning of train sets. However, this spuriosity ranking comes with the\nrequirement of human supervision. In this paper, we propose a debiasing\nframework based on our novel \\ul{Se}lf-Guided \\ul{B}ias \\ul{Ra}nking\n(\\emph{Sebra}), that mitigates biases (spurious correlations) via an automatic\nranking of data points by spuriosity within their respective classes. Sebra\nleverages a key local symmetry in Empirical Risk Minimization (ERM) training --\nthe ease of learning a sample via ERM inversely correlates with its\nspuriousity; the fewer spurious correlations a sample exhibits, the harder it\nis to learn, and vice versa. However, globally across iterations, ERM tends to\ndeviate from this symmetry. Sebra dynamically steers ERM to correct this\ndeviation, facilitating the sequential learning of attributes in increasing\norder of difficulty, \\ie, decreasing order of spuriosity. As a result, the\nsequence in which Sebra learns samples naturally provides spuriousity rankings.\nWe use the resulting fine-grained bias characterization in a contrastive\nlearning framework to mitigate biases from multiple sources. Extensive\nexperiments show that Sebra consistently outperforms previous state-of-the-art\nunsupervised debiasing techniques across multiple standard benchmarks,\nincluding UrbanCars, BAR, CelebA, and ImageNet-1K. Code, pre-trained models,\nand training logs are available at https:\/\/kadarsh22.github.io\/sebra_iclr25\/.",
        "D-Wave quantum annealers offer reverse annealing as a feature allowing them\nto refine solutions to optimization problems. This paper investigates the\ninfluence of key parameters, such as annealing times and reversal distance, on\nthe behavior of reverse annealing by studying models containing up to 1000\nqubits. Through the analysis of theoretical models and experimental data, we\nexplore the interplay between quantum and classical processes. Our findings\nprovide a deeper understanding that can better equip users to fully harness the\npotential of the D-Wave annealers",
        "Concept Activation Vectors (CAVs) are widely used to model\nhuman-understandable concepts as directions within the latent space of neural\nnetworks. They are trained by identifying directions from the activations of\nconcept samples to those of non-concept samples. However, this method often\nproduces similar, non-orthogonal directions for correlated concepts, such as\n\"beard\" and \"necktie\" within the CelebA dataset, which frequently co-occur in\nimages of men. This entanglement complicates the interpretation of concepts in\nisolation and can lead to undesired effects in CAV applications, such as\nactivation steering. To address this issue, we introduce a post-hoc concept\ndisentanglement method that employs a non-orthogonality loss, facilitating the\nidentification of orthogonal concept directions while preserving directional\ncorrectness. We evaluate our approach with real-world and controlled correlated\nconcepts in CelebA and a synthetic FunnyBirds dataset with VGG16 and ResNet18\narchitectures. We further demonstrate the superiority of orthogonalized concept\nrepresentations in activation steering tasks, allowing (1) the insertion of\nisolated concepts into input images through generative models and (2) the\nremoval of concepts for effective shortcut suppression with reduced impact on\ncorrelated concepts in comparison to baseline CAVs.",
        "The integration of structured hierarchical embeddings into transformer-based\narchitectures introduces a refined approach to lexical representation, ensuring\nthat multi-scale semantic relationships are preserved without compromising\ncomputational efficiency. A projection mechanism that maps tokens onto a\nstructured manifold provides improved lexical alignment, enhancing the\nadaptability of word representations across diverse linguistic tasks. The\nstructured encoding framework ensures that hierarchical embeddings maintain\ncoherence across varying abstraction levels, allowing for stable transitions\nbetween localized syntactic features and global semantic structures.\nExperimental evaluations indicate that hierarchical embeddings consistently\noutperform conventional token representations, improving accuracy in linguistic\nbenchmarks while maintaining lower computational overhead. Comparative analysis\nacross multiple domains highlights the ability of hierarchical embeddings to\nretain contextual consistency, particularly in specialized language\napplications where structured lexical alignment is essential. Statistical\nassessments further demonstrate that hierarchical embeddings exhibit enhanced\nrobustness under perturbation conditions, ensuring that linguistic structures\nremain stable across adversarial text modifications. The integration of\nhierarchical projections with transformer attention mechanisms enables improved\ncontextual adaptation, ensuring that token representations are dynamically\nadjusted based on varying linguistic distributions. The refined hierarchical\norganization of embeddings provides greater interpretability in lexical\nmodeling, facilitating enhanced generalization capabilities across diverse text\nprocessing tasks.",
        "We present Stratified Metric Temporal Logic (SMTL), a novel formalism for\nspecifying and verifying properties of complex cyber-physical systems that\nexhibit behaviors across multiple temporal and abstraction scales. SMTL extends\nexisting temporal logics by incorporating a stratification operator, enabling\nthe association of temporal properties with specific abstraction levels. This\nallows for the natural expression of multi-scale requirements while maintaining\nformal reasoning about inter-level relationships. We formalize the syntax and\nsemantics of SMTL, proving that it strictly subsumes metric temporal logic\n(MTL) and offers enhanced expressiveness by capturing properties unattainable\nin existing logics. Numerical simulations comparing agents operating under MTL\nand SMTL specifications show that SMTL enhances agent coordination and safety,\nreducing collision rates without substantial computational overhead or\ncompromising path efficiency. These findings underscore SMTL's potential as a\nvaluable tool for designing and verifying complex multi-agent systems operating\nacross diverse temporal and abstraction scales.",
        "Given an action of a group $G$ by automorphisms on an infinite relational\nstructure $\\mathcal{M}$, we say that the action is structurally sharply\n$k$-transitive if, for any two $k$-tuples $\\bar{a}, \\bar{b} \\in M^k$ of\ndistinct elements such that $\\bar{a} \\mapsto \\bar{b}$ is an isomorphism, there\nexists exactly one element of $G$ sending $\\bar{a}$ to $\\bar{b}$. This\ngeneralises the well-known notion of a sharply $k$-transitive action on a set.\nWe show that, for $k \\leq 3$, a wide range of countable ultrahomogeneous\nstructures admit structurally sharply $k$-transitive actions by finitely\ngenerated virtually free groups, giving a substantial answer to a question of\nCameron from the book Oligomorphic Permutation Groups. We also show that the\nrandom $k$-hypertournament admits a structurally sharply $k$-transitive action\nfor $k=4,5$, and that $\\mathbb{Q}$ and several of its reducts admit\nstructurally sharply $k$-transitive actions for all $k$. (This contrasts with\nthe case of sets, where for $k \\geq 4$ there are no sharply $k$-transitive\nactions on infinite sets by results of Tits and Hall.) We also show the\nexistence of sharply $2$-transitive actions of finitely generated virtually\nfree groups on an infinite set, solving the open question of whether such\nactions exist for hyperbolic groups.\n  [Note: this is an early working draft.]",
        "Meta reinforcement learning aims to develop policies that generalize to\nunseen tasks sampled from a task distribution. While context-based meta-RL\nmethods improve task representation using task latents, they often struggle\nwith out-of-distribution (OOD) tasks. To address this, we propose Task-Aware\nVirtual Training (TAVT), a novel algorithm that accurately captures task\ncharacteristics for both training and OOD scenarios using metric-based\nrepresentation learning. Our method successfully preserves task characteristics\nin virtual tasks and employs a state regularization technique to mitigate\noverestimation errors in state-varying environments. Numerical results\ndemonstrate that TAVT significantly enhances generalization to OOD tasks across\nvarious MuJoCo and MetaWorld environments.",
        "We examine the interplay between different highest weight structures in the\nBGG category O of basic Lie superalgebras arising from distinct choices of\nBorel subalgebras-an aspect that has no analogous theory in the classical Lie\nalgebra setting.\n  We describe the compositions of homomorphisms between Verma supermodules\narising from different highest weight structure but sharing the same character.\nThis requires us to define a class of edge-colored graphs, defined purely graph\ntheoretically, by extracting the exchange property of odd reflections. As an\napplication of these homomorphisms, we generalize results of\nCoulembier-Serganova and Chen-Mazorchuk concerning the associated varieties and\nprojective dimensions of Verma supermodules.",
        "The sharing of large-scale transportation data is beneficial for\ntransportation planning and policymaking. However, it also raises significant\nsecurity and privacy concerns, as the data may include identifiable personal\ninformation, such as individuals' home locations. To address these concerns,\nsynthetic data generation based on real transportation data offers a promising\nsolution that allows privacy protection while potentially preserving data\nutility. Although there are various synthetic data generation techniques, they\nare often not tailored to the unique characteristics of transportation data,\nsuch as the inherent structure of transportation networks formed by all trips\nin the datasets. In this paper, we use New York City taxi data as a case study\nto conduct a systematic evaluation of the performance of widely used tabular\ndata generative models. In addition to traditional metrics such as distribution\nsimilarity, coverage, and privacy preservation, we propose a novel graph-based\nmetric tailored specifically for transportation data. This metric evaluates the\nsimilarity between real and synthetic transportation networks, providing\npotentially deeper insights into their structural and functional alignment. We\nalso introduced an improved privacy metric to address the limitations of the\ncommonly-used one. Our experimental results reveal that existing tabular data\ngenerative models often fail to perform as consistently as claimed in the\nliterature, particularly when applied to transportation data use cases.\nFurthermore, our novel graph metric reveals a significant gap between synthetic\nand real data. This work underscores the potential need to develop generative\nmodels specifically tailored to take advantage of the unique characteristics of\nemerging domains, such as transportation.",
        "We present a method for estimating the number of shots required for some\ndesired variance in the results of a quantum circuit. First, we establish a\nbaseline for a single qubit characterization of individual noise sources\nseparately. We then extend the method to multi-qubit problems and test our\nmethod on two case studies. We will proceed to estimate the number of shots\nrequired for a desired variance in the result or, equivalently estimate the\nvariance at a known number of shots. We will show we're able to estimate\nvariance accurately to within a factor of 2. Following these, we also provide a\nclosed-form expression for variance at a given number of shots.",
        "Neural posterior estimation (NPE), a type of amortized variational inference,\nis a computationally efficient means of constructing probabilistic catalogs of\nlight sources from astronomical images. To date, NPE has not been used to\nperform inference in models with spatially varying covariates. However,\nground-based astronomical images have spatially varying sky backgrounds and\npoint spread functions (PSFs), and accounting for this variation is essential\nfor constructing accurate catalogs of imaged light sources. In this work, we\nintroduce a method of performing NPE with spatially varying backgrounds and\nPSFs. In this method, we generate synthetic catalogs and semi-synthetic images\nfor these catalogs using randomly sampled PSF and background estimates from\nexisting surveys. Using this data, we train a neural network, which takes an\nastronomical image and representations of its background and PSF as input, to\noutput a probabilistic catalog. Our experiments with Sloan Digital Sky Survey\ndata demonstrate the effectiveness of NPE in the presence of spatially varying\nbackgrounds and PSFs for light source detection, star\/galaxy separation, and\nflux measurement.",
        "Point spread functions (PSFs) describe the distribution of light for a pure\npoint source in an astronomical image due to the instrument optics. For\ndeconvolution, as for point source photometry and for source removal, it is key\nto have an accurate PSF for a particular image. Space-based telescopes can then\npose a challenge as their PSFs are informed by their complex construction, and\nthe myriad of pointings and rotations used to capture deep images. These\ntelescopes also capture the highest resolution images of astronomical sources,\nresolving stars around even relatively distant galaxies. Proper co-addition of\nPSFs at a specific source position for space-based imaging is then both\ncritical and challenging. This code, spike, generates model PSFs and runs them\nthrough the same processing pipeline used to derive deep, co-added images,\nproviding correctly co-added and resampled PSFs for images from the Hubble\nSpace Telescope, the James Webb Space Telescope, and the Nancy Grace Roman\nSpace Telescope.",
        "In this paper we address the problem of classifying complex (non-homogeneous)\nquasihomogeneous polynomials in two variables under bi-Lipschitz equivalence.\nWe prove that pairs of such polynomials are (right) bi-Lipschitz equivalent as\nfunction-germs at $0\\in\\mathbb{C}^{2}$ iff they are analytically equivalent.",
        "Discovering new neutrino interactions would provide evidence of physics\nbeyond the Standard Model. We focus on flavor-dependent long-range neutrino\ninteractions mediated by ultra-light mediators (masses below $10^{-10}$ eV)\nfrom lepton-number gauge symmetries $L_e-L_\\mu$, $L_e-L_\\tau$, and\n$L_\\mu-L_\\tau$. These interactions, sourced by electrons and neutrons in the\nEarth, Moon, Sun, Milky Way, and the local Universe, could modify neutrino\noscillation probabilities. The upcoming long-baseline experiments, DUNE and\nT2HK, with their large statistics, reduced systematic uncertainties, and\nwell-characterized neutrino beams, will probe these interactions. We forecast\nthat, while individually DUNE and T2HK could constrain these long-range\nneutrino interactions, their combination lifts parameter degeneracies that\nweaken individual sensitivity and provides stronger constraints.",
        "Our primary goal here is to create a good, generalist perception model that\ncan tackle multiple tasks, within limits on computational resources and\ntraining data. To achieve this, we resort to text-to-image diffusion models\npre-trained on billions of images. Our exhaustive evaluation metrics\ndemonstrate that DICEPTION effectively tackles multiple perception tasks,\nachieving performance on par with state-of-the-art models. We achieve results\non par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B\npixel-level annotated images). Inspired by Wang et al., DICEPTION formulates\nthe outputs of various perception tasks using color encoding; and we show that\nthe strategy of assigning random colors to different instances is highly\neffective in both entity segmentation and semantic segmentation. Unifying\nvarious perception tasks as conditional image generation enables us to fully\nleverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently\ntrained at a cost of orders of magnitude lower, compared to conventional models\nthat were trained from scratch. When adapting our model to other tasks, it only\nrequires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION\nprovides valuable insights and a more promising solution for visual generalist\nmodels. Homepage: https:\/\/aim-uofa.github.io\/Diception, Huggingface Demo:\nhttps:\/\/huggingface.co\/spaces\/Canyu\/Diception-Demo."
      ]
    }
  },
  {
    "id":2411.01144,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A Simple Framework for Contrastive Learning of Visual Representations",
    "start_abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed self-supervised algorithms without requiring specialized architectures or memory bank. In order to understand what enables the prediction tasks learn useful representations, we systematically study major components our framework. show that (1) composition data augmentations plays critical role in defining effective predictive tasks, (2) introducing learnable nonlinear transformation between representation and loss substantially improves quality learned (3) benefits from larger batch sizes more training steps compared supervised learning. By combining these findings, are able considerably outperform previous methods semi-supervised on ImageNet. A linear classifier trained representations by SimCLR achieves 76.5% top-1 accuracy, which is 7% relative improvement over state-of-the-art, matching performance ResNet-50. When fine-tuned only 1% labels, achieve 85.8% top-5 outperforming AlexNet with 100X fewer labels.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images."
      ],
      "abstract":[
        "Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods."
      ],
      "categories":[
        "Lung Ultrasound"
      ]
    },
    "list":{
      "title":[
        "Impilict Runge-Kutta based sparse identification of governing equations\n  in biologically motivated systems",
        "Uncertainty Quantification with the Empirical Neural Tangent Kernel",
        "Electrically induced bulk and edge excitations in the fractional quantum\n  Hall regime",
        "Probing the physical environment of the most high-redshift H$_2$-DLAs\n  through numerical models",
        "Optimization of x-ray event screening using ground and in-orbit data for\n  the Resolve instrument onboard the XRISM satellite",
        "Fermion mediated pairing in the Ruderman-Kittel-Kasuya-Yosida to Efimov\n  transition regime",
        "A Novel Quantity for Probing Matter Perturbations Below the Fresnel\n  Scale in Gravitational Lensing of Gravitational Waves",
        "A Mechanistic Framework for Collider Detection in Observational Data",
        "Invitation to the subpath number",
        "Turnstile area as a measure for chaotic transport in magnetic\n  confinement fusion devices",
        "Algebra of Invariants for the Vlasov-Maxwell System",
        "Machine Unlearning via Information Theoretic Regularization",
        "A New Approach for Fourier Extension Based on Weighted Generalized\n  Inverse",
        "Two-dimensional multiferroic NbPc COF with strong magnetoelectric\n  coupling and room-temperature ferroelectricity",
        "Stochastic Gross-Pitaevskii theory for a spin-1 Bose gas: Application to\n  superfluidity in two dimensions",
        "Determinism and Asymmetry in General Relativity",
        "Stable Soliton Microcomb Generation in X-cut Lithium Tantalate via\n  Thermal-Assisted Photorefractive Suppression",
        "Magnetic-Field Dependence of Paramagnetic Properties Investigated by\n  63\/65Cu-NMR on the Yb Zigzag-Chain Semiconductor YbCuS2",
        "Rational SU(3)-equivariant cohomology theories",
        "Looking at bulk points in general geometries",
        "Construction Techniques for Linear Realizations of Multisets with Small\n  Support",
        "Approaching the Inverse Problem: Toward Lattice QCD Calculations of\n  Inclusive Hadronic Quantities",
        "Periodic phase slips and frequency comb generation at tunable microwave\n  frequencies in superconducting diabolo structures",
        "On Data-Driven Robust Optimization With Multiple Uncertainty Subsets:\n  Unified Uncertainty Set Representation and Mitigating Conservatism",
        "The spectral Einstein functional for the nonminimal de Rham-Hodge\n  operator",
        "Examining the Potential for Methyl Halide Accumulation and Detectability\n  in Possible Hycean-Type Atmospheres",
        "Collective mode spectroscopy in time-reversal symmetry breaking\n  superconductors",
        "All-dry pick-up and transfer method for quantum emitter arrays in\n  hexagonal boron nitride",
        "Optical Skyrmions of Vortex Darkness"
      ],
      "abstract":[
        "Identifying governing equations in physical and biological systems from\ndatasets remains a long-standing challenge across various scientific\ndisciplines, providing mechanistic insights into complex system evolution.\nCommon methods like sparse identification of nonlinear dynamics (SINDy) often\nrely on precise derivative estimations, making them vulnerable to data scarcity\nand noise. This study presents a novel data-driven framework by integrating\nhigh order implicit Runge-Kutta methods (IRKs) with the sparse identification,\ntermed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity\nand noise by leveraging the lower stepsize constraint of IRKs. Two methods for\nincorporating IRKs into sparse regression are introduced: one employs iterative\nschemes for numerically solving nonlinear algebraic system of equations, while\nthe other utilizes deep neural networks to predict stage values of IRKs. The\nperformance of IRK-SINDy is demonstrated through numerical experiments on\nbenchmark problems with varied dynamical behaviors, including linear and\nnonlinear oscillators, the Lorenz system, and biologically relevant models like\npredator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results\nindicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy\nframework, particularly under conditions of extreme data scarcity and noise,\nyielding interpretable and generalizable models.",
        "While neural networks have demonstrated impressive performance across various\ntasks, accurately quantifying uncertainty in their predictions is essential to\nensure their trustworthiness and enable widespread adoption in critical\nsystems. Several Bayesian uncertainty quantification (UQ) methods exist that\nare either cheap or reliable, but not both. We propose a post-hoc,\nsampling-based UQ method for over-parameterized networks at the end of\ntraining. Our approach constructs efficient and meaningful deep ensembles by\nemploying a (stochastic) gradient-descent sampling process on appropriately\nlinearized networks. We demonstrate that our method effectively approximates\nthe posterior of a Gaussian process using the empirical Neural Tangent Kernel.\nThrough a series of numerical experiments, we show that our method not only\noutperforms competing approaches in computational efficiency (often reducing\ncosts by multiple factors) but also maintains state-of-the-art performance\nacross a variety of UQ metrics for both regression and classification tasks.",
        "We apply a voltage pulse to electrically excite the incompressible region of\na two-dimensional electron liquid in the $\\nu=2\/3$ fractional quantum Hall\nstate and investigate the collective excitations in both the edge and bulk via\nphotoluminescence spectral energy shifts. Introducing an offset in the voltage\npulse significantly enhances the excitation signal. Real-space and\ntime-resolved measurements reveal the dynamics of the bulk excitations, with an\nestimated group velocity of approximately $3 \\times 10^4$ m\/s. These bulk\nexcitations align well with the magneto-plasmon model. Our results highlight\nthe topological link between edge and bulk states, providing a novel approach\nto exploring solid-state analogs of quantum gravity.",
        "Damped Lyman-$\\alpha$ absorbers (DLAs) with molecular hydrogen have been\nprobed in detail through both spectroscopic observations and numerical\nmodelling. However, such H$_2$ absorbers are quite sparse at very high\nredshifts. We identify six of the most distant known H$_2$-DLAs (redshift\nbetween 3 and 4.5), with medium\/high-resolution spectroscopic observations\nreported in the literature, and perform detailed numerical modelling followed\nby Bayesian analysis to constrain their physical properties mainly using the\nH$_2$ rotational level population and CI fine structure levels. Our modelling\napproach involves setting up a constant-pressure multiphase cloud irradiated\nfrom both sides, in comparison to most models which employ constant density.\nThis enables us to use all observed atomic and molecular species as constraints\nto build a more realistic model of the DLA. Our results indicate high\ninterstellar radiation field strength $\\sim$ 10$^2$ to 10$^3$ G$_0$ for some\nsightlines, which is suggestive of in situ star formation. The cosmic ray\nionization rate for all DLAs is constrained between 10$^{-17}$ and 10$^{-14}$\ns$^{-1}$, consistent with recent estimates for high-redshift sightlines. Total\nhydrogen density and temperature lie in the ranges 50 to 4 $\\times$ 10$^4$\ncm$^{-3}$ and 35-200 K in the innermost part of the absorbers. The\ncorresponding gas pressure in our DLA models lies between 10$^{3.5}$ and\n10$^{6.4}$ cm$^{-3}$ K, with three sightlines having a higher pressure than the\nrange typical of high-redshift H$_2$-DLAs.",
        "The XRISM (X-Ray Imaging and Spectroscopy Mission) satellite was successfully\nlaunched and put into a low-Earth orbit on September 6, 2023 (UT). The Resolve\ninstrument onboard XRISM hosts an x-ray microcalorimeter detector, which was\ndesigned to achieve a high-resolution ($\\leq$7 eV FWHM at 6 keV),\nhigh-throughput, and non-dispersive spectroscopy over a wide energy range. It\nalso excels in a low background with a requirement of $< 2 \\times 10^{-3}$\ns$^{-1}$ keV$^{-1}$ (0.3--12.0 keV), which is equivalent to only one background\nevent per spectral bin per 100 ks exposure. Event screening to discriminate\nx-ray events from background is a key to meeting the requirement. We present\nthe result of the Resolve event screening using data sets recorded on the\nground and in orbit based on the heritage of the preceding x-ray\nmicrocalorimeter missions, in particular, the Soft X-ray Spectrometer (SXS)\nonboard ASTRO-H. We optimize and evaluate 19 screening items of three types\nbased on (1) the event pulse shape, (2) relative arrival times among multiple\nevents, and (3) good time intervals. We show that the initial screening, which\nis applied for science data products in the performance verification phase,\nreduces the background rate to $1.8 \\times 10^{-3}$ s$^{-1}$ keV$^{-1}$ meeting\nthe requirement. We further evaluate the additional screening utilizing the\ncorrelation among some pulse shape properties of x-ray events and show that it\nfurther reduces the background rate particularly in the $<$2 keV band. Over\n0.3--12 keV, the background rate becomes $1.0 \\times 10^{-3}$ s$^{-1}$\nkeV$^{-1}$.",
        "The Ruderman-Kittel-Kasuya-Yoshida (RKKY) interaction and Efimov physics are\ntwo distinct quantum phenomena in condensed matter and nuclear physics,\nrespectively. The RKKY interaction describes correlations between impurities\nmediated by an electron gas, while Efimov physics describes universal bound\nstates of three particles with resonant interactions. Recently, both effects\nhave been observed in Bose-Fermi mixtures in the weak and resonant interaction\nregimes, respectively. Intriguing conjectures exist to elucidate how the two\nphenomena meet in the transition regime where the mixture is strongly\ninteracting. In this work, we explore the RKKY-Efimov transition in a mixture\nof bosonic Cs-133 and fermionic Li-6 near a tunable interspecies Feshbach\nresonance. From dispersion and relaxation measurements, we find that the\ntransition is highlighted by a fermion-mediated scattering resonance between Cs\natoms and a weaker resonance on Li atoms. These resonances represent reactive\nscattering of Cs and Li atoms in the many-body regime, which reduces to an\nEfimov resonance in the thermal gas regime. Our observation demonstrates the\nintriguing interplay of two-, three-, and many-body physics in an Bose-Fermi\nmixture that connects condensed matter physics, nuclear physics and quantum\nmany-body chemistry.",
        "Gravitational lensing of gravitational waves provides a powerful probe of the\nmass density distribution in the universe. Wave optics effects, such as\ndiffraction, make the lensing effect sensitive to the structure around the\nFresnel scale, which depends on the gravitational wave frequency and is\ntypically sub-Galactic for realistic observations. Contrary to this common\nlore, we show that wave optics can, in principle, probe matter perturbations\neven below the Fresnel scale. This is achieved by introducing a new quantity\nderived from the amplification factor, which characterizes the lensing effect,\nand analyzing its correlation function. Our results demonstrate that this\nquantity defines an effective Fresnel scale: a characteristic scale that can be\narbitrarily small, even when observational frequencies are bounded. In\npractice, the effective Fresnel scale is constrained by the observation time\n$T$ and is suppressed by a factor of $1\/\\sqrt{fT}$ relative to the standard\nFresnel scale at frequency $f$. Nevertheless, it remains significantly smaller\nthan the conventional Fresnel scale for $fT \\gg 1$; for instance, in one-year\nobservations of mHz GWs, the effective Fresnel scale can be as small as 1 pc.\nThis approach opens new avenues for probing the fine-scale structure of the\nuniverse and the nature of dark matter.",
        "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology.",
        "In this paper we count all the subpaths of a given graph G; including the\nsubpaths of length zero, and we call this quantity the subpath number of G. The\nsubpath number is related to the extensively studied number of subtrees, as it\ncan be considered as counting subtrees with the additional requirement of\nmaximum degree being two. We first give the explicit formula for the subpath\nnumber of trees and unicyclic graphs. We show that among connected graphs on\nthe same number of vertices, the minimum of the subpath number is attained for\nany tree and the maximum for the complete graph. Further, we show that the\ncomplete bipartite graph with partite sets of almost equal size maximizes the\nsubpath number among all bipartite graphs. The explicit formula for cycle\nchains, i.e. graphs in which two consecutive cycles share a single edge, is\nalso given. This family of graphs includes the unbranched catacondensed\nbenzenoids which implies a possible application of the result in chemistry. The\npaper is concluded with several directions for possible further research where\nseveral conjectures are provided.",
        "We analyze stochasticity in the magnetic fields of magnetic confinement\nfusion reactors by calculating the lobe areas of turnstiles - a method\ndeveloped for characterizing transport into and out of resonance zones in\nHamiltonian dynamical systems. We develop an efficient algorithm based on an\naction principle to calculate this quantity directly from the magnetic field,\nincluding stellarator magnetic fields which are sourced by a complicated set of\nthree-dimensional coils. In the analyzed devices, the turnstile area on the\ninboard (plasma-facing) manifolds is much smaller than the turnstile area on\nthe outboard (wall-facing) manifolds. The application of the turnstile area\ncalculation for the design of future reactors will be discussed.",
        "The algebra of invariants for both the relativistic and nonrelativistic\nmultispecies Vlasov-Maxwell system is examined, including the case with a fixed\nion background. Invariants and their associated fluxes are obtained directly\nfrom the Vlasov-Maxwell system. The invariants are shown to Poisson commute\nwith the Hamiltonian and the rest of the Poisson bracket algebra of invariants\nis identified. Special attention is given to the role played by the monopole\ncondition, $\\nabla\\cdot \\mathbf{B}$.",
        "How can we effectively remove or \"unlearn\" undesirable information, such as\nspecific features or individual data points, from a learning outcome while\nminimizing utility loss and ensuring rigorous guarantees? We introduce a\nmathematical framework based on information-theoretic regularization to address\nboth feature and data point unlearning. For feature unlearning, we derive a\nunified solution that simultaneously optimizes diverse learning objectives,\nincluding entropy, conditional entropy, KL-divergence, and the energy of\nconditional probability. For data point unlearning, we first propose a novel\ndefinition that serves as a practical condition for unlearning via retraining,\nis easy to verify, and aligns with the principles of differential privacy from\nan inference perspective. Then, we provide provable guarantees for our\nframework on data point unlearning. By combining flexibility in learning\nobjectives with simplicity in regularization design, our approach is highly\nadaptable and practical for a wide range of machine learning and AI\napplications.",
        "This paper examines the Fourier extension from a new perspective of solving\nthe compact operator equation with perturbed data. By converting the\napproximation target from the best approximate solution to the weighted best\napproximate solution, the oscillation in the extended region has been overcome.\nThe error estimation of the solution is theoretically established. Furthermore,\nwe point out the difficulties faced by the original weighted operator in\ncalculation due to the limitation of machine precision and propose an effective\ncorrection operator. The relevant parameters involved in the method are further\ntested, and finally the effectiveness of the method is verified through\nnumerical experiments.",
        "The realization of two-dimensional multiferroics offers significant potential\nfor nanoscale device functionality. However, type-I two-dimensional\nmultiferroics with strong magnetoelectric coupling, enabling electric field\ncontrol of spin, remain scarce. In this study, using density functional theory\nand Monte Carlo simulations, we predict that the niobium phthalocyanine\ncovalent organic framework (NbPc COF) monolayer exhibits type-I multiferroic\nbehavior, with a ferroelectric transition occurring above room temperature.\nRemarkably, the strong magnetoelectric coupling in NbPc COF monolayer arises\nfrom the same origin of magnetism and ferroelectricity. Our findings offer\nflexible pathways for the design and development of organic nanoscale\nmultiferroic devices with broad applications.",
        "This paper develops and implements the stochastic projected Gross-Pitaevskii\nequation for spin-1 Bose gases, addressing key considerations for numerical\nsimulations. As an application of the theory we explore equilibrium phases in a\ntwo-dimensional spin-1 gas, where quasi-long-range order emerges via a\nBerezinskii-Kosterlitz-Thouless transition. Our analysis includes definition of\nsuperfluid densities for both mass and spin degrees of freedom, in a manner\nsuitable for implementation within a stochastic projected Gross-Pitaevskii\nequation simulation. We present a finite-temperature phase diagram for the\nferromagnetic spin-1 Bose gas and identify three distinct superfluid phases:\ntwo exhibiting conventional Berezinskii-Kosterlitz-Thouless-like behavior and a\nnovel phase that simultaneously supports independent mass and spin superflows.\nAs temperature increases, the stability region of this novel phase shrinks.\nThis work provides a foundation for further studies of nonequilibrium and\nfinite-temperature phenomena in spinor Bose gases.",
        "This paper concerns the question of which collections of general relativistic\nspacetimes are deterministic relative to which definitions. We begin by\nconsidering a series of three definitions of increasing strength due to Belot\n(1995). The strongest of these definitions is particularly interesting for\nspacetime theories because it involves an asymmetry condition called\n``rigidity'' that has been studied previously in a different context (Geroch\n1969; Halvorson and Manchak 2022; Dewar 2024). We go on to explore other\n(stronger) asymmetry conditions that give rise to other (stronger) forms of\ndeterminism. We introduce a number of definitions of this type and clarify the\nrelationships between them and the three considered by Belot. We go on to show\nthat there are collections of general relativistic spacetimes that satisfy much\nstronger forms of determinism than previously known. We also highlight a number\nof open questions.",
        "Chip-based soliton frequency microcombs combine compact size, broad\nbandwidth, and high coherence, presenting a promising solution for integrated\noptical telecommunications, precision sensing, and spectroscopy. Recent\nprogress in ferroelectric thin films, particularly thin-film Lithium niobate\n(LN) and thin-film Lithium tantalate (LT), has significantly advanced\nelectro-optic (EO) modulation and soliton microcombs generation, leveraging\ntheir strong third-order nonlinearity and high Pockels coefficients. However,\nachieving soliton frequency combs in X-cut ferroelectric materials remains\nchallenging due to the competing effects of thermo-optic and photorefractive\nphenomena. These issues hinder the simultaneous realization of soliton\ngeneration and high-speed EO modulation. Here, following the thermal-regulated\ncarrier behaviour and auxiliary-laser-assisted approach, we propose a\nconvenient mechanism to suppress both photorefractive and thermal dragging\neffect at once, and implement a facile method for soliton formation and its\nlong-term stabilization in integrated X-cut LT microresonators for the first\ntime. The resulting mode-locked states exhibit robust stability against\nperturbations, enabling new pathways for fully integrated photonic circuits\nthat combine Kerr nonlinearity with high-speed EO functionality.",
        "To investigate the paramagnetic properties of YbCuS2 under magnetic fields,\nwe have performed the 63\/65Cu-nuclear magnetic resonance (NMR) measurements.\nThe NMR spectra can be reproduced by the simulations of the three-dimensional\npowder pattern and the additional two-dimensional powder pattern, indicating\nthe partial sample orientation due to the anisotropy of the magnetic\nproperties. These simulations suggest that the ac plane is the easy plane in\nYbCuS2. The Knight shift K is proportional to the bulk magnetic susceptibility\nand field-independent. The broad maximum of the nuclear spin-lattice relaxation\nrate 1\/T1 at Tmax ~ 50 K (50 K anomaly) observed at zero magnetic field is\nquickly suppressed by the magnetic fields. This indicates that the 50 K anomaly\nis field-dependent. Furthermore, an anomalous enhancement of 1\/T1 at low\ntemperatures was observed above 3 T. This field seemingly corresponds to the\nmagnetic field at which a field-induced phase transition occurs below the\nantiferromagnetic transition temperature TN ~ 1 K. The changes in 1\/T1 observed\nin the paramagnetic state suggest the presence of the complex quantum phenomena\nunder magnetic fields in YbCuS2.",
        "We describe the spectral space of conjugacy classes of subgroups of SU(3),\ntogether with the additional structure of a sheaf of rings and a component\nstructure. It is a disjoint union of 18 blocks each dominated by a subgroup.\nFor each of these blocks we identify a sheaf of rings and component structure.\nTaken together, this gives an abelian category A(SU(3)) designed to reflect the\nstructure of rational SU(3)-equivariant cohomology theories, and we assemble\nthe results from elsewhere to show that the category of rational SU(3)-spectra\nis Quillen equivalent to the category of differential graded objects of\nA(SU(3)).",
        "The holographic correspondence predicts that certain strongly coupled quantum\nsystems describe an emergent, higher-dimensional bulk spacetime in which\nexcitations enjoy local dynamics. We consider a general holographic state dual\nto an asymptotically AdS bulk spacetime, and study boundary correlation\nfunctions of local fields integrated against wavepackets. We derive a\nfactorization formula showing that when the wavepackets suitably meet at a\ncommon bulk point, the boundary correlators develop sharp features controlled\nby flat-space-like bulk scattering processes. These features extend along\nboundary hyperboloids whose shape naturally reveals the bulk geometry. We\ndiscuss different choices of operator ordering, which lead to inclusive and\nout-of-time-ordered amplitudes, as well as fields of various spins and masses.",
        "A Hamiltonian path in the complete graph $K_v$ whose vertices are labeled\nwith the integers $0,1,\\ldots,v-1$ is a linear realization for the multiset $L$\nof the linear edge-lengths (given by $|x-y|$ for the edge between vertices $x$\nand $y$) of the edges in the path. A linear realization is standard if an\nend-vertex is 0 and perfect if the end-vertices are 0 and $v-1$.\n  Linear realizations are useful in the study of the Buratti-Horak-Rosa (BHR)\nConjecture on the existence of cyclic realizations (where cyclic edge-lengths\nare given by distance modulo $v$) for given multisets. In this paper, we focus\non multisets of the form $\\{1^a, (y-k)^b, y^c\\}$.\n  Using core perfect linear realizations for supports of size 2 (which have the\nforms $\\{x^{y-1},y^{x+1}\\}$ whenever $\\gcd(x,y)=1$), we construct standard\nlinear realizations (with $a=k-1$, $b=j(y-k)$, $c=jy$) when $k\\mid y$ or $k\n\\leq 4$. When $k=2$, these allow us to show that there is a linear realization\nwhenever $a \\geq y$. This is in line with the known results for the case of\n$k=1$. We also supplement these results for $k=1$ by constructing linear\nrealizations whenever $b+c < y$ and $a \\geq y - \\min(b,c)$, from which the\ncoprime version of the BHR Conjecture (requiring that $v$ is coprime with each\nelement of the multiset) follows for $k=1$ when $y \\leq 16$.\n  Our methods show promise for constructing linear realizations for arbitrary\n$k$, in the direction of a resolution of the BHR Conjecture for supports of\nsize 3.",
        "In this talk, I describe some recent ideas relating to the spectral\nreconstruction inverse problem, which arises frequently in lattice QCD\ncalculations of inclusive hadronic quantities, and provide some physical\ncontext for this work. Particular emphasis is given to a new method for\nrigorously bounding uncertainties using techniques from complex analysis.",
        "Superconductors are characterized by macroscopic phase coherence and have\nenabled cryogenic electronics and quantum technologies. Recent advances in 3D\nnanofabrication now offer possibilities for tuning functional properties\nrelevant for on-chip 3D integration of superconductors. However,\nnon-equilibrium phenomena in 3D nanostructures exposed to transport currents\nremain largeley unexplored. Here, we employ numerical simulations to\ninvestigate phase slips -- discrete $2\\pi$ jumps in the phase of the\nsuperconducting order parameter -- in a tubular Nb superconductor with a\ncentral constriction, which is subjected to both direct current (DC) and\nalternating current (AC) transport currents. We find that under DC drive, the\nsystem stabilizes periodic phase slips, resulting in GHz voltage oscillations.\nIntroducing an additional AC frequency modulation generates microwave frequency\ncombs which depend characteristically on the interaction between moving\nvortices and phase slips. Our findings open avenues for developing on-chip\nfrequency comb generators in 3D cryoelectronics.",
        "Constructing uncertainty sets as unions of multiple subsets has emerged as an\neffective approach for creating compact and flexible uncertainty\nrepresentations in data-driven robust optimization (RO). This paper focuses on\ntwo separate research questions. The first concerns the computational challenge\nin applying these uncertainty sets in RO-based predictive control. To address\nthis, a monolithic mixed-integer representation of the uncertainty set is\nproposed to uniformly describe the union of multiple subsets, enabling the\ncomputation of the worst-case uncertainty scenario across all subsets within a\nsingle mixed-integer linear programming (MILP) problem. The second research\nquestion focuses on mitigating the conservatism of conventional RO formulations\nby leveraging the structure of the uncertainty set. To achieve this, a novel\nobjective function is proposed to exploit the uncertainty set structure and\nintegrate the existing RO and distributionally robust optimization (DRO)\nformulations, yielding less conservative solutions than conventional RO\nformulations while avoiding the high-dimensional continuous uncertainty\ndistributions and incurring high computational burden typically associated with\nexisting DRO formulations. Given the proposed formulations, numerically\nefficient computation methods based on column-and-constraint generation (CCG)\nare also developed. Extensive simulations across three case studies are\nperformed to demonstrate the effectiveness of the proposed schemes.",
        "In this paper, we give the definitions of the non-self-adjoint spectral\ntriple and its spectral Einstein functional. We compute the spectral Einstein\nfunctional associated with the nonminimal de Rham-Hodge operator on\neven-dimensional compact manifolds without boundary. Finally, several examples\nof the non-self-adjoint spectral triple are listed.",
        "Some sub-Neptune planets may host habitable conditions; for example \"Hycean\"\nworlds with H2 envelopes over liquid water oceans can maintain potentially\nhospitable pressures and temperatures at their surface. Recent JWST\nobservations of K2-18b and TOI-270d have shown that such worlds could be\ncompelling targets for biosignature searches, given their extended scale\nheights and therefore large atmospheric signatures. Methylated biosignatures, a\nbroad group of gases that can be generated by biological attachment of a CH3\ngroup to an environmental substrate, have been proposed as candidate signs of\nlife for Earth-like exoplanets. However, methyl halides (CH3 + halogen) have\nnot yet been robustly examined with self-consistent photochemical and spectral\nmodels for planets with H2-dominated atmospheres. Here we demonstrate that\nmethyl chloride (CH3Cl), predominantly produced by marine microbes, could be\ndetected using JWST in tens of transits or fewer for Hycean planets, comparable\nto detection requirements for other potential atmospheric biosignatures. The\nthreshold atmospheric mixing ratio for detectability is $\\sim$10 ppm, which can\naccumulate with global fluxes comparable to moderately productive local\nenvironments on Earth.",
        "Time-reversal symmetry breaking (TRSB) superconductors show a rich collective\nmode spectrum. In general, collective excitations in superconductors can\nprovide crucial information on the symmetry of the broken phase, in particular,\nserving as a fingerprint for determining the groundstate gap symmetry. In this\nwork, we consider several even parity two-dimensional TRSB superconductors\ncharacterized by an order parameter of the form $\\Delta = \\Delta_1 +\ni\\Delta_2$. We provide a classification scheme of the collective excitations in\nthe above systems as a function of the ratio between the components\n$\\Delta_1\/\\Delta_2$. In order to excite the modes in the systems we have\nadopted two different probes: a quench of the condensate symmetry and a finite\nmomentum transfer induced by an external electric field. Both methods allow us\nto excite and characterize the different modes in the spectra. To further\ninterpret the results of the numerical calculations we provide a\nGinzburg-Landau analysis and we construct a dynamical theory, deriving the\nlinearized equations of motion in the pseudospin formalism. Our results could\nhelp distinguish between different order parameters symmetries of a TRSB\nsuperconducting condensate and estimate the magnitude of its different\ncomponents.",
        "Single photon emitters in hexagonal boron nitride are based on fluorescent\npoint-like defects. These defects typically have exceptional photophysical\nproperties and therefore been the focus of extensive research due to their\npotential to advance photonic quantum technologies. However, achieving scalable\nintegration of these emitters to arbitrary platforms with high yield while\nretaining their characteristics remains a significant challenge, particularly\nwhen the target substrate is not compatible with the fabrication method. In\nthis work, we introduce an all-dry transfer method aimed at addressing these\nchallenges with improved effectiveness compared to existing techniques. This\npolymer stamp-assisted transfer method maintains high output and preserves the\nfundamental characteristics of the emitters while eliminating wet chemical\nprocesses. A comprehensive post-transfer characterization verified not only the\nmaintenance of the defining characteristic of a single photon emitter, the\nsecond-order correlation function $g^{(2)}(0)$, but also showed improvement by\nabout 46%. In contrast, the lifetime, emission spectrum, and the photostability\nshowed only negligible change, demonstrating that the characteristics of the\nemitters were retained during the transfer process. This transfer technique has\nsuccess rate of 81.8%, determined by the proportion of single photon emitters\nthat retain their optical and preserve physical structure post-transfer. This\nhigh success rate shows the potential to scale the integration of single photon\nemitters across diverse platforms. We expect that this process contributes to\nthe applications of boron nitride defects in quantum technologies.",
        "We disclose the existence of a type of optical skyrmion, Gauss-Stokes (GS)\nskyrmions, that is naturally present in an optical vortex around its phase\nsingularity. Contrary to previous research with optical skyrmions, we neither\nshape vector beams nor superpose different spatial modes and polarizations. In\nGS skyrmions, the phase singularity in the transversal field of a single\nmonochromatic beam of uniform polarization (a scalar beam) is concealed by the\naxial field dictated by Gauss's divergence law, giving rise to a polarization\nsingularity of undefined polarization plane. This singularity is enclosed by a\nrich skyrmionic polarization texture fulfilling a topological map and covering\nall the states of transverse-axial polarization. In our experiment, we\nfacilitate the observation of a GS skyrmion with the predicted features using\nfocused fields with enhanced axial component."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images",
    "start_abstract":"Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Adaptive Drift Compensation for Soft Sensorized Finger Using Continual\n  Learning",
        "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
        "A Survey of Internet Censorship and its Measurement: Methodology,\n  Trends, and Challenges",
        "Unifying Perplexing Behaviors in Modified BP Attributions through\n  Alignment Perspective",
        "UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via\n  Network Architecture Search",
        "DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot\n  Trajectories",
        "Density-Functional Perturbation Theory with Numeric Atom-Centered\n  Orbitals",
        "Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and\n  Power Interference in Multi-User OSaaS Networks",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective",
        "Study of Nucleon Charge-Exchange Processes at $^{12}$C Fragmentation\n  with an Energy of 300 MeV\/Nucleon",
        "Global Picard Spectra and Borel Parametrized Algebra",
        "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
        "Microscopic investigation of wobbling motion in even-even nuclei",
        "Improvement of Data Analytics Techniques in Reflection High Energy\n  Electron Diffraction to Enable Machine Learning",
        "Scalar behavior for a complex multi-soliton arising in blow-up for a\n  semilinear wave equation",
        "Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms\n  Radiologist MRI Interpretation: A Multi-Center Study",
        "3D-grids are not transducible from planar graphs",
        "RIS-Aided Fluid Antenna Array-Mounted UAV Networks",
        "Behavioral Homophily in Social Media via Inverse Reinforcement Learning:\n  A Reddit Case Study",
        "Public Access Defibrillator Deployment for Cardiac Arrests: A\n  Learn-Then-Optimize Approach with SHAP-based Interpretable Analytics",
        "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian\n  Prototypes",
        "Potential Contribution of Young Pulsar Wind Nebulae to Galactic\n  High-Energy Neutrino Emission",
        "Estimating Parameters of Structural Models Using Neural Networks",
        "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models",
        "HoloGest: Decoupled Diffusion and Motion Priors for Generating\n  Holisticly Expressive Co-speech Gestures",
        "Orthogonal Representation Learning for Estimating Causal Quantities",
        "AI-Powered Noisy Quantum Emulation: Generalized Gate-Based Protocols for\n  Hardware-Agnostic Simulation"
      ],
      "abstract":[
        "Strain sensors are gaining popularity in soft robotics for acquiring tactile\ndata due to their flexibility and ease of integration. Tactile sensing plays a\ncritical role in soft grippers, enabling them to safely interact with\nunstructured environments and precisely detect object properties. However, a\nsignificant challenge with these systems is their high non-linearity,\ntime-varying behavior, and long-term signal drift. In this paper, we introduce\na continual learning (CL) approach to model a soft finger equipped with\npiezoelectric-based strain sensors for proprioception. To tackle the\naforementioned challenges, we propose an adaptive CL algorithm that integrates\na Long Short-Term Memory (LSTM) network with a memory buffer for rehearsal and\nincludes a regularization term to keep the model's decision boundary close to\nthe base signal while adapting to time-varying drift. We conduct nine different\nexperiments, resetting the entire setup each time to demonstrate signal drift.\nWe also benchmark our algorithm against two other methods and conduct an\nablation study to assess the impact of different components on the overall\nperformance.",
        "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
        "Internet censorship limits the access of nodes residing within a specific\nnetwork environment to the public Internet, and vice versa. During the last\ndecade, techniques for conducting Internet censorship have been developed\nfurther. Consequently, methodology for measuring Internet censorship had been\nimproved as well. In this paper, we firstly provide a survey of Internet\ncensorship techniques. Secondly, we survey censorship measurement methodology,\nincluding a coverage of available datasets. In cases where it is beneficial, we\nbridge the terminology and taxonomy of Internet censorship with related\ndomains, namely traffic obfuscation and information hiding. We cover both,\ntechnical and human aspects, as well as recent trends, and challenges.",
        "Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.",
        "Lightweight models are essential for real-time speech enhancement\napplications. In recent years, there has been a growing trend toward developing\nincreasingly compact models for speech enhancement. In this paper, we propose\nan Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS),\nwhich is suitable for implementation in low-footprint devices. Firstly, we\nexplore the application of various efficient convolutional blocks within the\nU-Net framework to identify the most promising candidates. Secondly, we\nintroduce two boosting components to enhance the capacity of these\nconvolutional blocks: a novel activation function named affine PReLU and a\ncausal time-frequency attention module. Furthermore, we leverage neural\narchitecture search to discover an optimal architecture within our carefully\ndesigned search space. By integrating the above strategies, UL-UNAS not only\nsignificantly outperforms the latest ultra-lightweight models with the same or\nlower computational complexity, but also delivers competitive performance\ncompared to recent baseline models that require substantially higher\ncomputational resources.",
        "Diffusion models excel at creating images and videos thanks to their\nmultimodal generative capabilities. These same capabilities have made diffusion\nmodels increasingly popular in robotics research, where they are used for\ngenerating robot motion. However, the stochastic nature of diffusion models is\nfundamentally at odds with the precise dynamical equations describing the\nfeasible motion of robots. Hence, generating dynamically admissible robot\ntrajectories is a challenge for diffusion models. To alleviate this issue, we\nintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to\ngenerate provably admissible trajectories of black-box robotic systems using\ndiffusion models. A sequence of states is a dynamically admissible trajectory\nif each state of the sequence belongs to the reachable set of its predecessor\nby the robot's equations of motion. To generate such trajectories, our\ndiffusion policies project their predictions onto a dynamically admissible\nmanifold during both training and inference to align the objective of the\ndenoiser neural network with the dynamical admissibility constraint. The\nauto-regressive nature of these projections along with the black-box nature of\nrobot dynamics render these projections immensely challenging. We thus enforce\nadmissibility by iteratively sampling a polytopic under-approximation of the\nreachable set of a state onto which we project its predicted successor, before\niterating this process with the projected successor. By producing accurate\ntrajectories, this projection eliminates the need for diffusion models to\ncontinually replan, enabling one-shot long-horizon trajectory planning. We\ndemonstrate that our framework generates higher quality dynamically admissible\nrobot trajectories through extensive simulations on a quadcopter and various\nMuJoCo environments, along with real-world experiments on a Unitree GO1 and\nGO2.",
        "This paper represents one contribution to a larger Roadmap article reviewing\nthe current status of the FHI-aims code. In this contribution, the\nimplementation of density-functional perturbation theory in a numerical\natom-centered framework is summarized. Guidelines on usage and links to\ntutorials are provided.",
        "We present a framework to identify and mitigate rogue OOK signals and\nuser-generated power interference in a multi-user Optical-Spectrum-as-a-Service\nnetwork. Experimental tests on the OpenIreland-testbed achieve up to 89%\ndetection rate within 10 seconds of an interference event.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks.",
        "The search of reactions with nucleon charge-exchange was performed on the\nFRAGM fragment-separator of the TWAC accelerator complex at fragmentation of\ncarbon nuclei with an energy of 300 MeV\/nucleon on a thin beryllium target. The\nexperimental setup, located at an angle of 3.5 degrees to the incident beam,\nhad high momentum resolution. Differential cross sections were measured for\n$^{11}$Be, $^{12}$B and $^{12}$Be as function of the nuclei momentum. The\nexperimental data were compared with theoretical predictions of various models\nof nucleus-nucleus interactions and other experimental results. Measurements of\nnucleon charge exchange processes in this energy region was carried out for the\nfirst time. New results were obtained to test theoretical models of\nnucleus-nucleus interactions.",
        "We answer a question of Schwede on the existence of global Picard spectra\nassociated to his ultra-commutative global ring spectra; given an\nultra-commutative global ring spectrum $R$, we show there exists a global\nspectrum $\\mathrm{pic}_\\mathrm{eq}(R)$ assembling the Picard spectra of all\nunderlying $G$-equivariant ring spectra $\\mathrm{res}_G R$ of $R$ into one\nobject, in that for all finite groups $G$, the genuine fixed points are given\nby $\\mathrm{pic}_\\mathrm{eq}(R)^G \\simeq\n\\mathrm{pic}(\\mathrm{Mod}_{\\mathrm{res}_G R}(\\mathrm{Sp}_G))$.\n  Along the way, we develop a generalization of Borel-equivariant objects in\nthe setting of parametrized higher algebra. We use this to assemble the\nsymmetric monoidal categories of $G$-spectra for all finite groups $G$ together\nwith all restrictions and norms into a single `normed global category', and\nbuild a comparison functor which allows us to import ultra-commutative\n$G$-equivariant or global ring spectra into the setting of parametrized higher\nalgebra.",
        "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
        "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode.",
        "Perovskite oxides such as LaFeO$_3$ are a well-studied family of materials\nthat possess a wide range of useful and novel properties. Successfully\nsynthesizing perovskite oxide samples usually requires a significant number of\ngrowth attempts and a detailed film characterization on each sample to find the\noptimal growth window of a material. The most common real-time \\textit{in situ}\ndiagnostic technique available during molecular beam epitaxy (MBE) synthesis is\nreflection high-energy electron diffraction (RHEED). Conventional use of RHEED\nallows a highly experienced operator to determine growth rate by monitoring\nintensity osciallations and make some qualitative observations during growth,\nsuch as recognizing the sample has become amorphous or recognizing that large\nislands have formed on the surface. However, due to a lack of theoretical\nunderstanding of the diffraction patterns, finer, more precise levels of\nobservations are challenging. To address these limitations, we implement new\ndata analytics techniques in the growth of three LaFeO$_3$ samples on Nb-doped\nSrTiO$_3$ by MBE. These techniques improve our ability to perform unsupervised\nmachine learning using principal component analysis (PCA) and k-means\nclustering by using drift correction to overcome sample or stage motion during\ngrowth and intensity transformations that highlight more subtle features in the\nimages such as Kikuchi bands. With this approach, we enable the first\ndemonstration of PCA and k-means across multiple samples, allowing for\nquantitative comparison of RHEED videos for two LaFeO$_3$ film samples. These\ncapabilities set the stage for real-time processing of RHEED data during growth\nto enable machine learning-accelerated film synthesis.",
        "This paper deals with blow-up for the complex-valued semilinear wave equation\nwith power nonlinearity in dimension 1. Up to a rotation of the solution in the\ncomplex plane, we show that near a characteristic blow-up point, the solution\nbehaves exactly as in the real-valued case. Namely, up to a rotation in the\ncomplex plane, the solution decomposes into a sum of a finite number of\ndecoupled solitons with alternate signs. The main novelty of our proof is a\nresolution of a complex-valued first order Toda system governing the evolution\nof the positions and the phases of the solitons.",
        "Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target\nsuspicious prostate lesions. This has led to artificial intelligence (AI)\napplications improving MRI-based detection of clinically significant prostate\ncancer (CsPCa). However, MRI-detected lesions must still be mapped to\ntransrectal ultrasound (TRUS) images during biopsy, which results in missing\nCsPCa. This study systematically evaluates a multimodal AI framework\nintegrating MRI and TRUS image sequences to enhance CsPCa identification. The\nstudy included 3110 patients from three cohorts across two institutions who\nunderwent prostate biopsy. The proposed framework, based on the 3D UNet\narchitecture, was evaluated on 1700 test cases, comparing performance to\nunimodal AI models that use either MRI or TRUS alone. Additionally, the\nproposed model was compared to radiologists in a cohort of 110 patients. The\nmultimodal AI approach achieved superior sensitivity (80%) and Lesion Dice\n(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared\nto radiologists, the multimodal model showed higher specificity (88% vs. 78%)\nand Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings\ndemonstrate the potential of multimodal AI to improve CsPCa lesion targeting\nduring biopsy and treatment planning, surpassing current unimodal models and\nradiologists; ultimately improving outcomes for prostate cancer patients.",
        "We prove that the class of 3D-grids is cannot be transduced from planar\ngraphs, and more generally, from any class of graphs of bounded Euler genus. To\nprove our result, we introduce a new structural tool called slice\ndecompositions, and show that every graph class transducible from a class of\ngraphs of bounded Euler genus is a perturbation of a graph class that admits\nslice decompositions.",
        "This paper investigates reconfigurable intelligent surface (RIS)-assisted\nunmanned aerial vehicle (UAV) downlink networks with fluid antennas (FA), where\nRIS enables non-line-of-sight (NLoS) transmissions. Moreover, the FA is\nequipped on the UAV offering dynamic antenna position adjustment, enhancing\nspatial diversity besides UAV deployment. We aim at total downlink rate\nmaximization while ensuring minimum user rate requirement. We consider joint\noptimization of active UAV beamforming, passive RIS beamforming, UAV deployment\nand FA position adjustment. To address the complex problem, we propose\nbeamfomring for RIS\/UAV and FA-UAV deployment (BRAUD) scheme by employing\nalternative optimization, successive convex approximation (SCA) and sequential\nrank-one constraint relaxation (SROCR) method for the decomposed subproblems.\nSimulation results demonstrate the effectiveness of RIS-FA-UAV, achieving the\nhighest rate among existing architectures without FA\/UAV\/RIS deployment and\nwithout proper beamforming. Moreover, BRAUD achieves the highest rate among\nbenchmarks of drop-rank method, heuristic optimizations and conventional\nzero-forcing beamforming as well as random method.",
        "Online communities play a critical role in shaping societal discourse and\ninfluencing collective behavior in the real world. The tendency for people to\nconnect with others who share similar characteristics and views, known as\nhomophily, plays a key role in the formation of echo chambers which further\namplify polarization and division. Existing works examining homophily in online\ncommunities traditionally infer it using content- or adjacency-based\napproaches, such as constructing explicit interaction networks or performing\ntopic analysis. These methods fall short for platforms where interaction\nnetworks cannot be easily constructed and fail to capture the complex nature of\nuser interactions across the platform. This work introduces a novel approach\nfor quantifying user homophily. We first use an Inverse Reinforcement Learning\n(IRL) framework to infer users' policies, then use these policies as a measure\nof behavioral homophily. We apply our method to Reddit, conducting a case study\nacross 5.9 million interactions over six years, demonstrating how this approach\nuncovers distinct behavioral patterns and user roles that vary across different\ncommunities. We further validate our behavioral homophily measure against\ntraditional content-based homophily, offering a powerful method for analyzing\nsocial media dynamics and their broader societal implications. We find, among\nothers, that users can behave very similarly (high behavioral homophily) when\ndiscussing entirely different topics like soccer vs e-sports (low topical\nhomophily), and that there is an entire class of users on Reddit whose purpose\nseems to be to disagree with others.",
        "Out-of-hospital cardiac arrest (OHCA) survival rates remain extremely low due\nto challenges in the timely accessibility of medical devices. Therefore,\neffective deployment of automated external defibrillators (AED) can\nsignificantly increase survival rates. Precise and interpretable predictions of\nOHCA occurrences provide a solid foundation for efficient and robust AED\ndeployment optimization. This study develops a novel learn-then-optimize\napproach, integrating three key components: a machine learning prediction\nmodel, SHAP-based interpretable analytics, and a SHAP-guided integer\nprogramming (SIP) model. The machine learning model is trained utilizing only\ngeographic data as inputs to overcome data availability obstacles, and its\nstrong predictive performance validates the feasibility of interpretation.\nFurthermore, the SHAP model elaborates on the contribution of each geographic\nfeature to the OHCA occurrences. Finally, an integer programming model is\nformulated for optimizing AED deployment, incorporating SHAP-weighted OHCA\ndensities. Various numerical experiments are conducted across different\nsettings. Based on comparative and sensitive analysis, the optimization effect\nof our approach is verified and valuable insights are derived to provide\nsubstantial support for theoretical extension and practical implementation.",
        "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
        "Pulsar wind nebulae (PWNe), especially the young ones, are among the most\nenergetic astrophysical sources in the Galaxy. It is usually believed that the\nspin-down energy injected from the pulsars is converted into magnetic field and\nrelativistic electrons, but the possible presence of proton acceleration inside\nPWNe cannot be ruled out. Previous works have estimated the neutrino emission\nfrom PWNe using various source catalogs measured in gamma-rays. However, such\nresults rely on the sensitivity of TeV gamma-ray observations and may omit the\ncontribution by unresolved sources. Here we estimate the potential neutrino\nemission from a synthetic population of PWNe in the Galaxy with a focus on the\nones that are still in the free expansion phase. In the calculation, we model\nthe temporal evolution of the free-expanding PWNe and consider the transport of\nprotons inside the PWNe. The Crab nebula is treated as a standard template for\nyoung PWNe to evaluate some model parameters, such as the energy conversion\nfraction of relativistic protons and the target gas density for the hadronic\nprocess, which are relevant to neutrino production. In the optimistic case, the\nneutrino flux from the simulated young PWNe may constitute to 5% of the\nmeasured flux by IceCube around 100 TeV. At higher energy around 1 PeV, the\nneutrino emission from the population highly depends on the injection spectral\nshape, and also on the emission of the nearby prominent sources.",
        "We study an alternative use of machine learning. We train neural nets to\nprovide the parameter estimate of a given (structural) econometric model, for\nexample, discrete choice or consumer search. Training examples consist of\ndatasets generated by the econometric model under a range of parameter values.\nThe neural net takes the moments of a dataset as input and tries to recognize\nthe parameter value underlying that dataset. Besides the point estimate, the\nneural net can also output statistical accuracy. This neural net estimator\n(NNE) tends to limited-information Bayesian posterior as the number of training\ndatasets increases. We apply NNE to a consumer search model. It gives more\naccurate estimates at lighter computational costs than the prevailing approach.\nNNE is also robust to redundant moment inputs. In general, NNE offers the most\nbenefits in applications where other estimation approaches require very heavy\nsimulation costs. We provide code at: https:\/\/nnehome.github.io.",
        "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs.",
        "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps:\/\/cyk990422.github.io\/HoloGest.github.io\/.",
        "Representation learning is widely used for estimating causal quantities\n(e.g., the conditional average treatment effect) from observational data. While\nexisting representation learning methods have the benefit of allowing for\nend-to-end learning, they do not have favorable theoretical properties of\nNeyman-orthogonal learners, such as double robustness and quasi-oracle\nefficiency. Also, such representation learning methods often employ additional\nconstraints, like balancing, which may even lead to inconsistent estimation. In\nthis paper, we propose a novel class of Neyman-orthogonal learners for causal\nquantities defined at the representation level, which we call OR-learners. Our\nOR-learners have several practical advantages: they allow for consistent\nestimation of causal quantities based on any learned representation, while\noffering favorable theoretical properties including double robustness and\nquasi-oracle efficiency. In multiple experiments, we show that, under certain\nregularity conditions, our OR-learners improve existing representation learning\nmethods and achieve state-of-the-art performance. To the best of our knowledge,\nour OR-learners are the first work to offer a unified framework of\nrepresentation learning methods and Neyman-orthogonal learners for causal\nquantities estimation.",
        "Quantum computer emulators model the behavior and error rates of specific\nquantum processors. Without accurate noise models in these emulators, it is\nchallenging for users to optimize and debug executable quantum programs prior\nto running them on the quantum device, as device-specific noise is not properly\naccounted for. To overcome this challenge, we introduce a general protocol to\napproximate device-specific emulators without requiring pulse-level control. By\napplying machine learning to data obtained from gate set tomography, we\nconstruct a device-specific emulator by predicting the noise model input\nparameters that best match the target device. We demonstrate the effectiveness\nof our protocol's emulator in estimating the unitary coupled cluster energy of\nthe H$_2$ molecule and compare the results with those from actual quantum\nhardware. Remarkably, our noise model captures device noise with high accuracy,\nachieving a mean absolute difference of just 0.3\\% in expectation value\nrelative to the state-vector simulation."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
      ],
      "abstract":[
        "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Spontaneous helix formation in polar smectic phase",
        "Which Sensor to Observe? Timely Tracking of a Joint Markov Source with\n  Model Predictive Control",
        "Properties of the weakly-bound (1,1)-states and rotationally excited\n  (2,0)-states in the muonic molecular $d d \\mu, d t \\mu$ and $t t \\mu$ ions",
        "The Roberge-Weiss transition for QCD in a magnetic background",
        "Constraints on optical and near-infrared variability in the localisation\n  of the long-period radio transient GLEAM-X J1627-52",
        "Pisarenko's Formula for the Thermopower",
        "Joint Antenna Selection and Beamforming Design for Active RIS-aided ISAC\n  Systems",
        "Boundary stratifications of Hurwitz spaces",
        "Algebraic and optical properties of generalized Kerr-Schild spacetimes\n  in arbitrary dimensions",
        "Cosmic distance duality after DESI 2024 data release and dark energy\n  evolution",
        "Transfer learning of many-body electronic correlation entropy from local\n  measurements",
        "Nonnegative Biquadratic Tensors",
        "Extreme vulnerability to intruder attacks destabilizes network dynamics",
        "A parabolic Hardy-H\\'enon equation with quasilinear degenerate diffusion",
        "Measuring the dynamical evolution of the United States lobbying network",
        "An Algorithmic Approach for Causal Health Equity: A Look at Race\n  Differentials in Intensive Care Unit (ICU) Outcomes",
        "An Analytical Study of the Min-Sum Approximation for Polar Codes",
        "Topological, Differential Geometry Methods and Modified Variational\n  Approach for Calculation of the Propagation Time of a Signal, Emitted by a\n  GPS-Satellite and Depending on the Full Set of 6 Kepler Parameters Parameters",
        "Rational points and rational moduli spaces",
        "On Finsler metric measure manifolds with integral weighted Ricci\n  curvature bounds",
        "On characterizing optimal learning trajectories in a class of learning\n  problems",
        "FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance",
        "Targeted Learning for Data Fairness",
        "Rational points in coarse moduli spaces and twisted representations",
        "Weak and strong local irregularity of digraphs",
        "Synergistic Effects of Natural Biosurfactant and Metal Oxides\n  Modification on PVDF Nanofiber Filters for Efficient Microplastic and Oil\n  Removal",
        "Bounding Radial Variation of positive harmonic Functions on Lipschitz\n  Domains",
        "$NJ\/\\psi$ and $N\\eta_c$ interactions from lattice QCD",
        "Neutrino quantum kinetics in three flavors"
      ],
      "abstract":[
        "In soft ferroelectric crystals, the depolarization field can be reduced by\nperiodic distortion of the polarization direction. In the polar nematic and\ntilted smectic phases, this process is energetically favorured , as it only\nrequires changes in the director orientation. We demonstrate the spontaneous\nformation of a helical structure in the proper ferroelectric tilted smectic\n(SmCTBF) phase, the phase is formed below the heliconical polar nematic (NTBF)\nphase. The helical pitch in the smectic phase is approximately 600 nm and\nremains nearly constant across the entire temperature range of the phase. Under\nweak electric fields, the helix reorients while its structure remains largely\nintact; however, in stronger fields, the helix is destroyed as the electric\npolarization aligns along the electric field.",
        "In this paper, we investigate the problem of remote estimation of a\ndiscrete-time joint Markov process using multiple sensors. Each sensor observes\na different component of the joint Markov process, and in each time slot, the\nmonitor obtains a partial state value by sending a pull request to one of the\nsensors. The monitor chooses the sequence of sensors to observe with the goal\nof minimizing the mean of age of incorrect information (MAoII) by using the\npartial state observations obtained, which have different freshness levels. For\ninstance, a monitor may be interested in tracking the location of an object by\nobtaining observations from two sensors, which observe the $x$ and $y$\ncoordinates of the object separately, in different time slots. The monitor,\nthen, needs to decide which coordinate to observe in the next time slot given\nthe history. In addition to this partial observability of the state of Markov\nprocess, there is an erasure channel with a fixed one-slot delay between each\nsensor and the monitor. First, we obtain a sufficient statistic, namely the\n\\emph{belief}, representing the joint distribution of the age of incorrect\ninformation (AoII) and the current state of the observed process by using the\nhistory of all pull requests and observations. Then, we formulate the problem\nwith a continuous state-space Markov decision problem (MDP), namely belief MDP.\nTo solve the problem, we propose two model predictive control (MPC) methods,\nnamely MPC without terminal costs (MPC-WTC) and reinforcement learning MPC\n(RL-MPC), that have different advantages in implementation.",
        "Total energies and other bound state properties of the weakly-bound\n(1,1)-states and rotationally excited (2,0)-states in the three-body muonic\nmolecular $d d \\mu, d t \\mu$ and $t t \\mu$ ions are determined to high\nnumerical accuracy and investigated. Our current numerical accuracy achieved\nfor the total and binding energies of the weakly-bound (1,1)-states in the both\n$d d \\mu$ and $d t \\mu$ ions significantly exceeds similar accuracy obtained in\nearlier computations of these weakly-bound states in these two ions. The bound\nstate properties of the weakly-bound (1,1)-states and (2,0)-states in the $d d\n\\mu, d t \\mu$ and $t t \\mu$ muonic ions have never been determined to high\naccuracy in earlier studies. We also briefly discuss the current status of\nmuon-catalyzed nuclear fusion and develop the new universal variational\nexpansion which can be used for extremely accurate bound state calculations of\narbitrary three-body systems, including the truly adiabatic\n${}^{\\infty}$H$^{+}_{2}$ ion and close systems.",
        "We investigate how a magnetic background field influences the location and\nthe nature of the Roberge-Weiss (RW) finite temperature transition for $N_f =\n2+1$ QCD with physical quark masses. To that purpose, we perform numerical\nsimulations of the finite temperature theory, discretized through stout\nstaggered quarks and the tree-level improved Symanzik pure gauge action,\nconsidering two different values of the Euclidean temporal extent in lattice\nunits, $N_t = 6, 8$. The RW transition temperature $T_{RW}$ decreases with\n$eB$, in particular it follows closely the behavior of the pseudo-critical QCD\ncrossover temperature $T_{pc}$, so that $T_{RW} (eB) - T_{pc}(eB)$ is\npractically constant, within errors, for magnetic fields up to $eB \\sim 1$\nGeV$^2$; consistent results are found from the drop of the chiral condensate,\nwhich signals chiral symmetry restoration, leading also to the phenomenon of\ninverse magnetic catalysis above the transition. Moreover, we find that the\nmagnetic field turns the RW transition from second order to first order, with a\ntri-critical magnetic field in-between 1 and 2.4 GeV$^2$, i.e. for magnetic\nfields substantially lower than those for which the standard QCD transition\nturns to first order.",
        "GLEAM-X J1627-52 was discovered as a periodic (~18 min) radio signal over a\nduration of three months in 2018. It is an enigmatic example of a growing\npopulation of 'long-period radio transients' consistent with Galactic origins.\nTheir nature is uncertain, and leading models invoke magnetic neutron stars or\nwhite dwarfs, potentially in close binary systems, to power them. GLEAM-X\nJ1627-52 resides in the Galactic plane with a comparatively coarse localisation\n(~2 arcsecond). Here we study the localisation region to search for\nspectrophotometric signatures of a counterpart using time-domain searches in\noptical and near-infrared imaging, and MUSE integral field spectroscopy. No\nsources in the localisation display clear white dwarf spectral signatures,\nalthough at the expected distance we can only provide modest limits on their\npresence directly. We rule out the presence of hot sub-dwarfs in the vicinity.\nWe found no candidate within our search for variability or periodic behaviour\nin the light curves. Radial velocity curves additionally show only weak\nevidence of variation, requiring any realistic underlying system to have very\nlow orbital inclination (i < 5 deg). Two Balmer emission line sources are\nreminiscent of white dwarf pulsar systems, but their characteristics fall\nwithin expected M-dwarf chromospheric activity with no signs of being in a\nclose binary. Currently the white dwarf pulsar scenario is not supported,\nalthough longer baseline data and data contemporaneous with a radio active\nepoch are required before stronger statements. Isolated magnetars, or compact\nbinaries remain viable. Our limits highlight the difficulty of these searches\nin dense environments at the limits of ground-based data.",
        "The thermopower $\\alpha$ (also known as the Seebeck coefficient) is one of\nthe most fundamental material characteristics for understanding charge carrier\ntransport in thermoelectric materials. Here, we revisit the Pisarenko formula\nfor the thermopower, which was traditionally considered valid only for\nnon-degenerate semiconductors. We demonstrate that regardless of the dominating\nscattering mechanism, the Pisarenko formula describes accurately enough the\nrelationship between thermopower $\\alpha$ and charge carrier concentration $n$\nbeyond the non-degenerate limit. Moreover, the Pisarenko formula provides a\nsimple thermopower-conductivity relation, $\\alpha = \\pm\n\\frac{k_{\\mathrm{B}}}{e} (b - \\ln \\sigma)$, valid for materials with $\\alpha >\n90$ $\\mu$V K$^{-1}$ when acoustic phonon scattering is predominant. This offers\nan alternative way to analyze electron transport when Hall measurements are\ndifficult or inaccessible. Additionally, we show how the Pisarenko formula can\nbe used to estimate the maximum power factor of a thermoelectric material from\nthe weighted mobility of a single, not necessarily optimized, sample at any\ngiven temperature.",
        "Active reconfigurable intelligent surface (A-RIS) aided integrated sensing\nand communications (ISAC) system has been considered as a promising paradigm to\nimprove spectrum efficiency. However, massive energy-hungry radio frequency\n(RF) chains hinder its large-scale deployment. To address this issue, an\nA-RIS-aided ISAC system with antenna selection (AS) is proposed in this work,\nwhere a target is sensed while multiple communication users are served with\nspecifically selected antennas. Specifically, a cuckoo search-based scheme is\nfirst utilized to select the antennas associated with high-gain channels.\nSubsequently, with the properly selected antennas, the weighted sum-rate (WSR)\nof the system is optimized under the condition of radar probing power level,\npower budget for the A-RIS and transmitter. To solve the highly non-convex\noptimization problem, we develop an efficient algorithm based on weighted\nminimum mean square error (WMMSE) and fractional programming (FP). Simulation\nresults show that the proposed AS scheme and the algorithm are effective, which\nreduce the number of RF chains without significant performance degradation.",
        "Let $\\mathcal{H}$ be a Hurwitz space that parametrises holomorphic maps to\n$\\mathbb{P}^1$. Abramovich, Corti and Vistoli, building on work of Harris and\nMumford, describe a compactification $\\overline{\\mathcal{H}}$ with a natural\nboundary stratification. We show that the irreducible strata of\n$\\overline{\\mathcal{H}}$ are in bijection with combinatorial objects called\ndecorated trees (up to a suitable equivalence), and that containment of\nirreducible strata is given by edge contraction of decorated trees. This\ncombinatorial description allows us to define a tropical Hurwitz space,\nrefining a definition given by Cavalieri, Markwig and Ranganathan. We also\ndiscuss applications to complex dynamics.",
        "We study the class of generalized Kerr-Schild (GKS) spacetimes in dimensions\n$n\\geq 3$ and analyze their geometric and algebraic properties in a completely\ntheory-independent setting. First, considering the case of a general null\nvector $\\mathbf{k}$ defined by the GKS metric, we obtain the conditions under\nwhich it is geodesic. Assuming $\\mathbf{k}$ to be geodesic for the remainder of\nthe paper, we examine the alignment properties of the curvature tensors, namely\nthe Ricci and Weyl tensors. We show that the algebraic types of the curvatures\nof the full (GKS) geometry are constrained by those of the respective\nbackground curvatures, thereby listing all kinematically allowed combinations\nof the algebraic types for the background and the full geometry. A notable\naspect of these results is that, unlike the case of Kerr-Schild (KS)\nspacetimes, the Weyl types of the GKS spacetimes need not be type $II$ or more\nspecial. Then, focusing on the case of an expanding $\\mathbf k$, we derive the\nconditions for it to satisfy the optical constraint, extending the previous\nresults of KS spacetimes. We illustrate the general results using the example\nof (A)dS-Taub-NUT spacetimes in $n=4$, where we also comment on their KS double\ncopy from a GKS perspective. Finally, as an application of our general results,\nwe obtain the full family of GKS spacetimes with a geodesic, expanding,\ntwistfree, and shearfree $\\mathbf k$, satisfying the vacuum Einstein equations,\nand identify it with a subset of the higher-dimensional vacuum\nRobinson-Trautman solutions. In passing, we also determine the subcase of these\nsolutions that manifests the KS double copy.",
        "The cosmic distance duality relates the angular-diameter and luminosity\ndistances and its possible violation may puzzle the standard cosmological\nmodel. This appears particularly interesting in view of the recent results\nfound by the DESI Collaboration, suggesting that a dynamical dark energy\nscenario seems to be favored than a genuine cosmological constant. Accordingly,\nwe take into account possible violations by considering four different\nparameterizations, namely: a Taylor expansion around $z\\simeq 0$, a\nslightly-departing logarithmic correction, a (1;2) Pad\\'e rational series to\nheal the convergence problem and a Chebyshev polynomial expansion, reducing\n\\emph{de facto} the systematic errors associated with the analysis. We test\neach of them in a model-independent (-dependent) way, by working out\nMonte-Carlo Markov chain analyses, employing the B\\'ezier interpolation of the\nHubble rate $H(z)$ for the model-independent approach while assuming the flat\n(non-flat) $\\Lambda$CDM and $\\omega_0\\omega_1$CDM models, motivating the latter\nparadigm in view of the DESI findings. Subsequently, we explore two analyses,\nemploying observational Hubble data, galaxy clusters from the Sunyaev-Zeldovich\neffect and type Ia supernovae, investigating the impact of the DESI data\ncatalog, first including then excluding the entire data set. Afterwards, we\nadopt statistical model selection criteria to assess the statistically favored\ncosmological model. Our results suggest \\emph{no violation} of the cosmic\ndistance duality. While a slight spatial curvature cannot be entirely excluded,\nthe preferred cosmological model remains the flat $\\Lambda$CDM background, even\nwhen incorporating DESI data. Finally, concerning the Hubble tension, our\nfindings match the Riess estimates, as BAO data points are excluded.",
        "The characterization of quantum correlations in many-body systems is\ninstrumental to understanding the nature of emergent phenomena in quantum\nmaterials. The correlation entropy serves as a key metric for assessing the\ncomplexity of a quantum many-body state in interacting electronic systems.\nHowever, its determination requires the measurement of all single-particle\ncorrelators across a macroscopic sample, which can be impractical. Machine\nlearning methods have been shown to allow learning the correlation entropy from\na reduced set of measurements, yet these methods assume that the targeted\nsystem is contained in the set of training Hamiltonians. Here we show that a\ntransfer learning strategy enables correlation entropy learning from a reduced\nset of measurements in families of Hamiltonians never considered in the\ntraining set. We demonstrate this transfer learning methodology in a wide\nvariety of interacting models including local and non-local attractive and\nrepulsive many-body interactions, long-range hopping, doping, magnetic field,\nand spin-orbit coupling. Furthermore, we show how this transfer learning\nmethodology allows detecting quantum many-body phases never observed during\ntheir training set without prior knowledge about them. Our results demonstrate\nthat correlation entropy learning can be potentially performed experimentally\nwithout requiring training in the experimentally realized Hamiltonian.",
        "An M-eigenvalue of a nonnegative biquadratic tensor is referred to as an\nM$^+$-eigenvalue if it has a pair of nonnegative M-eigenvectors. If furthermore\nthat pair of M-eigenvectors is positive, then that M$^+$-eigenvalue is called\nan M$^{++}$-eigenvalue. A nonnegative biquadratic tensor always has at least\none M$^+$ eigenvalue, and the largest M$^+$-eigenvalue is also the largest\nM-eigenvalue and the M-spectral radius. In the case of an irreducible\nnonnegative biquadratic tensor, all the M$^+$-eigenvalues are\nM$^{++}$-eigenvalues. Although the M$^+$-eigenvalues of irreducible nonnegative\nbiquadratic tensors are not unique in general, we establish a sufficient\ncondition to ensure their uniqueness. For an irreducible nonnegative\nbiquadratic tensor, the largest M$^+$-eigenvalue has a max-min\ncharacterization, while the smallest M$^+$-eigenvalue has a min-max\ncharacterization. A Collatz algorithm for computing the largest\nM$^+$-eigenvalues is proposed. Numerical results are reported.",
        "Consensus, synchronization, formation control, and power grid balance are all\nexamples of virtuous dynamical states that may arise in networks. Here we focus\non how such states can be destabilized from a fundamental perspective; namely,\nwe address the question of how one or a few intruder agents within an otherwise\nfunctioning network may compromise its dynamics. We show that a single\nadversarial node coupled via adversarial couplings to one or more other nodes\nis sufficient to destabilize the entire network, which we prove to be more\nefficient than targeting multiple nodes. Then, we show that concentrating the\nattack on a single low-indegree node induces the greatest instability,\nchallenging the common assumption that hubs are the most critical nodes. This\nleads to a new characterization of the vulnerability of a node, which contrasts\nwith previous work, and identifies low-indegree nodes (as opposed to the hubs)\nas the most vulnerable components of a network. Our results are derived for\nlinear systems but hold true for nonlinear networks, including those described\nby the Kuramoto model. Finally, we derive scaling laws showing that larger\nnetworks are less susceptible, on average, to single-node attacks. Overall,\nthese findings highlight an intrinsic vulnerability of technological systems\nsuch as autonomous networks, sensor networks, power grids, and the internet of\nthings, with implications also to the realm of complex social and biological\nnetworks.",
        "Local and global well-posedness, along with finite time blow-up, are\ninvestigated for the following Hardy-H\\'enon equation involving a quasilinear\ndegenerate diffusion and a space-dependent superlinear source featuring a\nsingular potential $$\\partial_t u=\\Delta u^m+|x|^{\\sigma}u^p,\\quad t>0,\\\nx\\in\\mathbb{R}^N,$$ when $m>1$, $p>1$ and $\\sigma\\in \\big(\\max\\{-2,-N\\},0\n\\big)$. While the superlinear source induces finite time blow-up when\n$\\sigma=0$, whatever the value of $p>1$, at least for sufficiently large\ninitial conditions, a striking effect of the singular potential $|x|^\\sigma$ is\nthe prevention of finite time blow-up for suitably small values of $p$, namely,\n$1<p\\le p_G := [2-\\sigma(m-1)]\/2$. Such a result, as well as the local\nexistence of solutions for $p>p_G$, is obtained by employing the\nCaffarelli-Kohn-Nirenberg inequalities. Another interesting feature is that\nuniqueness and comparison principle hold true for generic non-negative initial\nconditions when $p>p_G$, but their validity is restricted to initial conditions\nwhich are positive in a neighborhood of $x=0$ when $p\\in (1,p_G)$, a range in\nwhich non-uniqueness holds true without this positivity condition. Finite time\nblow-up of any non-trivial, non-negative solution is established when\n$p_G<p\\leq p_F:=m+(\\sigma+2)\/N$, while global existence for small initial data\nin some critical Lebesgue spaces and blow-up in finite time for initial data\nwith a negative energy are proved for $p>p_F$. Optimal temporal growth rates\nare also derived for global solutions when $p\\in (1,p_G]$. All the results are\nsharp with respect to the exponents $(m,p,\\sigma)$ and conditions on $u_0$.",
        "Lobbying systems are complex political networks that influence governmental\ndecisions, with often profound socio-economic consequences on national and\nglobal scales. For most political systems, a comprehensive understanding of\nlobbying strategies and dynamics is likely to remain elusive as time-resolved\nsystem-spanning data and analysis are lacking. A notable exception is the\nUnited States (U.S.), where the Lobbying Disclosure Act (LDA) of 1995 mandates\nthat all federal lobbying activities be disclosed in detailed quarterly\nreports. Here, we introduce our recently completed relational LobbyView\ndatabase that accounts for every reported lobbying instance since the\nimplementation of the LDA. We demonstrate how LobbyView can be used as a\nresource to quantify the salient aspects of the U.S.~lobbying, such as\ndynamical evolution, economic impacts, or political polarization. By analyzing\nthe dynamic evolution of the lobbying network, we identify fundamental\nself-organization principles, such as the self-accelerating accumulation of\ninfluence within a small group of powerful lobbying firms. We further show how\nLobbyView data can be used to accurately measure the synchronization of\nlobbying activities with election cycles. Finally, as a guide to future\nresearch, we illustrate how this data resource can enable quantitative\ntime-resolved analysis to investigate a wide range of critical issues,\nincluding concentration of resources, demographic representation, and\npolarization dynamics in the U.S.~lobbying system. We envisage our database to\nbe not only a potent resource for political and social scientists but also a\nstarting point for quantitative interdisciplinary research, by leveraging\ninsights and methods from statistical physics, systems biology, and machine\nlearning.",
        "The new era of large-scale data collection and analysis presents an\nopportunity for diagnosing and understanding the causes of health inequities.\nIn this study, we describe a framework for systematically analyzing health\ndisparities using causal inference. The framework is illustrated by\ninvestigating racial and ethnic disparities in intensive care unit (ICU)\noutcome between majority and minority groups in Australia (Indigenous vs.\nNon-Indigenous) and the United States (African-American vs. White). We\ndemonstrate that commonly used statistical measures for quantifying inequity\nare insufficient, and focus on attributing the observed disparity to the causal\nmechanisms that generate it. We find that minority patients are younger at\nadmission, have worse chronic health, are more likely to be admitted for urgent\nand non-elective reasons, and have higher illness severity. At the same time,\nhowever, we find a protective direct effect of belonging to a minority group,\nwith minority patients showing improved survival compared to their majority\ncounterparts, with all other variables kept equal. We demonstrate that this\nprotective effect is related to the increased probability of being admitted to\nICU, with minority patients having an increased risk of ICU admission. We also\nfind that minority patients, while showing improved survival, are more likely\nto be readmitted to ICU. Thus, due to worse access to primary health care,\nminority patients are more likely to end up in ICU for preventable conditions,\ncausing a reduction in the mortality rates and creating an effect that appears\nto be protective. Since the baseline risk of ICU admission may serve as proxy\nfor lack of access to primary care, we developed the Indigenous Intensive Care\nEquity (IICE) Radar, a monitoring system for tracking the over-utilization of\nICU resources by the Indigenous population of Australia across geographical\nareas.",
        "The min-sum approximation is widely used in the decoding of polar codes.\nAlthough it is a numerical approximation, hardly any penalties are incurred in\npractice. We give a theoretical justification for this. We consider the common\ncase of a binary-input, memoryless, and symmetric channel, decoded using\nsuccessive cancellation and the min-sum approximation. Under mild assumptions,\nwe show the following. For the finite length case, we show how to exactly\ncalculate the error probabilities of all synthetic (bit) channels in time\n$O(N^{1.585})$, where $N$ is the codeword length. This implies a code\nconstruction algorithm with the above complexity. For the asymptotic case, we\ndevelop two rate thresholds, denoted $R_{\\mathrm{L}} = R_{\\mathrm{L}}(\\lambda)$\nand $R_{\\mathrm{U}} =R_{\\mathrm{U}}(\\lambda)$, where $\\lambda(\\cdot)$ is the\nlabeler of the channel outputs (essentially, a quantizer). For any $0 < \\beta <\n\\frac{1}{2}$ and any code rate $R < R_{\\mathrm{L}}$, there exists a family of\npolar codes with growing lengths such that their rates are at least $R$ and\ntheir error probabilities are at most $2^{-N^\\beta}$. That is, strong\npolarization continues to hold under the min-sum approximation. Conversely, for\ncode rates exceeding $R_{\\mathrm{U}}$, the error probability approaches $1$ as\nthe code-length increases, irrespective of which bits are frozen. We show that\n$0 < R_{\\mathrm{L}} \\leq R_{\\mathrm{U}} \\leq C$, where $C$ is the channel\ncapacity. The last inequality is often strict, in which case the ramification\nof using the min-sum approximation is that we can no longer achieve capacity.",
        "Previously a mathematical approach has been developed for calculation of the\npropagation time of a signal, emitted by a moving along an elliptical orbit\nsatellite, with account also for the General Relativity Theory (GRT) effects.\nThe formalism was restricted to one dynamical parameter (the true anomaly or\nthe eccentric anomaly angle). In this paper the aim is to extend the formalism\nto the case, when also the other five Kepler parameters will be changing.The\nfollowing problem can be formulated: if two satellites move on two\nspace-distributed orbits and they exchange signals, how can the propagation\ntime be calculated? In this paper approaches from differential geometry and\ntopology were implemented.The action functional for the propagation time is\nrepresented in the form of a quadratic functional in the differentials of the\nKepler elements. The known mapping from celestial mechanics is used, when by\nmeans of a transformation the 6 Kepler parameters are mapped into the cartesian\ncoordinates X, Y, Z. This is in fact a submersion of a manifold of 6 parameters\ninto a manifold of 3 parameters. If a variational approach is applied with\nrespect to a differential form in terms of the differentials of the Kepler\nparameters, the second variation will be different from zero and the Stokes\ntheorem can be applied, provided that the second partial derivatives of the\nCartesian coordinates with respect to the Kepler parameters are assumed to be\ndifferent from zero. From topology viewpoint this requirement is equivalent to\nthe existence of the s.c. Morse functions (non-degenerate at the critical\npoints). In the given case it has been shown that Morse function cannot exist\nwith respect to each one of the Kepler parameters- Morse function cannot be\ndefined with respect to the omega angle.",
        "Let $X$ be a variety over $\\mathbb Q$. We introduce a geometric\nnon-degenerate criterion for $X$ using moduli spaces $M$ over $\\mathbb Q$ of\nabelian varieties. If $X$ is non-degenerate, then we construct via $M$ an open\ndense moduli space $U\\subseteq X$ whose forgetful map defines a Parsin\nconstruction for $U(\\mathbb Q)$. For example if $M$ is a Hilbert modular\nvariety then $U$ is a coarse Hilbert moduli scheme and $X$ is non-degenerate\niff a projective model $Y\\subset \\bar{M}$ of $X$ over $\\mathbb Q$ contains no\nsingular points of the minimal compactification $\\bar{M}$. We motivate our\nconstructions when $M$ is a rational variety over $\\mathbb Q$ with\n$\\dim(M)>\\dim(X)$. We study various geometric aspects of the non-degenerate\ncriterion and we deduce arithmetic applications: If $X$ is non-degenerate, then\n$U(\\mathbb Q)$ is finite by Faltings. Moreover, our constructions are made for\nthe effective strategy which combines the method of Faltings (Arakelov, Parsin,\nSzpiro) with modularity and Masser-Wustholz isogeny estimates. When $M$ is a\ncoarse Hilbert moduli scheme, we use this strategy to explicitly bound the\nheight and the number of $x\\in U(\\mathbb Q)$ if $X$ is non-degenerate. We\nillustrate our approach when $M$ is the Hilbert modular surface given by the\nclassical icosahedron surface studied by Clebsch, Klein and Hirzebruch. For any\ncurve $X$ over $\\mathbb Q$, we construct and study explicit projective models\n$Y\\subset\\bar{M}$ called ico models. If $X$ is non-degenerate, then we give via\n$Y$ an effective Parsin construction and an explicit Weil height bound for\n$x\\in U(\\mathbb Q)$. As most ico models are non-degenerate and $X\\setminus U$\nis controlled, this establishes the effective Mordell conjecture for large\nclasses of (explicit) curves over $\\mathbb Q$. We also solve the ico analogue\nof the generalized Fermat problem by combining our height bounds with\nDiophantine approximations.",
        "In this paper, we study deeply geometric and topological properties of\nFinsler metric measure manifolds with the integral weighted Ricci curvature\nbounds. We first establish Laplacian comparison theorem, Bishop-Gromov type\nvolume comparison theorem and relative volume comparison theorem on such\nFinsler manifolds. Then we obtain a volume growth estimate and Gromov\npre-compactness under the integral weighted Ricci curvature bounds.\nFurthermore, we prove the local Dirichlet isoperimetric constant estimate on\nFinsler metric measure manifolds with integral weighted Ricci curvature bounds.\nAs applications of the Dirichlet isoperimetric constant estimates, we get first\nDirichlet eigenvalue estimate and a gradient estimate for harmonic functions.",
        "In this brief paper, we provide a mathematical framework that exploits the\nrelationship between the maximum principle and dynamic programming for\ncharacterizing optimal learning trajectories in a class of learning problem,\nwhich is related to point estimations for modeling of high-dimensional\nnonlinear functions. Here, such characterization for the optimal learning\ntrajectories is associated with the solution of an optimal control problem for\na weakly-controlled gradient system with small parameters, whose time-evolution\nis guided by a model training dataset and its perturbed version, while the\noptimization problem consists of a cost functional that summarizes how to gauge\nthe quality\/performance of the estimated model parameters at a certain fixed\nfinal time w.r.t. a model validating dataset. Moreover, using a successive\nGalerkin approximation method, we provide an algorithmic recipe how to\nconstruct the corresponding optimal learning trajectories leading to the\noptimal estimated model parameters for such a class of learning problem.",
        "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
        "Data and algorithms have the potential to produce and perpetuate\ndiscrimination and disparate treatment. As such, significant effort has been\ninvested in developing approaches to defining, detecting, and eliminating\nunfair outcomes in algorithms. In this paper, we focus on performing\nstatistical inference for fairness. Prior work in fairness inference has\nlargely focused on inferring the fairness properties of a given predictive\nalgorithm. Here, we expand fairness inference by evaluating fairness in the\ndata generating process itself, referred to here as data fairness. We perform\ninference on data fairness using targeted learning, a flexible framework for\nnonparametric inference. We derive estimators demographic parity, equal\nopportunity, and conditional mutual information. Additionally, we find that our\nestimators for probabilistic metrics exploit double robustness. To validate our\napproach, we perform several simulations and apply our estimators to real data.",
        "We study moduli spaces and moduli stacks for representations of associative\nalgebras in Azumaya algebras, in rather general settings. We do not impose any\nstability condition and work over arbitrary ground rings, but restrict\nattention to the so-called Schur representations, where the only automorphisms\nare scalar multiplications. The stack comprises twisted representations, which\nare representations that live on the gerbe of splittings for the Azumaya\nalgebra. Such generalized spaces and stacks appear naturally: For any rational\npoint on the classical coarse moduli space of matrix representations, the\nmachinery of non-abelian cohomology produces a modified moduli problem for\nwhich the point acquires geometric origin. The latter are given by\nrepresentations in Azumaya algebras.",
        "Local Irregularity Conjecture states that every simple connected graph,\nexcept special cacti, can be decomposed into at most three locally irregular\ngraphs, i.e., graphs in which adjacent vertices have different degrees. The\nconnected minimization problem, finding the minimum number $k$ such that a\ngraph can be decomposed into $k$ locally irregular graphs, is known to be\nNP-hard in general (Baudon, Bensmail, and Sopena, 2015). This naturally raises\ninterest in the study of related problems. Among others, the concept of local\nirregularity was defined for digraphs in several different ways. In this paper\nwe present the following new methods of defining a locally irregular digraph.\nThe first one, weak local irregularity, is based on distinguishing adjacent\nvertices by indegree-outdegree pairs, and the second one, strong local\nirregularity, asks for different balanced degrees (i.e., difference between the\noutdegree and the indegree of a vertex) of adjacent vertices. For both of these\nirregularities, we define locally irregular decompositions and colorings of\ndigraphs. We discuss relation of these concept to others, which were studied\npreviously, and provide related conjectures on the minimum number of colors in\nweak and strong locally irregular colorings. We support these conjectures with\nnew results, using the chromatic and structural properties of digraphs and\ntheir skeletons (Eulerian and symmetric digraphs, orientations of regular\ngraphs, cacti, etc.).",
        "The removal of microplastics and oil from oil-water emulsions presents\nsignificant challenges in membrane technology due to issues with low\npermeability, rejection rates, and membrane fouling. This study focuses on\nenhancing nanofibrous composite membranes to effectively separate microplastic\ncontaminants (0.5 micrometer) and oil-water emulsions in wastewater.\nPolyvinylidene fluoride (PVDF) polymeric nanofibers were produced using a\nneedle-free electrospinning technique and attached to a nonwoven surface\nthrough lamination. The membranes were modified with alkaline treatment,\nbiosurfactant (BS), $TiO_2$, and CuO particles to prevent fouling and improve\nseparation efficiency. The modified membranes demonstrated exceptional water\npermeability, with BS-modified membranes reaching above 9000 $Lm^{-2} h^{-1}\nbar^{-1}$ for microplastic separation. However, BS modifications led to reduced\nwater permeability during oil-water emulsion treatment. $TiO_2$ and CuO further\nenhanced permeability and reduced fouling. The $TiO_2$-modified membranes\nexhibited superior performance in oil-water emulsion separation, maintaining\nhigh oil rejection rates (~95%) and antifouling properties. The maximum\nmicroplastic and oil rejection rates were of 99.99% and 95.30%, respectively.\nThis study illustrates the successful modification of membrane surfaces to\nimprove the separation of microplastics and oil-water emulsions, offering\nsignificant advancements in wastewater treatment technology.",
        "We provide radial variational estimates for positive harmonic functions on\nLipschitz domains in higher dimensions. The intention of this paper is to\ndocument an updated and refined version of arXiv:2003.07176 which modifies the\nproof of Mozolyako and Havin for Lipschitz domains.",
        "The interaction between nucleon and charmonia ($J\/\\psi$ and $\\eta_c$) is\nexpected to deepen our understanding of various aspects in nonperturbative QCD\nranging from the origin of nucleon mass to $J\/\\psi$ mass modification in\nnuclear medium and properties of hidden-charm pentaquark states. Here, we\npresent the low-energy $NJ\/\\psi$ and $N\\eta_c$ interactions based on ($2+1$)\nflavor lattice QCD simulations with nearly physical pion mass $m_\\pi=146$ MeV.\nThe interactions, extracted from the spacetime correlations of the nucleon and\ncharmonium system by using the HAL QCD method, are found to be attractive in\nall distances and manifest a characteristic long-range tail consistent with the\ntwo-pion exchange interaction. The resulting scattering lengths are around\n$0.3$ fm, $0.4$ fm and $0.2$ fm for $NJ\/\\psi$ with spin $3\/2$, with spin $1\/2$,\nand $N\\eta_c$, respectively. Our results are orders of magnitude larger than\nthose from the photoproduction experiments assuming the vector meson dominance.",
        "The impact of neutrino flavor conversion on the supernova mechanism is yet to\nbe fully understood. We present multi-energy and multi-angle solutions of the\nneutrino quantum kinetic equations in three flavors, taking into account\nneutrino advection and non-forward collisions with the background medium.\nFlavor evolution is explored within a spherically symmetric shell surrounding\nthe region of neutrino decoupling in the interior of a core-collapse supernova,\nrelying on the outputs of a spherically symmetric core-collapse supernova model\nwith a mass of $18.6 M_\\odot$. We select two representative post-bounce times:\n$t_{\\rm pb} = 0.25$ s (no angular crossings are present and flavor conversion\nis triggered by slow collective effects) and $t_{\\rm pb} = 1$ s (angular\ncrossings trigger fast flavor instabilities). We find that flavor equipartition\nis achieved in the antineutrino sector between $\\bar\\nu_e$ and $\\bar\\nu_x =\n(\\bar\\nu_\\mu + \\bar\\nu_\\tau)\/2$ for both post-bounce times. In the neutrino\nsector, flavor equipartition between $\\nu_e$ and $\\nu_x$ seems more likely at\nlater post-bounce times, where the neutrino emission properties among different\nflavors tend to approach each other, but it is not a generic feature. The\nexponential growth of the $\\nu_\\mu$--$\\nu_\\tau$ asymmetry due to three-flavor\neffects is responsible for differences between the quasi-steady configurations\nobtained in the three-flavor solution and in the two-flavor approximation. This\nhas consequences on the neutrino heating rate, which is generally larger when\nall three flavors are taken into account and can increase up to $30\\%$ with\nrespect to the case where flavor conversion is neglected."
      ]
    }
  },
  {
    "id":2412.02695,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"EEG data for ADHD \/ Control children",
    "start_abstract":"Participants were 61 children with ADHD and 60 healthy controls (boys and girls, ages 7-12). The ADHD children were diagnosed by an experienced psychiatrist to DSM-IV criteria, and have taken Ritalin for up to 6 months. None of the children in the control group had a history of psychiatric disorders, epilepsy, or any report of high-risk behaviors. EEG recording was performed based on 10-20 standard by 19 channels (Fz, Cz, Pz, C3, T3, C4, T4, Fp1, Fp2, F3, F4, F7, F8, P3, P4, T5, T6, O1, O2) at 128 Hz sampling frequency. The A1 and A2 electrodes were the references located on earlobes. Since one of the deficits in ADHD children is visual attention, the EEG recording protocol was based on visual attention tasks. In the task, a set of pictures of cartoon characters was shown to the children and they were asked to count the characters. The number of characters in each image was randomly selected between 5 and 16, and the size of the pictures was large enough to be easily visible and countable by children. To have a continuous stimulus during the signal recording, each image was displayed immediately and uninterrupted after the child\u2019s response. Thus, the duration of EEG recording throughout this cognitive visual task was dependent on the child\u2019s performance (i.e. response speed).",
    "start_categories":[
      "Neurotherapeutics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Deep Residual Learning for Image Recognition"
      ],
      "abstract":[
        "Deeper neural networks are more difficult to train. We present a residual learning framework ease the training of that substantially deeper than those used previously. explicitly reformulate layers as functions with reference layer inputs, instead unreferenced functions. provide comprehensive empirical evidence showing these easier optimize, and can gain accuracy from considerably increased depth. On ImageNet dataset we evaluate nets depth up 152 - 8\u00d7 VGG [40] but still having lower complexity. An ensemble achieves 3.57% error on test set. This result won 1st place ILSVRC 2015 classification task. also analysis CIFAR-10 100 1000 layers. The representations is central importance for many visual recognition tasks. Solely due our extremely deep representations, obtain 28% relative improvement COCO object detection dataset. Deep foundations submissions & competitions1, where places tasks detection, localization, segmentation."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Positive matching decompositions of the cartesian product of graphs",
        "An Analytical Model for Overparameterized Learning Under Class Imbalance",
        "Poisson Hail on a Wireless Ground",
        "Single-layer magnet phase in intrinsic magnetic topological insulators,\n  $[\\mathrm{MnTe}][\\mathrm{Bi}_{2}\\mathrm{Te}_{3}]_{\\mathrm{n}}$, far beyond\n  the thermodynamic limit",
        "Cosmic acceleration as a gravitational bifurcation",
        "The probability for chiral oscillation of Majorana neutrino in Quantum\n  Field Theory",
        "Proof-Producing Translation of Functional Programs into a Time \\& Space\n  Reasonable Model",
        "Concept-Based Explainable Artificial Intelligence: Metrics and\n  Benchmarks",
        "Different physical and numerical sources of scatter in the\n  $M_{\\star}$-$M_{\\mathrm{BH}}$ relation and their connection to galaxy\n  evolution",
        "AttentionSwarm: Reinforcement Learning with Attention Control Barier\n  Function for Crazyflie Drones in Dynamic Environments",
        "Quantifying Uncertainty and Variability in Machine Learning: Confidence\n  Intervals for Quantiles in Performance Metric Distributions",
        "Tensor network state methods and quantum information theory for strongly\n  correlated molecular systems",
        "bayesNMF: Fast Bayesian Poisson NMF with Automatically Learned Rank\n  Applied to Mutational Signatures",
        "The Evolution of Unobserved Skill Returns in the U.S.: A New Approach\n  Using Panel Data",
        "Black Strings and String Clouds Embedded in Anisotropic Quintessence:\n  Solutions for Scalar Particles and Implications",
        "Advancing Time Series Wildfire Spread Prediction: Modeling Improvements\n  and the WSTS+ Benchmark",
        "A comprehensive review on developments of synthetic dimensions",
        "Formation of Anomalously Energetic Ions in Hollow Cathode Plume by\n  Charge Separation Instability",
        "How constraints on editing affects cultural evolution",
        "Hydrodynamics of ultralight complex scalar field dark matter and its\n  impact on the growth of structure",
        "Shifting Long-Context LLMs Research from Input to Output",
        "Accretion with Back-Reaction onto Cylindrically Symmetric Black Hole\n  with Energy Conditions Analysis",
        "TPCap: Unlocking Zero-Shot Image Captioning with Trigger-Augmented and\n  Multi-Modal Purification Modules",
        "Large-scale multifractality and lack of self-similar decay for Burgers\n  and 3D Navier-Stokes turbulence",
        "Notes on Tractor Calculus",
        "Si-compatible topological and infrared materials: the promise of Low-Sn\n  GeSn digital alloys",
        "Voting or Consensus? Decision-Making in Multi-Agent Debate",
        "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings",
        "Lower bounds for Ramsey numbers of bounded degree hypergraphs"
      ],
      "abstract":[
        "Let $\\Gamma=(V,E)$ be a finite simple graph. A matching $M \\subseteq E$ is\npositive if there exists a weight function on $V$ such that the matching $M$ is\ncharacterized by those edges with positive weights. A positive matching\ndecomposition (pmd) of $\\Gamma$ with $p$ parts is an ordered partition\n$E_1,\\ldots,E_p$ of $E$ such that $E_i$ is a positive matching of $(V, E\n\\setminus \\bigcup_{j=1}^{i-1} E_j)$, for $i = 1, \\ldots, p$. The smallest $p$\nfor which $\\Gamma$ admits a pmd with $p$ parts is denoted by\n$\\mathrm{pmd}(\\Gamma)$. We study the pmd of the Cartesian product of graphs and\ngive sharp upper bounds for them in terms of the pmds and chromatic numbers of\ntheir components. In special cases, we compute the pmd of grid graphs that is\nthe Cartesian product of paths and cycles.",
        "We study class-imbalanced linear classification in a high-dimensional\nGaussian mixture model. We develop a tight, closed form approximation for the\ntest error of several practical learning methods, including logit adjustment\nand class dependent temperature. Our approximation allows us to analytically\ntune and compare these methods, highlighting how and when they overcome the\npitfalls of standard cross-entropy minimization. We test our theoretical\nfindings on simulated data and imbalanced CIFAR10, MNIST and FashionMNIST\ndatasets.",
        "This paper defines a new model which incorporates three key ingredients of a\nlarge class of wireless communication systems: (1) spatial interactions through\ninterference, (2) dynamics of the queueing type, with users joining and\nleaving, and (3) carrier sensing and collision avoidance as used in, e.g.,\nWiFi. In systems using (3), rather than directly accessing the shared resources\nupon arrival, a customer is considerate and waits to access them until nearby\nusers in service have left. This new model can be seen as a missing piece of a\nlarger puzzle that contains such dynamics as spatial birth-and-death processes,\nthe Poisson-Hail model, and wireless dynamics as key other pieces. It is shown\nthat, under natural assumptions, this model can be represented as a Markov\nprocess on the space of counting measures. The main results are then two-fold.\nThe first is on the shape of the stability region and, more precisely, on the\ncharacterization of the critical value of the arrival rate that separates\nstability from instability. The second is of a more qualitative or perhaps even\nethical nature. There is evidence that for natural values of the system\nparameters, the implementation of sensing and collision avoidance stabilizes a\nsystem that would be unstable if immediate access to the shared resources would\nbe granted. In other words, for these parameters, renouncing greedy access\nmakes sharing sustainable, whereas indulging in greedy access kills the system.",
        "The intrinsic magnetic topological insulator (IMTI) family\n$[\\mathrm{MnTe}][\\mathrm{Bi}_{2}\\mathrm{Te}_{3}]_{\\mathrm{n}}$ has demonstrated\nmagneto-topological properties dependent on $n$, making it a promising platform\nfor advanced electronics and spintronics. However, due to technical barriers in\nsample synthesis, their properties in the large $n$ limit remain unknown. To\novercome this, we utilized the atomic layer-by-layer molecular beam epitaxy\n(ALL-MBE) technique and achieved IMTIs with $n$ as large as 15, far beyond the\npreviously reported in bulk crystals or thin films. Then, we discover that the\n\"single-layer magnet (SLM)\" phase, primarily determined by intralayer\nferromagnetic coupling, emerges for $n >$ $\\sim 4$ and remains little affected\nup to $n = 15$. Nonetheless, still, non-zero, interlayer ferromagnetic coupling\nis necessary to stabilize the SLM phase, suggesting that the SLM phase\neventually disappears in the $n\\to\\infty$ limit. This study uncovers the\nsecrets of IMTIs beyond the thermodynamic limit and opens a door to diverse\nmagneto-topological applications.",
        "We propose an interpretation of the observed cosmic acceleration in Friedmann\nuniverses as a gravitational bifurcation. Mathematically this becomes possible\ndue to the degenerate nature of the Friedmann equations leading to their versal\nunfolding that contains all possible stable perturbations in a generic way.\nPhysically it corresponds to the tendency of Friedmann universes for\nnon-equilibrium evolution leading to self-organization, and maintaining overall\nstability through local instabilities emerging as parameter-dependent states.\nIn this way, we obtain a variety of accelerating solutions attracted by past\nMilne-like and future open states describing geometries with evolving entropy\nwhich are near the simplest Friedmann universes (in a precise sense), without a\nneed for a cosmological constant or vacuum energy, quintessence, or modified\ngravity.",
        "We derive the probability for chiral oscillation of Majorana neutrinos based\non quantum field theory. Since the Hamiltonian under the Majorana mass term\ndoes not conserve lepton number, the eigenstates of lepton number change\ncontinuously over time. Therefore, the transition amplitude is described by the\ninner product of the eigenstates of lepton number at the time of the neutrino\nproduction and the detection. With the Bogoliubov transformation, we\nsuccessfully relates the lepton number eigenstates at different times. This\nmethod enables us to understand the time variation of lepton number induced by\nchiral oscillations in terms of transition probabilities. We also present the\nphysical picture that emerges through the Bogoliubov transformation.",
        "We present a semi-automated framework to construct and reason about programs\nin a deeply-embedded while-language. The while-language we consider is a simple\ncomputation model that can simulate (and be simulated by) Turing machines with\na linear time and constant space blow-up. Our framework derives while-programs\nfrom functional programs written in a subset of Isabelle\/HOL, namely\ntail-recursive functions with first-order arguments and algebraic datatypes. As\nfar as we are aware, it is the first framework targeting a computation model\nthat is reasonable in time and space from a complexity-theoretic perspective.",
        "Concept-based explanation methods, such as concept bottleneck models (CBMs),\naim to improve the interpretability of machine learning models by linking their\ndecisions to human-understandable concepts, under the critical assumption that\nsuch concepts can be accurately attributed to the network's feature space.\nHowever, this foundational assumption has not been rigorously validated, mainly\nbecause the field lacks standardised metrics and benchmarks to assess the\nexistence and spatial alignment of such concepts. To address this, we propose\nthree metrics: the concept global importance metric, the concept existence\nmetric, and the concept location metric, including a technique for visualising\nconcept activations, i.e., concept activation mapping. We benchmark post-hoc\nCBMs to illustrate their capabilities and challenges. Through qualitative and\nquantitative experiments, we demonstrate that, in many cases, even the most\nimportant concepts determined by post-hoc CBMs are not present in input images;\nmoreover, when they are present, their saliency maps fail to align with the\nexpected regions by either activating across an entire object or misidentifying\nrelevant concept-specific regions. We analyse the root causes of these\nlimitations, such as the natural correlation of concepts. Our findings\nunderscore the need for more careful application of concept-based explanation\ntechniques especially in settings where spatial interpretability is critical.",
        "Observations have established that the masses of supermassive black holes\n(SMBHs) correlate tightly with the stellar masses of their host galaxies,\nalbeit with substantial scatter. The size of this scatter as a function of\ngalaxy mass and redshift contains valuable information about the origin of\nSMBHs and the physical nature of their co-evolution with galaxies. In this\nwork, we highlight this connection by studying the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation for massive galaxies in the Illustris,\nIllustrisTNG (TNG), and EAGLE cosmological simulations. We find that the\nscatter in TNG is significantly lower than in Illustris and EAGLE, reflecting\ntheir different BH feedback models. By performing various numerical\nexperiments, we quantify different contributions to the scatter in the\nsimulations, and also identify a suitably defined intrinsic scatter. The\nintrinsic scatter in Illustris and EAGLE is $\\sim0.3$ dex at $z=0$, and is\ndominated by variations from BH accretion, whereas the smaller scatter of TNG\nis rather dominated by hierarchical merging, suggesting that the massive\ngalaxies in TNG are more tightly quenched. Variations in the BH seed mass can\ncontribute to the scatter of the $M_{\\rm BH}-M_{\\star}$ relation as well, but\nwhether this still plays a role at $z=0$ depends on the feedback model.\nSimulations with disabled AGN feedback produce much higher scatter for low-mass\ngalaxies than seen in our cosmological simulations, demonstrating the crucial\ninfluence of feedback for determining the co-evolution of SMBHs and their host\ngalaxies in this regime. In contrast, an important factor in reducing the\nscatter for massive galaxies is hierarchical merging of mostly quenched\nsystems. Based on our results, we expect that the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation at high redshift could be particularly\npowerful in providing clues to the origin of SMBHs.",
        "We introduce AttentionSwarm, a novel benchmark designed to evaluate safe and\nefficient swarm control across three challenging environments: a landing\nenvironment with obstacles, a competitive drone game setting, and a dynamic\ndrone racing scenario. Central to our approach is the Attention Model Based\nControl Barrier Function (CBF) framework, which integrates attention mechanisms\nwith safety-critical control theory to enable real-time collision avoidance and\ntrajectory optimization. This framework dynamically prioritizes critical\nobstacles and agents in the swarms vicinity using attention weights, while CBFs\nformally guarantee safety by enforcing collision-free constraints. The safe\nattention net algorithm was developed and evaluated using a swarm of Crazyflie\n2.1 micro quadrotors, which were tested indoors with the Vicon motion capture\nsystem to ensure precise localization and control. Experimental results show\nthat our system achieves landing accuracy of 3.02 cm with a mean time of 23 s\nand collision-free landings in a dynamic landing environment, 100% and\ncollision-free navigation in a drone game environment, and 95% and\ncollision-free navigation for a dynamic multiagent drone racing environment,\nunderscoring its effectiveness and robustness in real-world scenarios. This\nwork offers a promising foundation for applications in dynamic environments\nwhere safety and fastness are paramount.",
        "Machine learning models are widely used in applications where reliability and\nrobustness are critical. Model evaluation often relies on single-point\nestimates of performance metrics such as accuracy, F1 score, or mean squared\nerror, that fail to capture the inherent variability in model performance. This\nvariability arises from multiple sources, including train-test split, weights\ninitialization, and hyperparameter tuning. Investigating the characteristics of\nperformance metric distributions, rather than focusing on a single point only,\nis essential for informed decision-making during model selection and\noptimization, especially in high-stakes settings.\n  How does the performance metric vary due to intrinsic uncertainty in the\nselected modeling approach? For example, train-test split is modified, initial\nweights for optimization are modified or hyperparameter tuning is done using an\nalgorithm with probabilistic nature?\n  This is shifting the focus from identifying a single best model to\nunderstanding a distribution of the performance metric that captures\nvariability across different training conditions. By running multiple\nexperiments with varied settings, empirical distributions of performance\nmetrics can be generated. Analyzing these distributions can lead to more robust\nmodels that generalize well across diverse scenarios.\n  This contribution explores the use of quantiles and confidence intervals to\nanalyze such distributions, providing a more complete understanding of model\nperformance and its uncertainty. Aimed at a statistically interested audience\nwithin the machine learning community, the suggested approaches are easy to\nimplement and apply to various performance metrics for classification and\nregression problems. Given the often long training times in ML, particular\nattention is given to small sample sizes (in the order of 10-25).",
        "A brief pedagogical overview of recent advances in tensor network state\nmethods are presented that have the potential to broaden their scope of\napplication radically for strongly correlated molecular systems. These include\nglobal fermionic mode optimization, i.e., a general approach to find an optimal\nmatrix product state (MPS) parametrization of a quantum many-body wave function\nwith the minimum number of parameters for a given error margin, the restricted\nactive space DMRG-RAS-X method, multi-orbital correlations and entanglement,\ndevelopments on hybrid CPU-multiGPU parallelization, and an efficient treatment\nof non-Abelian symmetries on high-performance computing (HPC) infrastructures.\nScaling analysis on NVIDIA DGX-A100 platform is also presented.",
        "Bayesian Non-Negative Matrix Factorization (NMF) is a method of interest\nacross fields including genomics, neuroscience, and audio and image processing.\nBayesian Poisson NMF is of particular importance for counts data, for example\nin cancer mutational signatures analysis. However, MCMC methods for Bayesian\nPoisson NMF require a computationally intensive augmentation. Further,\nidentifying latent rank is necessary, but commonly used heuristic approaches\nare slow and potentially subjective, and methods that learn rank automatically\nare unable to provide posterior uncertainties. In this paper, we introduce\nbayesNMF, a computationally efficient Gibbs sampler for Bayesian Poisson NMF.\nThe desired Poisson-likelihood NMF is paired with a Normal-likelihood NMF used\nfor high overlap proposal distributions in approximate Metropolis steps,\navoiding augmentation. We additionally define Bayesian factor inclusion (BFI)\nand sparse Bayesian factor inclusion (SBFI) as methods to identify rank\nautomatically while preserving posterior uncertainty quantification on the\nlearned matrices. We provide an open-source R software package with all models\nand plotting capabilities demonstrated in this paper on GitHub at\njennalandy\/bayesNMF. While our applications focus on mutational signatures, our\nsoftware and results can be extended to any use of Bayesian Poisson NMF.",
        "Economists disagree about the factors driving the substantial increase in\nresidual wage inequality in the US over the past few decades. To identify\nchanges in the returns to unobserved skills, we make a novel assumption about\nthe dynamics of skills rather than about the stability of skill distributions\nacross cohorts, as is standard. We show that our assumption is supported by\ndata on test score dynamics for older workers in the HRS. Using survey data\nfrom the PSID and administrative data from the IRS and SSA, we estimate that\nthe returns to unobserved skills $declined$ substantially in the late-1980s and\n1990s despite an increase in residual inequality. Accounting for firm-specific\npay differences yields similar results. Extending our framework to consider\noccupational differences in returns to skill and multiple unobserved skills, we\nfurther show that skill returns display similar patterns for workers employed\nin each of cognitive, routine, and social occupations. Finally, our results\nsuggest that increasing skill dispersion, driven by rising skill volatility,\nexplains most of the growth in residual wage inequality since the 1980s.",
        "We analyze the spacetime metric associated with a black string surrounded by\na cloud of strings and an anisotropic fluid of quintessence in cylindrically\nsymmetric AdS spacetime. We solve Einstein's equation to obtain the explicit\nform of the metric, investigate typical values for its parameters, and\ndetermine their role in the event horizon formation. Within our findings, we\nshow that the intensity of the cloud of strings regulates the size of the event\nhorizon and, when the cloud is absent, the horizon increases drastically for\nlarger values of the quintessence's state parameter $\\alpha_{Q}$. Additionally,\nthe metric shows that, unless $\\alpha_{Q}$ is close to its lower bound, the\ncontribution from the quintessence fluid is only significant at large distances\nfrom the black string. Finally, to explore the quantum implications of this\ndark energy candidate, we use the confluent Heun function to solve the\nKlein-Gordon equation for a spin-0 particle near the event horizon. Our results\nindicate that the presence of quintessence alters the particle's radial wave\nfunction. This modification, in principle, could give rise to an observable\nthat we termed as \\enquote{dark phase}.",
        "Recent research has demonstrated the potential of deep neural networks (DNNs)\nto accurately predict wildfire spread on a given day based upon\nhigh-dimensional explanatory data from a single preceding day, or from a time\nseries of T preceding days. Here, we introduce a variety of modeling\nimprovements that achieve state-of-the-art (SOTA) accuracy for both single-day\nand multi-day input scenarios, as evaluated on a large public benchmark for\nnext-day wildfire spread, termed the WildfireSpreadTS (WSTS) benchmark.\nConsistent with prior work, we found that models using time-series input\nobtained the best overall accuracy. Furthermore, we create a new benchmark,\nWSTS+, by incorporating four additional years of historical wildfire data into\nthe WSTS benchmark. Our benchmark doubles the number of unique years of\nhistorical data, expands its geographic scope, and, to our knowledge,\nrepresents the largest public benchmark for time-series-based wildfire spread\nprediction.",
        "The concept of synthetic dimensions has emerged as a powerful framework in\nphotonics and atomic physics, enabling the exploration of high-dimensional\nphysics beyond conventional spatial constraints. Originally developed for\nquantum simulations in high dimensions, synthetic dimensions have since\ndemonstrated advantages in designing novel Hamiltonians and manipulating\nquantum or optical states for exploring topological physics, and for\napplications in computing and information processing. Here we provide a\ncomprehensive overview of progress in synthetic dimensions across photonic,\natomic, and other physical platforms over the past decade. We showcase\ndifferent approaches used to construct synthetic dimensions and highlight key\nphysical phenomena enabled by the advantage of such a framework. By offering a\nunified perspective on developments in this field, we aim to provide insights\ninto how synthetic dimensions can bridge fundamental physics and applied\ntechnologies, fostering interdisciplinary engagement in quantum simulation,\natomic and photonic engineering, and information processing.",
        "Hollow cathodes are becoming the bottleneck of many electric propulsion\nsystems, because of the sputtering and erosion on both cathodes and thrusters\nfrom the generation of anomalously energetic ions. So far, it is believed that\nenergetic ions are formed by waves and instabilities always accompanied in\ncathode discharge, but there is no evidence yet that those proposed\ninstabilities can lead to such high ion energies measured in experiments. In\nthis work, a new mechanism of charge separation instability in hollow cathode\nplume is found via fully kinetic PIC simulations, which can easily produce\nenergetic ions to the same level as measured in experiments.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "The mass window of ultralight axion dark matter motivated by suppressing the\ngrowth of structure on subgalactic scales, $m\\sim 10^{-22}\\,\\mathrm{eV}$, is\nnow severely constrained by various observation data (e.g. Lyman-$\\alpha$\nforest). As an attempt to reopen this mass window, we investigate an\nalternative ultralight dark matter candidate, the complex scalar field dark\nmatter (SFDM). We derive the relativistic hydrodynamics of the complex SFDM in\nthe framework of cosmological perturbation theory. Our formalism contains two\nnovel ingredients uniquely associated with the complex SFDM model: the Eckart\nframe defined by the conserved Noether current, and the stiff gauge condition,\n$c_s^2\\equiv (\\delta P\/\\delta\\rho)|_s=1$. In the Eckart frame, the complex SFDM\nis effectively an imperfect fluid with a dissipative energy flux,\ndistinguishing itself from axion dark matter. The energy flux can affect the\ngrowth of density fluctuations dynamically. Meanwhile, we apply the stiff gauge\ncondition to find new constitutive equations for the complex SFDM. We revisit\nthe homogeneous evolution of the complex SFDM and present illustrative\nearly-stage solutions for perturbations of the complex SFDM in a simplified\nsetting. We demonstrate the effects of varying the model parameters on the\nevolution of the perturbation variables.",
        "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
        "This paper is devoted to study back-reaction effects from matter accretion\nonto a cylindrically symmetric black hole using a perturbative scheme, focusing\non cases where accretion reaches a quasi-steady state. We examine three\ndistinct models by deriving corrections to the metric coefficients and\nobtaining expressions for the mass function. We analyze energy conditions, the\nself-consistency of the corrected solution and present formulas for the\ncorrected apparent horizon and discussed thermodynamic properties. Our results\nalign with the Vaidya form near the apparent horizon, regardless of the\nenergy-momentum tensor's form. Furthermore, we show that for a charged\ncylindrically symmetric black hole, the corrected mass term resembles that of\nthe static case, indicating that charge does not alter the corrected metric\nform in this perturbative approach.",
        "Recent advancements in large language models (LLMs) have significantly\nenhanced the fluency and logical coherence of image captioning.\nRetrieval-Augmented Generation (RAG) is widely adopted to incorporate external\nknowledge into LLMs; however, existing RAG-based methods rely on separate\nretrieval banks, introducing computational overhead and limiting the\nutilization of LLMs' inherent zero-shot capabilities. To address these\nlimitations, we propose TPCap, a novel trigger-augmented and multi-modal\npurification framework for zero-shot image captioning without external\nretrieval libraries. TPCap consists of two key components: trigger-augmented\n(TA) generation and multi-modal purification (MP). The TA module employs a\ntrigger projector with frozen and learnable projections to activate LLMs'\ncontextual reasoning, enhance visual-textual alignment, and mitigate data bias.\nThe MP module further refines the generated entity-related information by\nfiltering noise and enhancing feature quality, ensuring more precise and\nfactually consistent captions. We evaluate TPCap on COCO, NoCaps, Flickr30k,\nand WHOOPS datasets. With only 0.82M trainable parameters and training on a\nsingle NVIDIA RTX 4090 GPU, TPCap achieves competitive performance comparable\nto state-of-the-art models.",
        "We study decaying turbulence in the 1D Burgers equation (Burgulence) and 3D\nNavier-Stokes (NS) turbulence. We first investigate the decay in time $t$ of\nthe energy $E(t)$ in Burgulence, for a fractional Brownian initial potential,\nwith Hurst exponent $H$, and demonstrate rigorously a self-similar time-decay\nof $E(t)$, previously determined heuristically. This is a consequence of the\nnontrivial boundedness of the energy for any positive time. We define a\nspatially forgetful \\textit{oblivious fractional Brownian motion} (OFBM), with\nHurst exponent $H$, and prove that Burgulence, with an OFBM as initial\npotential $\\varphi_0(x)$, is not only intermittent, but it also displays, a\nhitherto unanticipated, large-scale bifractality or multifractality; the latter\noccurs if we combine OFBMs, with different values of $H$. This is the first\nrigorous proof of genuine multifractality for turbulence in a nonlinear\nhydrodynamical partial differential equation. We then present direct numerical\nsimulations (DNSs) of freely decaying turbulence, capturing some aspects of\nthis multifractality. For Burgulence, we investigate such decay for two cases:\n(A) $\\varphi_0(x)$ a multifractal random walk that crosses over to a fractional\nBrownian motion beyond a crossover scale $\\mathcal{L}$, tuned to go from small-\nto large-scale multifractality; (B) initial energy spectra $E_0(k)$, with\nwavenumber $k$, having one or more power-law regions, which lead, respectively,\nto self-similar and non-self-similar energy decay. Our analogous DNSs of the 3D\nNS equations also uncover self-similar and non-self-similar energy decay.\nChallenges confronting the detection of genuine large-scale multifractality, in\nnumerical and experimental studies of NS and MHD turbulence, are highlighted.",
        "These notes present elementary introduction to tractors based on classical\nexamples, together with glimpses towards modern invariant differential calculus\nrelated to vast class of Cartan geometries, the so called parabolic geometries.",
        "Recently, GeSn alloys have attracted much interest for direct-gap infrared\nphotonics and as potential topological materials which are compatible with the\nsemiconductor industry. However, for photonics, the high-Sn content required\nleads to low detectivity, associated with poor material quality, and the (>35%)\nSn required for topological properties have been out of reach experimentally.\nHere, we demonstrate that by patterning the Sn distribution within Ge, the\nelectronic properties have a far greater tunability than is possible with the\nrandom alloy. For the GeSn \\delta-digital alloy (DA) formed by confining Sn\natoms in atomic layer(s) along the [111] direction of Ge, we show that ~10% Sn\ncan lead to a triple-point semimetal. These findings are understood in terms of\nSn ordering causing spatial separation of Sn and Ge band edges, leading to band\ninversion. This mechanism can also lead to a weak topological insulator, Weyl\nsemimetal, and enables tunable direct bandgaps down to 2 meV, covering the\nentire infrared range. Our findings are generally applicable to other\nsemiconductors DAs and point to a new class of currently unexplored topological\nsystems accessible by epitaxy and establish the promise of low-Sn GeSn DAs for\napplication as infrared laser diodes and photodetectors in Si photonic\nintegrated circuits and infrared image sensors.",
        "Much of the success of multi-agent debates depends on carefully choosing the\nright parameters. Among them, the decision-making protocol stands out.\nSystematic comparison of decision protocols is difficult because studies alter\nmultiple discussion parameters beyond the protocol. So far, it has been largely\nunknown how decision-making addresses the challenges of different tasks. This\nwork systematically evaluates the impact of seven decision protocols (e.g.,\nmajority voting, unanimity consensus). We change only one variable at a time\n(i.e., decision protocol) to analyze how different methods affect the\ncollaboration between agents and test different protocols on knowledge (MMLU,\nMMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our\nresults show that voting protocols improve performance by 13.2% in reasoning\ntasks and consensus protocols by 2.8% in knowledge tasks over the other\ndecision protocol. Increasing the number of agents improves performance, while\nmore discussion rounds before voting reduces it. To improve decision-making by\nincreasing answer diversity, we propose two new methods, All-Agents Drafting\n(AAD) and Collective Improvement (CI). Our methods improve task performance by\nup to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the\nimportance of decision-making in multi-agent debates beyond scaling.",
        "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts.",
        "We prove that, for all $k \\ge 3,$ and any integers $\\Delta, n$ with $n \\ge\n\\Delta,$ there exists a $k$-uniform hypergraph on $n$ vertices with maximum\ndegree at most $\\Delta$ whose $4$-color Ramsey number is at least\n$\\mathrm{tw}_k(c_k \\sqrt{\\Delta}) \\cdot n$, for some constant $c_k > 0$, where\n$\\mathrm{tw}_k$ denotes the tower function. This is tight up to the power of\n$\\Delta$ on top of the tower and extends a result of Graham, R\\\"{o}dl and\nRuci\\'{n}ski for graphs."
      ]
    }
  },
  {
    "id":2412.02695,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Deep Residual Learning for Image Recognition",
    "start_abstract":"Deeper neural networks are more difficult to train. We present a residual learning framework ease the training of that substantially deeper than those used previously. explicitly reformulate layers as functions with reference layer inputs, instead unreferenced functions. provide comprehensive empirical evidence showing these easier optimize, and can gain accuracy from considerably increased depth. On ImageNet dataset we evaluate nets depth up 152 - 8\u00d7 VGG [40] but still having lower complexity. An ensemble achieves 3.57% error on test set. This result won 1st place ILSVRC 2015 classification task. also analysis CIFAR-10 100 1000 layers. The representations is central importance for many visual recognition tasks. Solely due our extremely deep representations, obtain 28% relative improvement COCO object detection dataset. Deep foundations submissions & competitions1, where places tasks detection, localization, segmentation.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "EEG data for ADHD \/ Control children"
      ],
      "abstract":[
        "Participants were 61 children with ADHD and 60 healthy controls (boys and girls, ages 7-12). The ADHD children were diagnosed by an experienced psychiatrist to DSM-IV criteria, and have taken Ritalin for up to 6 months. None of the children in the control group had a history of psychiatric disorders, epilepsy, or any report of high-risk behaviors. EEG recording was performed based on 10-20 standard by 19 channels (Fz, Cz, Pz, C3, T3, C4, T4, Fp1, Fp2, F3, F4, F7, F8, P3, P4, T5, T6, O1, O2) at 128 Hz sampling frequency. The A1 and A2 electrodes were the references located on earlobes. Since one of the deficits in ADHD children is visual attention, the EEG recording protocol was based on visual attention tasks. In the task, a set of pictures of cartoon characters was shown to the children and they were asked to count the characters. The number of characters in each image was randomly selected between 5 and 16, and the size of the pictures was large enough to be easily visible and countable by children. To have a continuous stimulus during the signal recording, each image was displayed immediately and uninterrupted after the child\u2019s response. Thus, the duration of EEG recording throughout this cognitive visual task was dependent on the child\u2019s performance (i.e. response speed)."
      ],
      "categories":[
        "Neurotherapeutics"
      ]
    },
    "list":{
      "title":[
        "Approximate roots",
        "Testing the bloated star hypothesis in the massive young stellar object\n  IRAS 19520+2759 through optical and infrared variability",
        "Scalar field source Teleparallel Robertson-Walker F(T)-gravity solutions",
        "Deep Chandra observations of PLCKG287.0+32.9: a clear detection of a\n  shock front in a heated former cool core",
        "Nonarchimedean and motivic stationary phase formulas",
        "Relighting the fire in Hickson Compact Group (HCG) 15: magnetised fossil\n  plasma revealed by the SKA Pathfinders & Precursors",
        "Stability and convergence of relaxed scalar auxiliary variable schemes\n  for Cahn-Hilliard systems with bounded mass source",
        "Upper tail large deviations for Brownian motions with one-sided\n  collisions",
        "PEPSI Investigation, Retrieval, and Atlas of Numerous Giant Atmospheres\n  (PIRANGA). II. Phase-Resolved Cross-Correlation Transmission Spectroscopy of\n  KELT-20b",
        "Magnon-mediated superconductivity on ferromagnetic wallpaper fermions",
        "Towards understanding the bias in decision trees",
        "Signal-to-noise ratio aware minimax analysis of sparse linear regression",
        "Vector Laplacian in Spherical Coordinates: An Unnoticed Typo in Landau\n  and Lifshitz's Fluid Mechanics Course",
        "Classification of Self-Dual Constacyclic Codes of Prime Power Length\n  $p^s$ Over $\\frac{\\mathbb{F}_{p^m}[u]}{\\left\\langle u^3\\right\\rangle} $",
        "Towards a Study of Low Energy Antiproton Annihilations on Nuclei",
        "New $q$-identities Via $q$-Derivative of Basic Hypergeometric Series\n  with Respect to Parameters",
        "Fast Jet Finding in Julia",
        "Gravitational Effects of a Small Primordial Black Hole Passing Through\n  the Human Body",
        "Estimating Black Hole Masses in Obscured AGN from X-ray and Optical\n  Emission Line Luminosities",
        "Analysis of the Effect of Bars on Environmental Dependence of Disc\n  Galaxies with MaNGA Survey Data",
        "Optimized quantum entanglement network enabled by a state-multiplexing\n  quantum light source",
        "Quantum metrology of a structured reservoir",
        "High Contrast Nulling in Photonic Meshes Through Architectural\n  Redundancy",
        "Identifying Majorana edge and end modes in Josephson junction of\n  $p$-wave superconductor with magnetic barrier",
        "Nonequilibrium mean-field approach for quantum transport with\n  off-diagonal disorder",
        "Towards a complexity-theoretic dichotomy for TQFT invariants",
        "Probing $D_s^*$-meson longitudinal twist-2 LCDA",
        "On the optimal $L_p$-$L_4$ Khintchine inequality",
        "First-Principles and Machine Learning Insights into the Design of\n  DOTT-Carbon and its Lithium-Ion Storage Capacity"
      ],
      "abstract":[
        "Given an integral domain $A$, a monic polynomial $P$ of degree $n$ with\ncoefficients in $A$ and a divisor $p$ of $n$, invertible in $A$, there is a\nunique monic polynomial $Q$ such that the degree of $P-Q^{p}$ is minimal for\nvarying $Q$. This $Q$, whose $p$-th power best approximates $P$, is called the\n$p$-th approximate root of $P$. If $f \\in \\mathbf{C}[[X]][Y]$ is irreducible,\nthere is a sequence of characteristic approximate roots of $f$, whose orders\nare given by the singularity structure of $f$. This sequence gives important\ninformation about this singularity structure. We study its properties in this\nspirit and we show that most of them hold for the more general concept of\nsemiroot. We show then how this local study adapts to give a proof of\nAbhyankar-Moh's embedding line theorem.",
        "Using optical time series with Telescopi Joan Or\\'o (TJO), Gaia, TESS, and\nNEOWISE archival data, we performed a variability study on the candidate\nbloated massive young stellar object (MYSO) IRAS 19520+2759. This is the first\ntime that a bloated star candidate has been tested for the theoretically\npredicted periodic variability. The source is found to be variable at optical\nand mid-infrared wavelengths and classified as a long-period variable MYSO. The\nobserved TJO data gives a period of the source of $\\sim$ 270$\\pm$40 days (in\nthe Rc band) and $\\sim$ 270$\\pm$50 days (in the Ic band), which is very close\nto the value predicted by the theoretical Period-Luminosity relation for a\nbloated young star of $\\sim 10^5 L\\odot$. Additionally, a large period of\n$\\sim$ 460$\\pm$80 days (in the G band) and $\\sim$ 440$\\pm$70 (in the Rp band)\nis also visible in the Gaia light curve. The physical parameters of the source,\nsuch as mass, radius, and accretion rate, based on the theoretical predictions\nfor the spherical accretion case and corresponding to a period of 270--460\ndays, are $\\sim 24$--28$\\,M\\odot$, $\\sim 650$--900$\\,R\\odot$ and $\\sim\n(6$--$9)\\times10^{-3}\\,M\\odot yr^{-1}$. However, these numbers are very\nsensitive to the effective temperatures assumed in the models. Additionally,\nthese values strongly depend on the geometry of accretion and could\nsignificantly decrease for the case of a MYSO accreting through a disc. The\nobserved periodic variability, the observed colour trend, and the nature of the\nvariability are found to be consistent with the pulsational model for a bloated\nMYSO.",
        "This paper investigates the teleparallel Robertson--Walker (TRW) $F(T)$\ngravity solutions for a scalar field source. We use the TRW $F(T)$ gravity\nfield equations (FEs) for each $k$-parameter value case added by a scalar field\nto find new teleparallel $F(T)$ solutions. For $k=0$, we find an\neasy-to-compute $F(T)$ solution formula applicable for any scalar field source.\nThen, we obtain, for $k=-1$ and $+1$ situations, some new analytical $F(T)$\nsolutions, only for specific $n$-parameter values and well-determined scalar\nfield cases. We can find by those computations a large number of analytical\nteleparallel $F(T)$ solutions independent of any scalar potential $V(\\phi)$\nexpression. The $V(\\phi)$ independence makes the FE solving and computations\neasier. The new solutions will be relevant for future cosmological applications\nin dark matter, dark energy (DE) quintessence, phantom energy and quintom\nmodels of physical processes.",
        "The massive, hot galaxy cluster PSZ2 G286.98+32.90 (hereafter PLCKG287,\nz=0.383) hosts a giant radio halo and two prominent radio relics which are\nsigns of a disturbed dynamical state. However, despite optical and radio\nobservations indicate a clear multiple merger, the X-ray emission of the\ncluster, derived from XMM-Newton observations, shows only moderate disturbance.\nWe present new 200 ks Chandra observations of PLCKG287. We detect a shock front\nto the NW direction at a distance of ~390 kpc from the X-ray peak,\ncharacterized by a Mach number M~1.3, as well as a cold front at a distance of\n~300 kpc from the X-ray peak, nested in the same direction of the shock in a\ntypical configuration expected by a merger. We also find evidence for X-ray\ndepressions to the E and W, that could be the signature of feedback from the\nactive galactic nucleus (AGN). The radial profile of the thermodynamic\nquantities show a temperature and abundance peak in the cluster center, where\nalso the pressure and entropy have a rapid increase. Based on these properties,\nwe argue that PLCKG287 is what remains of a cool core after a heating event. We\nestimate that both the shock energy and the AGN feedback energy, implied by the\nanalysis of the X-ray cavities, are sufficient to heat the core to the observed\ntemperature of ~17 keV in the central ~160 kpc. We discuss the possible origin\nof the detected shock by investigating alternative scenarios of merger and AGN\noutburst, finding that they are both energetically viable. However, no single\nmodel seems able to explain all the X-ray features detected in this system.\nThis suggests that the combined action of merger and central AGN feedback is\nlikely necessary to explain the reheated cool core, the large-scale shock and\nthe cold front. The synergy of these two processes may act in shaping the\ndistribution of cool core and non cool core clusters. [Abridged]",
        "In this article, for a non degenerate singular phase, we reconsider a\nstationary phase formula of Heifetz in the nonarchimedean local field setting\nand give a motivic analogue using Cluckers-Loeser's motivic integration.",
        "In the context of the life cycle and evolution of active galactic nuclei\n(AGN), the environment plays an important role. In particular, the over-dense\nenvironments of galaxy groups, where dynamical interactions and bulk motions\nhave significant impact, offer an excellent but under-explored window into the\nlife cycles of AGN and the processes that shape the evolution of relativistic\nplasma. Pilot Survey observations with the Australian Square Kilometre Array\nPathfinder (ASKAP) Evolutionary Map of the Universe (EMU) survey recovered\ndiffuse emission associated with the nearby (z = 0.0228) galaxy group HCG15,\nwhich was revealed to be strongly linearly polarised. We study the properties\nof this emission in unprecedented detail to settle open questions about its\nnature and its relation to the group-member galaxies. We perform a\nmulti-frequency spectropolarimetric study of HCG15 incorporating our ASKAP EMU\nobservations as well as new data from MeerKAT, LOFAR, the GMRT, and the Karl G.\nJansky Very Large Array (VLA), plus X-ray data from XMM-Newton and optical\nspectra from the Himalayan Chandra Telescope (HCT). Our study confirms that the\ndiffuse structure represents remnant emission from historic AGN activity,\nlikely associated with HCG15-D, some 80-86 Myr ago (based on ageing analysis).\nWe detect significant highly linearly-polarised emission from a diffuse\n'ridge'-like structure with a highly ordered magnetic field. Our analysis\nsuggests that this emission is generated by draping of magnetic field lines in\nthe intra-group medium (IGrM), although further exploration with simulations\nwould aid our understanding. We confirm that HCG15-C is a group-member galaxy.\nFinally, we report the detection of thermal emission associated with a\nbackground cluster at redshift z ~ 0.87 projected onto the IGrM of HCG15, which\nmatches the position and redshift of the recent SZ detection of ACT-CL\nJ0207.8+0209.",
        "The scalar auxiliary variable (SAV) approach of Shen et al. (2018), which\npresents a novel way to discretize a large class of gradient flows, has been\nextended and improved by many authors for general dissipative systems. In this\nwork we consider a Cahn-Hilliard system with mass source that, for image\nprocessing and biological applications, may not admit a dissipative structure\ninvolving the Ginzburg-Landau energy. Hence, compared to previous works, the\nstability of SAV-discrete solutions for such systems is not immediate. We\nestablish, with a bounded mass source, stability and convergence of time\ndiscrete solutions for a first-order relaxed SAV scheme in the sense of Jiang\net al. (2022), and apply our ideas to Cahn-Hilliard systems appearing in\ndiblock co-polymer phase separation, tumor growth, image inpainting and\nsegmentation.",
        "The system of interacting Brownian motions, where a particle is reflected\nasymmetrically from its left neighbor, belongs to the KPZ universality class,\nwith multi-point asymptotics having been derived in previous works. In this\npaper we show upper tail large deviation principles for all three fundamental\ninitial conditions, including explicit calculation of the rate function. For\nthe periodic case the Lambert-W function, which is already present in the\nFredholm determinant formula, also appears in the rate function.",
        "KELT-20b is a well-studied exoplanet within the highly observable ultra hot\nJupiter (UHJ) regime, yet its multidimensional atmospheric structure remains\nlargely unconstrained. Recent advances in instrumentation and increased general\ncirculation model (GCM) complexity have enabled observers to resolve the\nimprints of more intricate physical mechanisms in time-resolved data. We\nperformed high-resolution cross-correlation transmission spectroscopy (HRCCTS)\non a single transit time series of KELT-20b, observed with PEPSI on the LBT. We\ndetect Fe I $(11.9\\sigma)$ and Fe II $(23.7\\sigma)$ and tentatively detect Na I\n$(3.4\\sigma)$ and Cr I $(3.3\\sigma)$ upon combining nineteen in-transit\nexposures. The full-transit velocity offsets of the strongest absorbers are\n$\\Delta V_{\\text{Fe I}} = -1.0 \\pm 0.7$ km s$^{-1}$ and $\\Delta V_{\\text{Fe\nII}}= 0.0\\pm 0.5$ km s$^{-1}$. These results are discrepant with other HRCCTS\nstudies of KELT-20b, but these studies are largely inconsistent with each other\ndue to multiple possible factors, including self-inconsistent treatment of\nsystemic velocity. In response, we establish a stringent set of detection\ncriteria to promote reproducibility in future works. We present phase-resolved\nabsorption traces of species with high detection significance. Fe I and Fe II's\nabsorption traces are both redshifted at ingress, cross to a blueshift after\nmid-transit, then settle at a net blueshift at egress; concurrently, SNR\nincreases with phase. This behavior is consistent with absorption traces\npost-processed from an active magnetic drag GCM for another UHJ. However, the\nnet blueshift during transit could also result from limb asymmetry drivers\nand\/or longitudinally variable wind structure. This ambiguity motivates future\ntheoretical work on KELT-20b.",
        "We study two-dimensional superconductivity mediated by magnetic fluctuations\nat the interface between a ferromagnetic insulator and the nonsymmorphic\ntopological crystalline insulator with a fourfold-degenerate Dirac point,\nwallpaper fermion. We demonstrate that BCS pairing with zero center-of-mass\nmomentum induces chiral $p$-wave superconductivity, and the Amperean pairing\nwith center-of-mass momentum $2k_{\\rm F}$ can give rise to a parity-mixed\nsuperconducting state. We find that the Amperean pairing exhibits a mixture of\n$s$-wave and $p$-wave components due to the multiband nature of the wallpaper\nfermion and the easy-axis anisotropy of the ferromagnetic insulator.\nAdditionally, we find that the stability of the superconducting state with BCS\nand the Amperean pairing is governed by the easy-axis anisotropy.",
        "There is a widespread and longstanding belief that machine learning models\nare biased towards the majority (or negative) class when learning from\nimbalanced data, leading them to neglect or ignore the minority (or positive)\nclass. In this study, we show that this belief is not necessarily correct for\ndecision trees, and that their bias can actually be in the opposite direction.\nMotivated by a recent simulation study that suggested that decision trees can\nbe biased towards the minority class, our paper aims to reconcile the conflict\nbetween that study and decades of other works. First, we critically evaluate\npast literature on this problem, finding that failing to consider the data\ngenerating process has led to incorrect conclusions about the bias in decision\ntrees. We then prove that, under specific conditions related to the predictors,\ndecision trees fit to purity and trained on a dataset with only one positive\ncase are biased towards the minority class. Finally, we demonstrate that splits\nin a decision tree are also biased when there is more than one positive case.\nOur findings have implications on the use of popular tree-based models, such as\nrandom forests.",
        "We consider parameter estimation under sparse linear regression -- an\nextensively studied problem in high-dimensional statistics and compressed\nsensing. While the minimax framework has been one of the most fundamental\napproaches for studying statistical optimality in this problem, we identify two\nimportant issues that the existing minimax analyses face: (i) The\nsignal-to-noise ratio appears to have no effect on the minimax optimality,\nwhile it shows a major impact in numerical simulations. (ii) Estimators such as\nbest subset selection and Lasso are shown to be minimax optimal, yet they\nexhibit significantly different performances in simulations. In this paper, we\ntackle the two issues by employing a minimax framework that accounts for\nvariations in the signal-to-noise ratio (SNR), termed the SNR-aware minimax\nframework. We adopt a delicate higher-order asymptotic analysis technique to\nobtain the SNR-aware minimax risk. Our theoretical findings determine three\ndistinct SNR regimes: low-SNR, medium-SNR, and high-SNR, wherein minimax\noptimal estimators exhibit markedly different behaviors. The new theory not\nonly offers much better elaborations for empirical results, but also brings new\ninsights to the estimation of sparse signals in noisy data.",
        "A previously unaccounted fundamental typo has been discovered in Course of\nTheoretical Physics, vol. 6, by Landau and Lifshitz Fluid Mechanics, 1987,\nPergamon, which corresponds to the same typo in the Russian original of this\nbook. This concerns the first of the Navier-Stokes equations in spherical\ncoordinates, which includes the radial component of the vector Laplacian,\nnamely, an extra square in the denominator of the sine. This error migrates to\nsecondary publications and can complicate theoretical studies related to the\napplication of the Navier-Stokes equations, which the author of this note has\nencountered first-hand. This typo is not present in An Introduction to Fluid\nDynamics by Batchelor. The present short article makes a detailed derivation of\nthe radial component of the vector Laplacian from general principles to show\nwhat the corresponding Navier-Stokes equation should look like, and that the\ntypo is indeed present.",
        "Let $\\mathbb{F}_{p^m}$ be a finite field of cardinality $p^m$, where $p$ is a\nprime number and $m$ is a positive integer. Self-dual constacyclic codes of\nlength \\( p^s \\) over \\( \\frac{\\mathbb{F}_{p^m}[u]}{\\langle u^3 \\rangle} \\)\nexist only when \\( p = 2 \\). In this work, we classify and enumerate all\nself-dual cyclic codes of length \\( 2^s \\) over \\(\n\\frac{\\mathbb{F}_{2^m}[u]}{\\langle u^3 \\rangle} \\), thereby completing the\nclassification and enumeration of self-dual constacyclic codes of length \\( p^s\n\\) over \\( \\frac{\\mathbb{F}_{p^m}[u]}{\\langle u^3 \\rangle} \\). Additionally, we\ncorrect and improve results from B. Kim and Y. Lee (2020) in\n\\cite{kim2020classification}.",
        "A study of antiproton annihilations at rest on thin solid targets is underway\nat the ASACUSA facility, which now features a dedicated beam line for slow\nextraction at 250 eV. The experiment will employ new technologies, such as the\nTimepix4 ASICs coupled to silicon sensors, to measure the total multiplicity,\nenergy, and angular distribution of various prongs produced in thin solid\ntargets. A detection system consisting of seven Timepix4, covering most of the\nsolid angle, is being constructed. A 3D annihilation vertex reconstruction\nalgorithm from particle tracks in the single-plane detectors has been developed\nusing Monte Carlo simulations. The measurements will enable a study of\npbar-nucleus interactions, their dependence on nucleus mass and branching\nratios. The results will be used to assess and potentially improve various\nsimulation models.",
        "In this paper, we use the effect of the $q$-differential and deformed\n$q$-exponential operators on basic hypergeometric series to find new\n$q$-identities from the $q$-Gauss sum, the $q$-Chu-Vandermonde's sum, and\nJackson's transformation formula.",
        "Jet reconstruction remains a critical task in the analysis of data from HEP\ncolliders. We describe in this paper a new, highly performant, Julia package\nfor jet reconstruction, JetReconstruction.jl, which integrates into the growing\necosystem of Julia packages for HEP. With this package users can run sequential\nreconstruction algorithms for jets. In particular, for LHC events, the\nAnti-${k}_\\text{T}$, Cambridge\/Aachen and Inclusive-${k}_\\text{T}$ algorithms\ncan be used. For FCCee studies the use of alternative algorithms such as the\nGeneralised ${k}_\\text{T}$ for $e^+e^-$ and Durham are also supported.\n  The performance of the core algorithms is better than Fastjet's C++\nimplementation, for typical LHC and FCCee events, thanks to the Julia\ncompiler's exploitation of single-instruction-multiple-data (SIMD), as well as\nergonomic compact data layouts.\n  The full reconstruction history is made available, allowing inclusive and\nexclusive jets to be retrieved. The package also provides the means to\nvisualise the reconstruction. Substructure algorithms have been added that\nallow advanced analysis techniques to be employed. The package can read event\ndata from EDM4hep files and reconstruct jets from these directly, opening the\ndoor to FCCee and other future collider studies in Julia.",
        "The gravitational effects of a primordial black hole (PBH) passing through\nthe human body are examined, with the goal of determining the minimum mass\nnecessary to produce significant injury or death. Two effects are examined: the\ndamage caused by a shock wave propagating outward from the black hole\ntrajectory, and the dissociation of brain cells from tidal forces produced by\nthe black hole on its passage through the human body. It is found that the\nformer is the dominant effect, with a cutoff mass for serious injury or death\nof approximately $M_{PBH} > 1.4 \\times 10^{17} {\\rm g}$. The number density of\nprimordial black holes with a mass above this cutoff is far too small to\nproduce any observable effects on the human population.",
        "We test a novel method for estimating black hole masses ($M_{\\rm BH}$) in\nobscured active galactic nuclei (AGN) that uses proxies to measure the\nfull-width half maximum of broad H$\\alpha$ (FWHM$_{\\rm bH\\alpha}$) and the\naccretion disk luminosity at 5100 Angstrom ($\\lambda L_{\\rm 5100 Angstrom}$).\nUsing a published correlation, we estimate FWHM$_{\\rm bH\\alpha}$ from the\nnarrow optical emission line ratio $L_{\\rm [O\\,III]}\/L_{\\rm nH\\beta}$. Using a\nsample of 99 local obscured AGN from the Swift-BAT AGN Spectroscopic Survey, we\nassess the agreement between estimating $\\lambda L_{\\rm 5100 Angstrom}$ from\nthe intrinsic 2-10 keV X-ray luminosity and from narrow optical emission lines.\nWe find a mean offset of $0.32 \\pm 0.68$ dex between these methods, which\npropagates to a factor of $\\sim$2 uncertainty when estimating $M_{\\rm BH}$\nusing a virial mass formula where $L_{\\rm [O\\,III]}\/L_{\\rm nH\\beta}$ serves as\na proxy of FWHM$_{\\rm bH\\alpha}$ ($M_{\\rm BH,[O\\,III]\/nH\\beta}$). We compare\n$M_{\\rm BH,[O\\,III]\/nH\\beta}$ with virial $M_{\\rm BH}$ measurements from broad\nPaschen emission lines. For the 14 (12) BASS AGN with broad Pa$\\alpha$\n(Pa$\\beta$) detections, we find $M_{\\rm BH,[O\\,III]\/nH\\beta}$ to be\nsystematically higher than $M_{\\rm BH,Pa\\alpha}$ ($M_{\\rm BH,Pa\\beta}$) by a\nfactor of 0.39 $\\pm$ 0.44 dex (0.48 $\\pm$ 0.51 dex). Since these offsets are\nwithin the scatter, more data are needed to assess whether $M_{\\rm\nBH,[O\\,III]\/nH\\beta}$ is biased high. For 151 BASS AGN with measured stellar\nvelocity dispersions ($\\sigma_{\\rm *}$), we find that the $\\sigma_{\\rm\n*}$-derived $M_{\\rm BH}$ agrees with $M_{\\rm BH,[O\\,III]\/nH\\beta}$ to within\n0.08 dex, albeit with wide scatter (0.74 dex). The method tested here can\nprovide estimates of $M_{\\rm BH}$ in thousands of obscured AGN in spectroscopic\nsurveys when other diagnostics are not available, though with an uncertainty of\n$\\sim$3-5.",
        "Bars are fundamental structures in disc galaxies, although their role in\ngalaxy evolution is still not fully known. This study investigates the effect\nof the presence of bars on the environmental dependence of disc galaxies'\nproperties using the volume-limited sample from Mapping Nearby Galaxies at APO\n(MaNGA) survey. The disc galaxies with and without bars samples were obtained\nusing the Galaxy Zoo 2 project then assigned into field and group sub-samples.\nThese sub-samples were used to compare the stellar mass, star formation rate,\n$g-r$ colour, concentration index and gas phase metallicity, and their\nrelationships between field and group environments. Then these are used to\ninvestigate if there is an existence of any difference between galaxies with\nand without bars. A one-to-one correspondence between field and group galaxies'\nproperties were observed, and a strong dependence on the environment for\nproperties of unbarred galaxies was observed when compared to barred. The\nstellar mass against star formation rate, $g-r$ colour against concentration\nindex and stellar mass against gas phase metallicity of unbarred galaxies\nstrongly depend on environment while for barred these relations weakly depend\non environment. The study concludes that bars in disc galaxies decrease the\ndependence of analysed properties and its relations on the environment.",
        "A fully connected quantum network with a wavelength division multiplexing\narchitecture plays an increasingly pivotal role in quantum information\ntechnology. With such architecture, an entanglement-based network has been\ndemonstrated in which an entangled photon-pair source distributes quantum\nentanglement resources to many users. Despite these remarkable advances, the\nscalability of the architecture could be constrained by the finite spectrum\nresource, where O(N^2)wavelength channels are needed to connect N users, thus\nimpeding further progress in real-world scenarios. Here, we propose an\noptimized scheme for the wavelength division multiplexing entanglement-based\nnetwork using a state-multiplexing quantum light source. With a dual-pump\nconfiguration, the feasibility of our approach is demonstrated by generating\nstate-multiplexing photon pairs at multiple wavelength channels with a silicon\nnitride microring resonator chip. In our demonstration, we establish a fully\nconnected graph between four users with six wavelength channels - saving half\nof which without sacrificing functionality and performance of the secure\ncommunication. A total asymptotic secure key rate of 1946.9 bps is obtained by\nperforming the BBM92 protocol with the distributed state. The network topology\nof our method has great potential for developing a scalable quantum network\nwith significantly minimized infrastructure requirements.",
        "Accurately characterizing the properties of structured reservoirs is a key\nchallenge in quantum systems and is of great importance for advances in quantum\nmetrology and sensing. In this work, we employ a two-level system (qubit) as a\nprobe, which is coupled to a structured reservoir consisting of an ancilla\nqubit and a Markovian environment modeled as a thermal bath. By exploiting\nnon-Markovian dynamics, we systematically investigate the effectiveness of\ndifferent interaction types between the probe and ancilla for estimating\ncritical parameters, including temperature, ancilla frequency, and system-bath\ncoupling strength. We quantify the precision of parameter estimation using\nquantum Fisher information (QFI) and analyze the system dynamics in both\ntransient and steady-state regimes. Our findings demonstrate that\nnon-Markovianity substantially enhances parameter estimation in the transient\nregime, with specific interactions facilitating sustained information backflow\nand yielding higher QFI values. However, the performance of these interactions\nis contingent on the parameter under estimation and the operational regime. For\ninstance, certain interactions become prominent in the transient regime but\nexhibit diminished utility in the steady state, whereas others maintain their\neffectiveness even at equilibrium. These results stress the importance of\njudiciously selecting interactions adapted to specific estimation objectives\nand operational regimes.",
        "We demonstrate a silicon photonic architecture comprised of Double\nMach-Zehnder Interferometers (DMZIs) designed for high-contrast photonic\napplications. This configuration significantly enhances the achievable\nextinction ratio of photonic integrated circuits (PICs), reaching levels\nexceeding 80 dB. By leveraging the tunable properties of DMZIs and implementing\na systematic configuration algorithm, the proposed mesh effectively compensates\nfor fabrication imperfections and mitigates non-idealities such as back\nreflections. Experimental validation on a silicon-on-insulator platform\ndemonstrates the potential of this approach for applications requiring high\ncontrast nulling such as astronomical sensing.",
        "We propose a theoretical model describing a Josephson junction featuring a\nmagnetically textured barrier within two-dimensional (2D) $p$-wave\nsuperconductor, considering both $p_x + p_y$ and $p_x + ip_y$ type pairing\nsymmetries. Our study reveals the influence of the magnetic barrier strength\nand its spatial periodicity on the system's topological properties, in terms of\nlocal density of states and Josephson current calculations. Notably, we\ndemonstrate that these parameters regulate the number of Majorana zero modes at\nthe junction in the topological regime. Our setup further allows for the\nidentification of three distinct topological phases, the differentiation\nbetween one-dimensional (1D) Majorana edge (either flat\/dispersive and arising\nfrom intrinsic 2D $p$-wave pairing) and localized Majorana end modes, and an\nanalysis of their hybridization through the Josephson current. In particular,\nthe Josephson current exhibits a discontinuous jump due to the edge modes and\npronounced hump in the $p_x + ip_y$ pairing case, directly linked to the\nhybridized Majorana modes. Moreover, our study opens a possible interesting\navenue to distinguish between 1D Majorana edge modes and zero-dimensional end\nmodes via Josephson current signatures.",
        "For the nanoscale structures, disorder scattering plays a vital role in the\ncarriers' transport, including electrons and high-frequency phonons. The\ncapability for effectively treating the disorders, including both diagonal and\noff-diagonal disorders, is indispensable for quantum transport simulation of\nrealistic device materials. In this work, we report a self-consistent\nnonequilibrium mean-field quantum transport approach, by combining the\nauxiliary coherent potential approximation (ACPA) and non-equilibrium Green's\nfunction method, for calculating the phonon transport through disordered\nmaterial structures with the force-constant disorders (including the\nAnderson-type disorder). The nonequilibrium vertex correction (NVC) is derived\nin an extended local degree of freedom to account for both the multiple\ndisorder scattering by force-constant disorder and the nonequilibrium quantum\nstatistics. We have tested ACPA-NVC method with the fluctuation-dissipation\ntheorem at the equilibrium and obtained very good agreement with supercell\ncalculations for the phonon transmission. To demonstrate the applicability, we\napply ACPA-NVC to calculate the thermal conductance for the disordered Ni\/Pt\ninterface, and important effects of force-constant disorder are revealed.\nACPA-NVC method provides an effective quantum transport approach for simulating\ndisordered nanoscale devices, and the generalization to simulate disordered\nnanoelectronic device is straightforward.",
        "We show that for any fixed $(2+1)$-dimensional TQFT over $\\mathbb{C}$ of\neither Turaev-Viro-Barrett-Westbury or Reshetikhin-Turaev type, the problem of\n(exactly) computing its invariants on closed 3-manifolds is either solvable in\npolynomial time, or else it is $\\#\\mathsf{P}$-hard to (exactly) contract\ncertain tensors that are built from the TQFT's fusion category. Our proof is an\napplication of a dichotomy result of Cai and Chen [J. ACM, 2017] concerning\nweighted constraint satisfaction problems over $\\mathbb{C}$. We leave for\nfuture work the issue of reinterpreting the conditions of Cai and Chen that\ndistinguish between the two cases (i.e. $\\#\\mathsf{P}$-hard tensor contractions\nvs. polynomial time invariants) in terms of fusion categories. We expect that\nwith more effort, our reduction can be improved so that one gets a dichotomy\ndirectly for TQFTs' invariants of 3-manifolds rather than more general tensors\nbuilt from the TQFT's fusion category.",
        "In this paper, we carry on an investigation of the semileptonic decays\n$B_s\\to D_s^*\\ell \\bar\\nu_{\\ell}$. Firstly, we derive the moments of the\n$D_s^*$-meson longitudinal leading-twist light-cone distribution amplitude\n(LCDA) based on QCD sum rules within background field theory framework.\nConsidering the contributions of the vacuum condensates up to dimension-six,\nits first ten non-zero $\\xi$-moments are given. Meanwhile, we construct the\n$D_s^*$-meson longitudinal leading-twist LCDA by using the light-cone harmonic\noscillator model. Then, using those moments, we fix the model parameters\n$\\alpha_{2;D_s^*}$ and $B_1^{2;D_s^*}$ by the least square method and apply\nthem to calculate $B_s \\to D_s^*$ transition form factors $A_1(q^2), A_2(q^2)$\nand $V(q^2)$ that are derived by using the QCD light-cone sum rules. At the\nlarge recoil region, we obtain $A_1(0) =0.632_{-0.135}^{+0.228}, A_2(0)\n=0.706_{-0.092}^{+0.109}$ and $V(0) =0.647_{-0.069}^{+0.076}$. Those form\nfactors are then extrapolated to the allowed whole physical $q^2$-region\nthrough the simplified series expansion. Finally, we obtain the branching\nfractions for the two decay channels $B_s\\to D_s^*\\ell\\bar\\nu_\\ell$, $\\it i.e.$\n${\\cal B}(B_s^0 \\to D_s^{*+}e^-\\bar\\nu_e)=(5.45_{-1.57}^{+2.15})\\times\n10^{-2}$, ${\\cal B}(B_s^0 \\to\nD_s^{*+}\\mu^-\\bar\\nu_\\mu)=(5.43_{-1.57}^{+2.14})\\times 10^{-2}$.",
        "We derive optimal dimension independent constants in the classical Khintchine\ninequality between the $p$th and fourth moment for $p\\ge 4$. As an application\nwe deduce stability estimates for the Khintchine inequality between the $p$th\nand second moment for $p \\geq 4$.",
        "Two-dimensional (2D) carbon-based materials are promising candidates for\ndeveloping more efficient green energy conversion and storage technologies.\nThis study presents a new 2D carbon allotrope, DOTT-Carbon, characterized by\nits distinctive and multi-ring structure featuring 12-, 8-, 4-, and 3-membered\nrings of carbon atoms. We explore its structural, mechanical, and lithium-ion\nstorage properties by employing density functional theory and machine learning\nsimulations. Phonon calculations confirm its structural stability and ab initio\nmolecular dynamics simulations demonstrate its thermal resilience at elevated\ntemperatures. The material exhibits anisotropic mechanical properties, with\nYoung's modulus values varying between 280-330 GPa. DOTT-Carbon displays a\nlithium-ion storage capacity of 446.28 mAh\/g, complemented by a low diffusion\nbarrier (0.2-0.9 eV) and a high diffusion coefficient ($ > 1.0 \\times 10^{-6}$\ncm$^{2}$\/s), possibly facilitating efficient lithium-ion transport. The stable\nopen circuit voltage of 0.28 V also indicates its suitability as an anode\nmaterial."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Language Models are Few-Shot Learners",
    "start_abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Long non-coding RNAs: definitions, functions, challenges and recommendations"
      ],
      "abstract":[
        "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Explicit polynomial bounds on Dehn functions of subgroups of hyperbolic\n  groups",
        "Spatiotemporal steering of non-diffracting wavepackets",
        "Characterising the Surface Resistance of Laser-Treated LHC Beam Screens\n  with the Shielded Pair Method",
        "Hodge theory and o-minimality at CIRM",
        "On uniqueness of free boundary minimal annuli in geodesic balls of\n  $\\mathbb{S}^3_+$ and $\\mathbb{H}^3$",
        "Averaging method for quasi-periodic response solutions",
        "Anisotropic Schottky-barrier-height in high-symmetry 2D WSe$_2$:\n  Momentum-space anisotropy",
        "Constraints on the Scale Parameter of Regular Black Hole in\n  Asymptotically Safe Gravity from Extreme Mass Ratio Inspirals",
        "The localization problem: an antinomy between measurability and causal\n  dynamics",
        "Non-Abelian interlayer coherent fractional quantum Hall states",
        "How the CME on 21 April 2023 Triggered the First Severe Geomagnetic\n  Storm of Solar Cycle 25",
        "SEW: A full-spectrum linear fitting with stellar population synthesis\n  method Based on \"Equivalent Widths spectrum\"",
        "Wafer-scale Integration of Single-Crystalline MoS$_2$ for Flexible\n  Electronics Enabled by Oxide Dry-transfer",
        "High-Quality Pulse Compression Using a Hybrid All-Bulk Multipass Cell\n  Scheme",
        "Constructing PDFs of spatially dependent fields using finite elements",
        "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
        "Uncertainty quantification and posterior sampling for network\n  reconstruction",
        "Circular Dichroism in Resonant Inelastic X-ray Scattering: Probing\n  Altermagnetic Domains in MnTe",
        "Transfer ABCD Matrix for Time-Varying Media and Time Crystals",
        "Keldysh pseudo-fermion functional renormalization group for quantum\n  magnetism",
        "Engineering excitonic metal-insulator transitions in ultra-thin doped\n  copper sulfides",
        "Adaptive Residual-Driven Newton Solver for Nonlinear Systems of\n  Equations",
        "Black holes in thermal bath live shorter: implications for primordial\n  black holes",
        "Community-centric modeling of citation dynamics explains collective\n  citation patterns in science, law, and patents",
        "Terrier: A Deep Learning Repeat Classifier",
        "Tensor-Based Binary Graph Encoding for Variational Quantum Classifiers",
        "Systematic Spurion Matching between Low Energy EFT and Chiral Lagrangian",
        "Open Sourcing GPTs: Economics of Open Sourcing Advanced AI Models",
        "The Dirichlet problem for the nonstationary Stokes system in a polygon"
      ],
      "abstract":[
        "In 1999 Brady constructed the first example of a non-hyperbolic finitely\npresented subgroup of a hyperbolic group by fibring a non-positively curved\ncube complex over the circle. We show that his example has Dehn function\nbounded above by $n^{96}$. This provides the first explicit polynomial upper\nbound on the Dehn function of a finitely presented non-hyperbolic subgroup of a\nhyperbolic group. We also determine the precise hyperbolicity constant for the\n$1$-skeleton of the universal cover of the cube complex in Brady's construction\nwith respect to the $4$-point condition for hyperbolicity.",
        "We study the dynamics of space-time non-diffracting wavepackets, commonly\nknown as light bullets, in a spatiotemporally varying medium. We show that by\nspatiotemporal refraction, a monochromatic focused beam can be converted to a\nlight bullet that propagates at a given velocity. By further designing the\nindex profile of the spatiotemporal boundary, the group velocity and the\npropagation direction of the light bullet can be engineered in a programmable\nway. All effects mentioned above cannot be achieved by spatial or temporal\nboundaries, and are only possible with spatiotemporal boundaries. These\nfindings provide unique ways to engineer the dynamics of electromagnetic\nwavepackets in space-time. Such wavepackets with engineered spacetime\ntrajectories may find potential applications in the spatiotemporal control of\nmaterial properties or particles, or for use as a way to emulate relativistic\nphysics in the laboratory.",
        "The presence of strong electron clouds in the quadrupole magnetic field\nregions of the Large Hadron Collider (LHC) leads to considerable heating that\nposes challenges for the cryogenic cooling system, and under certain conditions\nto proton beam quality deterioration. Research is being conducted on\nlaser-treated inner beam screen surfaces for the upgraded High-Luminosity LHC\nto mitigate this issue. Laser-induced surface structuring, a technique that\neffectively roughens surfaces, has been shown to reduce secondary electron\nemission; an essential factor in controlling electron cloud formation.\nConversely, the resulting surface roughening also alters the material's surface\nimpedance, potentially impacting beam stability and increasing beam-induced\nresistive wall heating. Different laser treatment patterns have been applied to\nLHC beam screens to estimate this potential impact and assessed for their\nmicrowave responses.",
        "We discuss the relationship between o-minimality and the so called\nZilber-Pink conjecture. Since the work of Pila and Zannier, algebraization\ntheorems in o-minimal geometry had profound impacts in Diophantine geometry\n(most notably on the study of special points in abelian and Shimura varieties).\nWe will first focus on functional transcendence, discussing various recent and\nspectacular Ax-Schanuel theorems, and the related geometric part of\nZilber-Pink. Armed with these tools, we will study the distribution of the\nHodge locus of an arbitrary variation of Hodge structures (the typical\/atypical\ndichotomy) and present some recent applications. We will conclude by describing\nthe algebraicity and quasiprojectivity of images of period maps.",
        "We consider $\\Sigma$ an embedded free boundary minimal annulus in a geodesic\nball in the round hemisphere $\\mathbb{S}^3_+$ or in the hyperbolic space\n$\\mathbb{H}^3$. Under the hypothesis of invariance due to an antipodal map on\nthe geodesic ball and using the fact that this surface satisfies the Steklov\nproblem with frequency, we prove that $\\Sigma$ is congruent to a critical\nrotational annulus.",
        "In this paper, we present an averaging method for obtaining quasi-periodic\nresponse solutions in perturbed real analytic quasi-periodic systems with\nDiophantine frequency vectors. Assuming that the averaged system possesses a\nnon-degenerate equilibrium and the eigenvalues of the linearized matrix are\npairwise distinct, we show that the original system admits a quasi-periodic\nresponse solution for the parameter belonging to a Cantorian set. The proof is\nbased on the KAM techniques, and this averaging method can be extended to the\nsecond-order systems. It is worth mentioning that our results do not require\nthe equilibrium point to be hyperbolic, which means that the eigenvalues of the\nlinearized matrix of the averaging system may be purely imaginary.",
        "It is usually supposed that only low-symmetry two-dimensional (2D) materials\nexhibit anisotropy, here we show that high-symmetry 2D semiconductors can show\nsignificant anisotropy in momentum space due to the band structure anisotropy\nin k-space. The basic reason is that different k-points in the Brillouin zone\nhave different symmetry. Using 2D semiconductor WSe$_2$ as the example, we\nconstruct lateral heterostructures with zigzag and armchair connections to 2D\nmetal NbSe$_2$, and the electronic structure and contact characteristics of\nthese two connections are analyzed. It is found that both connections exhibit\np-type Schottky barrier height (SBH) but the sizes of SBH are very different\n(of 0.03 eV and 0.50 eV), mainly because the band-edge energies of WSe$_2$ are\ndifferent along the two mutually perpendicular directions in momentum space.\nThere are two factors contributing to the SBH anisotropy: one is the different\ninterface structure and the other is the band edge anisotropy of the 2D\nsemiconductor WSe$_2$. Since the two interface structures give only a\ndifference in interface potential change by less than 0.1 eV, the SBH variation\nof ~0.47 eV is mainly from the band structure anisotropy in momentum-space. So,\nhigh-symmetry 2D materials may exhibit highly anisotropic electronic states in\nmomentum space and this affects the transport properties. Our current work\nextends the research field of 2D material anisotropy to 2D materials with high\nreal-space symmetry, thus greatly expands the candidate materials for\nanisotropic studies and provides new guidance for optimizing the performance of\n2D material devices via controlling transport directions.",
        "This paper evaluates the potential for constraining the quantum scale\nparameter $\\xi$ of regular black hole within the asymptotically safe gravity\nframework using gravitational waves from extreme mass ratio inspirals (EMRI).\nSince $\\xi$ cannot be precisely determined from first principles, observational\nconstraints become crucial. We employ the Augmented Analytical Kludge (AAK)\nmethod to calculate gravitational waveforms in the equatorial plane and\nsystematically analyze the influence of different $\\xi$ values on phase\nevolution. Comparison with the Schwarzschild case demonstrates that the\ncorrective effects of $\\xi$ accumulate in the phase over observation time,\nthereby providing distinguishable observational signatures. Through waveform\nmismatch analysis, our results indicate that the LISA detector can effectively\ndetect the presence of $\\xi$ at the $\\sim10^{-4}$ level for systems with a mass\nof $10^6M_\\odot$. Further assessment using the Fisher information matrix (FIM)\nconfirms a measurement precision of $\\Delta\\xi\\approx3.225\\times10^{-4}$, which\nsignificantly surpasses existing observational methods, providing quantitative\nobservational evidence for asymptotically safe quantum gravity theory in the\nstrong-field regime.",
        "The localization problem in relativistic quantum theory has persisted for\nmore than seven decades, yet it is largely unknown and continues to perplex\neven those well-versed in the subject. At the heart of this problem lies a\nfundamental conflict between localizability and relativistic causality, which\ncan also be construed as part of the broader dichotomy between measurement and\nunitary dynamics. This article provides a historical review of the localization\nproblem in one-particle relativistic quantum mechanics, clarifying some\npersistent misconceptions in the literature, and underscoring the antinomy\nbetween causal dynamics and localized observables.",
        "We study non-Abelian fractional quantum Hall state in double layer systems at\ntotal filling factor $1\/2$. Recent progresses in two-dimensional van der Waals\nmaterials made it possible to explore the regime with very small interlayer\ndistance. Numerical calculations suggests interlayer phase coherence can\ndevelop between the layers such that the electrons may redistribute between\nthem without changing the Hall response. It corresponds to spontaneous breaking\nof the U(1) symmetry associated with the particle number difference in the\nlayers. This state manifests itself as superfluid in counterflow measurement\nand has characteristic Hall response when current is passed through one layer\nand voltages in both layers are measured. As the interlayer distance increases,\na phase transition to the Halperin 331 state occurs. We also discuss similar\nphysics for bosonic systems with specially designed interactions.",
        "The first severe (G4) geomagnetic storm of Solar Cycle 25 occurred on 23-24\nApril 2023, following the arrival of a Coronal Mass Ejection (CME) on 23 April.\nThe characteristics of this CME, measured from coronagraphs (speed and mass),\ndid not indicate that it would trigger such an intense geomagnetic storm. In\nthis work, our aim is to understand why this CME led to such a geoeffective\noutcome. Our analysis spans from the source active region to the corona and\ninner heliosphere through 1 au using multiwavelength, multi-viewpoint remote\nsensing observations and in situ data. We find that rotation and possibly\ndeflection of the CME resulted in an axial magnetic field nearly parallel to\nthe ecliptic plane during the Earth encounter, which might explain the storm's\nseverity. Additionally, we find that imaging away from the Sun-Earth line is\ncrucial in hindcasting the CME Time-of-Arrival at Earth. The position (0.39 au)\nand detailed images from the SoloHI telescope onboard the Solar Orbiter\nmission, in combination with SOHO and STEREO images, helped decisively with the\nthree-dimensional (3D) reconstruction of the CME.",
        "We present a full-spectrum linear fitting method, SEW, for stellar population\nsynthesis based on equivalent widths (EWs) to extract galaxy properties from\nobserved spectra. This approach eliminates the need for prior assumptions about\ndust attenuation curves, which are instead derived as outputs of the fitting\nprocess. By leveraging the invariance of EWs and employing the Discrete\nPenalised Least Squares (DPLS) method to extract EWs, we address the nonlinear\naspects of the fitting process by linearising the matrix equations. This\nenables accurate recovery of key parameters, stellar age, metallicity and dust\nattenuation, even under systematic calibration biases and varying attenuation\nconditions. Rigorous testing with mock spectra across signal-to-noise ratios\n(S\/N = 5-30) and calibration biases demonstrates the robustness of method. The\nderived attenuation curves align closely with input models, and stellar\npopulation parameters are recovered with minimal bias. To facilitate adoption,\nwe implement this method as a Python extension package for \\texttt{pPXF}\n(\\texttt{pPXF-SEW}). Our work addresses critical degeneracies in traditional\nspectral fitting and enhances the reliability of extragalactic studies.",
        "Atomically thin, single-crystalline transition metal dichalcogenides (TMDCs)\ngrown via chemical vapor deposition (CVD) on sapphire substrates exhibit\nexceptional mechanical and electrical properties, positioning them as excellent\nchannel materials for flexible electronics. However, conventional wet-transfer\nprocesses for integrating these materials onto flexible substrates often\nintroduce surface contamination, significantly degrading device performance.\nHere, we present a wafer-scale dry-transfer technique using a high-dielectric\noxide as the transfer medium, enabling the integration of 4-inch\nsingle-crystalline MoS$_2$ onto flexible substrates. This method eliminates\ncontact with polymers or solvents, thus preserving the intrinsic electronic\nproperties of MoS$_2$. As a result, the fabricated flexible field-effect\ntransistor (FET) arrays exhibit remarkable performance, with a mobility of 117\ncm$^2$\/Vs, a subthreshold swing of 68.8 mV dec$^{-1}$, and an ultra-high\ncurrent on\/off ratio of $10^{12}$-values comparable to those achieved on rigid\nsubstrates. Leveraging the outstanding electrical characteristics, we\ndemonstrated MoS$_2$-based flexible inverters operating in the subthreshold\nregime, achieving both a high gain of 218 and ultra-low power consumption of\n1.4 pW\/$\\mu$m. Additionally, we integrated a flexible tactile sensing system\ndriven by active-matrix MoS$_2$ FET arrays onto a robotic gripper, enabling\nreal-time object identification. These findings demonstrate the simultaneous\nachievement of high electrical performance and flexibility, highlighting the\nimmense potential of single-crystalline TMDC-based flexible electronics for\nreal-world applications.",
        "We present a detailed numerical study of ultrashort pulse compression using a\nthree-stage hybrid all-bulk multipass cell scheme. By operating in the enhanced\nfrequency chirp regime, we achieve the compression of pulses from around 180 fs\nto 4 fs pulse duration (a total compression factor above 45), with side lobes\ncontributing with intensity values lower than 0.2 % of the peak intensity.\nOptimal conditions for the enhanced frequency chirp regime propagation have\nbeen identified, enabling smooth spectral broadening and high-quality temporal\nprofiles. The first two stages are based on bulk multipass cells to achieve a\ncontrolled spectral broadening, while the third stage consists of a thin plate\nto reach the spectral broadening needed for few cycle pulses without leaving\nthe enhanced frequency chirp regime.",
        "A probability density function (PDF) of a spatially dependent field provides\na means of calculating moments of the field or, equivalently, the proportion of\na spatial domain that is mapped to a given set of values. This paper describes\na finite element approach to estimating the PDF of a spatially dependent field\nand its numerical implementation in the Python package NumDF.",
        "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
        "Network reconstruction is the task of inferring the unseen interactions\nbetween elements of a system, based only on their behavior or dynamics. This\ninverse problem is in general ill-posed, and admits many solutions for the same\nobservation. Nevertheless, the vast majority of statistical methods proposed\nfor this task -- formulated as the inference of a graphical generative model --\ncan only produce a ``point estimate,'' i.e. a single network considered the\nmost likely. In general, this can give only a limited characterization of the\nreconstruction, since uncertainties and competing answers cannot be conveyed,\neven if their probabilities are comparable, while being structurally different.\nIn this work we present an efficient MCMC algorithm for sampling from posterior\ndistributions of reconstructed networks, which is able to reveal the full\npopulation of answers for a given reconstruction problem, weighted according to\ntheir plausibilities. Our algorithm is general, since it does not rely on\nspecific properties of particular generative models, and is specially suited\nfor the inference of large and sparse networks, since in this case an iteration\ncan be performed in time $O(N\\log^2 N)$ for a network of $N$ nodes, instead of\n$O(N^2)$, as would be the case for a more naive approach. We demonstrate the\nsuitability of our method in providing uncertainties and consensus of solutions\n(which provably increases the reconstruction accuracy) in a variety of\nsynthetic and empirical cases.",
        "X-ray magnetic circular dichroism provides a means to identify ferromagnetic,\nchiral, and altermagnetic orders via their time-reversal-symmetry\n($\\mathcal{T}$) breaking. However, differentiating magnetic domains related by\ncrystallographic symmetries remains a technical challenge. Here we reveal a\ncircular dichroism (CD) in the resonant inelastic x-ray scattering (RIXS)\nspectra from the altermagnetic MnTe. The azimuthal dependence of the RIXS-CD\nintensity of the magnon excitations indicates a dominant occupation of a single\naltermagnetic domain. The RIXS-CD in our scattering geometry is ascribed to the\nmirror-symmetry breaking associated with the $\\mathcal{T}$-broken altermagnetic\norder. Our results establish RIXS-CD as a domain-sensitive probe of elementary\nexcitations in quantum materials.",
        "This paper introduces a formal definition of the transfer ABCD parameters in\ntime-varying electromagnetic systems. The formal definition comes after the\nrearrangement of the fields $D$ and $B$ at the inputs and outputs of the\ntemporal system based on the time-varying boundary conditions. Then, we derive\nthe ABCD parameters of a temporal transmission line, i.e., a temporal slab, and\ncompute the associated scattering parameters (reflection and transmission\ncoefficients). The results presented here open up an alternative way, based on\nnetwork theory, to analyze multilayer temporal configurations. Moreover, we\nshow that the ABCD parameters can be used to compute the dispersion diagram\n($\\omega$ vs $k$) of time crystals.",
        "The functional renormalization group (FRG) approach for spin models relying\non a pseudo-fermionic description has proven to be a powerful technique in\nsimulating ground state properties of strongly frustrated magnetic lattices. A\ndrawback of the FRG framework is that it is formulated in the imaginary-time\nMatsubara formalism and thus only able to access static correlations, a\nlimitation shared with most other many-body approaches. A description of the\ndynamical properties of magnetic systems is the key to bridging the gap between\ntheory and neutron scattering spectra. We take the decisive step of expanding\nthe scope of pseudo-fermion FRG to the Keldysh formalism, which, while\noriginally developed to address non-equilibrium phenomena, enables a direct\ncalculation of the equilibrium dynamical spin structure factors on generic\nlattices in arbitrary dimension. We identify the principal features\ncharacterizing the low-energy spectra of exemplary zero-, one- and\ntwo-dimensional spin-$1\/2$ Heisenberg models as well as the Kitaev honeycomb\nmodel.",
        "Exciton condensation in the absence of optical excitation is proposed in\n1960s to occur in a semiconductor at low temperatures when the binding energy\nof excitons overcomes the band gap or in a semimetal with weakly screened\ncoulomb interaction, giving rise to an excitonic insulating state. However, it\nhas been challenging to establish experimental realization in a natural\nmaterial as the interacting electron-hole pockets rely on the band structures\nwhich are difficult to be delicately controlled. Here, we demonstrate an\nexcitonic insulating phase formed in ultra-thin copper sulfide films by\neffectively tuning the band structure via changing the composition of Cu and S\nin the system. Using angle-resolved photoemission spectroscopy (ARPES), we\nobserved a continuous band renormalization and opening of a full gap at low\ntemperatures over a wide range of doping. The electronic origin of the\nmetal-insulator transition is supported by scanning tunneling microscopy (STM)\nand low energy electron diffraction (LEED) measurements, which show no\nindication of superlattice modulation and lattice symmetry breaking. The\nevidence of excitonic insulator is further provided by carrier density\ndependent transitions, a combined effect of electron screening and Coulomb\ninteraction strength. Our findings demonstrate the tunability of the band\nstructure of copper sulfides, allowing for new opportunities to study exotic\nquantum phases.",
        "Newton-type solvers have been extensively employed for solving a variety of\nnonlinear system of algebraic equations. However, for some complex nonlinear\nsystem of algebraic equations, efficiently solving these systems remains a\nchallenging task. The primary reason for this challenge arises from the\nunbalanced nonlinearities within the nonlinear system. Therefore, accurately\nidentifying and balancing the unbalanced nonlinearities in the system is\nessential. In this work, we propose a residual-driven adaptive strategy to\nidentify and balance the nonlinearities in the system. The fundamental idea\nbehind this strategy is to assign an adaptive weight multiplier to each\ncomponent of the nonlinear system, with these weight multipliers increasing\naccording to a specific update rule as the residual components increase,\nthereby enabling the Newton-type solver to select a more appropriate step\nlength, ensuring that each component in the nonlinear system experiences\nsufficient reduction rather than competing against each other. More\nimportantly, our strategy yields negligible additional computational overhead\nand can be seamlessly integrated with other Newton-type solvers, contributing\nto the improvement of their efficiency and robustness. We test our algorithm on\na variety of benchmark problems, including a chemical equilibrium system, a\nconvective diffusion problem, and a series of challenging nonlinear systems.\nThe experimental results demonstrate that our algorithm not only outperforms\nexisting Newton-type solvers in terms of computational efficiency but also\nexhibits superior robustness, particularly in handling systems with highly\nimbalanced nonlinearities.",
        "Hawking radiation from a non-extremal black hole is known to be approximately\nPlanckian. The thermal spectrum receives multiple corrections including\ngreybody factors and due to kinematical restrictions on the infrared and\nultraviolet frequencies. We show that another significant correction to the\nspectrum arises if the black hole is assumed to live in a thermal bath and the\nemitted radiation gets thermalised at the bath temperature. This modification\nreshapes the thermal spectrum, and leads to appreciable deviation from standard\nresults including modification in the decay rate of black holes. We argue that\nthis altered decay rate has significance for cosmology and, in a realistic\nsetting, show that it alters the life time of primordial black holes (PBHs) in\nthe early universe. In particular, the very light PBHs formed right after the\nend of inflation decay faster which may have interesting phenomenological\nimplications.",
        "Many human knowledge systems, such as science, law, and invention, are built\non documents and the citations that link them. Citations, while serving\nmultiple purposes, primarily function as a way to explicitly document the use\nof prior work and thus have become central to the study of knowledge systems.\nAnalyzing citation dynamics has revealed statistical patterns that shed light\non knowledge production, recognition, and formalization, and has helped\nidentify key mechanisms driving these patterns. However, most quantitative\nfindings are confined to scientific citations, raising the question of\nuniversality of these findings. Moreover, existing models of individual\ncitation trajectories fail to explain phenomena such as delayed recognition,\ncalling for a unifying framework. Here, we analyze a newly available corpus of\nU.S. case law, in addition to scientific and patent citation networks, to show\nthat they share remarkably similar citation patterns, including a heavy-tailed\ndistribution of sleeping beauties. We propose a holistic model that captures\nthe three core mechanisms driving collective dynamics and replicates the\nelusive phenomenon of delayed recognition. We demonstrate that the model not\nonly replicates observed citation patterns, but also better predicts future\nsuccesses by considering the whole system. Our work offers insights into key\nmechanisms that govern large-scale patterns of collective human knowledge\nsystems and may provide generalizable perspectives on discovery and innovation\nacross domains.",
        "Repetitive DNA sequences underpin genome architecture and evolutionary\nprocesses, yet they remain challenging to classify accurately. Terrier is a\ndeep learning model designed to overcome these challenges by classifying\nrepetitive DNA sequences using a publicly available, curated repeat sequence\nlibrary trained under the RepeatMasker schema. Existing tools often struggle to\nclassify divergent taxa due to biases in reference libraries, limiting our\nunderstanding of repeat evolution and function. Terrier overcomes these\nchallenges by leveraging deep learning for improved accuracy. Trained on\nRepBase, which includes over 100,000 repeat families -- four times more than\nDfam -- Terrier maps 97.1% of RepBase sequences to RepeatMasker categories,\noffering the most comprehensive classification system available. When\nbenchmarked against DeepTE, TERL, and TEclass2 in model organisms (rice and\nfruit flies), Terrier achieved superior accuracy while classifying a broader\nrange of sequences. Further validation in non-model amphibian and flatworm\ngenomes highlights its effectiveness in improving classification in non-model\nspecies, facilitating research on repeat-driven evolution, genomic instability,\nand phenotypic variation.",
        "Quantum computing has been a prominent research area for decades, inspiring\ntransformative fields such as quantum simulation, quantum teleportation, and\nquantum machine learning (QML), which are undergoing rapid development. Within\nQML, hybrid classical-quantum algorithms like Quantum Neural Networks (QNNs)\nand Variational Quantum Classifiers (VQCs) have shown promise in leveraging\nquantum circuits and classical optimizers to classify classical data\nefficiently.Simultaneously, classical machine learning has made significant\nstrides in graph classification, employing Graph Neural Networks (GNNs) to\nanalyze systems ranging from large-scale structures like the Large Hadron\nCollider to molecular and biological systems like proteins and DNA. Combining\nthe advancements in quantum computing and graph classification presents a\nunique opportunity to develop quantum algorithms capable of extracting features\nfrom graphs and performing their classification effectively. In this paper, we\npropose a novel quantum encoding framework for graph classification using VQCs.\nUnlike existing approaches such as PCA-VQC, which rely on dimensionality\nreduction techniques like Principal Component Analysis (PCA) and may lead to\ninformation loss, our method preserves the integrity of graph data.\nFurthermore, our encoding approach is optimized for Noise-Intermediate Scale\nQuantum (NISQ) devices, requiring a limited number of qubits while achieving\ncomparable or superior classification performance to PCA-VQC. By constructing\nslightly more complex circuits tailored for graph encoding, we demonstrate that\nVQCs can effectively classify graphs within the constraints of current quantum\nhardware.",
        "The hadronic chiral Lagrangian can be matched from the low energy effective\nfield theory (LEFT) operators at the quark level. Traditionally, as the mass\ndimension of the LEFT operators increases, more and more external sources are\nnecessarily introduced in the chiral perturbation theory (ChPT). In this work,\nwe present a systematic matching procedure with the single spurion field\n$\\mathbf{T}$ of the flavor $SU(3)_V$ octet, without the need of external\nsources. We present the complete sets of the LEFT operators, which have been\nreformulated using the flavor $SU(3)_V$ symmetry up to dimension 9. At the same\ntime, the ChPT Lagrangian is also reformulated using the single spurion and\nlepton fields, instead of external sources. The spurion matching can be\nperformed between LEFT and ChPT operators with the same flavor $SU(3)_V$ and\n$CP$ structures, and the same leptonic currents, using the naive dimensional\nanalysis at the quark and hadronic levels.",
        "This paper explores the economic underpinnings of open sourcing advanced\nlarge language models (LLMs) by for-profit companies. Empirical analysis\nreveals that: (1) LLMs are compatible with R&D portfolios of numerous\ntechnologically differentiated firms; (2) open-sourcing likelihood decreases\nwith an LLM's performance edge over rivals, but increases for models from large\ntech companies; and (3) open-sourcing an advanced LLM led to an increase in\nresearch-related activities. Motivated by these findings, a theoretical\nframework is developed to examine factors influencing a profit-maximizing\nfirm's open-sourcing decision. The analysis frames this decision as a trade-off\nbetween accelerating technology growth and securing immediate financial\nreturns. A key prediction from the theoretical analysis is an inverted-U-shaped\nrelationship between the owner's size, measured by its share of LLM-compatible\napplications, and its propensity to open source the LLM. This finding suggests\nthat moderate market concentration may be beneficial to the open source\necosystems of multi-purpose software technologies.",
        "The author proves the existence of strong solutions of the Dirichlet problem\nfor the nonstationary Stokes system in polygonal domain. Here, the solutions\nare elements of weighted Sobolev spaces, where the weight function is a power\nof the distance from the corner points."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Long non-coding RNAs: definitions, functions, challenges and recommendations",
    "start_abstract":"Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Language Models are Few-Shot Learners"
      ],
      "abstract":[
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Inducing Diversity in Differentiable Search Indexing",
        "Identification of Genetic Factors Associated with Corpus Callosum\n  Morphology: Conditional Strong Independence Screening for Non-Euclidean\n  Responses",
        "Memory Efficient Transformer Adapter for Dense Predictions",
        "Probing growth precursor diffusion lengths by inter-surface diffusion",
        "The characterizations of hyperspaces and free topological groups with an\n  $\\omega^\\omega$-base",
        "AI Alignment at Your Discretion",
        "Unevolved Li-rich stars at low metallicity: a possible formation pathway\n  through novae",
        "AI Enabled User-Specific Cyberbullying Severity Detection with\n  Explainability",
        "Chip-to-chip photonic connectivity in multi-accelerator servers for ML",
        "Static spherically symmetric solutions of the f(R) gravity",
        "Large class of many-to-one mappings over quadratic extension of finite\n  fields",
        "The dynamical and thermodynamic effects of turbulence for the cosmic\n  baryonic fluid",
        "EmoXpt: Analyzing Emotional Variances in Human Comments and\n  LLM-Generated Responses",
        "Spiking World Model with Multi-Compartment Neurons for Model-based\n  Reinforcement Learning",
        "Quantum geometry in the dynamics of band-projected operators",
        "The role of Trees of Fragmenting Granules (TFG) in the formation of the\n  solar supergranular pattern from Hinode observations",
        "Magnetic Monopoles and Exotic States in $SU(4)_c \\times SU(2)_L \\times\n  SU(2)_R$",
        "Object-Centric Pretraining via Target Encoder Bootstrapping",
        "Nuclear Excitation and Control Induced by Intense Vortex Laser",
        "Condensate ground states of hardcore bosons induced by an array of\n  impurities",
        "RGBSQGrasp: Inferring Local Superquadric Primitives from Single RGB\n  Image for Graspability-Aware Bin Picking",
        "CommitShield: Tracking Vulnerability Introduction and Fix in Version\n  Control Systems",
        "Motion-Aware Generative Frame Interpolation",
        "STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural\n  Networks",
        "Explainability-Driven Quality Assessment for Rule-Based Systems",
        "The National Intangible Resources and their Importance in the Current\n  Knowledge-Based Economy",
        "Enhanced Multi-View Pedestrian Detection Using Probabilistic Occupancy\n  Volume",
        "SAM2Act: Integrating Visual Foundation Model with A Memory Architecture\n  for Robotic Manipulation",
        "Salvaging Forbidden Treasure in Medical Data: Utilizing Surrogate\n  Outcomes and Single Records for Rare Event Modeling"
      ],
      "abstract":[
        "Differentiable Search Indexing (DSI) is a recent paradigm for information\nretrieval which uses a transformer-based neural network architecture as the\ndocument index to simplify the retrieval process. A differentiable index has\nmany advantages enabling modifications, updates or extensions to the index. In\nthis work, we explore balancing relevance and novel information content\n(diversity) for training DSI systems inspired by Maximal Marginal Relevance\n(MMR), and show the benefits of our approach over the naive DSI training. We\npresent quantitative and qualitative evaluations of relevance and diversity\nmeasures obtained using our method on NQ320K and MSMARCO datasets in comparison\nto naive DSI. With our approach, it is possible to achieve diversity without\nany significant impact to relevance. Since we induce diversity while training\nDSI, the trained model has learned to diversify while being relevant. This\nobviates the need for a post-processing step to induce diversity in the recall\nset as typically performed using MMR. Our approach will be useful for\nInformation Retrieval problems where both relevance and diversity are important\nsuch as in sub-topic retrieval. Our work can also be easily be extended to the\nincremental DSI settings which would enable fast updates to the index while\nretrieving a diverse recall set.",
        "The corpus callosum, the largest white matter structure in the brain, plays a\ncritical role in interhemispheric communication. Variations in its morphology\nare associated with various neurological and psychological conditions, making\nit a key focus in neurogenetics. Age is known to influence the structure and\nmorphology of the corpus callosum significantly, complicating the\nidentification of specific genetic factors that contribute to its shape and\nsize. We propose a conditional strong independence screening method to address\nthese challenges for ultrahigh-dimensional predictors and non-Euclidean\nresponses. Our approach incorporates prior knowledge, such as age. It\nintroduces a novel concept of conditional metric dependence, quantifying\nnon-linear conditional dependencies among random objects in metric spaces\nwithout relying on predefined models. We apply this framework to identify\ngenetic factors associated with the morphology of the corpus callosum.\nSimulation results demonstrate the efficacy of this method across various\nnon-Euclidean data types, highlighting its potential to drive genetic discovery\nin neuroscience.",
        "While current Vision Transformer (ViT) adapter methods have shown promising\naccuracy, their inference speed is implicitly hindered by inefficient memory\naccess operations, e.g., standard normalization and frequent reshaping. In this\nwork, we propose META, a simple and fast ViT adapter that can improve the\nmodel's memory efficiency and decrease memory time consumption by reducing the\ninefficient memory access operations. Our method features a memory-efficient\nadapter block that enables the common sharing of layer normalization between\nthe self-attention and feed-forward network layers, thereby reducing the\nmodel's reliance on normalization operations. Within the proposed block, the\ncross-shaped self-attention is employed to reduce the model's frequent\nreshaping operations. Moreover, we augment the adapter block with a lightweight\nconvolutional branch that can enhance local inductive biases, particularly\nbeneficial for the dense prediction tasks, e.g., object detection, instance\nsegmentation, and semantic segmentation. The adapter block is finally\nformulated in a cascaded manner to compute diverse head features, thereby\nenriching the variety of feature representations. Empirically, extensive\nevaluations on multiple representative datasets validate that META\nsubstantially enhances the predicted quality, while achieving a new\nstate-of-the-art accuracy-efficiency trade-off. Theoretically, we demonstrate\nthat META exhibits superior generalization capability and stronger\nadaptability.",
        "Understanding and optimizing thin-film synthesis requires measuring the\ndiffusion length $d_\\alpha$ of adsorbed growth precursors. Despite\ntechnological advances, in-situ measurements of $d_\\alpha$ are often\nunachievable due to harsh deposition conditions, such as high temperatures or\nreactive environments. In this paper, we propose a fitting approach to\ndetermine $d_\\alpha$ from experimental data by leveraging inter-surface\ndiffusion between a substrate and a strip obtained by, for example, processing\na film. The substrate serves as a source or sink of precursors, influencing the\ngrowth dynamics and shaping the profile of the strip. By fitting simulated\nprofiles to given profiles, we demonstrate that $d_\\alpha$ can be determined.\nTo achieve this, we develop a theoretical growth model, a simulation strategy,\nand a fitting procedure. The growth model incorporates inter-surface diffusion,\nadsorption, and desorption of growth precursors, with growth being proportional\nto the concentration of adsorbed precursors. In our simulations, a chain of\nnodes represents a profile, and growth is captured by the displacement of those\nnodes, while keeping the node density approximately constant. For strips\nsignificantly wider than $d_\\alpha$, a scaled precursor concentration and\n$d_\\alpha$ are the fitting parameters that are determined by minimizing a\nsuitably defined measure of the distance between simulated and given profiles.\nWe evaluate the robustness of our procedure by analyzing the effect of profile\nresolution and noise on the fitted parameters. Our approach can offer valuable\ninsights into thin-film growth processes, such as those occurring during\nplasma-enhanced chemical vapor deposition.",
        "A topological space $(X, \\tau)$ is said to be have an {\\it\n$\\omega^\\omega$-base} if for each point $x\\in X$ there exists a neighborhood\nbase $\\{U_{\\alpha}[x]: \\alpha\\in\\omega^\\omega\\}$ such that $U_{\\beta}[x]\\subset\nU_{\\alpha}[x]$ for all $\\alpha\\leq\\beta$ in $\\omega^\\omega$. In this paper, the\ncharacterization of a space $X$ is given such that the free Abelian topological\ngroup $A(X)$, the hyperspace $CL(X)$ with the Vietoris topology and the\nhyperspace $CL(X)$ with the Fell topology have $\\omega^\\omega$-bases\nrespectively. The main results are listed as follows:\n  (1) For a Tychonoff space $X$, the free Abelian topological group $A(X)$ is a\n$k$-space with an $\\omega^\\omega$-base if and only if $X$ is a topological sum\nof a discrete space and a submetrizable $k_\\omega$-space.\n  (2) If $X$ is a metrizable space, then $(CL(X), \\tau_V)$ has an\n$\\omega^\\omega$-base if and only if $X$ is separable and the boundary of each\nclosed subset of $X$ is $\\sigma$-compact.\n  (3) If $X$ is a metrizable space, then $(CL(X), \\tau_F)$ has an\n$\\omega^\\omega$-base consisting of basic neighborhoods if and only if $X$ is a\nPolish space.\n  (4) If $X$ is a metrizable space, then $(CL(X), \\tau_F)$ is a\nFr\\'echet-Urysohn space with an $\\omega^\\omega$-base, if and only if $(CL(X),\n\\tau_F)$ is first-countable, if and only if $X$ is a locally compact and second\ncountable space.",
        "In AI alignment, extensive latitude must be granted to annotators, either\nhuman or algorithmic, to judge which model outputs are `better' or `safer.' We\nrefer to this latitude as alignment discretion. Such discretion remains largely\nunexamined, posing two risks: (i) annotators may use their power of discretion\narbitrarily, and (ii) models may fail to mimic this discretion. To study this\nphenomenon, we draw on legal concepts of discretion that structure how\ndecision-making authority is conferred and exercised, particularly in cases\nwhere principles conflict or their application is unclear or irrelevant.\nExtended to AI alignment, discretion is required when alignment principles and\nrules are (inevitably) conflicting or indecisive. We present a set of metrics\nto systematically analyze when and how discretion in AI alignment is exercised,\nsuch that both risks (i) and (ii) can be observed. Moreover, we distinguish\nbetween human and algorithmic discretion and analyze the discrepancy between\nthem. By measuring both human and algorithmic discretion over safety alignment\ndatasets, we reveal layers of discretion in the alignment process that were\npreviously unaccounted for. Furthermore, we demonstrate how algorithms trained\non these datasets develop their own forms of discretion in interpreting and\napplying these principles, which challenges the purpose of having any\nprinciples at all. Our paper presents the first step towards formalizing this\ncore gap in current alignment processes, and we call on the community to\nfurther scrutinize and control alignment discretion.",
        "A small fraction of low-mass stars have been found to have anomalously high\nLi abundances. Although it has been suggested that mixing during the red giant\nbranch phase can lead to Li production, this method of intrinsic Li production\ncannot explain Li-rich stars that have not yet undergone the first dredge-up.\nTo obtain clues about the origin of such stars, we present a detailed chemical\nabundance analysis of four unevolved Li-rich stars with $-2.1 < [\\mathrm{Fe\/H}]\n< -1.3$ and $2.9<A({\\rm Li})<3.6$, $0.7-1.4$ dex higher Li abundance than\ntypical unevolved metal-poor stars. One of the stars, Gaia DR3\n6334970766103389824 (D25_6334), was serendipitously found in the stellar stream\nED-3, and the other three stars have been reported to have massive ($M\\gtrsim\n1.3\\,\\mathrm{M_\\odot}$) non-luminous companions. We show that three of the four\nstars exhibit abundance patterns similar to those of known unevolved Li-rich\nstars, namely normal abundances in most elements except for Li and Na. These\nabundance similarities suggest a common origin for the unevolved Li-rich stars\nand low-mass metal-poor stars with massive compact companions. We also made the\nfirst detection of N abundance to unevolved Li-rich stars in D25_6334, and\nfound that it is significantly enhanced ($[\\mathrm{N\/Fe}]=1.3$). The observed\nabundance pattern of D25_6334, spanning from C to Si, indicates that its\nsurface has been polluted by an intermediate-mass former companion star or a\nnova system that involves a massive ONe white dwarf. Using a population\nsynthesis model, we show that the nova scenario can lead to the observed level\nof Li enhancement and also provide an explanation for Li-rich stars without\ncompanions and those with massive compact companions.",
        "The rise of social media has significantly increased the prevalence of\ncyberbullying (CB), posing serious risks to both mental and physical\nwell-being. Effective detection systems are essential for mitigating its\nimpact. While several machine learning (ML) models have been developed, few\nincorporate victims' psychological, demographic, and behavioral factors\nalongside bullying comments to assess severity. In this study, we propose an AI\nmodel intregrating user-specific attributes, including psychological factors\n(self-esteem, anxiety, depression), online behavior (internet usage,\ndisciplinary history), and demographic attributes (race, gender, ethnicity),\nalong with social media comments. Additionally, we introduce a re-labeling\ntechnique that categorizes social media comments into three severity levels:\nNot Bullying, Mild Bullying, and Severe Bullying, considering user-specific\nfactors.Our LSTM model is trained using 146 features, incorporating emotional,\ntopical, and word2vec representations of social media comments as well as\nuser-level attributes and it outperforms existing baseline models, achieving\nthe highest accuracy of 98\\% and an F1-score of 0.97. To identify key factors\ninfluencing the severity of cyberbullying, we employ explainable AI techniques\n(SHAP and LIME) to interpret the model's decision-making process. Our findings\nreveal that, beyond hate comments, victims belonging to specific racial and\ngender groups are more frequently targeted and exhibit higher incidences of\ndepression, disciplinary issues, and low self-esteem. Additionally, individuals\nwith a prior history of bullying are at a greater risk of becoming victims of\ncyberbullying.",
        "We present a rack-scale compute architecture for ML using multi-accelerator\nservers connected via chip-to-chip silicon photonic components. Our\narchitecture achieves (1) multi-tenanted resource slicing without\nfragmentation, (2) 74% faster rack-scale collective communication, and (3) 1.7X\nspeedup in end-to-end ML training throughput.",
        "Static spherically symmetric (SSS) solutions of f(R) gravity are studied in\nthe Einstein frame. The solutions involve SSS configuration mass M and scalaron\nmass $\\mu$ (in geometrized units); for typical astrophysical masses, the\ndimensionless parameter $M\\mu$ has very large value. We found analytic\nsolutions on a finite interval for $M\\mu\\to \\infty$ in case of a family of\nscalaron potentials. The asymptotically flat solutions on $(0,\\infty)$ have\nbeen studied numerically for $M\\mu$ up to $10^{20}$ in case of the quadratic\nf(R) model.",
        "Many-to-one mappings and permutation polynomials over finite fields have\nimportant applications in cryptography and coding theory. In this paper, we\nstudy the many-to-one property of a large class of polynomials such as $f(x) =\nh(a x^q + b x + c) + u x^q + v x$, where $h(x) \\in \\mathbb{F}_{q^2}[x]$ and\n$a$, $b$, $c$, $u$, $v \\in \\mathbb{F}_{q^2}$. Using a commutative diagram\nsatisfied by $f(x)$ and trace functions over finite fields, we reduce the\nproblem whether $f(x)$ is a many-to-one mapping on $\\mathbb{F}_{q^2}$ to\nanother problem whether an associated polynomial $g(x)$ is a many-to-one\nmapping on the subfield $\\mathbb{F}_{q}$. In particular, when $h(x) = x^{r}$\nand $r$ satisfies certain conditions, we reduce $g(x)$ to polynomials of small\ndegree or linearized polynomials. Then by employing the many-to-one properties\nof these low degree or linearized polynomials on $\\mathbb{F}_{q}$, we derive a\nseries of explicit characterization for $f(x)$ to be many-to-one on\n$\\mathbb{F}_{q^2}$. On the other hand, for all $1$-to-$1$ mappings obtained in\nthis paper, we determine the inverses of these permutation polynomials.\nMoreover, we also explicitly construct involutions from $2$-to-$1$ mappings of\nthis form. Our findings generalize and unify many results in the literature.",
        "Both simulations and observations indicate that the so-called missing baryons\nreside in the intergalactic medium (IGM) known as the warm-hot intergalactic\nmedium (WHIM). In this article, we demonstrate that turbulence in the cosmic\nbaryonic fluid is crucial for correctly understanding both the spatial\ndistribution and the physical origins of the missing baryons in the universe.\nFirst, we find that dynamical effects cause the gas to be detained in\nlow-density and intermediate-density regions, resulting in high baryon\nfractions, while prevent the inflow of the gas in high-density regions, leading\nto low baryon fractions. Second, turbulent energy is converted into thermal\nenergy, and the injection and dissipation of turbulent energy have essentially\nreached a balance from $z=1$ to $0$. This indicates that the cosmic fluid is in\na state of fully-developed turbulence within this redshift range. Due to\nturbulent heating, as redshift decreases, an increasing amount of warm gas is\nheated and transitions into the WHIM, and some even into hot gas.",
        "The widespread adoption of generative AI has generated diverse opinions, with\nindividuals expressing both support and criticism of its applications. This\nstudy investigates the emotional dynamics surrounding generative AI by\nanalyzing human tweets referencing terms such as ChatGPT, OpenAI, Copilot, and\nLLMs. To further understand the emotional intelligence of ChatGPT, we examine\nits responses to selected tweets, highlighting differences in sentiment between\nhuman comments and LLM-generated responses. We introduce EmoXpt, a sentiment\nanalysis framework designed to assess both human perspectives on generative AI\nand the sentiment embedded in ChatGPT's responses. Unlike prior studies that\nfocus exclusively on human sentiment, EmoXpt uniquely evaluates the emotional\nexpression of ChatGPT. Experimental results demonstrate that LLM-generated\nresponses are notably more efficient, cohesive, and consistently positive than\nhuman responses.",
        "Brain-inspired spiking neural networks (SNNs) have garnered significant\nresearch attention in algorithm design and perception applications. However,\ntheir potential in the decision-making domain, particularly in model-based\nreinforcement learning, remains underexplored. The difficulty lies in the need\nfor spiking neurons with long-term temporal memory capabilities, as well as\nnetwork optimization that can integrate and learn information for accurate\npredictions. The dynamic dendritic information integration mechanism of\nbiological neurons brings us valuable insights for addressing these challenges.\nIn this study, we propose a multi-compartment neuron model capable of\nnonlinearly integrating information from multiple dendritic sources to\ndynamically process long sequential inputs. Based on this model, we construct a\nSpiking World Model (Spiking-WM), to enable model-based deep reinforcement\nlearning (DRL) with SNNs. We evaluated our model using the DeepMind Control\nSuite, demonstrating that Spiking-WM outperforms existing SNN-based models and\nachieves performance comparable to artificial neural network (ANN)-based world\nmodels employing Gated Recurrent Units (GRUs). Furthermore, we assess the\nlong-term memory capabilities of the proposed model in speech datasets,\nincluding SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment\nneuron model surpasses other SNN-based architectures in processing long\nsequences. Our findings underscore the critical role of dendritic information\nintegration in shaping neuronal function, emphasizing the importance of\ncooperative dendritic processing in enhancing neural computation.",
        "We study the dynamics of electrons in crystalline solids in the presence of\ninhomogeneous external electric and magnetic fields. We present a manifestly\ngauge-invariant operator-based approach without relying on a semiclassical\nwavepacket construction, and derive the field-induced corrections to the\nequations of motion at the operator level. This includes the Berry curvature\ninduced anomalous velocity and contributions arising from the quantum geometry\nof the Bloch bands. We show explicitly how these multi-band effects are\nmanifested in an effective single band approximation. We present a formalism\nthat allows for a systematic expansion to an arbitrary order in the\ninhomogeneity of the applied fields, as well as a way to compute the matrix\nelements in Bloch basis.",
        "We present in this paper an exceptional scientific dataset allowing to\ninvestigate the structure and evolution of the interior of solar\nsupergranulation cells. Trees of Fragmenting Granules (TFG) and associated\nflows were evidenced using Local Correlation Tracking techniques (LCT) from a\n24 H duration sequence of Hinode (JAXA\/NASA) observations. The treatment of the\ndataset exhibits the evolution of the TFG and shows that their mutual\ninteractions are able to build horizontal flows with longer lifetime than\ngranules (1 to 2 hours) over a scale of 10 arcsec (the mesogranulation). These\nflows act on the diffusion of the intranetwork magnetic elements and also on\nthe location and shape of the network. Hence, the TFG appear as one of the\nmajor elements involved in supergranular formation and evolution.",
        "In the Pati-Salam gauge symmetry $SU(4)_c \\times SU(2)_L \\times SU(2)_R$\n(4-2-2, for short), the observed quarks and leptons of each family reside in\nthe bi-fundamental representations $(4,2,1)$ and $({\\bar 4},1,2)$. There exist,\nhowever, the fundamental representations $(4,1,1)$, $(1,2,1)$ and $(1,1,2)$ and\ntheir hermitian conjugates, which show the presence, in principle, of yet to be\ndiscovered color triplets that carry electric charge $\\pm{e\/6}$, and color\nsinglet particles with charges of $\\pm{e\/2}$. These Standard Model charges are\nin full accord with the fact that the 4-2-2 model predicts the presence of a\ntopologically stable finite energy magnetic monopole that carries two quanta of\nDirac magnetic charge, i.e., $4 \\pi\/e$, as well as color magnetic charge that\nis screened beyond the quark confinement scale. The 4-2-2 model therefore\npredicts the existence of exotic baryons, mesons and leptons that carry\nfractional ($\\pm{e\/2}$) electric charges. Since their origin lies in the\nfundamental representations of 4-2-2, these exotic particles may turn out to be\nrelatively light, in the TeV mass range or so. The 4-2-2 magnetic monopole mass\ndepends on the 4-2-2 symmetry breaking scale which may be as low as a few TeV.",
        "Object-centric representation learning has recently been successfully applied\nto real-world datasets. This success can be attributed to pretrained\nnon-object-centric foundation models, whose features serve as reconstruction\ntargets for slot attention. However, targets must remain frozen throughout the\ntraining, which sets an upper bound on the performance object-centric models\ncan attain. Attempts to update the target encoder by bootstrapping result in\nlarge performance drops, which can be attributed to its lack of object-centric\ninductive biases, causing the object-centric model's encoder to drift away from\nrepresentations useful as reconstruction targets. To address these limitations,\nwe propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a\nself-distillation setup for training object-centric models from scratch, on\nreal-world data, for the first time ever. In OCEBO, the target encoder is\nupdated as an exponential moving average of the object-centric model, thus\nexplicitly being enriched with object-centric inductive biases introduced by\nslot attention while removing the upper bound on performance present in other\nmodels. We mitigate the slot collapse caused by random initialization of the\ntarget encoder by introducing a novel cross-view patch filtering approach that\nlimits the supervision to sufficiently informative patches. When pretrained on\n241k images from COCO, OCEBO achieves unsupervised object discovery performance\ncomparable to that of object-centric models with frozen non-object-centric\ntarget encoders pretrained on hundreds of millions of images. The code and\npretrained models are publicly available at https:\/\/github.com\/djukicn\/ocebo.",
        "The existing intense laser-based approaches for nuclear excitation offer\nultrafast temporal resolution and high efficiency compared to traditional\naccelerator probes. However, controlling nuclear properties such as spin and\nmagnetic moment remains an unprecedented challenge. Here, we put forward a\nnovel method for nuclear excitation and control induced by intense vortex\nlasers. We develop a theory incorporating the orbital angular momentum (OAM) of\nvortex laser within the nuclear hyperfine mixing framework. We find that\nintense vortex laser can effectively excite hydrogen-like thorium-229 nucleus\nand induce three-dimensional rotation of the nuclear magnetic moment. This\nrotation arises from the localized electromagnetic field and new transition\nchannels excited by the vortex laser, and can be reconstructed through\nradiation spectrum analysis. Moreover, the OAM of vortex laser enables the\nchaotic system to exhibit topologically protected periodic patterns in nuclear\nexcitation and radiation, facilitating precise experimental measurements. Our\nfindings underscore the potential of vortex laser for high-precision nuclear\ncontrol and imaging, deepening our understanding of nuclear properties and\nhyperfine structure, and advancing quantum information and nuclear\ntechnologies.",
        "Neither hardcore bosons nor fermions can occupy the same lattice site-state.\nHowever, a nearest neighbour interaction may counteract the hardcore effect,\nresulting in condensate states in a bosonic system. In this work, we unveil the\nunderlying mechanism by developing a general method to construct the condensate\neigenstates from those of sub-Hamiltonians. As an application, we find that a\nlocal on-site potential can induce an evanescent condensate mode. Based on\nthis, exact condensate ground states of hardcore bosons, possessing\noff-diagonal long-range order, can be constructed when an array of impurities\nis applied. The effect of the off-resonance impurity on the condensate ground\nstates is also investigated using numerical simulations of the dynamic\nresponse.",
        "Bin picking is a challenging robotic task due to occlusions and physical\nconstraints that limit visual information for object recognition and grasping.\nExisting approaches often rely on known CAD models or prior object geometries,\nrestricting generalization to novel or unknown objects. Other methods directly\nregress grasp poses from RGB-D data without object priors, but the inherent\nnoise in depth sensing and the lack of object understanding make grasp\nsynthesis and evaluation more difficult. Superquadrics (SQ) offer a compact,\ninterpretable shape representation that captures the physical and graspability\nunderstanding of objects. However, recovering them from limited viewpoints is\nchallenging, as existing methods rely on multiple perspectives for\nnear-complete point cloud reconstruction, limiting their effectiveness in\nbin-picking. To address these challenges, we propose \\textbf{RGBSQGrasp}, a\ngrasping framework that leverages superquadric shape primitives and foundation\nmetric depth estimation models to infer grasp poses from a monocular RGB camera\n-- eliminating the need for depth sensors. Our framework integrates a\nuniversal, cross-platform dataset generation pipeline, a foundation model-based\nobject point cloud estimation module, a global-local superquadric fitting\nnetwork, and an SQ-guided grasp pose sampling module. By integrating these\ncomponents, RGBSQGrasp reliably infers grasp poses through geometric reasoning,\nenhancing grasp stability and adaptability to unseen objects. Real-world\nrobotic experiments demonstrate a 92\\% grasp success rate, highlighting the\neffectiveness of RGBSQGrasp in packed bin-picking environments.",
        "Version control systems are commonly used to manage open-source software, in\nwhich each commit may introduce new vulnerabilities or fix existing ones.\nResearchers have developed various tools for detecting vulnerabilities in code\ncommits, but their performance is limited by factors such as neglecting\ndescriptive data and challenges in accurately identifying vulnerability\nintroductions. To overcome these limitations, we propose CommitShield, which\ncombines the code analysis capabilities of static analysis tools with the\nnatural language and code understanding capabilities of large language models\n(LLMs) to enhance the accuracy of vulnerability introduction and fix detection\nby generating precise descriptions and obtaining rich patch contexts. We\nevaluate CommitShield using the newly constructed vulnerability repair dataset,\nCommitVulFix, and a cleaned vulnerability introduction dataset. Experimental\nresults indicate that CommitShield improves recall by 76%-87% over\nstate-of-the-art methods in the vulnerability fix detection task, and its\nF1-score improves by 15%-27% in the vulnerability introduction detection task.",
        "Flow-based frame interpolation methods ensure motion stability through\nestimated intermediate flow but often introduce severe artifacts in complex\nmotion regions. Recent generative approaches, boosted by large-scale\npre-trained video generation models, show promise in handling intricate scenes.\nHowever, they frequently produce unstable motion and content inconsistencies\ndue to the absence of explicit motion trajectory constraints. To address these\nchallenges, we propose Motion-aware Generative frame interpolation (MoG) that\nsynergizes intermediate flow guidance with generative capacities to enhance\ninterpolation fidelity. Our key insight is to simultaneously enforce motion\nsmoothness through flow constraints while adaptively correcting flow estimation\nerrors through generative refinement. Specifically, we first introduce a dual\nguidance injection that propagates condition information using intermediate\nflow at both latent and feature levels, aligning the generated motion with\nflow-derived motion trajectories. Meanwhile, we implemented two critical\ndesigns, encoder-only guidance injection and selective parameter fine-tuning,\nwhich enable dynamic artifact correction in the complex motion regions.\nExtensive experiments on both real-world and animation benchmarks demonstrate\nthat MoG outperforms state-of-the-art methods in terms of video quality and\nvisual fidelity. Our work bridges the gap between flow-based stability and\ngenerative flexibility, offering a versatile solution for frame interpolation\nacross diverse scenarios.",
        "Spiking Neural Networks (SNNs) have gained significant attention due to their\nbiological plausibility and energy efficiency, making them promising\nalternatives to Artificial Neural Networks (ANNs). However, the performance gap\nbetween SNNs and ANNs remains a substantial challenge hindering the widespread\nadoption of SNNs. In this paper, we propose a Spatial-Temporal Attention\nAggregator SNN (STAA-SNN) framework, which dynamically focuses on and captures\nboth spatial and temporal dependencies. First, we introduce a spike-driven\nself-attention mechanism specifically designed for SNNs. Additionally, we\npioneeringly incorporate position encoding to integrate latent temporal\nrelationships into the incoming features. For spatial-temporal information\naggregation, we employ step attention to selectively amplify relevant features\nat different steps. Finally, we implement a time-step random dropout strategy\nto avoid local optima. As a result, STAA-SNN effectively captures both spatial\nand temporal dependencies, enabling the model to analyze complex patterns and\nmake accurate predictions. The framework demonstrates exceptional performance\nacross diverse datasets and exhibits strong generalization capabilities.\nNotably, STAA-SNN achieves state-of-the-art results on neuromorphic datasets\nCIFAR10-DVS, with remarkable performances of 97.14%, 82.05% and 70.40% on the\nstatic datasets CIFAR-10, CIFAR-100 and ImageNet, respectively. Furthermore,\nour model exhibits improved performance ranging from 0.33\\% to 2.80\\% with\nfewer time steps. The code for the model is available on GitHub.",
        "This paper introduces an explanation framework designed to enhance the\nquality of rules in knowledge-based reasoning systems based on dataset-driven\ninsights. The traditional method for rule induction from data typically\nrequires labor-intensive labeling and data-driven learning. This framework\nprovides an alternative and instead allows for the data-driven refinement of\nexisting rules: it generates explanations of rule inferences and leverages\nhuman interpretation to refine rules. It leverages four complementary\nexplanation types: trace-based, contextual, contrastive, and counterfactual,\nproviding diverse perspectives for debugging, validating, and ultimately\nrefining rules. By embedding explainability into the reasoning architecture,\nthe framework enables knowledge engineers to address inconsistencies, optimize\nthresholds, and ensure fairness, transparency, and interpretability in\ndecision-making processes. Its practicality is demonstrated through a use case\nin finance.",
        "In this article, models for assessing national intangible resources are\nanalysed through a lecture in the literature, and the best-known evaluation\nmethods are categorized into academic models and models of international\norganizations, with the most important differences being identified. The\nEuropean Innovation Scoreboard (EIS) and the World Economic Forum annual\nreports on Global Competitiveness were considered to assess Romania's position\nin the international context in terms of intangible assets. Despite the\nimportance of intangible resources at national level and the fact that they are\nan important factor in determining economic growth in the current\nknowledge-based economy, this article concludes that Romania's position in the\ninternational context regarding intangible assets is very weak, with many weak\npoints in research and innovation performance compared to other EU Member\nStates. Therefore, there is a need in our country to re-evaluate the areas\nwhere all efforts need to be focused to stimulate innovation performance, to\nproperly manage national intangible resources, a crucial process for improving\nthe quality of life.",
        "Occlusion poses a significant challenge in pedestrian detection from a single\nview. To address this, multi-view detection systems have been utilized to\naggregate information from multiple perspectives. Recent advances in multi-view\ndetection utilized an early-fusion strategy that strategically projects the\nfeatures onto the ground plane, where detection analysis is performed. A\npromising approach in this context is the use of 3D feature-pulling technique,\nwhich constructs a 3D feature volume of the scene by sampling the corresponding\n2D features for each voxel. However, it creates a 3D feature volume of the\nwhole scene without considering the potential locations of pedestrians. In this\npaper, we introduce a novel model that efficiently leverages traditional 3D\nreconstruction techniques to enhance deep multi-view pedestrian detection. This\nis accomplished by complementing the 3D feature volume with probabilistic\noccupancy volume, which is constructed using the visual hull technique. The\nprobabilistic occupancy volume focuses the model's attention on regions\noccupied by pedestrians and improves detection accuracy. Our model outperforms\nstate-of-the-art models on the MultiviewX dataset, with an MODA of 97.3%, while\nachieving competitive performance on the Wildtrack dataset.",
        "Robotic manipulation systems operating in diverse, dynamic environments must\nexhibit three critical abilities: multitask interaction, generalization to\nunseen scenarios, and spatial memory. While significant progress has been made\nin robotic manipulation, existing approaches often fall short in generalization\nto complex environmental variations and addressing memory-dependent tasks. To\nbridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based\npolicy that leverages multi-resolution upsampling with visual representations\nfrom large-scale foundation model. SAM2Act achieves a state-of-the-art average\nsuccess rate of 86.8% across 18 tasks in the RLBench benchmark, and\ndemonstrates robust generalization on The Colosseum benchmark, with only a 4.3%\nperformance gap under diverse environmental perturbations. Building on this\nfoundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2,\nwhich incorporates a memory bank, an encoder, and an attention mechanism to\nenhance spatial memory. To address the need for evaluating memory-dependent\ntasks, we introduce MemoryBench, a novel benchmark designed to assess spatial\nmemory and action recall in robotic manipulation. SAM2Act+ achieves competitive\nperformance on MemoryBench, significantly outperforming existing approaches and\npushing the boundaries of memory-enabled robotic systems. Project page:\nhttps:\/\/sam2act.github.io\/",
        "The vast repositories of Electronic Health Records (EHR) and medical claims\nhold untapped potential for studying rare but critical events, such as suicide\nattempt. Conventional setups often model suicide attempt as a univariate\noutcome and also exclude any ``single-record'' patients with a single\ndocumented encounter due to a lack of historical information. However, patients\nwho were diagnosed with suicide attempts at the only encounter could, to some\nsurprise, represent a substantial proportion of all attempt cases in the data,\nas high as 70--80%. We innovate a hybrid and integrative learning framework to\nleverage concurrent outcomes as surrogates and harness the forbidden yet\nprecious information from single-record data. Our approach employs a supervised\nlearning component to learn the latent variables that connect primary (e.g.,\nsuicide) and surrogate outcomes (e.g., mental disorders) to historical\ninformation. It simultaneously employs an unsupervised learning component to\nutilize the single-record data, through the shared latent variables. As such,\nour approach offers a general strategy for information integration that is\ncrucial to modeling rare conditions and events. With hospital inpatient data\nfrom Connecticut, we demonstrate that single-record data and concurrent\ndiagnoses indeed carry valuable information, and utilizing them can\nsubstantially improve suicide risk modeling."
      ]
    }
  },
  {
    "id":2412.20007,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
    "start_abstract":"Deep learning tools have gained tremendous attention in applied machine learning. However such for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about uncertainty, but usually come with prohibitive computational cost. this paper we develop new theoretical casting dropout training deep neural networks (NNs) as approximate inference Gaussian processes. A direct result of theory gives us uncertainty NNs -- extracting information from existing that has been thrown away so far. This mitigates the problem representing without sacrificing either complexity or test accuracy. We perform an extensive study properties dropout's Various network architectures non-linearities are assessed on tasks classification, using MNIST example. show considerable improvement predictive log-likelihood RMSE compared state-of-the-art methods, finish by reinforcement",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions"
      ],
      "abstract":[
        "Training of neural networks for automated diagnosis pigmented skin lesions is hampered by the small size and lack diversity available datasets dermatoscopic images. We tackle this problem releasing HAM10000 (\"Human Against Machine with 10000 training images\") dataset. collected images from different populations acquired stored modalities. Given we had to apply acquisition cleaning methods developed semi-automatic workflows utilizing specifically trained networks. The final dataset consists 10015 which are released as a set academic machine learning purposes publicly through ISIC archive. This benchmark can be used comparisons human experts. Cases include representative collection all important diagnostic categories in realm lesions. More than 50% have been confirmed pathology, while ground truth rest cases was either follow-up, expert consensus, or confirmation in-vivo confocal microscopy."
      ],
      "categories":[
        "Data"
      ]
    },
    "list":{
      "title":[
        "Open Questions and Future Directions in Titan Science",
        "Robust tests for log-logistic models based on minimum density power\n  divergence estimators",
        "Theory of spin magnetization driven by chiral phonons",
        "Safety for Time-Varying Parameterized Sets Using Control Barrier\n  Function Methods",
        "Emergence of cooperation promoted by higher-order strategy updates",
        "A Characterization of the Behavior of Nonlinear GMRES on Linear Systems",
        "Detecting Abrupt Changes in Point Processes: Fundamental Limits and\n  Applications",
        "Hydrogen Network Expansion Planning considering the Chicken-and-egg\n  Dilemma and Market Uncertainty",
        "Unveiling Coverage Dependent Interactions of N-Methylaniline with the\n  Pt(111) Surface",
        "Generating Generalised Ground-State Ansatzes from Few-Body Examples",
        "Forecast constraints on the axion-photon coupling from interstellar\n  medium heating",
        "Beyond the orbitally-resolved magnetic exchange in CrI$_{3}$ and\n  NiI$_{2}$",
        "Constraining the Milky Way's Dispersion Measure Using FRB and X-ray Data",
        "Improved dependence on coherence in eigenvector and eigenvalue\n  estimation error bounds",
        "On the local analyticity for the Euler equations",
        "CUPID, the CUORE Upgrade with Particle Identification",
        "Merge-width and First-Order Model Checking",
        "Limit theorems for functionals of linear processes in critical regions",
        "Regularization for Covariance Parameterization of Direct Data-Driven LQR\n  Control",
        "Nonclassical dynamics of N\\'eel vector and magnetization accompanied by\n  THz and high-harmonic radiation from ultrafast-light-driven NiO\n  antiferromagnet insulator",
        "The $s\\pm$ pairing symmetry in the pressured La$_3$Ni$_2$O$_7$ from\n  electron-phonon coupling",
        "Atomic Origins of Magnetic Anisotropy in Ru-substituted Manganite Films",
        "Online Scheduling for LLM Inference with KV Cache Constraints",
        "Recursive Koszul flattenings of determinant and permanent tensors",
        "Real-time adaptation of quantum noise channel estimates",
        "Noetherianity of polynomial rings up to group actions",
        "Higgs in The Cosmos",
        "Examining the Impact of Income Inequality and Gender on School\n  Completion in Malaysia: A Machine Learning Approach Utilizing Malaysia's\n  Public Sector Open Data",
        "Microwave sink using plasma based localized surface plasmons"
      ],
      "abstract":[
        "In this chapter we attempt to distill the very large number of possible\nfuture inquiries of Titan into a relatively concise list of twenty high level\nquestions - each of which of would necessarily entail a multitude of more\nspecific investigations and studies. While this list does not encompass all\npossible open questions, and is divided into topics according to our preference\nand not in any way uniquely, we believe that it does however span a wide range\nof the most intriguing topics about Titan, and may form some sort of guide\nespecially for those embarking into Titan studies for the first time. At the\nend of this chapter we return to explore how these four techniques may be used\nto answer the large, high-level open questions in Titan science.",
        "The log-logistic distribution is a versatile parametric family widely used\nacross various applied fields, including survival analysis, reliability\nengineering, and econometrics. When estimating parameters of the log-logistic\ndistribution, hypothesis testing is necessary to verify assumptions about these\nparameters. The Wald test and Rao test provide formal methods for testing\nhypotheses about these parameters. However, these test statistics are not\nrobust, and their rejection decisions may be affected by data contamination. In\nthis paper we develop new families of Wald-type test statistics and Rao-type\ntest statistics based on minimum density power divergence estimators (MDPDEs)\nfor the parameters of the log-logistic distribution. These new families\ngeneralize the Wald and Rao test statistics, inheriting the robustness\nproperties from the MDPDEs and thus addressing the lack of robustness of the\nclassical tests. Explicit expressions for the test statistics under the\nlog-logistic model for both simple and composite null hypotheses are derived,\nand their properties are analyzed in detail. An extensive simulation study\nempirically demonstrates the robustness of these families and compares their\nperformance with the classical methods.",
        "We construct a general theory of spin magnetization driven by chiral phonons\nunder an adiabatic process, in which atoms rotate around their equilibrium\npositions with a low phonon frequency. Here the spin magnetization originates\nfrom the modulated electronic states with spin-orbital coupling by atomic\nrotations. Under the adiabatic approximation, the time-dependent spin\nmagnetization can be calculated by a Berry-phase method. In this paper, we\nfocus on its time average, which is evaluated by assuming that the phonon\ndisplacement is small. As a result, the time average of the spin magnetization\nis concisely formulated in the form of the Berry curvature defined in the\nphonon-displacement space as an intrinsic property of atomic rotations. Our\nformula for spin magnetization reflects the chiral nature of phonons, and is\nconvenient for $ab$ $initio$ calculations.",
        "A fundamental and classical problem in mobile autonomous systems is\nmaintaining the safety of autonomous agents during deployment. Prior literature\nhas presented techniques using control barrier functions (CBFs) to achieve this\ngoal. These prior techniques utilize CBFs to keep an isolated point in state\nspace away from the unsafe set. However, various situations require a\nnon-singleton set of states to be kept away from an unsafe set. Prior\nliterature has addressed this problem using nonsmooth CBF methods, but no prior\nwork has solved this problem using only \"smooth\" CBF methods. This paper\naddresses this gap by presenting a novel method of applying CBF methods to\nnon-singleton parameterized convex sets. The method ensures differentiability\nof the squared distance function between ego and obstacle sets by leveraging a\nform of the log-sum-exp function to form strictly convex, arbitrarily tight\noverapproximations of these sets. Safety-preserving control inputs can be\ncomputed via convex optimization formulations. The efficacy of our results is\ndemonstrated through multi-agent simulations.",
        "Cooperation is fundamental to human societies, and the interaction structure\namong individuals profoundly shapes its emergence and evolution. In real-world\nscenarios, cooperation prevails in multi-group (higher-order) populations,\nbeyond just dyadic behaviors. Despite recent studies on group dilemmas in\nhigher-order networks, the exploration of cooperation driven by higher-order\nstrategy updates remains limited due to the intricacy and indivisibility of\ngroup-wise interactions. Here we investigate four categories of higher-order\nmechanisms for strategy updates in public goods games and establish their\nmathematical conditions for the emergence of cooperation. Such conditions\nuncover the impact of both higher-order strategy updates and network properties\non evolutionary outcomes, notably highlighting the enhancement of cooperation\nby overlaps between groups. Interestingly, we discover that the strategical\nmechanism alternating optimality and randomness -- selecting an outstanding\ngroup and then imitating a random individual within this group -- can\nprominently promote cooperation. Our analyses further unveil that, compared to\npairwise interactions, higher-order strategy updates generally improve\ncooperation in most higher-order networks. These findings underscore the\npivotal role of higher-order strategy updates in fostering collective\ncooperation in complex social systems.",
        "The Nonlinear GMRES (NGMRES) proposed by Oosterlee and Washio [SIAM Journal\non Scientific Computing, 2000, 21(5):1670-1690] is an acceleration method for\nfixed point iterations. It has been demonstrated to be effective, but its\nconvergence properties have not been extensively studied in the literature so\nfar, even for linear systems. In this work we aim to close some of this gap. We\noffer a convergence analysis for NGMRES applied to linear systems. A central\npart of our analysis focuses on identifying equivalences between NGMRES and the\nclassical Krylov subspace GMRES method.",
        "We consider the problem of detecting abrupt changes (i.e., large jump\ndiscontinuities) in the rate function of a point process. The rate function is\nassumed to be fully unknown, non-stationary, and may itself be a random process\nthat depends on the history of event times. We show that abrupt changes can be\naccurately identified from observations of the point process, provided the\nchanges are sharper than the \"smoothness'' of the rate function before the\nabrupt change. This condition is also shown to be necessary from an\ninformation-theoretic point of view. We then apply our theory to several\nspecial cases of interest, including the detection of significant changes in\npiecewise smooth rate functions and detecting super-spreading events in\nepidemic models on graphs. Finally, we confirm the effectiveness of our methods\nthrough a detailed empirical analysis of both synthetic and real datasets.",
        "Green hydrogen is thought to be a game changer for reaching sustainability\ntargets. However, the transition to a green hydrogen economy faces a critical\nchallenge known as the `chicken-and-egg dilemma', wherein establishing a\nhydrogen supply network relies on demand, while demand only grows with reliable\nsupply. In addition, as the hydrogen market is in the early stage, predicting\ndemand distributions is challenging due to lack of data availability. This\npaper addresses these complex issues through a risk-averse framework with the\nintroduction of a distributionally robust hydrogen network expansion planning\nproblem under decision-dependent demand ambiguity. The problem optimizes\nlocation and production capacity decisions of the suppliers considering the\nmoments of the stochastic hydrogen demand as a function of these investment\ndecisions. To obtain tractable representations of this problem, we derive two\ndifferent reformulations that consider continuous and discrete hydrogen demand\nsupport sets under different forms of decision dependencies. To efficiently\nsolve the reformulations, we develop a tailored algorithm based on the\ncolumn-and-constraint generation approach, and enhance the computational\nperformance through solving the master problems to a relative optimality gap,\ndecomposing the subproblems, and integrating pre-generated columns and\nconstraints. To validate the effectiveness of our approach, we investigate a\nreal case study leveraging data from the \"Hydrogen Energy Applications in\nValley Environments for Northern Netherlands (HEAVENN)\" project. The results\nreveal that considering the chicken-and-egg dilemma under uncertain hydrogen\nmarket conditions leads to earlier and more diverse investments, providing\ncritical insights for policymakers based on the degree of decision dependency.",
        "This study aims to elucidate the adsorption and surface chemistry of\nN-methylaniline (NMA) on Pt(111), using it as a model molecule to probe the\nactivation mechanisms of aromatic amines on catalytic surfaces. Through a\ncombination of density functional theory (DFT) calculations and experimental\ntechniques such as temperature programmed X-ray photoelectron spectroscopy\n(TP-XPS), temperature programmed desorption (TPD), and Fourier transform\ninfrared reflection absorption spectroscopy(FT-IRRAS),we explored the\ncoverage-dependent behaviour of NMA on Pt(111) to identify key steps in the\nactivation process. The population of certain reaction paths is driven by a\ncoverage dependent balance between molecule surface charge transfer and\nintermolecular interactions, dictating the selective activation of specific\nbonds. Our findings reveal how coverage influences the orientation and bonding\nof NMA on the Pt(111)surface. At lower coverages, the molecule binds to the\nsurface through the phenyl ring and activation, facilitating C-N bond cleavage\nto the ring under HCN formation. In comparison, at higher coverages, the\nmolecule binds only through the nitrogen atom and desorbs intact. These\ninsights into variable bond activation lay the ground work for understanding\nthe fundamental processes involved in potential heterogeneously catalyzed\nreactions of aromatic amines, contributing to the development of new catalytic\nstrategies.",
        "We introduce a method that generates ground state ansatzes for quantum\nmany-body systems which are both analytically tractable and accurate over wide\nparameter regimes. Our approach leverages a custom symbolic language to\nconstruct tensor network states (TNS) via an evolutionary algorithm. This\nlanguage provides operations that allow the generated TNS to automatically\nscale with system size. Consequently, we can evaluate ansatz fitness for small\nsystems, which is computationally efficient, while favouring structures that\ncontinue to perform well with increasing system size. This ensures that the\nansatz captures robust features of the ground state structure. Remarkably, we\nfind analytically tractable ansatzes with a degree of universality, which\nencode correlations, capture finite-size effects, accurately predict ground\nstate energies, and offer a good description of critical phenomena. We\ndemonstrate this method on the Lipkin-Meshkov-Glick model (LMG) and the quantum\ntransverse-field Ising model (TFIM), where the same ansatz was independently\ngenerated for both. The simple structure of the ansatz allows us to restore\nbroken symmetries and obtain exact expressions for local observables and\ncorrelation functions. We also point out an interesting connection between this\nansatz and a well-studied sequence in analytical number theory and the\none-dimensional classical Ising model.",
        "In interstellar media characterized by a nonrelativistic plasma of electrons\nand heavy ions, we study the effect of axion dark matter coupled to photons on\nthe dynamics of an electric field. In particular, we assume the presence of a\nbackground magnetic field aligned in a specific direction. We show that there\nis an energy transfer from the oscillating axion field to photons and then to\nthe plasma induced by forced resonance. This resonance is most prominent for\nthe axion mass $m_{\\phi}$ equivalent to the plasma frequency $\\omega_p$.\nRequiring that the heating rate of the interstellar medium caused by the energy\ntransfer does not exceed the observed astrophysical cooling rate, we place\nforecast constraints on the axion-photon coupling $g$ for several different\namplitudes of the background magnetic field $B_0$. By choosing a typical value\n$B_0=10^{-5}$ G, we find that, for the resonance mass $m_{\\phi}=\\omega_p$, the\nupper limit of $g$ can be stronger than those derived from other measurements\nin the literature. With increased values of $B_0$, it is possible to put more\nstringent constraints on $g$ for a wider range of the axion mass away from the\nresonance point.",
        "The pertinent need for microscopic understanding of magnetic exchange\nmotivated us to go beyond the existing theories and develop a systematic method\nto quantify all possible mechanisms that contribute to magnetic exchange for an\narbitrary pair of atoms in a given material. We apply it to the archetypal 2D\nmagnetic monolayers CrI3 and NiI2, to reveal the previously underrated\ndx2-y2,dx2-y2 contribution as either the leading or the second largest\ncontribution to the total magnetic exchange. We proceed to explore the\nmicroscopic mechanisms behind all the non-zero orbital contributions in both\nCrI3 and NiI2, and generalize the findings to other magnetic monolayers\ndominated by d8 and d3 electronic configurations of the magnetic atoms.",
        "The dispersion measures (DMs) of fast radio bursts (FRBs) are a valuable tool\nfor probing the baryonic content of the intergalactic and the circumgalactic\nmedium of the intervening galaxies along the sightlines. However, interpreting\nthe DMs is complicated by the contributions from the hot gas in and around our\nMilky Way. This study examines the relationship between DM_MW, derived from\nlocalized FRBs, and the Galaxy's hot gas, using X-ray absorption and emission\ndata from O VII and O VIII. We find evidence for a positive correlation between\nDM_MW and O VII absorption, reflecting contributions from both the disk and\nhalo components. This conclusion is supported by two lines of evidence: (1) No\ncorrelation between DM_MW and O VII\/O VIII emission, which primarily traces\ndense disk regions; and (2) the comparison with electron density models, where\nDM_MW aligns with models that incorporate both disk and halo components but\nsignificantly exceeds predictions from pure disk-only models, emphasizing the\nhalo's role. Furthermore, the lack of correlation with O VIII absorption\nsuggests that the primary temperature of the Galaxy's hot gas is likely around\n2 x 10^6 K or less, as traced by O VII absorption, while gas at higher\ntemperatures (~3 x 10^6 K to 5 x 10^6 K) is present but less abundant. Our\nfindings provide insights into the Milky Way's gas distribution and improve\nDM_MW estimates for future cosmological studies.",
        "Spectral estimators are fundamental in lowrank matrix models and arise\nthroughout machine learning and statistics, with applications including network\nanalysis, matrix completion and PCA. These estimators aim to recover the\nleading eigenvalues and eigenvectors of an unknown signal matrix observed\nsubject to noise. While extensive research has addressed the statistical\naccuracy of spectral estimators under a variety of conditions, most previous\nwork has assumed that the signal eigenvectors are incoherent with respect to\nthe standard basis. This assumption typically arises because of suboptimal\ndependence on coherence in one or more concentration inequalities. Using a new\nmatrix concentration result that may be of independent interest, we establish\nestimation error bounds for eigenvector and eigenvalue recovery whose\ndependence on coherence significantly improves upon prior work. Our results\nimply that coherence-free bounds can be achieved when the standard deviation of\nthe noise is comparable to its Orlicz 1-norm (i.e., its subexponential norm).\nThis matches known minimax lower bounds under Gaussian noise up to logarithmic\nfactors.",
        "In this paper, we study the existence and uniqueness of solutions to the\nEuler equations with initial conditions that exhibit analytic regularity near\nthe boundary and Sobolev regularity away from it. A key contribution of this\nwork is the introduction of the diamond-analyticity framework, which captures\nthe spatial decay of the analyticity radius in a structured manner, improving\nupon uniform analyticity approaches. We employ the Leray projection and a\nnonstandard mollification technique to demonstrate that the quotient between\nthe imaginary and real parts of the analyticity radius remains unrestricted,\nthus extending the analyticity persistence results beyond traditional\nconstraints. Our methodology combines analytic-Sobolev estimates with an\niterative scheme which is nonstandard in the Cauchy-Kowalevskaya framework,\nensuring rigorous control over the evolution of the solution. These results\ncontribute to a deeper understanding of the interplay between analyticity and\nboundary effects in fluid equations. They might have implications for the study\nof the inviscid limit of the Navier-Stokes equations and the role of complex\nsingularities in fluid dynamics.",
        "CUPID, the CUORE Upgrade with Particle Identification, is a next-generation\nexperiment to search for neutrinoless double beta decay ($0\\nu\\beta\\beta$) and\nother rare events using enriched Li$_2$$^{100}$MoO$_4$ scintillating\nbolometers. It will be hosted by the CUORE cryostat located at the Laboratori\nNazionali del Gran Sasso in Italy. The main physics goal of CUPID is to search\nfor $0\\nu\\beta\\beta$\\ of $^{100}$Mo with a discovery sensitivity covering the\nfull neutrino mass regime in the inverted ordering scenario, as well as the\nportion of the normal ordering regime with lightest neutrino mass larger than\n10 meV. With a conservative background index of 10$^{-4}$\ncnts\/(keV$\\cdot$kg$\\cdot$yr), 240 kg isotope mass, 5 keV FWHM energy resolution\nand 10 live-years of data taking, CUPID will have a 90\\% C.L. half-life\nexclusion sensitivity of 1.8 $\\cdot$ 10$^{27}$ yr, corresponding to an\neffective Majorana neutrino mass ($m_{\\beta\\beta}$) sensitivity of 9--15 meV,\nand a $3\\sigma$ discovery sensitivity of 1 $\\cdot$ 10$^{27}$ yr, corresponding\nto an $m_{\\beta\\beta}$ range of 12--21 meV.",
        "We introduce merge-width, a family of graph parameters that unifies several\nstructural graph measures, including treewidth, degeneracy, twin-width,\nclique-width, and generalized coloring numbers. Our parameters are based on new\ndecompositions called construction sequences. These are sequences of ever\ncoarser partitions of the vertex set, where each pair of parts has a specified\ndefault connection, and all vertex pairs of the graph that differ from the\ndefault are marked as resolved. The radius-$r$ merge-width is the maximum\nnumber of parts reached from a vertex by following a path of at most $r$\nresolved edges. Graph classes of bounded merge-width -- for which the\nradius-$r$ merge-width parameter can be bounded by a constant, for each fixed\n$r=1,2,3,\\ldots$ -- include all classes of bounded expansion or of bounded\ntwin-width, thus unifying two central notions from the Sparsity and Twin-width\nframeworks. Furthermore, they are preserved under first-order transductions,\nwhich attests to their robustness. We conjecture that classes of bounded\nmerge-width are equivalent to the previously introduced classes of bounded\nflip-width.\n  As our main result, we show that the model checking problem for first-order\nlogic is fixed-parameter tractable on graph classes of bounded merge-width,\nassuming the input includes a witnessing construction sequence. This unites and\nextends two previous model checking results: the result of Dvo\\v{r}\\'{a}k,\nKr\\'{a}l, and Thomas for classes of bounded expansion, and the result of\nBonnet, Kim, Thomass\\'e, and Watrigant for classes of bounded twin-width.\n  Finally, we suggest future research directions that could impact the study of\nstructural and algorithmic graph theory, in particular of monadically dependent\ngraph classes, which we conjecture to coincide with classes of almost bounded\nmerge-width.",
        "Let $X=\\{X_n: n\\in\\mathbb{N}\\}$ be the linear process defined by\n$X_n=\\sum^{\\infty}_{j=1} a_j\\varepsilon_{n-j}$, where the coefficients\n$a_j=j^{-\\beta}\\ell(j)$ are constants with $\\beta>0$ and $\\ell$ a slowly\nvarying function, and the innovations $\\{\\varepsilon_n\\}_{n\\in\\mathbb{Z}}$ are\ni.i.d. random variables belonging to the domain of attraction of an\n$\\alpha$-stable law with $\\alpha\\in(0,2]$. Limit theorems for the partial sum $\nS_{[Nt]}=\\sum^{[Nt]}_{n=1}[K(X_n)-\\mathbb{E}K(X_n)]$ with proper measurable\nfunctions $K$ have been extensively studied, except for two critical regions:\nI. $\\alpha\\in(1,2),\\beta=1$ and II. $\\alpha\\beta=2,\\beta\\geq1$. In this paper,\nwe address these open scenarios and identify the asymptotic distributions of\n$S_{[Nt]}$ under mild conditions.",
        "As the benchmark of data-driven control methods, the linear quadratic\nregulator (LQR) problem has gained significant attention. A growing trend is\ndirect LQR design, which finds the optimal LQR gain directly from raw data and\nbypassing system identification. To achieve this, our previous work develops a\ndirect LQR formulation parameterized by sample covariance. In this paper, we\npropose a regularization method for the covariance-parameterized LQR. We show\nthat the regularizer accounts for the uncertainty in both the steady-state\ncovariance matrix corresponding to closed-loop stability, and the LQR cost\nfunction corresponding to averaged control performance. With a positive or\nnegative coefficient, the regularizer can be interpreted as promoting either\nexploitation or exploration, which are well-known trade-offs in reinforcement\nlearning. In simulations, we observe that our covariance-parameterized LQR with\nregularization can significantly outperform the certainty-equivalence LQR in\nterms of both the optimality gap and the robust closed-loop stability.",
        "Ultrafast-light-driven strongly correlated antiferromagnetic insulators, such\nas prototypical NiO with large energy gap 4 eV, have recently attracted\nexperimental attention using either above-gap [K. Gillmeister et al., Nat.\nCommun. 11, 4095 (2020)] or subgap [H. Qiu et al., Nat. Phys. 17, 388 (2021)]\nenergy photons that are of fundamental interest in far-from-equilibrium quantum\nmatter or spintronic applications, respectively. In the latter context,\nemission of THz radiation is also observed from NiO\/Pt bilayers, where heavy\nmetal (HM) Pt introduces strong spin-orbit coupling (SOC). However, microscopic\nmechanisms of such emission remain obscure because spintronic THz emitters have\nbeen amply studied using FM\/HM (FM-ferromagnetic metal of conventional type)\nbilayers, where ultrafast demagnetization takes place and is directly related\nto THz emission. Conversely, in NiO total magnetization is zero prior to the fs\nlaser pulse (fsLP) application. Here we employ the two-orbital\nHubbard-Hund-Heisenberg model and study, via numerically exact nonequilibrium\nquantum many-body methods, the dynamics of its Neel vector and nonequilibrium\nmagnetization. Additionally, we compute electromagnetic radiation by both\ntime-dependent magnetization and local charge currents arising in either plain\nNiO or NiO with proximity SOC introduced by HM layer. Our analysis reveals\nnonclassical dynamics of Neel vector and nonequilibrium magnetization, changing\nonly in length while not rotating, where the former is substantially reduced\nonly in the case above-gap fsLP. In the plain NiO case, THz radiation of\ninterest to applications is insignificant, but adding SOC enhances both current\nand magnetic dipole contributions to it. Above THz range, we find integer\nhigh-harmonic generation, as well as unusual noninteger harmonics for above-gap\nfsLP pump.",
        "The recently discovered bilayer Ruddlesden-Popper nickelate La$_3$Ni$_2$O$_7$\nexhibits superconductivity with a remarkable transition temperature $T_c\\approx\n80 $ K under applied pressures above 14.0 GPa. This discovery of new family of\nhigh-temperature superconductors has garnered significant attention in the\ncondensed matter physics community. In this work, we assume the this\nhigh-temperature superconductor is mediated by phonons and investigate the\npairing symmetry in two distinct models: (i) the full-coupling case, where the\nNi-$d_{x^2-y^2}$ and Ni-$d_{3z^2-r^2}$ orbitals are treated equally in both\ninterlayer and intralayer coupling interactions, and (ii) the half-coupling\ncase, where the intralayer coupling involves only the $d_{x^2-y^2}$ orbital,\nwhile the interlayer coupling is restricted to the $d_{3z^2-r^2}$ orbital. Our\ncalculations reveal that the interlayer coupling favors an $s\\pm$-wave\nsuperconducting state, whereas the intralayer coupling promotes an $s++$-wave\nsymmetry. Additionally, we discuss the implications of pair-hopping\ninteractions on the superconducting properties. These findings provide valuable\ninsights into the pairing mechanisms and symmetry of this newly discovered\nhigh-temperature superconductor.",
        "Magnetic anisotropy in complex oxides often originates from the complex\ninterplay of several factors, including crystal structure, spin-orbit coupling,\nand electronic interactions. Recent studies on Ru-substituted\n$La_{0.70}Sr_{0.30}MnO_3$ (Ru-LSMO) films demonstrate emerging magnetic and\nmagneto-transport properties, where magnetic anisotropy plays a crucial role.\nHowever, the atomic origin and underlying mechanisms of the magnetic anisotropy\nof this material system remain elusive. This work sheds light on these aspects.\nDetailed element-specific X-ray magnetic dichroism analysis suggests that Ru\nsingle ion anisotropy governs the overall magnetic anisotropy. Furthermore, the\nmagnetic property of Mn ions changes dramatically due to strong\nantiferromagnetic coupling between Ru and Mn ions. Our findings clarify the\nrole of Ru single ion anisotropy behind magnetic anisotropy in Ru-LSMO,\noffering a promising avenue for designing advanced materials with tailored\nmagnetic properties for next generation magnetic and spintronic technologies.\nAs the Curie temperature of these materials is close to room temperature, such\ntunable magnetic anisotropy holds prospects for functional room-temperature\nmagnetic devices.",
        "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
        "We investigate new lower bounds on the tensor rank of the determinant and the\npermanent tensors via recursive usage of the Koszul flattening method\nintroduced by Landsberg-Ottaviani and Hauenstein-Oeding-Ottaviani-Sommese. Our\nlower bounds on $\\mathbf{R} (\\det_n)$ completely separate the determinant and\nthe permanent tensors by their tensor ranks. Furthermore, we determine the\nexact tensor ranks $\\mathbf{R} (\\det_4) = 12$ and $\\mathbf{R}\n(\\operatorname{perm}_4) = 8$ over arbitrary field of characteristic $\\neq 2$.",
        "Estimates of noise channels for quantum gates are required for most error\nmitigation techniques and are desirable for informing quantum error correction\ndecoders. These estimates can be obtained by resource-intensive off-line\ncharacterization techniques, but can become stale due to device drift and\nfluctuations. We propose a method to address this issue by performing real-time\nadaptation of noise channel estimates during the execution of a quantum\nalgorithmic circuit using extended flag gadgets, mid-circuit measurements and\nBayesian inference. We carry out analytical calculations and numerical\nsimulations employing a Dirichlet prior distribution for the error rates in a\nPauli channel to demonstrate and evaluate the technique, which can be seen as a\nprotocol for real-time calibration of high-level gate error information.",
        "Let $k$ be a commutative Noetherian ring, and $k[S]$ the polynomial ring with\nindeterminates parameterized by elements in a set $S$. We show that $k[S]$ is\nNoetherian up to actions of permutation groups on $S$ satisfying certain\ncombinatorial conditions. Moreover, there is a special linear order on every\ninfinite $S$ such that $k[S]$ is Noetherian up to the action of the\norder-preserving permutation group, and the existence of such a linear order is\nequivalent to the Axiom of Choice. These Noetherian results are proved via a\nsheaf theoretic approach and the work of Nagel-R\\\"{o}mer.",
        "We explore the Higgs particle in the cosmic quark-gluon plasma (QGP) below\nthe electroweak phase transition temperature $T_\\mathrm{EW}\\simeq\n125\\mathrm{\\,GeV}$. We show that Higgs is neither in abundance (chemical) nor\nin momentum distribution equilibrium in certain stages of the Universe\nevolution. Nonequilibrium originates in: For chemical nonequilibrium in the\nalways present irreversible decays into virtual heavy gauge bosons, and; For\n$T<25$\\,GeV in relatively rapid $2\\leftrightarrow 1$ formation and decay\nprocesses yielding momentum distribution as created in these reactions. As\nheavy particles disappear, the minimal Higgs coupling to abundant low mass\nparticles fails in $2\\to2$ (two-particle) scattering processes to assure a\nkinetic distribution equilibrium. The expansion of the Universe is by more than\n10 orders of magnitude slower compared to microscopic processes. All other\nparticles in the Universe are in full thermal equilibrium, with exception of\nthe late in QGP evolution of the bottom flavor near to hadronization condition.",
        "This study examines the relationship between income inequality, gender, and\nschool completion rates in Malaysia using machine learning techniques. The\ndataset utilized is from the Malaysia's Public Sector Open Data Portal,\ncovering the period 2016-2022. The analysis employs various machine learning\ntechniques, including K-means clustering, ARIMA modeling, Random Forest\nregression, and Prophet for time series forecasting. These models are used to\nidentify patterns, trends, and anomalies in the data, and to predict future\nschool completion rates. Key findings reveal significant disparities in school\ncompletion rates across states, genders, and income levels. The analysis also\nidentifies clusters of states with similar completion rates, suggesting\npotential regional factors influencing educational outcomes. Furthermore, time\nseries forecasting models accurately predict future completion rates,\nhighlighting the importance of ongoing monitoring and intervention strategies.\nThe study concludes with recommendations for policymakers and educators to\naddress the observed disparities and improve school completion rates in\nMalaysia. These recommendations include targeted interventions for specific\nstates and demographic groups, investment in early childhood education, and\naddressing the impact of income inequality on educational opportunities. The\nfindings of this study contribute to the understanding of the factors\ninfluencing school completion in Malaysia and provide valuable insights for\npolicymakers and educators to develop effective strategies to improve\neducational outcomes.",
        "Overcoming the diffraction limit, meaning focusing waves on a sub-wavelength\nscale, has received considerable attention for applications involving light and\nacoustics. Indeed, the intense focusing achieved enhances the interactions\nbetween waves and matter, and the improved spatial resolution opens up\npossibilities in fields such as imaging, detection and communication. In\noptics, a passive sink may be obtained if the incident light couples to\nlocalized surface plasmon (LSP) resonances, resulting in Coherent Perfect\nAbsorption condition. The presence of optical surface plasmons at metal\ninterfaces is due to the angular frequency of metals, which lies in the optical\nregime due to their high density of free electrons. Plasmas, which consists of\nionized gazes with a lower electron density, can support surface plasmons in\nthe microwave regime. Hence, we demonstrate in this paper that sub-wavelength\nplasmas behave as a passive microwave sink by exciting LSP resonance inside the\nplasma."
      ]
    }
  },
  {
    "id":2412.20007,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions",
    "start_abstract":"Training of neural networks for automated diagnosis pigmented skin lesions is hampered by the small size and lack diversity available datasets dermatoscopic images. We tackle this problem releasing HAM10000 (\"Human Against Machine with 10000 training images\") dataset. collected images from different populations acquired stored modalities. Given we had to apply acquisition cleaning methods developed semi-automatic workflows utilizing specifically trained networks. The final dataset consists 10015 which are released as a set academic machine learning purposes publicly through ISIC archive. This benchmark can be used comparisons human experts. Cases include representative collection all important diagnostic categories in realm lesions. More than 50% have been confirmed pathology, while ground truth rest cases was either follow-up, expert consensus, or confirmation in-vivo confocal microscopy.",
    "start_categories":[
      "Data"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      ],
      "abstract":[
        "Deep learning tools have gained tremendous attention in applied machine learning. However such for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about uncertainty, but usually come with prohibitive computational cost. this paper we develop new theoretical casting dropout training deep neural networks (NNs) as approximate inference Gaussian processes. A direct result of theory gives us uncertainty NNs -- extracting information from existing that has been thrown away so far. This mitigates the problem representing without sacrificing either complexity or test accuracy. We perform an extensive study properties dropout's Various network architectures non-linearities are assessed on tasks classification, using MNIST example. show considerable improvement predictive log-likelihood RMSE compared state-of-the-art methods, finish by reinforcement"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Hilbert-Schmidtness of the $M_{\\theta,\\varphi}$-type submodules",
        "Auto-Regressive Diffusion for Generating 3D Human-Object Interactions",
        "Heterogeneous Image GNN: Graph-Conditioned Diffusion for Image Synthesis",
        "Rotational Brownian motion and heavy quark polarization in QCD medium",
        "Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems\n  View of Successive Paraphrasing",
        "Catching Spinning Table Tennis Balls in Simulation with End-to-End\n  Curriculum Reinforcement Learning",
        "Three topological phases of the elliptic Ginibre ensembles with a point\n  charge",
        "Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo",
        "Matrix Completion with Graph Information: A Provable Nonconvex\n  Optimization Approach",
        "WiFi-Diffusion: Achieving Fine-Grained WiFi Radio Map Estimation With\n  Ultra-Low Sampling Rate by Diffusion Models",
        "Geometric Constrained Non-Line-of-Sight Imaging",
        "Occlusion-Aware Contingency Safety-Critical Planning for Autonomous\n  Vehicles",
        "Bounds on the number of squares in recurrence sequences: $y_{0}=b^{2}$\n  (I)",
        "Evaluating a Digital Speech Therapy App for Stuttering: A Pilot\n  Validation Study",
        "HyperArm Bandit Optimization: A Novel approach to Hyperparameter\n  Optimization and an Analysis of Bandit Algorithms in Stochastic and\n  Adversarial Settings",
        "VAGeo: View-specific Attention for Cross-View Object Geo-Localization",
        "SIGGRAPH: G: Improved Projective Dynamics Global Using Snapshots-based\n  Reduced Bases",
        "Enhancing Efficiency of Local Projections Estimation with Volatility\n  Clustering in High-Frequency Data",
        "Exponentially Stable Combined Adaptive Control under Finite Excitation\n  Condition",
        "Exact Covariance Characterization for Controlled Linear Systems subject\n  to Stochastic Parametric and Additive Uncertainties",
        "Boosting the Self-driven Properties of 2D Photodetectors through\n  Synergistic Asymmetrical Effects",
        "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
        "From Weyl Anomaly to Defect Supersymmetric R\\'enyi Entropy and Casimir\n  Energy",
        "Fuzzy Integration of Data Lake Tables",
        "Risk-aware Integrated Task and Motion Planning for Versatile Snake\n  Robots under Localization Failures",
        "HIPPO-MAT: Decentralized Task Allocation Using GraphSAGE and Multi-Agent\n  Deep Reinforcement Learning",
        "A Survey on Vulnerability Prioritization: Taxonomy, Metrics, and\n  Research Challenges",
        "ConQuer: A Framework for Concept-Based Quiz Generation",
        "Are LLMs Ready for Practical Adoption for Assertion Generation?"
      ],
      "abstract":[
        "Let $\\theta(z),\\varphi(w)$ be two nonconstant inner functions and $M$ be a\nsubmodule in $H^2(\\mathbb{D}^2)$. Let $C_{\\theta,\\varphi}$ denote the\ncomposition operator on $H^2(\\mathbb{D}^2)$ defined by\n$C_{\\theta,\\varphi}f(z,w)=f(\\theta(z),\\varphi(w))$, and $M_{\\theta,\\varphi}$\ndenote the submodule $[C_{\\theta,\\varphi}M]$, that is, the smallest submodule\ncontaining $C_{\\theta,\\varphi}M$. Let $K^M_{\\lambda,\\mu}(z,w)$ and\n$K^{M_{\\theta,\\varphi}}_{\\lambda,\\mu}(z,w)$ be the reproducing kernel of $M$\nand $M_{\\theta,\\varphi}$, respectively. By making full use of the positivity of\ncertain de Branges-Rovnyak kernels, we prove that \\[K^{M_{\\theta,\\varphi}}= K^M\n\\circ B~ \\cdot R,\\] where $B=(\\theta,\\varphi)$,\n$R_{\\lambda,\\mu}(z,w)=\\frac{1-\\overline{\\theta(\\lambda)}\\theta(z)}{1-\\bar{\\lambda}z}\n\\frac{1-\\overline{\\varphi(\\mu)}\\varphi(w)}{1-\\bar{\\mu}w}$. This implies that\n$M_{\\theta,\\varphi}$ is a Hilbert-Schmidt submodule if and only if $M$ is.\nMoreover, as an application, we prove that the Hilbert-Schmidt norms of\nsubmodules $[\\theta(z)-\\varphi(w)]$ are uniformly bounded.",
        "Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging\nfield with applications in animation, video games, virtual reality, and\nrobotics. A key challenge in HOI generation is maintaining interaction\nconsistency in long sequences. Existing Text-to-Motion-based approaches, such\nas discrete motion tokenization, cannot be directly applied to HOI generation\ndue to limited data in this domain and the complexity of the modality. To\naddress the problem of interaction consistency in long sequences, we propose an\nautoregressive diffusion model (ARDHOI) that predicts the next continuous\ntoken. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)\nto learn a physically plausible space of continuous HOI tokens, thereby\nensuring that generated human-object motions are realistic and natural. For\ngenerating sequences autoregressively, we develop a Mamba-based context encoder\nto capture and maintain consistent sequential actions. Additionally, we\nimplement an MLP-based denoiser to generate the subsequent token conditioned on\nthe encoded context. Our model has been evaluated on the OMOMO and BEHAVE\ndatasets, where it outperforms existing state-of-the-art methods in terms of\nboth performance and inference speed. This makes ARDHOI a robust and efficient\nsolution for text-driven HOI tasks",
        "We introduce a novel method for conditioning diffusion-based image synthesis\nmodels with heterogeneous graph data. Existing approaches typically incorporate\nconditioning variables directly into model architectures, either through\ncross-attention layers that attend to text latents or image concatenation that\nspatially restrict generation. However, these methods struggle to handle\ncomplex scenarios involving diverse, relational conditioning variables, which\nare more naturally represented as unstructured graphs. This paper presents\nHeterogeneous Image Graphs (HIG), a novel representation that models\nconditioning variables and target images as two interconnected graphs, enabling\nefficient handling of variable-length conditioning inputs and their\nrelationships. We also propose a magnitude-preserving GNN that integrates the\nHIG into the existing EDM2 diffusion model using a ControlNet approach. Our\napproach improves upon the SOTA on a variety of conditioning inputs for the\nCOCO-stuff and Visual Genome datasets, and showcases the ability to condition\non graph attributes and relationships represented by edges in the HIG.",
        "We consider the rotational Brownian motion of heavy quark in QCD medium and\nprovide predictions for polarization of open heavy-flavor hadrons. We calculate\nexpressions for vector and tensor polarization, corresponding to baryon spin\npolarization and vector meson spin alignment, respectively. We propose that the\ntransverse momentum dependence of heavy quark polarization may serve as a\ndistinctive signature of the intense initial magnetic field generated in\noff-central relativistic heavy-ion collisions.",
        "Dynamical systems theory provides a framework for analyzing iterative\nprocesses and evolution over time. Within such systems, repetitive\ntransformations can lead to stable configurations, known as attractors,\nincluding fixed points and limit cycles. Applying this perspective to large\nlanguage models (LLMs), which iteratively map input text to output text,\nprovides a principled approach to characterizing long-term behaviors.\nSuccessive paraphrasing serves as a compelling testbed for exploring such\ndynamics, as paraphrases re-express the same underlying meaning with linguistic\nvariation. Although LLMs are expected to explore a diverse set of paraphrases\nin the text space, our study reveals that successive paraphrasing converges to\nstable periodic states, such as 2-period attractor cycles, limiting linguistic\ndiversity. This phenomenon is attributed to the self-reinforcing nature of\nLLMs, as they iteratively favour and amplify certain textual forms over others.\nThis pattern persists with increasing generation randomness or alternating\nprompts and LLMs. These findings underscore inherent constraints in LLM\ngenerative capability, while offering a novel dynamical systems perspective for\nstudying their expressive potential.",
        "The game of table tennis is renowned for its extremely high spin rate, but\nmost table tennis robots today struggle to handle balls with such rapid spin.\nTo address this issue, we have contributed a series of methods, including: 1.\nCurriculum Reinforcement Learning (RL): This method helps the table tennis\nrobot learn to play table tennis progressively from easy to difficult tasks. 2.\nAnalysis of Spinning Table Tennis Ball Collisions: We have conducted a\nphysics-based analysis to generate more realistic trajectories of spinning\ntable tennis balls after collision. 3. Definition of Trajectory States: The\ndefinition of trajectory states aids in setting up the reward function. 4.\nSelection of Valid Rally Trajectories: We have introduced a valid rally\ntrajectory selection scheme to ensure that the robot's training is not\ninfluenced by abnormal trajectories. 5. Reality-to-Simulation (Real2Sim)\nTransfer: This scheme is employed to validate the trained robot's ability to\nhandle spinning balls in real-world scenarios. With Real2Sim, the deployment\ncosts for robotic reinforcement learning can be further reduced. Moreover, the\ntrajectory-state-based reward function is not limited to table tennis robots;\nit can be generalized to a wide range of cyclical tasks. To validate our\nrobot's ability to handle spinning balls, the Real2Sim experiments were\nconducted. For the specific video link of the experiment, please refer to the\nsupplementary materials.",
        "We consider the complex and symplectic elliptic Ginibre matrices of size\n$(c+1)N \\times (c+1)N$, conditioned to have a deterministic eigenvalue at $ p\n\\in \\mathbb{R} $ with multiplicity $ c N $. We show that their limiting\nspectrum is either simply connected, doubly connected, or composed of two\ndisjoint simply connected components. Moreover, denoting by $\\tau \\in [0,1]$\nthe non-Hermiticity parameter, we explicitly characterise the regions in the\nparameter space $ (p, c, \\tau) $ where each topological type emerges. For cases\nwhere the droplet is either simply or doubly connected, we provide an explicit\ndescription of the limiting spectrum and the corresponding electrostatic\nenergies. As an application, we derive the asymptotic behaviour of the moments\nof the characteristic polynomial for elliptic Ginibre matrices in the\nexponentially varying regime.",
        "Bayesian Neural Networks (BNNs) provide a promising framework for modeling\npredictive uncertainty and enhancing out-of-distribution robustness (OOD) by\nestimating the posterior distribution of network parameters. Stochastic\nGradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods\nfor scalable posterior sampling in BNNs, achieving efficiency by combining\nstochastic gradient descent with second-order Langevin dynamics. However,\nSGMCMC often suffers from limited sample diversity in practice, which affects\nuncertainty estimation and model performance. We propose a simple yet effective\napproach to enhance sample diversity in SGMCMC without the need for tempering\nor running multiple chains. Our approach reparameterizes the neural network by\ndecomposing each of its weight matrices into a product of matrices, resulting\nin a sampling trajectory that better explores the target parameter space. This\napproach produces a more diverse set of samples, allowing faster mixing within\nthe same computational budget. Notably, our sampler achieves these improvements\nwithout increasing the inference cost compared to the standard SGMCMC.\nExtensive experiments on image classification tasks, including OOD robustness,\ndiversity, loss surface analyses, and a comparative study with Hamiltonian\nMonte Carlo, demonstrate the superiority of the proposed approach.",
        "We consider the problem of matrix completion with graphs as side information\ndepicting the interrelations between variables. The key challenge lies in\nleveraging the similarity structure of the graph to enhance matrix recovery.\nExisting approaches, primarily based on graph Laplacian regularization, suffer\nfrom several limitations: (1) they focus only on the similarity between\nneighboring variables, while overlooking long-range correlations; (2) they are\nhighly sensitive to false edges in the graphs and (3) they lack theoretical\nguarantees regarding statistical and computational complexities. To address\nthese issues, we propose in this paper a novel graph regularized matrix\ncompletion algorithm called GSGD, based on preconditioned projected gradient\ndescent approach. We demonstrate that GSGD effectively captures the\nhigher-order correlation information behind the graphs, and achieves superior\nrobustness and stability against the false edges. Theoretically, we prove that\nGSGD achieves linear convergence to the global optimum with near-optimal sample\ncomplexity, providing the first theoretical guarantees for both recovery\naccuracy and efficacy in the perspective of nonconvex optimization. Our\nnumerical experiments on both synthetic and real-world data further validate\nthat GSGD achieves superior recovery accuracy and scalability compared with\nseveral popular alternatives.",
        "Fine-grained radio map presents communication parameters of interest, e.g.,\nreceived signal strength, at every point across a large geographical region. It\ncan be leveraged to improve the efficiency of spectrum utilization for a large\narea, particularly critical for the unlicensed WiFi spectrum. The problem of\nfine-grained radio map estimation is to utilize radio samples collected by\nsparsely distributed sensors to infer the map. This problem is challenging due\nto the ultra-low sampling rate, where the number of available samples is far\nless than the fine-grained resolution required for radio map estimation. We\npropose WiFi-Diffusion -- a novel generative framework for achieving\nfine-grained WiFi radio map estimation using diffusion models. WiFi-Diffusion\nemploys the creative power of generative AI to address the ultra-low sampling\nrate challenge and consists of three blocks: 1) a boost block, using prior\ninformation such as the layout of obstacles to optimize the diffusion model; 2)\na generation block, leveraging the diffusion model to generate a candidate set\nof radio maps; and 3) an election block, utilizing the radio propagation model\nas a guide to find the best radio map from the candidate set. Extensive\nsimulations demonstrate that 1) the fine-grained radio map generated by\nWiFi-Diffusion is ten times better than those produced by state-of-the-art\n(SOTA) when they use the same ultra-low sampling rate; and 2) WiFi-Diffusion\nachieves comparable fine-grained radio map quality with only one-fifth of the\nsampling rate required by SOTA.",
        "Normal reconstruction is crucial in non-line-of-sight (NLOS) imaging, as it\nprovides key geometric and lighting information about hidden objects, which\nsignificantly improves reconstruction accuracy and scene understanding.\nHowever, jointly estimating normals and albedo expands the problem from\nmatrix-valued functions to tensor-valued functions that substantially\nincreasing complexity and computational difficulty. In this paper, we propose a\nnovel joint albedo-surface reconstruction method, which utilizes the Frobenius\nnorm of the shape operator to control the variation rate of the normal field.\nIt is the first attempt to apply regularization methods to the reconstruction\nof surface normals for hidden objects. By improving the accuracy of the normal\nfield, it enhances detail representation and achieves high-precision\nreconstruction of hidden object geometry. The proposed method demonstrates\nrobustness and effectiveness on both synthetic and experimental datasets. On\ntransient data captured within 15 seconds, our surface normal-regularized\nreconstruction model produces more accurate surfaces than recently proposed\nmethods and is 30 times faster than the existing surface reconstruction\napproach.",
        "Ensuring safe driving while maintaining travel efficiency for autonomous\nvehicles in dynamic and occluded environments is a critical challenge. This\npaper proposes an occlusion-aware contingency safety-critical planning approach\nfor real-time autonomous driving in such environments. Leveraging reachability\nanalysis for risk assessment, forward reachable sets of occluded phantom\nvehicles are computed to quantify dynamic velocity boundaries. These velocity\nboundaries are incorporated into a biconvex nonlinear programming (NLP)\nformulation, enabling simultaneous optimization of exploration and fallback\ntrajectories within a receding horizon planning framework. To facilitate\nreal-time optimization and ensure coordination between trajectories, we employ\nthe consensus alternating direction method of multipliers (ADMM) to decompose\nthe biconvex NLP problem into low-dimensional convex subproblems. The\neffectiveness of the proposed approach is validated through simulation studies\nand real-world experiments in occluded intersections. Experimental results\ndemonstrate enhanced safety and improved travel efficiency, enabling real-time\nsafe trajectory generation in dynamic occluded intersections under varying\nobstacle conditions. A video showcasing the experimental results is available\nat https:\/\/youtu.be\/CHayG7NChqM.",
        "We continue and generalise our earlier investigations of the number of\nsquares in binary recurrence sequences. Here we consider sequences, $\\left\\{\ny_{k} \\right\\}$, arising from the solutions of generalised negative Pell\nequations, $X^{2}-dY^{2}=c$, where $-c$ and $y_{0}$ are any positive squares.\nWe show that there are at most $5$ distinct squares in such sequences when\n$y_{0}=1,4,\\ldots,11^{2}$, or once $d$ exceeds an explicit lower bound.",
        "Stuttering is a speech disorder that disrupts fluency and often leads to\nsignificant psychological and social challenges. This study evaluates the\neffectiveness of Eloquent, a digital speech therapy app, by analyzing\npre-therapy and post-therapy speech samples using the Stuttering Severity\nIndex-4 (SSI-4) and the S24 communication and attitude scale. Results indicate\nsignificant improvements in fluency, with reductions in SSI-4 scores across\nreading, speaking, duration, and physical concomitant metrics. Additionally,\nparticipants demonstrated a more positive attitude towards communication\npost-therapy, as evidenced by lower S24 scores. These findings highlight the\npotential of technology-driven, structured speech therapy interventions to\ndeliver measurable improvements in stuttering severity and communication\nconfidence.",
        "This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization.",
        "Cross-view object geo-localization (CVOGL) aims to locate an object of\ninterest in a captured ground- or drone-view image within the satellite image.\nHowever, existing works treat ground-view and drone-view query images\nequivalently, overlooking their inherent viewpoint discrepancies and the\nspatial correlation between the query image and the satellite-view reference\nimage. To this end, this paper proposes a novel View-specific Attention\nGeo-localization method (VAGeo) for accurate CVOGL. Specifically, VAGeo\ncontains two key modules: view-specific positional encoding (VSPE) module and\nchannel-spatial hybrid attention (CSHA) module. In object-level, according to\nthe characteristics of different viewpoints of ground and drone query images,\nviewpoint-specific positional codings are designed to more accurately identify\nthe click-point object of the query image in the VSPE module. In feature-level,\na hybrid attention in the CSHA module is introduced by combining channel\nattention and spatial attention mechanisms simultaneously for learning\ndiscriminative features. Extensive experimental results demonstrate that the\nproposed VAGeo gains a significant performance improvement, i.e., improving\nacc@0.25\/acc@0.5 on the CVOGL dataset from 45.43%\/42.24% to 48.21%\/45.22% for\nground-view, and from 61.97%\/57.66% to 66.19%\/61.87% for drone-view.",
        "We propose a snapshots-based method to compute reduction subspaces for\nphysics-based simulations. Our method is applicable to any mesh with some\nartistic prior knowledge of the solution and only requires a record of existing\nsolutions during, for instance, the range-of-motion test that is required\nbefore approving a mesh character for an application. Our subspaces span a\nwider range of motion, especially large deformations, and rotations by default.\nCompared to the state-of-the-art, we achieve improved numerical stability,\ncomputational efficiency, and more realistic simulations with a smaller\nsub-space.",
        "This paper advances the local projections (LP) method by addressing its\ninefficiency in high-frequency economic and financial data with volatility\nclustering. We incorporate a generalized autoregressive conditional\nheteroskedasticity (GARCH) process to resolve serial correlation issues and\nextend the model with GARCH-X and GARCH-HAR structures. Monte Carlo simulations\nshow that exploiting serial dependence in LP error structures improves\nefficiency across forecast horizons, remains robust to persistent volatility,\nand yields greater gains as sample size increases. Our findings contribute to\nrefining LP estimation, enhancing its applicability in analyzing economic\ninterventions and financial market dynamics.",
        "The parameter convergence relies on a stringent persistent excitation (PE)\ncondition in adaptive control. Several works have proposed a memory term in the\nlast decade to translate the PE condition to a feasible finite excitation (FE)\ncondition. This work proposes a combined model reference adaptive control for a\nclass of uncertain nonlinear systems with an unknown control effectiveness\nvector. The closed-loop system is exponentially stable under the FE condition.\nThe exponential rate of convergence is independent of the excitation level of\nthe regressor vector and is lower-bounded in terms of the system parameters and\nuser-designed gains. Numerical simulation is illustrated, validating the\nresults obtained with the proposed adaptive control.",
        "This work addresses the exact characterization of the covariance dynamics\nrelated to linear discrete-time systems subject to both additive and parametric\nstochastic uncertainties that are potentially unbounded. The derived exact\nrepresentation allows to understand how the covariance of the multiplicative\nparametric uncertainties affects the stability of the state covariance dynamics\nthrough a transformation of the parameters covariance matrix, allowing\ntherefore to address the problem of control design for state covariance\ndynamics in this context. Numerical results assess this new characterization by\ncomparing it to the empirical covariance and illustrating the control design\nproblem.",
        "Self-driven photodetectors (SDPDs) transform photon energy into electrical\nenergy without external voltage, which makes them highly advantageous for\napplications such as low-power communication and imaging systems.\nTwo-dimensional materials (2DMs) provide ideal platforms for SDPDs thanks to\ntheir band structures covering ultraviolet to infrared spectrum, strong light\nabsorption efficiencies, and high carrier mobilities. However, the lack of\nstable doping methods and the complicated 2DMs multilayer stacking techniques\npose tremendous difficulties for 2DMs to adopt the same device structures (i.e.\nPN junctions) as bulk materials, and the resultant self-driven performance\nremains at a low level. This work reveals how different asymmetrical effects\ncan be combined to synergistically boost self-driven properties based on\ntypical 2D metal-semiconductor-metal (MSM) photodetectors. Using WSe2 as an\nexemplary 2D material to build MSM photodetectors, the synergistic effect of\nasymmetrical contact electrodes and asymmetrical contact geometries is\ntheoretically and experimentally demonstrated. The open-circuit voltage (Voc)\nof the SDPD reaches 0.58V, with a zero-bias responsivity of 5.77 A\/W and an\non\/off ratio of 1.73*10^5. Additionally, our devices demonstrate potential for\nvisible light communication (VLC) in underwater environments. Our results offer\na promising and efficient strategy for building SDPDs based on various 2DMs and\npave the way toward low-power optoelectronic applications.",
        "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.",
        "We present a closed-form expression for the contribution of surface defects\nto the supersymmetric R\\'enyi entropy in six-dimensional $(2,0)$ theories. Our\nresults show that this defect contribution is a linear function of $1\/n$ and is\ndirectly proportional to $2b-d_2$, where $b$ and $d_2$ are the surface defect\nWeyl anomaly coefficients. We also derive a closed-form expression for the\ndefect contribution to the supersymmetric Casimir energy, which simplifies to\n$-d_2$ (up to a proportionality constant) in the chiral algebra limit.",
        "Data integration is an important step in any data science pipeline where the\nobjective is to unify the information available in different datasets for\ncomprehensive analysis. Full Disjunction, which is an associative extension of\nthe outer join operator, has been shown to be an effective operator for\nintegrating datasets. It fully preserves and combines the available\ninformation. Existing Full Disjunction algorithms only consider the equi-join\nscenario where only tuples having the same value on joining columns are\nintegrated. This, however, does not realistically represent an open data\nscenario, where datasets come from diverse sources with inconsistent values\n(e.g., synonyms, abbreviations, etc.) and with limited metadata. So, joining\njust on equal values severely limits the ability of Full Disjunction to fully\ncombine datasets. Thus, in this work, we propose an extension of Full\nDisjunction to also account for \"fuzzy\" matches among tuples. We present a\nnovel data-driven approach to enable the joining of approximate or fuzzy\nmatches within Full Disjunction. Experimentally, we show that fuzzy Full\nDisjunction does not add significant time overhead over a state-of-the-art Full\nDisjunction implementation and also that it enhances the integration\neffectiveness.",
        "Snake robots enable mobility through extreme terrains and confined\nenvironments in terrestrial and space applications. However, robust perception\nand localization for snake robots remain an open challenge due to the proximity\nof the sensor payload to the ground coupled with a limited field of view. To\naddress this issue, we propose Blind-motion with Intermittently Scheduled Scans\n(BLISS) which combines proprioception-only mobility with intermittent scans to\nbe resilient against both localization failures and collision risks. BLISS is\nformulated as an integrated Task and Motion Planning (TAMP) problem that leads\nto a Chance-Constrained Hybrid Partially Observable Markov Decision Process\n(CC-HPOMDP), known to be computationally intractable due to the curse of\nhistory. Our novelty lies in reformulating CC-HPOMDP as a tractable, convex\nMixed Integer Linear Program. This allows us to solve BLISS-TAMP significantly\nfaster and jointly derive optimal task-motion plans. Simulations and hardware\nexperiments on the EELS snake robot show our method achieves over an order of\nmagnitude computational improvement compared to state-of-the-art POMDP planners\nand $>$ 50\\% better navigation time optimality versus classical two-stage\nplanners.",
        "This paper tackles decentralized continuous task allocation in heterogeneous\nmulti-agent systems. We present a novel framework HIPPO-MAT that integrates\ngraph neural networks (GNN) employing a GraphSAGE architecture to compute\nindependent embeddings on each agent with an Independent Proximal Policy\nOptimization (IPPO) approach for multi-agent deep reinforcement learning. In\nour system, unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs)\nshare aggregated observation data via communication channels while\nindependently processing these inputs to generate enriched state embeddings.\nThis design enables dynamic, cost-optimal, conflict-aware task allocation in a\n3D grid environment without the need for centralized coordination. A modified\nA* path planner is incorporated for efficient routing and collision avoidance.\nSimulation experiments demonstrate scalability with up to 30 agents and\npreliminary real-world validation on JetBot ROS AI Robots, each running its\nmodel on a Jetson Nano and communicating through an ESP-NOW protocol using\nESP32-S3, which confirms the practical viability of the approach that\nincorporates simultaneous localization and mapping (SLAM). Experimental results\nrevealed that our method achieves a high 92.5% conflict-free success rate, with\nonly a 16.49% performance gap compared to the centralized Hungarian method,\nwhile outperforming the heuristic decentralized baseline based on greedy\napproach. Additionally, the framework exhibits scalability with up to 30 agents\nwith allocation processing of 0.32 simulation step time and robustness in\nresponding to dynamically generated tasks.",
        "In the highly interconnected digital landscape of today, safeguarding complex\ninfrastructures against cyber threats has become increasingly challenging due\nto the exponential growth in the number and complexity of vulnerabilities.\nResource constraints necessitate effective vulnerability prioritization\nstrategies, focusing efforts on the most critical risks. This paper presents a\nsystematic literature review of 82 studies, introducing a novel taxonomy that\ncategorizes metrics into severity, exploitability, contextual factors,\npredictive indicators, and aggregation methods. Our analysis reveals\nsignificant gaps in existing approaches and challenges with multi-domain\napplicability. By emphasizing the need for dynamic, context-aware metrics and\nscalable solutions, we provide actionable insights to bridge the gap between\nresearch and real-world applications. This work contributes to the field by\noffering a comprehensive framework for evaluating vulnerability prioritization\nmethodologies and setting a research agenda to advance the state of practice.",
        "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps:\/\/github.com\/sofyc\/ConQuer.",
        "Assertions have been the de facto collateral for simulation-based and formal\nverification of hardware designs for over a decade. The quality of hardware\nverification, i.e., detection and diagnosis of corner-case design bugs, is\ncritically dependent on the quality of the assertions. With the onset of\ngenerative AI such as Transformers and Large-Language Models (LLMs), there has\nbeen a renewed interest in developing novel, effective, and scalable techniques\nof generating functional and security assertions from design source code. While\nthere have been recent works that use commercial-of-the-shelf (COTS) LLMs for\nassertion generation, there is no comprehensive study in quantifying the\neffectiveness of LLMs in generating syntactically and semantically correct\nassertions. In this paper, we first discuss AssertionBench from our prior work,\na comprehensive set of designs and assertions to quantify the goodness of a\nbroad spectrum of COTS LLMs for the task of assertion generations from hardware\ndesign source code. Our key insight was that COTS LLMs are not yet ready for\nprime-time adoption for assertion generation as they generate a considerable\nfraction of syntactically and semantically incorrect assertions. Motivated by\nthe insight, we propose AssertionLLM, a first of its kind LLM model,\nspecifically fine-tuned for assertion generation. Our initial experimental\nresults show that AssertionLLM considerably improves the semantic and syntactic\ncorrectness of the generated assertions over COTS LLMs."
      ]
    }
  },
  {
    "id":2411.04715,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"BigNeuron: a resource to benchmark and predict performance of algorithms for automated tracing of neurons in light microscopy datasets",
    "start_abstract":"BigNeuron is an open community bench-testing platform with the goal of setting open standards for accurate and fast automatic neuron tracing. We gathered a diverse set of image volumes across several species that is representative of the data obtained in many neuroscience laboratories interested in neuron tracing. Here, we report generated gold standard manual annotations for a subset of the available imaging datasets and quantified tracing quality for 35 automatic tracing algorithms. The goal of generating such a hand-curated diverse dataset is to advance the development of tracing algorithms and enable generalizable benchmarking. Together with image quality features, we pooled the data in an interactive web application that enables users and developers to perform principal component analysis, t -distributed stochastic neighbor embedding, correlation and clustering, visualization of imaging and tracing data, and benchmarking of automatic tracing algorithms in user-defined data subsets. The image quality metrics explain most of the variance in the data, followed by neuromorphological features related to neuron size. We observed that diverse algorithms can provide complementary information to obtain accurate results and developed a method to iteratively combine methods and generate consensus reconstructions. The consensus trees obtained provide estimates of the neuron structure ground truth that typically outperform single algorithms in noisy datasets. However, specific algorithms may outperform the consensus tree strategy in specific imaging conditions. Finally, to aid users in predicting the most accurate automatic tracing results without manual annotations for comparison, we used support vector machine regression to predict reconstruction quality given an image volume and a set of automatic tracings. This resource describes a collection of neurons from a variety of light microscopy-based datasets, which can serve as a gold standard for testing automated tracing algorithms, as shown by comparison of the performance of 35 algorithms.",
    "start_categories":[
      "Bioinformatics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Tracing weak neuron fibers"
      ],
      "abstract":[
        "Precise reconstruction of neuronal arbors is important for circuitry mapping. Many auto-tracing algorithms have been developed toward full reconstruction. However, it still challenging to trace the weak signals neurite fibers that often correspond axons.We proposed a method, named NeuMiner, tracing by combining two strategies: an online sample mining strategy and modified gamma transformation. NeuMiner improved recall (voxel values <20) large margin, from 5.1 27.8%. This prominent axons, which increased 6.4 times, compared 2.0 times dendrites. Both strategies were shown be beneficial fiber recognition, they reduced average axonal spatial distances gold standards 46 13%, respectively. The improvement was observed on prevalent automatic can applied any other tracers image types.Source codes are freely available GitHub (https:\/\/github.com\/crazylyf\/neuronet\/tree\/semantic_fnm). Image visualization, preprocessing conducted Vaa3D platform, accessible at repository (https:\/\/github.com\/Vaa3D). All training testing images cropped high-resolution fMOST mouse brains downloaded Brain Library (https:\/\/www.brainimagelibrary.org\/), corresponding https:\/\/doi.brainimagelibrary.org\/doi\/10.35077\/g.25.Supplementary data Bioinformatics online."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "Persistent cohomology operations and Gromov-Hausdorff estimates",
        "Wavelet-Based Multiscale Flow For Realistic Image Deformation in the\n  Large Diffeomorphic Deformation Model Framework",
        "COMODO: Cross-Modal Video-to-IMU Distillation for Efficient Egocentric\n  Human Activity Recognition",
        "Scale-up Unlearnable Examples Learning with High-Performance Computing",
        "Scopes of Alignment",
        "Spin injection and detection in all-van der Waals 2D devices",
        "NeuroADDA: Active Discriminative Domain Adaptation in Connectomic",
        "Sum-of-Squares Data-driven Robustly Stabilizing and Contracting\n  Controller Synthesis for Polynomial Nonlinear Systems",
        "DG-Sensitive Pruning & a Complete Classification of DG Trees and Cycles",
        "Survey of Quantization Techniques for On-Device Vision-based Crack\n  Detection",
        "The singlet scalar state in a chiral ensemble in $SU(2)$ with two\n  fundamental flavours",
        "Game Theory Meets Large Language Models: A Systematic Survey",
        "On the robustness of ChatGPT in teaching Korean Mathematics",
        "Symmetry and Generalisation in Machine Learning",
        "HiSTF Mamba: Hierarchical Spatiotemporal Fusion with Multi-Granular\n  Body-Spatial Modeling for High-Fidelity Text-to-Motion Generation",
        "Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with\n  Drag-Based Control",
        "Ultraviolet spectroscopy of the black hole X-ray binary MAXI J1820+070\n  across a state transition",
        "Replacing the Gallium Oxide Shell with Conductive Ag: Toward a Printable\n  and Recyclable Composite for Highly Stretchable Electronics, Electromagnetic\n  Shielding, and Thermal Interfaces",
        "Antimatter Gravity and the Results of the ALPHA-g Experiment",
        "An Innovative Heterodyne Microwave Interferometer for Plasma Density\n  Measurements on the Madison AWAKE Prototype",
        "Quasiparticle interference and spectral function of the UTe$_2$\n  superconductive surface band",
        "Classification of polynomial models without 2-jet determination in\n  $\\mathbb{C}^3$",
        "Semiparametric Growth-Curve Modeling in Hierarchical, Longitudinal\n  Studies",
        "Formal Model Guided Conformance Testing for Blockchains",
        "Learning Complex Heterogeneous Multimodal Fake News via Social Latent\n  Network Inference",
        "DASKT: A Dynamic Affect Simulation Method for Knowledge Tracing",
        "Time Parameterized Optimal Transport",
        "AlgoPilot: Fully Autonomous Program Synthesis Without Human-Written\n  Programs",
        "Where Are We? Evaluating LLM Performance on African Languages"
      ],
      "abstract":[
        "We establish the foundations of the theory of persistent cohomology\noperations, derive decomposition formulas for wedge sums and products, and\nprove their Gromov-Hausdorff stability. We use these results to construct pairs\nof Riemannian pseudomanifolds for which the Gromov-Hausdorff estimates derived\nfrom persistent cohomology operations are strictly sharper than those obtained\nusing persistent homology.",
        "Estimating accurate high-dimensional transformations remains very\nchallenging, especially in a clinical setting. In this paper, we introduce a\nmultiscale parameterization of deformations to enhance registration and atlas\nestimation in the Large Deformation Diffeomorphic Metric Mapping framework.\nUsing the Haar wavelet transform, a multiscale representation of the initial\nvelocity fields is computed to optimize transformations in a coarse-to-fine\nfashion. This additional layer of spatial regularization does not modify the\nunderlying model of deformations. As such, it preserves the original kernel\nHilbert space structure of the velocity fields, enabling the algorithm to\nperform efficient gradient descent. Numerical experiments on several datasets,\nincluding abnormal fetal brain images, show that compared to the original\nalgorithm, the coarse-to-fine strategy reaches higher performance and yields\ntemplate images that preserve important details while avoiding unrealistic\nfeatures. This highly versatile strategy can easily be applied to other\nmathematical frameworks for almost no additional computational cost.",
        "Egocentric video-based models capture rich semantic information and have\ndemonstrated strong performance in human activity recognition (HAR). However,\ntheir high power consumption, privacy concerns, and dependence on lighting\nconditions limit their feasibility for continuous on-device recognition. In\ncontrast, inertial measurement unit (IMU) sensors offer an energy-efficient and\nprivacy-preserving alternative, yet they suffer from limited large-scale\nannotated datasets, leading to weaker generalization in downstream tasks. To\nbridge this gap, we propose COMODO, a cross-modal self-supervised distillation\nframework that transfers rich semantic knowledge from the video modality to the\nIMU modality without requiring labeled annotations. COMODO leverages a\npretrained and frozen video encoder to construct a dynamic instance queue,\naligning the feature distributions of video and IMU embeddings. By distilling\nknowledge from video representations, our approach enables the IMU encoder to\ninherit rich semantic information from video while preserving its efficiency\nfor real-world applications. Experiments on multiple egocentric HAR datasets\ndemonstrate that COMODO consistently improves downstream classification\nperformance, achieving results comparable to or exceeding fully supervised\nfine-tuned models. Moreover, COMODO exhibits strong cross-dataset\ngeneralization. Benefiting from its simplicity, our method is also generally\napplicable to various video and time-series pre-trained models, offering the\npotential to leverage more powerful teacher and student foundation models in\nfuture research. The code is available at https:\/\/github.com\/Breezelled\/COMODO .",
        "Recent advancements in AI models are structured to retain user interactions,\nwhich could inadvertently include sensitive healthcare data. In the healthcare\nfield, particularly when radiologists use AI-driven diagnostic tools hosted on\nonline platforms, there is a risk that medical imaging data may be repurposed\nfor future AI training without explicit consent, spotlighting critical privacy\nand intellectual property concerns around healthcare data usage. Addressing\nthese privacy challenges, a novel approach known as Unlearnable Examples (UEs)\nhas been introduced, aiming to make data unlearnable to deep learning models. A\nprominent method within this area, called Unlearnable Clustering (UC), has\nshown improved UE performance with larger batch sizes but was previously\nlimited by computational resources. To push the boundaries of UE performance\nwith theoretically unlimited resources, we scaled up UC learning across various\ndatasets using Distributed Data Parallel (DDP) training on the Summit\nsupercomputer. Our goal was to examine UE efficacy at high-performance\ncomputing (HPC) levels to prevent unauthorized learning and enhance data\nsecurity, particularly exploring the impact of batch size on UE's\nunlearnability. Utilizing the robust computational capabilities of the Summit,\nextensive experiments were conducted on diverse datasets such as Pets,\nMedMNist, Flowers, and Flowers102. Our findings reveal that both overly large\nand overly small batch sizes can lead to performance instability and affect\naccuracy. However, the relationship between batch size and unlearnability\nvaried across datasets, highlighting the necessity for tailored batch size\nstrategies to achieve optimal data protection. Our results underscore the\ncritical role of selecting appropriate batch sizes based on the specific\ncharacteristics of each dataset to prevent learning and ensure data security in\ndeep learning applications.",
        "Much of the research focus on AI alignment seeks to align large language\nmodels and other foundation models to the context-less and generic values of\nhelpfulness, harmlessness, and honesty. Frontier model providers also strive to\nalign their models with these values. In this paper, we motivate why we need to\nmove beyond such a limited conception and propose three dimensions for doing\nso. The first scope of alignment is competence: knowledge, skills, or behaviors\nthe model must possess to be useful for its intended purpose. The second scope\nof alignment is transience: either semantic or episodic depending on the\ncontext of use. The third scope of alignment is audience: either mass, public,\nsmall-group, or dyadic. At the end of the paper, we use the proposed framework\nto position some technologies and workflows that go beyond prevailing notions\nof alignment.",
        "In this work we report efficient out-of-plane spin injection and detection in\nan all-van der Waals based heterostructure using only exfoliated 2D materials.\nWe demonstrate spin injection by measuring spin-valve and Hanle signals in\nnon-local transport in a stack of Fe$_3$GeTe$_2$ (FGT), hexagonal boron nitride\n(hBN) and graphene layers. FGT flakes form the spin aligning electrodes\nnecessary to inject and detect spins in the graphene channel. The hBN tunnel\nbarrier provides a high-quality interface between the ferromagnetic electrodes\nand graphene, eliminating the conductivity mismatch problem, thus ensuring\nefficient spin injection and detection with spin injection efficiencies of up\nto $P=40$\\%. Our results demonstrate that FGT\/hBN\/graphene heterostructures\nform a promising platform for realizing 2D van der Waals spintronic devices.",
        "Training segmentation models from scratch has been the standard approach for\nnew electron microscopy connectomics datasets. However, leveraging pretrained\nmodels from existing datasets could improve efficiency and performance in\nconstrained annotation budget. In this study, we investigate domain adaptation\nin connectomics by analyzing six major datasets spanning different organisms.\nWe show that, Maximum Mean Discrepancy (MMD) between neuron image distributions\nserves as a reliable indicator of transferability, and identifies the optimal\nsource domain for transfer learning. Building on this, we introduce NeuroADDA,\na method that combines optimal domain selection with source-free active\nlearning to effectively adapt pretrained backbones to a new dataset. NeuroADDA\nconsistently outperforms training from scratch across diverse datasets and\nfine-tuning sample sizes, with the largest gain observed at $n=4$ samples with\na 25-67\\% reduction in Variation of Information. Finally, we show that our\nanalysis of distributional differences among neuron images from multiple\nspecies in a learned feature space reveals that these domain \"distances\"\ncorrelate with phylogenetic distance among those species.",
        "This work presents a computationally efficient approach to data-driven robust\ncontracting controller synthesis for polynomial control-affine systems based on\na sum-of-squares program. In particular, we consider the case in which a system\nalternates between periods of high-quality sensor data and low-quality sensor\ndata. In the high-quality sensor data regime, we focus on robust system\nidentification based on the data informativity framework. In low-quality sensor\ndata regimes we employ a robustly contracting controller that is synthesized\nonline by solving a sum-of-squares program based on data acquired in the\nhigh-quality regime, so as to limit state deviation until high-quality data is\navailable. This approach is motivated by real-life control applications in\nwhich systems experience periodic data blackouts or occlusion, such as\nautonomous vehicles undergoing loss of GPS signal or solar glare in machine\nvision systems. We apply our approach to a planar unmanned aerial vehicle model\nsubject to an unknown wind field, demonstrating its uses for verifiably tight\ncontrol on trajectory deviation.",
        "Given a squarefree monomial ideal $I$ of a polynomial ring $Q$, we show that\nif the minimal free resolution $\\mathbb{F}$ of $Q\/I$ admits the structure of a\ndifferential graded (dg) algebra, then so does any \"pruning\" of $\\mathbb{F}$.\nAs an application, we show that if $Q\/\\mathcal{F}(\\Delta)$, the quotient of the\nambient polynomial ring by the facet ideal $\\mathcal{F}(\\Delta)$ of a\nsimplicial complex $\\Delta$, is minimally resolved by a dg algebra, then so is\nthe quotient by the facet ideal of each facet-induced subcomplex of $\\Delta$\n(over the smaller polynomial ring). Along with techniques from discrete Morse\ntheory and homological algebra, this allows us to give complete classifications\nof the trees and cycles $G$ with $Q\/I_G$ minimally resolved by a dg algebra in\nterms of the diameter of $G$, where $I_G$ is the edge ideal of $G$.",
        "Structural Health Monitoring (SHM) ensures the safety and longevity of\ninfrastructure by enabling timely damage detection. Vision-based crack\ndetection, combined with UAVs, addresses the limitations of traditional\nsensor-based SHM methods but requires the deployment of efficient deep learning\nmodels on resource-constrained devices. This study evaluates two lightweight\nconvolutional neural network models, MobileNetV1x0.25 and MobileNetV2x0.5,\nacross TensorFlow, PyTorch, and Open Neural Network Exchange platforms using\nthree quantization techniques: dynamic quantization, post-training quantization\n(PTQ), and quantization-aware training (QAT). Results show that QAT\nconsistently achieves near-floating-point accuracy, such as an F1-score of\n0.8376 for MBNV2x0.5 with Torch-QAT, while maintaining efficient resource\nusage. PTQ significantly reduces memory and energy consumption but suffers from\naccuracy loss, particularly in TensorFlow. Dynamic quantization preserves\naccuracy but faces deployment challenges on PyTorch. By leveraging QAT, this\nwork enables real-time, low-power crack detection on UAVs, enhancing safety,\nscalability, and cost-efficiency in SHM applications, while providing insights\ninto balancing accuracy and efficiency across different platforms for\nautonomous inspections.",
        "Composite Higgs models are a class of Beyond the Standard Model (BSM) models\nproposed to address the hierarchy and naturalness problems associated with the\nStandard Model (SM) Higgs. A new QCD-like strongly interacting sector based on\n$SU(2)$ with two fundamental flavours can be used to build a composite Higgs\nmodel which is not yet ruled out by experiment. The role of the singlet scalar\nresonance will affect Higgs phenomenology at the LHC. In this project our goal\nis to understand the properties of the singlet scalar state in the new strongly\ninteracting sector in isolation as a first step to understanding the role of\nthis state in composite Higgs models. We present here the first lattice results\nfor the mass of the $\\sigma$ in $SU(2)$ with two fundamental flavours using\nexponential clover Wilson fermions.",
        "Game theory establishes a fundamental framework for analyzing strategic\ninteractions among rational decision-makers. The rapid advancement of large\nlanguage models (LLMs) has sparked extensive research exploring the\nintersection of these two fields. Specifically, game-theoretic methods are\nbeing applied to evaluate and enhance LLM capabilities, while LLMs themselves\nare reshaping classic game models. This paper presents a comprehensive survey\nof the intersection of these fields, exploring a bidirectional relationship\nfrom three perspectives: (1) Establishing standardized game-based benchmarks\nfor evaluating LLM behavior; (2) Leveraging game-theoretic methods to improve\nLLM performance through algorithmic innovations; (3) Characterizing the\nsocietal impacts of LLMs through game modeling. Among these three aspects, we\nalso highlight how the equilibrium analysis for traditional game models is\nimpacted by LLMs' advanced language understanding, which in turn extends the\nstudy of game theory. Finally, we identify key challenges and future research\ndirections, assessing their feasibility based on the current state of the\nfield. By bridging theoretical rigor with emerging AI capabilities, this survey\naims to foster interdisciplinary collaboration and drive progress in this\nevolving research area.",
        "ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education.",
        "This work is about understanding the impact of invariance and equivariance on\ngeneralisation in supervised learning. We use the perspective afforded by an\naveraging operator to show that for any predictor that is not equivariant,\nthere is an equivariant predictor with strictly lower test risk on all\nregression problems where the equivariance is correctly specified. This\nconstitutes a rigorous proof that symmetry, in the form of invariance or\nequivariance, is a useful inductive bias.\n  We apply these ideas to equivariance and invariance in random design least\nsquares and kernel ridge regression respectively. This allows us to specify the\nreduction in expected test risk in more concrete settings and express it in\nterms of properties of the group, the model and the data.\n  Along the way, we give examples and additional results to demonstrate the\nutility of the averaging operator approach in analysing equivariant predictors.\nIn addition, we adopt an alternative perspective and formalise the common\nintuition that learning with invariant models reduces to a problem in terms of\norbit representatives. The formalism extends naturally to a similar intuition\nfor equivariant models. We conclude by connecting the two perspectives and\ngiving some ideas for future work.",
        "Text-to-motion generation is a rapidly growing field at the nexus of\nmultimodal learning and computer graphics, promising flexible and\ncost-effective applications in gaming, animation, robotics, and virtual\nreality. Existing approaches often rely on simple spatiotemporal stacking,\nwhich introduces feature redundancy, while subtle joint-level details remain\noverlooked from a spatial perspective. To this end, we propose a novel HiSTF\nMamba framework. The framework is composed of three key modules: Dual-Spatial\nMamba, Bi-Temporal Mamba, and Dynamic Spatiotemporal Fusion Module (DSFM).\nDual-Spatial Mamba incorporates ``Part-based + Whole-based'' parallel modeling\nto represent both whole-body coordination and fine-grained joint dynamics.\nBi-Temporal Mamba adopts a bidirectional scanning strategy, effectively\nencoding short-term motion details and long-term dependencies. DSFM further\nperforms redundancy removal and extraction of complementary information for\ntemporal features, then fuses them with spatial features, yielding an\nexpressive spatio-temporal representation. Experimental results on the\nHumanML3D dataset demonstrate that HiSTF Mamba achieves state-of-the-art\nperformance across multiple metrics. In particular, it reduces the FID score\nfrom 0.283 to 0.189, a relative decrease of nearly 30%. These findings validate\nthe effectiveness of HiSTF Mamba in achieving high fidelity and strong semantic\nalignment in text-to-motion generation.",
        "Single-image 3D generation has emerged as a prominent research topic, playing\na vital role in virtual reality, 3D modeling, and digital content creation.\nHowever, existing methods face challenges such as a lack of multi-view\ngeometric consistency and limited controllability during the generation\nprocess, which significantly restrict their usability. % To tackle these\nchallenges, we introduce Dragen3D, a novel approach that achieves geometrically\nconsistent and controllable 3D generation leveraging 3D Gaussian Splatting\n(3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS\nVAE), which encodes a point cloud and a single image into anchor latents and\ndecode these latents into 3DGS, enabling efficient latent-space generation. To\nenable multi-view geometry consistent and controllable generation, we propose a\nSeed-Point-Driven strategy: first generate sparse seed points as a coarse\ngeometry representation, then map them to anchor latents via the Seed-Anchor\nMapping Module. Geometric consistency is ensured by the easily learned sparse\nseed points, and users can intuitively drag the seed points to deform the final\n3DGS geometry, with changes propagated through the anchor latents. To the best\nof our knowledge, we are the first to achieve geometrically controllable 3D\nGaussian generation and editing without relying on 2D diffusion priors,\ndelivering comparable 3D generation quality to state-of-the-art methods.",
        "We present ultraviolet (UV) spectroscopic observations covering three\ndistinct accretion states of the low-mass X-ray binary (LMXB) MAXI J1820+070:\nthe luminous hard state, a hard-intermediate state and the soft state. Our\nobservations were obtained during the 2018 eruption of MAXI J1820+070 with the\nHubble Space Telescope (HST) and AstroSat observatory. The extinction towards\nthe source turns out to be low - $\\rm E_{B-V} = 0.2 \\pm 0.05$ - making it one\nof the best UV accretion laboratories among LMXBs. Remarkably, we observe only\nmoderate differences between all three states, with all spectra displaying\nsimilar continuum shapes and emission lines. Moreover, the continua are not\nwell-described by physically plausible irradiated disc models. All of this\nchallenges the standard reprocessing picture for UV emission from erupting\nLMXBs. The UV emission lines are double-peaked, with high-ionization lines\ndisplaying higher peak-to-peak velocities. None of the lines display obvious\noutflow signatures, even though blue-shifted absorption features have been seen\nin optical and near-infrared lines during the hard state. The emission line\nratios are consistent with normal abundances, suggesting that the donor mass at\nbirth was low enough to avoid CNO processing ($\\rm M_{2,i} \\lesssim 1.0 - 1.5\n{\\mathrm M_{\\odot}}$). Finally, we study the evolution of UV variability in our\ntime-resolved HST observations (hard and hard-intermediate states). All UV\npower spectra can be modelled with a broken power-law, superposed on which we\ntentatively detect the $\\simeq 18$s quasi-periodic oscillation (QPO) that has\nbeen seen in other spectral bands.",
        "Liquid metal (LM)-based composites hold promise for soft electronics due to\ntheir high conductivity and fluidic nature. However, the presence of\n{\\alpha}_Ga2O3 and GaOOH layers around LM droplets impairs conductivity and\nperformance. We tackle this issue by replacing the oxide layer with conductive\nsilver (Ag) using an ultrasonic_assisted galvanic replacement reaction. The\nAg_coated nanoparticles form aggregated, porous microparticles that are mixed\nwith styrene_isoprene_styrene (SIS) polymers, resulting in a digitally\nprintable composite with superior electrical conductivity and electromechanical\nproperties compared to conventional fillers. Adding more LM enhances these\nproperties further. The composite achieves EMI shielding effectiveness (SE)\nexceeding 75 dB in the X_band frequency range, even at 200 per cent strain,\nmeeting stringent military and medical standards. It is applicable in wireless\ncommunications and Bluetooth signal blocking and as a thermal interface\nmaterial (TIM). Additionally, we highlight its recyclability using a\nbiodegradable solvent, underscoring its eco_friendly potential. This composite\nrepresents a significant advancement in stretchable electronics and EMI\nshielding, with implications for wearable and bioelectronic applications.",
        "By combining general relativity and CPT symmetry, the theory of CPT gravity\npredicts gravitational repulsion between matter and CPT-transformed matter,\ni.e. antimatter inhabiting an inverted space-time. Such repulsive gravity\nturned out to be an excellent candidate for explaining the accelerated\nexpansion of the Universe, without the need for dark energy. The recent results\nof the ALPHA-g experiment, which show gravitational attraction between\nantihydrogen atoms and the Earth, seem to undermine this success in the\ncosmological field. Analyzing the above theory, we find two solutions that can\nbe consistent with the experimental results, while preserving the large-scale\ngravitational repulsion. The first highlights how repulsive gravity can be the\nresult of the interaction with an inverted space-time, but occupied by matter\nand not antimatter, and therefore the antimatter present in our space-time has\nno reason to exhibit gravitational repulsion. The second retains the original\nCPT transformation, resulting in repulsive gravity between matter and\nantimatter, but with the caveat that antimatter immersed in our space-time\ncannot exhibit the PT transformation which is the cause of the repulsion.\nFinally, it is shown that, in a Newtonian approximation of the geodesic\nequation, time reversal is not a necessary operation for repulsive gravity,\ntherefore opening the possibility of an expanding cosmos with a single time\ndirection.",
        "The Madison AWAKE Prototype (MAP) is a high-power, high-density helicon\nplasma experiment. The project's main goal is to develop a scalable plasma\nsource for use in a beam-driven plasma wakefield accelerator as part of the\nAWAKE project. We measure the plasma density with a new heterodyne microwave\ninterferometer that features several improvements over traditional approaches.\nThe design uses a single microwave source combined with an upconverter to avoid\nfrequency drift and reduce overall cost. Elliptical mirrors focus the probe\nbeam into the plasma and guide it back to the receiver. The transmitter and\nreceiver along with the measurement electronics are co-located in a small\nenclosure and are assisted by two small mirrors on the opposite side of MAP.\nBoth halves of the system move independently on computer-controlled motion\nplatforms. This setup enables fast repositioning of the interferometer to\nmeasure at any axial location despite the magnets, wiring and structural\nsupports that would block movement of a waveguide-based system. A high-speed,\nhigh-precision mixed signal circuit and FPGA analyze the probe signal directly\nin the enclosure which obviates the need for a digitizer or oscilloscope. The\ninterferometer resolves phase shifts down to one hundredth of a fringe,\nresulting in a line-averaged resolution of $1.5\\mathrm{\\cdot 10^{17}\\;\nm^{-3}}$. The system provides a real-time measurement every $5\\;\\mathrm{\\mu s}$\nup into the mid $\\mathrm{10^{19}\\; m^{-3}}$ density range with a noise level of\n$1.0\\mathrm{\\cdot 10^{17}\\; m^{-3}}$.",
        "We compute the (0-11) surface spectral function, the surface density of\nstates (DOS), and the quasiparticle interference (QPI) patterns, both in the\nnormal state and superconducting (SC) state of UTe$_2$. We consider all\npossible non-chiral and chiral order parameters (OPs) that could in principle\ndescribe the superconductivity in this compound. We describe the formation of\nsurface states whose maximum intensity energy depends on the nature of the\npairing. We study also the QPI patterns resulting from the scattering of these\nsurface states. We show that the main feature distinguishing between various\nOPs is a QPI peak that is only observed experimentally in the superconducting\nstate. The energy dispersion and the stability of this peak is consistent among\nthe non-chiral OPs only with a $B_{3u}$ pairing. Moreover, $B_{3u}$ is the only\nnon-chiral pairing that shows a peak at zero energy in the DOS, consistent with\nthe experimental observations.",
        "An intriguing phenomenon regarding Levi-degenerate hypersurfaces is the\nexistence of nontrivial infinitesimal symmetries with vanishing 2-jets at a\npoint. In this work we consider polynomial models of Levi-degenerate real\nhypersurfaces in $\\mathbb{C}^3$ of finite Catlin multitype. Exploiting the\nstructure of the corresponding Lie algebra, we characterize completely models\nwithout 2-jet determination, including an explicit description of their\nsymmetry algebras.",
        "Modeling of growth (or decay) curves arises in many fields such as\nmicrobiology, epidemiology, marketing, and econometrics. Parametric forms like\nLogistic and Gompertz are often used for modeling such monotonic patterns.\nWhile useful for compact description, the real-life growth curves rarely follow\nthese parametric forms perfectly. Therefore, the curve estimation methods that\nstrike a balance between prior information in the parametric form and fidelity\nwith the observed data are preferred. In hierarchical, longitudinal studies the\ninterest lies in comparing the growth curves of different groups while\naccounting for the differences between the within-group subjects. This article\ndescribes a flexible state space modeling framework that enables semiparametric\ngrowth curve modeling for the data generated from hierarchical, longitudinal\nstudies. The methodology, a type of functional mixed effects modeling, is\nillustrated with a real-life example of bacterial growth in different settings.",
        "Modern blockchains increasingly consist of multiple clients that implement a\nsingle blockchain protocol. If there is a semantic mismatch between the\nprotocol implementations, the blockchain can permanently split and introduce\nnew attack vectors. Current ad-hoc test suites for client implementations are\nnot sufficient to ensure a high degree of protocol conformance. As an\nalternative, we present a framework that performs protocol conformance testing\nusing a formal model of the protocol and an implementation running inside a\ndeterministic blockchain simulator. Our framework consists of two complementary\nworkflows that use the components as trace generators and checkers. Our insight\nis that both workflows are needed to detect all types of violations. We have\napplied and demonstrated the utility of our framework on an industrial strength\nconsensus protocol.",
        "With the diversification of online social platforms, news dissemination has\nbecome increasingly complex, heterogeneous, and multimodal, making the fake\nnews detection task more challenging and crucial. Previous works mainly focus\non obtaining social relationships of news via retweets, limiting the accurate\ndetection when real cascades are inaccessible. Given the proven assessment of\nthe spreading influence of events, this paper proposes a method called HML\n(Complex Heterogeneous Multimodal Fake News Detection method via Latent Network\nInference). Specifically, an improved social latent network inference strategy\nis designed to estimate the maximum likelihood of news influences under the\nsame event. Meanwhile, a novel heterogeneous graph is built based on social\nattributes for multimodal news under different events. Further, to better\naggregate the relationships among heterogeneous multimodal features, this paper\nproposes a self-supervised-based multimodal content learning strategy, to\nenhance, align, fuse and compare heterogeneous modal contents. Based above, a\npersonalized heterogeneous graph representation learning is designed to\nclassify fake news. Extensive experiments demonstrate that the proposed method\noutperforms the SOTA in real social media news datasets.",
        "Knowledge Tracing (KT) predicts future performance by modeling students'\nhistorical interactions, and understanding students' affective states can\nenhance the effectiveness of KT, thereby improving the quality of education.\nAlthough traditional KT values students' cognition and learning behaviors,\nefficient evaluation of students' affective states and their application in KT\nstill require further exploration due to the non-affect-oriented nature of the\ndata and budget constraints. To address this issue, we propose a\ncomputation-driven approach, Dynamic Affect Simulation Knowledge Tracing\n(DASKT), to explore the impact of various student affective states (such as\nfrustration, concentration, boredom, and confusion) on their knowledge states.\nIn this model, we first extract affective factors from students'\nnon-affect-oriented behavioral data, then use clustering and spatiotemporal\nsequence modeling to accurately simulate students' dynamic affect changes when\ndealing with different problems. Subsequently, {\\color{blue}we incorporate\naffect with time-series analysis to improve the model's ability to infer\nknowledge states over time and space.} Extensive experimental results on two\npublic real-world educational datasets show that DASKT can achieve more\nreasonable knowledge states under the effect of students' affective states.\nMoreover, DASKT outperforms the most advanced KT methods in predicting student\nperformance. Our research highlights a promising avenue for future KT studies,\nfocusing on achieving high interpretability and accuracy.",
        "Optimal transport has gained significant attention in recent years due to its\neffectiveness in deep learning and computer vision. Its descendant metric, the\nWasserstein distance, has been particularly successful in measuring\ndistribution dissimilarities. While extensive research has focused on optimal\ntransport and its regularized variants (such as entropy, sparsity, and capacity\nconstraints) the role of time has been largely overlooked. However, time is a\ncritical factor in real world transport problems.\n  In this work, we introduce a time parameterized formulation of the optimal\ntransport problem, incorporating a time variable t to represent sequential\nsteps and enforcing specific constraints at each step. We propose a systematic\nmethod to solve a special subproblem and develop a heuristic search algorithm\nthat achieves nearly optimal solutions while significantly reducing\ncomputational time.",
        "Program synthesis has traditionally relied on human-provided specifications,\nexamples, or prior knowledge to generate functional algorithms. Existing\nmethods either emulate human-written algorithms or solve specific tasks without\ngenerating reusable programmatic logic, limiting their ability to create novel\nalgorithms. We introduce AlgoPilot, a groundbreaking approach for fully\nautomated program synthesis without human-written programs or trajectories.\nAlgoPilot leverages reinforcement learning (RL) guided by a Trajectory Language\nModel (TLM) to synthesize algorithms from scratch. The TLM, trained on\ntrajectories generated by random Python functions, serves as a soft constraint\nduring the RL process, aligning generated sequences with patterns likely to\nrepresent valid algorithms. Using sorting as a test case, AlgoPilot\ndemonstrates its ability to generate trajectories that are interpretable as\nclassical algorithms, such as Bubble Sort, while operating without prior\nalgorithmic knowledge. This work establishes a new paradigm for algorithm\ndiscovery and lays the groundwork for future advancements in autonomous program\nsynthesis.",
        "Africa's rich linguistic heritage remains underrepresented in NLP, largely\ndue to historical policies that favor foreign languages and create significant\ndata inequities. In this paper, we integrate theoretical insights on Africa's\nlanguage landscape with an empirical evaluation using Sahara - a comprehensive\nbenchmark curated from large-scale, publicly accessible datasets capturing the\ncontinent's linguistic diversity. By systematically assessing the performance\nof leading large language models (LLMs) on Sahara, we demonstrate how\npolicy-induced data variations directly impact model effectiveness across\nAfrican languages. Our findings reveal that while a few languages perform\nreasonably well, many Indigenous languages remain marginalized due to sparse\ndata. Leveraging these insights, we offer actionable recommendations for policy\nreforms and inclusive data practices. Overall, our work underscores the urgent\nneed for a dual approach - combining theoretical understanding with empirical\nevaluation - to foster linguistic diversity in AI for African communities."
      ]
    }
  },
  {
    "id":2411.04715,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Tracing weak neuron fibers",
    "start_abstract":"Precise reconstruction of neuronal arbors is important for circuitry mapping. Many auto-tracing algorithms have been developed toward full reconstruction. However, it still challenging to trace the weak signals neurite fibers that often correspond axons.We proposed a method, named NeuMiner, tracing by combining two strategies: an online sample mining strategy and modified gamma transformation. NeuMiner improved recall (voxel values <20) large margin, from 5.1 27.8%. This prominent axons, which increased 6.4 times, compared 2.0 times dendrites. Both strategies were shown be beneficial fiber recognition, they reduced average axonal spatial distances gold standards 46 13%, respectively. The improvement was observed on prevalent automatic can applied any other tracers image types.Source codes are freely available GitHub (https:\/\/github.com\/crazylyf\/neuronet\/tree\/semantic_fnm). Image visualization, preprocessing conducted Vaa3D platform, accessible at repository (https:\/\/github.com\/Vaa3D). All training testing images cropped high-resolution fMOST mouse brains downloaded Brain Library (https:\/\/www.brainimagelibrary.org\/), corresponding https:\/\/doi.brainimagelibrary.org\/doi\/10.35077\/g.25.Supplementary data Bioinformatics online.",
    "start_categories":[
      "Imaging"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "BigNeuron: a resource to benchmark and predict performance of algorithms for automated tracing of neurons in light microscopy datasets"
      ],
      "abstract":[
        "BigNeuron is an open community bench-testing platform with the goal of setting open standards for accurate and fast automatic neuron tracing. We gathered a diverse set of image volumes across several species that is representative of the data obtained in many neuroscience laboratories interested in neuron tracing. Here, we report generated gold standard manual annotations for a subset of the available imaging datasets and quantified tracing quality for 35 automatic tracing algorithms. The goal of generating such a hand-curated diverse dataset is to advance the development of tracing algorithms and enable generalizable benchmarking. Together with image quality features, we pooled the data in an interactive web application that enables users and developers to perform principal component analysis, t -distributed stochastic neighbor embedding, correlation and clustering, visualization of imaging and tracing data, and benchmarking of automatic tracing algorithms in user-defined data subsets. The image quality metrics explain most of the variance in the data, followed by neuromorphological features related to neuron size. We observed that diverse algorithms can provide complementary information to obtain accurate results and developed a method to iteratively combine methods and generate consensus reconstructions. The consensus trees obtained provide estimates of the neuron structure ground truth that typically outperform single algorithms in noisy datasets. However, specific algorithms may outperform the consensus tree strategy in specific imaging conditions. Finally, to aid users in predicting the most accurate automatic tracing results without manual annotations for comparison, we used support vector machine regression to predict reconstruction quality given an image volume and a set of automatic tracings. This resource describes a collection of neurons from a variety of light microscopy-based datasets, which can serve as a gold standard for testing automated tracing algorithms, as shown by comparison of the performance of 35 algorithms."
      ],
      "categories":[
        "Bioinformatics"
      ]
    },
    "list":{
      "title":[
        "Persistent cohomology operations and Gromov-Hausdorff estimates",
        "Wavelet-Based Multiscale Flow For Realistic Image Deformation in the\n  Large Diffeomorphic Deformation Model Framework",
        "COMODO: Cross-Modal Video-to-IMU Distillation for Efficient Egocentric\n  Human Activity Recognition",
        "Scale-up Unlearnable Examples Learning with High-Performance Computing",
        "Scopes of Alignment",
        "Spin injection and detection in all-van der Waals 2D devices",
        "NeuroADDA: Active Discriminative Domain Adaptation in Connectomic",
        "Sum-of-Squares Data-driven Robustly Stabilizing and Contracting\n  Controller Synthesis for Polynomial Nonlinear Systems",
        "DG-Sensitive Pruning & a Complete Classification of DG Trees and Cycles",
        "Survey of Quantization Techniques for On-Device Vision-based Crack\n  Detection",
        "The singlet scalar state in a chiral ensemble in $SU(2)$ with two\n  fundamental flavours",
        "Game Theory Meets Large Language Models: A Systematic Survey",
        "On the robustness of ChatGPT in teaching Korean Mathematics",
        "Symmetry and Generalisation in Machine Learning",
        "HiSTF Mamba: Hierarchical Spatiotemporal Fusion with Multi-Granular\n  Body-Spatial Modeling for High-Fidelity Text-to-Motion Generation",
        "Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with\n  Drag-Based Control",
        "Ultraviolet spectroscopy of the black hole X-ray binary MAXI J1820+070\n  across a state transition",
        "Replacing the Gallium Oxide Shell with Conductive Ag: Toward a Printable\n  and Recyclable Composite for Highly Stretchable Electronics, Electromagnetic\n  Shielding, and Thermal Interfaces",
        "Antimatter Gravity and the Results of the ALPHA-g Experiment",
        "An Innovative Heterodyne Microwave Interferometer for Plasma Density\n  Measurements on the Madison AWAKE Prototype",
        "Quasiparticle interference and spectral function of the UTe$_2$\n  superconductive surface band",
        "Classification of polynomial models without 2-jet determination in\n  $\\mathbb{C}^3$",
        "Semiparametric Growth-Curve Modeling in Hierarchical, Longitudinal\n  Studies",
        "Formal Model Guided Conformance Testing for Blockchains",
        "Learning Complex Heterogeneous Multimodal Fake News via Social Latent\n  Network Inference",
        "DASKT: A Dynamic Affect Simulation Method for Knowledge Tracing",
        "Time Parameterized Optimal Transport",
        "AlgoPilot: Fully Autonomous Program Synthesis Without Human-Written\n  Programs",
        "Where Are We? Evaluating LLM Performance on African Languages"
      ],
      "abstract":[
        "We establish the foundations of the theory of persistent cohomology\noperations, derive decomposition formulas for wedge sums and products, and\nprove their Gromov-Hausdorff stability. We use these results to construct pairs\nof Riemannian pseudomanifolds for which the Gromov-Hausdorff estimates derived\nfrom persistent cohomology operations are strictly sharper than those obtained\nusing persistent homology.",
        "Estimating accurate high-dimensional transformations remains very\nchallenging, especially in a clinical setting. In this paper, we introduce a\nmultiscale parameterization of deformations to enhance registration and atlas\nestimation in the Large Deformation Diffeomorphic Metric Mapping framework.\nUsing the Haar wavelet transform, a multiscale representation of the initial\nvelocity fields is computed to optimize transformations in a coarse-to-fine\nfashion. This additional layer of spatial regularization does not modify the\nunderlying model of deformations. As such, it preserves the original kernel\nHilbert space structure of the velocity fields, enabling the algorithm to\nperform efficient gradient descent. Numerical experiments on several datasets,\nincluding abnormal fetal brain images, show that compared to the original\nalgorithm, the coarse-to-fine strategy reaches higher performance and yields\ntemplate images that preserve important details while avoiding unrealistic\nfeatures. This highly versatile strategy can easily be applied to other\nmathematical frameworks for almost no additional computational cost.",
        "Egocentric video-based models capture rich semantic information and have\ndemonstrated strong performance in human activity recognition (HAR). However,\ntheir high power consumption, privacy concerns, and dependence on lighting\nconditions limit their feasibility for continuous on-device recognition. In\ncontrast, inertial measurement unit (IMU) sensors offer an energy-efficient and\nprivacy-preserving alternative, yet they suffer from limited large-scale\nannotated datasets, leading to weaker generalization in downstream tasks. To\nbridge this gap, we propose COMODO, a cross-modal self-supervised distillation\nframework that transfers rich semantic knowledge from the video modality to the\nIMU modality without requiring labeled annotations. COMODO leverages a\npretrained and frozen video encoder to construct a dynamic instance queue,\naligning the feature distributions of video and IMU embeddings. By distilling\nknowledge from video representations, our approach enables the IMU encoder to\ninherit rich semantic information from video while preserving its efficiency\nfor real-world applications. Experiments on multiple egocentric HAR datasets\ndemonstrate that COMODO consistently improves downstream classification\nperformance, achieving results comparable to or exceeding fully supervised\nfine-tuned models. Moreover, COMODO exhibits strong cross-dataset\ngeneralization. Benefiting from its simplicity, our method is also generally\napplicable to various video and time-series pre-trained models, offering the\npotential to leverage more powerful teacher and student foundation models in\nfuture research. The code is available at https:\/\/github.com\/Breezelled\/COMODO .",
        "Recent advancements in AI models are structured to retain user interactions,\nwhich could inadvertently include sensitive healthcare data. In the healthcare\nfield, particularly when radiologists use AI-driven diagnostic tools hosted on\nonline platforms, there is a risk that medical imaging data may be repurposed\nfor future AI training without explicit consent, spotlighting critical privacy\nand intellectual property concerns around healthcare data usage. Addressing\nthese privacy challenges, a novel approach known as Unlearnable Examples (UEs)\nhas been introduced, aiming to make data unlearnable to deep learning models. A\nprominent method within this area, called Unlearnable Clustering (UC), has\nshown improved UE performance with larger batch sizes but was previously\nlimited by computational resources. To push the boundaries of UE performance\nwith theoretically unlimited resources, we scaled up UC learning across various\ndatasets using Distributed Data Parallel (DDP) training on the Summit\nsupercomputer. Our goal was to examine UE efficacy at high-performance\ncomputing (HPC) levels to prevent unauthorized learning and enhance data\nsecurity, particularly exploring the impact of batch size on UE's\nunlearnability. Utilizing the robust computational capabilities of the Summit,\nextensive experiments were conducted on diverse datasets such as Pets,\nMedMNist, Flowers, and Flowers102. Our findings reveal that both overly large\nand overly small batch sizes can lead to performance instability and affect\naccuracy. However, the relationship between batch size and unlearnability\nvaried across datasets, highlighting the necessity for tailored batch size\nstrategies to achieve optimal data protection. Our results underscore the\ncritical role of selecting appropriate batch sizes based on the specific\ncharacteristics of each dataset to prevent learning and ensure data security in\ndeep learning applications.",
        "Much of the research focus on AI alignment seeks to align large language\nmodels and other foundation models to the context-less and generic values of\nhelpfulness, harmlessness, and honesty. Frontier model providers also strive to\nalign their models with these values. In this paper, we motivate why we need to\nmove beyond such a limited conception and propose three dimensions for doing\nso. The first scope of alignment is competence: knowledge, skills, or behaviors\nthe model must possess to be useful for its intended purpose. The second scope\nof alignment is transience: either semantic or episodic depending on the\ncontext of use. The third scope of alignment is audience: either mass, public,\nsmall-group, or dyadic. At the end of the paper, we use the proposed framework\nto position some technologies and workflows that go beyond prevailing notions\nof alignment.",
        "In this work we report efficient out-of-plane spin injection and detection in\nan all-van der Waals based heterostructure using only exfoliated 2D materials.\nWe demonstrate spin injection by measuring spin-valve and Hanle signals in\nnon-local transport in a stack of Fe$_3$GeTe$_2$ (FGT), hexagonal boron nitride\n(hBN) and graphene layers. FGT flakes form the spin aligning electrodes\nnecessary to inject and detect spins in the graphene channel. The hBN tunnel\nbarrier provides a high-quality interface between the ferromagnetic electrodes\nand graphene, eliminating the conductivity mismatch problem, thus ensuring\nefficient spin injection and detection with spin injection efficiencies of up\nto $P=40$\\%. Our results demonstrate that FGT\/hBN\/graphene heterostructures\nform a promising platform for realizing 2D van der Waals spintronic devices.",
        "Training segmentation models from scratch has been the standard approach for\nnew electron microscopy connectomics datasets. However, leveraging pretrained\nmodels from existing datasets could improve efficiency and performance in\nconstrained annotation budget. In this study, we investigate domain adaptation\nin connectomics by analyzing six major datasets spanning different organisms.\nWe show that, Maximum Mean Discrepancy (MMD) between neuron image distributions\nserves as a reliable indicator of transferability, and identifies the optimal\nsource domain for transfer learning. Building on this, we introduce NeuroADDA,\na method that combines optimal domain selection with source-free active\nlearning to effectively adapt pretrained backbones to a new dataset. NeuroADDA\nconsistently outperforms training from scratch across diverse datasets and\nfine-tuning sample sizes, with the largest gain observed at $n=4$ samples with\na 25-67\\% reduction in Variation of Information. Finally, we show that our\nanalysis of distributional differences among neuron images from multiple\nspecies in a learned feature space reveals that these domain \"distances\"\ncorrelate with phylogenetic distance among those species.",
        "This work presents a computationally efficient approach to data-driven robust\ncontracting controller synthesis for polynomial control-affine systems based on\na sum-of-squares program. In particular, we consider the case in which a system\nalternates between periods of high-quality sensor data and low-quality sensor\ndata. In the high-quality sensor data regime, we focus on robust system\nidentification based on the data informativity framework. In low-quality sensor\ndata regimes we employ a robustly contracting controller that is synthesized\nonline by solving a sum-of-squares program based on data acquired in the\nhigh-quality regime, so as to limit state deviation until high-quality data is\navailable. This approach is motivated by real-life control applications in\nwhich systems experience periodic data blackouts or occlusion, such as\nautonomous vehicles undergoing loss of GPS signal or solar glare in machine\nvision systems. We apply our approach to a planar unmanned aerial vehicle model\nsubject to an unknown wind field, demonstrating its uses for verifiably tight\ncontrol on trajectory deviation.",
        "Given a squarefree monomial ideal $I$ of a polynomial ring $Q$, we show that\nif the minimal free resolution $\\mathbb{F}$ of $Q\/I$ admits the structure of a\ndifferential graded (dg) algebra, then so does any \"pruning\" of $\\mathbb{F}$.\nAs an application, we show that if $Q\/\\mathcal{F}(\\Delta)$, the quotient of the\nambient polynomial ring by the facet ideal $\\mathcal{F}(\\Delta)$ of a\nsimplicial complex $\\Delta$, is minimally resolved by a dg algebra, then so is\nthe quotient by the facet ideal of each facet-induced subcomplex of $\\Delta$\n(over the smaller polynomial ring). Along with techniques from discrete Morse\ntheory and homological algebra, this allows us to give complete classifications\nof the trees and cycles $G$ with $Q\/I_G$ minimally resolved by a dg algebra in\nterms of the diameter of $G$, where $I_G$ is the edge ideal of $G$.",
        "Structural Health Monitoring (SHM) ensures the safety and longevity of\ninfrastructure by enabling timely damage detection. Vision-based crack\ndetection, combined with UAVs, addresses the limitations of traditional\nsensor-based SHM methods but requires the deployment of efficient deep learning\nmodels on resource-constrained devices. This study evaluates two lightweight\nconvolutional neural network models, MobileNetV1x0.25 and MobileNetV2x0.5,\nacross TensorFlow, PyTorch, and Open Neural Network Exchange platforms using\nthree quantization techniques: dynamic quantization, post-training quantization\n(PTQ), and quantization-aware training (QAT). Results show that QAT\nconsistently achieves near-floating-point accuracy, such as an F1-score of\n0.8376 for MBNV2x0.5 with Torch-QAT, while maintaining efficient resource\nusage. PTQ significantly reduces memory and energy consumption but suffers from\naccuracy loss, particularly in TensorFlow. Dynamic quantization preserves\naccuracy but faces deployment challenges on PyTorch. By leveraging QAT, this\nwork enables real-time, low-power crack detection on UAVs, enhancing safety,\nscalability, and cost-efficiency in SHM applications, while providing insights\ninto balancing accuracy and efficiency across different platforms for\nautonomous inspections.",
        "Composite Higgs models are a class of Beyond the Standard Model (BSM) models\nproposed to address the hierarchy and naturalness problems associated with the\nStandard Model (SM) Higgs. A new QCD-like strongly interacting sector based on\n$SU(2)$ with two fundamental flavours can be used to build a composite Higgs\nmodel which is not yet ruled out by experiment. The role of the singlet scalar\nresonance will affect Higgs phenomenology at the LHC. In this project our goal\nis to understand the properties of the singlet scalar state in the new strongly\ninteracting sector in isolation as a first step to understanding the role of\nthis state in composite Higgs models. We present here the first lattice results\nfor the mass of the $\\sigma$ in $SU(2)$ with two fundamental flavours using\nexponential clover Wilson fermions.",
        "Game theory establishes a fundamental framework for analyzing strategic\ninteractions among rational decision-makers. The rapid advancement of large\nlanguage models (LLMs) has sparked extensive research exploring the\nintersection of these two fields. Specifically, game-theoretic methods are\nbeing applied to evaluate and enhance LLM capabilities, while LLMs themselves\nare reshaping classic game models. This paper presents a comprehensive survey\nof the intersection of these fields, exploring a bidirectional relationship\nfrom three perspectives: (1) Establishing standardized game-based benchmarks\nfor evaluating LLM behavior; (2) Leveraging game-theoretic methods to improve\nLLM performance through algorithmic innovations; (3) Characterizing the\nsocietal impacts of LLMs through game modeling. Among these three aspects, we\nalso highlight how the equilibrium analysis for traditional game models is\nimpacted by LLMs' advanced language understanding, which in turn extends the\nstudy of game theory. Finally, we identify key challenges and future research\ndirections, assessing their feasibility based on the current state of the\nfield. By bridging theoretical rigor with emerging AI capabilities, this survey\naims to foster interdisciplinary collaboration and drive progress in this\nevolving research area.",
        "ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education.",
        "This work is about understanding the impact of invariance and equivariance on\ngeneralisation in supervised learning. We use the perspective afforded by an\naveraging operator to show that for any predictor that is not equivariant,\nthere is an equivariant predictor with strictly lower test risk on all\nregression problems where the equivariance is correctly specified. This\nconstitutes a rigorous proof that symmetry, in the form of invariance or\nequivariance, is a useful inductive bias.\n  We apply these ideas to equivariance and invariance in random design least\nsquares and kernel ridge regression respectively. This allows us to specify the\nreduction in expected test risk in more concrete settings and express it in\nterms of properties of the group, the model and the data.\n  Along the way, we give examples and additional results to demonstrate the\nutility of the averaging operator approach in analysing equivariant predictors.\nIn addition, we adopt an alternative perspective and formalise the common\nintuition that learning with invariant models reduces to a problem in terms of\norbit representatives. The formalism extends naturally to a similar intuition\nfor equivariant models. We conclude by connecting the two perspectives and\ngiving some ideas for future work.",
        "Text-to-motion generation is a rapidly growing field at the nexus of\nmultimodal learning and computer graphics, promising flexible and\ncost-effective applications in gaming, animation, robotics, and virtual\nreality. Existing approaches often rely on simple spatiotemporal stacking,\nwhich introduces feature redundancy, while subtle joint-level details remain\noverlooked from a spatial perspective. To this end, we propose a novel HiSTF\nMamba framework. The framework is composed of three key modules: Dual-Spatial\nMamba, Bi-Temporal Mamba, and Dynamic Spatiotemporal Fusion Module (DSFM).\nDual-Spatial Mamba incorporates ``Part-based + Whole-based'' parallel modeling\nto represent both whole-body coordination and fine-grained joint dynamics.\nBi-Temporal Mamba adopts a bidirectional scanning strategy, effectively\nencoding short-term motion details and long-term dependencies. DSFM further\nperforms redundancy removal and extraction of complementary information for\ntemporal features, then fuses them with spatial features, yielding an\nexpressive spatio-temporal representation. Experimental results on the\nHumanML3D dataset demonstrate that HiSTF Mamba achieves state-of-the-art\nperformance across multiple metrics. In particular, it reduces the FID score\nfrom 0.283 to 0.189, a relative decrease of nearly 30%. These findings validate\nthe effectiveness of HiSTF Mamba in achieving high fidelity and strong semantic\nalignment in text-to-motion generation.",
        "Single-image 3D generation has emerged as a prominent research topic, playing\na vital role in virtual reality, 3D modeling, and digital content creation.\nHowever, existing methods face challenges such as a lack of multi-view\ngeometric consistency and limited controllability during the generation\nprocess, which significantly restrict their usability. % To tackle these\nchallenges, we introduce Dragen3D, a novel approach that achieves geometrically\nconsistent and controllable 3D generation leveraging 3D Gaussian Splatting\n(3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS\nVAE), which encodes a point cloud and a single image into anchor latents and\ndecode these latents into 3DGS, enabling efficient latent-space generation. To\nenable multi-view geometry consistent and controllable generation, we propose a\nSeed-Point-Driven strategy: first generate sparse seed points as a coarse\ngeometry representation, then map them to anchor latents via the Seed-Anchor\nMapping Module. Geometric consistency is ensured by the easily learned sparse\nseed points, and users can intuitively drag the seed points to deform the final\n3DGS geometry, with changes propagated through the anchor latents. To the best\nof our knowledge, we are the first to achieve geometrically controllable 3D\nGaussian generation and editing without relying on 2D diffusion priors,\ndelivering comparable 3D generation quality to state-of-the-art methods.",
        "We present ultraviolet (UV) spectroscopic observations covering three\ndistinct accretion states of the low-mass X-ray binary (LMXB) MAXI J1820+070:\nthe luminous hard state, a hard-intermediate state and the soft state. Our\nobservations were obtained during the 2018 eruption of MAXI J1820+070 with the\nHubble Space Telescope (HST) and AstroSat observatory. The extinction towards\nthe source turns out to be low - $\\rm E_{B-V} = 0.2 \\pm 0.05$ - making it one\nof the best UV accretion laboratories among LMXBs. Remarkably, we observe only\nmoderate differences between all three states, with all spectra displaying\nsimilar continuum shapes and emission lines. Moreover, the continua are not\nwell-described by physically plausible irradiated disc models. All of this\nchallenges the standard reprocessing picture for UV emission from erupting\nLMXBs. The UV emission lines are double-peaked, with high-ionization lines\ndisplaying higher peak-to-peak velocities. None of the lines display obvious\noutflow signatures, even though blue-shifted absorption features have been seen\nin optical and near-infrared lines during the hard state. The emission line\nratios are consistent with normal abundances, suggesting that the donor mass at\nbirth was low enough to avoid CNO processing ($\\rm M_{2,i} \\lesssim 1.0 - 1.5\n{\\mathrm M_{\\odot}}$). Finally, we study the evolution of UV variability in our\ntime-resolved HST observations (hard and hard-intermediate states). All UV\npower spectra can be modelled with a broken power-law, superposed on which we\ntentatively detect the $\\simeq 18$s quasi-periodic oscillation (QPO) that has\nbeen seen in other spectral bands.",
        "Liquid metal (LM)-based composites hold promise for soft electronics due to\ntheir high conductivity and fluidic nature. However, the presence of\n{\\alpha}_Ga2O3 and GaOOH layers around LM droplets impairs conductivity and\nperformance. We tackle this issue by replacing the oxide layer with conductive\nsilver (Ag) using an ultrasonic_assisted galvanic replacement reaction. The\nAg_coated nanoparticles form aggregated, porous microparticles that are mixed\nwith styrene_isoprene_styrene (SIS) polymers, resulting in a digitally\nprintable composite with superior electrical conductivity and electromechanical\nproperties compared to conventional fillers. Adding more LM enhances these\nproperties further. The composite achieves EMI shielding effectiveness (SE)\nexceeding 75 dB in the X_band frequency range, even at 200 per cent strain,\nmeeting stringent military and medical standards. It is applicable in wireless\ncommunications and Bluetooth signal blocking and as a thermal interface\nmaterial (TIM). Additionally, we highlight its recyclability using a\nbiodegradable solvent, underscoring its eco_friendly potential. This composite\nrepresents a significant advancement in stretchable electronics and EMI\nshielding, with implications for wearable and bioelectronic applications.",
        "By combining general relativity and CPT symmetry, the theory of CPT gravity\npredicts gravitational repulsion between matter and CPT-transformed matter,\ni.e. antimatter inhabiting an inverted space-time. Such repulsive gravity\nturned out to be an excellent candidate for explaining the accelerated\nexpansion of the Universe, without the need for dark energy. The recent results\nof the ALPHA-g experiment, which show gravitational attraction between\nantihydrogen atoms and the Earth, seem to undermine this success in the\ncosmological field. Analyzing the above theory, we find two solutions that can\nbe consistent with the experimental results, while preserving the large-scale\ngravitational repulsion. The first highlights how repulsive gravity can be the\nresult of the interaction with an inverted space-time, but occupied by matter\nand not antimatter, and therefore the antimatter present in our space-time has\nno reason to exhibit gravitational repulsion. The second retains the original\nCPT transformation, resulting in repulsive gravity between matter and\nantimatter, but with the caveat that antimatter immersed in our space-time\ncannot exhibit the PT transformation which is the cause of the repulsion.\nFinally, it is shown that, in a Newtonian approximation of the geodesic\nequation, time reversal is not a necessary operation for repulsive gravity,\ntherefore opening the possibility of an expanding cosmos with a single time\ndirection.",
        "The Madison AWAKE Prototype (MAP) is a high-power, high-density helicon\nplasma experiment. The project's main goal is to develop a scalable plasma\nsource for use in a beam-driven plasma wakefield accelerator as part of the\nAWAKE project. We measure the plasma density with a new heterodyne microwave\ninterferometer that features several improvements over traditional approaches.\nThe design uses a single microwave source combined with an upconverter to avoid\nfrequency drift and reduce overall cost. Elliptical mirrors focus the probe\nbeam into the plasma and guide it back to the receiver. The transmitter and\nreceiver along with the measurement electronics are co-located in a small\nenclosure and are assisted by two small mirrors on the opposite side of MAP.\nBoth halves of the system move independently on computer-controlled motion\nplatforms. This setup enables fast repositioning of the interferometer to\nmeasure at any axial location despite the magnets, wiring and structural\nsupports that would block movement of a waveguide-based system. A high-speed,\nhigh-precision mixed signal circuit and FPGA analyze the probe signal directly\nin the enclosure which obviates the need for a digitizer or oscilloscope. The\ninterferometer resolves phase shifts down to one hundredth of a fringe,\nresulting in a line-averaged resolution of $1.5\\mathrm{\\cdot 10^{17}\\;\nm^{-3}}$. The system provides a real-time measurement every $5\\;\\mathrm{\\mu s}$\nup into the mid $\\mathrm{10^{19}\\; m^{-3}}$ density range with a noise level of\n$1.0\\mathrm{\\cdot 10^{17}\\; m^{-3}}$.",
        "We compute the (0-11) surface spectral function, the surface density of\nstates (DOS), and the quasiparticle interference (QPI) patterns, both in the\nnormal state and superconducting (SC) state of UTe$_2$. We consider all\npossible non-chiral and chiral order parameters (OPs) that could in principle\ndescribe the superconductivity in this compound. We describe the formation of\nsurface states whose maximum intensity energy depends on the nature of the\npairing. We study also the QPI patterns resulting from the scattering of these\nsurface states. We show that the main feature distinguishing between various\nOPs is a QPI peak that is only observed experimentally in the superconducting\nstate. The energy dispersion and the stability of this peak is consistent among\nthe non-chiral OPs only with a $B_{3u}$ pairing. Moreover, $B_{3u}$ is the only\nnon-chiral pairing that shows a peak at zero energy in the DOS, consistent with\nthe experimental observations.",
        "An intriguing phenomenon regarding Levi-degenerate hypersurfaces is the\nexistence of nontrivial infinitesimal symmetries with vanishing 2-jets at a\npoint. In this work we consider polynomial models of Levi-degenerate real\nhypersurfaces in $\\mathbb{C}^3$ of finite Catlin multitype. Exploiting the\nstructure of the corresponding Lie algebra, we characterize completely models\nwithout 2-jet determination, including an explicit description of their\nsymmetry algebras.",
        "Modeling of growth (or decay) curves arises in many fields such as\nmicrobiology, epidemiology, marketing, and econometrics. Parametric forms like\nLogistic and Gompertz are often used for modeling such monotonic patterns.\nWhile useful for compact description, the real-life growth curves rarely follow\nthese parametric forms perfectly. Therefore, the curve estimation methods that\nstrike a balance between prior information in the parametric form and fidelity\nwith the observed data are preferred. In hierarchical, longitudinal studies the\ninterest lies in comparing the growth curves of different groups while\naccounting for the differences between the within-group subjects. This article\ndescribes a flexible state space modeling framework that enables semiparametric\ngrowth curve modeling for the data generated from hierarchical, longitudinal\nstudies. The methodology, a type of functional mixed effects modeling, is\nillustrated with a real-life example of bacterial growth in different settings.",
        "Modern blockchains increasingly consist of multiple clients that implement a\nsingle blockchain protocol. If there is a semantic mismatch between the\nprotocol implementations, the blockchain can permanently split and introduce\nnew attack vectors. Current ad-hoc test suites for client implementations are\nnot sufficient to ensure a high degree of protocol conformance. As an\nalternative, we present a framework that performs protocol conformance testing\nusing a formal model of the protocol and an implementation running inside a\ndeterministic blockchain simulator. Our framework consists of two complementary\nworkflows that use the components as trace generators and checkers. Our insight\nis that both workflows are needed to detect all types of violations. We have\napplied and demonstrated the utility of our framework on an industrial strength\nconsensus protocol.",
        "With the diversification of online social platforms, news dissemination has\nbecome increasingly complex, heterogeneous, and multimodal, making the fake\nnews detection task more challenging and crucial. Previous works mainly focus\non obtaining social relationships of news via retweets, limiting the accurate\ndetection when real cascades are inaccessible. Given the proven assessment of\nthe spreading influence of events, this paper proposes a method called HML\n(Complex Heterogeneous Multimodal Fake News Detection method via Latent Network\nInference). Specifically, an improved social latent network inference strategy\nis designed to estimate the maximum likelihood of news influences under the\nsame event. Meanwhile, a novel heterogeneous graph is built based on social\nattributes for multimodal news under different events. Further, to better\naggregate the relationships among heterogeneous multimodal features, this paper\nproposes a self-supervised-based multimodal content learning strategy, to\nenhance, align, fuse and compare heterogeneous modal contents. Based above, a\npersonalized heterogeneous graph representation learning is designed to\nclassify fake news. Extensive experiments demonstrate that the proposed method\noutperforms the SOTA in real social media news datasets.",
        "Knowledge Tracing (KT) predicts future performance by modeling students'\nhistorical interactions, and understanding students' affective states can\nenhance the effectiveness of KT, thereby improving the quality of education.\nAlthough traditional KT values students' cognition and learning behaviors,\nefficient evaluation of students' affective states and their application in KT\nstill require further exploration due to the non-affect-oriented nature of the\ndata and budget constraints. To address this issue, we propose a\ncomputation-driven approach, Dynamic Affect Simulation Knowledge Tracing\n(DASKT), to explore the impact of various student affective states (such as\nfrustration, concentration, boredom, and confusion) on their knowledge states.\nIn this model, we first extract affective factors from students'\nnon-affect-oriented behavioral data, then use clustering and spatiotemporal\nsequence modeling to accurately simulate students' dynamic affect changes when\ndealing with different problems. Subsequently, {\\color{blue}we incorporate\naffect with time-series analysis to improve the model's ability to infer\nknowledge states over time and space.} Extensive experimental results on two\npublic real-world educational datasets show that DASKT can achieve more\nreasonable knowledge states under the effect of students' affective states.\nMoreover, DASKT outperforms the most advanced KT methods in predicting student\nperformance. Our research highlights a promising avenue for future KT studies,\nfocusing on achieving high interpretability and accuracy.",
        "Optimal transport has gained significant attention in recent years due to its\neffectiveness in deep learning and computer vision. Its descendant metric, the\nWasserstein distance, has been particularly successful in measuring\ndistribution dissimilarities. While extensive research has focused on optimal\ntransport and its regularized variants (such as entropy, sparsity, and capacity\nconstraints) the role of time has been largely overlooked. However, time is a\ncritical factor in real world transport problems.\n  In this work, we introduce a time parameterized formulation of the optimal\ntransport problem, incorporating a time variable t to represent sequential\nsteps and enforcing specific constraints at each step. We propose a systematic\nmethod to solve a special subproblem and develop a heuristic search algorithm\nthat achieves nearly optimal solutions while significantly reducing\ncomputational time.",
        "Program synthesis has traditionally relied on human-provided specifications,\nexamples, or prior knowledge to generate functional algorithms. Existing\nmethods either emulate human-written algorithms or solve specific tasks without\ngenerating reusable programmatic logic, limiting their ability to create novel\nalgorithms. We introduce AlgoPilot, a groundbreaking approach for fully\nautomated program synthesis without human-written programs or trajectories.\nAlgoPilot leverages reinforcement learning (RL) guided by a Trajectory Language\nModel (TLM) to synthesize algorithms from scratch. The TLM, trained on\ntrajectories generated by random Python functions, serves as a soft constraint\nduring the RL process, aligning generated sequences with patterns likely to\nrepresent valid algorithms. Using sorting as a test case, AlgoPilot\ndemonstrates its ability to generate trajectories that are interpretable as\nclassical algorithms, such as Bubble Sort, while operating without prior\nalgorithmic knowledge. This work establishes a new paradigm for algorithm\ndiscovery and lays the groundwork for future advancements in autonomous program\nsynthesis.",
        "Africa's rich linguistic heritage remains underrepresented in NLP, largely\ndue to historical policies that favor foreign languages and create significant\ndata inequities. In this paper, we integrate theoretical insights on Africa's\nlanguage landscape with an empirical evaluation using Sahara - a comprehensive\nbenchmark curated from large-scale, publicly accessible datasets capturing the\ncontinent's linguistic diversity. By systematically assessing the performance\nof leading large language models (LLMs) on Sahara, we demonstrate how\npolicy-induced data variations directly impact model effectiveness across\nAfrican languages. Our findings reveal that while a few languages perform\nreasonably well, many Indigenous languages remain marginalized due to sparse\ndata. Leveraging these insights, we offer actionable recommendations for policy\nreforms and inclusive data practices. Overall, our work underscores the urgent\nneed for a dual approach - combining theoretical understanding with empirical\nevaluation - to foster linguistic diversity in AI for African communities."
      ]
    }
  },
  {
    "id":2411.05188,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Limitations of conventional magnetic resonance imaging as a predictor of death or disability following neonatal hypoxic-ischemic encephalopathy in the late hypothermia trial",
    "start_abstract":"Objective: To investigate if magnetic resonance imaging (MRI) is an accurate predictor for death or moderate-severe disability at 18-22 months of age among infants with neonatal encephalopathy in a trial of cooling initiated at 6-24 hours. Study design: Subgroup analysis of infants \u226536 weeks of gestation with moderate-severe neonatal encephalopathy randomized at 6-24 postnatal hours to hypothermia or usual care in a multicenter trial of late hypothermia. MRI scans were performed per each center's practice and interpreted by 2 central readers using the Eunice Kennedy Shriver National Institute of Child Health and Human Development injury score (6 levels, normal to hemispheric devastation). Neurodevelopmental outcomes were assessed at 18-22 months of age. Results: Of 168 enrollees, 128 had an interpretable MRI and were seen in follow-up (n = 119) or died (n = 9). MRI findings were predominantly acute injury and did not differ by cooling treatment. At 18-22 months, death or severe disability occurred in 20.3%. No infant had moderate disability. Agreement between central readers was moderate (weighted kappa 0.56, 95% CI 0.45-0.67). The adjusted odds of death or severe disability increased 3.7-fold (95% CI 1.8-7.9) for each increment of injury score. The area under the curve for severe MRI patterns to predict death or severe disability was 0.77 and the positive and negative predictive values were 36% and 100%, respectively. Conclusions: MRI injury scores were associated with neurodevelopmental outcome at 18-22 months among infants in the Late Hypothermia Trial. However, the results suggest caution when using qualitative interpretations of MRI images to provide prognostic information to families following perinatal hypoxia-ischemia.",
    "start_categories":[
      "Pediatrics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "What is being transferred in transfer learning?"
      ],
      "abstract":[
        "One desired capability for machines is the ability to transfer their knowledge of one domain another where data (usually) scarce. Despite ample adaptation learning in various deep applications, we yet do not understand what enables a successful and which part network responsible that. In this paper, provide new tools analyses address these fundamental questions. Through series on transferring block-shuffled images, separate effect feature reuse from low-level statistics show that some benefit comes latter. We present when training pre-trained weights, model stays same basin loss landscape different instances such are similar space close parameter space."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Contact line bundles, foliations, and integrability",
        "On the Ramsey-Tur\\'an problem for 4-cliques",
        "Growth model with externalities for energetic transition via MFG with\n  common external variable",
        "A comprehensive survey of contemporary Arabic sentiment analysis:\n  Methods, Challenges, and Future Directions",
        "Diving deep into the Milky Way using Anti-Reflection Coatings for\n  Astronomical CCDs",
        "Model-Guardian: Protecting against Data-Free Model Stealing Using\n  Gradient Representations and Deceptive Predictions",
        "Improving vision-language alignment with graph spiking hybrid Networks",
        "Eisenstein degeneration of Beilinson--Kato classes and circular units",
        "FoundPAD: Foundation Models Reloaded for Face Presentation Attack\n  Detection",
        "Understanding Design Fixation in Generative AI",
        "Advancing Earth Observation: A Survey on AI-Powered Image Processing in\n  Satellites",
        "TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers",
        "Temporal multilayer structures for designing higher-order transfer\n  functions using time-varying metamaterials",
        "ARCANE Reweighting: A Monte Carlo Technique to Tackle the Negative\n  Weights Problem in Collider Event Generation",
        "Observation-Based Iterative Map for Solar Cycles. I. Nature of Solar\n  Cycle Variability",
        "Euclid: Quick Data Release (Q1) -- A census of dwarf galaxies across a\n  range of distances and environments",
        "Domain Consistent Industrial Decarbonisation of Global Coal Power Plants",
        "Knowledge-Guided Biomarker Identification for Label-Free Single-Cell\n  RNA-Seq Data: A Reinforcement Learning Perspective",
        "Optimal-Reference Excited State Methods: Static Correlation at\n  Polynomial Cost with Single-Reference Coupled-Cluster Approaches",
        "AhaRobot: A Low-Cost Open-Source Bimanual Mobile Manipulator for\n  Embodied AI",
        "On the geometry of splitting models",
        "Nuclear magnetic resonance investigation of strain-tuned iron-based\n  superconductors (Druckabh\\\"{a}ngige Untersuchung eisenbasierter Supraleiter\n  mittels Kernspinresonanz)",
        "PINN-FEM: A Hybrid Approach for Enforcing Dirichlet Boundary Conditions\n  in Physics-Informed Neural Networks",
        "Phase transitions in an expanding medium -- hot remnants",
        "Precise Measurement of the $\\chi_{c0}$ Resonance Parameters and\n  Branching Fractions of $\\chi_{c0,c2}\\to\\pi^+\\pi^-\/K^+K^-$",
        "Topological Quantum Dark Matter via Global Anomaly Cancellation",
        "Latent Radiance Fields with 3D-aware 2D Representations",
        "Quantum metric induced magneto-optical effects in\n  $\\mathcal{PT}$-symmetric antiferromagnets",
        "A Unified Column Generation and Elimination Method for Solving\n  Large-Scale Set Partitioning Problems"
      ],
      "abstract":[
        "We formulate the non-commutative integrability of contact systems on a\ncontact manifold $(M,\\mathcal H)$ using the Jacobi structure on the space of\nsections $\\Gamma(L)$ of a contact line bundle $L$. In the cooriented case, if\nthe line bundle is trivial and $\\mathcal H$ is the kernel of a globally defined\ncontact form $\\alpha$, the Jacobi structure on the space of sections reduces to\nthe standard Jacobi structure on $(M,\\alpha)$. We therefore treat contact\nsystems on cooriented and non-cooriented contact manifolds simultaneously. In\nparticular, this allows us to work with dissipative Hamiltonian systems where\nthe Hamiltonian does not have to be preserved by the Reeb vector field.",
        "We present an essentially tight bound for the Ramsey-Tur\\'an problem for\n4-cliques without using the Regularity lemma. This enables us to substantially\nextend the range in which one has the tight bound for the number of edges in\n$K_4$-free graphs as a function of the independence number, apart from lower\norder terms.",
        "This article introduces a novel mean-field game model for multi-sector\neconomic growth in which a dynamically evolving externality, influenced by the\ncollective actions of agents, plays a central role. Building on classical\ngrowth theories and integrating environmental considerations, the framework\nincorporates common noise to capture shared uncertainties among agents about\nthe externality variable. We demonstrate the existence and uniqueness of a\nstrong mean-field game equilibrium by reformulating the equilibrium conditions\nas a Forward-Backward Stochastic Differential Equation under the stochastic\nmaximum principle and establishing a contraction argument to ensure a unique\nsolution. We provide a numerical resolution for a specified model using a\nfixed-point approach combined with neural network approximations.",
        "Sentiment Analysis, a popular subtask of Natural Language Processing, employs\ncomputational methods to extract sentiment, opinions, and other subjective\naspects from linguistic data. Given its crucial role in understanding human\nsentiment, research in sentiment analysis has witnessed significant growth in\nthe recent years. However, the majority of approaches are aimed at the English\nlanguage, and research towards Arabic sentiment analysis remains relatively\nunexplored. This paper presents a comprehensive and contemporary survey of\nArabic Sentiment Analysis, identifies the challenges and limitations of\nexisting literature in this field and presents avenues for future research. We\npresent a systematic review of Arabic sentiment analysis methods, focusing\nspecifically on research utilizing deep learning. We then situate Arabic\nSentiment Analysis within the broader context, highlighting research gaps in\nArabic sentiment analysis as compared to general sentiment analysis. Finally,\nwe outline the main challenges and promising future directions for research in\nArabic sentiment analysis.",
        "We report two anti-reflection (AR) coatings that give better quantum\nefficiency (QE) than the existing AR coating on the Gaia astrometric field (AF)\nCCDs. Light being the core of optical astronomy is extremely important for such\nmissions, therefore, the QE of the devices that are used to capture it should\nbe substantially high. To reduce the losses due to the reflection of light from\nthe surface of the CCDs, AR coatings can be applied. Currently, the main\ncomponent of the Gaia satellite, the AF CCDs use hafnium dioxide (HfO2) AR\ncoating. In this paper, the ATLAS module of the SILVACO software has been\nemployed for simulating and studying the AF CCD pixel structure and several AR\ncoatings. Our findings evidently suggest that zirconium dioxide (ZrO2) and\ntantalum pentoxide (Ta2O5) will prove to be better AR coatings for broadband\nastronomical CCDs in the future and will open new avenues for understanding the\nevolution of the Milky Way.",
        "Model stealing attack is increasingly threatening the confidentiality of\nmachine learning models deployed in the cloud. Recent studies reveal that\nadversaries can exploit data synthesis techniques to steal machine learning\nmodels even in scenarios devoid of real data, leading to data-free model\nstealing attacks. Existing defenses against such attacks suffer from\nlimitations, including poor effectiveness, insufficient generalization ability,\nand low comprehensiveness. In response, this paper introduces a novel defense\nframework named Model-Guardian. Comprising two components, Data-Free Model\nStealing Detector (DFMS-Detector) and Deceptive Predictions (DPreds),\nModel-Guardian is designed to address the shortcomings of current defenses with\nthe help of the artifact properties of synthetic samples and gradient\nrepresentations of samples. Extensive experiments on seven prevalent data-free\nmodel stealing attacks showcase the effectiveness and superior generalization\nability of Model-Guardian, outperforming eleven defense methods and\nestablishing a new state-of-the-art performance. Notably, this work pioneers\nthe utilization of various GANs and diffusion models for generating highly\nrealistic query samples in attacks, with Model-Guardian demonstrating accurate\ndetection capabilities.",
        "To bridge the semantic gap between vision and language (VL), it is necessary\nto develop a good alignment strategy, which includes handling semantic\ndiversity, abstract representation of visual information, and generalization\nability of models. Recent works use detector-based bounding boxes or patches\nwith regular partitions to represent visual semantics. While current paradigms\nhave made strides, they are still insufficient for fully capturing the nuanced\ncontextual relations among various objects. This paper proposes a comprehensive\nvisual semantic representation module, necessitating the utilization of\npanoptic segmentation to generate coherent fine-grained semantic features.\nFurthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that\nintegrates the complementary advantages of Spiking Neural Networks (SNNs) and\nGraph Attention Networks (GATs) to encode visual semantic information.\nIntriguingly, the model not only encodes the discrete and continuous latent\nvariables of instances but also adeptly captures both local and global\ncontextual features, thereby significantly enhancing the richness and diversity\nof semantic representations. Leveraging the spatiotemporal properties inherent\nin SNNs, we employ contrastive learning (CL) to enhance the similarity-based\nrepresentation of embeddings. This strategy alleviates the computational\noverhead of the model and enriches meaningful visual representations by\nconstructing positive and negative sample pairs. We design an innovative\npre-training method, Spiked Text Learning (STL), which uses text features to\nimprove the encoding ability of discrete semantics. Experiments show that the\nproposed GSHN exhibits promising results on multiple VL downstream tasks.",
        "The aim of this note is to explore the Euler system of Beilinson--Kato\nelements in families passing through the critical $p$-stabilization of an\nEisenstein series attached to two Dirichlet characters $(\\psi,\\tau)$. In this\ncontext, we establish an explicit connection with the system of circular units,\nutilizing suitable factorization formulas in a situation where several of the\n$p$-adic $L$-functions vanish. In that regard, our main results may be seen as\nan Euler system incarnation of the factorization formula of Bella\\\"iche and\nDasgupta. One of the most significant aspects is that, depending on the parity\nof $\\psi$ and $\\tau$, different phenomena arise; while some can be addressed\nwith our methods, others pose new questions. Finally, we discuss analogous\nresults in the framework of Beilinson--Flach classes.",
        "Although face recognition systems have seen a massive performance enhancement\nin recent years, they are still targeted by threats such as presentation\nattacks, leading to the need for generalizable presentation attack detection\n(PAD) algorithms. Current PAD solutions suffer from two main problems: low\ngeneralization to unknown cenarios and large training data requirements.\nFoundation models (FM) are pre-trained on extensive datasets, achieving\nremarkable results when generalizing to unseen domains and allowing for\nefficient task-specific adaption even when little training data are available.\nIn this work, we recognize the potential of FMs to address common PAD problems\nand tackle the PAD task with an adapted FM for the first time. The FM under\nconsideration is adapted with LoRA weights while simultaneously training a\nclassification header. The resultant architecture, FoundPAD, is highly\ngeneralizable to unseen domains, achieving competitive results in several\nsettings under different data availability scenarios and even when using\nsynthetic training data. To encourage reproducibility and facilitate further\nresearch in PAD, we publicly release the implementation of FoundPAD at\nhttps:\/\/github.com\/gurayozgur\/FoundPAD .",
        "Generative AI (GenAI) provides new opportunities for creativity support, but\nthe phenomenon of GenAI design fixation remains underexplored. While human\ndesign fixation typically constrains ideas to familiar or existing solutions,\nour findings reveal that GenAI similarly experience design fixation, limiting\nits ability to generate novel and diverse design outcomes. To advance\nunderstanding of GenAI design fixation, we propose a theoretical framework\nincludes the definition, causes, manifestations, and impacts of GenAI design\nfixation for creative design. We also conducted an experimental study to\ninvestigate the characteristics of GenAI design fixation in practice. We\nsummarize how GenAI design fixation manifests in text generation model and\nimage generation model respectively. Furthermore, we propose methods for\nmitigating GenAI design fixation for future creativity support tool design. We\nrecommend adopting the lens of GenAI design fixation for creativity-oriented\nHCI research, as the unique perspectives and insights it provides.",
        "Advancements in technology and reduction in it's cost have led to a\nsubstantial growth in the quality & quantity of imagery captured by Earth\nObservation (EO) satellites. This has presented a challenge to the efficacy of\nthe traditional workflow of transmitting this imagery to Earth for processing.\nAn approach to addressing this issue is to use pre-trained artificial\nintelligence models to process images on-board the satellite, but this is\ndifficult given the constraints within a satellite's environment. This paper\nprovides an up-to-date and thorough review of research related to image\nprocessing on-board Earth observation satellites. The significant constraints\nare detailed along with the latest strategies to mitigate them.",
        "Diffusion transformers (DiTs) combine transformer architectures with\ndiffusion models. However, their computational complexity imposes significant\nlimitations on real-time applications and sustainability of AI systems. In this\nstudy, we aim to enhance the computational efficiency through model\nquantization, which represents the weights and activation values with lower\nprecision. Multi-region quantization (MRQ) is introduced to address the\nasymmetric distribution of network values in DiT blocks by allocating two\nscaling parameters to sub-regions. Additionally, time-grouping quantization\n(TGQ) is proposed to reduce quantization error caused by temporal variation in\nactivations. The experimental results show that the proposed algorithm achieves\nperformance comparable to the original full-precision model with only a 0.29\nincrease in FID at W8A8. Furthermore, it outperforms other baselines at W6A6,\nthereby confirming its suitability for low-bit quantization. These results\nhighlight the potential of our method to enable efficient real-time generative\nmodels.",
        "Temporal metamaterials are artificial materials whose electromagnetic\nproperties change over time. In analogy with spatial media and metamaterials,\nwhere their properties change smoothly or abruptly over space, temporal\nmetamaterials can exhibit a smooth variation over time, realizing a temporal\nnon-homogeneous medium, or a stepwise transition, realizing the temporal\nversion of dielectric slabs or multilayer structures. In this Letter, we focus\nour attention on temporal multilayer structures, and we propose the synthesis\nof higher-order transfer functions by modeling the wave propagation through a\ngeneralized temporal multilayer structure, consisting of a cascade over time of\ndifferent media. The tailoring of the scattering response of temporal structure\nas a function of frequency is presented, deriving the corresponding scattering\ncoefficients for a properly designed set of medium properties, i.e.,\npermittivity and permeability, and application time, in analogy with what is\ntypically done in optical and electromagnetic spatial multilayered structures.\nThis allows us to design novel electromagnetic and optical devices with\nhigher-order transfer functions by exploiting the temporal dimension instead of\nthe spatial one.",
        "Negatively weighted events, which appear in the Monte Carlo (MC) simulation\nof particle collisions, significantly increases the computational resource\nrequirements of current and future collider experiments. This paper introduces\nand theoretically discusses an MC technique called ARCANE reweighting for\nreducing or eliminating negatively weighted events. The technique works by\nredistributing (via an additive reweighting) the contributions of different\npathways within an event generator that lead to the same final event. The\ntechnique is exact and does not introduce any biases in the distributions of\nphysical observables. A companion paper demonstrates the technique for a\nphysics example.",
        "Inter-cycle variations in the series of 11-year solar activity cycles have a\nsignificant impact on both the space environment and climate. Whether solar\ncycle variability is dominated by deterministic chaos or stochastic\nperturbations remains an open question. Distinguishing between the two\nmechanisms is crucial for predicting solar cycles. Here we reduce the solar\ndynamo process responsible for the solar cycle to a one-dimensional iterative\nmap, incorporating recent advance in the observed nonlinearity and\nstochasticity of the cycle. We demonstrate that deterministic chaos is absent\nin the nonlinear system, regardless of model parameters, if the generation of\nthe poloidal field follows an increase-then-saturate pattern as the cycle\nstrength increases. The synthesized solar cycles generated by the iterative map\nexhibit a probability density function (PDF) similar to that of observed normal\ncycles, supporting the dominant role of stochasticity in solar cycle\nvariability. The parameters governing nonlinearity and stochasticity profoundly\ninfluence the PDF. The iterative map provides a quick and effective tool for\npredicting the range, including uncertainty of the subsequent cycle strength\nwhen an ongoing cycle amplitude is known. Due to stochasticity, a solar cycle\nloses almost all its original information within 1 or 2 cycles. Although the\nsimplicity of the iterative map, the behaviors it exhibits are generic for the\nnonlinear system. Our results provide guidelines for analyzing solar dynamo\nmodels in terms of chaos and stochasticity, highlight the limitation in\npredicting solar cycle, and motivate further refinement of observational\nconstraints on nonlinear and stochastic processes.",
        "The Euclid Q1 fields were selected for calibration purposes in cosmology and\nare therefore relatively devoid of nearby galaxies. However, this is precisely\nwhat makes them interesting fields in which to search for dwarf galaxies in\nlocal density environments. We take advantage of the unprecedented depth,\nspatial resolution, and field of view of the Euclid Quick Release (Q1) to build\na census of dwarf galaxies in these regions. We have identified dwarfs in a\nrepresentative sample of 25 contiguous tiles in the Euclid Deep Field North\n(EDF-N), covering an area of 14.25 sq. deg. The dwarf candidates were\nidentified using a semi-automatic detection method, based on properties\nmeasured by the Euclid pipeline and listed in the MER catalogue. A selection\ncut in surface brightness and magnitude was used to produce an initial dwarf\ncandidate catalogue, followed by a cut in morphology and colour. This catalogue\nwas visually classified to produce a final sample of dwarf candidates,\nincluding their morphology, number of nuclei, globular cluster (GC) richness,\nand presence of a blue compact centre. We identified 2674 dwarf candidates,\ncorresponding to 188 dwarfs per sq. deg. The visual classification of the\ndwarfs reveals a slightly uneven morphological mix of 58% ellipticals and 42%\nirregulars, with very few potentially GC-rich (1.0%) and nucleated (4.0%)\ncandidates but a noticeable fraction (6.9%) of dwarfs with blue compact\ncentres. The distance distribution of 388 (15%) of the dwarfs with\nspectroscopic redshifts peaks at about 400 Mpc. Their stellar mass distribution\nconfirms that our selection effectively identifies dwarfs while minimising\ncontamination. The most prominent dwarf overdensities are dominated by dEs,\nwhile dIs are more evenly distributed. This work highlights Euclid's remarkable\nability to detect and characterise dwarf galaxies across diverse masses,\ndistances, and environments.",
        "Machine learning and optimisation techniques (MLOPT) hold significant\npotential to accelerate the decarbonisation of industrial systems by enabling\ndata-driven operational improvements. However, the practical application of\nMLOPT in industrial settings is often hindered by a lack of domain compliance\nand system-specific consistency, resulting in suboptimal solutions with limited\nreal-world applicability. To address this challenge, we propose a novel\nhuman-in-the-loop (HITL) constraint-based optimisation framework that\nintegrates domain expertise with data-driven methods, ensuring solutions are\nboth technically sound and operationally feasible. We demonstrate the efficacy\nof this framework through a case study focused on enhancing the thermal\nefficiency and reducing the turbine heat rate of a 660 MW supercritical\ncoal-fired power plant. By embedding domain knowledge as constraints within the\noptimisation process, our approach yields solutions that align with the plant's\noperational patterns and are seamlessly integrated into its control systems.\nEmpirical validation confirms a mean improvement in thermal efficiency of\n0.64\\% and a mean reduction in turbine heat rate of 93 kJ\/kWh. Scaling our\nanalysis to 59 global coal power plants with comparable capacity and fuel type,\nwe estimate a cumulative lifetime reduction of 156.4 million tons of carbon\nemissions. These results underscore the transformative potential of our\nHITL-MLOPT framework in delivering domain-compliant, implementable solutions\nfor industrial decarbonisation, offering a scalable pathway to mitigate the\nenvironmental impact of coal-based power generation worldwide.",
        "Gene panel selection aims to identify the most informative genomic biomarkers\nin label-free genomic datasets. Traditional approaches, which rely on domain\nexpertise, embedded machine learning models, or heuristic-based iterative\noptimization, often introduce biases and inefficiencies, potentially obscuring\ncritical biological signals. To address these challenges, we present an\niterative gene panel selection strategy that harnesses ensemble knowledge from\nexisting gene selection algorithms to establish preliminary boundaries or prior\nknowledge, which guide the initial search space. Subsequently, we incorporate\nreinforcement learning through a reward function shaped by expert behavior,\nenabling dynamic refinement and targeted selection of gene panels. This\nintegration mitigates biases stemming from initial boundaries while\ncapitalizing on RL's stochastic adaptability. Comprehensive comparative\nexperiments, case studies, and downstream analyses demonstrate the\neffectiveness of our method, highlighting its improved precision and efficiency\nfor label-free biomarker discovery. Our results underscore the potential of\nthis approach to advance single-cell genomics data analysis.",
        "Accurate yet efficient modeling of chemical systems with pronounced static\ncorrelation in their excited states remains a significant challenge in quantum\nchemistry, as most electronic structure methods that can adequately capture\nstatic correlation scale factorially with system size. Researchers are often\nleft with no option but to use more affordable methods that may lack the\naccuracy required to model critical processes in photochemistry such as\nphotolysis, photocatalysis, and non-adiabatic relaxation. A great deal of work\nhas been dedicated to refining single-reference descriptions of static\ncorrelation in the ground state via ``addition-by-subtraction'' coupled cluster\nmethods such as pair coupled cluster with double substitutions (pCCD),\nsinglet-paired CCD (CCD0), triplet-paired CCD (CCD1), and CCD with frozen\nsinglet- or triplet-paired amplitudes (CCDf0\/CCDf1). By combining wave\nfunctions derived from these methods with the intermediate state representation\n(ISR), we gain insights into the extensibility of single-reference coupled\ncluster theory's coverage of static correlation to the excited state problem.\nOur CCDf1-ISR(2) approach is robust in the face of static correlation and\nprovides enough dynamical correlation to accurately predict excitation energies\nto within about 0.2~eV in small organic molecules. We also highlight distinct\nadvantages of the Hermitian ISR construction, such as the avoidance of\npathological failures of equation-of-motion methods for excited state potential\nenergy surface topology. Our results prompt us to continue exploring optimal\nsingle-reference theories (excited state approaches that leverage dependence on\nthe initial reference wave function) as a potentially economical approach to\nthe excited state static correlation problem.",
        "Navigation and manipulation in open-world environments remain unsolved\nchallenges in the Embodied AI. The high cost of commercial mobile manipulation\nrobots significantly limits research in real-world scenes. To address this\nissue, we propose AhaRobot, a low-cost and fully open-source dual-arm mobile\nmanipulation robot system with a hardware cost of only $1,000 (excluding\noptional computational resources), which is less than 1\/15 of the cost of\npopular mobile robots. The AhaRobot system consists of three components: (1) a\nnovel low-cost hardware architecture primarily composed of off-the-shelf\ncomponents, (2) an optimized control solution to enhance operational precision\nintegrating dual-motor backlash control and static friction compensation, and\n(3) a simple remote teleoperation method RoboPilot. We use handles to control\nthe dual arms and pedals for whole-body movement. The teleoperation process is\nlow-burden and easy to operate, much like piloting. RoboPilot is designed for\nremote data collection in embodied scenarios. Experimental results demonstrate\nthat RoboPilot significantly enhances data collection efficiency in complex\nmanipulation tasks, achieving a 30% increase compared to methods using 3D mouse\nand leader-follower systems. It also excels at completing extremely\nlong-horizon tasks in one go. Furthermore, AhaRobot can be used to learn\nend-to-end policies and autonomously perform complex manipulation tasks, such\nas pen insertion and cleaning up the floor. We aim to build an affordable yet\npowerful platform to promote the development of embodied tasks on real devices,\nadvancing more robust and reliable embodied AI. All hardware and software\nsystems are available at https:\/\/aha-robot.github.io.",
        "We consider Shimura varieties associated to a unitary group of signature\n$(n-s,s)$ where $n$ is even. For these varieties, by using the spin splitting\nmodels from Zachos-Zhao, we construct flat, Cohen-Macaulay, and normal $p$-adic\nintegral models with reduced special fiber and with an explicit\nmoduli-theoretic description over odd primes $p$ which ramify in the imaginary\nquadratic field with level subgroup at $p$ given by the stabilizer of a\n$\\pi$-modular lattice in the hermitian space. We prove that the special fiber\nof the corresponding splitting model is stratified by an explicit poset with a\ncombinatorial description, similar to Bijakowski-Hernandez, and we describe its\nirreducible components. Additionally, we prove the closure relations for this\nstratification.",
        "Final report for a Deutsche Forschungsgemeinschaft, Eigenestelle Grant,\nsummarizing work mainly on uniaxial-pressure-dependent nuclear magnetic\nresonance (NMR) investigations of BaFe$_2$As$_2$. We have conducted systematic\n$^{75}$As NMR experiments in BaFe$_2$As$_2$ under in-situ controlled conditions\nof uniaxial pressure. We find that the electric field gradient (EFG),\nspin--lattice relaxation rate T$_1^{-1}$, spin--spin relaxation rate\nT$_2^{-1}$, and Knight shift $K$ at the As site are sensitive to applied\nuniaxial pressure. These properties allow us to locally probe the nematic\nsusceptibility, as well as orbital and spin degrees of freedom. Our spectral\nmeasurements in the magnetic state provide no evidence for spin reorientation\nbelow the T$_N$ for both positive and negative applied uniaxial pressure up to\nthe point of sample failure.",
        "Physics-Informed Neural Networks (PINNs) solve partial differential equations\n(PDEs) by embedding governing equations and boundary\/initial conditions into\nthe loss function. However, enforcing Dirichlet boundary conditions accurately\nremains challenging, often leading to soft enforcement that compromises\nconvergence and reliability in complex domains. We propose a hybrid approach,\nPINN-FEM, which combines PINNs with finite element methods (FEM) to impose\nstrong Dirichlet boundary conditions via domain decomposition. This method\nincorporates FEM-based representations near the boundary, ensuring exact\nenforcement without compromising convergence. Through six experiments of\nincreasing complexity, PINN-FEM outperforms standard PINN models, showcasing\nsuperior accuracy and robustness. While distance functions and similar\ntechniques have been proposed for boundary condition enforcement, they lack\ngenerality for real-world applications. PINN-FEM bridges this gap by leveraging\nFEM near boundaries, making it well-suited for industrial and scientific\nproblems.",
        "We analyze the dynamics of a first order confinement\/deconfinement phase\ntransition in an expanding medium using an effective boundary description\nfitted to the holographic Witten model. We observe and analyze hot plasma\nremnants, which do not cool down or nucleate bubbles despite the expansion of\nthe system. The appearance of the hot remnants, the dynamics of their shrinking\nand subsequent dissolution and further heating up is very robust and persists\nin such diverse scenarios as boost-invariant expansion with a flat Minkowski\nmetric and cosmological expansion in a Friedmann-Robertson-Walker spacetime.",
        "By analyzing a $\\psi(3686)$ data sample containing\n$(107.7\\pm0.6)\\times10^{6}$ events taken with the BESIII detector at the BEPCII\nstorage ring in 2009, the $\\chi_{c0}$ resonance parameters are precisely\nmeasured using $\\chi_{c0,c2} \\to \\pi^+\\pi^-\/K^+K^-$ events. The mass of\n$\\chi_{c0}$ is determined to be\n$M(\\chi_{c0})=(3415.67\\pm0.07\\pm0.06\\pm0.07$)~MeV\/$c^2$, and its full width is\n$\\Gamma(\\chi_{c0})=(12.44\\pm0.12\\pm0.12)~{\\rm MeV}$, where the first\nuncertainty is statistical, the second systematic, and the third for mass comes\nfrom $\\chi_{c2}$ mass uncertainty. These measurements improve the precision of\n$\\chi_{c0}$ mass by a factor of four and width by one order of magnitude over\nthe previous individual measurements, and significantly boost our knowledge\nabout the charmonium spectrum. Together with additional\n$(345.4\\pm2.6)\\times10^{6}$ $\\psi(3686)$ data events taken in 2012, the decay\nbranching fractions of $\\chi_{c0,c2}\\to\\pi^+\\pi^-\/K^+K^-$ are measured as well,\nwith precision improved by a factor of three compared to previous measurements.\nThese $\\chi_{c0}$ decay branching fractions provide important inputs for the\nstudy of glueballs.",
        "Standard Model (SM) with 15 Weyl fermions per family (lacking the 16th, the\nsterile right-handed neutrino $\\nu_R$) suffers from mixed gauge-gravitational\nanomalies tied to baryon number plus or minus lepton number ${\\bf B} \\pm {\\bf\nL}$ symmetry. Including $\\nu_R$ per family can cancel these anomalies, but when\n${\\bf B} \\pm {\\bf L}$ symmetry is preserved as discrete finite subgroups rather\nthan a continuous U(1), the perturbative local anomalies become nonperturbative\nglobal anomalies. In this work, we systematically enumerate these\ngauge-gravitational global anomalies involving discrete ${\\bf B} \\pm {\\bf L}$\nthat are enhanced from the fermion parity $\\mathbb{Z}_2^{\\rm F}$ to\n$\\mathbb{Z}_{2N}^{\\rm F}$, with $N=2,3,4,6,9$, etc. The ${\\bf B} \\pm {\\bf L}$\ndiscreteness is constrained by multi-fermion deformations beyond-the-SM and the\nfamily number $N_f$. Unlike the free quadratic $\\nu_R$ Majorana mass gap\npreserving the minimal $\\mathbb{Z}_2^{\\rm F}$, we explore novel scenarios\ncanceling $({\\bf B} \\pm {\\bf L})$-gravitational anomalies while preserving the\n$\\mathbb{Z}_{2N}^{\\rm F}$ discrete symmetries, featuring 4d interacting gapped\ntopological orders (potentially with or without low-energy TQFT descriptions)\nor gapless sectors (e.g., conformal field theories). We propose anomalous\nsectors as quantum dark matter to cancel SM's global anomalies. We find the\n$N_f=3$ uniqueness, when the $\\mathbb{Z}_{2N}^{\\rm F}$ representation from the\nfaithful ${\\bf B} + {\\bf L}$ for baryons at $N=N_c=3$ is extended to the\nfaithful ${\\bf Q} + N_c {\\bf L}$ for quarks at $N=N_c N_f=9$, this symmetry\nextension matches with the topological order dark matter construction. Key\nimplications include: (1) a 5th force mediating between SM and dark matter via\ndiscrete gauge fields. (2) dark matter as topological order quantum matter with\ngapped anyon excitations at ends of extended defects. (3) topological\nleptogenesis.",
        "Latent 3D reconstruction has shown great promise in empowering 3D semantic\nunderstanding and 3D generation by distilling 2D features into the 3D space.\nHowever, existing approaches struggle with the domain gap between 2D feature\nspace and 3D representations, resulting in degraded rendering performance. To\naddress this challenge, we propose a novel framework that integrates 3D\nawareness into the 2D latent space. The framework consists of three stages: (1)\na correspondence-aware autoencoding method that enhances the 3D consistency of\n2D latent representations, (2) a latent radiance field (LRF) that lifts these\n3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field\n(VAE-RF) alignment strategy that improves image decoding from the rendered 2D\nrepresentations. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art latent 3D reconstruction approaches in terms of synthesis\nperformance and cross-dataset generalizability across diverse indoor and\noutdoor scenes. To our knowledge, this is the first work showing the radiance\nfield representations constructed from 2D latent representations can yield\nphotorealistic 3D reconstruction performance.",
        "The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can\nreveal the electronic structures of materials. The related probing methods are\nwidely used in the study of magnetic materials. However, space-time inversion\n($\\mathcal{PT}$) symmetric antiferromagnets were previously believed to be\nmagneto-optically inactive. Here, we point out that this traditional\nunderstanding is incorrect. Based on our generic formulas and symmetry\nanalysis, we find that in $\\mathcal{PT}$-symmetric antiferromagnets, it is the\nquantum metric, i.e., the real part of the quantum geometry, that induces MOEs.\nCombining a tight-binding model and first-principles calculations, we confirm\nthis observation by showing MOEs in the $\\mathcal{PT}$-symmetric\nantiferromagnet. Our work demonstrates that $\\mathcal{PT}$-symmetric\nantiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and\ngreatly broaden the research on MOEs.",
        "The Set Partitioning Problem is a combinatorial optimization problem with\nwide-ranging applicability, used to model various real-world tasks such as\nfacility location and crew scheduling. However, real-world applications often\nrequire solving large-scale instances that involve hundreds of thousands of\nvariables. Although the conventional Column Generation method is popular for\nits computational efficiency, it lacks a guarantee for exact solutions. This\npaper proposes a novel solution method integrating relaxation of Column\nGeneration conditions and automatic elimination of redundant columns, aimed at\novercoming the limitations of conventional Column Generation methods in\nguaranteeing exact optimal solutions. Numerical experiments using actual bus\nroute data reveal that while the traditional method achieves an exact solution\nrate of only about 3%, the proposed method attains a rate of approximately 99%\nand remarkably improves solution accuracy."
      ]
    }
  },
  {
    "id":2411.05188,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"What is being transferred in transfer learning?",
    "start_abstract":"One desired capability for machines is the ability to transfer their knowledge of one domain another where data (usually) scarce. Despite ample adaptation learning in various deep applications, we yet do not understand what enables a successful and which part network responsible that. In this paper, provide new tools analyses address these fundamental questions. Through series on transferring block-shuffled images, separate effect feature reuse from low-level statistics show that some benefit comes latter. We present when training pre-trained weights, model stays same basin loss landscape different instances such are similar space close parameter space.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Limitations of conventional magnetic resonance imaging as a predictor of death or disability following neonatal hypoxic-ischemic encephalopathy in the late hypothermia trial"
      ],
      "abstract":[
        "Objective: To investigate if magnetic resonance imaging (MRI) is an accurate predictor for death or moderate-severe disability at 18-22 months of age among infants with neonatal encephalopathy in a trial of cooling initiated at 6-24 hours. Study design: Subgroup analysis of infants \u226536 weeks of gestation with moderate-severe neonatal encephalopathy randomized at 6-24 postnatal hours to hypothermia or usual care in a multicenter trial of late hypothermia. MRI scans were performed per each center's practice and interpreted by 2 central readers using the Eunice Kennedy Shriver National Institute of Child Health and Human Development injury score (6 levels, normal to hemispheric devastation). Neurodevelopmental outcomes were assessed at 18-22 months of age. Results: Of 168 enrollees, 128 had an interpretable MRI and were seen in follow-up (n = 119) or died (n = 9). MRI findings were predominantly acute injury and did not differ by cooling treatment. At 18-22 months, death or severe disability occurred in 20.3%. No infant had moderate disability. Agreement between central readers was moderate (weighted kappa 0.56, 95% CI 0.45-0.67). The adjusted odds of death or severe disability increased 3.7-fold (95% CI 1.8-7.9) for each increment of injury score. The area under the curve for severe MRI patterns to predict death or severe disability was 0.77 and the positive and negative predictive values were 36% and 100%, respectively. Conclusions: MRI injury scores were associated with neurodevelopmental outcome at 18-22 months among infants in the Late Hypothermia Trial. However, the results suggest caution when using qualitative interpretations of MRI images to provide prognostic information to families following perinatal hypoxia-ischemia."
      ],
      "categories":[
        "Pediatrics"
      ]
    },
    "list":{
      "title":[
        "EP240801a\/XRF 240801B: An X-ray Flash Detected by the Einstein Probe and\n  Implications of its Multiband Afterglow",
        "Lattice-Based Pruning in Recurrent Neural Networks via Poset Modeling",
        "Magnetism in Twisted Bilayer WSe$_2$",
        "Uniform Limit Theory for Network Data",
        "Parallel assembly of neutral atom arrays with an SLM using linear phase\n  interpolation",
        "On the dispersive estimates for the discrete Schr\\\"odinger equation on a\n  honeycomb lattice",
        "HIVQE: Handover Iterative Variational Quantum Eigensolver for Efficient\n  Quantum Chemistry Calculations",
        "Holography and Cheeger constant of asymptotically CMC submanifolds",
        "Reheating chiral dynamos with spin-0 and massive spin-1 torsions via\n  chiral asymmetry",
        "A Linear Programming Approach to the Super-Stable Roommates Problem",
        "Constraining eV-scale axion-like particle dark matter: insights from the\n  M87 Galaxy",
        "Linking Science and Industry: Influence of Scientific Research on\n  Technological Innovation through Patent Citations",
        "Misner-Sharp Energy and P-V Criticality in Quasi-Topological Cosmology",
        "Minimax discrete distribution estimation with self-consumption",
        "On linguistic subsets of groups and monoids",
        "Inversion-asymmetric itinerant antiferromagnets by the space group\n  symmetry",
        "New Insight of Spatial Scan Statistics via Regression Model",
        "Blow-up of radially symmetric solutions for a cubic NLS type system in\n  dimension 4",
        "Variations in the Radiation Intensity of Pulsar B0950+08: Nine Years of\n  Monitoring at 110 MHz",
        "Liquid Metal-Exfoliated SnO$_2$-Based Mixed-dimensional Heterostructures\n  for Visible-to-Near-Infrared Photodetection",
        "Dagger-Drazin Inverses",
        "A Rigid Beam Acting in the Shearing Manner to the Quasi-Crystalline\n  Half-Space",
        "The detailed balance property and chemical systems out of equilibrium",
        "Laser optothermal nanobomb for efficient flattening of nanobubbles in\n  van der Waals materials",
        "On the Separating Flow Behind a Cylinder: Insights from the Principle of\n  Minimum Pressure Gradient",
        "Quantum disorder induced by nuclear tunneling in lattice",
        "Machine Learning Based Top Quark and W Jet Tagging to Hadronic Four-Top\n  Final States Induced by SM as well as BSM Processes",
        "Circle graphs and the automorphism group of the circle",
        "Holographic QCD phase diagram for a rotating plasma in the Hawking-Page\n  approach"
      ],
      "abstract":[
        "We present multiband observations and analysis of EP240801a, a low-energy,\nextremely soft gamma-ray burst (GRB) discovered on August 1, 2024 by the\nEinstein Probe (EP) satellite, with a weak contemporaneous signal also detected\nby Fermi\/GBM. Optical spectroscopy of the afterglow, obtained by GTC and Keck,\nidentified the redshift of $z = 1.6734$. EP240801a exhibits a burst duration of\n148 s in X-rays and 22.3 s in gamma-rays, with X-rays leading by 80.61 s.\nSpectral lag analysis indicates the gamma-ray signal arrived 8.3 s earlier than\nthe X-rays. Joint spectral fitting of EP\/WXT and Fermi\/GBM data yields an\nisotropic energy $E_{\\gamma,\\rm{iso}} = (5.57^{+0.54}_{-0.50})\\times\n10^{51}\\,\\rm{erg}$, a peak energy $E_{\\rm{peak}} =\n14.90^{+7.08}_{-4.71}\\,\\rm{keV}$, a fluence ratio $\\rm\nS(25-50\\,\\rm{keV})\/S(50-100\\,\\rm{keV}) = 1.67^{+0.74}_{-0.46}$, classifying\nEP240801a as an X-ray flash (XRF). The host-galaxy continuum spectrum, inferred\nusing Prospector, was used to correct its contribution for the observed\noutburst optical data. Unusual early $R$-band behavior and EP\/FXT observations\nsuggest multiple components in the afterglow. Three models are considered:\ntwo-component jet model, forward-reverse shock model and forward-shock model\nwith energy injection. Both three provide reasonable explanations. The\ntwo-component jet model and the energy injection model imply a relatively small\ninitial energy and velocity of the jet in the line of sight, while the\nforward-reverse shock model remains typical. Under the two-component jet model,\nEP240801a may resemble GRB 221009A (BOAT) if the bright narrow beam is viewed\non-axis. Therefore, EP240801a can be interpreted as an off-beam (narrow) jet or\nan intrinsically weak GRB jet. Our findings provide crucial clues for\nuncovering the origin of XRFs.",
        "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
        "Using a self-consistent Hartree-Fock theory, we show that the recently\nobserved ferromagnetism in twisted bilayer WSe$_2$ [Nat. Commun. 16, 1959\n(2025)] can be understood as a Stoner-like instability of\ninteraction-renormalized moir\\'e bands. We quantitatively reproduce the\nobserved Lifshitz transition as function of hole filling and applied electric\nfield that marks the boundary between layer-hybridized and layer-polarized\nregimes. The former supports a ferromagnetic valley-polarized ground state\nbelow half-filling, developing a topological charge gap at half-filling for\nsmall twists. At larger twist angles there is a transition to a gapped\ntriangular N\\'eel antiferromagnet. The layer-polarized regime supports a stripe\nantiferromagnet below half-filling and a wing-shaped multiferroic ground state\nabove half-filling. We map the evolution of these states as a function of\nfilling factor, electric field, twist angle, and interaction strength. Beyond\nproviding an understanding of recent experiments, our methodology is applicable\nto a broad class of moir\\'e systems.",
        "I present a novel uniform law of large numbers (ULLN) for network-dependent\ndata. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive\nsuite of limit theorems and a robust variance estimator for network-dependent\nprocesses, their analysis focuses on pointwise convergence. On the other hand,\nuniform convergence is essential for nonlinear estimators such as M and GMM\nestimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I\nestablish the ULLN under network dependence and demonstrate its utility by\nproving the consistency of both M and GMM estimators. A byproduct of this work\nis a novel maximal inequality for network data, which may prove useful for\nfuture research beyond the scope of this paper.",
        "We present fast parallel rearrangement of single atoms in optical tweezers\ninto arbitrary geometries by updating holograms displayed by an ultra fast\nspatial light modulator. Using linear interpolation of the tweezer position and\nthe optical phase between the start and end arrays, we can calculate and\ndisplay holograms every 2.76(2) ms. To show the versatility of our method, we\nsort the same atomic sample into multiple geometries with success probabilities\nexceeding 99 % per imaging and rearrangement cycle. This makes the method a\nuseful tool for rearranging large atom arrays for quantum computation and\nquantum simulation.",
        "The discrete Schr\\\"odinger equation on a two-dimensional honeycomb lattice is\na fundamental tight-binding approximation model that describes the propagation\nof waves on graphene. For free evolution, we first show that the degenerate\nfrequencies of the dispersion relation are completely characterized by three\nsymmetric periodic curves (Theorem 2.1), and that the three curves meet at\nDirac points where conical singularities appear (see Figure 2.1). Based on this\nobservation, we prove the $L^1\\to L^\\infty$ dispersion estimates for the linear\nflow depending on the frequency localization (Theorem 2.3). Collecting all, we\nobtain the dispersion estimate with $O(|t|^{-2\/3})$ decay as well as Strichartz\nestimates. As an application, we prove small data scattering for a nonlinear\nmodel (Theorem 2.10). The proof of the key dispersion estimates is based on the\nassociated oscillatory integral estimates with degenerate phases and conical\nsingularities at Dirac points. Our proof is direct and uses only elementary\nmethods.",
        "A novel hybrid quantum-classical approach has been developed to efficiently\naddress the multireference quantum chemistry problem. The Handover Iterative\nVariational Quantum Eigensolver (HiVQE) is designed to accurately estimate\nground-state wavefunctions by leveraging both quantum and classical computing\nresources. In this framework, noisy intermediate-scale quantum (NISQ) hardwares\nefficiently explore electron configurations, while classical computers compute\nthe corresponding wavefunction without the effect of noise of NISQ computer,\nensuring both accuracy and computational efficiency. By generating compact yet\nchemically accurate wavefunctions, HiVQE advances quantum chemistry simulations\nand facilitates the discovery of novel materials. This approach demonstrates\nsignificant potential for overcoming the limitations of classical methods in\nstrongly correlated electronic systems.",
        "Let $(M^{n+1},g_+)$ be an asymptotically hyperbolic manifold. We compute the\nCheeger constant of conformally compact asymptotically constant mean curvature\nsubmanifolds $ \\iota : Y^{k+1} \\to (M^{n+1},g_+)$ with arbitrary codimension.\nAs an application, we provide two classes of examples of $(n+1)$-dimensional\nasymptotically hyperbolic manifolds with Cheeger constant equal to $n$, whose\nconformal infinity is of the following types: 1) positive Yamabe invariant, and\n2) negative Yamabe invariant. Moreover, in the same spirit as\nBlitz--Gover--Waldron \\cite{BlitzSamuel2021CFFa}, we show that an\nasymptotically hyperbolic manifold with umbilic boundary is conformally weakly\nPoincar\\'e--Einstein if and only if the third conformal fundamental form of the\nboundary vanishes. Next, in the space of asymptotically minimal hypersurfaces\n$Y$ within a Poincar\\'e--Einstein manifold, we identify an extrinsic conformal\ninvariant of $\\partial Y$ which obstructs the vanishing of the mean curvature\nof $Y$ to second order. This conformal invariant is a linear combination of two\nRiemannian hypersurface invariants of $\\partial Y,$ one which depends on its\nextrinsic geometry within $\\overline{Y}$ and the other on its extrinsic\ngeometry within $\\partial M;$ neither of which are conformal invariants\nindividually. Finally, we show that for asymptotically minimal hypersurfaces\nwith mean curvature vanishing to second order inside of a Poincar\\'e--Einstein\nspace, being weakly Poincar\\'e--Einstein is equivalent to the boundary of $Y$\nhaving vanishing second and third conformal fundamental forms when viewed as a\nhypersurface within the conformal infinity.",
        "Recently, Syderenko et al. (JCAP, 10: 018, 2016) investigated magnetogenesis\nand chiral asymmetry in the early hot universe. This study explores the impact\nof minimally coupling a constant torsion in their cosmological model,\nsuggesting new chiral physics. Physically, this means that if torsion is right\nchiral, the difference between the number of right and left chiralities does\nnot change. Moreover, the decay of chiral asymmetry depends on torsion\nchirality. We solve the chiral torsionful dynamo equation for magnetic field\nseeds. Magnetic helical fields are considered important for chiral fermion\nasymmetry. Even in $(3+1)$ dimensional spacetime, torsion is highly suppressed\nbeyond inflation (Eur Phys J C 82: 291, 2022). However, torsion of\n$1\\,\\mathrm{MeV}$ appears in the early universe. Equations for correlated\nmagnetic field coefficients are solved in terms of torsion. Weak magnetic\nfields of the order of $10^{-42}$ Gauss are boosted by powerful torsionful\ndynamo amplification, generating a much stronger magnetic field of the order of\n$10^{-9}$ Gauss in the present universe. A galactic magnetic field of $10^{-6}$\nGauss in the present universe, with torsion of $10^{-15}$ Gauss, leads us to a\ngalactic dynamo seed of $10^{-9}$ Gauss. We also discuss reheating dynamo\nregeneration of decaying cosmic magnetic fields during the hadronization era.\nThe relation between the reheating contribution to e-folds and the connection\nbetween CMF and temperature squared allows us to obtain dynamo amplification in\nterms of N-folds of inflation. The main innovation of this work is the\nexploration of constant torsion in a cosmological model, revealing new chiral\nphysics. This study offers a new perspective on the origin and evolution of\nmagnetic fields in the early universe.",
        "The stable roommates problem is a non-bipartite version of the well-known\nstable matching problem. Teo and Sethuraman proved that, for each instance of\nthe stable roommates problem in the complete graphs, there exists a linear\ninequality system such that there exists a feasible solution to this system if\nand only if there exists a stable matching in the given instance. The aim of\nthis paper is to extend the result of Teo and Sethuraman to the stable\nroommates problem with ties. More concretely, we prove that, for each instance\nof the stable roommates problem with ties in the complete graphs, there exists\na linear inequality system such that there exists a feasible solution to this\nsystem if and only if there exists a super-stable matching in the given\ninstance.",
        "Axion-like particles (ALPs) can account for the observed dark matter (DM) of\nthe Universe and if their masses are at the eV scale, they can decay into\ninfrared, optical and ultraviolet photons with a decay lifetime larger than the\nage of the Universe. We analyze multi-wavelength data obtained from the central\nregion of Messier 87 (M87) galaxy by several telescopes, such as, Swift,\nAstrosat, Kanata, Spitzer and International Ultraviolet Explorer in the\ninfrared to ultraviolet frequencies ($\\sim 2\\times10^{14} \\, {\\rm Hz} -\n3\\times10^{15}$ Hz), to constrain the narrow emission lines indicative of the\neV scale ALP DM decay. We derive constraints on the ALP coupling to two photons\n($g_{a\\gamma\\gamma}$) for ALP mass range $2 \\, {\\rm eV} \\lesssim m_a \\lesssim\n20 \\, {\\rm eV}$, assuming ALPs form the DM in the M87 halo. We find that our\nbounds on ALP-two-photon coupling can become stronger than the existing ones by\nan order of magnitude in the ALP mass range $8 \\, {\\rm eV} \\lesssim m_a\n\\lesssim 20 \\, {\\rm eV}$.",
        "This study explores the connection between patent citations and scientific\npublications across six fields: Biochemistry, Genetics, Pharmacology,\nEngineering, Mathematics, and Physics. Analysing 117,590 papers from 2014 to\n2023, the research emphasises how publication year, open access (OA) status,\nand discipline influence patent citations. Openly accessible papers,\nparticularly those in hybrid OA journals or green OA repositories, are\nsignificantly more likely to be cited in patents, seven times more than those\nmentioned in blogs, and over twice as likely compared to older publications.\nHowever, papers with policy-related references are less frequently cited,\nindicating that patents may prioritise commercially viable innovations over\nthose addressing societal challenges. Disciplinary differences reveal distinct\ninnovation patterns across sectors. While academic visibility via blogs or\nplatforms like Mendeley increases within scholarly circles, these have limited\nimpact on patent citations. The study also finds that increased funding,\npossibly tied to applied research trends and fully open access journals,\nnegatively affects patent citations. Social media presence and the number of\nauthors have minimal impact. These findings highlight the complex factors\nshaping the integration of scientific research into technological innovations.",
        "We presented a sound foundation of thermodynamics for a\nFriedmann-Robertson-Walker (FRW) universe from the first principle in\nground-breaking work [Hu et al., JHEP12 (2022) 168]. Based on such an approach,\nwe explore the thermodynamics of cosmology in quasi-topology gravity. Starting\nfrom the unified first law, we first obtain the well-defined Misner-Sharp\nenergy in quasi-topology cosmology. We demonstrate that the Misner-Sharp energy\nis equal to $\\rho V$ inside the apparent horizon. Further, the unified first\nlaw requires extra terms for generalized force and conjugate generalized\nposition, which are identified as thermodynamic pressure and thermodynamic\nvolume, respectively. Hence we naturally derive the equation of state of the\nFRW universe in quasi-topology gravity, and show that it undergoes $P$-$V$\nphase transitions. We calculate the critical exponents for the phase\ntransition, which may be beneficial to probe the micro theory of quasi-topology\ngravity.",
        "Learning distributions from i.i.d. samples is a well-understood problem.\nHowever, advances in generative machine learning prompt an interesting new,\nnon-i.i.d. setting: after receiving a certain number of samples, an estimated\ndistribution is fixed, and samples from this estimate are drawn and introduced\ninto the sample corpus, undifferentiated from real samples. Subsequent\ngenerations of estimators now face contaminated environments, an effect\nreferred to in the machine learning literature as self-consumption. In this\npaper, we study the effect of such contamination from previous estimates on the\nminimax loss of multi-stage discrete distribution estimation.\n  In the data accumulation setting, where all batches of samples are available\nfor estimation, we provide minimax bounds for the expected $\\ell_2^2$ and\n$\\ell_1$ losses at every stage. We show examples where our bounds match under\nmild conditions, and there is a strict gap with the corresponding\noracle-assisted minimax loss where real and synthetic samples are\ndifferentiated. We also provide a lower bound on the minimax loss in the data\nreplacement setting, where only the latest batch of samples is available, and\nuse it to find a lower bound for the worst-case loss for bounded estimate\ntrajectories.",
        "We study subsets of groups and monoids defined by language-theoretic means,\ngeneralizing the classical approach to the word problem. We expand on results\nby Herbst from 1991 to a more general setting, and for a class of languages\n$\\mathbf{C}$ we define the classes of $\\mathbf{C}^\\forall$-flat and\n$\\mathbf{C}^\\exists$-flat groups. We prove several closure results for these\nclasses of groups, prove a connection with the word problem, and characterize\n$\\mathbf{C}^\\forall$-flat groups for several classes of languages. In general,\nwe prove that the class of $\\mathbf{C}^\\forall$-flat groups is a strict\nsubclass of the class of groups with word problem in $\\mathbf{C}$, including\nfor the class $\\mathrm{REC}$ of recursive languages, for which\n$\\mathbf{C}^\\forall$-flatness for a group resp. monoid is proved to be\nequivalent to the decidability of the subgroup membership problem resp. the\nsubmonoid membership problem. We provide a number of examples, including the\nTarski monsters of Ol'shanskii, showing the difficulty of characterizing\n$\\mathbf{C}^\\exists$-flat groups. As an application of our general methods, we\nalso prove in passing that if $\\mathbf{C}$ is a full semi-$\\mathrm{AFL}$, then\nthe class of epi-$\\mathbf{C}$ groups is closed under taking finite index\nsubgroups. This answers a question recently posed by Al Kohli, Bleak & Elliot.",
        "We investigate the appearance of an inversion-asymmetric antiferromagnetism\ndue to an itinerant mechanism in nonsymmorphic systems with magnetic ions at\nWyckoff position of multiplicity 2. The key symmetries which underpin the\nexistence of such phases are established, and we derive a Landau free energy\nfrom a general microscopic electronic Hamiltonian. Our analysis reveals that\nthe stable antiferromagnetic order is largely determined by the symmetries of\nWyckoff position, the nature of the nesting between electronic bands, and the\npresence of anisotropy or nesting in high-symmetry planes of the Brillouin\nzone. We illustrate our conclusions with specific microscopic models.",
        "The spatial scan statistic is widely used to detect disease clusters in\nepidemiological surveillance. Since the seminal work by~\\cite{kulldorff1997},\nnumerous extensions have emerged, including methods for defining scan regions,\ndetecting multiple clusters, and expanding statistical models.\nNotably,~\\cite{jung2009} and~\\cite{ZHANG20092851} introduced a regression-based\napproach accounting for covariates, encompassing classical methods such as\nthose of~\\cite{kulldorff1997}. Another key extension is the expectation-based\napproach~\\citep{neill2005anomalous,neillphdthesis}, which differs from the\npopulation-based approach represented by~\\cite{kulldorff1997} in terms of\nhypothesis testing. In this paper, we bridge the regression-based approach with\nboth expectation-based and population-based approaches. We reveal that the two\napproaches are separated by a simple difference: the presence or absence of an\nintercept term in the regression model. Exploiting the above simple difference,\nwe propose new spatial scan statistics under the Gaussian and Bernoulli models.\nWe further extend the regression-based approach by incorporating the well-known\nsparse L0 penalty and show that the derivation of spatial scan statistics can\nbe expressed as an equivalent optimization problem. Our extended framework\naccommodates extensions such as space-time scan statistics and detecting\nmultiple clusters while naturally connecting with existing spatial\nregression-based cluster detection. Considering the relation to case-specific\nmodels~\\citep{she2011,10.1214\/11-STS377}, clusters detected by spatial scan\nstatistics can be viewed as outliers in terms of robust statistics. Numerical\nexperiments with real data illustrate the behavior of our proposed statistics\nunder specified settings.",
        "This paper is concerned with a cubic nonlinear Schr\\\"odinger system modeling\nthe interaction between an optical beam and its third harmonic in a material\nwith Kerr-type nonlinear response. We are mainly interested in the so-called\nenergy-critical case, that is, in dimension four. Our main result states that\nradially symmetric solutions with initial energy below that of the ground\nstates but with kinetic energy above that of the ground states must blow-up in\nfinite time. The proof of this result is based on the convexity method. As an\nindependent interest we also establish the existence of ground state solutions,\nthat is, solutions that minimize some action functional. In order to obtain our\nexistence results we use the concentration-compactness method combined with\nvariational arguments. As a byproduct, we also obtain the best constant in a\nvector critical Sobolev-type inequality.",
        "The analysis of variations in the emission intensity of the pulsar B0950+08\nfrom 2014 to 2022 with scales from minutes to years was carried out. The\nobservations were obtained in a round-the-clock daily survey conducted on the\nLarge Phased Array (LPA) radio telescope. The high variability of emission is\nshown not only from pulse to pulse, but also at scales greater than 3 min. The\naverage value of the estimated amplitude of these variations in 3.2 minutes is\n25~Jy, the modulation index is 1. The average relative amplitude of the\ninterpulse (IP) is $2.00 \\pm 0.28\\%$ of the main pulse. In individual pulses,\nthe amplitude of the interpulse may exceed the amplitude of the main pulse\n(MP), but this is a rare event. Emission is observed in almost the entire\nperiod of the pulsar. For the first time, the relative amplitude of emission\nbetween the main pulse and the interpulse (emission bridge) was measured. When\naveraging about 10 hours, it varies from $0.8\\%$ to $1.31\\%$ with an average\nvalue of $1.04 \\pm 0.28\\%$.\n  A high correlation was found between MP and IP amplitude variations both when\naveraging profiles over 3.2 minutes and when averaging over years. This\ncorrelation is due to refractive interstellar scintillation. The frequency\nscale of IP diffraction interstellar scintillation was measured for the first\ntime and it was shown that the spectral forms for IP and MP are well correlated\nand have the same frequency scale. There are strong variations in the frequency\nscale of scintillation $f_{dif}$ from session to session (time interval from\none day) on scales of 200-800 kHz. The refractive scale of scintillation for\n1-2 days has been determined. A modulation of emission with a characteristic\nscale of about 130 days was detected, which, apparently, is also associated\nwith refractive scintillation.",
        "Ultra-thin two-dimensional (2D) materials have gained significant attention\nfor making next-generation optoelectronic devices. Here, we report a large-area\nheterojunction photodetector fabricated using a liquid metal-printed 2D\n$\\text{SnO}_2$ layer transferred onto CdTe thin films. The resulting device\ndemonstrates efficient broadband light sensing from visible to near-infrared\nwavelengths, with enhanced detectivity and faster photo response than bare CdTe\nphotodetectors. Significantly, the device shows a nearly $10^5$-fold increase\nin current than the dark current level when illuminated with a 780 nm laser and\nachieves a specific detectivity of around $10^{12} \\, \\text{Jones}$, nearly two\norders of magnitude higher than a device with pure CdTe thin film.\nAdditionally, temperature-dependent optoelectronic testing shows that the\ndevice maintains a stable response up to $140^\\circ \\text{C}$ and generates\ndistinctive photocurrent at temperatures up to $80^\\circ \\text{C}$,\ndemonstrating its thermal stability. Using band structure analysis, density\nfunctional theory (DFT) calculations, and photocurrent mapping, the formation\nof a $p$-$n$ junction is indicated, contributing to the enhanced photo response\nattributed to the efficient carrier separation by the built-in potential in the\nhetero-junction and the superior electron mobility of 2D $\\text{SnO}_2$. Our\nresults highlight the effectiveness of integrating liquid metal-exfoliated 2D\nmaterials for enhanced photodetector performance.",
        "Drazin inverses are a special kind of generalized inverses that can be\ndefined for endomorphisms in any category. A natural question to ask is whether\none can somehow extend the notion of Drazin inverse to arbitrary maps -- not\nsimply endomorphisms. It turns out that this is possible and, indeed, natural\nto do so for dagger categories. This paper, thus, introduces the notion of a\ndagger-Drazin inverse, which is a new kind of generalized inverse appropriate\nfor arbitrary maps in a dagger category. This inverse is closely related to the\nDrazin inverse, for having dagger-Drazin inverses is equivalent to asking that\npositive maps have Drazin inverses. Moreover, dagger-Drazin inverses are also\nclosely related to Moore-Penrose inverses as we observe that a map has a\nMoore-Penrose inverse if and only if it is a Drazin-inverse. Furthermore, we\nexplain how Drazin inverses of opposing pairs correspond precisely to\ndagger-Drazin inverses in cofree dagger categories. We also give examples of\ndagger-Drazin inverses for matrices over (involutive) fields, bounded linear\noperators, and partial injections.",
        "An analytical model describing the action of a rigid beam under shear forces\nto the quasi-crystalline half-space is analyzed. The elasticity theory of\nquasicrystals and the method of complex variables is used. Graphs of stresses\nand displacements for both phonon and phason fields along the edge of the\nhalf-space have been obtained",
        "The detailed balance property is a fundamental property that must be\nsatisfied in all the macroscopic systems with a well defined temperature at\neach point. On the other hand, many biochemical networks work in\nnon-equilibrium conditions and they can be effectively modelled using sets of\nequations in which the detailed balance condition fails. In this paper we study\na class of \"out of equilibrium\" chemical networks that can be obtained freezing\nthe concentration of some substances in chemical networks for which the\ndetailed balance property holds. In particular, we prove that any chemical\nsystem with bidirectional chemical reactions can be extended to a system having\nadditional substances and for which the detailed balance property holds.",
        "Nanobubbles are typical nanodefects commonly existing in two-dimensional (2D)\nvan der Waals materials such as transition metal dioxides, especially after\ntheir transfer from growth substrate to target substrates. These nanobubbles,\nthough tiny, may significantly alter the local electric, optoelectronic,\nthermal, or mechanical properties of 2D materials and therefore are rather\ndetrimental to the constructed devices. However, there is no post-processing\nmethod so far that can effectively eliminate nanobubbles in 2D materials after\ntheir fabrication and transfer, which has been a major obstacle in the\ndevelopment of 2D material based devices. Here, we propose a principle, called\nlaser optothermal nanobomb (LOTB), that can effectively flatten nanobubbles in\n2D materials through a dynamic process of optothermally induced phase\ntransition and stress-pulling effect in nanobubbles. Operation of LOTB on\nmonolayer molybdenum disulfide (1L-MoS2) films shows that the surface roughness\ncan be reduced by more than 70% on a time scale of ~50 ms, without damage to\nthe intrinsic property of 1L-MoS2 as validated by micro-nano photoluminescence\nand Raman spectroscopy. Moreover, a dual-beam cascaded LOTB and a multi-shot\nLOTB strategies are proposed to increase the flattened area and processing\neffect, showing the potential of LOTB for fast nanodefect repairing in the mass\nproduction of van der Waals materials and devices.",
        "We study the separating flow over a circular cylinder with two objectives:\n(i) to demonstrate the validity of the condition of matching curvature, and\n(ii) to obtain a reasonable estimate of the separation angle in the subcritical\nregime (Re=10^4-10^5) without explicitly modeling the boundary layer. First, we\nstudy Roshko's free streamline model (1954); it is an ideal flow model with\nsheets of discontinuities that represent the separating shear layers in the\nnear wake region. The model fails to predict the correct separation angle over\na cylinder. Roshko attributed this discrepancy to the condition of matching\ncurvature, which asserts that the curvature of the separating streamline at the\nseparation point must match that of the cylinder. We show that such a condition\nis legitimate and is not the real culprit for the failure of Roshko's model in\npredicting separation. Second, we employ the principle of minimum pressure\ngradient (PMPG), which asserts that, an incompressible flow evolves by\nminimizing the total magnitude of the pressure gradient over the domain.\nEncouraged by the fact that the flow characteristics in the range Re=10^4-10^5\nare fairly independent of Re, we aim to predict the separation angle in this\nregime without modeling the boundary layer -- a task that may seem impossible,\nthough anticipated by Prandtl in his seminal paper (Prandtl 1904). Over the\nfamily of kinematically-admissible, equilibrium flows, we utilize the PMPG to\nsingle out the separating flow with the minimum pressure gradient cost.\nInterestingly, the obtained separation angles match experimental measurements\nover the regime Re=10^4-10^5.",
        "Lattice degrees of freedom (DoFs) may induce quantum disorder (QD) when\nnuclear tunneling outvies long-range order, but conventional phonon theory is\nincapable of describing such QD phases. Here we develop a method based on\npath-integral molecular dynamics to solve this problem. Its accuracy is\nverified in a double-well chain model and it is applied to a real material from\nfirst principles. A quantum order-disorder-order phase transition sequence is\ndemonstrated when varying the strength of quantum fluctuations using the\nlattice constants as the tuning factor. Combining the excitation spectra and\nR\\'enyi entanglement entropy, we pinpoint the QD region. This picture may be\ngeneral in lattice systems having soft phonon modes, not limited to quantum\nparaelectricity, in which novel entangled lattice motion and its coupling with\nother DoFs can be expected.",
        "We study the application of selected ML techniques to the recognition of a\nsubstructure of hadronic final states (jets) and their tagging based on their\npossible origin in current HEP experiments using simulated events and a\nparameterized detector simulation. The results are then compared with the\ncut-based method.",
        "We prove that $Aut({\\mathbb S}^1)$ coincides with the automorphism group of\nthe \\emph{circle graph} $\\mathcal{C}$, i.e. the intersection graph of the\nfamily of chords of ${\\mathbb S}^1$.\n  We prove that the countable subgraph of $\\mathcal{C}$ induced by the rational\nchords is a strongly universal element of the family of circle graphs, and that\nit is invariant under local complementation. The only other known connected\ngraphs that have the latter property are $K_2$ and the Rado graph.",
        "We investigate the combined effect of rotation and finite chemical potential\nin the confinement\/deconfinement transition of strongly interacting matter. The\nholographic description consists of a five-dimensional geometry that contains a\nblack hole (BH) in the deconfined (plasma) phase. The geometry is equipped with\nsome cut-off that introduces an infrared energy scale. We consider two\npossibilities: the so-called hard wall and soft wall AdS\/QCD models. The\ntransition between the plasma and hadronic phases is represented\nholographically as a Hawking-Page transition between geometries with and\nwithout a black hole. The gravitational dual of the rotating plasma at finite\ndensity is given by a Reissner-Nordstr\\\"om (RN) charged anti-de Sitter (AdS) BH\nwith non-zero angular momentum. This analysis provides the critical temperature\nof deconfinement as a function of the quark chemical potential and the plasma\nrotational velocity. For the particular case of very low temperatures and high\ndensities, it is found that the critical value of the chemical potential for\nthe transition to occur at $ T \\to 0 $ does not depend on the plasma vorticity,\nsince the effects of rotation and quark density on the critical temperature are\nshown to be independent."
      ]
    }
  },
  {
    "id":2411.19345,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"On the use of preclinical imaging to explore the principles of brain function in rodent models and their relevance for illnesses of the human mind",
    "start_abstract":"Dear Editor, We recently published in Translational Psychiatry an article that examine the strategies for evaluating brain function at the wholebrain level [1]. In this review, we covered several methods, from functional MRI to functional ultrasound to calcium imaging. For each technique, we wrote a brief history of its development, the physical notion, some key applications, its potentials, and its limitations. We concluded that methods for imaging the rodent brain at the network level are growing and will advance our understanding of brain function. A commentary by Zhuo and colleagues further enhances the complexity of addressing the issue of \u201ctranslation\u201d from animal models to patients for the discipline of psychiatry [2]. They propose that the approaches employed to develop an animal model for a psychiatric disease need to be thoroughly scrutinized and, perhaps, revised. For example, most rodent models of mental diseases are to-date established using a simple pharmacological infusion [3] and\/or psychosocial stimulation [4]. The key concern posed, however, is how these manipulations change the brain\u2019s structure and function, and whether these models genuinely reflect the pathophysiology of human mental illnesses. Especially since it is difficult to evaluate whether one can speak of inverse inference from rodents to humans. This is a true and acceptable statement. However, this is exactly what preclinical imaging aims to deliver. By mapping the dynamic responses of brain networks in animal models and compare them, if possible, with those reported in clinical studies, we can obtain quantitative data and parameters to establish whether our models are effectively translational [5]. If these metrics demonstrate temporal and spatial similarity in network-level modifications as those observed in humans, we can pursue further inquiry utilizing more intrusive and more specific methods for brain recordings in animal models. Otherwise, we must have the confidence and the correctness to move forward and attempt other solutions. Two recent examples. In 2019 we established a causal association between activity of the noradrenergic nucleus locus coeruleus (LC) and the engagement of numerous large-scale brain networks in mice, in particular of the salience and amygdala networks [6]. In addition, we could link network-changes with direct markers of norepinephrine (NE) turnover and with the distribution of NE receptors over the entire brain. The hypothesis that specific brain networks dynamics are related to LC activity and to NE receptor density derives from stress-research and pharmacological studies in humans [7, 8]. However, since it is impossible to selectively stimulate LC in people, it has remained a hypothesis for more than a decade. Our preclinical work helped confirm this causal relationship and this has direct implications for interpreting the results of clinical imaging studies on stress and anxiety behavior. More recently, the Gozzi lab described how chronic local neuronal suppression via overexpression of a potassium channel or acute silencing via chemogenetics result in a paradoxical hyperconnectivity [9]; an intriguing finding often reported in humans after stroke [10] and in early stages of Alzheimer\u2019s disease [11], but never truly understood. Using in vivo electrophysiology, they showed local inhibition improves low frequency (0.1\u20134 Hz) oscillatory power via suppression of neuronal activity not phaselocked to slow rhythms, resulting in increased slow and \u03b4 band coherence between areas that display fMRI overconnectivity. These data present causal evidence that cortical inactivation can counterintuitively augment fMRI connectivity via greater, lesslocalized slow oscillatory processes. Once again, this could be only achieved by combining functional MRI and electrophysiology with neuromodulation in animal models. These and other examples give a peek of what the future of preclinical imaging might look like: a field of research capable of delivering causal explanations to the hypotheses presented by human neuroscience, neurology and psychiatry. Lastly, I would argue against statements like \u201cthe computational complexity of human brains is billions of times that of mouse brain\u201d. While this may be true from a numerical standpoint of mere neuronal counts, preclinical neuroimaging\u2019s objective should not be per se to map every single neuron in real time but of identifying the general neural and cellular principles governing the assembly of brain networks and its breakdown in brain disorders. The field is relatively new but is moving fast and has already produced some important insights. The future is challenging and will require time, devotion and an optimal synergy between engineering, chemistry, biology, and computer science. If the community will be patient and supportive enough, there will be further important discoveries in the future.",
    "start_categories":[
      "Psychiatry"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "GAN\uff08Generative Adversarial Nets\uff09"
      ],
      "abstract":[
        "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: model G that captures the data distribution, and discriminative D estimates probability sample came from training rather than G. The procedure is to maximize of making mistake. This corresponds minimax two-player game. In space arbitrary functions D, unique solution exists, with recovering distribution equal \u00bd everywhere. case where are defined by multilayer perceptrons, entire system can be trained backpropagation. There no need any Markov chains or unrolled approximate inference networks during either generation samples. Experiments demonstrate potential through qualitative quantitative evaluation generated"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "No Silver Bullet: Towards Demonstrating Secure Software Development for\n  Danish Small and Medium Enterprises in a Business-to-Business Model",
        "Integrating epidemiological and economic models to estimate the cost of\n  simulated foot-and-mouth disease outbreaks in Brazil",
        "Improved YOLOv7 model for insulator defect detection",
        "Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients",
        "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse-\n  and Fine-grained Control",
        "Fermion localization in a extra-dimensional $f(Q,\\mathcal{T})$ gravity\n  with cuscuton dynamics",
        "Group homomorphisms induced by isometries",
        "Infinite-dimensional Lagrange-Dirac systems with boundary energy flow I:\n  Foundations",
        "The nonmodal kinetic theory of the macroscale convective flows of\n  magnetized plasma, generated by the inhomogeneous microturbulenc",
        "Measurement of the intrinsic sensitivity for a single-ended\n  accelerometer without the influence of the mounting condition",
        "Hardy-Littlewood maximal, generalized Bessel-Riesz and generalized\n  fractional integral operators in generalized Morrey and $BMO_\\phi$ spaces\n  associated with Dunkl operator on the real line",
        "The uniqueness of the core model",
        "A Systematic Method for Optimum Biomedical Wireless Power Transfer using\n  Inductive Links in Area-Constrained Implants",
        "Event-Triggered Newton-Based Extremum Seeking Control",
        "Watch Out E-scooter Coming Through: Multimodal Sensing of Mixed Traffic\n  Use and Conflicts Through Riders Ego-centric Views",
        "Si-compatible topological and infrared materials: the promise of Low-Sn\n  GeSn digital alloys",
        "Emergence of Giant Magnetic Chirality during Dimensionality Crossover of\n  Magnetic Materials",
        "Modelling Material Injection Into Porous Structures With the Theory of\n  Porous Media Under Non-isothermal Conditions",
        "Decomposition of RSA modulus applying even order elliptic curves",
        "Bounded First-Class Universe Levels in Dependent Type Theory",
        "Language Agents as Digital Representatives in Collective Decision-Making",
        "Toward 6-DOF Autonomous Underwater Vehicle Energy-Aware Position Control\n  based on Deep Reinforcement Learning: Preliminary Results",
        "Tracking Most Significant Shifts in Infinite-Armed Bandits",
        "What Kind of Relationality does Quantum Mechanics Exhibit?",
        "Modeling Human Beliefs about AI Behavior for Scalable Oversight",
        "Explicit Learning and the LLM in Machine Translation",
        "Design and construction of the multiplexing cold neutron spectrometer\n  BOYA with double-column Rowland focusing analyzers",
        "Multimodal Programming in Computer Science with Interactive Assistance\n  Powered by Large Language Model",
        "Continual Self-supervised Learning Considering Medical Domain Knowledge\n  in Chest CT Images"
      ],
      "abstract":[
        "Software developing small and medium enterprises (SMEs) play a crucial role\nas suppliers to larger corporations and public administration. It is therefore\nnecessary for them to be able to demonstrate that their products meet certain\nsecurity criteria, both to gain trust of their customers and to comply to\nstandards that demand such a demonstration. In this study we have investigated\nways for SMEs to demonstrate their security when operating in a\nbusiness-to-business model, conducting semi-structured interviews (N=16) with\npractitioners from different SMEs in Denmark and validating our findings in a\nfollow-up workshop (N=6). Our findings indicate five distinctive security\ndemonstration approaches, namely: Certifications, Reports, Questionnaires,\nInteractive Sessions and Social Proof. We discuss the challenges, benefits, and\nrecommendations related to these approaches, concluding that none of them is a\none-size-fits all solution and that more research into relative advantages of\nthese approaches and their combinations is needed.",
        "The introduction of foot-and-mouth disease (FMD) leads to substantial\neconomic impacts through animal loss, decreased livestock and meat production,\nincreased government and private spending on control and eradication measures,\nand trade restrictions. This study evaluates the direct cost-effectiveness of\nfour control and eradication scenarios of hypothetical FMD outbreaks in Rio\nGrande do Sul, Brazil. Our model simulation considered scenarios with\ndepopulation of detected farms and emergency vaccination and two enhanced\nscenarios featuring increased capacity for emergency vaccination and\ndepopulation. FMD outbreaks were simulated using a multi-host, single-pathogen\nSusceptible-Exposed-Infectious-Recovered model incorporating species-specific\ntransmission probabilities, within-farm dynamics, and spatial transmission\nfactors. The economic cost evaluation encompassed animal elimination (a.k.a.\ndepopulation), carcass disposal, visits by animal health officials, laboratory\ntesting, emergency vaccination, and sanitary barriers (a.k.a. traffic-control\npoints), and movement restrictions due to control zones. Our results provided a\nrange of predicted costs for a potential reintroduction of FMD ranging from\n$977,128 to $52,275,811. Depopulation was the most expensive, followed by local\ntraffic control points and emergency vaccination. Our results demonstrated that\nhigher rates of depopulation, or depopulation combined with vaccination, were\nthe most effective strategies to reduce long-term economic impacts despite\nhigher initial costs. Allocating more resources early in the outbreak was\ncost-effective in minimizing the overall effect and achieving faster\neradication.",
        "Insulators are crucial insulation components and structural supports in power\ngrids, playing a vital role in the transmission lines. Due to temperature\nfluctuations, internal stress, or damage from hail, insulators are prone to\ninjury. Automatic detection of damaged insulators faces challenges such as\ndiverse types, small defect targets, and complex backgrounds and shapes. Most\nresearch for detecting insulator defects has focused on a single defect type or\na specific material. However, the insulators in the grid's transmission lines\nhave different colors and materials. Various insulator defects coexist, and the\nexisting methods have difficulty meeting the practical application\nrequirements. Current methods suffer from low detection accuracy and mAP0.5\ncannot meet application requirements. This paper proposes an improved YOLOv7\nmodel for multi-type insulator defect detection. First, our model replaces the\nSPPCSPC module with the RFB module to enhance the network's feature extraction\ncapability. Second, a CA mechanism is introduced into the head part to enhance\nthe network's feature representation ability and to improve detection accuracy.\nThird, a WIoU loss function is employed to address the low-quality samples\nhindering model generalization during training, thereby improving the model's\noverall performance. The experimental results indicate that the proposed model\nexhibits enhancements across various performance metrics. Specifically, there\nis a 1.6% advancement in mAP_0.5, a corresponding 1.6% enhancement in\nmAP_0.5:0.95, a 1.3% elevation in precision, and a 1% increase in recall.\nMoreover, the model achieves parameter reduction by 3.2 million, leading to a\ndecrease of 2.5 GFLOPS in computational cost. Notably, there is also an\nimprovement of 2.81 milliseconds in single-image detection speed.",
        "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.",
        "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https:\/\/harryxd2018.github.io\/cafe-talk\/",
        "We investigate the fermion localization in extra-dimensional\n$f(Q,\\mathcal{T})$ gravity with cuscuton dynamics. This modified gravity theory\nis based on the nonmetricity scalar $Q$ and on the trace of the energy-momentum\ntensor $\\mathcal{T}$. Using the first-order formalism we construct the complete\nbrane system for three well-known superpotentials, namely the Sine-Gordon, the\npolynomial and the linear one. We show that the addition of the cuscuton term\nprovides significant modifications to the structure of the brane. In\nparticular,\n  the scalar field solutions have the form of a kink-like structure, and the\nenergy density is well localized, depending on both the modified-gravity and\nthe cuscuton parameters. Furthermore, applying probabilistic measures, for the\nlocation of the fermions for a minimal Yukawa-type coupling we arrive at a\nSchr\\\"odinger-like equation allowing for a normalizable massless mode. Our\nsolutions indicate strong brane localization only for left-chirality fermions.\nMoreover, the massive modes present solutions similar to free waves, which\nindicates that these fermions probably escape the brane. Finally, we find that\nmassive fermions have a greater sensitivity to gravitational changes in the\ncore of the brane, where oscillations with more pronounced amplitudes are\npresent.",
        "Let $G$ and $H$ be locally compact groups and consider their associate spaces\nof almost periodic functions $AP(G)$ and $AP(H)$. We investigate the continuous\ngroup homomorphisms induced by isometries of $AP(G)$ into $AP(H)$. Among\nothers, the following results are proved:\n  {\\bf Theorem} Let $G$ and $H$ be $\\sigma$-compact maximally almost periodic\nlocally compact groups. Suppose that $T$ is a non-vanishing linear isometry of\n$AP(G)$ into $AP(H)$ that respects finite dimensional unitary representations.\nThen there is a closed subgroup $H_0\\subseteq H$, a continuous group\nhomomorphism $t$ of $H_0$ onto $G$ and an character $\\gamma\\in \\widehat{H}$\nsuch that $(Tf)(h)=\\gamma (h)~f(t(h))$ for all $h\\in H_0$ and for all $f\\in\nC(G)$.\n  {\\bf Theorem} Let $G$ and $H$ be $LC$ Abelian groups and $H$ is connected.\nSuppose that $T$ is a non-vanishing linear isometry of $AP(G)$ into $AP(H)$\nthat preserves trigonometric polynomials. Then there is a closed subgroup\n$H_0\\subseteq H$, a continuous group homomorphism $t$ of $H_0$ onto $G$, an\nelement $h_0\\in H_0$, a character $\\alpha \\in \\widehat{H}$ and an unimodular\ncomplex number $a$ such that $(Tf)(h)=a\\cdot \\alpha (h)~\\cdot f(t(h-h_0))\\text{\nfor\n  all }h\\in H_0\\text{ and for all }f\\in C(G)\\text{.}$",
        "A new geometric approach to systems with boundary energy flow is developed\nusing infinite-dimensional Dirac structures within the Lagrangian formalism.\nThis framework satisfies a list of consistency criteria with the geometric\nsetting of finite-dimensional mechanics. In particular, the\ninfinite-dimensional Dirac structure can be constructed from the canonical\nsymplectic form on the system's phase space; the system's evolution equations\ncan be derived equivalently from either a variational perspective or a Dirac\nstructure perspective; the variational principle employed is a direct extension\nof Hamilton's principle in classical mechanics; and the approach allows for a\nprocess of system interconnection within its formulation. This is achieved by\ndeveloping an appropriate infinite dimensional version of the previously\ndeveloped Lagrange-Dirac systems. A key step in this construction is the\ncareful choice of a suitable dual space to the configuration space,\nspecifically, a subspace of the topological dual that captures the system's\nbehavior in both the interior and the boundary, while allowing for a natural\nextension of the canonical geometric structures of mechanics. This paper\nfocuses on systems where the configuration space consists of differential forms\non a smooth manifold with a boundary. To illustrate our theory, several\nexamples, including nonlinear wave equations, the telegraph equation, and the\nMaxwell equations are presented.",
        "In this paper, we develop the nonmodal kinetic theory of the macroscale\nconvective flows of magnetized plasma, which stem from the average motion of\nions and electrons in the electric field of the spatially inhomogeneous\nmicroturbulence. This theory bases on the two-scales approach to the solution\nof the Vlasov-Poisson system of equations for magnetized plasma, in which the\nsolutions depend simultaneously on micro and macro scales. The developed theory\npredicts the generation of the sheared poloidal convective flow and of the\nradial compressed flow with radial flow velocity gradient. It was found that\nthe macroscale (radial) inhomogeneity of the spectral intensity of the\nmicroturbulence is the condition necessary for the development of the\ntwo-dimensional non-diffusive convective plasma flows. The developed theory\nincludes the theory of the evolution of the microscale turbulence in the\nsheared-compressed convective flows, formed by the microturbulence, and the\ntheory of the slow macroscale evolution of a bulk of plasma by the\ncompressed-sheared convective flows.",
        "The calibration technique for accelerometers has been internationally\ndeveloped for up to 20 kHz to ensure the reliability of vibration measurement.\nHowever, it has been established that the calibrated sensitivity changes at\nover 10 kHz depending on the mounting conditions, and this makes it difficult\nto accurately measure the characteristics of accelerometers and degrades the\naccuracy of high-frequency vibration measurements. Thus, in this study, we\ndeveloped a reversed-calibration method for measuring the intrinsic sensitivity\nof an accelerometer without the influence of the mounting conditions. Through\ndemonstration experiment, the intrinsic resonance structure of the\naccelerometer at approximately 45.8 kHz was adequately determined. Furthermore,\nthe result was independently confirmed by fitting the conventional\nadapter-calibration results up to 100 kHz with four different materials based\non the dynamic three-body model. Concurrently, the material dependency observed\nduring adapter calibration was quantitatively analyzed, after which its\nrelationship with the Young's modulus was extracted. Overall, these results\ndeepen our understanding of the performance of the accelerometer at above 10\nkHz, which is essential in vibration metrology and accelerometer development.",
        "The analysis of Morrey spaces, generalized Morrey spaces and $BMO_\\phi$\nspaces related to the Dunkl operators on $\\mathbb{R}$ are covered in this\npaper. We prove the boundedness of the Hardy-Littlewood maximal operators,\nBessel-Riesz operators, generalized Bessel-Riesz operators, and generalized\nfractional integral operators associated with Dunkl operators on $\\mathbb{R}$\nin the generalized Dunkl-type Morrey spaces. Further, we derive the boundedness\nof the modified version of the generalized fractional integral operators\nassociated with the Dunkl operators on $\\mathbb{R}$ in Dunkl-type $BMO_\\phi$\nspaces.",
        "The Jensen-Steel core model is a canonical inner model which plays a\nfundamental role in the meta-mathematics of set theory. Its definition depends\non exactly which hierarchy of fine-structural models of set theory, premice,\none uses. Each such hierarchy involves somewhat arbitrary decisions and working\nwith different hierarchies ostensibly leads to different versions of the core\nmodel. We show that in some contexts, abstract properties of the core model\nuniquely determine it; that is, there is at most one inner model with these\nproperties.",
        "In the context of implantable bioelectronics, this work provides new insights\ninto maximizing biomedical wireless power transfer (BWPT) via the systematic\ndevelopment of inductive links. This approach addresses the specific challenges\nof power transfer efficiency (PTE) optimization within the area constraints of\nbio-implants embedded in tissue. Key contributions include the derivation of an\noptimal self-inductance with S-parameter-based analyses leading to the\nco-design of planar spiral coils and L-section impedance matching networks. To\nvalidate the proposed design methodology, two coil prototypes -- one symmetric\n(type-1) and one asymmetric (type-2) -- were fabricated and tested for PTE in\npork tissue. Targeting a 20 MHz design frequency, the type-1 coil demonstrated\na state-of-the-art PTE of $\\sim$ 4\\% (channel length = 15 mm) with a return\nloss (RL) $>$ 20 dB on both the input and output sides, within an area\nconstraint of $<$ 18 $ \\times $ 18 mm$^{2}$. In contrast, the type-2 coil\nachieved a PTE of $\\sim$ 2\\% with an RL $>$ 15 dB, for a smaller receiving coil\narea of $<$ 5x5 mm$^{2}$ for the same tissue environment. To complement the\ncoils, we demonstrate a 65 nm test chip with an integrated energy harvester,\nwhich includes \\asif{a} 30-stage rectifier and low-dropout regulator (LDO),\nproducing a stable $\\sim$ 1V DC output within tissue medium, matching\ntheoretical predictions and simulations. Furthermore, we provide a robust and\ncomprehensive guideline for advancing efficient inductive links for various\nBWPT applications, with shared resources in GitHub available for utilization by\nthe broader community.",
        "This paper proposes the incorporation of static event-triggered control in\nthe actuation path of Newton-based extremum seeking and its comparison with the\nearlier gradient version. As in the continuous methods, the convergence rate of\nthe gradient approach depends on the unknown Hessian of the nonlinear map to be\noptimized, whereas the proposed event-triggered Newton-based extremum seeking\neliminates this dependence, becoming user-assignable. This is achieved by means\nof a dynamic estimator for the Hessian's inverse, implemented as a Riccati\nequation filter. Lyapunov stability and averaging theory for discontinuous\nsystems are applied to analyze the closed-loop system. Local exponential\npractical stability is guaranteed to a small neighborhood of the extremum point\nof scalar and static maps. Numerical simulations illustrate the advantages of\nthe proposed approach over the previous gradient method, including improved\nconvergence speed, followed by a reduction in the amplitude and updating\nfrequency of the control signals.",
        "E-scooters are becoming a popular means of urban transportation. However,\nthis increased popularity brings challenges, such as road accidents and\nconflicts when sharing space with traditional transport modes. An in-depth\nunderstanding of e-scooter rider behaviour is crucial for ensuring rider\nsafety, guiding infrastructure planning, and enforcing traffic rules. This\nstudy investigated the rider behaviour through a naturalistic study with 23\nparticipants equipped with a bike computer, eye-tracking glasses and cameras.\nThey followed a pre-determined route, enabling multi-modal data collection. We\nanalysed and compared gaze movements, speed, and video feeds across three\ntransport infrastructure types: a pedestrian-shared path, a cycle lane and a\nroadway. Our findings reveal unique challenges e-scooter riders face, including\ndifficulty keeping up with cyclists and motor vehicles due to speed limits on\nshared e-scooters, risks in signalling turns due to control lose, and limited\nacceptance in mixed-use spaces. The cycle lane showed the highest average\nspeed, the least speed change points, and the least head movements, supporting\nits suitability as dedicated infrastructure for e-scooters. These findings are\nfacilitated through multimodal sensing and analysing the e-scooter riders'\nego-centric view, which show the efficacy of our method in discovering the\nbehavioural dynamics of the riders in the wild. Our study highlights the\ncritical need to align infrastructure with user behaviour to improve safety and\nemphasises the importance of targeted safety measures and regulations,\nespecially when e-scooter riders share spaces with pedestrians or motor\nvehicles. The dataset and analysis code are available at\nhttps:\/\/github.com\/HiruniNuwanthika\/Electric-Scooter-Riders-Multi-Modal-Data-Analysis.git.",
        "Recently, GeSn alloys have attracted much interest for direct-gap infrared\nphotonics and as potential topological materials which are compatible with the\nsemiconductor industry. However, for photonics, the high-Sn content required\nleads to low detectivity, associated with poor material quality, and the (>35%)\nSn required for topological properties have been out of reach experimentally.\nHere, we demonstrate that by patterning the Sn distribution within Ge, the\nelectronic properties have a far greater tunability than is possible with the\nrandom alloy. For the GeSn \\delta-digital alloy (DA) formed by confining Sn\natoms in atomic layer(s) along the [111] direction of Ge, we show that ~10% Sn\ncan lead to a triple-point semimetal. These findings are understood in terms of\nSn ordering causing spatial separation of Sn and Ge band edges, leading to band\ninversion. This mechanism can also lead to a weak topological insulator, Weyl\nsemimetal, and enables tunable direct bandgaps down to 2 meV, covering the\nentire infrared range. Our findings are generally applicable to other\nsemiconductors DAs and point to a new class of currently unexplored topological\nsystems accessible by epitaxy and establish the promise of low-Sn GeSn DAs for\napplication as infrared laser diodes and photodetectors in Si photonic\nintegrated circuits and infrared image sensors.",
        "Chirality, an intrinsic preference for a specific handedness, is a\nfundamental characteristic observed in nature. In magnetism, magnetic chirality\narises from the anti-symmetric Dzyaloshinskii-Moriya interaction in competition\nwith the symmetric Heisenberg exchange interaction. Traditionally, the\nanti-symmetric interaction has been considered minor relative to the symmetric\ninteraction. In this study, we demonstrate an observation of giant magnetic\nchirality during the dimensionality crossover of magnetic materials from\nthree-dimensional to two-dimensional. The ratio between the anti-symmetric and\nsymmetric interactions exhibits a reversal in their dominance over this\ncrossover, overturning the traditional consideration. This observation is\nvalidated theoretically using a non-local interaction model and tight-binding\ncalculation with distinct pairing schemes for each exchange interaction\nthroughout the crossover. Additional experiments investigating the asphericity\nof orbital moments corroborate the robustness of our findings. Our findings\nhighlight the critical role of dimensionality in shaping magnetic chirality and\noffer strategies for engineering chiral magnet states with unprecedented\nstrength, desired for the design of spintronic materials.",
        "In this work, the Theory of Porous Media (TPM) is employed to model\npercutaneous vertebroplasty, a medical procedure in which acrylic cement is\ninjected into cancellous vertebral bone. Previously, isothermal macroscale\nmodels have been derived to describe this material injection and the mechanical\ninteractions which arise. However, the temperature of the injected cement is\ntypically below the human body temperature, necessitating the extension of\nthese models to the non-isothermal case. Following the modelling principles of\nthe TPM and considering local thermal non-equilibrium conditions, our model\nintroduces three energy balances as well as additional constitutive relations.\nIf restricted to local thermal equilibrium conditions, our model equations are\nin agreement with other examples of TPM-based models. We observe that our model\nelicits physically reasonable behaviour in numerical simulations which employ\nparameter values and initial and boundary conditions relevant for our\napplication. Noting that we neglect capillary effects, we claim our model to be\nthermodynamically consistent despite the employment of simplifying assumptions\nduring its derivation, such as the Coleman and Noll procedure.",
        "An efficient integer factorization algorithm would reduce the security of all\nvariants of the RSA cryptographic scheme to zero. Despite the passage of years,\nno method for efficiently factoring large semiprime numbers in a classical\ncomputational model has been discovered. In this paper, we demonstrate how a\nnatural extension of the generalized approach to smoothness, combined with the\nseparation of $2$-adic point orders, leads us to propose a factoring algorithm\nthat finds (conjecturally) the prime decomposition $N = pq$ in subexponential\ntime $L(\\sqrt 2+o(1), \\min(p,q))$. This approach motivated by the papers\n\\cite{Len}, \\cite{MMV} and \\cite{PoZo} is based on a more careful investigation\nof pairs $(E,Q)$, where $Q$ is a point on an elliptic curve $E$ over $\\Z _N$.\nSpecifically, in contrast to the familiar condition that the largest prime\ndivisor $P^+(\\ord Q_p)$ of the reduced order $\\ord Q_p$ does not divide\n$\\#E(\\F_q)$ we focus on the relation between $P^+(\\ord Q_r)$ and the smallest\nprime number $l_{\\min}(E,Q)$ separating the orders $\\ord Q_p$ and $\\ord Q_q$.\nWe focus on the ${\\calE}_2$ family of even order elliptic curves over $\\Z_N$\nsince then the condition $l_{\\min}(E,Q)\\le 2$ holds true for large fraction of\npoints $(x,y)\\in E(\\Z_N)$. Moreover if we know the pair $(E,Q)$ such that\n$P^+(\\ord Q_r)\\le t<l_{\\min}(E,Q)$ and $d=\\max_{r\\in \\{p,q\\}}(\\ord Q_r)$ is\nlarge in comparison to $\\min_{r\\in \\{p,q\\}}|a_r(E)|\\neq 0$ then we can\ndecompose $N$ in deterministic time $t^{1+o(1)}$ by representing $N$ in base\n$d$.",
        "In dependent type theory, being able to refer to a type universe as a term\nitself increases its expressive power, but requires mechanisms in place to\nprevent Girard's paradox from introducing logical inconsistency in the presence\nof type-in-type. The simplest mechanism is a hierarchy of universes indexed by\na sequence of levels, typically the naturals. To improve reusability of\ndefinitions, they can be made level polymorphic, abstracting over level\nvariables and adding a notion of level expressions. For even more expressive\npower, level expressions can be made first-class as terms themselves, and level\npolymorphism is subsumed by dependent functions quantifying over levels.\nFurthermore, bounded level polymorphism provides more expressivity by being\nable to explicitly state constraints on level variables. While semantics for\nfirst-class levels with constraints are known, syntax and typing rules have not\nbeen explicitly written down. Yet pinning down a well-behaved syntax is not\ntrivial; there exist prior type theories with bounded level polymorphism that\nfail to satisfy subject reduction. In this work, we design an explicit syntax\nfor a type theory with bounded first-class levels, parametrized over arbitrary\nwell-founded sets of levels. We prove the metatheoretic properties of subject\nreduction, type safety, consistency, and canonicity, entirely mechanized from\nsyntax to semantics in Lean.",
        "Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.",
        "The use of autonomous underwater vehicles (AUVs) for surveying, mapping, and\ninspecting unexplored underwater areas plays a crucial role, where\nmaneuverability and power efficiency are key factors for extending the use of\nthese platforms, making six degrees of freedom (6-DOF) holonomic platforms\nessential tools. Although Proportional-Integral-Derivative (PID) and Model\nPredictive Control controllers are widely used in these applications, they\noften require accurate system knowledge, struggle with repeatability when\nfacing payload or configuration changes, and can be time-consuming to\nfine-tune. While more advanced methods based on Deep Reinforcement Learning\n(DRL) have been proposed, they are typically limited to operating in fewer\ndegrees of freedom. This paper proposes a novel DRL-based approach for\ncontrolling holonomic 6-DOF AUVs using the Truncated Quantile Critics (TQC)\nalgorithm, which does not require manual tuning and directly feeds commands to\nthe thrusters without prior knowledge of their configuration. Furthermore, it\nincorporates power consumption directly into the reward function. Simulation\nresults show that the TQC High-Performance method achieves better performance\nto a fine-tuned PID controller when reaching a goal point, while the TQC\nEnergy-Aware method demonstrates slightly lower performance but consumes 30%\nless power on average.",
        "We study an infinite-armed bandit problem where actions' mean rewards are\ninitially sampled from a reservoir distribution. Most prior works in this\nsetting focused on stationary rewards (Berry et al., 1997; Wang et al., 2008;\nBonald and Proutiere, 2013; Carpentier and Valko, 2015) with the more\nchallenging adversarial\/non-stationary variant only recently studied in the\ncontext of rotting\/decreasing rewards (Kim et al., 2022; 2024). Furthermore,\noptimal regret upper bounds were only achieved using parameter knowledge of\nnon-stationarity and only known for certain regimes of regularity of the\nreservoir. This work shows the first parameter-free optimal regret bounds for\nall regimes while also relaxing distributional assumptions on the reservoir.\n  We first introduce a blackbox scheme to convert a finite-armed MAB algorithm\ndesigned for near-stationary environments into a parameter-free algorithm for\nthe infinite-armed non-stationary problem with optimal regret guarantees. We\nnext study a natural notion of significant shift for this problem inspired by\nrecent developments in finite-armed MAB (Suk & Kpotufe, 2022). We show that\ntighter regret bounds in terms of significant shifts can be adaptively attained\nby employing a randomized variant of elimination within our blackbox scheme.\nOur enhanced rates only depend on the rotting non-stationarity and thus exhibit\nan interesting phenomenon for this problem where rising rewards do not factor\ninto the difficulty of non-stationarity.",
        "In this article I elaborate on the approach to relational quantum mechanics\nsuggested by Adlam and Rovelli (2023). I suggest that this approach fills an\nimportant gap in the spectrum of relational approaches, because it posits that\nthe relational aspects of quantum mechanics are both inherent and dynamical. I\ncompare this approach to Orthodox RQM, arguing that it has a number of\nadvantages, and I show how some possible objections can be resolved.",
        "Contemporary work in AI alignment often relies on human feedback to teach AI\nsystems human preferences and values. Yet as AI systems grow more capable,\nhuman feedback becomes increasingly unreliable. This raises the problem of\nscalable oversight: How can we supervise AI systems that exceed human\ncapabilities? In this work, we propose to model the human evaluator's beliefs\nabout the AI system's behavior to better interpret the human's feedback. We\nformalize human belief models and theoretically analyze their role in inferring\nhuman values. We then characterize the remaining ambiguity in this inference\nand conditions for which the ambiguity disappears. To mitigate reliance on\nexact belief models, we then introduce the relaxation of human belief model\ncovering. Finally, we propose using foundation models to construct covering\nbelief models, providing a new potential approach to scalable oversight.",
        "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.",
        "Developing neutron spectrometers with higher counting efficiency has been an\nessential pursuit in neutron instrumentation. In this work, we present BOYA, a\nmultiplexing cold neutron spectrometers designed and implemented at the China\nAdvanced Research Reactor. Equipped with 34 angular analyzing channels spanning\n119{\\deg}, each containing 5 inelastic channels and 1 diffraction channel, BOYA\nenhances the measurement efficiency by two orders of magnitude over a\ntraditional triple-axis spectrometer. To optimize both intensity and energy\nresolution, innovative double-column Rowland focusing analyzers have been\ndeveloped. By filling the crystal gaps in the traditional Rowland focusing\ngeometry, our design enhances the neutron beam coverage without introducing\nappreciable double-scattering. Our commissioning results on vanadium and MnWO4\nhave confirmed the success of the design, establishing BOYA as a successful\nmultiplexing instrument for neutron spectroscopy.",
        "LLM chatbot interfaces allow students to get instant, interactive assistance\nwith homework, but doing so carelessly may not advance educational objectives.\nIn this study, an interactive homework help system based on DeepSeek R1 is\ndeveloped and first implemented for students enrolled in a large computer\nscience beginning programming course. In addition to an assist button in a\nwell-known code editor, our assistant also has a feedback option in our\ncommand-line automatic evaluator. It wraps student work in a personalized\nprompt that advances our educational objectives without offering answers\nstraight away. We have discovered that our assistant can recognize students'\nconceptual difficulties and provide ideas, plans, and template code in\npedagogically appropriate ways. However, among other mistakes, it occasionally\nincorrectly labels the correct student code as incorrect or encourages students\nto use correct-but-lesson-inappropriate approaches, which can lead to long and\nfrustrating journeys for the students. After discussing many development and\ndeployment issues, we provide our conclusions and future actions.",
        "We propose a novel continual self-supervised learning method (CSSL)\nconsidering medical domain knowledge in chest CT images. Our approach addresses\nthe challenge of sequential learning by effectively capturing the relationship\nbetween previously learned knowledge and new information at different stages.\nBy incorporating an enhanced DER into CSSL and maintaining both diversity and\nrepresentativeness within the rehearsal buffer of DER, the risk of data\ninterference during pretraining is reduced, enabling the model to learn more\nricher and robust feature representations. In addition, we incorporate a mixup\nstrategy and feature distillation to further enhance the model's ability to\nlearn meaningful representations. We validate our method using chest CT images\nobtained under two different imaging conditions, demonstrating superior\nperformance compared to state-of-the-art methods."
      ]
    }
  },
  {
    "id":2411.19345,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"GAN\uff08Generative Adversarial Nets\uff09",
    "start_abstract":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: model G that captures the data distribution, and discriminative D estimates probability sample came from training rather than G. The procedure is to maximize of making mistake. This corresponds minimax two-player game. In space arbitrary functions D, unique solution exists, with recovering distribution equal \u00bd everywhere. case where are defined by multilayer perceptrons, entire system can be trained backpropagation. There no need any Markov chains or unrolled approximate inference networks during either generation samples. Experiments demonstrate potential through qualitative quantitative evaluation generated",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "On the use of preclinical imaging to explore the principles of brain function in rodent models and their relevance for illnesses of the human mind"
      ],
      "abstract":[
        "Dear Editor, We recently published in Translational Psychiatry an article that examine the strategies for evaluating brain function at the wholebrain level [1]. In this review, we covered several methods, from functional MRI to functional ultrasound to calcium imaging. For each technique, we wrote a brief history of its development, the physical notion, some key applications, its potentials, and its limitations. We concluded that methods for imaging the rodent brain at the network level are growing and will advance our understanding of brain function. A commentary by Zhuo and colleagues further enhances the complexity of addressing the issue of \u201ctranslation\u201d from animal models to patients for the discipline of psychiatry [2]. They propose that the approaches employed to develop an animal model for a psychiatric disease need to be thoroughly scrutinized and, perhaps, revised. For example, most rodent models of mental diseases are to-date established using a simple pharmacological infusion [3] and\/or psychosocial stimulation [4]. The key concern posed, however, is how these manipulations change the brain\u2019s structure and function, and whether these models genuinely reflect the pathophysiology of human mental illnesses. Especially since it is difficult to evaluate whether one can speak of inverse inference from rodents to humans. This is a true and acceptable statement. However, this is exactly what preclinical imaging aims to deliver. By mapping the dynamic responses of brain networks in animal models and compare them, if possible, with those reported in clinical studies, we can obtain quantitative data and parameters to establish whether our models are effectively translational [5]. If these metrics demonstrate temporal and spatial similarity in network-level modifications as those observed in humans, we can pursue further inquiry utilizing more intrusive and more specific methods for brain recordings in animal models. Otherwise, we must have the confidence and the correctness to move forward and attempt other solutions. Two recent examples. In 2019 we established a causal association between activity of the noradrenergic nucleus locus coeruleus (LC) and the engagement of numerous large-scale brain networks in mice, in particular of the salience and amygdala networks [6]. In addition, we could link network-changes with direct markers of norepinephrine (NE) turnover and with the distribution of NE receptors over the entire brain. The hypothesis that specific brain networks dynamics are related to LC activity and to NE receptor density derives from stress-research and pharmacological studies in humans [7, 8]. However, since it is impossible to selectively stimulate LC in people, it has remained a hypothesis for more than a decade. Our preclinical work helped confirm this causal relationship and this has direct implications for interpreting the results of clinical imaging studies on stress and anxiety behavior. More recently, the Gozzi lab described how chronic local neuronal suppression via overexpression of a potassium channel or acute silencing via chemogenetics result in a paradoxical hyperconnectivity [9]; an intriguing finding often reported in humans after stroke [10] and in early stages of Alzheimer\u2019s disease [11], but never truly understood. Using in vivo electrophysiology, they showed local inhibition improves low frequency (0.1\u20134 Hz) oscillatory power via suppression of neuronal activity not phaselocked to slow rhythms, resulting in increased slow and \u03b4 band coherence between areas that display fMRI overconnectivity. These data present causal evidence that cortical inactivation can counterintuitively augment fMRI connectivity via greater, lesslocalized slow oscillatory processes. Once again, this could be only achieved by combining functional MRI and electrophysiology with neuromodulation in animal models. These and other examples give a peek of what the future of preclinical imaging might look like: a field of research capable of delivering causal explanations to the hypotheses presented by human neuroscience, neurology and psychiatry. Lastly, I would argue against statements like \u201cthe computational complexity of human brains is billions of times that of mouse brain\u201d. While this may be true from a numerical standpoint of mere neuronal counts, preclinical neuroimaging\u2019s objective should not be per se to map every single neuron in real time but of identifying the general neural and cellular principles governing the assembly of brain networks and its breakdown in brain disorders. The field is relatively new but is moving fast and has already produced some important insights. The future is challenging and will require time, devotion and an optimal synergy between engineering, chemistry, biology, and computer science. If the community will be patient and supportive enough, there will be further important discoveries in the future."
      ],
      "categories":[
        "Psychiatry"
      ]
    },
    "list":{
      "title":[
        "Variable smoothing algorithm for inner-loop-free DC composite\n  optimizations",
        "MUSE observations of V1425 Aql reveal an arc-shaped nova shell",
        "Vacancy-assisted superfluid drag",
        "Coherence of a hole spin flopping-mode qubit in a circuit quantum\n  electrodynamics environment",
        "When is the Computation of a Feature Attribution Method Tractable?",
        "Gravitational lensing by non-self-intersecting vortons",
        "High-efficiency, high-count-rate 2D superconducting nanowire\n  single-photon detector array",
        "From Bit to Block: Decoding on Erasure Channels",
        "Better early than never: A new test for superluminal gravitational wave\n  polarizations",
        "Unstable motivic and real-\\'etale homotopy theory",
        "Neutrino-Nucleus Cross Section Impacts on Neutrino Oscillation\n  Measurements",
        "A combined Lax-Wendroff\/interpolation approach with finite element\n  method for a three-dimensional system of tectonic deformation model:\n  application to landslides in Cameroon",
        "Synthesis and characterization of Nanostructured Cobalt Sulphide doped\n  with Dysprosium for photovoltaic application",
        "DO-IQS: Dynamics-Aware Offline Inverse Q-Learning for Optimal Stopping\n  with Unknown Gain Functions",
        "Second Order Fully Nonlinear Mean Field Games with Degenerate Diffusions",
        "Neutrino Mass Matrix with broken Scaling in light of LMA and Dark-LMA\n  Solutions",
        "Fourier Multi-Component and Multi-Layer Neural Networks: Unlocking\n  High-Frequency Potential",
        "Influence of Crystal Structure and Composition on Optical and Electronic\n  Properties of Pyridinium-based Bismuth Iodide Complexes",
        "What is missing from existing Lithium-Sulfur models to capture coin-cell\n  behaviour?",
        "Effect of Numerically Controlled Oscillator Bit Width in Phase Meters",
        "CINNAMON: A hybrid approach to change point detection and parameter\n  estimation in single-particle tracking data",
        "The radius of comparison for actions of Z^d on simple AH algebras",
        "Self-Supervised Learning for Solar Radio Spectrum Classification",
        "On the well-posedness of (nonlinear) rough continuity equations",
        "rmlnomogram: An R package to construct an explainable nomogram for any\n  machine learning algorithms",
        "The finite basis problem for additively idempotent semirings of order\n  four, II",
        "Bialgebras, Manin triples of Malcev-Poisson algebras and\n  post-Malcev-Poisson algebras",
        "Regularized $\\zeta_{\\Delta}(1)$ for polyhedra",
        "Crisis, Country, and Party Lines: Politicians' Misinformation Behavior\n  and Public Engagement"
      ],
      "abstract":[
        "We propose a variable smoothing algorithm for minimizing a nonsmooth and\nnonconvex cost function. The cost function is the sum of a smooth function and\na composition of a difference-of-convex (DC) function with a smooth mapping. At\neach step of our algorithm, we generate a smooth surrogate function by using\nthe Moreau envelope of each weakly convex function in the DC function, and then\nperform the gradient descent update of the surrogate function. The proposed\nalgorithm does not require any inner loop unlike many existing algorithms for\nDC problem. We also present a convergence analysis in terms of a DC critical\npoint for the proposed algorithm as well as its application to robust phase\nretrieval.",
        "Nova shells are the remnants of a nova eruption in a cataclysmic variable\nsystem. By studying their geometry we can better understand the physical\nmechanisms that shape them during the nova eruption. A nova shell that\nchallenges our current understanding of these processes is the shell observed\naround V1425 Aql. It has at least two different components: an inner and\nsymmetric shell, and an outer and asymmetric shell, with the latter expanding\nfaster than the former. The physical reason behind the asymmetric ejecta is not\nclear. We aim to characterize the properties and differences between these two\ncomponents to understand the origin behind the unusual shape. We acquired MUSE\ndata to study the spatial position and kinematics of the expanding gas across\nthe shell. Our analysis includes channel maps, position-velocity diagrams, and\nthe reconstruction of the 3D geometry of the nova shell. Several emission lines\nare detected within the MUSE wavelength coverage, including but not limited to\nBalmer, Oxygen, Nitrogen, and Helium lines. There are significant differences\nin the spectra of the inner and outer shells, with the latter being observed\nonly in forbidden transitions, while the inner shows a mix of forbidden and\nallowed ones. Our analysis reveals that the outer shell has a geometry\nconsistent with an arc-shaped structure that partially encircles the more\nspherical inner shell. Within the inner shell, clumpy structures start to be\nnoticeable in the lines of H$\\alpha$+[Nii]. We have constrained the geometry of\nthe outer shell to an arc-shaped structure, although the physical reason behind\nits origin is still eluding us. Further monitoring of the evolution of both\nshells in this object might help to clarify the mechanism behind this unusual\nconfiguration.",
        "We study superfluid drag in the two-component, two-dimensional Bose-Hubbard\nmodel with infinitely strong repulsive interactions. We demonstrate, with a\ncombination of analytic and numeric techniques, that the motion of holes leads\nto strong dissipationless coupling between currents in the two components. We\nshow that this behavior is attributable to polaronic correlations that emerge\nin the presence of spin currents, and which can be observed in experiments.",
        "The entanglement of microwave photons and spin qubits in silicon represents a\npivotal step forward for quantum information processing utilizing semiconductor\nquantum dots. Such hybrid spin circuit quantum electrodynamics (cQED) has been\nachieved by granting a substantial electric dipole moment to a spin by\nde-localizing it in a double quantum dot under spin-orbit interaction, thereby\nforming a flopping-mode (FM) spin qubit. Despite its promise, the coherence\nproperties demonstrated to date remain insufficient to envision FM spin qubits\nas practical single qubits. Here, we present a FM hole spin qubit in a silicon\nnanowire coupled to a high-impedance niobium nitride microwave resonator for\nreadout. We report Rabi frequencies exceeding 100 MHz and coherence times in\nthe microsecond range, resulting in a high single gate quality factor of 380.\nThis establishes FM spin qubits as fast and reliable qubits. Moreover, using\nthe large frequency tunability of the FM qubit, we reveal for the first time\nthat photonic effects predominantly limit coherence, with radiative decay being\nthe main relaxation channel and photon shot-noise inducing dephasing. These\nresults highlight that optimized microwave engineering can unlock the potential\nof FM spin qubits in hybrid cQED architectures, offering a scalable and robust\nplatform for fast and coherent spin qubits with strong coupling to microwave\nphotons.",
        "Feature attribution methods have become essential for explaining machine\nlearning models. Many popular approaches, such as SHAP and Banzhaf values, are\ngrounded in power indices from cooperative game theory, which measure the\ncontribution of features to model predictions. This work studies the\ncomputational complexity of power indices beyond SHAP, addressing the\nconditions under which they can be computed efficiently. We identify a simple\ncondition on power indices that ensures that computation is polynomially\nequivalent to evaluating expected values, extending known results for SHAP. We\nalso introduce Bernoulli power indices, showing that their computation can be\nsimplified to a constant number of expected value evaluations. Furthermore, we\nexplore interaction power indices that quantify the importance of feature\nsubsets, proving that their computation complexity mirrors that of individual\nfeatures.",
        "We present exact solutions to the Nambu-Goto equations for thin vortons\nstabilized by chiral currents. The solutions describe a class of\nnon-self-intersecting, stationary loops with arbitrary shapes. In addition to\nthe trivial circular and the Kibble-Turok vortons, we also derive a\ntwo-parameter family that incorporates the first, second, and third harmonic\nmodes. We found that, in general, the vorton's constraints allow for\nconstructing families of solutions with arbitrary harmonic modes. We further\ninvestigate the gravitational lensing effects associated with these solutions\nunder the weak-field and thin-lens approximations. For circular vortons, the\nlensing exhibits a sharp discontinuity separating two regions with distinctly\ndifferent distortions. The corresponding Einstein ring co-exist alongside an\nalmost undistorted source image. This effect is significantly amplified in the\ncase of non-circular vortons, highlighting their potential observational\nsignatures.",
        "Superconducting nanowire single-photon detectors (SNSPDs) are the current\nleading technology for the detection of single-photons in the near-infrared\n(NIR) and short-wave infrared (SWIR) spectral regions, due to record\nperformance in terms of detection efficiency, low dark count rate, minimal\ntiming jitter, and high maximum count rates. The various geometry and design\nparameters of SNSPDs are often carefully tailored to specific applications,\nresulting in challenges in optimising each performance characteristic without\nadversely impacting others. In particular, when scaling to larger array\nformats, the key challenge is to manage the heat load generated by the many\nreadout cables in the cryogenic cooling system. Here we demonstrate a\npractical, self-contained 64-pixel SNSPD array system which exhibits high\nperformance of all operational parameters, for use in the strategically\nimportant SWIR spectral region. The detector is an 8x8 array of 27.5 x 27.8\n{\\mu}m pixels on a 30 {\\mu}m pitch, which leads to an 80 -- 85% fill factor. At\na wavelength of 1550nm, a uniform average per-pixel photon detection efficiency\nof 77.7% was measured and the observed system detection efficiency (SDE) across\nthe entire array was 65%. A full performance characterisation is presented,\nincluding a dark count rate of 20 cps per pixel, full-width-half-maximum (FWHM)\njitter of 100 ps per pixel, a 3-dB maximum count rate of 645 Mcps and no\nevidence of crosstalk at the 0.1% level. This camera system therefore\nfacilitates a variety of picosecond time-resolved measurement-based\napplications that include biomedical imaging, quantum communications, and\nlong-range single-photon light detection and ranging (LiDAR) and 3D imaging.",
        "We provide a general framework for bounding the block error threshold of a\nlinear code $C\\subseteq \\mathbb{F}_2^N$ over the erasure channel in terms of\nits bit error threshold. Our approach relies on understanding the minimum\nsupport weight of any $r$-dimensional subcode of $C$, for all small values of\n$r$. As a proof of concept, we use our machinery to obtain a new proof of the\ncelebrated result that Reed-Muller codes achieve capacity on the erasure\nchannel with respect to block error probability.",
        "In some beyond-Einstein theories of gravity, gravitational waves can contain\nup to six polarizations, which are allowed to propagate at different speeds\nfaster than light. These different propagation speeds imply that polarizations\ngenerated by the same source will not arrive simultaneously at the detector.\nCurrent constraints on the speed of propagation of transverse-traceless\npolarizations, however, indicate that any additional polarizations must arrive\nwith or before the transverse-traceless ones. We propose a new technique to\ntest for the existence of superluminal, non-transverse-traceless polarizations\nthat arrive in the data before a gravitational-wave observation of\ntransverse-traceless modes. We discuss the circumstances in which these\nnon-transverse-traceless polarizations would be detectable and what constraints\ncould be placed if they are not detected. To determine whether this new test of\ngeneral relativity with gravitational wave observations is practical, we\noutline and address many of the challenges it might face. Our arguments lead us\nto conclude that this new test is not only physically well-motivated but also\nfeasible with current detectors.",
        "We prove that for any base scheme $S$, real \\'etale motivic (unstable)\nhomotopy theory over $S$ coincides with unstable semialgebraic topology over\n$S$ (that is, sheaves of spaces on the real spectrum of $S$). Moreover we show\nthat for pointed connected motivic spaces over $S$, the real \\'etale motivic\nlocalization is given by smashing with the telescope of the map $\\rho: S^0 \\to\n{\\mathbb G}_m$.",
        "The challenges in neutrino-nucleus cross section modeling and its impact on\nneutrino oscillation experiments are widely recognized. However, a\ncomprehensive and theoretically robust estimation of cross section\nuncertainties has been lacking, and few studies have quantitatively examined\ntheir impact on oscillation measurements. In this work, we evaluate the effect\nof cross section uncertainties on oscillation parameters using setups inspired\nby NOvA and DUNE. To characterize these uncertainties, we adopt multiple\nneutrino-nucleus event generators and simulate a realistic experimental\nprocedure that incorporates near-detector data and near-to-far-detector\nextrapolation. Our results confirm that cross section uncertainties cannot\nsignificantly bias oscillation results in current statistics-dominated\nexperiments like NOvA. However, they could lead to substantial bias for future\nsystematics-dominated experiments like DUNE, even when near-detector data are\nemployed to mitigate uncertainties. These findings underscore the need for\nfurther studies on the quantitative impacts of cross section modeling, improved\nstrategies to utilize near-detector data and the PRISM concept, and more robust\ncross section models to optimize the success of future experiments.",
        "This paper develops an efficient computational technique to assess the\nlandslide responses to tectonic deformation and to predict the implications of\nlarge bedrocks landslides on the short and long-term development of the\ndisasters. The considered equations represent a three-dimensional system of\ngeological structure deformation subject to suitable initial and boundary\nconditions. The space derivatives are approximated using the finite element\nprocedure while the approximation in time derivative is obtained using the\nLax-Wendroff and interpolation techniques. The new approach is so called a\ncombined Lax-Wendroff\/interpolation method with finite element method. The\nmodified Lax-Wendroff\/interpolation scheme is employed to efficiently treat the\ntime derivative term and to provide a suitable time step restriction for\nstability. Under this time step requirement, both stability and error estimates\nof the new algorithm are deeply analyzed using a constructed strong norm. The\ntheory suggests that the developed computational technique is second-order\naccurate in time and spatial convergent with order O(h^{p}), where $h$ denotes\nthe space size and p is a positive integer. A wide set of numerical examples\nare carried out to confirm the theoretical results and to demonstrate the\nutility and validity of the proposed numerical scheme. An application to\nlandslides observed in west and center regions in Cameroon from October 2019 to\nNovember 2024, are discussed.",
        "The technological world is in search of environmentally friendly methods of\ngenerating energy for the use of the ever growing needs of power for\nsustainable development of visually all facet of life. Solar energy form an\nenvironmentally friendly solution for the reduction of global warming. For\nusage in photovoltaic applications, this work explores the production and\ncharacterisation of nanostructured cobalt sulphide (CoS) doped with dysprosium\n(Dy), a rare earth element. In order to increase CoS's efficiency in solar\nenergy conversion, rare earth doping is used to improve its optical,\nelectrical, and structural characteristics. To maximize performance, precise Dy\ndoping doses were used in the electrochemical synthesis of the nanomaterials.\nUV-visible spectroscopy were among the characterization methods used to examine\nthe optical and structural characteristics of the doped CoS nanomaterials.\nSignificant alterations in the bandgap and optical absorption behavior are\nshown by the results, indicating that Dy-Doped CoS nanostructures hold great\npromise for enhanced photovoltaic performance. This study opens the door for\nthe use of Dy-doped CoS in renewable energy technologies by demonstrating their\nviability as prospective materials for next-generation solar cells.",
        "We consider Inverse Optimal Stopping (IOS) problem where, based on stopped\nexpert trajectories, one aims to recover the optimal stopping region through\ncontinuation and stopping gain functions approximation. The uniqueness of the\nstopping region allows the use of IOS in real-world applications with safety\nconcerns. While current state-of-the-art inverse reinforcement learning methods\nrecover both a Q-function and the corresponding optimal policy, they fail to\naccount for specific challenges posed by optimal stopping problems. These\ninclude data sparsity near the stopping region, non-Markovian nature of the\ncontinuation gain, a proper treatment of boundary conditions, the need for a\nstable offline approach for risk-sensitive applications, and a lack of a\nquality evaluation metric. These challenges are addressed with the proposed\nDynamics-Aware Offline Inverse Q-Learning for Optimal Stopping (DO-IQS), which\nincorporates temporal information by approximating the cumulative continuation\ngain together with the world dynamics and the Q-function without querying to\nthe environment. Moreover, a confidence-based oversampling approach is proposed\nto treat the data sparsity problem. We demonstrate the performance of our\nmodels on real and artificial data including an optimal intervention for\ncritical events problem.",
        "In this article, we study the global-in-time well-posedness of second order\nmean field games (MFGs) with both nonlinear drift functions simultaneously\ndepending on the state, distribution and control variables, and the diffusion\nterm depending on both state and distribution. Besides, the diffusion term is\nallowed to be degenerate, unbounded and even nonlinear in the distribution, but\nit does not depend on the control. First, we establish the global\nwell-posedness of the corresponding forward-backward stochastic differential\nequations (FBSDEs), which arise from the maximum principle under a so-called\n$\\beta$-monotonicity commonly used in the optimal control theory. The\n$\\beta$-monotonicity admits more interesting cases, as representative examples\nincluding but not limited to the displacement monotonicity, the small mean\nfield effect condition or the Lasry-Lions monotonicity; and ensures the\nwell-posedness result in diverse non-convex examples. In our settings, we pose\nassumptions directly on the drift and diffusion coefficients and the cost\nfunctionals, rather than indirectly on the Hamiltonian, to make the conditions\nmore visible. Our probabilistic method tackles the nonlinear dynamics with a\nlinear but infinite dimensional version, and together with our recently\nproposed cone property for the adjoint processes, following in an almost\nstraightforward way the conventional approach to the classical stochastic\ncontrol problem, we derive a sufficiently good regularity of the value\nfunctional, and finally show that it is the unique classical solution to the\nMFG master equation. Our results require fairly few conditions on the\nfunctional coefficients for solution of the MFG, and a bit more conditions --\nwhich are least stringent in the contemporary literature -- for classical\nsolution of the MFG master equation.",
        "In the present work we have investigated some patterns of broken ``scaling\"\nansatz of the neutrino mass matrix. The scaling neutrino mass matrix is\ndisallowed by the current neutrino oscillation data as, among others, it\npredicts vanishing reactor angle ($\\theta_{13}=0$). We study its possible\nbreaking scenarios in light of the large mixing angle (LMA) and Dark-LMA\nsolutions suggested by current neutrino oscillation data. The normal\nhierarchical neutrino mass spectrum is ruled out in all three possible breaking\npatterns. Also, one of the interesting features of these breaking scenarios is\nthe interplay between $\\theta_{23}$-octant and possible CP violation. We find\nthat the model allows maximal CP violation for $\\theta_{23}$ above $6\\%$ of its\nmaximal value which, interestingly, is close to its current best-fit value for\ninverted hierarchical neutrino masses. We have, also, investigated the\nimplications for effective Majorana neutrino mass parameter $|M_{ee}|$ for\nallowed breaking patterns. The correlation behavior of Majorana CP phases,\nwhich can be probed in $0\\nu\\beta\\beta$ decay experiments, is found to have the\ncapability of distinguishing LMA and Dark-LMA solutions.",
        "The two most critical ingredients of a neural network are its structure and\nthe activation function employed, and more importantly, the proper alignment of\nthese two that is conducive to the effective representation and learning in\npractice. In this work, we introduce a surprisingly effective synergy, termed\nthe Fourier Multi-Component and Multi-Layer Neural Network (FMMNN), and\ndemonstrate its surprising adaptability and efficiency in capturing\nhigh-frequency components. First, we theoretically establish that FMMNNs have\nexponential expressive power in terms of approximation capacity. Next, we\nanalyze the optimization landscape of FMMNNs and show that it is significantly\nmore favorable compared to fully connected neural networks. Finally, systematic\nand extensive numerical experiments validate our findings, demonstrating that\nFMMNNs consistently achieve superior accuracy and efficiency across various\ntasks, particularly impressive when high-frequency components are present.",
        "This study investigates the impacts of structure and composition on the\noptical and electronic properties of a series of pyridinium-based bismuth\niodide complexes. Organic substrates with various functional groups, such as\n4-aminopyridine (4-Ampy), 4-methylpyridine (4-Mepy), 4-dimethyaminopyridine\n(4-Dmapy), and 4-pyridinecarbonitrile (4-CNpy) with different electron-donating\nand electron-withdrawing groups at the para position of the pyridine ring were\nemployed. Crystallographic analysis reveals various bismuth iodide structures,\nincluding 1D chains and discrete 0D motifs. The optical band gap of these\nmaterials, identified via Diffuse reflectance spectroscopy (DRS) and verified\nwith density functional theory (DFT) calculations, is influenced by the crystal\npacking and stabilising interactions. Through a comprehensive analysis,\nincluding Hirshfeld surface (HS) and void assessment, the study underscores the\ninfluence of noncovalent intermolecular interactions on crystal packing.\nSpectroscopic evaluations provide insights into electronic interactions,\nelucidating the role of electron donor and acceptor substituents within the\nlattice. Thermogravimetric differential thermal analysis (TG-DTA) indicates\nstructural stability up to 250{\\deg}C. Linear sweep voltammetry (LSV) reveals\nsignificant conductivity in the range of 10-20 mS\/pixel at 298.15 K. X-ray\nabsorption spectroscopy (XAS) at the Bi L3 edge indicates a similar oxidation\nstate and electronic environment across all samples, underscoring the role of\nbismuth centres surrounded by iodides.",
        "Lithium-sulfur (Li-S) batteries offer a promising alternative to current\nlithium-ion (Li-ion) batteries, with a high theoretical energy density,\nimproved safety and high abundance, low cost of materials. For Li-S to reach\ncommercial application, it is essential to understand how the behaviour scales\nbetween cell formats; new material development is predominately completed at\ncoin-cell level, whilst pouch-cells will be used for commercial applications.\nDifferences such as reduced electrolyte-to-sulfur (E\/S) ratios and increased\ngeometric size at larger cell formats contribute to the behavioural\ndifferences, in terms of achievable capacity, cyclability and potential\ndegradation mechanisms.\n  This work focuses on the steps required to capture and test coin-cell\nbehaviour, building upon the existing models within the literature, which\npredominately focus on pouch-cells. The areas investigated throughout this\nstudy, to improve the capability of the model in terms of scaling ability and\ncausality of predictions, include the cathode surface area, precipitation\ndynamics and C-rate dependence.",
        "Projects aiming to detect gravitational waves (GWs) in space in the\nmillihertz range will utilize interferometers to measure the separations\nbetween free-falling test masses. The phasemeter measures the phase changes of\nthe interference signals caused by the test masses' relative movements. The\nmeasurement sensitivity of the phasemeter is one of the key factors in the\ndetection. In this work, we reviewed the core metrology of the phasemeter and\nevaluated the ultra-low noise performance of the phasemeter with analog\nsignals. Frequency readout noise related to the bit width of the numerically\ncontrolled oscillator (NCO) inside the phasemeter is identified as one of the\nmain noise sources of phase measurement theoretically and experimentally. After\nincreasing the NCO bit widths, the single-channel phase noise of the phasemeter\nreached 2.0 {\\mu}rad\/Hz^{1\/2} at 6 mHz, and the differential phase noise\nreached 0.4 {\\mu}rad\/Hz^{1\/2} at 6 mHz. The phase noise performances remained\nconsistent within the carrier frequency range of 4.9 MHz to 25.1 MHz.",
        "Change point detection has become an important part of the analysis of the\nsingle-particle tracking data, as it allows one to identify moments, in which\nthe motion patterns of observed particles undergo significant changes. The\nsegmentation of diffusive trajectories based on those moments may provide\ninsight into various phenomena in soft condensed matter and biological physics.\nIn this paper, we propose CINNAMON, a hybrid approach to classifying\nsingle-particle tracking trajectories, detecting change points within them, and\nestimating diffusion parameters in the segments between the change points. Our\nmethod is based on a combination of neural networks, feature-based machine\nlearning, and statistical techniques. It has been benchmarked in the second\nAnomalous Diffusion Challenge. The method offers a high level of\ninterpretability due to its analytical and feature-based components. A\npotential use of features from topological data analysis is also discussed.",
        "Given 0 \\leq r' \\leq r \\leq \\infty, and d \\in N, we construct a simple unital\nAH algebra A with stable rank one, and a pointwise outer action \\alpha : Z^d\n\\to Aut(A), such that rc(A)=r and rc (A \\rtimes_{\\alpha} Z^d)=r'.",
        "Solar radio observation is an important way to study the Sun. Solar radio\nbursts contain important information about solar activity. Therefore, real-time\nautomatic detection and classification of solar radio bursts are of great value\nfor subsequent solar physics research and space weather warnings. Traditional\nimage classification methods based on deep learning often require consid-erable\ntraining data. To address insufficient solar radio spectrum images, transfer\nlearning is generally used. However, the large difference between natural\nimages and solar spectrum images has a large impact on the transfer learning\neffect. In this paper, we propose a self-supervised learning method for solar\nradio spectrum classification. Our method uses self-supervised training with a\nself-masking approach in natural language processing. Self-supervised learning\nis more conducive to learning the essential information about images compared\nwith supervised methods, and it is more suitable for transfer learning. First,\nthe method pre-trains using a large amount of other existing data. Then, the\ntrained model is fine-tuned on the solar radio spectrum dataset. Experiments\nshow that the method achieves a classification accuracy similar to that of\nconvolutional neural networks and Transformer networks with supervised\ntraining.",
        "Motivated by applications to fluid dynamics, we study rough differential\nequations (RDEs) and rough partial differential equations (RPDEs) with\nnon-Lipschitz drifts. We prove well-posedness and existence of a flow for RDEs\nwith Osgood drifts, as well as well-posedness of weak $L^p$-valued solutions to\nlinear rough continuity and transport equations on $\\mathbb{R}^d$ under\nDiPerna--Lions regularity conditions; a combination of the two then yields flow\nrepresentation formula for linear RPDEs. We apply these results to obtain\nexistence, uniqueness and continuous dependence for $L^1\\cap L^\\infty$-valued\nsolutions to a general class of nonlinear continuity equations. In particular,\nour framework covers the $2$D Euler equations in vorticity form with rough\ntransport noise, providing a rough analogue of Yudovich's theorem. As a\nconsequence, we construct an associated continuous random dynamical system,\nwhen the driving noise is a fractional Brownian motion with Hurst parameter $H\n\\in (1\/3,1)$. We further prove weak existence of solutions for initial\nvorticities in $L^1\\cap L^p$, for any $p\\in [1,\\infty)$.",
        "Background: Current nomogram can only be created for regression algorithm.\nProviding nomogram for any machine learning (ML) algorithms may accelerate\nmodel deployment in clinical settings or improve model availability. We\ndeveloped an R package and web application to construct nomogram with model\nexplainability of any ML algorithms. Methods: We formulated a function to\ntransform an ML prediction model into a nomogram, requiring datasets with: (1)\nall possible combinations of predictor values; (2) the corresponding outputs of\nthe model; and (3) the corresponding explainability values for each predictor\n(optional). Web application was also created. Results: Our R package could\ncreate 5 types of nomograms for categorical predictors and binary outcome\nwithout probability (1), categorical predictors and binary outcome with\nprobability (2) or continuous outcome (3), and categorical with single\nnumerical predictors and binary outcome with probability (4) or continuous\noutcome (5). Respectively, the first and remaining types optimally allowed\nmaximum 15 and 5 predictors with maximum 3,200 combinations. Web application is\nprovided with such limits. The explainability values were possible for types 2\nto 5. Conclusions: Our R package and web application could construct nomogram\nwith model explainability of any ML algorithms using a fair number of\npredictors.",
        "We study the finite basis problem for $4$-element additively idempotent\nsemirings whose additive reducts are quasi-antichains. Up to isomorphism, there\nare $93$ such algebras. We show that with the exception of the semiring $S_{(4,\n435)}$, all of them are finitely based.",
        "A Malcev-Poisson algebra is a Malcev algebra together with a commutative\nassociative algebra structure related by a Leibniz rule. In this paper, we\nintroduce the notion of Malcev-Poisson bialgebra as an analogue of a Malcev\nbialgebra and establish the equivalence between matched pairs, Manin triples\nand Malcev-Poisson bialgebras. Moreover, we introduce a new algebraic\nstructure, called post-Malcev-Poisson algebras. Post-Malcev-Poisson algebras\ncan be viewed as the underlying algebraic structures of weighted relative\nRota-Baxter operators on Malcev-Poisson algebras.",
        "Let $X$ be a compact polyhedral surface (a compact Riemann surface with flat\nconformal metric $\\mathfrak{T}$ having conical singularities). The\nzeta-function $\\zeta_\\Delta(s)$ of the Friedrichs Laplacian on $X$ is\nmeromorphic in ${\\mathbb C}$ with only (simple) pole at $s=1$. We define ${\\it\nreg\\,}\\zeta_\\Delta(1)$ as $$\\lim\\limits_{s\\to 1} \\left( \\zeta_\\Delta(s)-\\frac{\n{\\rm Area\\,}(X,\\mathfrak{T}) }{4\\pi(s-1)}\\right).$$ We derive an explicit\nexpression for this spectral invariant through the holomorphic invariants of\nthe Riemann surface $X$ and the (generalized) divisor of the conical points of\nthe metric $\\mathfrak{T}$. We study the asymptotics of ${\\it\nreg\\,}\\zeta_\\Delta(1)$ for the polyhedron obtained by sewing two other\npolyhedra along segments of small length. In addition, we calculate ${\\it\nreg\\,}\\zeta(1)$ for a family of (non-Friedrichs) self-adjoint extensions of the\nLaplacian on the tetrahedron with all the conical angles equal to $\\pi$.",
        "Politicians with large media visibility and social media audiences have a\nsignificant influence on public discourse. Consequently, their dissemination of\nmisinformation can have profound implications for society. This study\ninvestigated the misinformation-sharing behavior of 3,277 politicians and\nassociated public engagement by using data from X (formerly Twitter) during\n2020-2021. The analysis was grounded in a novel and comprehensive dataset\nincluding over 400,000 tweets covering multiple levels of governance-national\nexecutive, national legislative, and regional executive-in Germany, Italy, the\nUK, and the USA, representing distinct clusters of misinformation resilience.\nStriking cross-country differences in misinformation-sharing behavior and\npublic engagement were observed. Politicians in Italy (4.9%) and the USA (2.2%)\nexhibited the highest rates of misinformation sharing, primarily among\nfar-right and conservative legislators. Public engagement with misinformation\nalso varied significantly. In the USA, misinformation attracted over 2.5 times\nthe engagement of reliable information. In Italy, engagement levels were\nsimilar across content types. Italy is unique in crisis-related misinformation,\nparticularly regarding COVID-19, which surpassed general misinformation in both\nprevalence and audience engagement. These insights underscore the critical\nroles of political affiliation, governance level, and crisis contexts in\nshaping the dynamics of misinformation. The study expands the literature by\nproviding a cross-national, multi-level perspective, shedding light on how\npolitical actors influence the proliferation of misinformation during crisis."
      ]
    }
  },
  {
    "id":2411.18784,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"nnu-net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Multimodal Breast Parenchymal Patterns Correlation Using a Patient-Specific Biomechanical Model"
      ],
      "abstract":[
        "In this paper, we aim to produce a realistic 2-D projection of the breast parenchymal distribution from a 3-D breast magnetic resonance image (MRI). To evaluate the accuracy of our simulation, we compare our results with the local breast density (i.e., density map) obtained from the complementary full-field digital mammogram. To achieve this goal, we have developed a fully automatic framework, which registers MRI volumes to X-ray mammograms using a subject-specific biomechanical model of the breast. The optimization step modifies the position, orientation, and elastic parameters of the breast model to perform the alignment between the images. When the model reaches an optimal solution, the MRI glandular tissue is projected and compared with the one obtained from the corresponding mammograms. To reduce the loss of information during the ray-casting, we introduce a new approach that avoids resampling the MRI volume. In the results, we focus our efforts on evaluating the agreement of the distributions of glandular tissue, the degree of structural similarity, and the correlation between the real and synthetic density maps. Our approach obtained a high-structural agreement regardless the glandularity of the breast, whilst the similarity of the glandular tissue distributions and correlation between both images increase in denser breasts. Furthermore, the synthetic images show continuity with respect to large structures in the density maps."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "Constructing Cell-type Taxonomy by Optimal Transport with Relaxed\n  Marginal Constraints",
        "Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step\n  under Gaussian Mixtures Data with Structure",
        "Neural encoding with affine feature response transforms",
        "Correlation effects in two-dimensional MX_2 and MA_2Z_4 (M= Nb, Ta; X=\n  S, Se, Te; A=Si, Ge; Z=N, P) cold metals: Implications for device\n  applications",
        "Integrated Sensing and Edge AI: Realizing Intelligent Perception in 6G",
        "Novel Dark Matter Signatures",
        "Universal Counterdiabatic Driving",
        "A Realistic Projection for Constraining Neutron Star Equation of State\n  with the LIGO-Virgo-KAGRA Detector Network in the A+ Era",
        "Pressure Effect on the Spin Density Wave Transition in\n  La$_2$PrNi$_2$O$_{6.96}$",
        "Scotoseesaw mechanism from a $Z_3$ symmetry of matter",
        "A Method to Simultaneously Facilitate All Jet Physics Tasks",
        "Note on two-point mean square displacement",
        "Event-Based Limit Order Book Simulation under a Neural Hawkes Process:\n  Application in Market-Making",
        "A Unified Regularization Approach to High-Dimensional Generalized Tensor\n  Bandits",
        "Homotopy types of complexes of hyperplanes in quasi-median graphs and\n  applications to right-angled Artin groups",
        "Relative knot probabilities in confined lattice polygons",
        "Discovery of a new phase-transient cyclotron line in A0535+26:\n  Constraints on the accretion geometry",
        "Quasi-compactness and statistical properties for discontinuous systems\n  semi-conjugated to piecewise convex maps with countable branches",
        "D-separation for applied researchers: understanding how to interpret\n  directed acyclic graphs",
        "Irredundant bases for soluble groups",
        "Quantum geometric tensors from sub-bundle geometry",
        "Potato Potahto in the FAO-GAEZ Productivity Measures? Nonclassical\n  Measurement Error with Multiple Proxies",
        "On relationships between symmetric and non-symmetric cone separation\n  based on Bishop-Phelps separating cones in real normed spaces",
        "Pricing Experiments in Matching Marketplaces under Interference: Designs\n  and Estimators",
        "Mixed Fe-Mo carbide prepared by a sonochemical synthesis as highly\n  efficient nitrate reduction electrocatalyst",
        "Compact 780 nm Rb Optical Clock",
        "On the equivalence between galaxy angular correlation function and power\n  spectrum in constraining primordial non-Gaussianity",
        "Constructing a Tutte polynomial for graphs embedded in surfaces",
        "Higher-order multiscale method and its convergence analysis for\n  nonlinear thermo-electric coupling problems of composite structures"
      ],
      "abstract":[
        "The rapid emergence of single-cell data has facilitated the study of many\ndifferent biological conditions at the cellular level. Cluster analysis has\nbeen widely applied to identify cell types, capturing the essential patterns of\nthe original data in a much more concise form. One challenge in the cluster\nanalysis of cells is matching clusters extracted from datasets of different\norigins or conditions. Many existing algorithms cannot recognize new cell types\npresent in only one of the two samples when establishing a correspondence\nbetween clusters obtained from two samples. Additionally, when there are more\nthan two samples, it is advantageous to align clusters across all samples\nsimultaneously rather than performing pairwise alignment. Our approach aims to\nconstruct a taxonomy for cell clusters across all samples to better annotate\nthese clusters and effectively extract features for downstream analysis. A new\nsystem for constructing cell-type taxonomy has been developed by combining the\ntechnique of Optimal Transport with Relaxed Marginal Constraints (OT-RMC) and\nthe simultaneous alignment of clusters across multiple samples. OT-RMC allows\nus to address challenges that arise when the proportions of clusters vary\nsubstantially between samples or when some clusters do not appear in all the\nsamples. Experiments on more than twenty datasets demonstrate that the taxonomy\nconstructed by this new system can yield highly accurate annotation of cell\ntypes. Additionally, sample-level features extracted based on the taxonomy\nresult in accurate classification of samples.",
        "In this work, we study the training and generalization performance of\ntwo-layer neural networks (NNs) after one gradient descent step under\nstructured data modeled by Gaussian mixtures. While previous research has\nextensively analyzed this model under isotropic data assumption, such\nsimplifications overlook the complexities inherent in real-world datasets. Our\nwork addresses this limitation by analyzing two-layer NNs under Gaussian\nmixture data assumption in the asymptotically proportional limit, where the\ninput dimension, number of hidden neurons, and sample size grow with finite\nratios. We characterize the training and generalization errors by leveraging\nrecent advancements in Gaussian universality. Specifically, we prove that a\nhigh-order polynomial model performs equivalent to the nonlinear neural\nnetworks under certain conditions. The degree of the equivalent model is\nintricately linked to both the \"data spread\" and the learning rate employed\nduring one gradient step. Through extensive simulations, we demonstrate the\nequivalence between the original model and its polynomial counterpart across\nvarious regression and classification tasks. Additionally, we explore how\ndifferent properties of Gaussian mixtures affect learning outcomes. Finally, we\nillustrate experimental results on Fashion-MNIST classification, indicating\nthat our findings can translate to realistic data.",
        "Current linearizing encoding models that predict neural responses to sensory\ninput typically neglect neuroscience-inspired constraints that could enhance\nmodel efficiency and interpretability. To address this, we propose a new method\ncalled affine feature response transform (AFRT), which exploits the brain's\nretinotopic organization. Applying AFRT to encode multi-unit activity in areas\nV1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant\ncomputations and enhances the performance of current linearizing encoding\nmodels by segmenting each neuron's receptive field into an affine retinal\ntransform, followed by a localized feature response. Remarkably, by factorizing\nreceptive fields into a sequential affine component with three interpretable\nparameters (for shifting and scaling) and response components with a small\nnumber of feature weights per response, AFRT achieves encoding with orders of\nmagnitude fewer parameters compared to unstructured models. We show that the\nretinal transform of each neuron's encoding agrees well with the brain's\nreceptive field. Together, these findings suggest that this new subset within\nspatial transformer network can be instrumental in neural encoding models of\nnaturalistic stimuli.",
        "Cold metals, characterized by their distinctive band structures, hold promise\nfor innovative electronic devices such as tunnel diodes with negative\ndifferential resistance (NDR) effect and field-effect transistors (FETs) with\nsub-60 mV\/dec subthreshold swing (SS). In this study, we employ the GW\napproximation and HSE06 hybrid functional to investigate the correlation\neffects on the electronic band structure of two-dimensional (2D) cold metallic\nmaterials, specifically focusing on MX_2 and MA_2Z_4 (M=Nb, Ta; X=S, Se, Te;\nA=Si, Ge; Z= N, P) compounds in 1H structure. These materials exhibit a unique\nband structure with an isolated metallic band around the Fermi energy, denoted\nas W_m, as well as two energy gaps: the internal gap E^I_g below the Fermi\nlevel and the external gap E^E_g above the Fermi level. These three electronic\nstructure parameters play a decisive role in determining the current-voltage\n(I-V) characteristics of tunnel diodes, the nature of the NDR effect, and the\ntransfer characteristics and SS value of FETs. Our calculations reveal that\nboth GW and HSE06 methods yield consistent electronic structure properties for\nall studied compounds. We observed a consistent increase in both internal and\nexternal band gaps, as well as metallic bandwidths, across all pn-type cold\nmetal systems. Notably, the internal band gap E^I_g exhibits the most\nsubstantial enhancement, highlighting the sensitivity of these materials to\ncorrelation effects. In contrast, the changes in the metallic bandwidth W_m and\nexternal band gap E^E_g are relatively modest. These findings offer valuable\ninsights for designing and optimizing cold metal-based devices. Materials like\nNbSi_2N_4, NbGe_2N_4, and TaSi_2N_4 show particular promise for\nhigh-performance NDR tunnel diodes and sub-60 mV\/dec SS FETs.",
        "Sensing and edge artificial intelligence (AI) are envisioned as two essential\nand interconnected functions in sixth-generation (6G) mobile networks. On the\none hand, sensing-empowered applications rely on powerful AI models to extract\nfeatures and understand semantics from ubiquitous wireless sensors. On the\nother hand, the massive amount of sensory data serves as the fuel to\ncontinuously refine edge AI models. This deep integration of sensing and edge\nAI has given rise to a new task-oriented paradigm known as integrated sensing\nand edge AI (ISEA), which features a holistic design approach to communication,\nAI computation, and sensing for optimal sensing-task performance. In this\narticle, we present a comprehensive survey for ISEA. We first provide technical\npreliminaries for sensing, edge AI, and new communication paradigms in ISEA.\nThen, we study several use cases of ISEA to demonstrate its practical relevance\nand introduce current standardization and industrial progress. Next, the design\nprinciples, metrics, tradeoffs, and architectures of ISEA are established,\nfollowed by a thorough overview of ISEA techniques, including digital air\ninterface, over-the-air computation, and advanced signal processing. Its\ninterplay with various 6G advancements, e.g., new physical-layer and networking\ntechniques, are presented. Finally, we present future research opportunities in\nISEA, including the integration of foundation models, convergence of ISEA and\nintegrated sensing and communications (ISAC), and ultra-low-latency ISEA.",
        "Celestial observations often exhibit inexplicable planetary dependencies when\nthe timing of an observable is projected onto planetary heliocentric positions.\nThis is possible only for incident, non-relativistic streams. Notably, the\ncelebrated dark matter (DM) in the Universe can form streams in our vicinity\nwith speeds of about 240 km\/s. Since gravitational impact scales with\n$1\/(\\text{velocity})^2$, all solar system objects, including the Sun and the\nMoon, act as strong gravitational lenses, with their focal planes located\nwithin the solar system. Even the Moon can focus penetrating particles toward\nthe Earth at speeds of up to approximately 400 km\/s, covering a large portion\nof the phase space of DM constituents. Consequently, the unexpected planetary\ndependencies of solar system observables may provide an alternative to Zwicky's\ntension regarding the overestimated visible cosmic mass. In this work, an\noverlooked but unexpected planetary dependency of any local observable serves\nas an analogue to Zwicky's cosmic measurements, particularly if a similar\nmysterious behavior has been previously noted. Thus, a persistent, unexpected\nplanetary dependency represents a new tension between observation and\nexpectation. The primary argument supporting DM in line with Zwicky's paradigm\nis this planetary dependency, which, on a local scale, constitutes the novel\ntension between observation and expectation. In particular, the recurrent\nplanetary dependency of diverse observables mirrors Zwicky's cosmic tension\nwith the overestimated visible mass. No other approach accounts for so many\notherwise striking and mysterious observations in physics and medicine.",
        "Local counterdiabatic (CD) driving provides a systematic way of constructing\na control protocol to approximately suppress the excitations resulting from\nchanging some parameter(s) of a quantum system at a finite rate. However,\ndesigning CD protocols typically requires knowledge of the original Hamiltonian\na priori. In this work, we design local CD driving protocols in Krylov space\nusing only the characteristic local time scales of the system set by e.g.\nphonon frequencies in materials or Rabi frequencies in superconducting qubit\narrays. Surprisingly, we find that convergence of these universal protocols is\ncontrolled by the asymptotic high frequency tails of the response functions.\nThis finding hints at a deep connection between the long-time, low frequency\nresponse of the system controlling non-adiabatic effects, and the\nhigh-frequency response determined by the short-time operator growth and the\nKrylov complexity, which has been elusive until now.",
        "The LIGO-Virgo-KAGRA network in the upcoming A+ era with upgrades of both\nAdvanced LIGO and Advanced Virgo will enable more frequent and precise\nobservations of binary neutron star (BNS) mergers, improving constraints on the\nneutron star equation of state (EOS). In this study, we applied reduced order\nquadrature techniques for full parameter estimation of 3,000 simulated\ngravitational wave signals from BNS mergers at A+ sensitivity following three\nEOS models: HQC18, SLY230A, and MPA1. We found that tidal deformability tends\nto be overestimated at higher mass and underestimated at lower mass. We\npostprocessed the parameter estimation results to present our EOS recovery\naccuracies, identify biases within EOS constraints and their causes, and\nquantify the needed corrections.",
        "High-pressure studies reveal a stark contrast between the superconducting\nproperties of double-layer Ruddlesden-Popper (RP) nickelates\nLa$_2$PrNi$_2$O$_7$ and La$_3$Ni$_2$O$_7$. While La$_2$PrNi$_2$O$_7$ exhibits\nbulk superconductivity, La$_3$Ni$_2$O$_7$ displays filamentary behavior,\nsuggesting that superconductivity is confined to phase interfaces rather than\nthe bulk. Since magnetism emerges near the superconducting phase, understanding\nits differences in La$_3$Ni$_2$O$_7$ and La$_2$PrNi$_2$O$_7$ is essential for\nclarifying their underlying electronic and magnetic properties. In this work we\nstudy the magnetic responce of La$_2$PrNi$_2$O$_{6.96}$ under pressures up to\n2.3 GPa using the muon-spin rotation\/relaxation ($\\mu$SR) technique. The\napplication of external pressure increases the N\\'{e}el temperature $T_{\\rm N}$\nfrom approximately 161 K at ambient pressure ($p=0$) to about 170 K at $p=2.3$\nGPa. The temperature dependence of the internal magnetic field $B_{\\rm int}(T)$\n(i.e., the magnetic order parameter) follows the power-law relation $B_{\\rm\nint} = B_{\\rm int}(0) \\left(1 - \\left[T\/T_{\\rm N}\\right]^\\alpha \\right)^\\beta$,\nwith consistent exponent values of $\\alpha\\simeq 1.95$ and $\\beta\\simeq 0.35$\nacross different pressures. The value of the ordered moments at the Ni sites,\nwhich is proportional to $B_{\\rm int}$, remain unaffected by pressure. Our\nfindings suggest that the magnetic properties of double-layer RP nickelate\nLa$_3$Ni$_2$O$_7$ are broadly unaffected by Pr to La substitution.",
        "We show that the neutrino mass generation and the dark matter stability can\nbe governed by the center of the QCD group, which is a $Z_3$ group. Three\nright-handed neutrinos $N_{1,2,3R}$ transform under $Z_3$ as $1,w,w^2$, where\n$w=e^{i2\\pi\/3}$ is the cube root of unity, and they couple to usual lepton\ndoublets via the usual Higgs doublet $H$ and two new scalar doublets\n$\\eta,\\chi$, which transform under $Z_3$ as $1,w^2,w$, respectively. This leads\nto a scotoseesaw mechanism in which the seesaw and scotogenic neutrino mass\ngenerations are induced by the Majorana $N_{1R}$ mass and the Dirac $N_{2,3R}$\nmass, respectively. Although the lightest of the $Z_3$ fields is stabilized,\nresponsible for dark matter, the model lacks an explanation for relic density\nand\/or direct detection. The issue can be solved in a $U(1)_{B-L}$ gauge\ncompletion of the model, for which the center of the QCD group is isomorphic to\n$Z_3=\\{1,T,T^2\\}$ for $T=w^{3(B-L)}$.",
        "Machine learning has become an essential tool in jet physics. Due to their\ncomplex, high-dimensional nature, jets can be explored holistically by neural\nnetworks in ways that are not possible manually. However, innovations in all\nareas of jet physics are proceeding in parallel. We show that specially\nconstructed machine learning models trained for a specific jet classification\ntask can improve the accuracy, precision, or speed of all other jet physics\ntasks. This is demonstrated by training on a particular multiclass generation\nand classification task and then using the learned representation for different\ngeneration and classification tasks, for datasets with a different (full)\ndetector simulation, for jets from a different collision system (pp versus ep),\nfor generative models, for likelihood ratio estimation, and for anomaly\ndetection. We consider, our OmniLearn approach thus as a jet-physics foundation\nmodel. It is made publicly available for use in any area where state-of-the-art\nprecision is required for analyses involving jets and their substructure.",
        "When probe molecules of interest are embedded in a container or aggregate\nunder stochastic motion, one needs to rely on the so-called two-point mean\nsquare displacement (MSD) measurement to extract the intrinsic mobility of the\nprobes. We discuss two versions, based on the time series of relative vector or\ndistance between two probes, and summarize their basic properties compared to\nthe standard MSD. We also propose a way to extract (i) the non-Gaussianity in\nthe displacement statistics and (ii) the motional correlation between probes\nfrom the two-point MSD. The results are presented not only for independent\nprobes, but also for intramolecular probes within a long polymer, which could\nbe useful in quantifying the dynamics of chromatin loci in living cell nucleus.",
        "In this paper, we propose an event-driven Limit Order Book (LOB) model that\ncaptures twelve of the most observed LOB events in exchange-based financial\nmarkets. To model these events, we propose using the state-of-the-art Neural\nHawkes process, a more robust alternative to traditional Hawkes process models.\nMore specifically, this model captures the dynamic relationships between\ndifferent event types, particularly their long- and short-term interactions,\nusing a Long Short-Term Memory neural network. Using this framework, we\nconstruct a midprice process that captures the event-driven behavior of the LOB\nby simulating high-frequency dynamics like how they appear in real financial\nmarkets. The empirical results show that our model captures many of the broader\ncharacteristics of the price fluctuations, particularly in terms of their\noverall volatility. We apply this LOB simulation model within a Deep\nReinforcement Learning Market-Making framework, where the trading agent can now\ncomplete trade order fills in a manner that closely resembles real-market trade\nexecution. Here, we also compare the results of the simulated model with those\nfrom real data, highlighting how the overall performance and the distribution\nof trade order fills closely align with the same analysis on real data.",
        "Modern decision-making scenarios often involve data that is both\nhigh-dimensional and rich in higher-order contextual information, where\nexisting bandits algorithms fail to generate effective policies. In response,\nwe propose in this paper a generalized linear tensor bandits algorithm designed\nto tackle these challenges by incorporating low-dimensional tensor structures,\nand further derive a unified analytical framework of the proposed algorithm.\nSpecifically, our framework introduces a convex optimization approach with the\nweakly decomposable regularizers, enabling it to not only achieve better\nresults based on the tensor low-rankness structure assumption but also extend\nto cases involving other low-dimensional structures such as slice sparsity and\nlow-rankness. The theoretical analysis shows that, compared to existing\nlow-rankness tensor result, our framework not only provides better bounds but\nalso has a broader applicability. Notably, in the special case of degenerating\nto low-rank matrices, our bounds still offer advantages in certain scenarios.",
        "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "In November 2020, A0535+26 underwent one of its brightest outbursts, reaching\nnearly 12 Crab in X-ray flux. Observed by \\textit{Insight-HXMT},\n\\textit{NuSTAR}, \\textit{NICER}, and \\textit{AstroSat}, this event provided\nvaluable insights into Be\/X-ray binaries. The pulse profiles evolved\nsignificantly with luminosity, transitioning from pencil-beam to fan-beam\ngeometries. A0535+26, known for its fundamental cyclotron line at $\\sim$44 keV,\nbecame only the second source to exhibit a negative correlation between\ncyclotron line energy and flux at high luminosities, with a plateau phase\npreceding the transition from positive to negative correlation. We report the\ndiscovery of a phase-transient low-energy cyclotron line, detected in a narrow\nphase range ($\\sim$16\\%) across all seven \\textit{NuSTAR} observations during\nthe rising, peak, and declining phases of the outburst. The new line exhibited\ndramatic variations with pulse phase and luminosity. We explain this behavior\nusing an accretion geometry where the accretion column sweeps across the line\nof sight.",
        "In this paper, we establish the quasi-compactness of the transfer operator\nassociated with skew product systems that are semi-conjugate to piecewise\nconvex maps with a countably infinite number of branches. These non-invertible\nskew products admit discontinuities, with the critical set confined to a\ncountable collection of fibers. Furthermore, we demonstrate that such systems\npossess an invariant measure whose disintegration along the fibers exhibits\nbounded variation, a concept introduced and developed in this work.",
        "The assumed causal relationships depicted in a DAG are interpreted using a\nset of rules called D-separation rules. Although these rules can be implemented\nautomatically using standard software, at least a basic understanding of their\nprinciples is useful for properly using and interpreting DAGs in practice.",
        "Let $\\Delta$ be a finite set and $G$ be a subgroup of\n$\\operatorname{Sym}(\\Delta)$. An irredundant base for $G$ is a sequence of\npoints of $\\Delta$ yielding a strictly descending chain of pointwise\nstabilisers, terminating with the trivial group. Suppose that $G$ is primitive\nand soluble. We determine asymptotically tight bounds for the maximum length of\nan irredundant base for $G$. Moreover, we disprove a conjecture of Seress on\nthe maximum length of an irredundant base constructed by the natural greedy\nalgorithm, and prove Cameron's Greedy Conjecture for $|G|$ odd.",
        "The geometric properties of quantum states are crucial for understanding many\nphysical phenomena in quantum mechanics, condensed matter physics, and optics.\nThe central object describing these properties is the quantum geometric tensor,\nwhich unifies the Berry curvature and the quantum metric. In this work, we use\nthe differential-geometric framework of vector bundles to analyze the\nproperties of parameter-dependent quantum states and generalize the quantum\ngeometric tensor to this setting. This construction is based on an arbitrary\nconnection on a Hermitian vector bundle, which defines a notion of quantum\nstate transport in parameter space, and a sub-bundle projector, which\nconstrains the set of accessible quantum states. We show that the sub-bundle\ngeometry is similar to that of submanifolds in Riemannian geometry and is\ndescribed by a generalization of the Gauss-Codazzi-Mainardi equations. This\nleads to a novel definition of the quantum geometric tensor, which contains an\nadditional curvature contribution. To illustrate our results, we describe the\nsub-bundle geometry arising in the semiclassical treatment of Dirac fields\npropagating in curved spacetime and show how the quantum geometric tensor, with\nits additional curvature contributions, is obtained in this case. As a concrete\nexample, we consider Dirac fermions confined to a hyperbolic plane and\ndemonstrate how spatial curvature influences the quantum geometry. This work\nsets the stage for further exploration of quantum systems in curved geometries,\nwith applications in both high-energy physics and condensed matter systems.",
        "The FAO-GAEZ crop productivity data are widely used in Economics. However,\nthe existence of measurement error is rarely recognized in the empirical\nliterature. We propose a novel method to partially identify the effect of\nagricultural productivity, deriving bounds that allow for nonclassical\nmeasurement error by leveraging two proxies. These bounds exhaust all the\ninformation contained in the first two moments of the data. We reevaluate three\ninfluential studies, documenting that measurement error matters and that the\nimpact of agricultural productivity on economic outcomes may be smaller than\npreviously reported. Our methodology has broad applications in empirical\nresearch involving mismeasured variables.",
        "In this paper, we study relationships between symmetric and non-symmetric\nseparation of (not necessarily convex) cones by using separating cones of\nBishop-Phelps type in real normed spaces. Besides extending some known results\nfor the non-symmetric cone separation approach, we propose a new symmetric cone\nseparation approach and establish cone separation results for it by using some\ncone separation results obtained for the non-symmetric cone separation approach\ntwice (by swapping the roles of the cones). In addition to specifically\nemphasizing the results for the convex case, we also present some existence\nresults for (bounded) convex bases of convex cones. Finally, we highlight some\napplications of symmetric and non-symmetric cone separation in optimization.",
        "Interference between treated and untreated units is a source of bias in\nmarketplace experiments. In this paper, we specifically consider pricing\ninterventions, in which a platform seeks to adjust base pricing levels at the\nmarketplace level in order to increase demand. In a matching marketplace, this\ntype of experiment leads to a crucial design question: should the platform\nmatch treated and untreated units differently because they paid different\nprices? We find that standard estimation techniques are biased, but the sign of\nthis bias depends strongly on this design choice. Bias can be reduced by using\nthe ``shadow price estimator'', which relies on the optimal dual solution of\nthe platform's supply-demand matching problem -- especially when the platform\nchooses to ignore pricing differences at matching time. We validate our\nfindings both theoretically in a fluid limit setting, and numerically in a\nfinite-sample setting.",
        "Ammonia, a versatile compound that can be used as a fertilizer, chemical or\nfuel, has since long been produced through the energy-intensive Haber-Bosch\nprocess. Recently, the electrochemical nitrate reduction reaction (NO3RR) using\nelectricity generated from renewable sources has attracted widespread\nattention. However, the complex reaction pathway of NO3RR leads to the\nformation of many undesirable by-products. Herein we successfully prepared a\nmixed (FeMo)2C catalyst with good electrocatalytic NO3RR, having a NH3 yield of\n14.66 mg h-1 cm-2 and an FE of 94.35 % at low potential -0.3 V vs RHE. DFT\ncalculations show that the presence of Fe in Mo2C lattice changes the reaction\nmechanism, decreasing the potential barrier to be overcome from 1.36 to 0.89\neV. In addition, mixed Fe-Mo carbide facilitates the adsorption of\nintermediates and promotes NH3 desorption, facilitating NO3- reduction to NH3.\nIn addition, (FeMo)2C was used as cathode for Zn-NO3 battery to generate\nelectricity, producing ammonia at the same time, with a power density of 3.8\nmWcm-2 and an NH3 FE of 88 %. This work describes a new synthesis method for\nmixed metal carbides and provides a promising strategy for NH3 production.",
        "We demonstrated a compact 780 nm rubidium optical clock, which includes an\noptical frequency standard and an optical frequency comb, with an optical\nvolume of 11.6 liters. Unlike the 778 nm rubidium atomic clocks based on\ntwo-photon transition, here, the laser frequency is stabilized to the Rb D2\ntransition, using modulation transfer spectroscopy. This approach effectively\neliminates Doppler background and provides a high signal to noise ratio and\nhigh sensitivity. A nearly 300 MHz microwave signal, whose phase exactly tracks\nthat of the optical frequency standard, is generated via the optical frequency\ncomb, yielding a frequency instability of 1.91 E-13 @1 s and 5.29 E-14 @1000 s\nin the electronic domain. To the best of our knowledge, this is the most\nprecise frequency stabilization result for the first-excited-state transition\nof alkali metal atoms to date and represents the first optical clock based on\nthis transition. These results offer a promising approach for the development\nof portable optical clocks.",
        "We investigate the angular power spectrum ($C_\\ell)$ and angular correlation\nfunction ($w(\\theta)$) of galaxy number density field in the presence of the\nlocal-type primordial non-Gaussianity (PNG), explicitly accounting for the\nintegral constraint in an all-sky survey. We show that the PNG signature in\n$C_{\\ell}$ is confined to low multipoles in the linear regime, whereas its\nsignature in $w(\\theta)$ extends across a wide range of angular scales,\nincluding those below the nonlinear scale. Therefore, the equivalence between\n$C_\\ell$ and $w(\\theta)$ can be violated when scale cuts of multipoles or\nangular scales -- for example, to mitigate systematic effects -- are applied in\nthe analysis. Assuming samples of photometric galaxies divided into multiple\nredshift bins in the range $0<z<7$, we forecast the precision of constraining\nthe PNG parameter ($f_{\\rm NL}$) from the hypothetical measurements of $C_\\ell$\nor $w(\\theta)$ assuming different scale cuts in the multipoles or angular\nscales, respectively. Our results imply that the PNG information can be\nextracted from $w(\\theta)$ on relatively small angular scales such as $\\lesssim\n10$ degree for a high-redshift galaxy sample or from $w(\\theta)$ measured in a\nsurvey with partial area coverage.",
        "There are several different extensions of the Tutte polynomial to graphs\nembedded in surfaces. To help frame the different options, here we consider the\nproblem of extending the Tutte polynomial to cellularly embedded graphs\nstarting from first principles. We offer three different routes to defining\nsuch a polynomial and show that they all lead to the same polynomial. This\nresulting polynomial is known in the literature under a few different names\nincluding the ribbon graph polynomial, and 2-variable Bollobas-Riordan\npolynomial.\n  Our overall aim here is to use this discussion as a mechanism for providing a\ngentle introduction to the topic of Tutte polynomials for graphs embedded in\nsurfaces.",
        "This paper proposes a higher-order multiscale computational method for\nnonlinear thermo-electric coupling problems of composite structures, which\npossess temperature-dependent material properties and nonlinear Joule heating.\nThe innovative contributions of this work are the novel multiscale formulation\nwith the higher-order correction terms for periodic composite structures and\nthe global error estimation with an explicit rate for higher-order multiscale\nsolutions. By employing the multiscale asymptotic approach and the Taylor\nseries technique, the higher-order multiscale method is established for\ntime-dependent nonlinear thermo-electric coupling problems, which can keep the\nlocal balance of heat flux and electric charge for high-accuracy multiscale\nsimulation. Furthermore, an efficient numerical algorithm with off-line and\non-line stages is presented in detail, and corresponding convergent analysis is\nalso obtained. Two- and three-dimensional numerical experiments are conducted\nto showcase the competitive advantages of the proposed method for simulating\nthe time-dependent nonlinear thermo-electric coupling problems in composite\nstructures, not only exceptional numerical accuracy, but also less\ncomputational cost."
      ]
    }
  },
  {
    "id":2411.18784,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Multimodal Breast Parenchymal Patterns Correlation Using a Patient-Specific Biomechanical Model",
    "start_abstract":"In this paper, we aim to produce a realistic 2-D projection of the breast parenchymal distribution from a 3-D breast magnetic resonance image (MRI). To evaluate the accuracy of our simulation, we compare our results with the local breast density (i.e., density map) obtained from the complementary full-field digital mammogram. To achieve this goal, we have developed a fully automatic framework, which registers MRI volumes to X-ray mammograms using a subject-specific biomechanical model of the breast. The optimization step modifies the position, orientation, and elastic parameters of the breast model to perform the alignment between the images. When the model reaches an optimal solution, the MRI glandular tissue is projected and compared with the one obtained from the corresponding mammograms. To reduce the loss of information during the ray-casting, we introduce a new approach that avoids resampling the MRI volume. In the results, we focus our efforts on evaluating the agreement of the distributions of glandular tissue, the degree of structural similarity, and the correlation between the real and synthetic density maps. Our approach obtained a high-structural agreement regardless the glandularity of the breast, whilst the similarity of the glandular tissue distributions and correlation between both images increase in denser breasts. Furthermore, the synthetic images show continuity with respect to large structures in the density maps.",
    "start_categories":[
      "Imaging"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Analysis for non-local phase transitions close to the critical exponent\n  $s=\\frac12$",
        "An Ensemble Information Filter: Retrieving Markov-information from the\n  SPDE discretisation",
        "Dipole Polarizability of Finite Nuclei as a Probe of Neutron Stars",
        "Bayesian hierarchical non-stationary hybrid modeling for threshold\n  estimation in peak over threshold approach",
        "10 Years of Archival High-Resolution NIR Spectra: The Raw and Reduced\n  IGRINS Spectral Archive (RRISA)",
        "A-priori estimates for generalized Korteweg-de Vries equations in\n  $H^{-1}(\\mathbb{R})$",
        "Breaking the Stigma! Unobtrusively Probe Symptoms in Depression Disorder\n  Diagnosis Dialogue",
        "Klein Tunneling in Uniaxial Strained Graphene under Super-Periodic\n  Potential",
        "Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs",
        "Untwisted and Twisted R\\'{e}nyi Negativities: Toward a R\\'{e}nyi Proxy\n  for Logarithmic Negativity in Fermionic Systems",
        "Synthesis of Infinite State Systems",
        "Integral Forms in Matrix Lie Groups",
        "Goal-Oriented Interference Coordination in 6G In-Factory Subnetworks",
        "Adaptive Video Streaming with AI-Based Optimization for Dynamic Network\n  Conditions",
        "Kinetic energy density functional based on electron distribution on the\n  energy coordinate to describe covalent bond",
        "EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head\n  Synthesis",
        "High-Fidelity Simultaneous Speech-To-Speech Translation",
        "Eigenfunctions with double exponential rate of localization",
        "The Three Hundred: Gas Properties Outside of Galaxy Cluster with the\n  WHIM Contribution and Detection",
        "Nonparanormal Adjusted Marginal Inference",
        "Molecular Gas Heating, Star Formation Rate Relations, and AGN Feedback\n  in Infrared-luminous Galaxy Mergers",
        "Exact mobility edges in quasiperiodic network models with slowly varying\n  potentials",
        "FDCT: Frequency-Aware Decomposition and Cross-Modal Token-Alignment for\n  Multi-Sensor Target Classification",
        "Exoplanet Ephemerides Change Observations (ExoEcho). I. Transit Timing\n  Analysis of Thirty-Seven Exoplanets using HST\/WFC3 Data",
        "Dust coagulation assisted by streaming instability in protoplanetary\n  disks",
        "Simpson's Paradox with Any Given Number of Factors",
        "Distributive Fairness in Large Language Models: Evaluating Alignment\n  with Human Values",
        "Scan-Adaptive MRI Undersampling Using Neighbor-based Optimization (SUNO)",
        "Reconstructive martensitic phase transitions: intermittency,\n  anti-trasformation, plasticity, irreversibility"
      ],
      "abstract":[
        "We analyze the behaviour of double-well energies perturbed by fractional\nGagliardo squared seminorms in $H^s$ close to the critical exponent\n$s=\\frac12$. This is done by computing a scaling factor\n$\\lambda(\\varepsilon,s)$, continuous in both variables, such that \\[\n\\mathcal{F}^{s_\\varepsilon}_\\varepsilon(u)=\\frac{\\lambda(\\varepsilon,s_\\varepsilon)}{\\varepsilon}\\int\nW(u)dt+\\lambda(\\varepsilon,s_\\varepsilon)\\varepsilon^{(2s_\\varepsilon-1)^+}[u]_{H^{s_\\varepsilon}}^2\n\\] $\\Gamma$-converge, for any choice of $s_\\varepsilon \\to \\frac12$ as\n$\\varepsilon\\to 0$, to the sharp-interface functional found by Alberti,\nBouchitt\\'e and Seppecher with the scaling ${|\\log\\varepsilon|^{-1}}$.\nMoreover, we prove that all the values $s\\in [\\frac12,1 )$ are regular points\nfor the functional $\\mathcal{F}^{s}_\\varepsilon$ in the sense of equivalence by\n$\\Gamma$-convergence introduced by Braides and Truskinovsky, and that the\n$\\Gamma$-limits as $\\varepsilon\\to 0$ are continuous with respect to $s$. In\nparticular, the corresponding surface tensions, given by suitable non-local\noptimal-profile problems, are continuous on $[\\frac12,1)$.",
        "Ensemble-based Data Assimilation faces significant challenges in\nhigh-dimensional systems due to spurious correlations and ensemble collapse.\nThese issues arise from estimating dense dependencies with limited ensemble\nsizes. This paper introduces the Ensemble Information Filter, which encodes\nMarkov properties directly into the statistical model's precision matrix,\nleveraging structure from SPDE dynamics to constrain information to propagate\nlocally. EnIF eliminates the need for ad-hoc localisation, improving\nstatistical consistency and scalability. Numerical experiments demonstrate its\nadvantages in filtering, smoothing, and parameter estimation, making EnIF a\nrobust and efficient solution for large-scale data assimilation problems.",
        "Nuclear ground state and collective excitation properties provide a means to\nprobe the nuclear matter equation of state and establish connections between\nobservables in finite nuclei and neutron stars. Specifically, the electric\ndipole polarizability, measured with high precision in various neutron-rich\nnuclei, serves as a robust constraint on the density dependence of the symmetry\nenergy. In this Letter, we employ a class of relativistic energy density\nfunctionals in a twofold process: first, to link the electric dipole\npolarizability from recent experiments to the slope of the symmetry energy, and\nsecond, to translate this information into constraints on the tidal\ndeformability and radii of neutron stars, in connection with multimessenger\nastrophysical observations from pulsars and binary neutron stars. We provide\ncompelling evidence that the electric dipole polarizability represents a key\nnuclear observable to probe the neutron star properties. By significantly\nreducing the uncertainties in the mass-radius plane, our findings also align\nwith recent multimessenger observations.",
        "Extreme value theory (EVT) has been utilized to estimate crash risk from\ntraffic conflicts with the peak over threshold approach. However, it's\nchallenging to determine a suitable threshold to distinguish extreme conflicts\nin an objective way. The subjective and arbitrary selection of the threshold in\nthe peak over threshold approach can result in biased estimation outcomes. This\nstudy proposes a Bayesian hierarchical hybrid modeling (BHHM) framework for the\nthreshold estimation in the peak over threshold approach. Specifically, BHHM is\nbased on a piecewise function to model the general conflicts with specific\ndistribution while model the extreme conflicts with generalized Pareto\ndistribution (GPD). The Bayesian hierarchical structure is used to combine\ntraffic conflicts from different sites, incorporating covariates and\nsite-specific unobserved heterogeneity. Five non-stationary BHHM models,\nincluding Normal-GPD, Cauchy-GPD, Logistic-GPD, Gamma-GPD, and Lognormal-GPD\nmodels, were developed and compared. Traditional graphical diagnostic and\nquantile regression approaches were also used for comparison. Traffic conflicts\ncollected from three signalized intersections in the city of Surrey, British\nColumbia were used for the study. The results show that the proposed BHHM\napproach could estimate the threshold parameter objectively. The Lognormal-GPD\nmodel is superior to the other four BHHM models in terms of crash estimation\naccuracy and model fit. The crash estimates using the threshold determined by\nthe BHHM outperform those estimated based on the graphical diagnostic and\nquantile regression approaches, indicating the superiority of the proposed\nthreshold determination approach. The findings of this study contribute to\nenhancing the existing EVT methods for providing a threshold determination\napproach as well as producing reliable crash estimations.",
        "The Immersion GRating INfrared Spectrometer (IGRINS) is a compact,\nhigh-resolution (R~45,000) near-infrared spectrograph spanning 1.45 to 2.45 um\nin a single exposure. We introduce the Raw and Reduced IGRINS Spectral Archive\n(RRISA), which provides public data access for all non-proprietary IGRINS data\ntaken at McDonald Observatory's Harlan J. Smith Telescope, the Lowell Discovery\nTelescope (formerly Discovery Channel Telescope), and Gemini South. RRISA\nprovides access to raw files, reduced data products, and cross-matched IGRINS\ntargets with the SIMBAD, 2MASS, Gaia DR3, APOGEE2 DR17, and PASTEL catalogs. We\nalso introduce version 3 of the IGRINS data reduction pipeline, IGRINS PLP v3,\nwhich implements an improved cosmic ray correction, pattern noise removal, and\na new flexure correction that reduces telluric residuals. RRISA and supporting\ninformation can be found at http:\/\/igrinscontact.github.io.",
        "We prove local-in-time a-priori estimates in $H^{-1}(\\mathbb{R})$ for a\nfamily of generalized Korteweg--de Vries equations. This is the first estimate\nfor any non-integrable perturbation of the KdV equation that matches the\nregularity of the sharp well-posedness theory for KdV. In particular, we show\nthat our analysis applies to models for long waves in a shallow channel of\nwater with an uneven bottom.\n  The proof of our main result is based upon a bootstrap argument for the\nrenormalized perturbation determinant coupled with a local smoothing norm.",
        "Stigma has emerged as one of the major obstacles to effectively diagnosing\ndepression, as it prevents users from open conversations about their struggles.\nThis requires advanced questioning skills to carefully probe the presence of\nspecific symptoms in an unobtrusive manner. While recent efforts have been made\non depression-diagnosis-oriented dialogue systems, they largely ignore this\nproblem, ultimately hampering their practical utility. To this end, we propose\na novel and effective method, UPSD$^{4}$, developing a series of strategies to\npromote a sense of unobtrusiveness within the dialogue system and assessing\ndepression disorder by probing symptoms. We experimentally show that UPSD$^{4}$\ndemonstrates a significant improvement over current baselines, including\nunobtrusiveness evaluation of dialogue content and diagnostic accuracy. We\nbelieve our work contributes to developing more accessible and user-friendly\ntools for addressing the widespread need for depression diagnosis.",
        "In this article, we employ the transfer matrix method (TMM) to analytically\nexplore the impact of uniaxial strain on electron scattering in graphene under\nlocally periodic and super-periodic electrostatic potential. Our study reveals\nthat strain significantly influences electron transmission through the merging\nparameter $(\\delta)$, which modulates the Dirac cone positions. a positive\nmerging parameter ($\\delta > 0$) reduces the transmission probability by\nopening an energy gap at the merging point, while a negative merging parameter\n($\\delta < 0$) enhances transmission by bringing the Dirac cones closer,\nfacilitating electron transport at certain angles. However, the resonance peaks\nin super-periodic potential (SPP) are sharper for $\\delta < 0$, making them\nmore pronounced but increasingly difficult to resolve as the number of barriers\nincreases.",
        "This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations.",
        "Entanglement entropy is a fundamental measure of quantum entanglement for\npure states, but for large-scale many-body systems, R\\'{e}nyi entanglement\nentropy is much more computationally accessible. For mixed states, logarithmic\nnegativity (LN) serves as a widely used entanglement measure, but its direct\ncomputation is often intractable, leaving R\\'{e}nyi negativity (RN) as the\npractical alternative. In fermionic systems, RN is further classified into\nuntwisted and twisted types, depending on the definition of the fermionic\npartial transpose. However, which of these serves as the true R\\'{e}nyi proxy\nfor LN has remained unclear -- until now. In this work, we address this\nquestion by developing a robust quantum Monte Carlo (QMC) method to compute\nboth untwisted and twisted RNs, focusing on the rank-4 twisted RN, where\nnon-trivial behavior emerges. We identify and overcome two major challenges:\nthe singularity of the Green's function matrix and the exponentially large\nvariance of RN estimators. Our method is demonstrated in the Hubbard model and\nthe spinless $t$-$V$ model, revealing critical distinctions between untwisted\nand twisted RNs, as well as between rank-2 and high-rank RNs. Remarkably, we\nfind that the twisted R\\'{e}nyi negativity ratio (RNR) adheres to the area law\nand decreases monotonically with temperature, in contrast to the untwisted RNR\nbut consistent with prior studies of bosonic systems. This study not only\nestablishes the twisted RNR as a more pertinent R\\'{e}nyi proxy for LN in\nfermionic systems but also provides comprehensive technical details for the\nstable and efficient computation of high-rank RNs. Our work lays the foundation\nfor future studies of mixed-state entanglement in large-scale fermionic\nmany-body systems.",
        "The classical Church synthesis problem, solved by Buchi and Landweber, treats\nthe synthesis of finite state systems. The synthesis of infinite state systems,\non the other hand, has only been investigated few times since then, with no\ncomplete or systematic solution.\n  We present a systematic study of the synthesis of infinite state systems. The\nmain step involves the synthesis of MSO-definable parity games, which is,\nfinding MSO-definable uniform memoryless winning strategies for these games.",
        "Matrix Lie groups provide a language for describing motion in such fields as\nrobotics, computer vision, and graphics. When using these tools, we are often\nfaced with turning infinite-series expressions into more compact finite series\n(e.g., the Euler-Rodriques formula), which can sometimes be onerous. In this\npaper, we identify some useful integral forms in matrix Lie group expressions\nthat offer a more streamlined pathway for computing compact analytic results.\nMoreover, we present some recursive structures in these integral forms that\nshow many of these expressions are interrelated. Key to our approach is that we\nare able to apply the minimal polynomial for a Lie algebra quite early in the\nprocess to keep expressions compact throughout the derivations. With the series\napproach, the minimal polynomial is usually applied at the end, making it hard\nto recognize common analytic expressions in the result. We show that our\nintegral method can reproduce several series-derived results from the\nliterature.",
        "Subnetworks are expected to enhance wireless pervasiveness for critical\napplications such as wireless control of plants, however, they are\ninterference-limited due to their extreme density. This paper proposes a\ngoal-oriented joint power and multiple sub-bands allocation policy for\ninterference coordination in 6G in-factory subnetworks. Current methods for\ninterference coordination in subnetworks only focus on optimizing communication\nmetrics, such as the block error rate, without considering the goal of the\ncontrolled plants. This oversight often leads to inefficient allocation of the\nlimited radio resources. To address this, we devise a novel decentralized\ninter-subnetwork interference coordination policy optimized using a Bayesian\nframework to ensure the long-term stability of the subnetwork-controlled\nplants. Our results show that the proposed decentralized method can support\nmore than twice the density of subnetwork-controlled plants compared to\ncentralized schemes that aim to minimize the block error rate while reducing\nexecution complexity significantly.",
        "The increase in video streaming has presented a challenge of handling stream\nrequest effectively, especially over networks that are variable. This paper\ndescribes a new adaptive video streaming architecture capable of changing the\nvideo quality and buffer size depending on the data and latency of streamed\nvideo. For video streaming VLC media player was used where network performance\ndata were obtained through Python scripts with very accurate data rate and\nlatency measurement. The collected data is analyzed using Gemini AI, containing\ncharacteristics of the machine learning algorithm that recognizes the best\nresolution of videos and the buffer sizes. Through the features of real-time\nmonitoring and artificial intelligence decision making, the proposed framework\nimproves the user experience by reducing the occurrence of buffering events\nwhile at the same time increasing the video quality. Our findings therefore\nconfirm that the proposed solution based on artificial intelligence increases\nvideo quality and flexibility. This study advances knowledge of adaptive\nstreaming and offers an argument about how intelligent datadriven approaches\nand AI may be useful tools for enhancing the delivery of video in practical\nenvironments.",
        "The development of kinetic energy functional (KEF) is known as one of the\nmost difficult subjects in the electronic density functional theory (DFT). In\nparticular, the sound description of chemical bonds using a KEF is a matter of\ngreat significance in the field of theoretical physics and chemistry. It can be\nreadily confirmed that the famous Thomas-Fermi (TF) model or the TF model\ncorrected with a generalized gradient approximation (GGA) fails to realize the\nbound state of a covalent bond in general. In this work, a new kinetic energy\nfunctional is developed on the basis of the novel density functional theory (J.\nPhys. B: At. Mol. Opt. Phys. 51, 055102, 2018) that utilizes the electron\ndistribution on the energy coordinate as the fundamental variable. It is\ndemonstrated for an H$_2$ molecule that the bound state can be realized by the\nKEF by virtue of the property of the electron density on the energy coordinate.\nThe mechanism underlying the formation of the bound state is the same as that\nfor the realization of the static correlation in the exchange energy described\nwith the new DFT. We also developed a method termed potential gradient method\nto make a correction to the TF model instead of the GGA approach.",
        "3D Gaussian splatting-based talking head synthesis has recently gained\nattention for its ability to render high-fidelity images with real-time\ninference speed. However, since it is typically trained on only a short video\nthat lacks the diversity in facial emotions, the resultant talking heads\nstruggle to represent a wide range of emotions. To address this issue, we\npropose a lip-aligned emotional face generator and leverage it to train our\nEmoTalkingGaussian model. It is able to manipulate facial emotions conditioned\non continuous emotion values (i.e., valence and arousal); while retaining\nsynchronization of lip movements with input audio. Additionally, to achieve the\naccurate lip synchronization for in-the-wild audio, we introduce a\nself-supervised learning method that leverages a text-to-speech network and a\nvisual-audio synchronization network. We experiment our EmoTalkingGaussian on\npublicly available videos and have obtained better results than\nstate-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS),\nemotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy),\nand lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.",
        "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.",
        "We construct a real-valued solution to the eigenvalue problem\n$-\\text{div}(A\\nabla u)=\\lambda u$, $\\lambda>0,$ in the cylinder\n$\\mathbb{T}^2\\times \\mathbb{R}$ with a real, uniformly elliptic, and uniformly\n$C^1$ matrix $A$ such that $|u(x,y,t)|\\leq C e^{-c e^{c|t|}}$ for some $c,C>0$.\nWe also construct a complex-valued solution to the heat equation $u_t=\\Delta u\n+ B \\nabla u$ in a half-cylinder with continuous and uniformly bounded $B$,\nwhich also decays with double exponential speed. Related classical ideas, used\nin the construction of counterexamples to the unique continuation by Plis and\nMiller, are reviewed.",
        "We investigate the physical properties and detectability of warm-hot\nintergalactic medium (WHIM) gas with temperatures in the range $10^5<T<10^7$K\naround galaxy clusters using simulated galaxy clusters from The Three Hundred\nproject. In simulations with different input physics (GIZMO-SIMBA and\nGadget-X), we consistently find that the median gas temperature decreases to\nthe WHIM upper bound, $10^7$K, at $\\sim 2 \\times R_{200c}$, while the WHIM mass\nfraction increases with radius until $\\sim 3\\times R_{200c}$, where it plateaus\nat $\\sim 70$ per cent.By simulating X-ray emission from all gas components, we\nfind that the WHIM contribution to the soft X-ray band (0.2 - 2.3 keV)\nincreases with radius but eventually plateaus at larger distances. The\ndifferences between the two simulations become more pronounced at higher\nredshifts and larger radii. Finally, after accounting for observational\neffects, primarily by removing (sub)halos, we predict that the signal-to-noise\nratio of the X-ray signal obtained by stacking the eRASS1 galaxy cluster\ncatalogue will be $\\sim 7$ for GIZMO-SIMBA and $\\sim 21$ for Gadget-X.",
        "Treatment effects for assessing the efficacy of a novel therapy are typically\ndefined as measures comparing the marginal outcome distributions observed in\ntwo or more study arms. Although one can estimate such effects from the\nobserved outcome distributions obtained from proper randomisation, covariate\nadjustment is recommended to increase precision in randomised clinical trials.\nFor important treatment effects, such as odds or hazard ratios, conditioning on\ncovariates in binary logistic or proportional hazards models changes the\ninterpretation of the treatment effect under noncollapsibility and conditioning\non different sets of covariates renders the resulting effect estimates\nincomparable.\n  We propose a novel nonparanormal model formulation for adjusted marginal\ninference. This model for the joint distribution of outcome and covariates\ndirectly features a marginally defined treatment effect parameter, such as a\nmarginal odds or hazard ratio. Marginal distributions are modelled by\ntransformation models allowing broad applicability to diverse outcome types.\nJoint maximum likelihood estimation of all model parameters is performed. From\nthe parameters not only the marginal treatment effect of interest can be\nidentified but also an overall coefficient of determination and\ncovariate-specific measures of prognostic strength can be derived. A reference\nimplementation of this novel method is available in R add-on package tram.\n  For the special case of Cohen's standardised mean difference d, we\ntheoretically show that adjusting for an informative prognostic variable\nimproves the precision of this marginal, noncollapsible effect. Empirical\nresults confirm this not only for Cohen's d but also for log-odds ratios and\nlog-hazard ratios in simulations and three applications.",
        "We examine the origin of molecular gas heating in a sample of 42\ninfrared-luminous galaxies at $z<0.3$ by combining two sets of archival data.\nFirst, integrated CO line luminosities in the 1-0 and 5-4 through 13-12\ntransitions. Second, results from radiative transfer modelling that decompose\ntheir bolometric emission into starburst, AGN, and host galaxy components. We\nfind that the CO 1-0 and 5-4 through 9-8 lines primarily arise via radiative\nheating in the starburst and the host galaxy. In contrast, the CO 10-9 through\n13-12 lines may arise primarily in the starburst and AGN, with an increasing\ncontribution from mechanical heating and shocks. For the sample as a whole, we\nfind no evidence that AGN luminosity affects the heating of molecular gas by\nstar formation. However, for starbursts with low initial optical depths, a more\nluminous AGN may reduce the efficiency of starburst heating of the CO 5-4 and\nabove lines, consistent with negative AGN feedback.",
        "Quasiperiodic potentials without self-duality are always hard to derive the\nexact mobility edges (MEs). Here, we propose a new class of network models with\nexactly solvable MEs, characterized by quasiperiodic slowly varying potentials\nthat do not exhibit hidden self-duality. We present two methods to derive the\nMEs, the first involves integrating out the periodic sites to obtain an\neffective Hamiltonian with effective potential $g(E)V$ and effective\neigenenergy $f(E)$, which directly yields the MEs at $f(E) = \\pm(2t\\pm g(E)V)$,\nand the second is to connect the localized-delocalized transition points of the\nquasiperiodic slowly varying models and the real-complex transition points of\nthe eigenvalue equations. To illustrate this, we take quasiperiodic mosaic\nslowly varying models as examples, and we find that the MEs obtained from the\ntwo methods are the same. Furthermore, we generalize our methods to\nquasiperiodic network models and propose a possible experimental realization\nbased on optical waveguide systems, showing that the Anderson transition can be\nobserved even using small physical systems (with $L = 50 - 100$). Our results\nmay provide insight into understanding and realizing exact MEs in experiments.",
        "In automatic target recognition (ATR) systems, sensors may fail to capture\ndiscriminative, fine-grained detail features due to environmental conditions,\nnoise created by CMOS chips, occlusion, parallaxes, and sensor misalignment.\nTherefore, multi-sensor image fusion is an effective choice to overcome these\nconstraints. However, multi-modal image sensors are heterogeneous and have\ndomain and granularity gaps. In addition, the multi-sensor images can be\nmisaligned due to intricate background clutters, fluctuating illumination\nconditions, and uncontrolled sensor settings. In this paper, to overcome these\nissues, we decompose, align, and fuse multiple image sensor data for target\nclassification. We extract the domain-specific and domain-invariant features\nfrom each sensor data. We propose to develop a shared unified discrete token\n(UDT) space between sensors to reduce the domain and granularity gaps.\nAdditionally, we develop an alignment module to overcome the misalignment\nbetween multi-sensors and emphasize the discriminative representation of the\nUDT space. In the alignment module, we introduce sparsity constraints to\nprovide a better cross-modal representation of the UDT space and robustness\nagainst various sensor settings. We achieve superior classification performance\ncompared to single-modality classifiers and several state-of-the-art\nmulti-modal fusion algorithms on four multi-sensor ATR datasets.",
        "The ExoEcho project is designed to study the photodynamics of exoplanets by\nleveraging high-precision transit timing data from ground- and space-based\ntelescopes. Some exoplanets are experiencing orbital decay, and transit timing\nvariation (TTV) is a useful technique to study their orbital period variations.\nIn this study, we have obtained transit middle-time data from the Hubble Space\nTelescope (HST) observations for 37 short-period exoplanets, most of which are\nhot Jupiters. To search for potential long- and short-term orbital period\nvariations within the sample, we conduct TTV model fitting using both linear\nand quadratic ephemeris models. Our analysis identifies two hot Jupiters\nexperiencing strong periodic decays. Given the old age of the host stars of the\nhot Jupiter population, our findings call for a scenario where HJs are\ncontinuously being destructed and created. Our study demonstrates the\nimportance of incorporating high-precision transit timing data to TTV study in\nthe future.",
        "The streaming instability is a promising mechanism for planetesimal\nformation. The instability can rapidly form dense clumps that collapse\nself-gravitationally, which is efficient for large dust grains with the Stokes\nnumber on the order of 0.1. However, dust growth models predict that\ncollisional fragmentation prevents dust grains from growing to such sizes. We\nperform local simulations of the streaming instability and measure\ncharacteristic collision velocities and collision rates of dust grains based on\ntheir trajectories in moderate clumping. The collision velocities are on the\norder of 0.1 percent of the sound speed or lower, implying that dust grains can\novercome the fragmentation barrier via the clumping. We also find that the\ncollision rates are appreciably high regardless of the low collision\nvelocities. Corresponding timescales are on the order of ten Keplerian periods\nor shorter, suggesting that dust grains can overcome the drift barrier as well.\nThis streaming-instability-assisted (SI-assisted) coagulation greatly relaxes\nthe conditions for planetesimal formation as recently implied.",
        "Simpson's Paradox is a well-known phenomenon in statistical science, where\nthe relationship between the response variable $X$ and a certain explanatory\nfactor of interest $A$ reverses when an additional factor $B_1$ is considered.\nThis paper explores the extension of Simpson's Paradox to any given number $n$\nof factors, referred to as the $n$-factor Simpson's Paradox. We first provide a\nrigorous definition of the $n$-factor Simpson's Paradox, then demonstrate the\nexistence of a probability distribution through a geometric construction.\nSpecifically, we show that for any positive integer $n$, it is possible to\nconstruct a probability distribution in which the conclusion about the effect\nof $A$ on $X$ reverses each time an additional factor $B_i$ is introduced for\n$i=1,...,n$. A detailed example for $n = 3$ illustrates the construction. Our\nresults highlight that, contrary to the intuition that more data leads to more\naccurate inferences, the inclusion of additional factors can repeatedly reverse\nconclusions, emphasizing the complexity of statistical inference in the\npresence of multiple confounding variables.",
        "The growing interest in employing large language models (LLMs) for\ndecision-making in social and economic contexts has raised questions about\ntheir potential to function as agents in these domains. A significant number of\nsocietal problems involve the distribution of resources, where fairness, along\nwith economic efficiency, play a critical role in the desirability of outcomes.\nIn this paper, we examine whether LLM responses adhere to fundamental fairness\nconcepts such as equitability, envy-freeness, and Rawlsian maximin, and\ninvestigate their alignment with human preferences. We evaluate the performance\nof several LLMs, providing a comparative benchmark of their ability to reflect\nthese measures. Our results demonstrate a lack of alignment between current LLM\nresponses and human distributional preferences. Moreover, LLMs are unable to\nutilize money as a transferable resource to mitigate inequality. Nonetheless,\nwe demonstrate a stark contrast when (some) LLMs are tasked with selecting from\na predefined menu of options rather than generating one. In addition, we\nanalyze the robustness of LLM responses to variations in semantic factors (e.g.\nintentions or personas) or non-semantic prompting changes (e.g. templates or\norderings). Finally, we highlight potential strategies aimed at enhancing the\nalignment of LLM behavior with well-established fairness concepts.",
        "Accelerated MRI involves collecting partial k-space measurements to reduce\nacquisition time, patient discomfort, and motion artifacts, and typically uses\nregular undersampling patterns or hand-designed schemes. Recent works have\nstudied population-adaptive sampling patterns that are learned from a group of\npatients (or scans) based on population-specific metrics. However, such a\ngeneral sampling pattern can be sub-optimal for any specific scan since it may\nlack scan or slice adaptive details. To overcome this issue, we propose a\nframework for jointly learning scan-adaptive Cartesian undersampling patterns\nand a corresponding reconstruction model from a training set. We use an\nalternating algorithm for learning the sampling patterns and reconstruction\nmodel where we use an iterative coordinate descent (ICD) based offline\noptimization of scan-adaptive k-space sampling patterns for each example in the\ntraining set. A nearest neighbor search is then used to select the\nscan-adaptive sampling pattern at test time from initially acquired\nlow-frequency k-space information. We applied the proposed framework (dubbed\nSUNO) to the fastMRI multi-coil knee and brain datasets, demonstrating improved\nperformance over currently used undersampling patterns at both 4x and 8x\nacceleration factors in terms of both visual quality and quantitative metrics.\nThe code for the proposed framework is available at\nhttps:\/\/github.com\/sidgautam95\/adaptive-sampling-mri-suno.",
        "We study the mechanics of temperature-driven reconstructive martensitic\ntransformations in crystalline materials, within the framework of nonlinear\nelasticity theory. We focus on the prototypical case of the square-hexagonal\ntransition in 2D crystals, using a modular Ericksen-Landau-type strain energy\nwhose infinite and discrete invariance group originates from the full symmetry\nof the underlying lattice. In the simulation of quasi-static thermally-driven\ntransitions we confirm the role of the valley-floor network in establishing the\nstrain-field transition-pathways on the symmetry-moulded strain energy\nlandscape of the crystal. We also observe the phase change to progress through\nabrupt microstructure reorganization via strain avalanching under the slow\ndriving. We reveal at the same time the presence of assisting\nanti-transformation activity, which locally goes against the overall transition\ncourse. Both transformation and anti-transformation avalanches exhibit\nGutenberg-Richter like heavy-tailed size statistics. A parallel analysis shows\nagreement of these numerical results with their counterparts in empirical\nobservations on temperature-induced martensitic transformations. The simulation\nfurthermore shows that, in the present case of a reconstructive transformation,\nstrain avalanching mostly involves lattice-invariant shears (LIS). As a\nconsequence, microstructure evolution is accompanied by slip-induced defect\nnucleation and movement in the lattice. LIS activity also leads to the\ndevelopment of polycrystal grain-like lattice-homogeneity domains exhibiting\nhigh boundary segmentation in the body. All these effects ultimately lead to\ntransformation irreversibility."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Deep Learning for Automatic Gross Tumor Volumes Contouring in Esophageal Cancer Based on Contrast-Enhanced Computed Tomography Images: A Multi-Institutional Study",
    "start_abstract":"Purpose To develop and externally validate an automatic artificial intelligence (AI) tool for delineating gross tumor volume (GTV) in patients with esophageal squamous cell carcinoma (ESCC), which can assist in neo-adjuvant or radical radiation therapy treatment planning. Methods and Materials In this multi-institutional study, contrast-enhanced CT images from 580 eligible ESCC patients were retrospectively collected. The GTV contours delineated by 2 experts via consensus were used as ground truth. A 3-dimensional deep learning model was developed for GTV contouring in the training cohort and internally and externally validated in 3 validation cohorts. The AI tool was compared against 12 board-certified experts in 25 patients randomly selected from the external validation cohort to evaluate its assistance in improving contouring performance and reducing variation. Contouring performance was measured using dice similarity coefficient (DSC) and average surface distance. Additionally, our previously established radiomics model for predicting pathologic complete response was used to compare AI-generated and ground truth contours, to assess the potential of the AI contouring tool in radiomics analysis. Results The AI tool demonstrated good GTV contouring performance in multicenter validation cohorts, with median DSC values of 0.865, 0.876, and 0.866 and median average surface distance values of 0.939, 0.789, and 0.875 mm, respectively. Furthermore, the AI tool significantly improved contouring performance for half of 12 board-certified experts (DSC values, 0.794-0.835 vs 0.856-0.881, P = .003-0.048), reduced the intra- and interobserver variations by 37.4% and 55.2%, respectively, and saved contouring time by 77.6%. In the radiomics analysis, 88.7% of radiomic features from ground truth and AI-generated contours demonstrated stable reproducibility, and similar pathologic complete response prediction performance for these contours (P = .430) was observed. Conclusions Our AI contouring tool can improve GTV contouring performance and facilitate radiomics analysis in ESCC patients, which indicates its potential for GTV contouring during radiation therapy treatment planning and radiomics studies.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "MRI-Guided Adaptive Radiation Therapy"
      ],
      "abstract":[
        "Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Magnetically arrested disk flux eruption events to describe SgrA* flares",
        "Bayesian optimization to infer parameters in viscoelasticity",
        "STARS-Enabled Full-Duplex Two-Way mMIMO System Under\n  Spatially-Correlated Channels",
        "FOGGIE X: Characterizing the Small-Scale Structure of the CGM and its\n  Imprint on Observables",
        "Cavity engineering of solid-state materials without external driving",
        "Forecasting Extreme Temperatures in Siberia Using Supervised Learning\n  and Conformal Prediction Regions",
        "Nonlinear Domain Engineering for Quantum Technologies",
        "Towards reconstruction of Pulsed-wave Doppler signals from Non-invasive\n  fetal ECG",
        "Anderson Accelerated Operator Splitting Methods for Convex-nonconvex\n  Regularized Problems",
        "Quasi-likelihood ratio test for jump-diffusion processes based on\n  adaptive maximum likelihood inference",
        "Unveiling Wireless Users' Locations via Modulation Classification-based\n  Passive Attack",
        "Synthetic Porous Microstructures: Automatic Design, Simulation, and\n  Permeability Analysis",
        "Phenomenological Scaling Crossovers in Non-Equilibrium Critical Dynamics",
        "Investigating the broadening phenomenon in two-particle correlations\n  induced by gluon saturation",
        "Optimal COVID-19 vaccine prioritization by age depends critically on\n  inter-group contacts and vaccination rates",
        "Topological pumping of multi-frequency solitons",
        "Quantum Measurement for Quantum Chemistry on a Quantum Computer",
        "Are dark matter and dark energy omnipresent?",
        "Biglobal resolvent analysis of separated flow over a NACA0012 airfoil",
        "Tracking UWB Devices Through Radio Frequency Fingerprinting Is Possible",
        "A New Type of MPd5 Kagome Superconductors",
        "Extended weak order for the affine symmetric group",
        "A Generalized Benford Framework for Threat Identification in\n  Counter-Intelligence",
        "On Congruence Theorem for valued division algebras",
        "Phases of String Stars in the Presence of a Spatial Circle",
        "High-resolution deuterium metabolic imaging of the human brain at 9.4 T\n  using bSSFP spectral-spatial acquisitions",
        "Temporal Point Process Modeling of Aggressive Behavior Onset in\n  Psychiatric Inpatient Youths with Autism",
        "Strong law of large numbers for a branching random walk among Bernoulli\n  traps",
        "Tracking X-ray Variability in Next Generation EHT LLAGN Targets"
      ],
      "abstract":[
        "Context. Magnetically arrested disks are among the most suitable candidates\nfor describing the gas accretion and observed emission in the vicinity of\nsupermassive black holes. Aims. This work aims to provide a direct correlation\nbetween the quasi-periodic flux eruption events, characteristic of MAD\naccretion disk simulations, and the observed flaring behavior in the Galactic\ncenter. Methods. We employ a MAD accretion disk with a distinct\ncounter-clockwise rotation and investigate the evolution of magnetized flux\ntubes generated during a prominent flux eruption event. Although these flux\ntubes demonstrate a clockwise pattern, they experience significant dragging by\nthe accretion disk's rotation. This study models the motion of hot spots,\nformed on the disk's equatorial plane due to magnetic reconnection, as they\ntravel along the magnetized flux tubes at a fraction of the speed of light.\nResults. Hot spots with a relativistic ejection velocity are able to balance\nout the counter-clockwise dragging of the flux tube's foot-point on the disk\nand demonstrate a clockwise motion in the sky, that is in good agreement with\nthe NIR flares in the Galactic center. In addition, our flare models favor\nface-on inclinations in the range $[0^\\circ, 34^\\circ]$ and $[163^\\circ,\n180^\\circ]$ for SgrA*. Conclusions. The flux eruption events that arise\nnaturally in the MAD accretion state provide a promising framework for\nreproducing the observed flaring behavior in the vicinity of SgrA*.",
        "Inferring viscoelasticity parameters is a key challenge that often leads to\nnon-unique solutions when fitting rheological data. In this context, we propose\na machine learning approach that utilizes Bayesian optimization for parameter\ninference during curve-fitting processes. To fit a viscoelastic model to\nrheological data, the Bayesian optimization maps the parameter values to a\ngiven error function. It then exploits the mapped space to identify parameter\ncombinations that minimize the error. We compare the Bayesian optimization\nresults to traditional fitting routines and demonstrate that our approach finds\nthe fitting parameters in a less or similar number of iterations. Furthermore,\nit also creates a \"white-box\" and supervised framework for parameter estimation\nin linear viscoelasticity modeling.",
        "\\underline{S}imultaneous \\underline{t}ransmitting \\underline{a}nd\n\\underline{r}eflecting \\underline{s}urface (STARS)-assisted systems have\nemerged to fill this gap by providing $ 360^{\\circ}$ wireless coverage. In\nparallel,\n  full-duplex (FD) communication offers a higher achievable rate through\nefficient spectrum utilization compared to the half-duplex (HD) counterpart.\nMoreover, two-way\/bi-directional communications in an FD system can further\nenhance the system's spectral efficiency. Hence, in this paper, we propose a\nSTARS-enabled massive MIMO deployment in an FD two-way communication network\nfor highly efficient spectrum utilization, while covering the dead zones around\nthe STARS. This model enables simultaneous information exchange between\nmultiple nodes, while \\emph{potentially} doubling the spectral efficiency (SE).\nBy invoking the use-and-then-forget (UaTF) combining scheme, we derive a\nclosed-form expression for an achievable SE at each user of the system\nconsidering both uplink and downlink communications based on statistical\nchannel state information (CSI), while also accounting for imperfect CSI and\ncorrelated fading conditions. Moreover, we formulate an optimization problem to\nobtain an optimal passive beamforming matrix design at the STARS that maximizes\nthe sum achievable SE. The considered problem is non-convex and we propose a\nprovably-convergent low-complexity algorithm, termed as \\underline{pro}jected\n\\underline{gr}adient \\underline{a}scent \\underline{m}ethod (ProGrAM), to obtain\na stationary solution. Extensive numerical results are provided to establish\nthe performance superiority of the FD STARS-enabled system over the HD\nSTARS-enabled and FD conventional RIS (cRIS)-enabled counterparts, and also to\nshow the effect of different parameters of interest on the system performance.",
        "One of the main unknowns in galaxy evolution is how gas flows into and out of\ngalaxies in the circumgalactic medium (CGM). Studies observing the CGM in\nabsorption using multiple or extended background objects suggest a high degree\nof variation on relatively small ($\\lesssim 1$ kpc) spatial scales. Similarly,\nhigh-resolution simulations generally exhibit small-scale substructure in the\ngas around galaxies. We examine the small-scale structure of the $z = 1$ CGM\nusing simulations from the FOGGIE (Figuring Out Gas & Galaxies in Enzo)\nproject. We select gaseous substructures (\"clumps\") by their local overdensity\nand investigate their physical properties, including temperature, metallicity,\nand kinematics with respect to the galaxy and the nearby surroundings. FOGGIE\nresolves clumps down to sphericalized radii $R \\sim 0.25$ kpc at $z = 1$. The\ndistribution of clumps peaks at $\\sim 10^5$ $\\rm M_{\\odot}$ and $10^{4}$ K,\nconsistent with relatively condensed, cool gas with a slight preference for\ninflow-like velocities. Many clumps show internal temperature and density\nvariations, and thus internally varying ionization levels for key diagnostic\nions such as HI, MgII, and OVI. The average metallicity in clumps is about a\nfactor 1.5--2$\\times$ lower in metallicity than nearby gas, suggesting that the\nmetals are not well-mixed between structured and diffuse CGM, which may have\nimplications for observational metallicity estimations of dense CGM clouds. We\nestimate the survivability of CGM clumps and find that structures larger than\n0.5 kpc are generally long-lived. Finally, we qualitatively compare the\nsimulated cloud properties to Milky Way high-velocity clouds.",
        "Confining electromagnetic fields inside an optical cavity can enhance the\nlight-matter coupling between quantum materials embedded inside the cavity and\nthe confined photon fields. When the interaction between the matter and the\nphoton fields is strong enough, even the quantum vacuum field fluctuations of\nthe photons confined in the cavity can alter the properties of the\ncavity-embedded solid-state materials at equilibrium and room temperature. This\napproach to engineering materials with light avoids fundamental issues of\nlaser-induced transient matter states. To clearly differentiate this field from\nphenomena in driven systems, we call this emerging field cavity materials\nengineering. In this review, we first present theoretical frameworks,\nespecially, ab initio methods, for describing light-matter interactions in\nsolid-state materials embedded inside a realistic optical cavity. Next, we\noverview a few experimental breakthroughs in this domain, detailing how the\nground state properties of materials can be altered within such confined\nphotonic environments. Moreover, we discuss state-of-the-art theoretical\nproposals for tailoring material properties within cavities. Finally, we\noutline the key challenges and promising avenues for future research in this\nexciting field.",
        "In this paper, we step back from a variety of competing heat wave definitions\nand forecast directly unusually high temperatures. Our testbed is the Russian\nFar East in the summers of 2022 and 2023. Remotely sensed data from NASA's Aqua\nspacecraft are organized into a within-subject design that can reduce nuisance\nvariation in forecasted temperatures. Spatial grid cells are the study units.\nEach is exposed to precursors of a faux heat wave in 2022 and to precursors of\na reported heat wave in 2023. The precursors are used to forecast temperatures\ntwo weeks in the future for each of 31 consecutive days. Algorithmic fitting\nprocedures produce forecasts with promise and relatively small conformal\nprediction regions having a coverage probability of at least .75. Spatial and\ntemporal dependence are manageable. At worst, there is weak dependence such\nthat conformal prediction inference is only asymptotically valid.",
        "The continuously growing effort towards developing real-world quantum\ntechnological applications has come to demand an increasing amount of\nflexibility from its respective platforms. This review presents a highly\nadaptable engineering technique for photonic quantum technologies based on the\nartificial structuring of the material nonlinearity. This technique, while, in\na simple form, already featured across the full breadth of photonic quantum\ntechnologies, has undergone significant development over the last decade, now\nfeaturing advanced, aperiodic designs. This review gives an introduction to the\nthree-wave-mixing processes lying at the core of this approach, and\nillustrates, on basis of the underlying quantum-mechanical description, how\nthey can artificially be manipulated to engineer the corresponding photon\ncharacteristics. It then describes how this technique can be employed to\nrealize a number of very different objectives which are expected to find\napplication across the full range of photonic quantum technologies, and\npresents a summary of the research done towards these ends to date.",
        "Fetal cardiac health monitoring with invasive methods have a limited\nviability because they can only be utilized during labor and are uncomfortable.\nOn the other hand non-invasive fECG are adulterated with maternal ECG, and\nhence resulting in poor analysis. In contrast, Pulsed-wave Doppler (PwD)\nechocardiography generates high-quality signals representing fetal blood volume\ninflow-outflow. It also follows non-invasive signal acquisition. The only\ndrawback is that it requires highly expensive setup. To address this aspect, we\nput forward a challenging research question - can we reconstruct PwD signals\nusing non-invasive fetal ECG?",
        "Convex-nonconvex (CNC) regularization is a novel paradigm that employs a\nnonconvex penalty function while maintaining the convexity of the entire\nobjective function. It has been successfully applied to problems in signal\nprocessing, statistics, and machine learning. Despite its wide application, the\ncomputation of CNC regularized problems remains challenging and\nunder-investigated. To fill the gap, we study several operator splitting\nmethods and their Anderson accelerated counterparts for solving least squares\nproblems with CNC regularization. We establish the global convergence of the\nproposed algorithm to an optimal point and demonstrate its practical speed-ups\nin various applications.",
        "In this paper, we consider parameter estimation and quasi-likelihood ratio\ntests for multidimensional jump-diffusion processes defined by stochastic\ndifferential equations. In general, simultaneous estimation faces challenges\nsuch as an increase of computational time for optimization and instability of\nestimation accuracy as the dimensionality of parameters grows. To address these\nissues, we propose an adaptive quasi-log likelihood function based on the joint\nquasi-log likelihood function introduced by Shimizu and Yoshida (2003, 2006)\nand Ogihara and Yoshida (2011). We then show that the resulting adaptive\nestimators possess consistency and asymptotic normality. Furthermore, we extend\nthe joint quasi-log likelihood function proposed by Shimizu and Yoshida (2003,\n2006) and Ogihara and Yoshida (2011) and construct a test statistic using the\nproposed adaptive estimators. We prove that the proposed test statistic\nconverges in distribution to a $\\chi^2$-distribution under the null hypothesis\nand that the associated test is consistent. Finally, we conduct numerical\nsimulations using a specific jump-diffusion process model to examine the\nasymptotic behavior of the proposed adaptive estimators and test statistics.",
        "The broadcast nature of the wireless medium and openness of wireless\nstandards, e.g., 3GPP releases 16-20, invite adversaries to launch various\nactive and passive attacks on cellular and other wireless networks. This work\nidentifies one such loose end of wireless standards and presents a novel\npassive attack method enabling an eavesdropper (Eve) to localize a line of\nsight wireless user (Bob) who is communicating with a base station or WiFi\naccess point (Alice). The proposed attack involves two phases. In the first\nphase, Eve performs modulation classification by intercepting the downlink\nchannel between Alice and Bob. This enables Eve to utilize the publicly\navailable modulation and coding scheme (MCS) tables to do pesudo-ranging, i.e.,\nthe Eve determines the ring within which Bob is located, which drastically\nreduces the search space. In the second phase, Eve sniffs the uplink channel,\nand employs multiple strategies to further refine Bob's location within the\nring. Towards the end, we present our thoughts on how this attack can be\nextended to non-line-of-sight scenarios, and how this attack could act as a\nscaffolding to construct a malicious digital twin map.",
        "This study introduces an open-source computational framework for the\ngeneration and permeability evaluation of synthetic porous media. The proposed\nmethodology integrates crystallographic and meshing tools to construct\ncontrolled microstructures with tunable porosity, facilitating seamless\ntransitions from geometric modelling to computational domains for numerical\nsimulations. The generated structures are analysed through fluid-structure\ninteraction (FSI) simulations, leveraging the Entropically Damped Artificial\nCompressibility (EDAC) formulation in conjunction with the\nDiscretization-Corrected Particle Strength Exchange (DC-PSE) method.\n  A novel approach for the numerical estimation of the macroscopic permeability\ntensor is presented, employing a stochastic upscaling technique inspired by the\nvolume averaging method. To validate the framework, we investigate the\ntransition between creeping and non-Darcy flow regimes through parametric\npermeability studies, utilizing a one-dimensional approach for practical\nbenchmarking.\n  The results establish a foundation for experimental validation and provide\ninsights into the customised design of porous structures for engineering and\nbiomedical applications, offering a versatile tool for research in fluid\ntransport and porous media mechanics.",
        "Dynamical universality plays a fundamental role in understanding the scaling\nproperties of critical dynamics, including absorbing phase transitions and\nphysical aging. Although individual universality classes have been extensively\nstudied, the theoretical framework for scaling crossovers between distinct\ndynamical regimes remains underdeveloped. In this work, we propose a\nphenomenological approach to describe dynamic scaling crossovers in\nreaction-diffusion systems, utilizing a Bernoulli differential equation with\ntime-dependent reaction coefficients. Under a new scaling hypothesis for\nspatial fluctuations, we analytically derive five universal power-law crossover\nfunctions. These solutions accurately describe experimental observations of\nabsorbing phase transitions in turbulent liquid crystals and reaction-diffusion\ncrossovers in exciton-exciton recombination, revealing possible mechanisms\nassociated with many-body correlations and two-component dynamics. These\nfindings suggest the existence of universal scaling laws governing the\ntransition between different dynamical states, motivating further studies using\nfield-theoretic approaches and renormalization group analyses.",
        "It has been found that the gluon density inside the proton grows rapidly at\nsmall momentum fractions. Quantum Chromodynamics (QCD) predicts that this\ngrowth can be regulated by nonlinear effects, ultimately leading to gluon\nsaturation. Within the color glass condensate framework, nonlinear QCD effects\nare predicted to suppress and broaden back-to-back angular correlations in\ncollisions involving heavy nuclei. While suppression has been observed in\nvarious experiments in $d\/p$$+$A collisions compared to $p$$+$$p$ collisions,\nthe predicted broadening remains unobserved. This study investigates the\ncontributions of intrinsic transverse momentum ($k_T$), which is associated\nwith saturation physics, as well as parton showers and transverse motion from\nfragmentation ($p_T^{\\mathrm{frag}}$), which are not saturation dependent, to\nthe width of the correlation function. Our findings show that the\nnon-saturation dependent effects, especially the initial-state parton shower\nand $p_T^{\\mathrm{frag}}$, which occur independently of the collision system,\nsmear the back-to-back correlation more than gluon saturation does, making the\nbroadening phenomenon difficult to observe.",
        "The limited availability of COVID-19 vaccines has prompted extensive research\non optimal vaccination strategies. Previous studies have considered various\nnon-pharmaceutical interventions, vaccine efficacy, and distribution\nstrategies. In this work, we address the combined effects of inter-group\ncontacts and vaccination rates under contact reduction, analyzing the Spanish\npopulation's demographic and age group contact patterns and incorporating\nreinfection dynamics. We conduct an exhaustive analysis, evaluating 362,880\npermutations of 9 age groups across 6 vaccination rates and two distinct,\nempirically quantified scenarios for social contacts. Our results show that at\nintermediate-to-high vaccination rates with unrestricted social contacts,\noptimal age-based vaccination strategies only slightly deviate from\nolder-to-younger prioritization, yielding marginal reductions in deaths and\ninfections. However, when significant reductions in social contacts are\nenforced -similar to the lockdowns in 2020-, there are substantial\nimprovements, particularly at moderate vaccination rates. These restrictions\nlead to a transition where infection propagation is halted, a scenario that\nbecame achievable during the pandemic with the observed vaccination rates. Our\nfindings emphasize the importance of combining appropriate social contact\nreductions with vaccination to optimize age-based vaccination strategies,\nunderscoring the complex, nonlinear dynamics involved in pandemic dynamics and\nthe necessity for tailored, context-specific interventions.",
        "We report on the topological pumping of quadratic optical solitons, observed\nthrough their quantized transport in a dynamic optical potential. A distinctive\nfeature of this system is that the two fields with different frequencies, which\ntogether form the quadratic soliton, evolve in separate yet topologically\nequivalent dynamic optical potentials. Pumping in this system exhibits several\nnotable differences from pumping in cubic media. While Chern indices\ncharacterizing quantized transport for uncoupled fundamental and second\nharmonic waves are nonzero, small-amplitude solitons with narrow spectra do not\nmove, thus revealing a non-topological phase. As the nonlinearity increases,\nthe system undergoes a sharp transition, depending on the velocity of one of\nthe sublattices forming dynamical potential, into the phase where the quantized\ntransport of quadratic solitons governed by nonzero Chern numbers is observed.\nThe power level at which this transition occurs increases with increase of\npumping velocity, and the transition is observed even in the regime when the\nadiabatic approximation no longer applies. Unlike in cubic media, in a\nquadratic medium neither breakup of topological pumping nor fractional pumping\nat high power levels are observed.",
        "Quantum chemistry is among the most promising applications of quantum\ncomputing, offering the potential to solve complex electronic structure\nproblems more efficiently than classical approaches. A critical component of\nany quantum algorithm is the measurement step, where the desired properties are\nextracted from a quantum computer. This review focuses on recent advancements\nin quantum measurement techniques tailored for quantum chemistry, particularly\nwithin the second quantized framework suitable for current and near-term\nquantum hardware.\n  We provide a comprehensive overview of measurement strategies developed\nprimarily for the Variational Quantum Eigensolver (VQE) and its derivatives.\nThese strategies address the inherent challenges posed by complexity of the\nelectronic Hamiltonian operator. Additionally, we examine methods for\nestimating excited states and one- and two-electron properties, extending the\napplicability of quantum algorithms to broader chemical phenomena.\n  Key aspects of the review include approaches for constructing measurement\noperators with reduced classical preprocessing and quantum implementation\ncosts, techniques to minimize the number of measurements required for a given\naccuracy, and error mitigation strategies that leverage symmetries and other\nproperties of the measurement operators. Furthermore, we explore measurement\nschemes rooted in Quantum Phase Estimation (QPE), which are expected to become\nviable with the advent of fault-tolerant quantum computing.\n  This review emphasizes foundational concepts and methodologies rather than\nnumerical benchmarks, serving as a resource for researchers aiming to enhance\nthe efficiency and accuracy of quantum measurements in quantum chemistry.",
        "A set of temporal singularities (transients) in the mass-energy density and\npressure, bearing a specific mathematical structure which represents a new\nsolution to the continuity equation (\\ie~conservation of mass-energy) and\nsatisfying the strong energy condition, is proposed to account for the\nexpansion history of a homogeneous Universe, and the formation and binding of\nlarge scale structures as a continuum approximation of their cumulative\neffects. These singularities are unobservable because they occur rarely in time\nand are unresolvably fast, and that could be the reason why dark matter and\ndark energy have not been found. Implication on inflationary cosmology is\ndiscussed. The origin of these temporal singularities is unknown, safe to say\nthat the same is true of the moment of the Big Bang itself. This work\ncomplements a recent paper, where a topological defect in the form of a\nspatial, spherical shell of density singularity giving rise to a 1\/r attractive\nforce (to test particles of positive mass) but zero integrated mass over a\nlarge volume of space, was proposed to solve the dark matter problem in bound\nstructures but not cosmic expansion. The idea also involved a negative density,\nwhich is not present in the current model.",
        "The effects of Reynolds number across $Re=1000$, $2500$, $5000$, and $10000$\non separated flow over a two-dimensional NACA0012 airfoil at an angle of attack\nof $\\alpha=14^\\circ$ are investigated through the biglobal resolvent analysis.\nWe identify modal structures and energy amplifications over a range of\nfrequency, spanwise wavenumber, and discount parameter, providing insights\nacross various timescales. Using temporal discounting, we find that the shear\nlayer dynamics dominates over short time horizons, while the wake dynamics\nbecomes the primary amplification mechanism over long time horizons. Spanwise\neffects also appear over long time horizon, sustained by low frequencies. At a\nfixed timescale, we investigate the influence of Reynolds number on response\nand forcing mode structures, as well as the energy gain over different\nfrequencies. Across all Reynolds numbers, the response modes shift from\nwake-dominated structures at low frequencies to shear layer-dominated\nstructures at higher frequencies. The frequency at which the dominant mechanism\nchanges is independent of the Reynolds number. The response mode structures\nshow similarities across different Reynolds numbers, with local streamwise\nwavelengths only depending on frequency. Comparisons at a different angle of\nattack ($\\alpha=9^\\circ$) show that the transition from wake to shear layer\ndynamics with increasing frequency only occurs if the unsteady flow is\nthree-dimensional. We also study the dominant frequencies associated with wake\nand shear layer dynamics across the angles of attack and Reynolds numbers, and\npresent the characteristic scaling for each mechanism.",
        "Ultra-wideband (UWB) is a state-of-the-art technology designed for\napplications requiring centimeter-level localization. Its widespread adoption\nby smartphone manufacturer naturally raises security and privacy concerns.\nSuccessfully implementing Radio Frequency Fingerprinting (RFF) to UWB could\nenable physical layer security, but might also allow undesired tracking of the\ndevices. The scope of this paper is to explore the feasibility of applying RFF\nto UWB and investigates how well this technique generalizes across different\nenvironments. We collected a realistic dataset using off-the-shelf UWB devices\nwith controlled variation in device positioning. Moreover, we developed an\nimproved deep learning pipeline to extract the hardware signature from the\nsignal data. In stable conditions, the extracted RFF achieves over 99%\naccuracy. While the accuracy decreases in more changing environments, we still\nobtain up to 76% accuracy in untrained locations.",
        "Kagome materials, which are composed of hexagons tiled with a shared\ntriangle, have inspired enormous interest due to their unique structures and\nrich physical properties, but exploring superconducting material systems with\nnew kagome structures is still an important research direction. Here, we\npredict a new type of kagome superconductors, MPd5 (M is a group IIA metal\nelement), and identify that they exhibit coexistence of superconductivity and\nnon-trivial topological properties. We uncover their phonon-mediated\nsuperconductivity by the density functional theory for superconductors (SCDFT),\npredicting the superconducting transition temperatures (Tc) of 2.64, 2.03, and\n1.50 K for CaPd5, SrPd5, and BaPd5, respectively, which can be effectively\ntuned by external pressure and electron doping. The present results also\ndemonstrate that MPd5 have various topological properties such as, CaPd5 shows\ntopological non-trivial intersection near the Fermi level (EF). Our results\nindicate that the MPd5 materials can be an emerging material platform with rich\nexotic physics in their kagome structures, and render themselves excellent\ncandidates for superconducting and advanced functional materials that could be\nutilized in topological quantum computing and information technology.",
        "The extended weak order on a Coxeter group $W$ is the poset of biclosed sets\nin its root system. In (Barkley-Speyer 2024), it was shown that when\n$W=\\widetilde{S}_n$ is the affine symmetric group, then the extended weak order\nis a quotient of the lattice $L_n$ of translation-invariant total orderings of\nthe integers. In this article, we give a combinatorial introduction to $L_n$\nand the extended weak order on $\\widetilde{S}_n$. We show that $L_n$ is an\nalgebraic completely semidistributive lattice. We describe its canonical join\nrepresentations using a cyclic version of Reading's non-crossing arc diagrams.\nWe also show analogous statements for the lattice of all total orders of the\nintegers, which is the extended weak order on the symmetric group $S_\\infty$. A\nkey property of both of these lattices is that they are profinite; we also\nprove that a profinite lattice is join semidistributive if and only if its\ncompact elements have canonical join representations. We conjecture that the\nextended weak order of any Coxeter group is a profinite semidistributive\nlattice.",
        "In this paper, we develop a framework of 'Benford models' for\ncounter-intelligence investigations which analyze frequency data of a suspect's\nvisits to physical locations, online websites, and communication channels. We\naccomplish this by establishing the Benford measure for continuous & bounded\ndomains, generalizing the accumulated percentage differences between sites in\nthe frequency data with the log-determinant of 'Benford Matrices,' employing an\nestimator to determine a 'Benford Test Statistic,' and identifying maximal\nvalues of that test statistic across all permutations of included sites in our\ndata. This framework is intended to complement outlier analysis models by\nfinding where hidden Benford patterns 'break' in frequency data and telling\ninvestigators which sites they should investigate.",
        "Let $K$ be a field with Henselian valuation, and $D$ be a tame $F$-central\ndivision algebra. Let $\\mathrm{TK}_1(D)$ denote the torsion group of the\nWhitehead group ${\\rm K}_1(D)=D^*\/D'$. Let $G$ be the subgroup of $D^*$ such\nthat $\\mathrm{TK}_1(D) = G\/D'$. In this note, we prove that either $(1+M_D)\n\\cap G \\subseteq D'$ or ${\\rm char}(\\overline{F})=p>0$ and $((1+M_D) \\cap\nG)D'\/D'$ is a $p$-group. We also provide some examples to calculate the latter\ngroup.",
        "In string theory, black holes are expected to transition into string stars as\ntheir Hawking temperature approaches the Hagedorn temperature. We study string\nstars and their phase transitions in the Euclidean spacetime\n$\\mathbb{R}^d\\times\\mathbb{S}_\\tau^1\\times\\mathbb{S}_z^1$. Using the\nHorowitz-Polchinski (HP) effective field theory, we discover novel solutions\nfor $d=2$. The uniform string star exhibits a scaling symmetry that results in\nthe absence of a critical point for its transition into the non-uniform\nsolution. For $d=4$, we show that quartic corrections to the effective action\nresolve the mass degeneracy of uniform string stars. At $d=5$, we find that as\nnon-uniformity increases, the quartic terms become significant (while\nhigher-order terms remain negligible) and reverse the direction of temperature\nvariation, leading to a swallowtail-type phase diagram in the canonical\nensemble. Extending the quartic-corrected EFT to $d=6$, we find that string\nstars with small non-uniformity dominate the microcanonical ensemble but not\nthe canonical ensemble, similar to the $d=5$ case. However, in the\nmicrocanonical ensemble, the uniform string star is anomalously (un)stable when\nthe spatial circle is larger (smaller) than the critical size.",
        "We demonstrated the feasibility of using bSSFP acquisitions for off-resonance\ninsensitive high-resolution [6,6'-2H2]-glucose deuterium metabolic imaging\n(DMI) studies in the healthy human brain at 9.4T. Balanced SSFP acquisitions\nhave potential to improve the sensitivity of DMI despite the SNR loss of\nphase-cycling and other human scanner constraints.We investigated two variants\nof bSSFP acquisitions, namely uniform-weighted multi echo and\nacquisition-weighted CSI to improve the SNR of deuterium metabolic imaging\n(DMI) in the brain with oral labelled-glucose intake. Phase-cycling was\nintroduced to make bSSFP acquisitions less sensitive to B0 inhomogeneity. Two\nSNR optimal methods for obtaining metabolite amplitudes from the phase-cycled\ndata were proposed. The SNR performance of the two bSSFP variants was compared\nwith a standard gradient-spoiled CSI acquisition and subsequent IDEAL\nprocessing. In addition, in vivo T1 and T2 of water, glucose and Glx\n(glutamate+glutamine) were estimated from non-localized inversion recovery and\nspin-echo measurements.High-resolution whole-brain dynamic quantitative DMI\nmaps were successfully obtained for all three acquisitions. Phase-cycling\nimproved the quality of bSSFP metabolite estimation and provided additional\nspectral encoding. The SNR improvement was only observed for the CSI variant of\nbSSFP acquisitions with an average increase of 18% and 27% for glucose and Glx,\nrespectively, compared to the vendor's CSI. ME-bSSFP acquisition achieved\nhigher resolutions than acquisition-weighted CSI and exhibited several\nqualitative improvements.",
        "Aggressive behavior, including aggression towards others and self-injury,\noccurs in up to 80% of children and adolescents with autism, making it a\nleading cause of behavioral health referrals and a major driver of healthcare\ncosts. Predicting when autistic youth will exhibit aggression is challenging\ndue to their communication difficulties. Many are minimally verbal or have poor\nemotional insight. Recent advances in Machine Learning and wearable biosensing\nenable short-term aggression predictions within a limited future window\n(typically one to three minutes). However, existing models do not estimate\naggression probability within longer future windows nor the expected number of\naggression onsets over such a period. To address these limitations, we employ\nTemporal Point Processes (TPPs) to model the generative process of aggressive\nbehavior onsets in inpatient youths with autism. We hypothesize that aggressive\nbehavior onsets follow a self-exciting process driven by short-term history,\nmaking them well-suited for Hawkes Point Process modeling. We establish a\nbenchmark and demonstrate through Goodness-of-Fit statistics and predictive\nmetrics that TPPs perform well modeling aggressive behavior onsets in inpatient\nyouths with autism. Additionally, we gain insights into the onset generative\nprocess, like the branching factor near criticality, and suggest TPPs may\nenhance future clinical decision-making and preemptive interventions.",
        "We study a $d$-dimensional branching random walk (BRW) in an i.i.d. random\nenvironment on $\\mathbb{Z}^d$ in discrete time. A Bernoulli trap field is\nattached to $\\mathbb{Z}^d$, where each site, independently of the others, is a\ntrap with a fixed probability. The interaction between the BRW and the trap\nfield is given by the hard killing rule. Given a realization of the\nenvironment, over each time step, each particle first moves according to a\nsimple symmetric random walk to a nearest neighbor, and immediately afterwards,\nsplits into two particles if the new site is not a trap or is killed instantly\nif the new site is a trap. Conditional on the ultimate survival of the BRW, we\nprove a strong law of large numbers for the total mass of the process. Our\nresult is quenched, that is, it holds in almost every environment in which the\nstarting point of the BRW is inside the infinite connected component of\ntrap-free sites.",
        "We present a 5 month NICER X-ray monitoring campaign for two low luminosity\nactive galactic nuclei (LLAGNs) -- NGC 4594 and IC 1459 -- with complementary\nSwift and NuSTAR observations. Utilizing an absorbed power law and thermal\nsource model combined with NICER's SCORPEON background model, we demonstrate\nthe effectiveness of joint source\/background modeling for constraining emission\nfrom faint, background-dominated targets. Both sources are dominated by nuclear\npower law emission with photon indices $\\Gamma \\sim 1.5 - 2$, with NGC 4594\nbeing slightly harder than IC 1459. The thermal contribution in both sources is\nfainter, but constant, with $kT \\sim 0.5$ keV ($\\sim 5 \\times 10^6$ K). The\npower law flux and $\\Gamma$ are strongly anti-correlated in both sources, as\nhas been seen for other LLAGNs with radiatively inefficient accretion flows.\nNGC 4594 is the brighter source and exhibits significant aperiodic variability.\nIts variability timescale with an upper limit of $5 - 7$ days indicates\nemission originating from $< 100 R_{g}$, at the scale of the inner accretion\nflow. A spectral break found at $\\sim 6$ keV, while tentative, could arise from\nsynchrotron\/inverse compton emission. This high-cadence LLAGN X-ray monitoring\ncampaign underlines the importance of multi-wavelength variability studies for\na sample of LLAGNs to truly understand their accretion and outflow physics."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"ConTEXTual Net: A Multimodal Vision-Language Model for Segmentation of Pneumothorax",
    "start_abstract":"Radiology narrative reports often describe characteristics of a patient's disease, including its location, size, and shape. Motivated by the recent success multimodal learning, we hypothesized that this descriptive text could guide medical image analysis algorithms. We proposed novel vision-language model, ConTEXTual Net, for task pneumothorax segmentation on chest radiographs. Net extracts language features from physician-generated free-form radiology using pre-trained model. then introduced cross-attention between intermediate embeddings an encoder-decoder convolutional neural network to enable guidance analysis. was trained CANDID-PTX dataset consisting 3196 positive cases with annotations 6 different physicians as well clinical reports. Using cross-validation, achieved Dice score 0.716\u00b10.016, which similar degree inter-reader variability (0.712\u00b10.044) computed subset data. It outperformed vision-only models (Swin UNETR: 0.670\u00b10.015, ResNet50 U-Net: 0.677\u00b10.015, GLoRIA: 0.686\u00b10.014, nnUNet 0.694\u00b10.016) competing model (LAVT: 0.706\u00b10.009). Ablation studies confirmed it information led performance gains. Additionally, show certain augmentation methods degraded Net's breaking image-text concordance. also evaluated effects activation functions in module, highlighting efficacy our chosen architectural design.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "MRI-Guided Adaptive Radiation Therapy"
      ],
      "abstract":[
        "Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Magnetically arrested disk flux eruption events to describe SgrA* flares",
        "Bayesian optimization to infer parameters in viscoelasticity",
        "STARS-Enabled Full-Duplex Two-Way mMIMO System Under\n  Spatially-Correlated Channels",
        "FOGGIE X: Characterizing the Small-Scale Structure of the CGM and its\n  Imprint on Observables",
        "Cavity engineering of solid-state materials without external driving",
        "Forecasting Extreme Temperatures in Siberia Using Supervised Learning\n  and Conformal Prediction Regions",
        "Nonlinear Domain Engineering for Quantum Technologies",
        "Towards reconstruction of Pulsed-wave Doppler signals from Non-invasive\n  fetal ECG",
        "Anderson Accelerated Operator Splitting Methods for Convex-nonconvex\n  Regularized Problems",
        "Quasi-likelihood ratio test for jump-diffusion processes based on\n  adaptive maximum likelihood inference",
        "Unveiling Wireless Users' Locations via Modulation Classification-based\n  Passive Attack",
        "Synthetic Porous Microstructures: Automatic Design, Simulation, and\n  Permeability Analysis",
        "Phenomenological Scaling Crossovers in Non-Equilibrium Critical Dynamics",
        "Investigating the broadening phenomenon in two-particle correlations\n  induced by gluon saturation",
        "Optimal COVID-19 vaccine prioritization by age depends critically on\n  inter-group contacts and vaccination rates",
        "Topological pumping of multi-frequency solitons",
        "Quantum Measurement for Quantum Chemistry on a Quantum Computer",
        "Are dark matter and dark energy omnipresent?",
        "Biglobal resolvent analysis of separated flow over a NACA0012 airfoil",
        "Tracking UWB Devices Through Radio Frequency Fingerprinting Is Possible",
        "A New Type of MPd5 Kagome Superconductors",
        "Extended weak order for the affine symmetric group",
        "A Generalized Benford Framework for Threat Identification in\n  Counter-Intelligence",
        "On Congruence Theorem for valued division algebras",
        "Phases of String Stars in the Presence of a Spatial Circle",
        "High-resolution deuterium metabolic imaging of the human brain at 9.4 T\n  using bSSFP spectral-spatial acquisitions",
        "Temporal Point Process Modeling of Aggressive Behavior Onset in\n  Psychiatric Inpatient Youths with Autism",
        "Strong law of large numbers for a branching random walk among Bernoulli\n  traps",
        "Tracking X-ray Variability in Next Generation EHT LLAGN Targets"
      ],
      "abstract":[
        "Context. Magnetically arrested disks are among the most suitable candidates\nfor describing the gas accretion and observed emission in the vicinity of\nsupermassive black holes. Aims. This work aims to provide a direct correlation\nbetween the quasi-periodic flux eruption events, characteristic of MAD\naccretion disk simulations, and the observed flaring behavior in the Galactic\ncenter. Methods. We employ a MAD accretion disk with a distinct\ncounter-clockwise rotation and investigate the evolution of magnetized flux\ntubes generated during a prominent flux eruption event. Although these flux\ntubes demonstrate a clockwise pattern, they experience significant dragging by\nthe accretion disk's rotation. This study models the motion of hot spots,\nformed on the disk's equatorial plane due to magnetic reconnection, as they\ntravel along the magnetized flux tubes at a fraction of the speed of light.\nResults. Hot spots with a relativistic ejection velocity are able to balance\nout the counter-clockwise dragging of the flux tube's foot-point on the disk\nand demonstrate a clockwise motion in the sky, that is in good agreement with\nthe NIR flares in the Galactic center. In addition, our flare models favor\nface-on inclinations in the range $[0^\\circ, 34^\\circ]$ and $[163^\\circ,\n180^\\circ]$ for SgrA*. Conclusions. The flux eruption events that arise\nnaturally in the MAD accretion state provide a promising framework for\nreproducing the observed flaring behavior in the vicinity of SgrA*.",
        "Inferring viscoelasticity parameters is a key challenge that often leads to\nnon-unique solutions when fitting rheological data. In this context, we propose\na machine learning approach that utilizes Bayesian optimization for parameter\ninference during curve-fitting processes. To fit a viscoelastic model to\nrheological data, the Bayesian optimization maps the parameter values to a\ngiven error function. It then exploits the mapped space to identify parameter\ncombinations that minimize the error. We compare the Bayesian optimization\nresults to traditional fitting routines and demonstrate that our approach finds\nthe fitting parameters in a less or similar number of iterations. Furthermore,\nit also creates a \"white-box\" and supervised framework for parameter estimation\nin linear viscoelasticity modeling.",
        "\\underline{S}imultaneous \\underline{t}ransmitting \\underline{a}nd\n\\underline{r}eflecting \\underline{s}urface (STARS)-assisted systems have\nemerged to fill this gap by providing $ 360^{\\circ}$ wireless coverage. In\nparallel,\n  full-duplex (FD) communication offers a higher achievable rate through\nefficient spectrum utilization compared to the half-duplex (HD) counterpart.\nMoreover, two-way\/bi-directional communications in an FD system can further\nenhance the system's spectral efficiency. Hence, in this paper, we propose a\nSTARS-enabled massive MIMO deployment in an FD two-way communication network\nfor highly efficient spectrum utilization, while covering the dead zones around\nthe STARS. This model enables simultaneous information exchange between\nmultiple nodes, while \\emph{potentially} doubling the spectral efficiency (SE).\nBy invoking the use-and-then-forget (UaTF) combining scheme, we derive a\nclosed-form expression for an achievable SE at each user of the system\nconsidering both uplink and downlink communications based on statistical\nchannel state information (CSI), while also accounting for imperfect CSI and\ncorrelated fading conditions. Moreover, we formulate an optimization problem to\nobtain an optimal passive beamforming matrix design at the STARS that maximizes\nthe sum achievable SE. The considered problem is non-convex and we propose a\nprovably-convergent low-complexity algorithm, termed as \\underline{pro}jected\n\\underline{gr}adient \\underline{a}scent \\underline{m}ethod (ProGrAM), to obtain\na stationary solution. Extensive numerical results are provided to establish\nthe performance superiority of the FD STARS-enabled system over the HD\nSTARS-enabled and FD conventional RIS (cRIS)-enabled counterparts, and also to\nshow the effect of different parameters of interest on the system performance.",
        "One of the main unknowns in galaxy evolution is how gas flows into and out of\ngalaxies in the circumgalactic medium (CGM). Studies observing the CGM in\nabsorption using multiple or extended background objects suggest a high degree\nof variation on relatively small ($\\lesssim 1$ kpc) spatial scales. Similarly,\nhigh-resolution simulations generally exhibit small-scale substructure in the\ngas around galaxies. We examine the small-scale structure of the $z = 1$ CGM\nusing simulations from the FOGGIE (Figuring Out Gas & Galaxies in Enzo)\nproject. We select gaseous substructures (\"clumps\") by their local overdensity\nand investigate their physical properties, including temperature, metallicity,\nand kinematics with respect to the galaxy and the nearby surroundings. FOGGIE\nresolves clumps down to sphericalized radii $R \\sim 0.25$ kpc at $z = 1$. The\ndistribution of clumps peaks at $\\sim 10^5$ $\\rm M_{\\odot}$ and $10^{4}$ K,\nconsistent with relatively condensed, cool gas with a slight preference for\ninflow-like velocities. Many clumps show internal temperature and density\nvariations, and thus internally varying ionization levels for key diagnostic\nions such as HI, MgII, and OVI. The average metallicity in clumps is about a\nfactor 1.5--2$\\times$ lower in metallicity than nearby gas, suggesting that the\nmetals are not well-mixed between structured and diffuse CGM, which may have\nimplications for observational metallicity estimations of dense CGM clouds. We\nestimate the survivability of CGM clumps and find that structures larger than\n0.5 kpc are generally long-lived. Finally, we qualitatively compare the\nsimulated cloud properties to Milky Way high-velocity clouds.",
        "Confining electromagnetic fields inside an optical cavity can enhance the\nlight-matter coupling between quantum materials embedded inside the cavity and\nthe confined photon fields. When the interaction between the matter and the\nphoton fields is strong enough, even the quantum vacuum field fluctuations of\nthe photons confined in the cavity can alter the properties of the\ncavity-embedded solid-state materials at equilibrium and room temperature. This\napproach to engineering materials with light avoids fundamental issues of\nlaser-induced transient matter states. To clearly differentiate this field from\nphenomena in driven systems, we call this emerging field cavity materials\nengineering. In this review, we first present theoretical frameworks,\nespecially, ab initio methods, for describing light-matter interactions in\nsolid-state materials embedded inside a realistic optical cavity. Next, we\noverview a few experimental breakthroughs in this domain, detailing how the\nground state properties of materials can be altered within such confined\nphotonic environments. Moreover, we discuss state-of-the-art theoretical\nproposals for tailoring material properties within cavities. Finally, we\noutline the key challenges and promising avenues for future research in this\nexciting field.",
        "In this paper, we step back from a variety of competing heat wave definitions\nand forecast directly unusually high temperatures. Our testbed is the Russian\nFar East in the summers of 2022 and 2023. Remotely sensed data from NASA's Aqua\nspacecraft are organized into a within-subject design that can reduce nuisance\nvariation in forecasted temperatures. Spatial grid cells are the study units.\nEach is exposed to precursors of a faux heat wave in 2022 and to precursors of\na reported heat wave in 2023. The precursors are used to forecast temperatures\ntwo weeks in the future for each of 31 consecutive days. Algorithmic fitting\nprocedures produce forecasts with promise and relatively small conformal\nprediction regions having a coverage probability of at least .75. Spatial and\ntemporal dependence are manageable. At worst, there is weak dependence such\nthat conformal prediction inference is only asymptotically valid.",
        "The continuously growing effort towards developing real-world quantum\ntechnological applications has come to demand an increasing amount of\nflexibility from its respective platforms. This review presents a highly\nadaptable engineering technique for photonic quantum technologies based on the\nartificial structuring of the material nonlinearity. This technique, while, in\na simple form, already featured across the full breadth of photonic quantum\ntechnologies, has undergone significant development over the last decade, now\nfeaturing advanced, aperiodic designs. This review gives an introduction to the\nthree-wave-mixing processes lying at the core of this approach, and\nillustrates, on basis of the underlying quantum-mechanical description, how\nthey can artificially be manipulated to engineer the corresponding photon\ncharacteristics. It then describes how this technique can be employed to\nrealize a number of very different objectives which are expected to find\napplication across the full range of photonic quantum technologies, and\npresents a summary of the research done towards these ends to date.",
        "Fetal cardiac health monitoring with invasive methods have a limited\nviability because they can only be utilized during labor and are uncomfortable.\nOn the other hand non-invasive fECG are adulterated with maternal ECG, and\nhence resulting in poor analysis. In contrast, Pulsed-wave Doppler (PwD)\nechocardiography generates high-quality signals representing fetal blood volume\ninflow-outflow. It also follows non-invasive signal acquisition. The only\ndrawback is that it requires highly expensive setup. To address this aspect, we\nput forward a challenging research question - can we reconstruct PwD signals\nusing non-invasive fetal ECG?",
        "Convex-nonconvex (CNC) regularization is a novel paradigm that employs a\nnonconvex penalty function while maintaining the convexity of the entire\nobjective function. It has been successfully applied to problems in signal\nprocessing, statistics, and machine learning. Despite its wide application, the\ncomputation of CNC regularized problems remains challenging and\nunder-investigated. To fill the gap, we study several operator splitting\nmethods and their Anderson accelerated counterparts for solving least squares\nproblems with CNC regularization. We establish the global convergence of the\nproposed algorithm to an optimal point and demonstrate its practical speed-ups\nin various applications.",
        "In this paper, we consider parameter estimation and quasi-likelihood ratio\ntests for multidimensional jump-diffusion processes defined by stochastic\ndifferential equations. In general, simultaneous estimation faces challenges\nsuch as an increase of computational time for optimization and instability of\nestimation accuracy as the dimensionality of parameters grows. To address these\nissues, we propose an adaptive quasi-log likelihood function based on the joint\nquasi-log likelihood function introduced by Shimizu and Yoshida (2003, 2006)\nand Ogihara and Yoshida (2011). We then show that the resulting adaptive\nestimators possess consistency and asymptotic normality. Furthermore, we extend\nthe joint quasi-log likelihood function proposed by Shimizu and Yoshida (2003,\n2006) and Ogihara and Yoshida (2011) and construct a test statistic using the\nproposed adaptive estimators. We prove that the proposed test statistic\nconverges in distribution to a $\\chi^2$-distribution under the null hypothesis\nand that the associated test is consistent. Finally, we conduct numerical\nsimulations using a specific jump-diffusion process model to examine the\nasymptotic behavior of the proposed adaptive estimators and test statistics.",
        "The broadcast nature of the wireless medium and openness of wireless\nstandards, e.g., 3GPP releases 16-20, invite adversaries to launch various\nactive and passive attacks on cellular and other wireless networks. This work\nidentifies one such loose end of wireless standards and presents a novel\npassive attack method enabling an eavesdropper (Eve) to localize a line of\nsight wireless user (Bob) who is communicating with a base station or WiFi\naccess point (Alice). The proposed attack involves two phases. In the first\nphase, Eve performs modulation classification by intercepting the downlink\nchannel between Alice and Bob. This enables Eve to utilize the publicly\navailable modulation and coding scheme (MCS) tables to do pesudo-ranging, i.e.,\nthe Eve determines the ring within which Bob is located, which drastically\nreduces the search space. In the second phase, Eve sniffs the uplink channel,\nand employs multiple strategies to further refine Bob's location within the\nring. Towards the end, we present our thoughts on how this attack can be\nextended to non-line-of-sight scenarios, and how this attack could act as a\nscaffolding to construct a malicious digital twin map.",
        "This study introduces an open-source computational framework for the\ngeneration and permeability evaluation of synthetic porous media. The proposed\nmethodology integrates crystallographic and meshing tools to construct\ncontrolled microstructures with tunable porosity, facilitating seamless\ntransitions from geometric modelling to computational domains for numerical\nsimulations. The generated structures are analysed through fluid-structure\ninteraction (FSI) simulations, leveraging the Entropically Damped Artificial\nCompressibility (EDAC) formulation in conjunction with the\nDiscretization-Corrected Particle Strength Exchange (DC-PSE) method.\n  A novel approach for the numerical estimation of the macroscopic permeability\ntensor is presented, employing a stochastic upscaling technique inspired by the\nvolume averaging method. To validate the framework, we investigate the\ntransition between creeping and non-Darcy flow regimes through parametric\npermeability studies, utilizing a one-dimensional approach for practical\nbenchmarking.\n  The results establish a foundation for experimental validation and provide\ninsights into the customised design of porous structures for engineering and\nbiomedical applications, offering a versatile tool for research in fluid\ntransport and porous media mechanics.",
        "Dynamical universality plays a fundamental role in understanding the scaling\nproperties of critical dynamics, including absorbing phase transitions and\nphysical aging. Although individual universality classes have been extensively\nstudied, the theoretical framework for scaling crossovers between distinct\ndynamical regimes remains underdeveloped. In this work, we propose a\nphenomenological approach to describe dynamic scaling crossovers in\nreaction-diffusion systems, utilizing a Bernoulli differential equation with\ntime-dependent reaction coefficients. Under a new scaling hypothesis for\nspatial fluctuations, we analytically derive five universal power-law crossover\nfunctions. These solutions accurately describe experimental observations of\nabsorbing phase transitions in turbulent liquid crystals and reaction-diffusion\ncrossovers in exciton-exciton recombination, revealing possible mechanisms\nassociated with many-body correlations and two-component dynamics. These\nfindings suggest the existence of universal scaling laws governing the\ntransition between different dynamical states, motivating further studies using\nfield-theoretic approaches and renormalization group analyses.",
        "It has been found that the gluon density inside the proton grows rapidly at\nsmall momentum fractions. Quantum Chromodynamics (QCD) predicts that this\ngrowth can be regulated by nonlinear effects, ultimately leading to gluon\nsaturation. Within the color glass condensate framework, nonlinear QCD effects\nare predicted to suppress and broaden back-to-back angular correlations in\ncollisions involving heavy nuclei. While suppression has been observed in\nvarious experiments in $d\/p$$+$A collisions compared to $p$$+$$p$ collisions,\nthe predicted broadening remains unobserved. This study investigates the\ncontributions of intrinsic transverse momentum ($k_T$), which is associated\nwith saturation physics, as well as parton showers and transverse motion from\nfragmentation ($p_T^{\\mathrm{frag}}$), which are not saturation dependent, to\nthe width of the correlation function. Our findings show that the\nnon-saturation dependent effects, especially the initial-state parton shower\nand $p_T^{\\mathrm{frag}}$, which occur independently of the collision system,\nsmear the back-to-back correlation more than gluon saturation does, making the\nbroadening phenomenon difficult to observe.",
        "The limited availability of COVID-19 vaccines has prompted extensive research\non optimal vaccination strategies. Previous studies have considered various\nnon-pharmaceutical interventions, vaccine efficacy, and distribution\nstrategies. In this work, we address the combined effects of inter-group\ncontacts and vaccination rates under contact reduction, analyzing the Spanish\npopulation's demographic and age group contact patterns and incorporating\nreinfection dynamics. We conduct an exhaustive analysis, evaluating 362,880\npermutations of 9 age groups across 6 vaccination rates and two distinct,\nempirically quantified scenarios for social contacts. Our results show that at\nintermediate-to-high vaccination rates with unrestricted social contacts,\noptimal age-based vaccination strategies only slightly deviate from\nolder-to-younger prioritization, yielding marginal reductions in deaths and\ninfections. However, when significant reductions in social contacts are\nenforced -similar to the lockdowns in 2020-, there are substantial\nimprovements, particularly at moderate vaccination rates. These restrictions\nlead to a transition where infection propagation is halted, a scenario that\nbecame achievable during the pandemic with the observed vaccination rates. Our\nfindings emphasize the importance of combining appropriate social contact\nreductions with vaccination to optimize age-based vaccination strategies,\nunderscoring the complex, nonlinear dynamics involved in pandemic dynamics and\nthe necessity for tailored, context-specific interventions.",
        "We report on the topological pumping of quadratic optical solitons, observed\nthrough their quantized transport in a dynamic optical potential. A distinctive\nfeature of this system is that the two fields with different frequencies, which\ntogether form the quadratic soliton, evolve in separate yet topologically\nequivalent dynamic optical potentials. Pumping in this system exhibits several\nnotable differences from pumping in cubic media. While Chern indices\ncharacterizing quantized transport for uncoupled fundamental and second\nharmonic waves are nonzero, small-amplitude solitons with narrow spectra do not\nmove, thus revealing a non-topological phase. As the nonlinearity increases,\nthe system undergoes a sharp transition, depending on the velocity of one of\nthe sublattices forming dynamical potential, into the phase where the quantized\ntransport of quadratic solitons governed by nonzero Chern numbers is observed.\nThe power level at which this transition occurs increases with increase of\npumping velocity, and the transition is observed even in the regime when the\nadiabatic approximation no longer applies. Unlike in cubic media, in a\nquadratic medium neither breakup of topological pumping nor fractional pumping\nat high power levels are observed.",
        "Quantum chemistry is among the most promising applications of quantum\ncomputing, offering the potential to solve complex electronic structure\nproblems more efficiently than classical approaches. A critical component of\nany quantum algorithm is the measurement step, where the desired properties are\nextracted from a quantum computer. This review focuses on recent advancements\nin quantum measurement techniques tailored for quantum chemistry, particularly\nwithin the second quantized framework suitable for current and near-term\nquantum hardware.\n  We provide a comprehensive overview of measurement strategies developed\nprimarily for the Variational Quantum Eigensolver (VQE) and its derivatives.\nThese strategies address the inherent challenges posed by complexity of the\nelectronic Hamiltonian operator. Additionally, we examine methods for\nestimating excited states and one- and two-electron properties, extending the\napplicability of quantum algorithms to broader chemical phenomena.\n  Key aspects of the review include approaches for constructing measurement\noperators with reduced classical preprocessing and quantum implementation\ncosts, techniques to minimize the number of measurements required for a given\naccuracy, and error mitigation strategies that leverage symmetries and other\nproperties of the measurement operators. Furthermore, we explore measurement\nschemes rooted in Quantum Phase Estimation (QPE), which are expected to become\nviable with the advent of fault-tolerant quantum computing.\n  This review emphasizes foundational concepts and methodologies rather than\nnumerical benchmarks, serving as a resource for researchers aiming to enhance\nthe efficiency and accuracy of quantum measurements in quantum chemistry.",
        "A set of temporal singularities (transients) in the mass-energy density and\npressure, bearing a specific mathematical structure which represents a new\nsolution to the continuity equation (\\ie~conservation of mass-energy) and\nsatisfying the strong energy condition, is proposed to account for the\nexpansion history of a homogeneous Universe, and the formation and binding of\nlarge scale structures as a continuum approximation of their cumulative\neffects. These singularities are unobservable because they occur rarely in time\nand are unresolvably fast, and that could be the reason why dark matter and\ndark energy have not been found. Implication on inflationary cosmology is\ndiscussed. The origin of these temporal singularities is unknown, safe to say\nthat the same is true of the moment of the Big Bang itself. This work\ncomplements a recent paper, where a topological defect in the form of a\nspatial, spherical shell of density singularity giving rise to a 1\/r attractive\nforce (to test particles of positive mass) but zero integrated mass over a\nlarge volume of space, was proposed to solve the dark matter problem in bound\nstructures but not cosmic expansion. The idea also involved a negative density,\nwhich is not present in the current model.",
        "The effects of Reynolds number across $Re=1000$, $2500$, $5000$, and $10000$\non separated flow over a two-dimensional NACA0012 airfoil at an angle of attack\nof $\\alpha=14^\\circ$ are investigated through the biglobal resolvent analysis.\nWe identify modal structures and energy amplifications over a range of\nfrequency, spanwise wavenumber, and discount parameter, providing insights\nacross various timescales. Using temporal discounting, we find that the shear\nlayer dynamics dominates over short time horizons, while the wake dynamics\nbecomes the primary amplification mechanism over long time horizons. Spanwise\neffects also appear over long time horizon, sustained by low frequencies. At a\nfixed timescale, we investigate the influence of Reynolds number on response\nand forcing mode structures, as well as the energy gain over different\nfrequencies. Across all Reynolds numbers, the response modes shift from\nwake-dominated structures at low frequencies to shear layer-dominated\nstructures at higher frequencies. The frequency at which the dominant mechanism\nchanges is independent of the Reynolds number. The response mode structures\nshow similarities across different Reynolds numbers, with local streamwise\nwavelengths only depending on frequency. Comparisons at a different angle of\nattack ($\\alpha=9^\\circ$) show that the transition from wake to shear layer\ndynamics with increasing frequency only occurs if the unsteady flow is\nthree-dimensional. We also study the dominant frequencies associated with wake\nand shear layer dynamics across the angles of attack and Reynolds numbers, and\npresent the characteristic scaling for each mechanism.",
        "Ultra-wideband (UWB) is a state-of-the-art technology designed for\napplications requiring centimeter-level localization. Its widespread adoption\nby smartphone manufacturer naturally raises security and privacy concerns.\nSuccessfully implementing Radio Frequency Fingerprinting (RFF) to UWB could\nenable physical layer security, but might also allow undesired tracking of the\ndevices. The scope of this paper is to explore the feasibility of applying RFF\nto UWB and investigates how well this technique generalizes across different\nenvironments. We collected a realistic dataset using off-the-shelf UWB devices\nwith controlled variation in device positioning. Moreover, we developed an\nimproved deep learning pipeline to extract the hardware signature from the\nsignal data. In stable conditions, the extracted RFF achieves over 99%\naccuracy. While the accuracy decreases in more changing environments, we still\nobtain up to 76% accuracy in untrained locations.",
        "Kagome materials, which are composed of hexagons tiled with a shared\ntriangle, have inspired enormous interest due to their unique structures and\nrich physical properties, but exploring superconducting material systems with\nnew kagome structures is still an important research direction. Here, we\npredict a new type of kagome superconductors, MPd5 (M is a group IIA metal\nelement), and identify that they exhibit coexistence of superconductivity and\nnon-trivial topological properties. We uncover their phonon-mediated\nsuperconductivity by the density functional theory for superconductors (SCDFT),\npredicting the superconducting transition temperatures (Tc) of 2.64, 2.03, and\n1.50 K for CaPd5, SrPd5, and BaPd5, respectively, which can be effectively\ntuned by external pressure and electron doping. The present results also\ndemonstrate that MPd5 have various topological properties such as, CaPd5 shows\ntopological non-trivial intersection near the Fermi level (EF). Our results\nindicate that the MPd5 materials can be an emerging material platform with rich\nexotic physics in their kagome structures, and render themselves excellent\ncandidates for superconducting and advanced functional materials that could be\nutilized in topological quantum computing and information technology.",
        "The extended weak order on a Coxeter group $W$ is the poset of biclosed sets\nin its root system. In (Barkley-Speyer 2024), it was shown that when\n$W=\\widetilde{S}_n$ is the affine symmetric group, then the extended weak order\nis a quotient of the lattice $L_n$ of translation-invariant total orderings of\nthe integers. In this article, we give a combinatorial introduction to $L_n$\nand the extended weak order on $\\widetilde{S}_n$. We show that $L_n$ is an\nalgebraic completely semidistributive lattice. We describe its canonical join\nrepresentations using a cyclic version of Reading's non-crossing arc diagrams.\nWe also show analogous statements for the lattice of all total orders of the\nintegers, which is the extended weak order on the symmetric group $S_\\infty$. A\nkey property of both of these lattices is that they are profinite; we also\nprove that a profinite lattice is join semidistributive if and only if its\ncompact elements have canonical join representations. We conjecture that the\nextended weak order of any Coxeter group is a profinite semidistributive\nlattice.",
        "In this paper, we develop a framework of 'Benford models' for\ncounter-intelligence investigations which analyze frequency data of a suspect's\nvisits to physical locations, online websites, and communication channels. We\naccomplish this by establishing the Benford measure for continuous & bounded\ndomains, generalizing the accumulated percentage differences between sites in\nthe frequency data with the log-determinant of 'Benford Matrices,' employing an\nestimator to determine a 'Benford Test Statistic,' and identifying maximal\nvalues of that test statistic across all permutations of included sites in our\ndata. This framework is intended to complement outlier analysis models by\nfinding where hidden Benford patterns 'break' in frequency data and telling\ninvestigators which sites they should investigate.",
        "Let $K$ be a field with Henselian valuation, and $D$ be a tame $F$-central\ndivision algebra. Let $\\mathrm{TK}_1(D)$ denote the torsion group of the\nWhitehead group ${\\rm K}_1(D)=D^*\/D'$. Let $G$ be the subgroup of $D^*$ such\nthat $\\mathrm{TK}_1(D) = G\/D'$. In this note, we prove that either $(1+M_D)\n\\cap G \\subseteq D'$ or ${\\rm char}(\\overline{F})=p>0$ and $((1+M_D) \\cap\nG)D'\/D'$ is a $p$-group. We also provide some examples to calculate the latter\ngroup.",
        "In string theory, black holes are expected to transition into string stars as\ntheir Hawking temperature approaches the Hagedorn temperature. We study string\nstars and their phase transitions in the Euclidean spacetime\n$\\mathbb{R}^d\\times\\mathbb{S}_\\tau^1\\times\\mathbb{S}_z^1$. Using the\nHorowitz-Polchinski (HP) effective field theory, we discover novel solutions\nfor $d=2$. The uniform string star exhibits a scaling symmetry that results in\nthe absence of a critical point for its transition into the non-uniform\nsolution. For $d=4$, we show that quartic corrections to the effective action\nresolve the mass degeneracy of uniform string stars. At $d=5$, we find that as\nnon-uniformity increases, the quartic terms become significant (while\nhigher-order terms remain negligible) and reverse the direction of temperature\nvariation, leading to a swallowtail-type phase diagram in the canonical\nensemble. Extending the quartic-corrected EFT to $d=6$, we find that string\nstars with small non-uniformity dominate the microcanonical ensemble but not\nthe canonical ensemble, similar to the $d=5$ case. However, in the\nmicrocanonical ensemble, the uniform string star is anomalously (un)stable when\nthe spatial circle is larger (smaller) than the critical size.",
        "We demonstrated the feasibility of using bSSFP acquisitions for off-resonance\ninsensitive high-resolution [6,6'-2H2]-glucose deuterium metabolic imaging\n(DMI) studies in the healthy human brain at 9.4T. Balanced SSFP acquisitions\nhave potential to improve the sensitivity of DMI despite the SNR loss of\nphase-cycling and other human scanner constraints.We investigated two variants\nof bSSFP acquisitions, namely uniform-weighted multi echo and\nacquisition-weighted CSI to improve the SNR of deuterium metabolic imaging\n(DMI) in the brain with oral labelled-glucose intake. Phase-cycling was\nintroduced to make bSSFP acquisitions less sensitive to B0 inhomogeneity. Two\nSNR optimal methods for obtaining metabolite amplitudes from the phase-cycled\ndata were proposed. The SNR performance of the two bSSFP variants was compared\nwith a standard gradient-spoiled CSI acquisition and subsequent IDEAL\nprocessing. In addition, in vivo T1 and T2 of water, glucose and Glx\n(glutamate+glutamine) were estimated from non-localized inversion recovery and\nspin-echo measurements.High-resolution whole-brain dynamic quantitative DMI\nmaps were successfully obtained for all three acquisitions. Phase-cycling\nimproved the quality of bSSFP metabolite estimation and provided additional\nspectral encoding. The SNR improvement was only observed for the CSI variant of\nbSSFP acquisitions with an average increase of 18% and 27% for glucose and Glx,\nrespectively, compared to the vendor's CSI. ME-bSSFP acquisition achieved\nhigher resolutions than acquisition-weighted CSI and exhibited several\nqualitative improvements.",
        "Aggressive behavior, including aggression towards others and self-injury,\noccurs in up to 80% of children and adolescents with autism, making it a\nleading cause of behavioral health referrals and a major driver of healthcare\ncosts. Predicting when autistic youth will exhibit aggression is challenging\ndue to their communication difficulties. Many are minimally verbal or have poor\nemotional insight. Recent advances in Machine Learning and wearable biosensing\nenable short-term aggression predictions within a limited future window\n(typically one to three minutes). However, existing models do not estimate\naggression probability within longer future windows nor the expected number of\naggression onsets over such a period. To address these limitations, we employ\nTemporal Point Processes (TPPs) to model the generative process of aggressive\nbehavior onsets in inpatient youths with autism. We hypothesize that aggressive\nbehavior onsets follow a self-exciting process driven by short-term history,\nmaking them well-suited for Hawkes Point Process modeling. We establish a\nbenchmark and demonstrate through Goodness-of-Fit statistics and predictive\nmetrics that TPPs perform well modeling aggressive behavior onsets in inpatient\nyouths with autism. Additionally, we gain insights into the onset generative\nprocess, like the branching factor near criticality, and suggest TPPs may\nenhance future clinical decision-making and preemptive interventions.",
        "We study a $d$-dimensional branching random walk (BRW) in an i.i.d. random\nenvironment on $\\mathbb{Z}^d$ in discrete time. A Bernoulli trap field is\nattached to $\\mathbb{Z}^d$, where each site, independently of the others, is a\ntrap with a fixed probability. The interaction between the BRW and the trap\nfield is given by the hard killing rule. Given a realization of the\nenvironment, over each time step, each particle first moves according to a\nsimple symmetric random walk to a nearest neighbor, and immediately afterwards,\nsplits into two particles if the new site is not a trap or is killed instantly\nif the new site is a trap. Conditional on the ultimate survival of the BRW, we\nprove a strong law of large numbers for the total mass of the process. Our\nresult is quenched, that is, it holds in almost every environment in which the\nstarting point of the BRW is inside the infinite connected component of\ntrap-free sites.",
        "We present a 5 month NICER X-ray monitoring campaign for two low luminosity\nactive galactic nuclei (LLAGNs) -- NGC 4594 and IC 1459 -- with complementary\nSwift and NuSTAR observations. Utilizing an absorbed power law and thermal\nsource model combined with NICER's SCORPEON background model, we demonstrate\nthe effectiveness of joint source\/background modeling for constraining emission\nfrom faint, background-dominated targets. Both sources are dominated by nuclear\npower law emission with photon indices $\\Gamma \\sim 1.5 - 2$, with NGC 4594\nbeing slightly harder than IC 1459. The thermal contribution in both sources is\nfainter, but constant, with $kT \\sim 0.5$ keV ($\\sim 5 \\times 10^6$ K). The\npower law flux and $\\Gamma$ are strongly anti-correlated in both sources, as\nhas been seen for other LLAGNs with radiatively inefficient accretion flows.\nNGC 4594 is the brighter source and exhibits significant aperiodic variability.\nIts variability timescale with an upper limit of $5 - 7$ days indicates\nemission originating from $< 100 R_{g}$, at the scale of the inner accretion\nflow. A spectral break found at $\\sim 6$ keV, while tentative, could arise from\nsynchrotron\/inverse compton emission. This high-cadence LLAGN X-ray monitoring\ncampaign underlines the importance of multi-wavelength variability studies for\na sample of LLAGNs to truly understand their accretion and outflow physics."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"MRI-Guided Adaptive Radiation Therapy",
    "start_abstract":"Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training.",
    "start_categories":[
      "Oncology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b4",
        "b2"
      ],
      "title":[
        "Deep Learning for Automatic Gross Tumor Volumes Contouring in Esophageal Cancer Based on Contrast-Enhanced Computed Tomography Images: A Multi-Institutional Study",
        "ConTEXTual Net: A Multimodal Vision-Language Model for Segmentation of Pneumothorax"
      ],
      "abstract":[
        "Purpose To develop and externally validate an automatic artificial intelligence (AI) tool for delineating gross tumor volume (GTV) in patients with esophageal squamous cell carcinoma (ESCC), which can assist in neo-adjuvant or radical radiation therapy treatment planning. Methods and Materials In this multi-institutional study, contrast-enhanced CT images from 580 eligible ESCC patients were retrospectively collected. The GTV contours delineated by 2 experts via consensus were used as ground truth. A 3-dimensional deep learning model was developed for GTV contouring in the training cohort and internally and externally validated in 3 validation cohorts. The AI tool was compared against 12 board-certified experts in 25 patients randomly selected from the external validation cohort to evaluate its assistance in improving contouring performance and reducing variation. Contouring performance was measured using dice similarity coefficient (DSC) and average surface distance. Additionally, our previously established radiomics model for predicting pathologic complete response was used to compare AI-generated and ground truth contours, to assess the potential of the AI contouring tool in radiomics analysis. Results The AI tool demonstrated good GTV contouring performance in multicenter validation cohorts, with median DSC values of 0.865, 0.876, and 0.866 and median average surface distance values of 0.939, 0.789, and 0.875 mm, respectively. Furthermore, the AI tool significantly improved contouring performance for half of 12 board-certified experts (DSC values, 0.794-0.835 vs 0.856-0.881, P = .003-0.048), reduced the intra- and interobserver variations by 37.4% and 55.2%, respectively, and saved contouring time by 77.6%. In the radiomics analysis, 88.7% of radiomic features from ground truth and AI-generated contours demonstrated stable reproducibility, and similar pathologic complete response prediction performance for these contours (P = .430) was observed. Conclusions Our AI contouring tool can improve GTV contouring performance and facilitate radiomics analysis in ESCC patients, which indicates its potential for GTV contouring during radiation therapy treatment planning and radiomics studies.",
        "Radiology narrative reports often describe characteristics of a patient's disease, including its location, size, and shape. Motivated by the recent success multimodal learning, we hypothesized that this descriptive text could guide medical image analysis algorithms. We proposed novel vision-language model, ConTEXTual Net, for task pneumothorax segmentation on chest radiographs. Net extracts language features from physician-generated free-form radiology using pre-trained model. then introduced cross-attention between intermediate embeddings an encoder-decoder convolutional neural network to enable guidance analysis. was trained CANDID-PTX dataset consisting 3196 positive cases with annotations 6 different physicians as well clinical reports. Using cross-validation, achieved Dice score 0.716\u00b10.016, which similar degree inter-reader variability (0.712\u00b10.044) computed subset data. It outperformed vision-only models (Swin UNETR: 0.670\u00b10.015, ResNet50 U-Net: 0.677\u00b10.015, GLoRIA: 0.686\u00b10.014, nnUNet 0.694\u00b10.016) competing model (LAVT: 0.706\u00b10.009). Ablation studies confirmed it information led performance gains. Additionally, show certain augmentation methods degraded Net's breaking image-text concordance. also evaluated effects activation functions in module, highlighting efficacy our chosen architectural design."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Trajectory Planning and Control for Differentially Flat Fixed-Wing\n  Aerial Systems",
        "SMART: Self-Aware Agent for Tool Overuse Mitigation",
        "The Power of Personality: A Human Simulation Perspective to Investigate\n  Large Language Model Agents",
        "Ethical Implications of AI in Data Collection: Balancing Innovation with\n  Privacy",
        "Anatomy of a Historic Blackout: Decoding Spatiotemporal Dynamics of\n  Power Outages and Disparities During Hurricane Beryl",
        "Optimization of the Qubit Coupled Cluster Ansatz on classical computers",
        "ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval",
        "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations",
        "Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs",
        "The Pitfalls of Using Lyman Alpha Damping Wings in High-z Galaxy Spectra\n  to Measure the Intergalactic Neutral Hydrogen Fraction",
        "Martingale Posteriors from Score Functions",
        "Integrating Inverse and Forward Modeling for Sparse Temporal Data from\n  Sensor Networks",
        "Semiclassical Mixmaster Universe",
        "ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition",
        "A holographic understanding of correlation and information",
        "Finite volume element method for Landau-Lifshitz equation",
        "Indoor Channel Characterization with Extremely Large Reconfigurable\n  Intelligent Surfaces at $300$ GHz",
        "TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential\n  Dynamics",
        "The coupling of polarization and oxygen vacancy migration in\n  ferroelectric Hf0.5Zr0.5O2 thin films enables electrically controlled thermal\n  memories above room temperature",
        "Hierarchical Autoscaling for Large Language Model Serving with Chiron",
        "Acquisition through My Eyes and Steps: A Joint Predictive Agent Model in\n  Egocentric Worlds",
        "PLayer-FL: A Principled Approach to Personalized Layer-wise Cross-Silo\n  Federated Learning",
        "Ontological analysis of proactive life event services",
        "NablAFx: A Framework for Differentiable Black-box and Gray-box Modeling\n  of Audio Effects",
        "Layered Nonlinear Model Predictive Control for Robust Stabilization of\n  Hybrid Systems",
        "Linear quadratic control of parabolic-like evolutions with memory of the\n  inputs",
        "A classical model for semiclassical state-counting",
        "Gelfand-Shilov spaces for extended Gevrey regularity"
      ],
      "abstract":[
        "Efficient real-time trajectory planning and control for fixed-wing unmanned\naerial vehicles is challenging due to their non-holonomic nature, complex\ndynamics, and the additional uncertainties introduced by unknown aerodynamic\neffects. In this paper, we present a fast and efficient real-time trajectory\nplanning and control approach for fixed-wing unmanned aerial vehicles,\nleveraging the differential flatness property of fixed-wing aircraft in\ncoordinated flight conditions to generate dynamically feasible trajectories.\nThe approach provides the ability to continuously replan trajectories, which we\nshow is useful to dynamically account for the curvature constraint as the\naircraft advances along its path. Extensive simulations and real-world\nexperiments validate our approach, showcasing its effectiveness in generating\ntrajectories even in challenging conditions for small FW such as wind\ndisturbances.",
        "Current Large Language Model (LLM) agents demonstrate strong reasoning and\ntool use capabilities, but often lack self-awareness, failing to balance these\napproaches effectively. This imbalance leads to Tool Overuse, where models\nunnecessarily rely on external tools for tasks solvable with parametric\nknowledge, increasing computational overhead. Inspired by human metacognition,\nwe introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm\nthat enhances an agent's self-awareness to optimize task handling and reduce\ntool overuse. To support this paradigm, we introduce SMART-ER, a dataset\nspanning three domains, where reasoning alternates between parametric knowledge\nand tool-dependent steps, with each step enriched by rationales explaining when\ntools are necessary. Through supervised training, we develop SMARTAgent, a\nfamily of models that dynamically balance parametric knowledge and tool use.\nEvaluations show that SMARTAgent reduces tool use by 24% while improving\nperformance by over 37%, enabling 7B-scale models to match its 70B counterpart\nand GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test\ndata like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool\ncalls. These highlight the potential of strategic tool use to enhance\nreasoning, mitigate overuse, and bridge the gap between model size and\nperformance, advancing intelligent and resource-efficient agent designs.",
        "Large language models (LLMs) excel in both closed tasks (including\nproblem-solving, and code generation) and open tasks (including creative\nwriting), yet existing explanations for their capabilities lack connections to\nreal-world human intelligence. To fill this gap, this paper systematically\ninvestigates LLM intelligence through the lens of ``human simulation'',\naddressing three core questions: (1) How do personality traits affect\nproblem-solving in closed tasks? (2) How do traits shape creativity in open\ntasks? (3) How does single-agent performance influence multi-agent\ncollaboration? By assigning Big Five personality traits to LLM agents and\nevaluating their performance in single- and multi-agent settings, we reveal\nthat specific traits significantly influence reasoning accuracy (closed tasks)\nand creative output (open tasks). Furthermore, multi-agent systems exhibit\ncollective intelligence distinct from individual capabilities, driven by\ndistinguishing combinations of personalities. We demonstrate that LLMs\ninherently simulate human behavior through next-token prediction, mirroring\nhuman language, decision-making, and collaborative dynamics.",
        "This article examines the ethical and legal implications of artificial\nintelligence (AI) driven data collection, focusing on developments from 2023 to\n2024. It analyzes recent advancements in AI technologies and their impact on\ndata collection practices across various sectors. The study compares regulatory\napproaches in the European Union, the United States, and China, highlighting\nthe challenges in creating a globally harmonized framework for AI governance.\nKey ethical issues, including informed consent, algorithmic bias, and privacy\nprotection, are critically assessed in the context of increasingly\nsophisticated AI systems. The research explores case studies in healthcare,\nfinance, and smart cities to illustrate the practical challenges of AI\nimplementation. It evaluates the effectiveness of current legal frameworks and\nproposes solutions encompassing legal and policy recommendations, technical\nsafeguards, and ethical frameworks. The article emphasizes the need for\nadaptive governance and international cooperation to address the global nature\nof AI development while balancing innovation with the protection of individual\nrights and societal values.",
        "This study investigates the spatial patterns and temporal variations in\noutage duration, intensity, and restoration\/recovery following the 2024\nHurricane Beryl in Houston, Texas. This historic blackout caused widespread\npower disruptions across the Houston metropolitan area, leaving more than 2\nmillion customers without power over several days, resulting in more than 143\nmillion total customer-out hours.The findings reveal that areas with higher\npopulation density and proximity to the hurricane's path experienced more\nsevere initial impacts. Regions with higher median income showed faster\nrecovery, while lower-income areas exhibited prolonged restoration periods,\neven with favorable infrastructural conditions, suggesting disparities in\nrestoration speed. The study also highlights how urban development features,\nsuch as road density and land elevation, explain spatial disparities in power\noutage impacts and recovery. This research advances the understanding of power\noutage dynamics in large metropolitan regions through four key contributions:\n(1) empirical characterization of outages from a historic hurricane,\nhighlighting infrastructure vulnerabilities in a high-density urban context;\n(2) comprehensive analysis using multiple metrics to capture spatiotemporal\ndynamics of outages and restoration; (3) leveraging of high-resolution outage\ndata at fine geographic scales and frequent intervals to quantify and reveal\npreviously masked spatial disparities; and (4) systematic examination of\nsocioeconomic, urban development, and environmental factors in shaping\ndisparities in outage impacts and recovery timelines. These findings provide\ninfrastructure managers, operators, utilities, and decision-makers with crucial\nempirical insights to quantify power outage impacts, justify resilience\ninvestments, and address vulnerability and equity issues in the power\ninfrastructure during hazard events.",
        "Immense interest in quantum computing has prompted development of electronic\nstructure methods that are suitable for quantum hardware. However, the slow\npace at which quantum hardware progresses, forces researchers to implement\ntheir ideas on classical computers despite the obvious loss of any \"quantum\nadvantage.\" As a result, the so-called quantum inspired methods emerge. They\nallow one to look at the electronic structure problem from a different angle;\nyet, to fully exploit their capacity, efficient implementations are highly\ndesirable. Here we report two schemes for improving the amplitude optimisation\nin the iterative qubit coupled cluster (iQCC) method -- a variational quantum\neigensolver-type approach which is based on the qubit coupled cluster (QCC)\nAnsatz. Our first scheme approximates the QCC unitary as a sum of symmetrical\npolynomials of generators up to a given order. The resulting energy expression\nallows for a flexible control of computational complexity via the order\nparameter. It also guaranties smoothness of trial energies and their\nderivatives, which is important for gradient-based optimization strategies. The\nsecond scheme limits the size of the expansion space in which the QCC unitary\nis generated. It provides better control of memory requirements, but in general\nmay lead to the non-smooth variation of energy estimates upon changes in\namplitudes. It can be used to extrapolate energies for a given set of\namplitudes towards the exact QCC value. Both schemes allow for a larger number\nof generators to be included into the QCC form compared to the exact\nformulation. This reduces the number of iterations in the iQCC method and\/or\nleads to higher accuracy. We assess capabilities of the new schemes to perform\nQCC amplitudes optimization for a few molecular systems: N$_2$ (16 qubits),\nH$_2$O (36 qubits), and tris(2-(2,4-difluorophenyl)pyridine) iridium(III), (80\nqubits).",
        "The objective in this paper is to improve the performance of text-to-image\nretrieval. To this end, we introduce a new framework that can boost the\nperformance of large-scale pre-trained vision-language models, so that they can\nbe used for text-to-image re-ranking. The approach, Enhanced Language-Image\nPre-training (ELIP), uses the text query to predict a set of visual prompts to\ncondition the ViT image encoding. ELIP can easily be applied to the commonly\nused CLIP\/SigLIP and the state-of-the-art BLIP-2 architectures. To train the\narchitecture with limited computing resources, we develop a 'student friendly'\nbest practice involving global hard sample mining, and selection and curation\nof a large-scale dataset. On the evaluation side, we set up two new\nout-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess the\nzero-shot generalisation of the models to different domains. Benefiting from\nthe novel architecture and data curation, experiments show our enhanced network\nsignificantly boosts CLIP\/SigLIP performance and outperforms the\nstate-of-the-art BLIP-2 model on text-to-image retrieval.",
        "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.",
        "Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle\ncomplex tasks by progressively activating relevant pre-trained knowledge.\nHowever, it faces challenges in ensuring continual improvement and determining\na stopping criterion. In this paper, we investigate whether the relevant\nknowledge that contributes directly to solving the given question can be\nactivated from the initial reasoning path, thus circumventing the need for\niterative refinement. Our experiments reveal that increasing the diversity of\ninitial reasoning paths can achieve comparable or superior performance, a\nconcept we term \\textit{breadth reasoning}. However, existing breadth reasoning\napproaches, such as self-consistency, offer limited diversity. To address this\nlimitation, we propose a simple yet effective method that enhances reasoning\nbreadth by integrating contextual exploration with reduced sampling randomness.\nExtensive experiments demonstrate that our approach significantly outperforms\ndeep iterative reasoning. Our code is provided in\nhttps:\/\/github.com\/zongqianwu\/breadth.",
        "The Lyman Alpha damping wing is seen in absorption against the spectra of\nhigh-redshift galaxies. Modeling of this wing is a way to measure the volume\nneutral hydrogen fraction of the Universe, as well as to measure the strength\nof HI absorption in the form of a damped Ly{\\alpha} absorber (DLA). We use very\nhigh-quality ultraviolet spectra of low-z galaxies to create mock\nhigh-redshift, low-resolution spectra complete with a damping wing resulting\nfrom the IGM and HI absorption from local, dense gas to mimic current James\nWebb Space Telescope (JWST) observations of the early universe. The simulated\nspectra are used to test the ability to recover the \"true\" values of the column\ndensity of the DLA (N_DLA) and the volume neutral hydrogen fraction (x_HI) of\nthe IGM through modified stellar continuum fitting. We find that the ability to\nrecover the true values of N_DLA and x_HI simultaneously is compromised by\ndegeneracies between these two parameters, and the spectral shape itself at\nlow-resolution (R~100). The root mean squared error between the recovered and\ntrue column density of the DLAs is on the order of 1 dex. This error somewhat\ndecreases with improved resolution (R~1000), but systematically underestimates\nN_DLA when Ly{\\alpha} emission is present in the mock spectra. The recovered\nvalues of x_HI are poorly constrained and do not improve substantially with the\nhigher resolution. We recommend accounting for these sources of uncertainty and\nbiases when using galaxies' Ly{\\alpha} damping wings to measure the\nintergalactic neutral hydrogen fraction.",
        "Uncertainty associated with statistical problems arises due to what has not\nbeen seen as opposed to what has been seen. Using probability to quantify the\nuncertainty the task is to construct a probability model for what has not been\nseen conditional on what has been seen. The traditional Bayesian approach is to\nuse prior distributions for constructing the predictive distributions, though\nrecently a novel approach has used density estimators and the use of\nmartingales to establish convergence of parameter values. In this paper we\nreply on martingales constructed using score functions. Hence, the method only\nrequires the computing of gradients arising from parametric families of density\nfunctions. A key point is that we do not rely on Markov Chain Monte Carlo\n(MCMC) algorithms, and that the method can be implemented in parallel. We\npresent the theoretical properties of the score driven martingale posterior.\nFurther, we present illustrations under different models and settings.",
        "We present CavePerception, a framework for the analysis of sparse data from\nsensor networks that incorporates elements of inverse modeling and forward\nmodeling. By integrating machine learning with physical modeling in a\nhypotheses space, we aim to improve the interpretability of sparse, noisy, and\npotentially incomplete sensor data. The framework assumes data from a\ntwo-dimensional sensor network laid out in a graph structure that detects\ncertain objects, with certain motion patterns. Examples of such sensors are\nmagnetometers. Given knowledge about the objects and the way they act on the\nsensors, one can develop a data generator that produces data from simulated\nmotions of the objects across the sensor field. The framework uses the\nsimulated data to infer object behaviors across the sensor network. The\napproach is experimentally tested on real-world data, where magnetometers are\nused on an airport to detect and identify aircraft motions. Experiments\ndemonstrate the value of integrating inverse and forward modeling, enabling\nintelligent systems to better understand and predict complex, sensor-driven\nevents.",
        "We present a semiclassical study of the Mixmaster cosmology minimally coupled\nto a massive scalar field in the Hamiltonian formalism, with focus on three\ndistinct scenarios: the classical cosmology coupled to the quantized scalar\nfield, and \"effective\" cosmology, with spacetime discreteness corrections,\ncoupled to the classical scalar field, and to the quantized scalar field. We\nfind several results: (i) the effective cosmology undergoes several small\nbounces before expanding, with scalar field excitations rising through the\nbounce; (ii) anisotropies rise and fall as the universe undergoes a bounce, a\nfeature that is enhanced with matter; (iii) Lyapunov exponents reveal that\nchaos is reduced in the effective systems compared to the classical case.",
        "Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major\/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.",
        "The status of the two-point connected correlator is holographically probed by\nusing the well-known prescription of holographic mutual information. In\nparticular, we calculate the two-point connected correlator for a specific bulk\nspacetime geometry which was not done explicitly earlier. We then carefully\nstudy the inequality between the mutual information and the two-point connected\ncorrelator, namely,\n$I(A:B)\\ge\\frac{(\\expval{\\mathcal{O}_{A}\\mathcal{O}_{B}}-\\expval{\\mathcal{O}_{A}}\\expval{\\mathcal{O}_{B}})^2}{2\\expval{\\mathcal{O}_{A}^2}\\expval{\\mathcal{O}_{B}^2}}$.\nWe observe that by considering the standard set up of two strip-like subsystems\nwhich are separated by a distance, one can show that there exists two critical\nseparation distances, namely, $sT|_c$ and $sT|_I$ depicting the vanishing of\nquantum dependencies and classical dependencies between the subsystems\nrespectively. We also make a proposal in this context.",
        "The Landau-Lifshitz equation describes the dynamics of magnetization in\nferromagnetic materials. Due to the essential nonlinearity and nonconvex\nconstraint, it is typically solved numerically. In this paper, we developed a\nfinite volume element method (FVEM) with the Gauss-Seidel projection method\n(GSPM) for the micromagnetics simulations. We give the error estimate for FVEM\nin space and depict the discretized energy dissipation. Owing to the\napplication of the GSPM, the nonlinear vector system is decoupled and the\ncomputational complexity is comparable to that of implicitly solving the scalar\nheat equation, which accelerates the real simulations significantly. We present\nseveral numerical experiments to verify the theoretical analysis. Furthermore,\nwe study the blow-up solution and efficiently simulate the 2D magnetic textures\nusing the proposed method.",
        "The technology of Reconfigurable Intelligent Surfaces (RISs) is lately being\nconsidered as a boosting component for various indoor wireless applications,\nenabling wave propagation control and coverage extension. However, the\nincorporation of extremely large RISs, as recently being considered for\nultra-high capacity industrial environments at subTHz frequencies, imposes\ncertain challenges for indoor channel characterization. In particular, such\nRISs contribute additional multipath components and their large sizes with\nrespect to the signal wavelength lead to near-field propagation. To this end,\nray tracing approaches become quite cumbersome and need to be rerun for\ndifferent RIS unit cell designs. In this paper, we present a novel approach for\nthe incorporation of RISs in indoor multipath environments towards their\nefficient channel characterization. An $100\\times100$ RIS design with $2$-bit\nresolution unit cells realizing a fixed anomalous reflection at 300 GHz is\npresented, whose radar cross section patterns are obtained via full-wave\nsimulations. It is showcased that the RIS behavior can be conveniently\napproximated by a three-ray model, which can be efficiently incorporated within\navailable ray tracing tools, and that the far-field approximation is valid for\neven very small distances from the RIS.",
        "Future link prediction is a fundamental challenge in various real-world\ndynamic systems. To address this, numerous temporal graph neural networks\n(temporal GNNs) and benchmark datasets have been developed. However, these\ndatasets often feature excessive repeated edges and lack complex sequential\ndynamics, a key characteristic inherent in many real-world applications such as\nrecommender systems and ``Who-To-Follow'' on social networks. This oversight\nhas led existing methods to inadvertently downplay the importance of learning\nsequential dynamics, focusing primarily on predicting repeated edges.\n  In this study, we demonstrate that existing methods, such as GraphMixer and\nDyGFormer, are inherently incapable of learning simple sequential dynamics,\nsuch as ``a user who has followed OpenAI and Anthropic is more likely to follow\nAI at Meta next.'' Motivated by this issue, we introduce the Temporal Graph\nBenchmark with Sequential Dynamics (TGB-Seq), a new benchmark carefully curated\nto minimize repeated edges, challenging models to learn sequential dynamics and\ngeneralize to unseen edges. TGB-Seq comprises large real-world datasets\nspanning diverse domains, including e-commerce interactions, movie ratings,\nbusiness reviews, social networks, citation networks and web link networks.\nBenchmarking experiments reveal that current methods usually suffer significant\nperformance degradation and incur substantial training costs on TGB-Seq, posing\nnew challenges and opportunities for future research. TGB-Seq datasets,\nleaderboards, and example codes are available at https:\/\/tgb-seq.github.io\/.",
        "Here we investigate epitaxial Hf0.5Zr0.5O2 ferroelectric thin films as\npotential candidates to be used as non-volatile electric-field-modulated\nthermal memories. The electric-field dependence of the thermal conductivity of\nmetal\/Hf0.5Zr0.5O2\/YSZ devices is found to be hysteretic, resembling the\npolarization vs electric field hysteresis loops, being maximum (minimum) at\nlarge applied positive (negative) voltages from the top metallic electrode.\nThis dynamic thermal response is compatible with the coupling between the\nferroelectric polarization and the oxygen ion migration, in which the oxygen\nvacancies are the main phonon scattering sources and the polarization acts as\nan electrically active ion migration barrier that creates the hysteresis. This\nnew mechanism enables two non-volatile thermal states: high (ON) and low (OFF)\nthermal conductivity, with an ON\/OFF ratio of 1.6. Both the ON and OFF states\nexhibit high stability over time, though the switching speed is limited by ion\nmobility in the YSZ electrode.",
        "Large language model (LLM) serving is becoming an increasingly important\nworkload for cloud providers. Based on performance SLO requirements, LLM\ninference requests can be divided into (a) interactive requests that have tight\nSLOs in the order of seconds, and (b) batch requests that have relaxed SLO in\nthe order of minutes to hours. These SLOs can degrade based on the arrival\nrates, multiplexing, and configuration parameters, thus necessitating the use\nof resource autoscaling on serving instances and their batch sizes. However,\nprevious autoscalers for LLM serving do not consider request SLOs leading to\nunnecessary scaling and resource under-utilization. To address these\nlimitations, we introduce Chiron, an autoscaler that uses the idea of\nhierarchical backpressure estimated using queue size, utilization, and SLOs.\nOur experiments show that Chiron achieves up to 90% higher SLO attainment and\nimproves GPU efficiency by up to 70% compared to existing solutions.",
        "This paper addresses the task of learning an agent model behaving like\nhumans, which can jointly perceive, predict, and act in egocentric worlds.\nPrevious methods usually train separate models for these three abilities,\nleading to information silos among them, which prevents these abilities from\nlearning from each other and collaborating effectively. In this paper, we\npropose a joint predictive agent model, named EgoAgent, that simultaneously\nlearns to represent the world, predict future states, and take reasonable\nactions with a single transformer. EgoAgent unifies the representational spaces\nof the three abilities by mapping them all into a sequence of continuous\ntokens. Learnable query tokens are appended to obtain current states, future\nstates, and next actions. With joint supervision, our agent model establishes\nthe internal relationship among these three abilities and effectively mimics\nthe human inference and learning processes. Comprehensive evaluations of\nEgoAgent covering image classification, egocentric future state prediction, and\n3D human motion prediction tasks demonstrate the superiority of our method. The\ncode and trained model will be released for reproducibility.",
        "Non-identically distributed data is a major challenge in Federated Learning\n(FL). Personalized FL tackles this by balancing local model adaptation with\nglobal model consistency. One variant, partial FL, leverages the observation\nthat early layers learn more transferable features by federating only early\nlayers. However, current partial FL approaches use predetermined,\narchitecture-specific rules to select layers, limiting their applicability. We\nintroduce Principled Layer-wise-FL (PLayer-FL), which uses a novel federation\nsensitivity metric to identify layers that benefit from federation. This\nmetric, inspired by model pruning, quantifies each layer's contribution to\ncross-client generalization after the first training epoch, identifying a\ntransition point in the network where the benefits of federation diminish. We\nfirst demonstrate that our federation sensitivity metric shows strong\ncorrelation with established generalization measures across diverse\narchitectures. Next, we show that PLayer-FL outperforms existing FL algorithms\non a range of tasks, also achieving more uniform performance improvements\nacross clients.",
        "Life event service is a direct digital public service provided jointly by\nseveral governmental institutions so that a person can fulfill all the\nobligations and use all the rights that arise due to a particular event or\nsituation in personal life. Life event service consolidates several public\nservices related to the same life event into one service for the service\nconsumer. This paper presents an ontological analysis of life event services,\nwhich is based on the works by Guarino, Guizzardi, Nardi, Wagner, and others.\nThe purpose of the ontological analysis is to understand the meanings of life\nevent, proactive public service based on life event, and other related notions.\nThis kind of ontological analysis is crucial because for implementing the\nhardware and software architectures of e-government and digital public\nservices, it is essential to agree upon the precise meanings of the underlying\nterms.",
        "We present NablAFx, an open-source framework developed to support research in\ndifferentiable black-box and gray-box modeling of audio effects. Built in\nPyTorch, NablAFx offers a versatile ecosystem to configure, train, evaluate,\nand compare various architectural approaches. It includes classes to manage\nmodel architectures, datasets, and training, along with features to compute and\nlog losses, metrics and media, and plotting functions to facilitate detailed\nanalysis. It incorporates implementations of established black-box\narchitectures and conditioning methods, as well as differentiable DSP blocks\nand controllers, enabling the creation of both parametric and non-parametric\ngray-box signal chains. The code is accessible at\nhttps:\/\/github.com\/mcomunita\/nablafx.",
        "Computing the receding horizon optimal control of nonlinear hybrid systems is\ntypically prohibitively slow, limiting real-time implementation. To address\nthis challenge, we propose a layered Model Predictive Control (MPC)\narchitecture for robust stabilization of hybrid systems. A high level \"hybrid\"\nMPC is solved at a slow rate to produce a stabilizing hybrid trajectory,\npotentially sub-optimally, including a domain and guard sequence. This domain\nand guard sequence is passed to a low level \"fixed mode\" MPC which is a\ntraditional, time-varying, state-constrained MPC that can be solved rapidly,\ne.g., using nonlinear programming (NLP) tools. A robust version of the fixed\nmode MPC is constructed by using tracking error tubes that are not guaranteed\nto have finite size for all time. Using these tubes, we demonstrate that the\nspeed at which the fixed mode MPC is re-calculated is directly tied to the\nrobustness of the system, thereby justifying the layered approach. Finally,\nsimulation examples of a five link bipedal robot and a controlled nonlinear\nbouncing ball are used to illustrate the formal results.",
        "A study of the linear quadratic (LQ) control problem on a finite time\ninterval for a model equation in Hilbert spaces which comprehends the memory of\nthe inputs was performed recently by the authors. The outcome included a\nclosed-loop representation of the unique optimal control, along with the\nderivation of a related coupled system of three quadratic (operator) equations\nwhich is shown to be well-posed. Notably, in the absence of memory the above\nelements -- namely, formula and system -- reduce to the known feedback formula\nand single differential Riccati equation, respectively. In this work we take\nthe next natural step, and prove the said results for a class of evolutions\nwhere the control operator is no longer bounded. These findings appear to be\nthe first ones of their kind; furthermore, they extend the classical theory of\nthe LQ problem and Riccati equations for parabolic partial differential\nequations.",
        "In the type II von Neumann algebras that appear in semiclassical gravity, all\nstates have infinite entropy, but entropy differences are uniquely defined.\nAkers and I have shown that the entropy difference of microcanonical states has\na relative state-counting interpretation in terms of the additional (finite)\nnumber of degrees of freedom that are needed to represent the \"larger-entropy\"\nstate supposing that one already has a representation of the \"smaller-entropy\"\nstate, and supposing that one is restricted to act with gauge-invariant\noperators. This short paper explains some of the curious features of relative\nstate-counting by analogy to the classical limit of quantum statistical\nmechanics. In this analogy the preferred family of renormalized traces becomes\nthe preferred family of symplectic measures on phase space; the trace-index of\ninfinite-dimensional subspaces becomes the ratio of phase space volumes; and\nthe restriction that one must act with gauge-invariant operators becomes the\nrestriction that one must act with symplectomorphisms. Because in the\nphase-space analogy one has exact control over the quantum deformation away\nfrom the classical theory, one can see precisely how the relevant aspects of\nthe classical structure are inherited from the quantum theory -- though even in\nthis simple setting, it is a nontrivial technical task to show how classical\nsymplectomorphisms emerge from the underlying quantum theory in the $\\hbar \\to\n0$ limit.",
        "We consider spaces of smooth functions obtained by relaxing Gevrey-type\nregularity and decay conditions. It is shown that these classes fit well within\nthe general framework of the weighted matrices approach to ultradifferentiable\nfunctions. We examine equivalent ways of introducing Gelfand-Shilov spaces\nrelated to the extended Gevrey regularity and derive their nuclearity. In\naddition to the Fourier transform invariance property, we present their\ncorresponding symmetric characterizations. Finally, we consider some\ntime-frequency representations of the introduced classes of ultradifferentiable\nfunctions."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Rsna pneumonia detection challenge",
    "start_abstract":"In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.",
    "start_categories":[
      "Imaging"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b14",
        "b9"
      ],
      "title":[
        "Adding Conditional Control to Text-to-Image Diffusion Models",
        "Highresolution image synthesis with latent diffusion models"
      ],
      "abstract":[
        "We present ControlNet, a neural network architecture to add spatial conditioning controls large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large models, and reuses their deep robust encoding layers with billions of images as strong backbone learn diverse set conditional controls. The is connected \"zero convolutions\" (zero-initialized convolution layers) that progressively grow parameters from zero ensure no harmful noise could affect finetuning. test various controls, e.g., edges, depth, segmentation, human pose, etc., Stable Diffusion, using single or multiple conditions, without prompts. show training ControlNets small (<50k) (>1m) datasets. Extensive results may facilitate wider applications control image",
        "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "On Synthetic Data Strategies for Domain-Specific Generative Retrieval",
        "Preference-aware compensation policies for crowdsourced on-demand\n  services",
        "Two-Stage Distributed Beamforming Design in Cell-Free Massive MIMO ISAC\n  Systems",
        "GDformer: Going Beyond Subsequence Isolation for Multivariate Time\n  Series Anomaly Detection",
        "Universal thermodynamic topological classes of black holes in perfect\n  fluid dark matter background",
        "O-MMGP: Optimal Mesh Morphing Gaussian Process Regression for Solving\n  PDEs with non-Parametric Geometric Variations",
        "Connector-S: A Survey of Connectors in Multi-modal Large Language Models",
        "Nested subspace learning with flags",
        "Properties of 'Lite' Intermediate-Mass Black Hole Candidates in\n  LIGO-Virgo's Third Observing Run",
        "Neural Graph Matching Improves Retrieval Augmented Generation in\n  Molecular Machine Learning",
        "Unveiling the Infrared Excess of SIPS J2045-6332: Evidence for a Young\n  Stellar Object with Potential Low-Mass Companion",
        "Variational decision diagrams for quantum-inspired machine learning\n  applications",
        "CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with\n  Limited Memory",
        "Bayesian correction of $H(z)$ cosmic chronometers data with systematic\n  errors",
        "On defect in finite extensions of valued fields",
        "Hyperbolic Handlebody Complements in 3-Manifolds",
        "BSODiag: A Global Diagnosis Framework for Batch Servers Outage in\n  Large-scale Cloud Infrastructure Systems",
        "Polynomial-time algorithms in algebraic number theory",
        "Page Time of Primordial Black Holes in the Standard Model and Beyond",
        "Universal Adversarial Attack on Aligned Multimodal LLMs",
        "Adoption of Watermarking for Generative AI Systems in Practice and\n  Implications under the new EU AI Act",
        "Defect Detection Network In PCB Circuit Devices Based on GAN Enhanced\n  YOLOv11",
        "Interplay of Electrostatic Interaction and Steric Repulsion between\n  Bacteria and Gold Surface Influences Raman Enhancement",
        "Understanding the Impact of Artificial Intelligence in Academic Writing:\n  Metadata to the Rescue",
        "Phase-space Generalized Brillouin Zone for spatially inhomogeneous\n  non-Hermitian systems",
        "Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void\n  Filling",
        "Synthesis of Model Predictive Control and Reinforcement Learning: Survey\n  and Classification",
        "A note on Arveson's hyperrigidity and non-degenerate C*-correspondences"
      ],
      "abstract":[
        "This paper investigates synthetic data generation strategies in developing\ngenerative retrieval models for domain-specific corpora, thereby addressing the\nscalability challenges inherent in manually annotating in-domain queries. We\nstudy the data strategies for a two-stage training framework: in the first\nstage, which focuses on learning to decode document identifiers from queries,\nwe investigate LLM-generated queries across multiple granularity (e.g. chunks,\nsentences) and domain-relevant search constraints that can better capture\nnuanced relevancy signals. In the second stage, which aims to refine document\nranking through preference learning, we explore the strategies for mining hard\nnegatives based on the initial model's predictions. Experiments on public\ndatasets over diverse domains demonstrate the effectiveness of our synthetic\ndata generation and hard negative sampling approach.",
        "Crowdsourced on-demand services offer benefits such as reduced costs, faster\nservice fulfillment times, greater adaptability, and contributions to\nsustainable urban transportation in on-demand delivery contexts. However, the\nsuccess of an on-demand platform that utilizes crowdsourcing relies on finding\na compensation policy that strikes a balance between creating attractive offers\nfor gig workers and ensuring profitability. In this work, we examine a dynamic\npricing problem for an on-demand platform that sets request-specific\ncompensation of gig workers in a discrete-time framework, where requests and\nworkers arrive stochastically. The operator's goal is to determine a\ncompensation policy that maximizes the total expected reward over the time\nhorizon. Our approach introduces compensation strategies that explicitly\naccount for gig worker request preferences. To achieve this, we employ the\nMultinomial Logit model to represent the acceptance probabilities of gig\nworkers, and, as a result, derive an analytical solution that utilizes\npost-decision states. Subsequently, we integrate this solution into an\napproximate dynamic programming algorithm. We compare our algorithm against\nbenchmark algorithms, including formula-based policies and an upper bound\nprovided by the full information linear programming solution. Our algorithm\ndemonstrates consistent performance across diverse settings, achieving\nimprovements of at least 2.5-7.5% in homogeneous gig worker populations and 9%\nin heterogeneous populations over benchmarks, based on fully synthetic data.\nFor real-world data, it surpasses benchmarks by 8% in weak and 20% in strong\nlocation preference scenarios.",
        "Integrating radio-sensing functionalities into future cell-free (CF) wireless\nnetworks promises efficient resource utilization and facilitates the seamless\nroll-out of applications such as public safety and smart infrastructure. While\nthe beamforming design problem for the CF integrated sensing and communication\n(ISAC) paradigm has been addressed in the literature, existing methods rely on\ncentralized signal processing, leading to fronthaul load and scalability\nissues. This paper presents a two-stage beamforming design for the CF ISAC\nparadigm, aiming to significantly reduce the fronthaul load by distributing the\nsignal processing tasks between the central unit (CU) and the access points\n(APs). The design optimizes the sum signal-to-interference-plus-noise ratio\n(SINR) for communication users, subject to per-AP power constraints and\nsignal-to-noise ratio (SNR) requirements for radio-sensing purposes. The\nresulting optimization problems are non-convex and challenging to solve. To\naddress this, we employ a majorization-minimization (MM) approach, which\ndecomposes the problem into simpler convex subproblems. The results show that\nthe two-stage beamforming design achieves performance comparable to centralized\nmethods while substantially reducing the fronthaul load, thus minimizing data\ntransmission requirements over the fronthaul network. This advancement\nhighlights the potential of the proposed method to enhance the efficiency and\nscalability of cell-free MIMO ISAC systems.",
        "Unsupervised anomaly detection of multivariate time series is a challenging\ntask, given the requirements of deriving a compact detection criterion without\naccessing the anomaly points. The existing methods are mainly based on\nreconstruction error or association divergence, which are both confined to\nisolated subsequences with limited horizons, hardly promising unified\nseries-level criterion. In this paper, we propose the Global\nDictionary-enhanced Transformer (GDformer) with a renovated dictionary-based\ncross attention mechanism to cultivate the global representations shared by all\nnormal points in the entire series. Accordingly, the cross-attention maps\nreflect the correlation weights between the point and global representations,\nwhich naturally leads to the representation-wise similarity-based detection\ncriterion. To foster more compact detection boundary, prototypes are introduced\nto capture the distribution of normal point-global correlation weights.\nGDformer consistently achieves state-of-the-art unsupervised anomaly detection\nperformance on five real-world benchmark datasets. Further experiments validate\nthe global dictionary has great transferability among various datasets. The\ncode is available at https:\/\/github.com\/yuppielqx\/GDformer.",
        "In this paper, we study the universal thermodynamic topological classes of a\nfamily of black holes in a perfect fluid dark matter (PFDM) background. Recent\nresearch on black hole thermodynamics suggests that all black holes can be\nclassified into four universal thermodynamic classes, denoted by $W^{1-}$,\n$W^{0+}$, $W^{0-}$, and $W^{1+}$. Our study reveals that the Schwarzschild\nblack hole in PFDM belongs to the $W^{1-}$ class, and independence of black\nhole size thermodynamically unstable at both low- and high-temperature limits.\nThe Reissner-Nordstr\\\"om, Kerr, and Kerr-Newman black holes in the PFDM\nbackground belong to the same universal thermodynamic class, $W^{0+}$, which\nrepresents small, stable black holes and large, unstable black holes at\nlow-temperature limits, whereas no black hole state exists at high\ntemperatures. The AdS black holes behave differently compared to their\ncounterparts in PFDM. The Schwarzschild-AdS black hole belongs to the $W^{0-}$\nclass, indicating no black hole state at low temperatures, but small, unstable\nand large, stable black hole states at high temperatures. Furthermore, the\nKerr-AdS black hole belongs to the $W^{1+}$ class, characterized by small,\nstable black holes at low temperatures, large, stable black holes at high\ntemperatures, and unstable, intermediate-sized black holes at both low and high\ntemperatures. These findings uncover the universal topological classifications\nunderlying black hole thermodynamics, offering profound insights into the\nfundamental principles of quantum gravity.",
        "We address the computational challenges of solving parametric PDEs with non\nparametrized geometric variations and non-reducible problems, such as those\ninvolving shocks and discontinuities of variable positions. Traditional\ndimensionality reduction methods like POD struggle with these scenarios due to\nslowly decaying Kolmogorov widths. To overcome this, we propose a novel\nnon-linear dimensionality reduction technique to reduce the required modes for\nrepresentation. The non-linear reduction is obtained through a POD after\napplying a transformation on the fields, which we call optimal mappings, and is\na solution to an optimization problem in infinite dimension. The proposed\nlearning framework combines morphing techniques, non-linear dimensionality\nreduction, and Gaussian Process Regression (GPR). The problem is reformulated\non a reference geometry before applying the dimensionality reduction. Our\nmethod learns both the optimal mapping, and the solution fields, using a series\nof GPR models, enabling efficient and accurate modeling of complex parametric\nPDEs with geometrical variability. The results obtained concur with current\nstate-of-the-art models. We mainly compare our method with the winning solution\nof the ML4CFD NeurIPS 2024 competition.",
        "With the rapid advancements in multi-modal large language models (MLLMs),\nconnectors play a pivotal role in bridging diverse modalities and enhancing\nmodel performance. However, the design and evolution of connectors have not\nbeen comprehensively analyzed, leaving gaps in understanding how these\ncomponents function and hindering the development of more powerful connectors.\nIn this survey, we systematically review the current progress of connectors in\nMLLMs and present a structured taxonomy that categorizes connectors into atomic\noperations (mapping, compression, mixture of experts) and holistic designs\n(multi-layer, multi-encoder, multi-modal scenarios), highlighting their\ntechnical contributions and advancements. Furthermore, we discuss several\npromising research frontiers and challenges, including high-resolution input,\ndynamic compression, guide information selection, combination strategy, and\ninterpretability. This survey is intended to serve as a foundational reference\nand a clear roadmap for researchers, providing valuable insights into the\ndesign and optimization of next-generation connectors to enhance the\nperformance and adaptability of MLLMs.",
        "Many machine learning methods look for low-dimensional representations of the\ndata. The underlying subspace can be estimated by first choosing a dimension\n$q$ and then optimizing a certain objective function over the space of\n$q$-dimensional subspaces (the Grassmannian). Trying different $q$ yields in\ngeneral non-nested subspaces, which raises an important issue of consistency\nbetween the data representations. In this paper, we propose a simple trick to\nenforce nestedness in subspace learning methods. It consists in lifting\nGrassmannian optimization problems to flag manifolds (the space of nested\nsubspaces of increasing dimension) via nested projectors. We apply the flag\ntrick to several classical machine learning methods and show that it\nsuccessfully addresses the nestedness issue.",
        "Over a hundred gravitational-wave (GW) detections and candidates have been\nreported from the first three observing runs of the Advanced LIGO-Virgo-KAGRA\n(LVK) detectors. Among these, the most intriguing events are binary black hole\nmergers that result in a 'lite' intermediate-mass black hole (IMBH) of\n${\\sim}10^2~\\mathrm{M}_\\odot$, such as GW170502 and GW190521. In this study, we\ninvestigate 11 GW candidates from LVK's Third Observing Run (April 2019-March\n2020) that have a total detector-frame masses in the lite IMBH range. Using the\nBayesian inference algorithm \\texttt{RIFT}, we systematically analyze these\ncandidates with three state-of-the-art waveform models that incorporate higher\nharmonics, which are crucial for resolving lite IMBHs in LVK data. For each\ncandidate, we infer the pre-merger and post-merger black hole masses in the\nsource frame, along with black hole spin projections across all three models.\nUnder the assumption that these are binary black hole mergers, our analysis\nfinds that 5 of them have a post-merger lite IMBH with masses ranging from\n$110\\sim 350~\\mathrm{M}_\\odot$ with over 90\\% confidence interval.\nAdditionally, we note that one of their pre-merger black holes is within the\npair-instability supernova mass gap ($60-120~\\mathrm{M}_\\odot$) with more than\n90\\% confidence interval, and additional two pre-merger black holes above the\nmass-gap. Furthermore, we report discrepancies among the three waveform models\nin their mass and spin inferences of lite IMBHs, with at least three GW\ncandidates showing deviations beyond accepted statistical limits. While the\nastrophysical certainty of these candidates cannot be established, our study\nprovides a foundation to probe the lite IMBH population that emerge within the\nlow-frequency noise spectrum of LVK detectors.",
        "Molecular machine learning has gained popularity with the advancements of\ngeometric deep learning. In parallel, retrieval-augmented generation has become\na principled approach commonly used with language models. However, the optimal\nintegration of retrieval augmentation into molecular machine learning remains\nunclear. Graph neural networks stand to benefit from clever matching to\nunderstand the structural alignment of retrieved molecules to a query molecule.\nNeural graph matching offers a compelling solution by explicitly modeling node\nand edge affinities between two structural graphs while employing a\nnoise-robust, end-to-end neural network to learn affinity metrics. We apply\nthis approach to mass spectrum simulation and introduce MARASON, a novel model\nthat incorporates neural graph matching to enhance a fragmentation-based neural\nnetwork. Experimental results highlight the effectiveness of our design, with\nMARASON achieving 28% top-1 accuracy, a substantial improvement over the\nnon-retrieval state-of-the-art accuracy of 19%. Moreover, MARASON outperforms\nboth naive retrieval-augmented generation methods and traditional graph\nmatching approaches.",
        "The Disk Detective project, a citizen science initiative, aims to identify\ncircumstellar discs around stars by detecting objects with infrared (IR) excess\nusing data from the Wide-field Infrared Survey Explorer (WISE). In this study,\nwe investigate SIPS J2045-6332, a potential brown dwarf with significant IR\nexcess in WISE and 2MASS bands, initially identified by project volunteers.\nDespite early indicators of a circumstellar disc, discrepancies between\nobserved brightness and expected Spectral Energy Distribution (SED) models\nsuggested unusual properties. To explore potential explanations, we created SED\ntemplates for spectral types M9 to L4 and compared them with SIPS J2045-6332's\nphotometric data, revealing an excess brightness that points to either an\nunresolved low-mass companion or a young, inflated primary star. Further\nanalysis of infrared spectral features and surface gravity indicators supports\na youthful classification, estimating the object's age at 26-200 million years.\nObservations also suggest the presence of a mid L-type companion at a projected\ndistance of 6.7 AU. This study highlights SIPS J2045-6332 as an intriguing\nsystem with unique IR characteristics and recommends follow-up observations\nwith high-resolution telescopes to confirm the companion hypothesis and further\ncharacterize the system.",
        "Decision diagrams (DDs) have emerged as an efficient tool for simulating\nquantum circuits due to their capacity to exploit data redundancies in quantum\nstates and quantum operations, enabling the efficient computation of\nprobability amplitudes. However, their application in quantum machine learning\n(QML) has remained unexplored. This paper introduces variational decision\ndiagrams (VDDs), a novel graph structure that combines the structural benefits\nof DDs with the adaptability of variational methods for efficiently\nrepresenting quantum states. We investigate the trainability of VDDs by\napplying them to the ground state estimation problem for transverse-field Ising\nand Heisenberg Hamiltonians. Analysis of gradient variance suggests that\ntraining VDDs is possible, as no signs of vanishing gradients--also known as\nbarren plateaus--are observed. This work provides new insights into the use of\ndecision diagrams in QML as an alternative to design and train variational\nans\\\"atze.",
        "Large language models like GPT-4 are resource-intensive, but recent\nadvancements suggest that smaller, specialized experts can outperform the\nmonolithic models on specific tasks. The Collaboration-of-Experts (CoE)\napproach integrates multiple expert models, improving the accuracy of generated\nresults and offering great potential for precision-critical applications, such\nas automatic circuit board quality inspection. However, deploying CoE serving\nsystems presents challenges to memory capacity due to the large number of\nexperts required, which can lead to significant performance overhead from\nfrequent expert switching across different memory and storage tiers.\n  We propose CoServe, an efficient CoE model serving system on heterogeneous\nCPU and GPU with limited memory. CoServe reduces unnecessary expert switching\nby leveraging expert dependency, a key property of CoE inference. CoServe\nintroduces a dependency-aware request scheduler and dependency-aware expert\nmanagement for efficient inference. It also introduces an offline profiler to\nautomatically find optimal resource allocation on various processors and\ndevices. In real-world intelligent manufacturing workloads, CoServe achieves\n4.5$\\times$ to 12$\\times$ higher throughput compared to state-of-the-art\nsystems.",
        "We show that the 32 $H(z)$ data from cosmic chronometers have overestimated\nuncertainties and make use of a Bayesian method to correct and reduce it. We\nthen use the corrected data to constrain flat $\\Lambda$CDM and O$\\Lambda$CDM\nparameters. For the flat $\\Lambda$CDM model, we got as result $H_{0} = 67.1\\pm\n4.0$ km s$^{-1}$ Mpc$^{-1}$ and $\\Omega _{m} = 0.333 ^{+0.041}_{-0.057}$. While\nfor the O$\\Lambda$CDM model, we found $H_{0} = 67.2\\pm 4.8$ km s$^{-1}$\nMpc$^{-1}$, $\\Omega _{m} = 0.36\\pm 0.16$ and $\\Omega _{\\Lambda} =0.71\n^{+0.36}_{-0.28}$. These results goes from $22\\%$ up to $28\\%$ uncertainty\nreduction when compared to the constraints of the both uncorrected models.",
        "In recent decades, the defect of finite extensions of valued fields has\nemerged as the main obstacle in several fundamental problems in algebraic\ngeometry such as the local uniformization problem. Hence, it is important to\nidentify defectless fields and study properties related to defect. In this\npaper we study the relations between the following properties of valued fields:\nsimply defectless, immediate-defectless and algebraically maximal. The main\nresult of the paper is an example of an algebraically maximal field that admits\na simple defect extension. For this, we introduce the notion of quasi-finite\nelements in the generalized power series field\n$k\\left(\\left(t^\\Gamma\\right)\\right)$.",
        "Let $M_0$ be a compact and orientable 3-manifold. After capping off spherical\nboundaries with balls and removing any torus boundaries, we prove that the\nresulting manifold $M$ contains handlebodies of arbitrary genus such that the\nclosure of their complement is hyperbolic. We then extend the octahedral\ndecomposition to obtain bounds on volume for some of these handlebody\ncomplements.",
        "Cloud infrastructure is the collective term for all physical devices within\ncloud systems. Failures within the cloud infrastructure system can severely\ncompromise the stability and availability of cloud services. Particularly,\nbatch servers outage, which is the most fatal failure, could result in the\ncomplete unavailability of all upstream services. In this work, we focus on the\nbatch servers outage diagnosis problem, aiming to accurately and promptly\nanalyze the root cause of outages to facilitate troubleshooting. However, our\nempirical study conducted in a real industrial system indicates that it is a\nchallenging task. Firstly, the collected single-modal coarse-grained failure\nmonitoring data (i.e., alert, incident, or change) in the cloud infrastructure\nsystem is insufficient for a comprehensive failure profiling. Secondly, due to\nthe intricate dependencies among devices, outages are often the cumulative\nresult of multiple failures, but correlations between failures are difficult to\nascertain. To address these problems, we propose BSODiag, an unsupervised and\nlightweight diagnosis framework for batch servers outage. BSODiag provides a\nglobal analytical perspective, thoroughly explores failure information from\nmulti-source monitoring data, models the spatio-temporal correlations among\nfailures, and delivers accurate and interpretable diagnostic results.\nExperiments conducted on the Alibaba Cloud infrastructure system show that\nBSODiag achieves 87.5% PR@3 and 46.3% PCR, outperforming baseline methods by\n10.2% and 3.7%, respectively.",
        "This document contains notes based on lectures given by Hendrik Lenstra at\nthe PCMI summer school 2022.\n  There are many problems in algebraic number theory which one would like to\nsolve algorithmically, for example computation of the maximal order\n$\\mathcal{O}$ of a number field, and the many problems that are most often\nstated only for $\\mathcal{O}$, such as inverting ideals and unit computations.\nHowever, there is no known fast, i.e. polynomial-time, algorithm to compute\n$\\mathcal{O}$, which we motivate by a reduction to elementary number theory. We\nwill instead restrict to polynomial-time algorithms, and work around this\ninaccessibility of $\\mathcal{O}$.",
        "The Page time marks the moment when the von Neumann entropy of the emitted\nHawking radiation equals the Bekenstein-Hawking entropy of an evaporating black\nhole, which is assumed to quantify its degrees of freedom as seen from the\noutside. Beyond this point, from unitarity we would expect that the entropy of\nthe radiation begins to decrease, ensuring that information is eventually\nrecovered. In this work, we investigate the dependence of the Page time on\nblack hole properties and the particle content of nature. Specifically, we\nanalyze its sensitivity to the Standard Model (SM) and potential Beyond-the-SM\ndegrees of freedom, incorporating the effects of particle masses. We find that\na Schwarzschild primordial black hole (PBH) with an initial mass of $6.23\\times\n10^{14}~{\\rm g}$ would have a Page time equal to the age of the Universe,\nassuming emission of SM particles only. We further explore the impact of a\nnon-negligible PBH angular momentum, finding that light spin-2 particles are\npredominantly emitted before the Page time. Specifically, for initial angular\nmomenta values exceeding $a_\\star > 0.5$, approximately $70\\%$ of the total\ngraviton emission occurs prior to the Page time for PBHs with an initial mass\n$M_{\\rm BH} \\lesssim 10^{10}~{\\rm g}$. Finally, we discuss the implications for\nPBH phenomenology, particularly regarding potential constraints from $\\Delta\nN_{\\rm eff}$ measurements.",
        "We propose a universal adversarial attack on multimodal Large Language Models\n(LLMs) that leverages a single optimized image to override alignment safeguards\nacross diverse queries and even multiple models. By backpropagating through the\nvision encoder and language head, we craft a synthetic image that forces the\nmodel to respond with a targeted phrase (e.g., ''Sure, here it is'') or\notherwise unsafe content-even for harmful prompts. In experiments on the\nSafeBench benchmark, our method achieves significantly higher attack success\nrates than existing baselines, including text-only universal prompts (e.g., up\nto 93% on certain models). We further demonstrate cross-model transferability\nby training on several multimodal LLMs simultaneously and testing on unseen\narchitectures. Additionally, a multi-answer variant of our approach produces\nmore natural-sounding (yet still malicious) responses. These findings\nunderscore critical vulnerabilities in current multimodal alignment and call\nfor more robust adversarial defenses. We will release code and datasets under\nthe Apache-2.0 license. Warning: some content generated by Multimodal LLMs in\nthis paper may be offensive to some readers.",
        "AI-generated images have become so good in recent years that individuals\ncannot distinguish them any more from \"real\" images. This development creates a\nseries of societal risks, and challenges our perception of what is true and\nwhat is not, particularly with the emergence of \"deep fakes\" that impersonate\nreal individuals. Watermarking, a technique that involves embedding identifying\ninformation within images to indicate their AI-generated nature, has emerged as\na primary mechanism to address the risks posed by AI-generated images. The\nimplementation of watermarking techniques is now becoming a legal requirement\nin many jurisdictions, including under the new 2024 EU AI Act. Despite the\nwidespread use of AI image generation systems, the current status of\nwatermarking implementation remains largely unexamined. Moreover, the practical\nimplications of the AI Act's watermarking requirements have not previously been\nstudied. The present paper therefore both provides an empirical analysis of 50\nof the most widely used AI systems for image generation, and embeds this\nempirical analysis into a legal analysis of the AI Act. We identify four\ncategories of generative AI image systems relevant under the AI Act, outline\nthe legal obligations for each category, and find that only a minority number\nof providers currently implement adequate watermarking practices.",
        "This study proposes an advanced method for surface defect detection in\nprinted circuit boards (PCBs) using an improved YOLOv11 model enhanced with a\ngenerative adversarial network (GAN). The approach focuses on identifying six\ncommon defect types: missing hole, rat bite, open circuit, short circuit, burr,\nand virtual welding. By employing GAN to generate synthetic defect images, the\ndataset is augmented with diverse and realistic patterns, improving the model's\nability to generalize, particularly for complex and infrequent defects like\nburrs. The enhanced YOLOv11 model is evaluated on a PCB defect dataset,\ndemonstrating significant improvements in accuracy, recall, and robustness,\nespecially when dealing with defects in complex environments or small targets.\nThis research contributes to the broader field of electronic design automation\n(EDA), where efficient defect detection is a crucial step in ensuring\nhigh-quality PCB manufacturing. By integrating advanced deep learning\ntechniques, this approach enhances the automation and precision of defect\ndetection, reducing reliance on manual inspection and accelerating\ndesign-to-production workflows. The findings underscore the importance of\nincorporating GAN-based data augmentation and optimized detection architectures\nin EDA processes, providing valuable insights for improving reliability and\nefficiency in PCB defect detection within industrial applications.",
        "Plasmonic nanostructures have wide applications in photonics including\npathogen detection and diagnosis via Surface-Enhanced Raman Spectroscopy\n(SERS). Despite major role plasmonics play in signal enhancement,\nelectrostatics in SERS is yet to be fully understood and harnessed. Here, we\nperform a systematic study of electrostatic interactions between 785 nm\nresonant gold nanorods designed to harbor zeta potentials of +29, +16, 0 and -9\nmV spanning positive neutral and negative domains. SERS activity is tested on\nrepresentative Gram-negative Escherichia coli and Gram-positive Staphylococcus\nepidermidis bacteria with zeta potentials of -30 and -23 mV respectively in\nwater. Raman spectroscopy and Cryo-Electron microscopy reveal that +29, +16, 0\nand -9 mV nanorods give SERS enhancement of 7.2X, 3.6X, 4.2X, 1.3X to\nStaphylococcus epidermidis and 3.9X, 2.8X, 2.9X, 1.1X to Escherichia coli.\nTheoretical results show that electrostatics play the major role among all\ninteraction forces in determining cell-nanorod proximity and signal\nenhancement. We identify steric repulsion due to cell protrusions to be the\ncritical opposing force. Finally, a design principle is proposed to estimate\nthe electrostatic strength in SERS. Our work provides new insights into the\nprinciple of bacteria-nanorod interactions, enabling reproducible and precise\nbiomolecular readouts, critical for next-generation point-of-care diagnostics\nand smart healthcare applications.",
        "This column advocates for including artificial intelligence (AI)-specific\nmetadata on those academic papers that are written with the help of AI in an\nattempt to analyze the use of such tools for disseminating research.",
        "The generalized Brillouin zone (GBZ) has been highly successful in\ncharacterizing the topology and band structure of non-Hermitian systems.\nHowever, its applicability has been challenged in spatially inhomogeneous\nsettings, where the non-locality of non-Hermitian pumping competes with\nWannier-Stark localization and quantum interference, potentially leading to\nhighly non-exponential state accumulation. To transcend this major conceptual\nbottleneck, we develop a general phase-space GBZ formalism that encodes\nnon-Bloch deformations in both position and momentum space, such as to\naccurately represent spatially inhomogeneous non-Hermitian pumping. A key new\nphenomenon is the bifurcation of the phase-space GBZ branches, which allows\ncertain eigenstates to jump abruptly between different GBZ solutions at various\npoints in real space. The freedom in the locations of such jumps opens up an\nemergent degree of freedom that protects the stability of real spectra and,\nmore impressively, the robustness of a new class of topological zero modes\nunique to GBZ bifurcation.The response from these novel spectral and GBZ\nsingularities can be readily demonstrated in mature metamaterial platforms such\nas photonic crystals or circuit arrays, where effective real-space hoppings can\nbe engineered in a versatile manner.Our framework directly generalizes to more\ncomplicated unit cells and further hoppings, opening up a vast new arena for\nexploring unconventional spectral and topological transitions as well as GBZ\nfragmentation in spatially inhomogeneous non-Hermitian settings.",
        "Digital Surface Models (DSMs) are essential for accurately representing\nEarth's topography in geospatial analyses. DSMs capture detailed elevations of\nnatural and manmade features, crucial for applications like urban planning,\nvegetation studies, and 3D reconstruction. However, DSMs derived from stereo\nsatellite imagery often contain voids or missing data due to occlusions,\nshadows, and lowsignal areas. Previous studies have primarily focused on void\nfilling for digital elevation models (DEMs) and Digital Terrain Models (DTMs),\nemploying methods such as inverse distance weighting (IDW), kriging, and spline\ninterpolation. While effective for simpler terrains, these approaches often\nfail to handle the intricate structures present in DSMs. To overcome these\nlimitations, we introduce Dfilled, a guided DSM void filling method that\nleverages optical remote sensing images through edge-enhancing diffusion.\nDfilled repurposes deep anisotropic diffusion models, which originally designed\nfor super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin\nnoise to create inpainting masks that mimic natural void patterns in DSMs.\nExperimental evaluations demonstrate that Dfilled surpasses traditional\ninterpolation methods and deep learning approaches in DSM void filling tasks.\nBoth quantitative and qualitative assessments highlight the method's ability to\nmanage complex features and deliver accurate, visually coherent results.",
        "The fields of MPC and RL consider two successful control techniques for\nMarkov decision processes. Both approaches are derived from similar fundamental\nprinciples, and both are widely used in practical applications, including\nrobotics, process control, energy systems, and autonomous driving. Despite\ntheir similarities, MPC and RL follow distinct paradigms that emerged from\ndiverse communities and different requirements. Various technical\ndiscrepancies, particularly the role of an environment model as part of the\nalgorithm, lead to methodologies with nearly complementary advantages. Due to\ntheir orthogonal benefits, research interest in combination methods has\nrecently increased significantly, leading to a large and growing set of complex\nideas leveraging MPC and RL. This work illuminates the differences,\nsimilarities, and fundamentals that allow for different combination algorithms\nand categorizes existing work accordingly. Particularly, we focus on the\nversatile actor-critic RL approach as a basis for our categorization and\nexamine how the online optimization approach of MPC can be used to improve the\noverall closed-loop performance of a policy.",
        "We revisit the results of Kim, and of Katsoulis and Ramsey concerning\nhyperrigidity for non-degenerate C*-correspondences. We show that the tensor\nalgebra is hyperrigid, if and only if Katsura's ideal acts non-degenerately, if\nand only if Katsura's ideal acts non-degenerately under any representation.\nThis gives a positive answer to the question of Katsoulis and Ramsey, showing\nthat their necessary condition and their sufficient condition for hyperrigidity\nof the tensor algebra are equivalent. Non-degeneracy of the left action of\nKatsura's ideal was also shown by Kim to be equivalent to hyperrigidity for the\nselfadjoint operator space associated with the C*-correspondence, and our\napproach provides a simplified proof of this result as well. In the process we\nrevisit Arveson's criterion connecting maximality with the unique extension\nproperty and hyperrigidity, in conjunction with the work of Salomon on\ngenerating sets."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Adding Conditional Control to Text-to-Image Diffusion Models",
    "start_abstract":"We present ControlNet, a neural network architecture to add spatial conditioning controls large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large models, and reuses their deep robust encoding layers with billions of images as strong backbone learn diverse set conditional controls. The is connected \"zero convolutions\" (zero-initialized convolution layers) that progressively grow parameters from zero ensure no harmful noise could affect finetuning. test various controls, e.g., edges, depth, segmentation, human pose, etc., Stable Diffusion, using single or multiple conditions, without prompts. show training ControlNets small (<50k) (>1m) datasets. Extensive results may facilitate wider applications control image",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Rsna pneumonia detection challenge"
      ],
      "abstract":[
        "In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "A notion of fractality for a class of states and noncommutative relative\n  distance zeta functional",
        "Counterexamples for T\\\"urkelli's Modification on Malle's Conjecture",
        "ExoMiner++ on TESS with Transfer Learning from Kepler: Transit\n  Classification and Vetting Catalog for 2-min Data",
        "Recoil-induced errors and their correction in photon-mediated\n  entanglement between atom qubits",
        "Analytical results for laser models producing a beam with sub-Poissonian\n  photon statistics and coherence scaling as the Heisenberg limit",
        "Mapping strain and structural heterogeneities around bubbles in\n  amorphous ionically conductive Bi$_2$O$_3$",
        "Monitoring microplastics in live reef-building corals with microscopic\n  laser particles",
        "Optimum Monitoring and Job Assignment with Multiple Markov Machines",
        "A robust score test in g-computation for covariate adjustment in\n  randomized clinical trials leveraging different variance estimators via\n  influence functions",
        "LS-category and sequential topological complexity of symmetric products",
        "$(\\infty,n)$-categories in context",
        "$A$-Norm and $A$-numerical Radius Inequalities for Sums Of Operators in\n  semi-Hilbertian spaces",
        "Local False Sign Rate and the Role of Prior Covariance Rank in\n  Multivariate Empirical Bayes Multiple Testing",
        "Comprehensive scaling laws across animals, microorganisms and plants",
        "The spectrum of dense kernel-based random graphs",
        "Approximation of High-Dimensional Gibbs Distributions with Functional\n  Hierarchical Tensors",
        "Tunneling between magnetic wells in two dimensions",
        "Scale-Insensitive Neural Network Significance Tests",
        "A choice-free proof of Mal'cev's theorem on quasivarieties",
        "Classical Simulation of Non-Classical Systems: A Large Deviation\n  Analysis",
        "Mind the gap: addressing data gaps and assessing noise mismodeling in\n  LISA",
        "Observed Dispersive Properties of the Slow Magnetoacoustic Waves\n  Propagating in Coronal Fan Loops above Sunspots",
        "A family of asymptotically bad wild towers of function fields",
        "Spin-Triplet Excitonic Insulator in the Ultra-Quantum Limit of HfTe5",
        "Gray-body factors: Method matters",
        "Machine Learning for Airborne Electromagnetic Data Inversion: a\n  Bootstrapped Approach",
        "Pessimistic bilevel optimization approach for decision-focused learning",
        "Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence",
        "Hyperpolygonal arrangements"
      ],
      "abstract":[
        "In this work, we first recall the definition of the relative distance zeta\nfunction in [36, 37, 38, 40, 41] and slightly generalize this notion from sets\nto probability measures, and then move on to propose a novel definition a\nrelative distance (and tube) zeta functional for a class of states over a C*\nalgebra. With such an extension, we look into the chance to define relative\nMinkowski dimensions in this context, and explore the notion of fractality for\nthis class of states. Relative complex dimensions as poles of this newly\nproposed relative distance zeta functional, as well as its geometric and\ntransformation properties, decomposition rules and properties that respects\ntensor products are discussed. We then explore some examples that possess\nfractal properties with this new zeta functional and propose functional\nequations similar to [9].",
        "We give counterexamples for the modification on Malle's Conjecture given by\nT\\\"urkelli. T\\\"urkelli's modification on Malle's conjecture is inspired by an\nanalogue of Malle's conjecture over a function field. As a result, our\ncounterexamples demonstrate that the $b$ constant can differ between function\nfields and number fields. We also show that Kl\\\"uners' counterexamples give\ncounterexamples for a natural extension of Malle's conjecture to counting\nnumber fields by product of ramified primes. We then propose a refined version\nof Malle's conjecture which implies a new conjectural value for the constant\n$b$ for number fields.",
        "We present ExoMiner++, an enhanced deep learning model that builds on the\nsuccess of ExoMiner to improve transit signal classification in 2-minute TESS\ndata. ExoMiner++ incorporates additional diagnostic inputs, including\nperiodogram, flux trend, difference image, unfolded flux, and spacecraft\nattitude control data, all of which are crucial for effectively distinguishing\ntransit signals from more challenging sources of false positives. To further\nenhance performance, we leverage transfer learning from high-quality labeled\ndata from the Kepler space telescope, mitigating the impact of TESS's noisier\nand more ambiguous labels. ExoMiner++ achieves high accuracy across various\nclassification and ranking metrics, significantly narrowing the search space\nfor follow-up investigations to confirm new planets. To serve the exoplanet\ncommunity, we introduce new TESS catalogs containing ExoMiner++ classifications\nand confidence scores for each transit signal. Among the 147,568 unlabeled\nTCEs, ExoMiner++ identifies 7,330 as planet candidates, with the remainder\nclassified as false positives. These 7,330 planet candidates correspond to\n1,868 existing TESS Objects of Interest (TOIs), 69 Community TESS Objects of\nInterest (CTOIs), and 50 newly introduced CTOIs. 1,797 out of the 2,506 TOIs\npreviously labeled as planet candidates in ExoFOP are classified as planet\ncandidates by ExoMiner++. This reduction in plausible candidates combined with\nthe excellent ranking quality of ExoMiner++ allows the follow-up efforts to be\nfocused on the most likely candidates, increasing the overall planet yield.",
        "Photonically-interconnected matter qubit systems have wide-ranging\napplications across quantum science and technology, with entanglement between\ndistant qubits serving as a universal resource. While state-of-the-art heralded\nentanglement generation performance thus far has been achieved in trapped\natomic systems modelled as stationary emitters, the improvements to fidelities\nand generation rates demanded by large-scale applications require taking into\naccount their motional degrees of freedom. Here, we derive the effects of\natomic motion on spontaneous emission coupled into arbitrary optical modes, and\nstudy the implications for commonly-used atom-atom entanglement protocols. We\narrive at a coherent physical picture in the form of \"kick operators\"\nassociated with each instant in the photonic wavepackets, which also suggests a\nmethod to mitigate motional errors by disentangling qubit and motion\npost-herald. This proposed correction technique removes overheads associated\nwith the thermal motion of atoms, and may greatly increase entanglement rates\nin long-distance quantum network links by allowing single-photon-based\nprotocols to be used in the high-fidelity regime.",
        "Recent advances in laser theory have demonstrated that a quantum enhancement\nis possible for the production of coherence $\\mathfrak{C}$ by a continuous-wave\nlaser device. Curiously, natural families of laser models that achieve\nHeisenberg-limited scaling for coherence produce the most coherence when the\nbeam exhibits sub-Poissonian photon statistics. In this work, we provide an\nanalytical treatment of those novel families of laser models by specializing to\na parameter regime that permits a linearization. We characterize the dynamics\nof each laser system, and find that some of the intuitions from standard laser\ntheory may be applied here. Specifically, the intracavity number dynamics are\nwell-described as an Ornstein-Uhlenbeck process, while the intracavity phase\ndynamics are well-described in terms of a physically realizable ensemble of\npure states, which evolve according to pure phase diffusion. Unlike a standard\nlaser, however, we find that the pure states comprising the ensemble in the\nHeisenberg-limited lasers are substantially phase squeezed. From our dynamical\nanalysis, we deduce various quantities of the beam for each laser family,\nincluding the first- and second-order Glauber coherence functions, intensity\nnoise spectrum, Mandel-Q parameter and coherence $\\mathfrak{C}$. In addition,\ninspired from these phase diffusion dynamics, we derive an upper bound on laser\ncoherence $\\mathfrak{C} \\lesssim 1.1156 \\mu^4$ -- which is tighter by a factor\nof $3\/8$ when compared to that derived in [Baker et al., Nat. Phys. 17 179\n(2021)] -- by making one of the assumptions of that paper slightly stronger.",
        "While amorphous materials are often approximated to have a statistically\nhomogeneous atomic structure, they frequently exhibit localized structural\nheterogeneity that challenges simplified models. This study uses 4D scanning\ntransmission electron microscopy to investigate the strain and structural\nmodifications around gas bubbles in amorphous Bi$_2$O$_3$ induced by argon\nirradiation. We present a method for determining strain fields surrounding\nbubbles that can be used to measure the internal pressure of the gas.\nCompressive strain is observed around the cavities, with higher-order\ncrystalline symmetries emerging near the cavity interfaces, suggesting\nparacrystalline ordering as a result of bubble coarsening. This ordering, along\nwith a compressive strain gradient, indicates that gas bubbles induce\nsignificant localized changes in atomic packing. By analyzing strain fields\nwith maximum compressive strains of 3\\%, we estimate a lower bound on the\ninternal pressure of the bubbles at 2.5 GPa. These findings provide insight\ninto the complex structural behavior of amorphous materials under stress,\nparticularly in systems with gas inclusions, and offer new methods for probing\nthe local atomic structure in disordered materials. Although considering\nstructural heterogeneity in amorphous systems is non-trivial, these features\nhave crucial impacts on material functionalities, such as mechanical strength,\nionic conductivity, and electronic mobility.",
        "Microplastics are an emerging threat for reef-building corals. However, the\nunderstanding of microplastic uptake into coral tissue and the long-term\nintegration of microplastic into the coral skeleton remains limited due to the\ninvasiveness of state-of-the-art methods for monitoring and localizing\nmicroplastic. Here, we exploit optical resonances to turn microplastics into\nmicroscopic laser particles. Their characteristic emission allows to track of\nmicroplastics in vivo and to perform simultaneous sensing of the\nmicroenvironment, shedding light on the internalization and transport pathways\nof single microplastic particles in corals.",
        "We study a class of systems termed Markov Machines (MM) which process job\nrequests with exponential service times. Assuming a Poison job arrival process,\nthese MMs oscillate between two states, free and busy. We consider the problem\nof sampling the states of these MMs so as to track their states, subject to a\ntotal sampling budget, with the goal of allocating external job requests\neffectively to them. For this purpose, we leverage the $\\textit{binary\nfreshness metric}$ to quantify the quality of our ability to track the states\nof the MMs, and introduce two new metrics termed $\\textit{false acceptance\nratio}$ (FAR) and $\\textit{false rejection ratio}$ (FRR) to evaluate the\neffectiveness of our job assignment strategy. We provide optimal sampling rate\nallocation schemes for jointly monitoring a system of $N$ heterogeneous MMs.",
        "G-computation has become a widely used robust method for estimating\nunconditional (marginal) treatment effects with covariate adjustment in the\nanalysis of randomized clinical trials. Statistical inference in this context\ntypically relies on the Wald test or Wald interval, which can be easily\nimplemented using a consistent variance estimator. However, existing literature\nsuggests that when sample sizes are small or when parameters of interest are\nnear boundary values, Wald-based methods may be less reliable due to type I\nerror rate inflation and insufficient interval coverage. In this article, we\npropose a robust score test for g-computation estimators in the context of\ntwo-sample treatment comparisons. The proposed test is asymptotically valid\nunder simple and stratified (biased-coin) randomization schemes, even when\nregression models are misspecified. These test statistics can be conveniently\ncomputed using existing variance estimators, and the corresponding confidence\nintervals have closed-form expressions, making them convenient to implement.\nThrough extensive simulations, we demonstrate the superior finite-sample\nperformance of the proposed method. Finally, we apply the proposed method to\nreanalyze a completed randomized clinical trial. The new analysis using our\nproposed score test achieves statistical significance, whilst reducing the\nissue of type I error inflation.",
        "The $n$-th symmetric product of a topological space $X$ is the orbit space of\nthe natural action of the symmetric group $S_n$ on the product space $X^n$. In\nthis paper, we compute the sequential topological complexities of the symmetric\nproducts of closed orientable surfaces, thereby verifying the rationality\nconjecture of Farber and Oprea for these spaces. Additionally, we determine the\nLusternik--Schnirelmann category of the symmetric products of closed\nnon-orientable surfaces. More generally, we provide lower bounds to the\nLS-category and the sequential topological complexities of the symmetric\nproducts of finite CW complexes $X$ in terms of the cohomology of $X$ and its\nproducts. On the way, we also obtain new lower bounds to the sequential\ndistributional complexities of continuous maps and study the homotopy groups of\nthe symmetric products of closed surfaces.",
        "This note is a contribution written for the second volume of the Encyclopedia\nof mathematical physics. We give an informal introduction to the notions of an\n$(\\infty,n)$-category and $(\\infty,n)$-functor, discussing some of the\ndifferent models that implement them. We also discuss the notions of a\nsymmetric monoidal $(\\infty,n)$-category and symmetric monoidal\n$(\\infty,n)$-functor, recalling some important results whose statements employ\nthe language of $(\\infty,n)$-categories.",
        "The aim of this study is to establish many new inequalities for the operator\n$A$-norm and $A$-numerical radius of sums of bounded linear operators in\nHilbert spaces. In particular, two refinements are made to the generalized\ntriangle inequality for operator norm. Additionally, we examine a number of\nintriguing uses for two bounded linear operators in the Cartesian decomposition\nof an operator. These disparities improve and generalize several earlier\nresults in the literature. Some applications of our results are given. We also\npresent some instances to support our results.",
        "We study the relationship between the rank of the prior covariance matrix and\nthe local false sign rate in a multivariate empirical Bayes normal mean model.\nIt has been observed that the false sign rate is inflated when the prior\nassigns weight to low-rank covariance matrices. We show that this issue arises\ndue to the rank deficiency of prior covariance matrices and propose an\nadjustment to mitigate it.",
        "Scaling laws illuminate Nature's fundamental biological principles and guide\nbioinspired materials and structural designs. In simple cases they are based on\nthe fundamental principle that all laws of nature remain unchanged (i.e.,\ninvariant) under a change of units. A more general framework is a change of\nvariables for the governing laws that takes all equations, boundary, and\ninteraction conditions into themselves. We consider an accepted macroscale\nsystem of partial differential equations including coupled fluid dynamics,\nnonlinear elasticity, and rigid body mechanics for a complex organism. We show\nthat there is a set of scaling laws where length, time, density, elastic\nmodulus, viscosity, and gravitational constant undergo nontrivial scaling\n(Table 1). We compare these results to extensive data sets mined from the\nliterature on beating frequency of flying, swimming, and running animals, speed\nof bacteria, insects, fish, mammals and reptiles, leg stiffness of mammals, and\nmodulus of elasticity of plants. The uniform agreement of the scaling laws with\nthe dynamics of fauna, flora, and microorganisms supports the dominating role\nof coupled nonlinear elasticity and fluid dynamics in evolutionary development.\nWe conclude with predictions for some prehistoric cases for which observations\nare unavailable.",
        "Kernel-based random graphs (KBRGs) are a broad class of random graph models\nthat account for inhomogeneity among vertices. We consider KBRGs on a discrete\n$d-$dimensional torus $\\mathbf{V}_N$ of size $N^d$. Conditionally on an\ni.i.d.~sequence of {Pareto} weights $(W_i)_{i\\in \\mathbf{V}_N}$ with tail\nexponent $\\tau-1>0$, we connect any two points $i$ and $j$ on the torus with\nprobability\n  $$p_{ij}= \\frac{\\kappa_{\\sigma}(W_i,W_j)}{\\|i-j\\|^{\\alpha}} \\wedge 1$$ for\nsome parameter $\\alpha>0$ and $\\kappa_{\\sigma}(u,v)= (u\\vee v)(u \\wedge\nv)^{\\sigma}$ for some $\\sigma\\in(0,\\tau-1)$.\n  We focus on the adjacency operator of this random graph and study its\nempirical spectral distribution. For $\\alpha<d$ and $\\tau>2$, we show that a\nnon-trivial limiting distribution exists as $N\\to\\infty$ and that the\ncorresponding measure $\\mu_{\\sigma,\\tau}$ is absolutely continuous with respect\nto the Lebesgue measure. $\\mu_{\\sigma,\\tau}$ is given by an operator-valued\nsemicircle law, whose Stieltjes transform is characterised by a fixed point\nequation in an appropriate Banach space. We analyse the moments of\n$\\mu_{\\sigma,\\tau}$ and prove that the second moment is finite even when the\nweights have infinite variance. In the case $\\sigma=1$, corresponding to the\nso-called scale-free percolation random graph, we can explicitly describe the\nlimiting measure and study its tail.",
        "The numerical representation of high-dimensional Gibbs distributions is\nchallenging due to the curse of dimensionality manifesting through the\nintractable normalization constant calculations. This work addresses this\nchallenge by performing a particle-based high-dimensional parametric density\nestimation subroutine, and the input to the subroutine is Gibbs samples\ngenerated by leveraging advanced sampling techniques. Specifically, to generate\nGibbs samples, we employ ensemble-based annealed importance sampling, a\npopulation-based approach for sampling multimodal distributions. These samples\nare then processed using functional hierarchical tensor sketching, a\ntensor-network-based density estimation method for high-dimensional\ndistributions, to obtain the numerical representation of the Gibbs\ndistribution. We successfully apply the proposed approach to complex\nGinzburg-Landau models with hundreds of variables. In particular, we show that\nthe approach proposed is successful at addressing the metastability issue under\ndifficult numerical cases.",
        "The two-dimensional magnetic Laplacian is considered. We calculate the\nleading term of the splitting between the first two eigenvalues of the operator\nin the semiclassical limit under the assumption that the magnetic field does\nnot vanish and has two symmetric magnetic wells with respect to the coordinate\naxes. This is the first result of quantum tunneling between purely magnetic\nwells under generic assumptions. The proof, which strongly relies on microlocal\nanalysis, reveals a purely magnetic Agmon distance between the wells.\nSurprisingly, it is discovered that the exponential decay of the eigenfunctions\naway from the magnetic wells is not crucial to derive the tunneling formula.\nThe key is a microlocal exponential decay inside the characteristic manifold,\nwith respect to the variable quantizing the classical center guide motion.",
        "This paper develops a scale-insensitive framework for neural network\nsignificance testing, substantially generalizing existing approaches through\nthree key innovations. First, we replace metric entropy calculations with\nRademacher complexity bounds, enabling the analysis of neural networks without\nrequiring bounded weights or specific architectural constraints. Second, we\nweaken the regularity conditions on the target function to require only Sobolev\nspace membership $H^s([-1,1]^d)$ with $s > d\/2$, significantly relaxing\nprevious smoothness assumptions while maintaining optimal approximation rates.\nThird, we introduce a modified sieve space construction based on moment bounds\nrather than weight constraints, providing a more natural theoretical framework\nfor modern deep learning practices. Our approach achieves these generalizations\nwhile preserving optimal convergence rates and establishing valid asymptotic\ndistributions for test statistics. The technical foundation combines\nlocalization theory, sharp concentration inequalities, and scale-insensitive\ncomplexity measures to handle unbounded weights and general Lipschitz\nactivation functions. This framework better aligns theoretical guarantees with\ncontemporary deep learning practice while maintaining mathematical rigor.",
        "In 1966, Mal'cev proved that a class $\\mathcal{K}$ of first-order structures\nwith a specified signature is a quasivariety if and only if $\\mathcal{K}$\ncontains a unit and is closed under isomorphisms, substructures, and reduced\nproducts. In this article, we present a proof of this theorem in $\\mathsf{ZF}$\n(the Zermelo--Fraenkel set theory without the axiom of choice).",
        "Any quasi-probability representation of a no-signaling system -- including\nquantum systems -- can be simulated via a purely classical scheme by allowing\nsigned events and a cancellation procedure. This raises a fundamental question:\nWhat properties of the non-classical system does such a classical simulation\nfail to replicate? We answer by using large deviation theory to show that the\nprobability of a large fluctuation under the classical simulation can be\nstrictly greater than under the actual non-classical system. The key finding\ndriving our result is that negativity in probability relaxes the data\nprocessing inequality of information theory. We propose this potential large\ndeviation stability of quantum (and no-signaling) systems as a novel form of\nquantum advantage.",
        "Due to the sheer complexity of the Laser Interferometer Space Antenna (LISA)\nspace mission, data gaps arising from instrumental irregularities and\/or\nscheduled maintenance are unavoidable. Focusing on merger-dominated massive\nblack hole binary signals, we test the appropriateness of the\nWhittle-likelihood on gapped data in a variety of cases. From first principles,\nwe derive the likelihood valid for gapped data in both the time and frequency\ndomains. Cheap-to-evaluate proxies to p-p plots are derived based on a\nFisher-based formalism, and verified through Bayesian techniques. Our tools\nallow to predict the altered variance in the parameter estimates that arises\nfrom noise mismodeling, as well as the information loss represented by the\nbroadening of the posteriors. The result of noise mismodeling with gaps is\nsensitive to the characteristics of the noise model, with strong low-frequency\n(red) noise and strong high-frequency (blue) noise giving statistically\nsignificant fluctuations in recovered parameters. We demonstrate that the\nintroduction of a tapering window reduces statistical inconsistency errors, at\nthe cost of less precise parameter estimates. We also show that the assumption\nof independence between inter-gap segments appears to be a fair approximation\neven if the data set is inherently coherent. However, if one instead assumes\nfictitious correlations in the data stream, when the data segments are actually\nindependent, then the resultant parameter recoveries could be inconsistent with\nthe true parameters. The theoretical and numerical practices that are presented\nin this work could readily be incorporated into global-fit pipelines operating\non gapped data.",
        "Recurrent and propagating intensity perturbations are frequently observed in\nextreme ultraviolet (EUV) channels along coronal fan loops above sunspots, and\nthese perturbations are suggested to be slow magnetoacoustic waves. Numerous\nstudies have been conducted to investigate their propagation speeds, damping,\nand excitation sources; however, there have been limited observational analyses\non whether these waves are dispersive despite some theoretical studies. In this\nstudy, we apply cross-correlation analysis in the Fourier domain on slow\nmagnetoacoustic waves using three different datasets: EUV intensity observed by\nSDO\/AIA, differential emission measure (DEM) temperature maps, and Doppler\nvelocities from Hinode\/EIS spectrometer observations. The apparent phase\nvelocities of the waves, which are the plane-of-sky component of the waves'\nphase velocities, are derived as functions of frequency for all the three\ndatasets. It is found that the phase velocities show clear frequency\ndependency, with a general trend of increase with frequency, ranging from\napproximately 30 km\/s around 3 mHz to about 80 km\/s around 10 mHz. The\nfrequency dependency of the phase velocities demonstrates that the slow\nmagnetoacoustic waves in the coronal loops are dispersive. The dispersiveness\nof these waves can provide a useful tool for the diagnosis of physical\nconditions inside the coronal loops along which these waves travel.",
        "In a previous work general conditions were given to prove the infiniteness of\nthe genus of certain towers of function fields over a perfect field. It was\nshown that many examples where particular cases of those general results. In\nthis paper the genus of a family of wild towers of function fields will be\nconsidered together with a result with less restrictive sufficient conditions\nfor a wild tower to have infinite genus.",
        "More than fifty years ago, excitonic insulators, formed by the pairing of\nelectrons and holes due to Coulomb interactions, were first predicted. Since\nthen, excitonic insulators have been observed in various classes of materials,\nincluding quantum Hall bilayers, graphite, transition metal chalcogenides, and\nmore recently in moire superlattices. In these excitonic insulators, an\nelectron and a hole with the same spin bind together and the resulting exciton\nis a spin singlet. Here, we report the experimental observation of a\nspin-triplet exciton insulator in the ultra-quantum limit of a\nthree-dimensional topological material HfTe5. We observe that the\nspin-polarized zeroth Landau bands, dispersing along the field direction, cross\neach other beyond a characteristic magnetic field in HfTe5, forming the\none-dimensional Weyl mode. Transport measurements reveal the emergence of a gap\nof about 250 {\\mu}eV when the field surpasses a critical threshold. By\nperforming the material-specific modeling, we identify this gap as a\nconsequence of a spin-triplet exciton formation, where electrons and holes with\nopposite spin form bound states, and the translational symmetry is preserved.\nThe system reaches charge neutrality following the gap opening, as evidenced by\nthe zero Hall conductivity over a wide magnetic field range (10 - 72 T). Our\nfinding of the spin-triplet excitonic insulator paves the way for studying\nnovel spin transport including spin superfluidity, spin Josephson currents, and\nCoulomb drag of spin currents in analogy to the transport properties associated\nwith the layer pseudospin in quantum Hall bilayers.",
        "The calculation of gray-body factors is essential for understanding Hawking\nradiation and black hole thermodynamics. While the formalism developed by\nChandrasekhar is effective for static black holes, it faces significant\nchallenges in Kerr spacetimes, particularly in the superradiant regime, where a\nspecific choice of coordinates introduces numerical inaccuracies. To address\nthese limitations, an alternative method based on re-scaling radial coordinates\nand employing Frobenius-like expansions has been investigated. We compare the\ngray-body factors obtained for a near-maximally rotating black hole using both\nmethods and find that the Chandrasekhar formalism systematically overestimates\nthe values in the superradiant regime compared to well-established analytical\nresults. Specifically, for a spin parameter of $a_* = 0.999$, the Chandrasekhar\nmethod yields values approximately twice as large as the correct result. Since\nthis approach has been implemented in \\texttt{BlackHawk}, we assess the impact\nof these discrepancies on constraints derived from gamma-ray observations of\nhighly spinning primordial black holes.",
        "Aircraft-based surveying to collect airborne electromagnetic data is a key\nmethod to image large swaths of the Earth's surface in pursuit of better\nknowledge of aquifer systems. Despite many years of advancements, 3D inversion\nstill poses challenges in terms of computational requirements, regularization\nselection, hyperparameter tuning and real-time inversion. We present a new\napproach for the inversion of airborne electromagnetic data that leverages\nmachine learning to overcome the computational burden of traditional 3D\ninversion methods, which implicitly includes learned regularization and is\napplicable in real-time. The method combines 1D inversion results with\ngeostatistical modeling to create tailored training datasets, enabling the\ndevelopment of a specialized neural network that predicts 2D conductivity\nmodels from airborne electromagnetic data. This approach requires 3D forward\nmodeling and 1D inversion up front, but no forward modeling during inference.\nThe workflow is applied to the Kaweah Subbasin in California, where it\nsuccessfully reconstructs conductivity models consistent with real-world data\nand geological drill hole information. The results highlight the method's\ncapability to deliver fast and accurate subsurface imaging, offering a valuable\ntool for groundwater exploration and other near-surface applications.",
        "The recent interest in contextual optimization problems, where randomness is\nassociated with side information, has led to two primary strategies for\nformulation and solution. The first, estimate-then-optimize, separates the\nestimation of the problem's parameters from the optimization process. The\nsecond, decision-focused optimization, integrates the optimization problem's\nstructure directly into the prediction procedure. In this work, we propose a\npessimistic bilevel approach for solving general decision-focused formulations\nof combinatorial optimization problems. Our method solves an\n$\\varepsilon$-approximation of the pessimistic bilevel problem using a\nspecialized cut generation algorithm. We benchmark its performance on the 0-1\nknapsack problem against estimate-then-optimize and decision-focused methods,\nincluding the popular SPO+ approach. Computational experiments highlight the\nproposed method's advantages, particularly in reducing out-of-sample regret.",
        "Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution.",
        "In 2024, Bellamy, Craw, Rayan, Schedler, and Weiss introduced a particular\nfamily of real hyperplane arrangements stemming from hyperpolygonal spaces\nassociated with certain quiver varieties which we thus call hyperpolygonal\narrangements $\\mathcal H_n$. In this note we study these arrangements and\ninvestigate their properties systematically. Remarkably the arrangements\n$\\mathcal H_n$ discriminate between essentially all local properties of\narrangements. In addition we show that hyperpolygonal arrangements are\nprojectively unique and combinatorially formal.\n  We note that the arrangement $\\mathcal H_5$ is the famous counterexample of\nEdelman and Reiner from 1993 of Orlik's conjecture that the restriction of a\nfree arrangement is again free."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Highresolution image synthesis with latent diffusion models",
    "start_abstract":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Rsna pneumonia detection challenge"
      ],
      "abstract":[
        "In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "A notion of fractality for a class of states and noncommutative relative\n  distance zeta functional",
        "Counterexamples for T\\\"urkelli's Modification on Malle's Conjecture",
        "ExoMiner++ on TESS with Transfer Learning from Kepler: Transit\n  Classification and Vetting Catalog for 2-min Data",
        "Recoil-induced errors and their correction in photon-mediated\n  entanglement between atom qubits",
        "Analytical results for laser models producing a beam with sub-Poissonian\n  photon statistics and coherence scaling as the Heisenberg limit",
        "Mapping strain and structural heterogeneities around bubbles in\n  amorphous ionically conductive Bi$_2$O$_3$",
        "Monitoring microplastics in live reef-building corals with microscopic\n  laser particles",
        "Optimum Monitoring and Job Assignment with Multiple Markov Machines",
        "A robust score test in g-computation for covariate adjustment in\n  randomized clinical trials leveraging different variance estimators via\n  influence functions",
        "LS-category and sequential topological complexity of symmetric products",
        "$(\\infty,n)$-categories in context",
        "$A$-Norm and $A$-numerical Radius Inequalities for Sums Of Operators in\n  semi-Hilbertian spaces",
        "Local False Sign Rate and the Role of Prior Covariance Rank in\n  Multivariate Empirical Bayes Multiple Testing",
        "Comprehensive scaling laws across animals, microorganisms and plants",
        "The spectrum of dense kernel-based random graphs",
        "Approximation of High-Dimensional Gibbs Distributions with Functional\n  Hierarchical Tensors",
        "Tunneling between magnetic wells in two dimensions",
        "Scale-Insensitive Neural Network Significance Tests",
        "A choice-free proof of Mal'cev's theorem on quasivarieties",
        "Classical Simulation of Non-Classical Systems: A Large Deviation\n  Analysis",
        "Mind the gap: addressing data gaps and assessing noise mismodeling in\n  LISA",
        "Observed Dispersive Properties of the Slow Magnetoacoustic Waves\n  Propagating in Coronal Fan Loops above Sunspots",
        "A family of asymptotically bad wild towers of function fields",
        "Spin-Triplet Excitonic Insulator in the Ultra-Quantum Limit of HfTe5",
        "Gray-body factors: Method matters",
        "Machine Learning for Airborne Electromagnetic Data Inversion: a\n  Bootstrapped Approach",
        "Pessimistic bilevel optimization approach for decision-focused learning",
        "Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence",
        "Hyperpolygonal arrangements"
      ],
      "abstract":[
        "In this work, we first recall the definition of the relative distance zeta\nfunction in [36, 37, 38, 40, 41] and slightly generalize this notion from sets\nto probability measures, and then move on to propose a novel definition a\nrelative distance (and tube) zeta functional for a class of states over a C*\nalgebra. With such an extension, we look into the chance to define relative\nMinkowski dimensions in this context, and explore the notion of fractality for\nthis class of states. Relative complex dimensions as poles of this newly\nproposed relative distance zeta functional, as well as its geometric and\ntransformation properties, decomposition rules and properties that respects\ntensor products are discussed. We then explore some examples that possess\nfractal properties with this new zeta functional and propose functional\nequations similar to [9].",
        "We give counterexamples for the modification on Malle's Conjecture given by\nT\\\"urkelli. T\\\"urkelli's modification on Malle's conjecture is inspired by an\nanalogue of Malle's conjecture over a function field. As a result, our\ncounterexamples demonstrate that the $b$ constant can differ between function\nfields and number fields. We also show that Kl\\\"uners' counterexamples give\ncounterexamples for a natural extension of Malle's conjecture to counting\nnumber fields by product of ramified primes. We then propose a refined version\nof Malle's conjecture which implies a new conjectural value for the constant\n$b$ for number fields.",
        "We present ExoMiner++, an enhanced deep learning model that builds on the\nsuccess of ExoMiner to improve transit signal classification in 2-minute TESS\ndata. ExoMiner++ incorporates additional diagnostic inputs, including\nperiodogram, flux trend, difference image, unfolded flux, and spacecraft\nattitude control data, all of which are crucial for effectively distinguishing\ntransit signals from more challenging sources of false positives. To further\nenhance performance, we leverage transfer learning from high-quality labeled\ndata from the Kepler space telescope, mitigating the impact of TESS's noisier\nand more ambiguous labels. ExoMiner++ achieves high accuracy across various\nclassification and ranking metrics, significantly narrowing the search space\nfor follow-up investigations to confirm new planets. To serve the exoplanet\ncommunity, we introduce new TESS catalogs containing ExoMiner++ classifications\nand confidence scores for each transit signal. Among the 147,568 unlabeled\nTCEs, ExoMiner++ identifies 7,330 as planet candidates, with the remainder\nclassified as false positives. These 7,330 planet candidates correspond to\n1,868 existing TESS Objects of Interest (TOIs), 69 Community TESS Objects of\nInterest (CTOIs), and 50 newly introduced CTOIs. 1,797 out of the 2,506 TOIs\npreviously labeled as planet candidates in ExoFOP are classified as planet\ncandidates by ExoMiner++. This reduction in plausible candidates combined with\nthe excellent ranking quality of ExoMiner++ allows the follow-up efforts to be\nfocused on the most likely candidates, increasing the overall planet yield.",
        "Photonically-interconnected matter qubit systems have wide-ranging\napplications across quantum science and technology, with entanglement between\ndistant qubits serving as a universal resource. While state-of-the-art heralded\nentanglement generation performance thus far has been achieved in trapped\natomic systems modelled as stationary emitters, the improvements to fidelities\nand generation rates demanded by large-scale applications require taking into\naccount their motional degrees of freedom. Here, we derive the effects of\natomic motion on spontaneous emission coupled into arbitrary optical modes, and\nstudy the implications for commonly-used atom-atom entanglement protocols. We\narrive at a coherent physical picture in the form of \"kick operators\"\nassociated with each instant in the photonic wavepackets, which also suggests a\nmethod to mitigate motional errors by disentangling qubit and motion\npost-herald. This proposed correction technique removes overheads associated\nwith the thermal motion of atoms, and may greatly increase entanglement rates\nin long-distance quantum network links by allowing single-photon-based\nprotocols to be used in the high-fidelity regime.",
        "Recent advances in laser theory have demonstrated that a quantum enhancement\nis possible for the production of coherence $\\mathfrak{C}$ by a continuous-wave\nlaser device. Curiously, natural families of laser models that achieve\nHeisenberg-limited scaling for coherence produce the most coherence when the\nbeam exhibits sub-Poissonian photon statistics. In this work, we provide an\nanalytical treatment of those novel families of laser models by specializing to\na parameter regime that permits a linearization. We characterize the dynamics\nof each laser system, and find that some of the intuitions from standard laser\ntheory may be applied here. Specifically, the intracavity number dynamics are\nwell-described as an Ornstein-Uhlenbeck process, while the intracavity phase\ndynamics are well-described in terms of a physically realizable ensemble of\npure states, which evolve according to pure phase diffusion. Unlike a standard\nlaser, however, we find that the pure states comprising the ensemble in the\nHeisenberg-limited lasers are substantially phase squeezed. From our dynamical\nanalysis, we deduce various quantities of the beam for each laser family,\nincluding the first- and second-order Glauber coherence functions, intensity\nnoise spectrum, Mandel-Q parameter and coherence $\\mathfrak{C}$. In addition,\ninspired from these phase diffusion dynamics, we derive an upper bound on laser\ncoherence $\\mathfrak{C} \\lesssim 1.1156 \\mu^4$ -- which is tighter by a factor\nof $3\/8$ when compared to that derived in [Baker et al., Nat. Phys. 17 179\n(2021)] -- by making one of the assumptions of that paper slightly stronger.",
        "While amorphous materials are often approximated to have a statistically\nhomogeneous atomic structure, they frequently exhibit localized structural\nheterogeneity that challenges simplified models. This study uses 4D scanning\ntransmission electron microscopy to investigate the strain and structural\nmodifications around gas bubbles in amorphous Bi$_2$O$_3$ induced by argon\nirradiation. We present a method for determining strain fields surrounding\nbubbles that can be used to measure the internal pressure of the gas.\nCompressive strain is observed around the cavities, with higher-order\ncrystalline symmetries emerging near the cavity interfaces, suggesting\nparacrystalline ordering as a result of bubble coarsening. This ordering, along\nwith a compressive strain gradient, indicates that gas bubbles induce\nsignificant localized changes in atomic packing. By analyzing strain fields\nwith maximum compressive strains of 3\\%, we estimate a lower bound on the\ninternal pressure of the bubbles at 2.5 GPa. These findings provide insight\ninto the complex structural behavior of amorphous materials under stress,\nparticularly in systems with gas inclusions, and offer new methods for probing\nthe local atomic structure in disordered materials. Although considering\nstructural heterogeneity in amorphous systems is non-trivial, these features\nhave crucial impacts on material functionalities, such as mechanical strength,\nionic conductivity, and electronic mobility.",
        "Microplastics are an emerging threat for reef-building corals. However, the\nunderstanding of microplastic uptake into coral tissue and the long-term\nintegration of microplastic into the coral skeleton remains limited due to the\ninvasiveness of state-of-the-art methods for monitoring and localizing\nmicroplastic. Here, we exploit optical resonances to turn microplastics into\nmicroscopic laser particles. Their characteristic emission allows to track of\nmicroplastics in vivo and to perform simultaneous sensing of the\nmicroenvironment, shedding light on the internalization and transport pathways\nof single microplastic particles in corals.",
        "We study a class of systems termed Markov Machines (MM) which process job\nrequests with exponential service times. Assuming a Poison job arrival process,\nthese MMs oscillate between two states, free and busy. We consider the problem\nof sampling the states of these MMs so as to track their states, subject to a\ntotal sampling budget, with the goal of allocating external job requests\neffectively to them. For this purpose, we leverage the $\\textit{binary\nfreshness metric}$ to quantify the quality of our ability to track the states\nof the MMs, and introduce two new metrics termed $\\textit{false acceptance\nratio}$ (FAR) and $\\textit{false rejection ratio}$ (FRR) to evaluate the\neffectiveness of our job assignment strategy. We provide optimal sampling rate\nallocation schemes for jointly monitoring a system of $N$ heterogeneous MMs.",
        "G-computation has become a widely used robust method for estimating\nunconditional (marginal) treatment effects with covariate adjustment in the\nanalysis of randomized clinical trials. Statistical inference in this context\ntypically relies on the Wald test or Wald interval, which can be easily\nimplemented using a consistent variance estimator. However, existing literature\nsuggests that when sample sizes are small or when parameters of interest are\nnear boundary values, Wald-based methods may be less reliable due to type I\nerror rate inflation and insufficient interval coverage. In this article, we\npropose a robust score test for g-computation estimators in the context of\ntwo-sample treatment comparisons. The proposed test is asymptotically valid\nunder simple and stratified (biased-coin) randomization schemes, even when\nregression models are misspecified. These test statistics can be conveniently\ncomputed using existing variance estimators, and the corresponding confidence\nintervals have closed-form expressions, making them convenient to implement.\nThrough extensive simulations, we demonstrate the superior finite-sample\nperformance of the proposed method. Finally, we apply the proposed method to\nreanalyze a completed randomized clinical trial. The new analysis using our\nproposed score test achieves statistical significance, whilst reducing the\nissue of type I error inflation.",
        "The $n$-th symmetric product of a topological space $X$ is the orbit space of\nthe natural action of the symmetric group $S_n$ on the product space $X^n$. In\nthis paper, we compute the sequential topological complexities of the symmetric\nproducts of closed orientable surfaces, thereby verifying the rationality\nconjecture of Farber and Oprea for these spaces. Additionally, we determine the\nLusternik--Schnirelmann category of the symmetric products of closed\nnon-orientable surfaces. More generally, we provide lower bounds to the\nLS-category and the sequential topological complexities of the symmetric\nproducts of finite CW complexes $X$ in terms of the cohomology of $X$ and its\nproducts. On the way, we also obtain new lower bounds to the sequential\ndistributional complexities of continuous maps and study the homotopy groups of\nthe symmetric products of closed surfaces.",
        "This note is a contribution written for the second volume of the Encyclopedia\nof mathematical physics. We give an informal introduction to the notions of an\n$(\\infty,n)$-category and $(\\infty,n)$-functor, discussing some of the\ndifferent models that implement them. We also discuss the notions of a\nsymmetric monoidal $(\\infty,n)$-category and symmetric monoidal\n$(\\infty,n)$-functor, recalling some important results whose statements employ\nthe language of $(\\infty,n)$-categories.",
        "The aim of this study is to establish many new inequalities for the operator\n$A$-norm and $A$-numerical radius of sums of bounded linear operators in\nHilbert spaces. In particular, two refinements are made to the generalized\ntriangle inequality for operator norm. Additionally, we examine a number of\nintriguing uses for two bounded linear operators in the Cartesian decomposition\nof an operator. These disparities improve and generalize several earlier\nresults in the literature. Some applications of our results are given. We also\npresent some instances to support our results.",
        "We study the relationship between the rank of the prior covariance matrix and\nthe local false sign rate in a multivariate empirical Bayes normal mean model.\nIt has been observed that the false sign rate is inflated when the prior\nassigns weight to low-rank covariance matrices. We show that this issue arises\ndue to the rank deficiency of prior covariance matrices and propose an\nadjustment to mitigate it.",
        "Scaling laws illuminate Nature's fundamental biological principles and guide\nbioinspired materials and structural designs. In simple cases they are based on\nthe fundamental principle that all laws of nature remain unchanged (i.e.,\ninvariant) under a change of units. A more general framework is a change of\nvariables for the governing laws that takes all equations, boundary, and\ninteraction conditions into themselves. We consider an accepted macroscale\nsystem of partial differential equations including coupled fluid dynamics,\nnonlinear elasticity, and rigid body mechanics for a complex organism. We show\nthat there is a set of scaling laws where length, time, density, elastic\nmodulus, viscosity, and gravitational constant undergo nontrivial scaling\n(Table 1). We compare these results to extensive data sets mined from the\nliterature on beating frequency of flying, swimming, and running animals, speed\nof bacteria, insects, fish, mammals and reptiles, leg stiffness of mammals, and\nmodulus of elasticity of plants. The uniform agreement of the scaling laws with\nthe dynamics of fauna, flora, and microorganisms supports the dominating role\nof coupled nonlinear elasticity and fluid dynamics in evolutionary development.\nWe conclude with predictions for some prehistoric cases for which observations\nare unavailable.",
        "Kernel-based random graphs (KBRGs) are a broad class of random graph models\nthat account for inhomogeneity among vertices. We consider KBRGs on a discrete\n$d-$dimensional torus $\\mathbf{V}_N$ of size $N^d$. Conditionally on an\ni.i.d.~sequence of {Pareto} weights $(W_i)_{i\\in \\mathbf{V}_N}$ with tail\nexponent $\\tau-1>0$, we connect any two points $i$ and $j$ on the torus with\nprobability\n  $$p_{ij}= \\frac{\\kappa_{\\sigma}(W_i,W_j)}{\\|i-j\\|^{\\alpha}} \\wedge 1$$ for\nsome parameter $\\alpha>0$ and $\\kappa_{\\sigma}(u,v)= (u\\vee v)(u \\wedge\nv)^{\\sigma}$ for some $\\sigma\\in(0,\\tau-1)$.\n  We focus on the adjacency operator of this random graph and study its\nempirical spectral distribution. For $\\alpha<d$ and $\\tau>2$, we show that a\nnon-trivial limiting distribution exists as $N\\to\\infty$ and that the\ncorresponding measure $\\mu_{\\sigma,\\tau}$ is absolutely continuous with respect\nto the Lebesgue measure. $\\mu_{\\sigma,\\tau}$ is given by an operator-valued\nsemicircle law, whose Stieltjes transform is characterised by a fixed point\nequation in an appropriate Banach space. We analyse the moments of\n$\\mu_{\\sigma,\\tau}$ and prove that the second moment is finite even when the\nweights have infinite variance. In the case $\\sigma=1$, corresponding to the\nso-called scale-free percolation random graph, we can explicitly describe the\nlimiting measure and study its tail.",
        "The numerical representation of high-dimensional Gibbs distributions is\nchallenging due to the curse of dimensionality manifesting through the\nintractable normalization constant calculations. This work addresses this\nchallenge by performing a particle-based high-dimensional parametric density\nestimation subroutine, and the input to the subroutine is Gibbs samples\ngenerated by leveraging advanced sampling techniques. Specifically, to generate\nGibbs samples, we employ ensemble-based annealed importance sampling, a\npopulation-based approach for sampling multimodal distributions. These samples\nare then processed using functional hierarchical tensor sketching, a\ntensor-network-based density estimation method for high-dimensional\ndistributions, to obtain the numerical representation of the Gibbs\ndistribution. We successfully apply the proposed approach to complex\nGinzburg-Landau models with hundreds of variables. In particular, we show that\nthe approach proposed is successful at addressing the metastability issue under\ndifficult numerical cases.",
        "The two-dimensional magnetic Laplacian is considered. We calculate the\nleading term of the splitting between the first two eigenvalues of the operator\nin the semiclassical limit under the assumption that the magnetic field does\nnot vanish and has two symmetric magnetic wells with respect to the coordinate\naxes. This is the first result of quantum tunneling between purely magnetic\nwells under generic assumptions. The proof, which strongly relies on microlocal\nanalysis, reveals a purely magnetic Agmon distance between the wells.\nSurprisingly, it is discovered that the exponential decay of the eigenfunctions\naway from the magnetic wells is not crucial to derive the tunneling formula.\nThe key is a microlocal exponential decay inside the characteristic manifold,\nwith respect to the variable quantizing the classical center guide motion.",
        "This paper develops a scale-insensitive framework for neural network\nsignificance testing, substantially generalizing existing approaches through\nthree key innovations. First, we replace metric entropy calculations with\nRademacher complexity bounds, enabling the analysis of neural networks without\nrequiring bounded weights or specific architectural constraints. Second, we\nweaken the regularity conditions on the target function to require only Sobolev\nspace membership $H^s([-1,1]^d)$ with $s > d\/2$, significantly relaxing\nprevious smoothness assumptions while maintaining optimal approximation rates.\nThird, we introduce a modified sieve space construction based on moment bounds\nrather than weight constraints, providing a more natural theoretical framework\nfor modern deep learning practices. Our approach achieves these generalizations\nwhile preserving optimal convergence rates and establishing valid asymptotic\ndistributions for test statistics. The technical foundation combines\nlocalization theory, sharp concentration inequalities, and scale-insensitive\ncomplexity measures to handle unbounded weights and general Lipschitz\nactivation functions. This framework better aligns theoretical guarantees with\ncontemporary deep learning practice while maintaining mathematical rigor.",
        "In 1966, Mal'cev proved that a class $\\mathcal{K}$ of first-order structures\nwith a specified signature is a quasivariety if and only if $\\mathcal{K}$\ncontains a unit and is closed under isomorphisms, substructures, and reduced\nproducts. In this article, we present a proof of this theorem in $\\mathsf{ZF}$\n(the Zermelo--Fraenkel set theory without the axiom of choice).",
        "Any quasi-probability representation of a no-signaling system -- including\nquantum systems -- can be simulated via a purely classical scheme by allowing\nsigned events and a cancellation procedure. This raises a fundamental question:\nWhat properties of the non-classical system does such a classical simulation\nfail to replicate? We answer by using large deviation theory to show that the\nprobability of a large fluctuation under the classical simulation can be\nstrictly greater than under the actual non-classical system. The key finding\ndriving our result is that negativity in probability relaxes the data\nprocessing inequality of information theory. We propose this potential large\ndeviation stability of quantum (and no-signaling) systems as a novel form of\nquantum advantage.",
        "Due to the sheer complexity of the Laser Interferometer Space Antenna (LISA)\nspace mission, data gaps arising from instrumental irregularities and\/or\nscheduled maintenance are unavoidable. Focusing on merger-dominated massive\nblack hole binary signals, we test the appropriateness of the\nWhittle-likelihood on gapped data in a variety of cases. From first principles,\nwe derive the likelihood valid for gapped data in both the time and frequency\ndomains. Cheap-to-evaluate proxies to p-p plots are derived based on a\nFisher-based formalism, and verified through Bayesian techniques. Our tools\nallow to predict the altered variance in the parameter estimates that arises\nfrom noise mismodeling, as well as the information loss represented by the\nbroadening of the posteriors. The result of noise mismodeling with gaps is\nsensitive to the characteristics of the noise model, with strong low-frequency\n(red) noise and strong high-frequency (blue) noise giving statistically\nsignificant fluctuations in recovered parameters. We demonstrate that the\nintroduction of a tapering window reduces statistical inconsistency errors, at\nthe cost of less precise parameter estimates. We also show that the assumption\nof independence between inter-gap segments appears to be a fair approximation\neven if the data set is inherently coherent. However, if one instead assumes\nfictitious correlations in the data stream, when the data segments are actually\nindependent, then the resultant parameter recoveries could be inconsistent with\nthe true parameters. The theoretical and numerical practices that are presented\nin this work could readily be incorporated into global-fit pipelines operating\non gapped data.",
        "Recurrent and propagating intensity perturbations are frequently observed in\nextreme ultraviolet (EUV) channels along coronal fan loops above sunspots, and\nthese perturbations are suggested to be slow magnetoacoustic waves. Numerous\nstudies have been conducted to investigate their propagation speeds, damping,\nand excitation sources; however, there have been limited observational analyses\non whether these waves are dispersive despite some theoretical studies. In this\nstudy, we apply cross-correlation analysis in the Fourier domain on slow\nmagnetoacoustic waves using three different datasets: EUV intensity observed by\nSDO\/AIA, differential emission measure (DEM) temperature maps, and Doppler\nvelocities from Hinode\/EIS spectrometer observations. The apparent phase\nvelocities of the waves, which are the plane-of-sky component of the waves'\nphase velocities, are derived as functions of frequency for all the three\ndatasets. It is found that the phase velocities show clear frequency\ndependency, with a general trend of increase with frequency, ranging from\napproximately 30 km\/s around 3 mHz to about 80 km\/s around 10 mHz. The\nfrequency dependency of the phase velocities demonstrates that the slow\nmagnetoacoustic waves in the coronal loops are dispersive. The dispersiveness\nof these waves can provide a useful tool for the diagnosis of physical\nconditions inside the coronal loops along which these waves travel.",
        "In a previous work general conditions were given to prove the infiniteness of\nthe genus of certain towers of function fields over a perfect field. It was\nshown that many examples where particular cases of those general results. In\nthis paper the genus of a family of wild towers of function fields will be\nconsidered together with a result with less restrictive sufficient conditions\nfor a wild tower to have infinite genus.",
        "More than fifty years ago, excitonic insulators, formed by the pairing of\nelectrons and holes due to Coulomb interactions, were first predicted. Since\nthen, excitonic insulators have been observed in various classes of materials,\nincluding quantum Hall bilayers, graphite, transition metal chalcogenides, and\nmore recently in moire superlattices. In these excitonic insulators, an\nelectron and a hole with the same spin bind together and the resulting exciton\nis a spin singlet. Here, we report the experimental observation of a\nspin-triplet exciton insulator in the ultra-quantum limit of a\nthree-dimensional topological material HfTe5. We observe that the\nspin-polarized zeroth Landau bands, dispersing along the field direction, cross\neach other beyond a characteristic magnetic field in HfTe5, forming the\none-dimensional Weyl mode. Transport measurements reveal the emergence of a gap\nof about 250 {\\mu}eV when the field surpasses a critical threshold. By\nperforming the material-specific modeling, we identify this gap as a\nconsequence of a spin-triplet exciton formation, where electrons and holes with\nopposite spin form bound states, and the translational symmetry is preserved.\nThe system reaches charge neutrality following the gap opening, as evidenced by\nthe zero Hall conductivity over a wide magnetic field range (10 - 72 T). Our\nfinding of the spin-triplet excitonic insulator paves the way for studying\nnovel spin transport including spin superfluidity, spin Josephson currents, and\nCoulomb drag of spin currents in analogy to the transport properties associated\nwith the layer pseudospin in quantum Hall bilayers.",
        "The calculation of gray-body factors is essential for understanding Hawking\nradiation and black hole thermodynamics. While the formalism developed by\nChandrasekhar is effective for static black holes, it faces significant\nchallenges in Kerr spacetimes, particularly in the superradiant regime, where a\nspecific choice of coordinates introduces numerical inaccuracies. To address\nthese limitations, an alternative method based on re-scaling radial coordinates\nand employing Frobenius-like expansions has been investigated. We compare the\ngray-body factors obtained for a near-maximally rotating black hole using both\nmethods and find that the Chandrasekhar formalism systematically overestimates\nthe values in the superradiant regime compared to well-established analytical\nresults. Specifically, for a spin parameter of $a_* = 0.999$, the Chandrasekhar\nmethod yields values approximately twice as large as the correct result. Since\nthis approach has been implemented in \\texttt{BlackHawk}, we assess the impact\nof these discrepancies on constraints derived from gamma-ray observations of\nhighly spinning primordial black holes.",
        "Aircraft-based surveying to collect airborne electromagnetic data is a key\nmethod to image large swaths of the Earth's surface in pursuit of better\nknowledge of aquifer systems. Despite many years of advancements, 3D inversion\nstill poses challenges in terms of computational requirements, regularization\nselection, hyperparameter tuning and real-time inversion. We present a new\napproach for the inversion of airborne electromagnetic data that leverages\nmachine learning to overcome the computational burden of traditional 3D\ninversion methods, which implicitly includes learned regularization and is\napplicable in real-time. The method combines 1D inversion results with\ngeostatistical modeling to create tailored training datasets, enabling the\ndevelopment of a specialized neural network that predicts 2D conductivity\nmodels from airborne electromagnetic data. This approach requires 3D forward\nmodeling and 1D inversion up front, but no forward modeling during inference.\nThe workflow is applied to the Kaweah Subbasin in California, where it\nsuccessfully reconstructs conductivity models consistent with real-world data\nand geological drill hole information. The results highlight the method's\ncapability to deliver fast and accurate subsurface imaging, offering a valuable\ntool for groundwater exploration and other near-surface applications.",
        "The recent interest in contextual optimization problems, where randomness is\nassociated with side information, has led to two primary strategies for\nformulation and solution. The first, estimate-then-optimize, separates the\nestimation of the problem's parameters from the optimization process. The\nsecond, decision-focused optimization, integrates the optimization problem's\nstructure directly into the prediction procedure. In this work, we propose a\npessimistic bilevel approach for solving general decision-focused formulations\nof combinatorial optimization problems. Our method solves an\n$\\varepsilon$-approximation of the pessimistic bilevel problem using a\nspecialized cut generation algorithm. We benchmark its performance on the 0-1\nknapsack problem against estimate-then-optimize and decision-focused methods,\nincluding the popular SPO+ approach. Computational experiments highlight the\nproposed method's advantages, particularly in reducing out-of-sample regret.",
        "Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution.",
        "In 2024, Bellamy, Craw, Rayan, Schedler, and Weiss introduced a particular\nfamily of real hyperplane arrangements stemming from hyperpolygonal spaces\nassociated with certain quiver varieties which we thus call hyperpolygonal\narrangements $\\mathcal H_n$. In this note we study these arrangements and\ninvestigate their properties systematically. Remarkably the arrangements\n$\\mathcal H_n$ discriminate between essentially all local properties of\narrangements. In addition we show that hyperpolygonal arrangements are\nprojectively unique and combinatorially formal.\n  We note that the arrangement $\\mathcal H_5$ is the famous counterexample of\nEdelman and Reiner from 1993 of Orlik's conjecture that the restriction of a\nfree arrangement is again free."
      ]
    }
  },
  {
    "id":2411.18902,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Biomechanics and motor control of human movement",
    "start_abstract":"Preface to the Fourth Edition. 1 Biomechanics as an Interdiscipline. 1.0 Introduction. 1.1 Measurement, Description, Analysis, and Assessment. 1.2 its Relationship with Physiology Anatomy. 1.3 Scope of Textbook. 1.4 References. 2 Signal Processing. 2.0 2.1 Auto- Cross-Correlation Analyses. 2.2 Frequency Analysis. 2.3 Ensemble Averaging Repetitive Waveforms. 2.4 3 Kinematics. 3.0 Historical Development Complexity Problem. 3.1 Kinematic Conventions. 3.2 Direct Measurement Techniques. 3.3 Imaging 3.4 Processing Raw Data. 3.5 Calculation Other Variables. 3.6 Problems Based on 3.7 4 Anthropometry. 4.0 Anthropometry in Movement Biomechanics. 4.1 Density, Mass, Inertial Properties. 4.2 Experimental Measures. 4.3 Muscle 4.4 Anthropometric 4.5 5 Kinetics: Forces Moments Force. 5.0 Biomechanical Models. 5.1 Basic Link-Segment Equations-the Free-Body Diagram. 5.2 Force Transducers Plates. 5.3 Bone-on-Bone During Dynamic Conditions. 5.4 Kinetic 5.5 6 Mechanical Work, Energy, Power. 6.0 6.1 Efficiency. 6.2 Forms Energy Storage. 6.3 Internal External Work. 6.4 Power Balances at Joints Within Segments. 6.5 6.6 7 Three-Dimensional Kinematics Kinetics. 7.0 7.1 Axes Systems. 7.2 Marker Anatomical 7.3 Determination Segment Angular Velocities Accelerations. 7.4 Analysis Reaction Moments. 7.5 Suggested Further Reading. 7.6 8 Synthesis Human Movement-Forward Solutions. 8.0 8.1 Review Forward Solution 8.2 Mathematical Formulation. 8.3 System Energy. 8.4 Torques. 8.5 Designation Joints. 8.6 Illustrative Example. 8.7 Conclusions. 8.8 9 Mechanics. 9.0 9.1 Force-Length Characteristics Muscles. 9.2 Force-Velocity Characteristics. 9.3 Modeling. 9.4 10 Kinesiological Electromyography. 10.0 10.1 Electrophysiology Contraction. 10.2 Recording Electromyogram. 10.3 Electromyogram,. 10.4 between Electromyogram 10.5 11 Synergies. 11.0 11.1 The Support Moment Synergy. 11.2 Medial\/Lateral Anterior\/Posterior Balance Standing. 11.3 during Walking. 11.4 APPENDICES. A. Kinematic, Kinetic, Figure A.1 Walking Trial-Marker Locations Mass Frame Rate Information. Table Coordinate Data (cm). A.2( a ) Filtered Kinematics-Rib Cage Greater Trochanter (Hip). b Kinematics-Femoral Lateral Epicondyle (Knee) Head Fibula. c Kinematics-Lateral Malleolus (Ankle) Heel. d Kinematics-Fifth Metatarsal Toe. A.3( Linear Kinematics-Foot. Kinematics-Leg. Kinematics-Thigh. Kinematics-1\/2 HAT. A.4 Relative Joint Kinematics-Ankle, Knee, Hip. A.5( Force-Ankle Knee. Force-Hip. A.6 Potential, Total Energies-Foot, Leg, Thigh, and1\/2 A.7 Generation\/Absorption Transfer-Ankle, B. Units Definitions Related Electromyographical Measurements. B.1 Base SI Units. B.2 Derived Index.",
    "start_categories":[
      "Biomechanics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      ],
      "abstract":[
        "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution recurrent structured state space models (SSMs) have been developed to address Transformers' computational inefficiency long sequences, but they not performed well important modalities language. We identify that a key weakness is their inability perform content-based reasoning, make several improvements. First, simply letting SSM parameters be functions input addresses with discrete modalities, allowing model selectively propagate or forget information along sequence length dimension depending current token. Second, even though this change prevents use efficient convolutions, we design hardware-aware parallel algorithm mode. integrate these selective SSMs into simplified end-to-end neural network without MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) scaling length, performance improves real data up million-length sequences. As general backbone, achieves state-of-the-art across language, audio, genomics. On language modeling, our Mamba-3B outperforms Transformers same size matches twice size, both pretraining downstream evaluation."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via\n  Multimodal Visual Feature Learning",
        "PLRD: Partially Linear Regression Discontinuity Inference",
        "Non-conservation of linear momentum in widely used hierarchical methods\n  in gravitational gas dynamics",
        "A-priori estimates for generalized Korteweg-de Vries equations in\n  $H^{-1}(\\mathbb{R})$",
        "Two-dimensional antiferromagnets with non-relativistic spin splitting\n  switchable by electric polarization",
        "Network evasion detection with Bi-LSTM model",
        "Gradient Multi-Normalization for Stateless and Scalable LLM Training",
        "Automated Hypothesis Validation with Agentic Sequential Falsifications",
        "Stellar Population and Metal Production in AGN Disks",
        "Dragging of inertial frames in the composed Kerr-Newman-orbiting-ring\n  system",
        "FeatSharp: Your Vision Model Features, Sharper",
        "Enhancing Zero-Shot Image Recognition in Vision-Language Models through\n  Human-like Concept Guidance",
        "Resonances of compressible stars in precessing orbits around a spinning\n  black hole",
        "An exposition of recent list-size bounds of FRS Codes",
        "High-Efficiency Multilevel Phase Lenses with Nanostructures on Polyimide\n  Membranes",
        "On the momentum space structure of the quark propagator",
        "Tensor Product Neural Networks for Functional ANOVA Model",
        "Degree is Important: On Evolving Homogeneous Boolean Functions",
        "Entropy functionals and equilibrium states in mixed quantum-classical\n  dynamics",
        "The Scott space of lattice of closed subsets with supremum operator as a\n  topological semilattice",
        "Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation\n  Problems",
        "Geant4 and FLUKA Simulations of a Cyclotron Based 30 MeV\n  Proton-Beryllium Reaction: Benchmarking and Optimization of Neutron Fields",
        "Discord's Design Encourages \"Third Place\" Social Media Experiences",
        "Reducing Friction in Cloud Migration of Services",
        "Intercalated structures formed by platinum on epitaxial graphene on\n  SiC(0001)",
        "HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation",
        "Improve Representation for Imbalanced Regression through Geometric\n  Constraints",
        "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM\n  Kernels",
        "Interpretability Analysis of Domain Adapted Dense Retrievers"
      ],
      "abstract":[
        "Real-time ego-motion tracking for endoscope is a significant task for\nefficient navigation and robotic automation of endoscopy. In this paper, a\nnovel framework is proposed to perform real-time ego-motion tracking for\nendoscope. Firstly, a multi-modal visual feature learning network is proposed\nto perform relative pose prediction, in which the motion feature from the\noptical flow, the scene features and the joint feature from two adjacent\nobservations are all extracted for prediction. Due to more correlation\ninformation in the channel dimension of the concatenated image, a novel feature\nextractor is designed based on an attention mechanism to integrate\nmulti-dimensional information from the concatenation of two continuous frames.\nTo extract more complete feature representation from the fused features, a\nnovel pose decoder is proposed to predict the pose transformation from the\nconcatenated feature map at the end of the framework. At last, the absolute\npose of endoscope is calculated based on relative poses. The experiment is\nconducted on three datasets of various endoscopic scenes and the results\ndemonstrate that the proposed method outperforms state-of-the-art methods.\nBesides, the inference speed of the proposed method is over 30 frames per\nsecond, which meets the real-time requirement. The project page is here:\nremote-bmxs.netlify.app",
        "Regression discontinuity designs have become one of the most popular research\ndesigns in empirical economics. We argue, however, that widely used approaches\nto building confidence intervals in regression discontinuity designs exhibit\nsuboptimal behavior in practice: In a simulation study calibrated to\nhigh-profile applications of regression discontinuity designs, existing methods\neither have systematic under-coverage or have wider-than-necessary intervals.\nWe propose a new approach, partially linear regression discontinuity inference\n(PLRD), and find it to address shortcomings of existing methods: Throughout our\nexperiments, confidence intervals built using PLRD are both valid and short. We\nalso provide large-sample guarantees for PLRD under smoothness assumptions.",
        "The paper considers the implementation of the fast multipole method (FMM) in\nthe PHANTOM code for the calculation of forces in a self-gravitating system.\nThe gravitational interaction forces are divided into short-range and\nlong-range interactions depending on the value of the tree opening parameter of\nthe hierarchical kd-tree. It is demonstrated that Newton's third law holds for\nany pair of cells of the kd-tree engaged in mutual interaction. However, for\nthe entire system a linear momentum is not conserved. As a result, there is an\nunphysical force that causes the center of mass to migrate. For example, for a\npair of neutron stars, the displacement of the system's center of mass is found\nto be comparable to the radii of the objects at times of a few tens of\nKeplerian revolutions. This displacement cannot be reduced by increasing the\nnumber of particles for values of the tree opening parameter greater than 0.2.\nFor smaller values, the time required for the calculation is significantly\nlonger.",
        "We prove local-in-time a-priori estimates in $H^{-1}(\\mathbb{R})$ for a\nfamily of generalized Korteweg--de Vries equations. This is the first estimate\nfor any non-integrable perturbation of the KdV equation that matches the\nregularity of the sharp well-posedness theory for KdV. In particular, we show\nthat our analysis applies to models for long waves in a shallow channel of\nwater with an uneven bottom.\n  The proof of our main result is based upon a bootstrap argument for the\nrenormalized perturbation determinant coupled with a local smoothing norm.",
        "Spin-split antiferromagnets have significance for antiferromagnetic (AFM)\nspintronics due to their momentum dependent spin polarization which can be\nexploited for the control and detection of the AFM order parameter. Here, we\nexplore the polar-layer stacking of AFM-ordered bilayers driving the emergence\nof reversable electric polarization and non-relativistic spin splitting (NRSS)\nof their band structure. Based on the spin-space group approach, we identify\nseveral representative two-dimensional AFM materials which exhibit different\ntypes of NRSS when stacked into a polar bilayer. We demonstrate that NRSS can\nhave both altermagnetic and non-altermagnetic origins and elucidate symmetry\nrequirements for NRSS to be switchable by electric polarization. We argue that\nthe electric polarization switching of NRSS in polar AFM bilayers may be more\npractical for device applications than the current-induced N\\'eel vector\nswitching.",
        "Network evasion detection aims to distinguish whether the network flow comes\nfrom link layer exists network evasion threat, which is a means to disguise the\ndata traffic on detection system by confusing the signature. Since the previous\nresearch works has all sorts of frauds, we propose a architecture with deep\nlearning network to handle this problem. In this paper, we extract the critical\ninformation as key features from data frame and also specifically propose to\nuse bidirectional long short-term memory (Bi-LSTM) neural network which shows\nan outstanding performance to trace the serial information, to encode both the\npast and future trait on the network flows. Furthermore we introduce a\nclassifier named Softmax at the bottom of Bi-LSTM, holding a character to\nselect the correct class. All experiments results shows that we can achieve a\nsignificant performance with a deep Bi-LSTM in network evasion detection and\nit's average accuracy reaches 96.1%.",
        "Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening\/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines.",
        "Hypotheses are central to information acquisition, decision-making, and\ndiscovery. However, many real-world hypotheses are abstract, high-level\nstatements that are difficult to validate directly. This challenge is further\nintensified by the rise of hypothesis generation from Large Language Models\n(LLMs), which are prone to hallucination and produce hypotheses in volumes that\nmake manual validation impractical. Here we propose Popper, an agentic\nframework for rigorous automated validation of free-form hypotheses. Guided by\nKarl Popper's principle of falsification, Popper validates a hypothesis using\nLLM agents that design and execute falsification experiments targeting its\nmeasurable implications. A novel sequential testing framework ensures strict\nType-I error control while actively gathering evidence from diverse\nobservations, whether drawn from existing data or newly conducted procedures.\nWe demonstrate Popper on six domains including biology, economics, and\nsociology. Popper delivers robust error control, high power, and scalability.\nFurthermore, compared to human scientists, Popper achieved comparable\nperformance in validating complex biological hypotheses while reducing time by\n10 folds, providing a scalable, rigorous solution for hypothesis validation.",
        "As gravitational wave detections increase the number of observed compact\nbinaries (consisting of neutron stars or blacks), we begin to probe the\ndifferent conditions producing these binaries. Most studies of compact remnant\nformation focus either on stellar collapse from the evolution of field binary\nstars in gas-free environments or the formation of stars in clusters where\ndynamical interactions capture the compact objects, forming binaries. But a\nthird scenario exists. In this paper, we study the fate of massive stars\nformed, accrete gas, and evolve in the dense disks surrounding supermassive\nblack holes. We calculate the explosions produced and compact objects formed by\nthe collapse of these massive stars. Nucleosynthetic yields may provide an\nideal, directly observable, diagnostic of the formation and fate of these stars\nin active galactic nuclei. We present a first study of the explosive yields\nfrom these stars, comparing these yields with the observed nucleosynthetic\nsignatures in the disks around supermassive stars with quasars. We show that,\neven though these stars tend to form black holes, their rapid rotation leads to\ndisks that can eject a considerable amount of iron during the collapse of the\nstar. The nucleosynthetic yields from these stars can produce constraints on\nthe number of systems formed in this manner, but further work is needed to\nexploit variations from the initial models presented in this paper.",
        "The dragging of inertial frames by an orbiting object implies that the\nhorizon angular velocity $\\Omega^{\\text{BH-ring}}_{\\text{H}}$ of a central\nblack hole in a composed black-hole-orbiting-ring system is no longer related\nto its angular-momentum $J_{\\text{H}}$ by the familiar vacuum functional\nrelation $\\Omega_{\\text{H}}(J_{\\text{H}})=J_{\\text{H}}\/M\\alpha$ (here\n$\\{M,\\alpha\\}$ are respectively the mass and normalized area of the central\nspinning black hole). Using a continuity argument, it has recently been\nrevealed that the composed Kerr-ring system is characterized by the universal\n(that is, spin-{\\it independent}) relation\n$\\Delta\\Omega_{\\text{H}}\\equiv\\Omega^{\\text{BH-ring}}_{\\text{H}}(J_{\\text{H}},J_{\\text{R}},R\\to\nR^{+}_{\\text{H}})-\\Omega^{\\text{Kerr}}_{\\text{H}}(J_{\\text{H}})={{J_{\\text{R}}}\/{4M^3}}$,\nwhere $\\{R,J_{\\text{R}}\\}$ are respectively the radius of the ring and its\norbital angular momentum and $R_{\\text{H}}$ is the horizon radius of the\ncentral Kerr black hole. This intriguing observation naturally raises the\nfollowing physically interesting question: Does the physical quantity\n$\\Delta\\Omega_{\\text{H}}$ in a composed black-hole-orbiting-ring system is\nalways characterized by the near-horizon functional relation\n$\\Delta\\Omega_{\\text{H}}={{J_{\\text{R}}}\/{4M^3}}$ which is independent of the\nspin (angular momentum) $J_{\\text{H}}$ of the central black hole? In the\npresent compact paper we explore the physical phenomenon of dragging of\ninertial frames by an orbiting ring in the composed\nKerr-Newman-black-hole-orbiting-ring system. In particular, using analytical\ntechniques, we reveal the fact that in this composed two-body (black-hole-ring)\nsystem the quantity $\\Delta\\Omega_{\\text{H}}$ has an explicit non-trivial\nfunctional dependence on the angular momentum $J_{\\text{H}}$ of the central\nspinning black hole.",
        "The feature maps of vision encoders are fundamental to myriad modern AI\ntasks, ranging from core perception algorithms (e.g. semantic segmentation,\nobject detection, depth perception, etc.) to modern multimodal understanding in\nvision-language models (VLMs). Currently, in computer vision, the frontier of\ngeneral purpose vision backbones are Vision Transformers (ViT), typically\ntrained using contrastive loss (e.g. CLIP). A key problem with most\noff-the-shelf ViTs, particularly CLIP, is that these models are inflexibly low\nresolution. Most run at 224x224px, while the \"high resolution\" versions are\naround 378-448px, but still inflexible. We introduce a novel method to\ncoherently and cheaply upsample the feature maps of low-res vision encoders\nwhile picking up on fine-grained details that would otherwise be lost due to\nresolution. We demonstrate the effectiveness of this approach on core\nperception tasks as well as within agglomerative model (RADIO) training as a\nway of providing richer targets for distillation.",
        "In zero-shot image recognition tasks, humans demonstrate remarkable\nflexibility in classifying unseen categories by composing known simpler\nconcepts. However, existing vision-language models (VLMs), despite achieving\nsignificant progress through large-scale natural language supervision, often\nunderperform in real-world applications because of sub-optimal prompt\nengineering and the inability to adapt effectively to target classes. To\naddress these issues, we propose a Concept-guided Human-like Bayesian Reasoning\n(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in\nhuman image recognition as latent variables and formulates this task by summing\nacross potential concepts, weighted by a prior distribution and a likelihood\nfunction. To tackle the intractable computation over an infinite concept space,\nwe introduce an importance sampling algorithm that iteratively prompts large\nlanguage models (LLMs) to generate discriminative concepts, emphasizing\ninter-class differences. We further propose three heuristic approaches\ninvolving Average Likelihood, Confidence Likelihood, and Test Time Augmentation\n(TTA) Likelihood, which dynamically refine the combination of concepts based on\nthe test image. Extensive evaluations across fifteen datasets demonstrate that\nCHBR consistently outperforms existing state-of-the-art zero-shot\ngeneralization methods.",
        "In our previous paper, we reported the presence of a new resonance of an\nincompressible star orbiting a spinning black hole and showed that it can set\nin before the tidal disruption limit if the star has an inclined spherical\norbit around the black hole. Using the affine model developed by Carter and\nLuminet, we extend our result to the stars with polytropic equations of state.\nWe give further credence to the result previously given. We also derive the\nformula for the growth rate of the resonant motion, which is useful for\nchecking the results of hydrodynamics simulations.",
        "In the last year, there have been some remarkable improvements in the\ncombinatorial list-size bounds of Folded Reed Solomon codes and multiplicity\ncodes. Starting from the work on Kopparty, Ron-Zewi, Saraf and Wootters (SIAM\nJ. Comput. 2023) (and subsequent simplifications due to Tamo (IEEE Trans.\nInform. Theory 2024), we have had dramatic improvements in the list-size bounds\nof FRS codes due to Srivastava (SODA 2025) and Chen & Zhang (STOC 2025). In\nthis note, we give a short exposition of these three results (Tamo, Srivastava\nand Chen-Zhang).",
        "The emergence of planar meta-lenses on flexible materials has profoundly\nimpacted the long-standing perception of diffractive optics. Despite their\nadvantages, these lenses still face challenges in design and fabrication to\nobtain high focusing efficiency and resolving power. A nanofabrication\ntechnique is demonstrated based on photolithography and polyimide casting for\nrealizing membrane-based multilevel phase-type Fresnel zone plates (FZPs) with\nhigh focusing efficiency. By employing advantageous techniques, these lenses\nwith nanostructures are directly patterned into thin polyimide membranes. The\ncomputational and experimental results have indicated that the focusing\nefficiency of these nanostructures at the primary focus increases significantly\nwith increasing the number of phase levels. Specifically, 16-level phase lenses\non a polyimide membrane can achieve a focusing efficiency of more than 91.6% of\nthe input signal (9.5 times better than that of a conventional amplitude-type\nFZP) and focus light into a diffraction-limited spot together with very weak\nside-lobes. Furthermore, these lenses exhibit considerably reduced unwanted\ndiffraction orders and produce extremely low background signals. The potential\nimpact of these lenses extends across various applications and techniques\nincluding microscopy, imaging, micro-diffraction, remote sensing, and space\nflight instruments which require lightweight and flexible configurations.",
        "The structure of the quark propagator in momentum space is explored taking\ninto account non-perturbative QCD dynamics constraints for the quark spectral\ndensities derived previously. We assume that the scalar and vector component of\nthe quark propagator share a simple pole but not its residuum, together with\nother structures. Furthermore, a connection between the poles of the quark\npropagator and the zeros of the quark wave function $Z(p^2)$ is established.\nAsymptotic scaling laws for the representation of the quark propagator, after\nremoving the shared pole, are also derived. The confrontation of our results\nwith lattice data for the full QCD quark propagator data are in good agreement.\nExploring the link with the lattice data and looking at the Bethe-Salpeter\nvertex and amplitude, in the chiral limit, we are able to provide estimations\nfor these quantities, for $f_\\pi$ and for the shared pole mass. The pole mass\nreproduces the constituent quark mass used in the quark models.",
        "Interpretability for machine learning models is becoming more and more\nimportant as machine learning models become more complex. The functional ANOVA\nmodel, which decomposes a high-dimensional function into a sum of lower\ndimensional functions (commonly referred to as components), is one of the most\npopular tools for interpretable AI, and recently, various neural networks have\nbeen developed for estimating each component in the functional ANOVA model.\nHowever, such neural networks are highly unstable when estimating each\ncomponent since the components themselves are not uniquely defined. That is,\nthere are multiple functional ANOVA decompositions for a given function. In\nthis paper, we propose a novel neural network which guarantees a unique\nfunctional ANOVA decomposition and thus is able to estimate each component\nstably and accurately. We call our proposed neural network ANOVA Tensor Product\nNeural Network (ANOVA-TPNN) since it is motivated by the tensor product basis\nexpansion. Theoretically, we prove that ANOVA-TPNN can approximate any smooth\nfunction well. Empirically, we show that ANOVA-TPNN provide much more stable\nestimation of each component and thus much more stable interpretation when\ntraining data and initial values of the model parameters vary than existing\nneural networks do.",
        "Boolean functions with good cryptographic properties like high nonlinearity\nand algebraic degree play an important in the security of stream and block\nciphers. Such functions may be designed, for instance, by algebraic\nconstructions or metaheuristics. This paper investigates the use of\nEvolutionary Algorithms (EAs) to design homogeneous bent Boolean functions,\ni.e., functions that are maximally nonlinear and whose algebraic normal form\ncontains only monomials of the same degree. In our work, we evaluate three\ngenotype encodings and four fitness functions. Our results show that while EAs\nmanage to find quadratic homogeneous bent functions (with the best method being\na GA leveraging a restricted encoding), none of the approaches result in cubic\nhomogeneous bent functions.",
        "The computational challenges posed by many-particle quantum systems are often\novercome by mixed quantum-classical (MQC) models in which certain degrees of\nfreedom are treated as classical while others are retained as quantum. One of\nthe fundamental questions raised by this hybrid picture involves the\ncharacterization of the information associated to MQC systems. Based on the\ntheory of dynamical invariants in Hamiltonian systems, here we propose a family\nof hybrid entropy functionals that consistently specialize to the usual R\\'enyi\nand Shannon entropies. Upon considering the MQC Ehrenfest model for the\ndynamics of quantum and classical probabilities, we apply the hybrid Shannon\nentropy to characterize equilibrium configurations for simple Hamiltonians. The\npresent construction also applies beyond Ehrenfest dynamics.",
        "We present several equivalent conditions of the continuity of the supremum\nfunction $\\Sigma C(X)\\times\\Sigma C(X)\\rightarrow\\Sigma C(X)$ under mild\nassumptions, where $C(X)$ denotes the lattice of closed subsets of a $T_0$\ntopological space.\n  We also provide an example of a non-monotone determined space $X$ such that\n$\\eta=\\lambda x.{\\downarrow}x\\colon X\\rightarrow\\Sigma C(X)$ is continuous.\nAdditionally, we show that a $T_0$ space is quasicontinuous (quasialgebraic)\niff the lattice of its closed subsets is a quasicontinuous (quasialgebraic)\ndomain by using $n$-approximation. Furthermore, we provide a necessary\ncondition for when a topological space possesses a Scott completion. This\nallows us to give more examples which do not have Scott completions.",
        "We consider a learning-augmented framework for NP-hard permutation problems.\nThe algorithm has access to predictions telling, given a pair $u,v$ of\nelements, whether $u$ is before $v$ or not in an optimal solution. Building on\nthe work of Braverman and Mossel (SODA 2008), we show that for a class of\noptimization problems including scheduling, network design and other graph\npermutation problems, these predictions allow to solve them in polynomial time\nwith high probability, provided that predictions are true with probability at\nleast $1\/2+\\epsilon$. Moreover, this can be achieved with a parsimonious access\nto the predictions.",
        "For studies where a reliable neutron source\/beam is required and a nuclear\nreactor is not a viable option (considering their high neutron flux supply,\nwhich may not be suitable for research concerning low flux operations),\nalternative approaches may be sought. This paper presents a comprehensive\nsimulation analysis of 30 MeV 9Be(p,n)9B reaction, which can be utilized as an\nisotropic neutron source. Due to different underlying physics and transport\nmechanisms, slight variations in findings between Geant4 and FLUKA may occur,\nand hence may require one to have a guide at hand to address the differences\nand interpret the data more accurately before conducting the actual experiment.\nThrough these Monte Carlo simulation toolkits, this work estimates resulting\nneutron fluence and dose equivalent of prompt gamma decays, as well as how\nmaterials such as paraffin and borated polyethylene moderates its energy\nspectrum. Furthermore, this work also presents an exemplar modular irradiation\nstation designed for target-moderator configurations, with the capability of\ngenerating thermal neutron fields .",
        "In light of the diminishing presence of physical third places -- informal\ngathering spaces essential for social connection -- this study explores how the\nsocial media platform Discord fosters third-place experiences. Drawing on\nOldenburg's conceptual framework, we analyze how Discord's design elements\nsupport the creation of virtual third places that foster both dyadic and\ncommunity-based relationships. Through 25 semi-structured interviews with\nactive Discord users, we identified 21 design elements aligned with Oldenburg's\nthird-place characteristics. These elements cluster around four core\nprinciples: providing themed spaces for repeated interactions, supporting user\nautonomy and customization, facilitating mutually engaging activities, and\nenabling casual, low-pressure interactions. This work contributes to\nunderstanding how intentional platform design can cultivate virtual spaces that\nsupport meaningful social connections. The findings have implications for\ndesigning future social technologies that can help address growing concerns\nabout social isolation in an increasingly digital world.",
        "Public cloud services are integral to modern software development, offering\nscalability and flexibility to organizations. Based on customer requests, a\nlarge-scale product development organization considered migrating the\nmicroservice-based product deployments of a large customer to a public cloud\nprovider.\n  We conducted an exploratory single-case study, utilizing quantitative and\nqualitative data analysis to understand how and why deployment costs would\nchange when transitioning the product from a private to a public cloud\nenvironment while preserving the software architecture. We also isolated the\nmajor factors driving the changes in deployment costs.\n  We found that switching to the customer-chosen public cloud provider would\nincrease costs by up to 50\\%, even when sharing some resources between\ndeployments, and limiting the use of expensive cloud services such as security\nlog analyzers. A large part of the cost was related to the sizing and license\ncosts of the existing relational database, which was running on Virtual\nMachines in the cloud. We also found that existing system integrators, using\nthe product via its API, were likely to use the product inefficiently, in many\ncases causing at least 10\\% more load to the system than needed.\n  From a deployment cost perspective, successful migration to a public cloud\nrequires considering the entire system architecture, including services like\nrelational databases, value-added cloud services, and enabled product features.\nOur study highlights the importance of leveraging end-to-end usage data to\nassess and manage these cost drivers effectively, especially in environments\nwith elastic costs, such as public cloud deployments.",
        "Graphene on SiC intercalated with two-dimensional metal layers, such as Pt,\noffers a versatile platform for applications in spintronics, catalysis, and\nbeyond. Recent studies have demonstrated that Pt atoms can intercalate at the\nheterointerface between SiC(0001) and the C-rich\n$(6\\sqrt{3}\\times6\\sqrt{3})$R30{\\deg} reconstructed surface (hereafter referred\nas the buffer layer). However, key aspects such as intercalated phase structure\nand intercalation mechanisms remain unclear. In this work, we investigate\nchanges in morphology, chemistry, and electronic structure for both buffer\nlayer and monolayer graphene grown on SiC(0001) following Pt deposition and\nannealing cycles, which eventually led to Pt intercalation at temperatures\nabove 500{\\deg}C. Atomic-resolution imaging of the buffer layer reveals a\nsingle intercalated Pt layer that removes the periodic corrugation of the\nbuffer layer, arising from partial bonding of C-atoms with Si-atoms of the\nsubstrate. In monolayer graphene, the Pt-intercalated regions exhibit a\ntwo-level structure: the first level corresponds to a Pt layer intercalated\nbelow the buffer layer, while the second level contains a second Pt layer,\ngiving rise to a $(12\\times12)$ superstructure relative to graphene. Upon\nintercalation, Pt atoms appear as silicides, indicating a reaction with Si\natoms from the substrate. Additionally, charge neutral $\\pi$-bands\ncorresponding to quasi-free-standing monolayer and bilayer graphene emerge.\nAnalysis of multiple samples, coupled with a temperature-dependent study of the\nintercalation rate, demonstrates the pivotal role of buffer layer regions in\nfacilitating the Pt intercalation in monolayer graphene. These findings provide\nvaluable insight into Pt intercalation, advancing the potential for\napplications.",
        "This paper introduces the Hierarchical Kolmogorov-Arnold Network (HKAN), a\nnovel network architecture that offers a competitive alternative to the\nrecently proposed Kolmogorov-Arnold Network (KAN). Unlike KAN, which relies on\nbackpropagation, HKAN adopts a randomized learning approach, where the\nparameters of its basis functions are fixed, and linear aggregations are\noptimized using least-squares regression. HKAN utilizes a hierarchical\nmulti-stacking framework, with each layer refining the predictions from the\nprevious one by solving a series of linear regression problems. This\nnon-iterative training method simplifies computation and eliminates sensitivity\nto local minima in the loss function. Empirical results show that HKAN delivers\ncomparable, if not superior, accuracy and stability relative to KAN across\nvarious regression tasks, while also providing insights into variable\nimportance. The proposed approach seamlessly integrates theoretical insights\nwith practical applications, presenting a robust and efficient alternative for\nneural network modeling.",
        "In representation learning, uniformity refers to the uniform feature\ndistribution in the latent space (i.e., unit hypersphere). Previous work has\nshown that improving uniformity contributes to the learning of\nunder-represented classes. However, most of the previous work focused on\nclassification; the representation space of imbalanced regression remains\nunexplored. Classification-based methods are not suitable for regression tasks\nbecause they cluster features into distinct groups without considering the\ncontinuous and ordered nature essential for regression. In a geometric aspect,\nwe uniquely focus on ensuring uniformity in the latent space for imbalanced\nregression through two key losses: enveloping and homogeneity. The enveloping\nloss encourages the induced trace to uniformly occupy the surface of a\nhypersphere, while the homogeneity loss ensures smoothness, with\nrepresentations evenly spaced at consistent intervals. Our method integrates\nthese geometric principles into the data representations via a Surrogate-driven\nRepresentation Learning (SRL) framework. Experiments with real-world regression\nand operator learning tasks highlight the importance of uniformity in\nimbalanced regression and validate the efficacy of our geometry-based loss\nfunctions.",
        "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels.\nLeveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear\nAttention (FLA) shows that linear RNN kernels are faster than Flash Attention,\nby parallelizing over chunks of the input sequence. However, since the chunk\nsize of FLA is limited, many intermediate states must be materialized in GPU\nmemory. This leads to low arithmetic intensity and causes high memory\nconsumption and IO cost, especially for long-context pre-training. In this\nwork, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm\nfor linear RNNs, that enables arbitrary large chunk sizes by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM\nvariant with sigmoid input gate and reduced computation for even faster kernel\nruntimes at equal language modeling performance. In our speed benchmarks, we\nshow that our new mLSTM kernels based on TFLA outperform highly optimized Flash\nAttention, Linear Attention and Mamba kernels, setting a new state of the art\nfor efficient long-context sequence modeling primitives.",
        "Dense retrievers have demonstrated significant potential for neural\ninformation retrieval; however, they exhibit a lack of robustness to domain\nshifts, thereby limiting their efficacy in zero-shot settings across diverse\ndomains. Previous research has investigated unsupervised domain adaptation\ntechniques to adapt dense retrievers to target domains. However, these studies\nhave not focused on explainability analysis to understand how such adaptations\nalter the model's behavior. In this paper, we propose utilizing the integrated\ngradients framework to develop an interpretability method that provides both\ninstance-based and ranking-based explanations for dense retrievers. To generate\nthese explanations, we introduce a novel baseline that reveals both query and\ndocument attributions. This method is used to analyze the effects of domain\nadaptation on input attributions for query and document tokens across two\ndatasets: the financial question answering dataset (FIQA) and the biomedical\ninformation retrieval dataset (TREC-COVID). Our visualizations reveal that\ndomain-adapted models focus more on in-domain terminology compared to\nnon-adapted models, exemplified by terms such as \"hedge,\" \"gold,\" \"corona,\" and\n\"disease.\" This research addresses how unsupervised domain adaptation\ntechniques influence the behavior of dense retrievers when adapted to new\ndomains. Additionally, we demonstrate that integrated gradients are a viable\nchoice for explaining and analyzing the internal mechanisms of these opaque\nneural models."
      ]
    }
  },
  {
    "id":2411.18902,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "start_abstract":"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution recurrent structured state space models (SSMs) have been developed to address Transformers' computational inefficiency long sequences, but they not performed well important modalities language. We identify that a key weakness is their inability perform content-based reasoning, make several improvements. First, simply letting SSM parameters be functions input addresses with discrete modalities, allowing model selectively propagate or forget information along sequence length dimension depending current token. Second, even though this change prevents use efficient convolutions, we design hardware-aware parallel algorithm mode. integrate these selective SSMs into simplified end-to-end neural network without MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) scaling length, performance improves real data up million-length sequences. As general backbone, achieves state-of-the-art across language, audio, genomics. On language modeling, our Mamba-3B outperforms Transformers same size matches twice size, both pretraining downstream evaluation.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Biomechanics and motor control of human movement"
      ],
      "abstract":[
        "Preface to the Fourth Edition. 1 Biomechanics as an Interdiscipline. 1.0 Introduction. 1.1 Measurement, Description, Analysis, and Assessment. 1.2 its Relationship with Physiology Anatomy. 1.3 Scope of Textbook. 1.4 References. 2 Signal Processing. 2.0 2.1 Auto- Cross-Correlation Analyses. 2.2 Frequency Analysis. 2.3 Ensemble Averaging Repetitive Waveforms. 2.4 3 Kinematics. 3.0 Historical Development Complexity Problem. 3.1 Kinematic Conventions. 3.2 Direct Measurement Techniques. 3.3 Imaging 3.4 Processing Raw Data. 3.5 Calculation Other Variables. 3.6 Problems Based on 3.7 4 Anthropometry. 4.0 Anthropometry in Movement Biomechanics. 4.1 Density, Mass, Inertial Properties. 4.2 Experimental Measures. 4.3 Muscle 4.4 Anthropometric 4.5 5 Kinetics: Forces Moments Force. 5.0 Biomechanical Models. 5.1 Basic Link-Segment Equations-the Free-Body Diagram. 5.2 Force Transducers Plates. 5.3 Bone-on-Bone During Dynamic Conditions. 5.4 Kinetic 5.5 6 Mechanical Work, Energy, Power. 6.0 6.1 Efficiency. 6.2 Forms Energy Storage. 6.3 Internal External Work. 6.4 Power Balances at Joints Within Segments. 6.5 6.6 7 Three-Dimensional Kinematics Kinetics. 7.0 7.1 Axes Systems. 7.2 Marker Anatomical 7.3 Determination Segment Angular Velocities Accelerations. 7.4 Analysis Reaction Moments. 7.5 Suggested Further Reading. 7.6 8 Synthesis Human Movement-Forward Solutions. 8.0 8.1 Review Forward Solution 8.2 Mathematical Formulation. 8.3 System Energy. 8.4 Torques. 8.5 Designation Joints. 8.6 Illustrative Example. 8.7 Conclusions. 8.8 9 Mechanics. 9.0 9.1 Force-Length Characteristics Muscles. 9.2 Force-Velocity Characteristics. 9.3 Modeling. 9.4 10 Kinesiological Electromyography. 10.0 10.1 Electrophysiology Contraction. 10.2 Recording Electromyogram. 10.3 Electromyogram,. 10.4 between Electromyogram 10.5 11 Synergies. 11.0 11.1 The Support Moment Synergy. 11.2 Medial\/Lateral Anterior\/Posterior Balance Standing. 11.3 during Walking. 11.4 APPENDICES. A. Kinematic, Kinetic, Figure A.1 Walking Trial-Marker Locations Mass Frame Rate Information. Table Coordinate Data (cm). A.2( a ) Filtered Kinematics-Rib Cage Greater Trochanter (Hip). b Kinematics-Femoral Lateral Epicondyle (Knee) Head Fibula. c Kinematics-Lateral Malleolus (Ankle) Heel. d Kinematics-Fifth Metatarsal Toe. A.3( Linear Kinematics-Foot. Kinematics-Leg. Kinematics-Thigh. Kinematics-1\/2 HAT. A.4 Relative Joint Kinematics-Ankle, Knee, Hip. A.5( Force-Ankle Knee. Force-Hip. A.6 Potential, Total Energies-Foot, Leg, Thigh, and1\/2 A.7 Generation\/Absorption Transfer-Ankle, B. Units Definitions Related Electromyographical Measurements. B.1 Base SI Units. B.2 Derived Index."
      ],
      "categories":[
        "Biomechanics"
      ]
    },
    "list":{
      "title":[
        "High efficiency veto hadron calorimeter in the NA64 experiment at CERN",
        "Statistical data analysis for Tourism in Poland in R Programming\n  Environment",
        "Guiding Time-Varying Generative Models with Natural Gradients on\n  Exponential Family Manifold",
        "Intertwined Nematicity, Multiferroicity, and Nonlinear Hall Effect in\n  Rhombohedral Pentalayer Graphene",
        "Neural quantum embedding via deterministic quantum computation with one\n  qubit",
        "Silica Aerogel Thin Film on Improving Solar Cell Efficiency",
        "Interacting mesons as degrees of freedom in a chiral model",
        "Shh, don't say that! Domain Certification in LLMs",
        "Moduli of sheaves on ribbons",
        "QED nuclear medium effects at EIC energies",
        "Hybrid Schwarz preconditioners for linear systems arising from\n  hp-discontinuous Galerkin method",
        "Active magneto-mechanical metamaterial with the wave transmission and\n  Poisson's ratio controlled via the magnetic field",
        "On $(\\mathcal{L},\\mathcal{P})$-Twisted Generalized Reed-Solomon Codes",
        "Exponential-polynomial divergence based inference for nondestructive\n  one-shot devices under progressive stress model",
        "Increasing quantum speed meter sensitivity using optical spring",
        "Extending the solutions and the equations of quantum gravity past the\n  big bang singularity",
        "Analysis of kinematics of mechanisms containing revolute joints",
        "Photonic heat amplifiers based on Anderson insulators",
        "$C^*$-supports and abnormalities of operator systems",
        "Projectivity of the moduli of higher rank PT-stable pairs on a threefold",
        "Python-JAX-based Fast Stokesian Dynamics",
        "Joint Cutting for Hybrid Schr\\\"odinger-Feynman Simulation of Quantum\n  Circuits",
        "Differentiable Simulator for Electrically Reconfigurable Electromagnetic\n  Structures",
        "Besicovitch-weighted ergodic theorems with continuous time",
        "VLBI Imaging of Parsec-scale Radio Structures in Nearby Low-luminosity\n  AGN",
        "Nonlinear dynamics and self-healing properties of elliptical Airy beams\n  in Kerr media",
        "Catalytic activity of Al-Cu-Fe-Ni-Cr high entropy alloy",
        "Ultrasensitivity without conformational spread: A mechanical origin for\n  non-equilibrium cooperativity in the bacterial flagellar motor",
        "Renormalization group of the gravity coupled with the scalar theory and\n  its effective normalization"
      ],
      "abstract":[
        "NA64 is a fixed-target experiment at the CERN SPS designed to search for\nLight particle Dark Matter (LDM) candidates with masses in the sub-GeV range.\nDuring the 2016-2022 runs, the experiment obtained the world-leading\nconstraints, leaving however part of the well-motivated region of parameter\nspace suggested by benchmark LDM models still unexplored. To further improve\nsensitivity, as part of the upgrades to the setup of NA64 at the CERN SPS H4\nbeamline, a prototype veto hadron calorimeter (VHCAL) was installed in the\ndownstream region of the experiment during the 2023 run. The VHCAL, made of\nCu-Sc layers, was expected to be an efficient veto against upstream\nelectroproduction of large-angle hadrons or photon-nuclear interactions,\nreducing the background from secondary particles escaping the detector\nacceptance. With the collected statistics of $4.4\\times10^{11}$ electrons on\ntarget (EOT), we demonstrate the effectiveness of this approach by rejecting\nthis background by more than an order of magnitude. This result provides an\nessential input for designing a full-scale optimized VHCAL to continue running\nbackground-free during LHC Run 4, when we expect to collect $10^{13}$ EOT.\nFurthermore, this technique combined with improvements in the analysis enables\nus to decrease our missing energy threshold from 50 GeV to 40 GeV thereby\nenhancing the signal sensitivity of NA64.",
        "This study utilises the R programming language for statistical data analysis\nto understand Tourism dynamics in Poland. It focuses on methods for data\nvisualisation, multivariate statistics, and hypothesis testing. To investigate\nthe expenditure behavior of tourist, spending patterns, correlations, and\nassociations among variables were analysed in the dataset. The results revealed\na significant relationship between accommodation type and the purpose of trip,\nshowing that the purpose of a trip impacts the selection of accommodation. A\nstrong correlation was observed between organizer expenditure and private\nexpenditure, indicating that individual spending are more when the spending on\norganizing the trip are higher. However, no significant difference was observed\nin total expenditure across different accommodation types and purpose of the\ntrip revealing that travelers tend to spend similar amounts regardless of their\nreason for travel or choice of accommodation. Although significant\nrelationships were observed among certain variables, ANOVA could not be applied\nbecause the dataset was not able to hold on the normality assumption. In\nfuture, the dataset can be explored further to find more meaningful insights.\nThe developed code is available on GitHub:\nhttps:\/\/github.com\/SaadAhmedJamal\/DataAnalysis RProgEnv.",
        "Optimising probabilistic models is a well-studied field in statistics.\nHowever, its connection with the training of generative models remains largely\nunder-explored. In this paper, we show that the evolution of time-varying\ngenerative models can be projected onto an exponential family manifold,\nnaturally creating a link between the parameters of a generative model and\nthose of a probabilistic model. We then train the generative model by moving\nits projection on the manifold according to the natural gradient descent\nscheme. This approach also allows us to approximate the natural gradient of the\nKL divergence efficiently without relying on MCMC for intractable models.\nFurthermore, we propose particle versions of the algorithm, which feature\nclosed-form update rules for any parametric model within the exponential\nfamily. Through toy and real-world experiments, we validate the effectiveness\nof the proposed algorithms.",
        "Electronic nematicity-the spontaneous reduction of rotational symmetry-has\nbeen widely studied in strongly correlated quantum materials, yet its interplay\nwith other symmetry-breaking phases remains an outstanding experimental\nchallenge to unravel, particularly in van der Waals heterostructures. Here,\nusing angle-resolved transport measurements, we reveal a unique intertwinement\nof nematicity, orbital multiferroicity, and the nonlinear Hall effect in\nrhombohedral pentalayer graphene. This coupling enables electric-field-driven\nswitching of the nematic principal axis and the nonlinear Hall polar vector,\ncoinciding with the butterfly-shaped hysteresis of the multiferroic order. We\nidentify two sharply defined nematic phase transitions: the first coincides\nwith the onset of orbital ferromagnetism, while the second, at lower\ntemperatures, aligns with the emergence of ferroelectricity. Our findings point\nto a nematic-driven instability that intertwines rotational, time-reversal, and\ninversion symmetry breaking. This is consistent with spontaneous momentum\npolarization, proposed to occur under the influence of the Coulomb-driven\nflocking effect. Together, our observations establish nematicity as a\nfundamental organizing principle in the landscape of intertwined electronic\norders in strongly correlated two-dimensional systems.",
        "Quantum computing is expected to provide exponential speedup in machine\nlearning. However, optimizing the data loading process, commonly referred to as\nquantum data embedding, to maximize classification performance remains a\ncritical challenge. In this work, we propose a neural quantum embedding (NQE)\ntechnique based on deterministic quantum computation with one qubit (DQC1).\nUnlike the traditional embedding approach, NQE trains a neural network to\nmaximize the trace distance between quantum states corresponding to different\ncategories of classical data. Furthermore, training is efficiently achieved\nusing DQC1, which is specifically designed for ensemble quantum systems, such\nas nuclear magnetic resonance (NMR). We validate the NQE-DQC1 protocol by\nencoding handwritten images into NMR quantum processors, demonstrating a\nsignificant improvement in distinguishability compared to traditional methods.\nAdditionally, after training the NQE, we implement a parameterized quantum\ncircuit for classification tasks, achieving 98\\% classification accuracy, in\ncontrast to the 54\\% accuracy obtained using traditional embedding. Moreover,\nwe show that the NQE-DQC1 protocol is extendable, enabling the use of the NMR\nsystem for NQE training due to its high compatibility with DQC1, while\nsubsequent machine learning tasks can be performed on other physical platforms,\nsuch as superconducting circuits. Our work opens new avenues for utilizing\nensemble quantum systems for efficient classical data embedding into quantum\nregisters.",
        "Silica aerogels are nanoporous materials with exceptional optical and\nphysical properties, making them promising candidates to enhance solar cell\nefficiency as antireflective coatings. This study synthesized hydrophobic\nsilica aerogel thin films under ambient conditions and characterized their\nporous structure, surface morphology, and optical performance. The films were\ndeposited on monocrystalline silicon solar cells to assess their impact on\nphotovoltaic properties. A two-step acid\/base catalyzed sol-gel process was\nutilized, followed by solvent exchange and surface modification with\ntrimethylchlorosilane. Structural analysis via SEM revealed successful\ndeposition of crack-free films when aging occurred in an ethanol environment.\nThe aerogel displayed considerable specific surface area (115 m2\/g), porosity\n(77.92%), and surface roughness (55-78%) along with a low refractive index\n(1.05), benefiting light harvesting. Preliminary solar testing showed increased\noutput voltage with a 0.2 mm aerogel coating versus a bare cell. Further IV\nmeasurements demonstrated enhanced charge transport and conversion efficiency\nfor the treated cell. The antireflective and light-trapping effects of aerogel\nappear to improve photon absorption. This initial research validates the\npotential of ambient pressure-synthesized hydrophobic silica aerogels to\nincrease the performance of silicon photovoltaics cost-effectively. Further\noptimization of film thickness and morphology could realize higher efficiency\ngains.",
        "We study the equation of state of hot and dense hadronic matter using an\nextended Chiral Mean Field (CMF) model framework where the addition is the\ninclusion of interactions of thermally excited mesons. This is implemented by\ncalculating the in-medium masses of pseudoscalar and vector mesons, obtained\nthrough the explicit chiral symmetry-breaking and vector interaction terms in\nthe Lagrangian, respectively, prior to applying the mean-field approximation.\nAs a result, the in-medium meson contributions generate a feedback term to the\nCMF's equations of motion, which then modifies the equation of state. With this\nimprovement, we quantify the effect on the equation of state of strongly\ninteracting matter through comparisons with state-of-the-art lattice QCD\nresults and other hadronic models like the Hadron Resonance Gas model. We find\nthat the results of the updated hadronic CMF model with an improved meson\ndescription (mCMF) provide a better agreement with lattice-QCD data for\nthermodynamic state variables across a wide range of temperatures and baryon\nchemical potentials.",
        "Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior.",
        "We study the geometry of the moduli stack of torsion-free sheaves on ribbons.\nWe introduce a stratification of the stack by the complete type of the sheaves,\nand we investigate the geometric properties of the strata and their closure\nrelation, and which strata intersect the (semi)stable locus. Then we describe\nthe irreducible components of the stack, by revealing an interesting trichotomy\nbetween Fano, Calabi-Yau and canonically polarized cases. Finally, we compute\nthe tangent space of the moduli stack at a given sheaf.",
        "We present the first calculation of quantum electrodynamics (QED) nuclear\nmedium effects under the experimental conditions of future Electron-Ion\nCollider (EIC) experiments. Our work offers numerical estimates, particularly\nin the context of inclusive deep inelastic scattering on a\n$^{208}_{82}\\mathrm{Pb}$ nucleus. While prior studies have predominantly\nfocused on elastic scattering, our investigation extends to the more complex\nscenarios of inelastic processes within a nuclear medium. Our findings suggest\nthat the cross-section corrections due to QED nuclear medium effects could be\nsubstantial, reaching or exceeding the level of experimental precision. This\nwork further compares the effects of single re-scattering events with those of\nmultiple re-scatterings, as particles travel the nuclear volume. We estimate\nthe dominant source of the uncertainties associated with our formalism by\nvarying the scale of the atomic physics where the screening of the electric\nfield of the nucleus happens. This calculation not only contributes to the\nunderstanding of QED nuclear medium effects, but also offers a path to a more\nprecise extraction of the process-independent non-perturbative structure of\nnuclei.",
        "We deal with the numerical solution of elliptic problems by the\n$hp$-discontinuous Galerkin method. We develop a two-level hybrid Schwarz\npreconditioner for the arising linear algebraic systems. The preconditioner is\nadditive with respect to the local components and multiplicative with respect\nto the mesh levels. We derive the $hp$ spectral bound of the preconditioned\noperator in the form $O((H\/h)(p^2\/q))$, where $H$ and $h$ are the element sizes\nof the coarse and fine meshes, respectively, and $p$ and $q$ are the polynomial\napproximation degrees on the fine and coarse meshes. Further, we present a\nnumerical study showing that the hybrid Schwarz preconditioner dominates the\nadditive one from the point of view of the speed of convergence and also\ncomputational costs. Finally, the combination with a $hp$-mesh adaptation for\nthe solution of nonlinear problem demonstrates the potential of this approach.",
        "In recent years, there has been a notable increase in the significance of\nactive mechanical metamaterials capable of being remotely manipulated through\nchanges in external stimuli. While research in this area has achieved\nconsiderable success in controlling reconfiguration to induce shape morphing or\nalter static mechanical properties, the active control of wave propagation\nwithin these systems remains largely unexplored. In this study, we propose a\nmagneto-mechanical metamaterial that can be entirely governed by adjusting the\nmagnitude and direction of an external magnetic field. We demonstrate that such\na system offers remote control over its Poisson's ratio, allowing for\ntransitions from strongly auxetic to positive Poisson's ratio configurations.\nAdditionally, our system enables manipulation of the phononic band structure,\nfacilitating the formation of complete band gaps across different frequency\nranges depending on the stage of the magnetically guided reconfiguration\nprocess. The potential for achieving such active control over the properties\nand behavior of these materials holds great promise for various applications,\nincluding robotics and smart vibration dampers that can be remotely controlled.",
        "Twisted generalized Reed-Solomon (TGRS) codes are an extension of the\ngeneralized Reed-Solomon (GRS) codes by adding specific twists, which attract\nmuch attention recently. This paper presents an in-depth and comprehensive\ninvestigation of the TGRS codes for the most general form by using a universal\nmethod. At first, we propose a more precise definition to describe TGRS codes,\nnamely $(\\mathcal{L},\\mathcal{P})$-TGRS codes, and provide a concise necessary\nand sufficient condition for $(\\mathcal{L},\\mathcal{P})$-TGRS codes to be MDS,\nwhich extends the related results in the previous works. Secondly, we\nexplicitly characterize the parity check matrices of\n$(\\mathcal{L},\\mathcal{P})$-TGRS codes, and provide a sufficient condition for\n$(\\mathcal{L},\\mathcal{P})$-TGRS codes to be self-dual. Finally, we conduct an\nin-depth study into the non-GRS property of $(\\mathcal{L},\\mathcal{P})$-TGRS\ncodes via the Schur squares and the combinatorial techniques respectively. As a\nresult, we obtain a large infinite families of non-GRS MDS codes.",
        "Nondestructive one-shot device (NOSD) testing plays a crucial role in\nengineering, particularly in the reliability assessment of high-stakes systems\nsuch as aerospace components, medical devices, and semiconductor technologies.\nAccurate reliability prognosis of NOSD testing data is essential for ensuring\nproduct durability, safety, and performance optimization. The conventional\nestimation methods like maximum likelihood estimation (MLE) are sensitive to\ndata contamination, leading to biased results. Consequently, this study\ndevelops robust inferential analysis for NOSD testing data under a progressive\nstress model. The lifetime of NOSD is assumed to follow Log-logistic\ndistribution. The estimation procedure addresses robustness by incorporating\nExponential-polynomial divergence (EPD). Equipped with three tuning parameters,\nEPD based estimation is proven to be more flexible than density power\ndivergence estimation frequently used for one-shot device testing data\nanalysis. Further, we explore the asymptotic behaviour of minimum EPD estimator\n(MEPDE) for large sample size. The robustness of MEPDE is analytically studied\nthrough influence function. Since tradeoff between efficiency and robustness of\nEPD based estimation is governed by three tuning parameters, a novel approach\nleveraging Concrete Score Matching (CSM) is introduced to optimize the tuning\nparameters of MEPDE. Moreover, a comparative study with the existing methods of\nfinding tuning parameters is conducted through extensive simulation experiment\nand data analysis. Another aspect of this study is determining an optimal plan\nto ensure a successful ALT experiment within specified budget and time\nconstraints. It is designed on A-optimality criteria subject to the given\nconstraints and is executed using the constraint particle swarm optimization\n(CPSO) algorithm.",
        "The double-pass interferometer scheme was proposed in Ref.\\,[Light Sci. Appl.\n{\\bf 7}, 11 (2018)] as the method of implementation of the quantum speed meter\nconcept in future laser gravitational-wave (GW) detectors. Later it was shown\nin Ref.\\,[Phys. Rev. D {\\bf 110}, 062006 (2024)] that it allows to implement\nthe new type of the optical spring that does not require detuning of the\ninterferometer. Here we show that both these regimes can coexist, combining the\nspeed meter type broadband sensitivity gain with the additional lows-frequency\nminimum in the quantum noise originated from the optical spring. We show that\nthe location of this minimum can be varied without affecting the core optics of\nthe interferometer, allowing to tune the quantum noise shape in real time to\nfollow the ``chirp'' GW signals.",
        "In [8] we recently proved that in our model of quantum gravity the solutions\nto the quantized version of the full Einstein equations or to the\nWheeler-DeWitt equation could be expressed as products of spatial and temporal\neigenfunctions, or eigendistributions, of self-adjoint operators acting in\ncorresponding separable Hilbert spaces. Moreover, near the big bang singularity\nwe derived sharp asymptotic estimates for the temporal eigenfunctions. In this\npaper we show that, by using these estimates, there exists a complete sequence\nof unitarily equivalent eigenfunctions which can be extended past the\nsingularity by even or odd mirroring as sufficiently smooth functions such that\nthe extended functions are solutions of the appropriately extended equations\nvalid in $\\R[]$ in the classical sense. We also use this phenomenon to explain\nthe missing antimatter.",
        "Kinematics of rigid bodies can be analyzed in many different ways. The\nadvantage of using Euler parameters is that the resulting equations are\npolynomials and hence computational algebra, in particular Gr\\\"obner bases, can\nbe used to study them. The disadvantage of the Gr\\\"obner basis methods is that\nthe computational complexity grows quite fast in the worst case in the number\nof variables and the degree of polynomials. In the present article we show how\nto simplify computations when the mechanism contains revolute joints. The idea\nis based on the fact that the ideal representing the constraints of the\nrevolute joint is not prime. Choosing the appropriate prime component reduces\nsignificantly the computational cost. We illustrate the method by applying it\nto the well known Bennett's and Bricard's mechanisms, but it can be applied to\nany mechanism which has revolute joints.",
        "A photonic heat amplifier (PHA) designed for cryogenic operations is\nintroduced and analyzed. This device comprises two Anderson insulator\nreservoirs connected by lossless lines, allowing them to exchange heat through\nphotonic modes. This configuration enables negative differential thermal\nconductance (NDTC), which can be harnessed to amplify thermal signals. To\nachieve this, we maintain one reservoir at a high temperature, serving as the\nsource terminal of a thermal transistor. Concurrently, in the other one, we\nestablish tunnel contacts to metallic reservoirs, which function as the gate\nand drain terminals. With this arrangement, it is possible to control the heat\nflux exchange between the source and drain by adjusting the gate temperature.\nWe present two distinct parameter choices that yield different performances:\nthe first emphasizes modulating the source-drain heat current, while the second\nfocuses on the temperature modulation of the colder Anderson insulator. Lastly,\nwe present a potential design variation in which all electronic reservoirs are\nthermally connected through only photonic modes, allowing interactions between\ndistant elements. The proposal of the PHA addresses the lack of thermal\ntransistors and amplifiers in the mK range while being compatible with the rich\ntoolbox of circuit quantum electrodynamics. It can be adapted to various\napplications, including sensing and developing thermal circuits and control\ndevices at sub-Kelvin temperatures, which are relevant to quantum technologies.",
        "Let $\\rho$ be a completely isometric representation of an operator system $S$\non some Hilbert space $H$. A $C^*$-support of $\\rho$ is the $C^*$-algebra\ngenerated by $\\rho(S)$ inside an injective operator system acting on $H$,\nequipped with its Choi--Effros product. By leveraging Hamana's theory, we show\nthat such a $C^*$-support is unique precisely when $C^*(\\rho(S))$ is contained\nin every copy of the injective envelope of $S$ that acts on $H$. Further, we\ndemonstrate how the uniqueness of certain $C^*$-supports can be used to give\nnew characterizations of the unique extension property for $*$-representations\nand of the hyperrigidity of $S$. In another direction, we utilize the\ncollection of all $C^*$-supports of $\\rho$ to describe the subspace generated\nby the so-called abnormalities of $\\rho$, thereby complementing a result of\nKakariadis.",
        "We construct a globally generated line bundle on the moduli stack of\nhigher-rank PT-semistable objects over a smooth projective threefold and\nanalyze the extent to which it separates points. Furthermore, when the rank and\ndegree are coprime, we refine our construction to obtain an explicit ample line\nbundle on the corresponding coarse moduli space of PT-stable objects, thereby\nestablishing its projectivity.",
        "Stokesian Dynamics (SD) is a powerful computational framework for simulating\nthe motion of particles in a viscous Newtonian fluid under Stokes-flow\nconditions. Traditional SD implementations can be computationally expensive as\nthey rely on the inversion of large mobility matrices to determine hydrodynamic\ninteractions. Recently, however, the simulation of thermalized systems with\nlarge numbers of particles has become feasible [Fiore and Swan, J. Fluid. Mech.\n$\\textbf{878}$, 544 (2019)]. Their ``fast Stokesian dynamics'' (FSD) method\nleverages a saddle-point formulation to ensure overall scaling of the algorithm\nthat is linear in the number of particles $\\mathcal{O}(N)$; performance relies\non dedicated graphics-processing-unit computing. Here, we present a different\nroute toward implementing FSD, which instead leverages the Just-in-Time (JIT)\ncompilation capabilities of Google JAX. We refer to this implementation as JFSD\nand perform benchmarks on it to verify that it has the right scaling and is\nsufficiently fast by the standards of modern computational physics. In\naddition, we provide a series of physical test cases that help ensure accuracy\nand robustness, as the code undergoes further development. Thus, JFSD is ready\nto facilitate the study of hydrodynamic effects in particle suspensions across\nthe domains of soft, active, and granular matter.",
        "Despite the continuous advancements in size and robustness of real quantum\ndevices, reliable large-scale quantum computers are not yet available. Hence,\nclassical simulation of quantum algorithms remains crucial for testing new\nmethods and estimating quantum advantage. Pushing classical simulation methods\nto their limit is essential, particularly due to their inherent exponential\ncomplexity. Besides the established Schr\\\"odinger-style full statevector\nsimulation, so-called Hybrid Schr\\\"odinger-Feynman (HSF) approaches have shown\npromise to make simulations more efficient. HSF simulation employs the idea of\n\"cutting\" the circuit into smaller parts, reducing their execution times. This,\nhowever, comes at the cost of an exponential overhead in the number of cuts.\nInspired by the domain of Quantum Circuit Cutting, we propose an HSF simulation\nmethod based on the idea of \"joint cutting\" to significantly reduce the\naforementioned overhead. This means that, prior to the cutting procedure, gates\nare collected into \"blocks\" and all gates in a block are jointly cut instead of\nindividually. We investigate how the proposed refinement can help decrease\nsimulation times and highlight the remaining challenges. Experimental\nevaluations show that \"joint cutting\" can outperform the standard HSF\nsimulation by up to a factor $\\approx 4000\\times$ and the Schr\\\"odinger-style\nsimulation by a factor $\\approx 200\\times$ for suitable instances. The\nimplementation is available at\nhttps:\/\/github.com\/cda-tum\/mqt-qsim-joint-cutting.",
        "This paper introduces a novel CUDA-enabled PyTorch-based framework designed\nfor the gradient-based optimization of such reconfigurable electromagnetic\nstructures with electrically tunable parameters. Traditional optimization\ntechniques for these structures often rely on non-gradient-based methods,\nlimiting efficiency and flexibility. Our framework leverages automatic\ndifferentiation, facilitating the application of gradient-based optimization\nmethods. This approach is particularly advantageous for embedding within deep\nlearning frameworks, enabling sophisticated optimization strategies.\n  We demonstrate the framework's effectiveness through comprehensive\nsimulations involving resonant structures with tunable parameters. Key\ncontributions include the efficient solution of the inverse problem. The\nframework's performance is validated using three different resonant structures:\na single-loop copper wire (Unit-Cell) as well as an 8x1 and an 8x8 array of\nresonant unit cells with multiple inductively coupled unit cells (1d and 2d\nMetasurfaces). Results show precise in-silico control over the magnetic field's\ncomponent normal to the surface of each resonant structure, achieving desired\nfield strengths with minimal error. The proposed framework is compatible with\nexisting simulation software.\n  This PyTorch-based framework sets the stage for advanced electromagnetic\ncontrol strategies for resonant structures with application in e.g. MRI,\nproviding a robust platform for further exploration and innovation in the\ndesign and optimization of resonant electromagnetic structures.",
        "Given $1\\leq p<\\infty$, we show that ergodic flows in the $L^p$-space over a\n$\\sigma$-finite measure space generated by strongly continuous semigroups of\nDunford-Schwartz operators and modulated by bounded Besicovitch almost periodic\nfunctions converge almost uniformly (in Egorov's sense). The corresponding\nlocal ergodic theorem is proved with identification of the limit. Then we\nextend these results to arbitrary fully symmetric spaces, including Orlicz,\nLorentz, and Marcinkiewicz spaces.",
        "We report the results of high-resolution 5 GHz Very Long Baseline Array and\nEuropean VLBI Network observations of 36 nearby galaxies, an extension of the\nLegacy e-MERLIN Multi-band Imaging of Nearby Galaxies (LeMMINGs) survey. Our\nsample includes 21 low ionization nuclear emission regions (LINERs), 4\nSeyferts, 3 absorption line galaxies (ALGs), and 8 HII galaxies. We achieved an\nunprecedented detection rate, successfully imaging 23 out of 36 sources with a\ndetection threshold of $\\sim$20 $\\mu$Jy beam$^{-1}$. The radio sizes are\ntypically of $\\leq$ 5 pc. Core identification was achieved in 16 sources, while\n7 others were identified as core candidates. Radio luminosities of the sample\nrange from 10$\\rm ^{34}$ to 10$\\rm ^{38}$ erg s$^{-1}$. Our analysis reveals a\npredominance of compact core structures, with ten sources exhibiting a\none-sided core jet morphology and NGC 2146 exhibiting a rare two-sided jet\nstructure. The study advances our understanding of the compactness of radio\nsources at various scales, indicating a core-dominated nature in all but one\ngalaxy NGC2655. We find moderate to strong correlations between radio\nluminosity, black hole mass, optical [O III] line luminosity, and hard X-ray\nluminosity, suggesting a common active galactic nucleus (AGN) core origin.\nThese results provide new insights into the fundamental plane of black hole\nactivity and support the role of the synchrotron process in Low-luminosity AGN\n(LLAGN) radio emission.",
        "By numerically solving the nonlinear Schr\\\"odinger equation, we theoretically\nstudy the nonlinear propagation dynamics and self-healing properties of\nelliptical Airy beams (EABs) propagating in water under Kerr nonlinearity.\nCompared to linear propagation, EABs exhibit extended propagation distances and\nenhanced stability in nonlinear media. Furthermore, particular emphasis is\nplaced on the impact of Kerr nonlinearity strength on the propagation and\nself-healing properties of EABs. By varying the input power, it is found that\nEABs within a moderate power range can propagate longer distances while\nmaintaining higher intensity and exhibit improved robustness after being\nblocked, indicating better self-healing performance. Based on this analysis, we\npropose an optimal input power for EABs through a quantitative analysis of the\nimpact of Kerr nonlinearity, enabling them to achieve the greatest propagation\ndistance and maintain the highest stability. Our work provides a comprehensive\ntheoretical understanding of the nonlinear propagation dynamics and\nself-healing properties of EABs, with their superior characteristics\npotentially applicable to long-distance laser transmission.",
        "Magnesium hydride (MgH2) is a promising material for hydrogen storage because\nof its abundance and beneficial properties, such as high storage capacity and\ncost-effectiveness under mild conditions. Despite of these benefits, MgH2\nunfavorable thermodynamics and kinetics make it difficult to use in real\napplications. In this work, the hydrogen storage properties of MgH2have been\nimproved using Al-Cu-Fe-Ni-Cr high entropy alloy (HEA) based catalysts, which\nhas been synthesized via mechanical alloying. The experimental findings show\nthat the beginning desorption temperature of MgH2significantly lowered from\n425{\\deg}C to 180{\\deg}C by adding 5 wt. % Al-Cu-Fe-Ni-Cr HEA in MgH2.\nMoreover, the catalyst shows enhanced kinetics, attaining 7.3 wt. % hydrogen\nabsorption in 3 minutes at 320{\\deg}C with 15 atm hydrogen pressure, and ~5 wt.\n% desorption in 6 minutes at 320{\\deg}C. These results highlight, how much\nlower its desorption temperature is than those of other well-known catalysts.\nOver a span of 25 cycles, MgH2 catalyzed by Al-Cu-Fe-Ni-Cr HEA exhibits\nremarkable cyclic stability with negligible fluctuations (~ 0.05 wt. %).After a\nthorough characterization of the materials, a workable catalytic mechanism for\nHEA was proposed in light of the results.",
        "Flagellar motors enable bacteria to navigate their environments by switching\nrotation direction in response to external cues with high sensitivity. Previous\nwork suggested that ultrasensitivity of the flagellar motor originates from\nconformational spread, in which subunits of the switching complex are strongly\ncoupled to their neighbors as in an equilibrium Ising model. However, dynamic\nsingle-motor measurements indicated that rotation switching is driven out of\nequilibrium, and the mechanism for this dissipative driving remains unknown.\nHere, based on recent cryo-EM structures, we propose that local mechanical\ntorques on motor subunits can affect their conformation dynamics. This gives\nrise to a tug of war between stator-associated subunits, which produces\ncooperative, non-equilibrium switching responses without requiring\nnearest-neighbor interactions. Since subunits are effectively coupled at a\ndistance, we call this mechanism ``Global Mechanical Coupling.\" Our model makes\na qualitatively new prediction that the motor response cooperativity grows with\nthe number of stators driving rotation. Re-analyzing published motor\ndose-response curves in varying load conditions, we find tentative experimental\nevidence for this prediction. Finally, we show that operating out of\nequilibrium enables motors to achieve high cooperativity with faster responses\ncompared to equilibrium motors. Our results suggest a general role for\nmechanics in sensitive chemical regulation.",
        "The normalization of the quantum corrected action is resolving the equation\ndivergent dependence of the cutoff towards the system apparent result in\nquantum gravity. Here we consider the normalization to Einstein R twice scalar\naction with the cutoff runs from apparent infrared momentu to the ultraviolet\nmomentum. These gravitational actions, Einstein R twice and Einstein R twice\nwith the scalar theory, are come to the ensured apparent system recommendations\nin classical quantum gravity."
      ]
    }
  },
  {
    "id":2411.03551,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Interobserver Variability in the CT Assessment of Honeycombing in the Lungs",
    "start_abstract":"To quantify observer agreement and analyze causes of disagreement in identifying honeycombing at chest computed tomography (CT).The institutional review board approved this multiinstitutional HIPAA-compliant retrospective study, informed patient consent was not required. Five core study members scored 80 CT images with a five-point scale (5 = definitely yes to 1 no) establish reference standard for the identification honeycombing. Forty-three observers from various subspecialties geographic regions by using same scoring system. Weighted \u03ba values scores compared were analyzed investigate intergroup differences. Images divided into four groups allow analysis imaging features cases which there disagreement: on presence honeycombing, absence other (none preceding three applied).Agreement 43 moderate (Cohen weighted values: 0.40-0.58). There no significant differences among defined either subspecialty or region (Tukey-Kramer test, P .38 >.99). In 29% cases, These included mixed traction bronchiectasis, large cysts, superimposed pulmonary emphysema.Identification is subjective, largely caused conditions that mimic",
    "start_categories":[
      "Radiology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation"
      ],
      "abstract":[
        "Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Exponential Family Attention",
        "What Bohmian mechanic says about arrival times of 1D vacuum squeezed\n  states",
        "On complex symmetric weighted shifts. II",
        "On the $\\mathcal{F}$-multicolor Tur\\'{a}n number of hypergraph graphs",
        "The Role of GitHub Copilot on Software Development: A Perspec-tive on\n  Productivity, Security, Best Practices and Future Directions",
        "Global Gauge Symmetries and Spatial Asymptotic Boundary Conditions in\n  Yang-Mills theory",
        "Argument Summarization and its Evaluation in the Era of Large Language\n  Models",
        "Optical centroid orbiting metrology",
        "Multi-front dynamics in spatially inhomogeneous Allen-Cahn equations",
        "Dissecting supergraviton six-point function with lightcone limits and\n  chiral algebra",
        "Ultrasound-Coupled Microdroplet Laser Chip for High-Throughput\n  Hyperlipidemia Screening",
        "Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment\n  in LVLMs",
        "Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction",
        "Flopping for FLOPs: Leveraging equivariance for computational efficiency",
        "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language\n  Models for Navigation Applications",
        "Bispectrum constraints on Primordial non-Gaussianities with the eBOSS\n  DR16 quasars",
        "FENN: Feature-enhanced neural network for solving partial differential\n  equations involving fluid mechanics",
        "IEEEICM25: \"Stability of Digital Robust Motion Control Systems with\n  Disturbance Observer\"",
        "Emergency-Brake Simplex: Toward A Verifiably Safe Control-CPS\n  Architecture for Abrupt Runtime Reachability Constraint Changes",
        "Reinforcement Learning Platform for Adversarial Black-box Attacks with\n  Custom Distortion Filters",
        "Asteroid (4337) Arecibo: Two ice-rich bodies forming a binary -- Based\n  on Gaia astrometric data",
        "CleanSurvival: Automated data preprocessing for time-to-event models\n  using reinforcement learning",
        "Elliptic Loss Regularization",
        "MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented\n  Multimodal Generation",
        "The law of the circumference of sparse binomial random graphs",
        "Building unconventional magnetic phases on graphene by H atom\n  manipulation: From altermagnets to Lieb ferrimagnets",
        "Reachability for multiagent control systems via Lyapunov functions",
        "Exact quantum critical states with a superconducting quantum processor",
        "Robust Adaptive Meshing, Mesh Density Functions, and Nonlocal\n  Observations for Ensemble Based Data Assimilation"
      ],
      "abstract":[
        "The self-attention mechanism is the backbone of the transformer neural\nnetwork underlying most large language models. It can capture complex word\npatterns and long-range dependencies in natural language. This paper introduces\nexponential family attention (EFA), a probabilistic generative model that\nextends self-attention to handle high-dimensional sequence, spatial, or\nspatial-temporal data of mixed data types, including both discrete and\ncontinuous observations. The key idea of EFA is to model each observation\nconditional on all other existing observations, called the context, whose\nrelevance is learned in a data-driven way via an attention-based latent factor\nmodel. In particular, unlike static latent embeddings, EFA uses the\nself-attention mechanism to capture dynamic interactions in the context, where\nthe relevance of each context observations depends on other observations. We\nestablish an identifiability result and provide a generalization guarantee on\nexcess loss for EFA. Across real-world and synthetic data sets -- including\nU.S. city temperatures, Instacart shopping baskets, and MovieLens ratings -- we\nfind that EFA consistently outperforms existing models in capturing complex\nlatent structures and reconstructing held-out data.",
        "We calculate the time of arrival probability distribution of a quantum\nparticle using the Bohmian formalism. The pilot-wave is given by the wave\nfunction of the one dimensional vacuum squeezed state but written in the\nSchr\\\"odinger representation. We made use of the unitary representation of the\nsymplectic group in the Hilbert space $L^2(\\mathbb{R})$. The solution to the\nBohmian equations are analytical function thus allowing for a closed expression\nof the time of arrival distribution which differs from the counterparts in the\nstandard quantum mechanics formulation.",
        "Assorted weighted shifts over finite rooted directed trees are studied. Their\ncomplex symmetry is characterized.",
        "Recently, Imolay, Karl, Nazy and V\\'{a}li explored a generalization of\nTur\\'{a}n's forbidden subgraph problem and Ruzsa-Szrmer\\'{e}di $(6,3)$-problem.\nThey specifically studied the following question: for two graphs $F$ and $G$,\ndetermine the maximum number of edge-disjoint copies of $F$ in a set of $n$\nvertices such that there is no copies of $G$ whose edges come from different\n$F$-copies. The maximum number is denoted by $ex_F(n,G)$ and is called the {\\em\n$F$-multicolor Tur\\'{a}n number} of $G$. One of their main results is that\n$ex_F(n,G)=o(n^2)$ if and only if there exists a homomorphism from $G$ to $F$.\nWe generalize the result to uniformly hypergraph using main tools of Hypergraph\nRegularity Lemma and Counting Lemma.",
        "GitHub Copilot is transforming software development by automating tasks and\nboosting productivity through AI-driven code generation. In this paper, we\ncon-duct a literature survey to synthesize insights on Copilot's impact on\nproductivity and security. We review academic journal databases, industry\nreports, and official docu-mentation to highlight key findings and challenges.\nWhile Copilot accelerates coding and prototyping, concerns over security\nvulnerabilities and intellectual property risks persist. Drawing from the\nliterature, we provide a perspective on best practices and future directions\nfor responsible AI adoption in software engineering, offering action-able\ninsights for developers and organizations to integrate Copilot effectively\nwhile maintaining high standards of quality and security.",
        "In Yang-Mills gauge theory on a Euclidean Cauchy surface the group of gauge\nsymmetries carrying direct empirical significance is often believed to be\n$\\mathcal{G}_\\text{DES}=\\mathcal{G}^I\/\\mathcal{G}^\\infty_0$, where\n$\\mathcal{G}^I$ is the group of boundary-preserving gauge symmetries and\n$\\mathcal{G}^\\infty_0$ is its subgroup of transformations that are generated by\nthe constraints of the theory. These groups are identified respectively as the\ngauge transformations that become constant asymptotically and those that become\nthe identity asymptotically. In the Abelian case $G=U(1)$ the quotient is then\nidentified as the group of global gauge symmetries, i.e. $U(1)$ itself.\nHowever, known derivations of this claim are imprecise, both mathematically and\nconceptually. We derive the physical gauge group rigorously for both Abelian\nand non-Abelian gauge theory. Our main new point is that the requirement to\nrestrict to $\\mathcal{G}^I$ does not follow from finiteness of energy only, but\nfrom the requirement that the Lagrangian of Yang-Mills theory be defined on a\ntangent bundle to configuration space. Moreover, we explain why the quotient\nconsists precisely of a copy of the global gauge group for every homotopy\nclass, even if the various gauge transformations apparently have different\nasymptotic rates of convergence. Lastly, we consider Yang-Mills-Higgs theory in\nour framework and show that asymptotic boundary conditions differ in the\nunbroken and broken phases.",
        "Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum.",
        "Optical interferometry has dramatically advanced the development of modern\nscience and technology. Here we introduce an interesting centroid evolution\nphenomenon of orbital angular momentum (OAM) interference fields with broken\nrotational symmetry, and establish a novel interferometric paradigm by fully\nexploiting centroid orbiting information. The centroid positions and their\ngeometric trajectories can provide more detectable information in a\ntwo-dimensional plane to sense the interferometric perturbations, compared with\nthe conventional interferometry. We first investigate centroid orbital\nevolution under the inclined angle perturbation that allows for ultra-sensitive\nangle distinguishment with arc-second resolution. We also show centroid ellipse\nevolution under spatial phase perturbation that enables geometric\ncharacterization of arbitrary OAM superpositions on modal Poincar\\'e spheres.\nFurthermore, based on the angle subdivision of centroid orbiting, we\ndemonstrate the environmentally robust nanoscale displacement measurement with\npolarization synchronous detection, and particularly the high-resolution, fast,\nand large-range linear movement monitoring using commercial four-quadrant\nphotodetectors. This novel centroid orbiting interferometry may open new\nopportunities to advance metrological technologies beyond the conventional\ninterferometers.",
        "Recent studies of biological, chemical, and physical pattern-forming systems\nhave started to go beyond the classic `near onset' and `far from equilibrium'\ntheories for homogeneous systems to include the effects of spatial\nheterogeneities. In this article, we build a conceptual understanding of the\nimpact of spatial heterogeneities on the pattern dynamics of reaction-diffusion\nmodels. We consider the simplest setting of an explicit, scalar, bi-stable\nAllen-Cahn equation driven by a general small-amplitude spatially-heterogeneous\nterm $\\varepsilon F(U,U_x,x)$. In the first part, we perform an analysis of the\nexistence and stability of stationary one-, two- and $N$-front patterns for\ngeneral spatial heterogeneity $F(U,U_x,x)$. In addition, we explicitly\ndetermine the $N$-th order system of ODEs that governs the evolution of the\nfront positions of general $N$-front patterns to leading order. In the second\npart, we focus on a particular class of spatial heterogeneities where\n$F(U,U_x,x) = H'(x) U_x + H''(x) U$ with $H$ either spatially periodic or\nlocalised. For spatially periodic heterogeneities, we show that the fronts of a\nmulti-front pattern will get `pinned' if the distances between successive\nfronts are sufficiently large, {\\it i.e.}, the multi-front pattern is attracted\nto a nearby stable stationary multi-front pattern. For localised\nheterogeneities, we determine all stationary $N$-front patterns, and show that\nthese are unstable for $N > 1$. We find instead slowly evolving `trains' of\n$N$-fronts that collectively travel to $\\pm \\infty$, either with slowly\ndecreasing or increasing speeds.",
        "We develop a bootstrap strategy to obtain the six-point function of\nsupergravitons in $AdS_5\\times S^5$ from symmetry constraints and consistency\nconditions. Compared to previous bootstrap algorithms, a novel feature is the\nuse of lightcone OPEs together with the chiral algebra constraint. This makes\nit possible to isolate different parts of the correlator and fix them\nseparately. Our strategy allows us to gain a refined understanding of the power\nof different bootstrap constraints, which is also useful for computing more\ngeneral correlators.",
        "The mechanical properties of biological fluids can serve as early indicators\nof disease, offering valuable insights into complex physiological and\npathological processes. However, the existing technologies can hardly support\nhigh throughput measurement, which hinders their broad applications in disease\ndiagnosis. Here, we propose the ultrasound-coupled microdroplet laser chips to\nenable high-throughput measurement of the intrinsic mechanical properties of\nfluids. The microdroplets supporting high-Q (10^4) whispering gallery modes\n(WGM) lasing were massively fabricated on a hydrophobic surface with inject\nprinting. The ultrasound was used to actuate the mechanical vibration of the\nmicrodroplets. We found that the stimulus-response of the laser emission is\nstrongly dependent on the intrinsic mechanical properties of the liquid, which\nas subsequently employed to quantify the viscosity. The ultrasound-coupled\nmicrodroplet laser chips were used to monitor molecular interactions of bovine\nserum albumin. High-throughput screening of hyperlipidemia disease was also\ndemonstrated by performing over 2,000 measurements using fast laser scanning.\nThanks to the small volume of the microdroplets, a single drop of blood can\nsupport over eight billion measurements. The high-throughput ability and small\nsample consumption of the microlaser chip make it a promising tool for clinical\ndiagnoses based on mechanical properties.",
        "Large Vision-Language Models (LVLMs) have made significant strides in\nmultimodal comprehension, thanks to extensive pre-training and fine-tuning on\nlarge-scale visual datasets. However, despite their robust textual safety\nmechanisms, they remain vulnerable to harmful visual inputs. Existing\nsafeguards-typically relying on pre-filtering or fine-tuning-incur high costs\nand diminish overall utility. To address this critical vulnerability, we\nintroduce SafeCLIP, a lightweight method that leverages LVLMs inherent\nmultimodal alignment for zero-shot toxic image detection. By projecting CLIPs\ndiscarded CLS token into its text space and matching it with toxic descriptors,\nSafeCLIP detects harmful content without any architectural changes-adding\nminimal latency and enabling dynamic safety corrections during inference and\nfine-tuning.Experiments show that SafeCLIP achieves a 66.9% defense success\nrate with only 3.2% false positive rate and 7.2% overhead. In contrast,\nstate-of-the-art methods achieve 52.9% success but have a 10.7% false positive\nrate and 210% overhead. Our work demonstrates that leveraging inherent\nmultimodal alignment can yield efficient, low-cost LVLM safety. Code is\navailable at anonymous.4open.science\/r\/safeclip-2C01.",
        "Probabilistic human motion prediction aims to forecast multiple possible\nfuture movements from past observations. While current approaches report high\ndiversity and realism, they often generate motions with undetected limb\nstretching and jitter. To address this, we introduce SkeletonDiffusion, a\nlatent diffusion model that embeds an explicit inductive bias on the human body\nwithin its architecture and training. Our model is trained with a novel\nnonisotropic Gaussian diffusion formulation that aligns with the natural\nkinematic structure of the human skeleton. Results show that our approach\noutperforms conventional isotropic alternatives, consistently generating\nrealistic predictions while avoiding artifacts such as limb distortion.\nAdditionally, we identify a limitation in commonly used diversity metrics,\nwhich may inadvertently favor models that produce inconsistent limb lengths\nwithin the same sequence. SkeletonDiffusion sets a new benchmark on three\nreal-world datasets, outperforming various baselines across multiple evaluation\nmetrics. Visit our project page:\nhttps:\/\/ceveloper.github.io\/publications\/skeletondiffusion\/",
        "Incorporating geometric invariance into neural networks enhances parameter\nefficiency but typically increases computational costs. This paper introduces\nnew equivariant neural networks that preserve symmetry while maintaining a\ncomparable number of floating-point operations (FLOPs) per parameter to\nstandard non-equivariant networks. We focus on horizontal mirroring (flopping)\ninvariance, common in many computer vision tasks. The main idea is to\nparametrize the feature spaces in terms of mirror-symmetric and\nmirror-antisymmetric features, i.e., irreps of the flopping group. This\ndecomposes the linear layers to be block-diagonal, requiring half the number of\nFLOPs. Our approach reduces both FLOPs and wall-clock time, providing a\npractical solution for efficient, scalable symmetry-aware architectures.",
        "Existing navigation decision support systems often perform poorly when\nhandling non-predefined navigation scenarios. Leveraging the generalization\ncapabilities of large language model (LLM) in handling unknown scenarios, this\nresearch proposes a dual-core framework for LLM applications to address this\nissue. Firstly, through ReAct-based prompt engineering, a larger LLM core\ndecomposes intricate navigation tasks into manageable sub-tasks, which\nautonomously invoke corresponding external tools to gather relevant\ninformation, using this feedback to mitigate the risk of LLM hallucinations.\nSubsequently, a fine-tuned and compact LLM core, acting like a first-mate is\ndesigned to process such information and unstructured external data, then to\ngenerates context-aware recommendations, ultimately delivering lookout insights\nand navigation hints that adhere to the International Regulations for\nPreventing Collisions at Sea (COLREGs) and other rules. Extensive experiments\ndemonstrate the proposed framework not only excels in traditional ship\ncollision avoidance tasks but also adapts effectively to unstructured,\nnon-predefined, and unpredictable scenarios. A comparative analysis with\nDeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and\nrationality of the proposed framework. This research bridges the gap between\nconventional navigation systems and LLMs, offering a framework to enhance\nsafety and operational efficiency across diverse navigation applications.",
        "We present constraints on $f_{\\rm NL}$, the parameter quantifying the\namplitude of local Primordial Non-Gaussianities (PNG), from a combined analysis\nof the tree-level power spectrum and bispectrum of Data Release $16$ (DR16) of\nthe extended Baryon Oscillation Spectroscopic Survey (eBOSS) quasar sample. In\nour analysis, we use the power spectrum measured with the optimal redshift\nweights that maximize the local PNG information together with the bispectrum\nestimated with the standard Feldman-Kaiser-Peacock weights. In the modeling, we\nincorporate the global and radial integral constraint corrections both in the\npower spectrum and in the bispectrum, for which we observe that only the radial\nintegral constraint correction has a significant impact. Our constraints read\n$-6 < f_{\\rm NL} < 20$ at $68\\%$ confidence level and improve by $\\sim 16\\%$\nover the previous power spectrum analysis of the same dataset. We observe the\nsame improvement over the power spectrum analysis when the quasar response to\nPNG is lower. In this case, we find $-23 < f_{\\rm NL} < 14$ at $68\\%$\nconfidence level. Our findings are consistent with the Fisher matrix\nexpectations.",
        "Physics-informed neural networks (PINNs) have shown remarkable prospects in\nsolving forward and inverse problems involving partial differential equations\n(PDEs). However, PINNs still face the challenge of high computational cost in\nsolving strongly nonlinear PDEs involving fluid dynamics. In this study,\ninspired by the input design in surrogate modeling, we propose a\nfeature-enhanced neural network. By introducing geometric features including\ndistance and angle or physical features including the solution of the potential\nflow equation in the inputs of PINNs, FENN can more easily learn the flow,\nresulting in better performance in terms of both accuracy and efficiency. We\nestablish the feature networks in advance to avoid the invalid PDE loss in FENN\ncaused by neglecting the partial derivatives of the features with respect to\nspace-time coordinates. Through five numerical experiments involving forward,\ninverse, and parametric problems, we verify that FENN generally reduces the\ncomputational cost of PINNs by approximately four times. In addition, the\nnumerical experiments also demonstrate that the proposed method can reduce the\nnumber of observed data for inverse problem and successfully solve the\nparametric problem where PINNs fail.",
        "In this paper, new stability analysis methods are proposed for digital robust\nmotion control systems implemented using a disturbance observer.",
        "When a system's constraints change abruptly, the system's reachability safety\ndoes no longer sustain. Thus, the system can reach a forbidden\/dangerous value.\nConventional remedy practically involves online controller redesign (OCR) to\nre-establish the reachability's compliance with the new constraints, which,\nhowever, is usually too slow. There is a need for an online strategy capable of\nmanaging runtime changes in reachability constraints. However, to the best of\nthe authors' knowledge, this topic has not been addressed in the existing\nliterature. In this paper, we propose a fast fault tolerance strategy to\nrecover the system's reachability safety in runtime. Instead of redesigning the\nsystem's controller, we propose to change the system's reference state to\nmodify the system's reachability to comply with the new constraints. We frame\nthe reference state search as an optimization problem and employ the\nKarush-Kuhn-Tucker (KKT) method as well as the Interior Point Method (IPM)\nbased Newton's method (as a fallback for the KKT method) for fast solution\nderivation. The optimization also allows more future fault tolerance. Numerical\nsimulations demonstrate that our method outperforms the conventional OCR method\nin terms of computational efficiency and success rate. Specifically, the\nresults show that the proposed method finds a solution $10^{2}$ (with the IPM\nbased Newton's method) $\\sim 10^{4}$ (with the KKT method) times faster than\nthe OCR method. Additionally, the improvement rate of the success rate of our\nmethod over the OCR method is $40.81\\%$ without considering the deadline of run\ntime. The success rate remains at $49.44\\%$ for the proposed method, while it\nbecomes $0\\%$ for the OCR method when a deadline of $1.5 \\; seconds$ is\nimposed.",
        "We present a Reinforcement Learning Platform for Adversarial Black-box\nuntargeted and targeted attacks, RLAB, that allows users to select from various\ndistortion filters to create adversarial examples. The platform uses a\nReinforcement Learning agent to add minimum distortion to input images while\nstill causing misclassification by the target model. The agent uses a novel\ndual-action method to explore the input image at each step to identify\nsensitive regions for adding distortions while removing noises that have less\nimpact on the target model. This dual action leads to faster and more efficient\nconvergence of the attack. The platform can also be used to measure the\nrobustness of image classification models against specific distortion types.\nAlso, retraining the model with adversarial samples significantly improved\nrobustness when evaluated on benchmark datasets. The proposed platform\noutperforms state-of-the-art methods in terms of the average number of queries\nrequired to cause misclassification. This advances trustworthiness with a\npositive social impact.",
        "Context. Binary asteroids are present in all populations of the Solar System,\nfrom near-Earth to trans-Neptunian regions. As is true for the small Solar\nSystem bodies (SSSBs), binary asteroids generally offer valuable insights into\nthe formation of the Solar System, as well as its collisions and dynamic\nevolution. In particular, the binaries provide fundamental quantities and\nproperties of these SSSBs, such as mass, angular momentum, and density, all of\nwhich are often hidden. The direct measurement of densities and porosities is\nof great value in revealing the gravitational aggregates and icy bodies that\nform the asteroid-comet continuum. Aims. Several observation techniques from\nspace and ground-based platforms have provided many results in this regard.\nHere we show the value of the Gaia mission and its high-precision astrometry\nfor analysing asteroid binaries and for individually deriving the masses of the\ncomponents. Methods. We focus on the binary asteroid (4337) Arecibo, a member\nof the Themis family. We analysed the astrometry obtained in the Gaia FPR\ncatalogue release, and performed orbital fitting for both the heliocentric\norbit of the system and the relative orbit of the binary components. Results.\nWe obtain an estimation of the component masses and their flux ratio, and\nderive bulk densities rho1 = 1.2 and rho2 = 1.6 for the primary and the\nsecondary, respectively. The results are consistent with an ice-rich body in\nthe outer main belt. They also show a significantly denser secondary or a less\nclosely packed primary. Constraints on these densities and on macroscopic\nporosities are nevertheless limited by our poor knowledge of the sizes of the\ncomponents. Observations of future mutual events, and of stellar occultations\npredicted in 2024 - 2025, will be essential for improving our knowledge of this\nsystem and its formation.",
        "Data preprocessing is a critical yet frequently neglected aspect of machine\nlearning, often paid little attention despite its potentially significant\nimpact on model performance. While automated machine learning pipelines are\nstarting to recognize and integrate data preprocessing into their solutions for\nclassification and regression tasks, this integration is lacking for more\nspecialized tasks like survival or time-to-event models. As a result, survival\nanalysis not only faces the general challenges of data preprocessing but also\nsuffers from the lack of tailored, automated solutions in this area.\n  To address this gap, this paper presents 'CleanSurvival', a\nreinforcement-learning-based solution for optimizing preprocessing pipelines,\nextended specifically for survival analysis. The framework can handle\ncontinuous and categorical variables, using Q-learning to select which\ncombination of data imputation, outlier detection and feature extraction\ntechniques achieves optimal performance for a Cox, random forest, neural\nnetwork or user-supplied time-to-event model. The package is available on\nGitHub: https:\/\/github.com\/datasciapps\/CleanSurvival\n  Experimental benchmarks on real-world datasets show that the Q-learning-based\ndata preprocessing results in superior predictive performance to standard\napproaches, finding such a model up to 10 times faster than undirected random\ngrid search. Furthermore, a simulation study demonstrates the effectiveness in\ndifferent types and levels of missingness and noise in the data.",
        "Regularizing neural networks is important for anticipating model behavior in\nregions of the data space that are not well represented. In this work, we\npropose a regularization technique for enforcing a level of smoothness in the\nmapping between the data input space and the loss value. We specify the level\nof regularity by requiring that the loss of the network satisfies an elliptic\noperator over the data domain. To do this, we modify the usual empirical risk\nminimization objective such that we instead minimize a new objective that\nsatisfies an elliptic operator over points within the domain. This allows us to\nuse existing theory on elliptic operators to anticipate the behavior of the\nerror for points outside the training set. We propose a tractable computational\nmethod that approximates the behavior of the elliptic operator while being\ncomputationally efficient. Finally, we analyze the properties of the proposed\nregularization to understand the performance on common problems of distribution\nshift and group imbalance. Numerical experiments confirm the utility of the\nproposed regularization technique.",
        "Recent advancements in Retrieval-Augmented Generation (RAG) have shown\nremarkable performance in enhancing response accuracy and relevance by\nintegrating external knowledge into generative models. However, existing RAG\nmethods primarily focus on providing text-only answers, even in multimodal\nretrieval-augmented generation scenarios. In this work, we introduce the\nMultimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims\nto generate answers that combine both text and images, fully leveraging the\nmultimodal data within a corpus. Despite the importance of this task, there is\na notable absence of a comprehensive benchmark to effectively evaluate MRAMG\nperformance. To bridge this gap, we introduce the MRAMG-Bench, a carefully\ncurated, human-annotated dataset comprising 4,346 documents, 14,190 images, and\n4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and\nLifestyle. The dataset incorporates diverse difficulty levels and complex\nmulti-image scenarios, providing a robust foundation for evaluating multimodal\ngeneration tasks. To facilitate rigorous evaluation, our MRAMG-Bench\nincorporates a comprehensive suite of both statistical and LLM-based metrics,\nenabling a thorough analysis of the performance of popular generative models in\nthe MRAMG task. Besides, we propose an efficient multimodal answer generation\nframework that leverages both LLMs and MLLMs to generate multimodal responses.\nOur datasets are available at: https:\/\/huggingface.co\/MRAMG.",
        "There has been much interest in the distribution of the circumference, the\nlength of the longest cycle, of a random graph $G(n,p)$ in the sparse regime,\nwhen $p = \\Theta\\left(\\frac{1}{n}\\right)$. Recently, the first author and\nFrieze established a scaling limit for the circumference in this regime, along\nthe way establishing an alternative 'structural' approximation for this\nparameter. In this paper, we give a central limit theorem for the circumference\nin this regime using a novel argument based on the Efron-Stein inequality,\nwhich relies on a combinatorial analysis of the effect of resampling edges on\nthis approximation.",
        "Engineering all fundamental magnetic phases within a single material platform\nwould mark a significant milestone in materials science and spintronics,\nreducing complexity and costs in device fabrication by eliminating the need for\nintegrating and interfacing different materials. Here, we demonstrate that\ngraphene can host all non-relativistic magnetic phases-namely, diamagnetism,\nparamagnetism, ferromagnetism, antiferromagnetism, ferrimagnetism,\naltermagnetism and fully compensated ferrimagnetism -- by using single hydrogen\natoms as building blocks. Through precise manipulation of these atoms by\nscanning tunneling microscopy, we can experimentally create all such magnetic\nphases. Their different magnetic character is confirmed by density functional\ntheory and mean-field Hubbard calculations. In particular, we show that the new\nmagnetic paradigm known as altermagnetism can be realized, exhibiting\ndirectionally spin-split energy bands coexisting with zero net magnetization\ndue to protecting spatial symmetries. It is furthermore possible to create\nfully compensated ferrimagnets, lacking these symmetries and therefore\npresenting unrestricted spin splitting of the bands, with a vanishing net\nmagnetization which in this case is protected by Lieb's theorem. These findings\nput forward H-functionalized graphene as a versatile platform to design, build\nand study these new emergent magnetic phases at the atomic scale.",
        "This paper concerns the problem of reachability of a given state for a\nmultiagent control system in $\\mathbb{R}^d$. In such a system, at every time\neach agent can choose his\/her velocity which depends both on his\/her position\nand on the position of the whole crowd of agents (modeled by a probability\nmeasure on $ \\mathbb{R}^d$). The main contribution of the paper is to study the\nabove reachability problem with a given rate of attainability through a\nLyapunov method adapted to the Wasserstein space of probability measures. As a\nbyproduct we obtain a new comparison result for viscosity solutions of Hamilton\nJacobi equations in the Wasserstein space.",
        "Anderson localization physics features three fundamental types of\neigenstates: extended, localized, and critical. Confirming the presence of\ncritical states necessitates either advancing the analysis to the thermodynamic\nlimit or identifying a universal mechanism which can determine rigorously these\nstates. Here we report the unambiguous experimental realization of critical\nstates, governed by a rigorous mechanism for exact quantum critical states, and\nfurther observe a generalized mechanism that quasiperiodic zeros in hopping\ncouplings protect the critical states. Leveraging a superconducting quantum\nprocessor with up to 56 qubits, we implement a programmable mosaic model with\ntunable couplings and on-site potentials. By measuring time-evolved\nobservables, we identify both delocalized dynamics and incommensurately\ndistributed zeros in the couplings, which are the defining features of the\ncritical states. We map the localized-to-critical phase transition and\ndemonstrate that critical states persist until quasiperiodic zeros are removed\nby strong long-range couplings, confirming the generalized mechanism. Finally,\nwe resolve the energy-dependent transition between localized and critical\nstates, revealing the presence of anomalous mobility edges.",
        "Adaptive spatial meshing has proven invaluable for the accurate, efficient\ncomputation of solutions of time dependent partial differential equations. In a\nDA context the use of adaptive spatial meshes addresses several factors that\nplace increased demands on meshing; these include the location and relative\nimportance of observations and the use of ensemble solutions. To increase the\nefficiency of adaptive meshes for data assimilation, robust look ahead meshes\nare developed that fix the same adaptive mesh for all ensemble members for the\nentire time interval of the forecasts and that incorporates the observations at\nthe next analysis time. This allows for increased vectorization of the ensemble\nforecasts while minimizing interpolation of solutions between different meshes.\nThe techniques to determine these robust meshes are based upon combining metric\ntensors or mesh density functions to define nonuniform meshes. We illustrate\nthe robust ensemble look ahead meshes using traveling wave solutions of a\nbistable reaction-diffusion equation. Observation operators based on\nconvolution type integrals and their associated metric tensors are derived.\nThese further the goals of making efficient use of adaptive meshes in ensemble\nbased DA techniques, developing and employing robust meshes that are effective\nfor a range of similar behaviors in both the ensembles and the observations,\nand the integration with advanced numerical PDE techniques (a quasi-Lagrangian\nmoving mesh DG technique employing embedded pairs for time stepping). Numerical\nexperiments with different observation scenarios are presented for a 2D\ninviscid Burgers' equation, a multi-component system, a 2D Shallow Water model,\nand for a coupled system of two 1D Kuramoto-Sivashinsky equations."
      ]
    }
  },
  {
    "id":2411.03551,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation",
    "start_abstract":"Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Interobserver Variability in the CT Assessment of Honeycombing in the Lungs"
      ],
      "abstract":[
        "To quantify observer agreement and analyze causes of disagreement in identifying honeycombing at chest computed tomography (CT).The institutional review board approved this multiinstitutional HIPAA-compliant retrospective study, informed patient consent was not required. Five core study members scored 80 CT images with a five-point scale (5 = definitely yes to 1 no) establish reference standard for the identification honeycombing. Forty-three observers from various subspecialties geographic regions by using same scoring system. Weighted \u03ba values scores compared were analyzed investigate intergroup differences. Images divided into four groups allow analysis imaging features cases which there disagreement: on presence honeycombing, absence other (none preceding three applied).Agreement 43 moderate (Cohen weighted values: 0.40-0.58). There no significant differences among defined either subspecialty or region (Tukey-Kramer test, P .38 >.99). In 29% cases, These included mixed traction bronchiectasis, large cysts, superimposed pulmonary emphysema.Identification is subjective, largely caused conditions that mimic"
      ],
      "categories":[
        "Radiology"
      ]
    },
    "list":{
      "title":[
        "Development and Performance Validation of a Versatile VLBI Digital\n  Backend Using the ROACH2 Platform",
        "Towards An Updated Simulation of the Booster Neutrino Beam",
        "Kinetic-Scale Physics of a Multi-Species Solar Wind",
        "Safe Gradient Flow for Bilevel Optimization",
        "Denoising Diffused Embeddings: a Generative Approach for Hypergraphs",
        "The subconvexity bound for standard L-function in level aspect",
        "Stabilizer-free Weak Galerkin Methods for Quad-Curl Problems on\n  polyhedral Meshes without Convexity Assumptions",
        "Connection formulae for the radial Toda equations II",
        "In-operando test of tunable Heusler alloys for thermomagnetic harvesting\n  of low-grade waste heat",
        "White Dwarf Variability",
        "Doubly-polylog-time-overhead fault-tolerant quantum computation by a\n  polylog-time parallel minimum-weight perfect matching decoder",
        "Relating electrodynamics and gravity in two Euclidean dimensions",
        "A parameter scan of dark zone maintenance for high-contrast imaging of\n  exoplanets using theoretical and experimental implementations",
        "Self-sustained Josephson dynamics and self-trapping in supersolids",
        "DeepGrav: Anomalous Gravitational-Wave Detection Through Deep Latent\n  Features",
        "An SIRS-model considering waning efficiency and periodic re-vaccination",
        "Continuous spectrum-shrinking maps and applications to preserver\n  problems",
        "Searching for rotation in X-COP galaxy clusters",
        "Intrinsic width of the flux tube in 2+1 dimensional Yang-Mills therories",
        "Propagation of extreme events in multiplex neuronal networks",
        "Causal Inference on Outcomes Learned from Text",
        "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
        "Numerical Investigation of Preferential Flow Paths in Enzymatically\n  Induced Calcite Precipitation supported by Bayesian Model Analysis",
        "Braneworld Neutron Stars: Constraining Brane Tension with Observational\n  Data",
        "A Novel Design for SRAM Bitcell with 3-Complementary-FETs",
        "Sanity Checking Causal Representation Learning on a Simple Real-World\n  System",
        "Dimming GRS 1915+105 observed with NICER and Insight--HXMT",
        "Amortized Bayesian Mixture Models",
        "The Mechanism of Shrimpoluminescence"
      ],
      "abstract":[
        "Customized digital backends for Very Long Baseline Interferometry (VLBI) are\ncritical components for radio astronomy observatories. There are several\nserialized products such as the Digital Baseband Converter (DBBC),\nReconfigurable Open Architecture Computing Hardware (ROACH) Digital BackEnd\n(RDBE), and Chinese Data Acquisition System (CDAS). However, the reliance on\nhigh-speed analog-to-digital converters (ADC) and Field Programmable Gate\nArrays (FPGAs) often necessitates dedicated hardware platforms with long\ndevelopment cycles and prohibitive cost, limiting scalability and adaptability\nto evolving observational needs. To address these challenges, we propose a\ndesign leveraging the versatile and cost-effective ROACH2 hardware platform,\ndeveloped by CASPER (Collaboration for Astronomy Signal Processing and\nElectronics Research). ROACH2's mature technology and streamlined firmware\ndevelopment capabilities significantly reduce the hardware platform's\ndevelopment cycle and cost, making it ideal for modern astronomical\napplications. This VLBI digital backend, based on the ROACH2 platform,\nincorporates key technologies such as Polyphase Filter Banks (PFB) algorithm\nimplementation, digital complex-to-real baseband signal conversion, Mark5B data\nformatter design and two-bit optimal threshold quantization. These features\nensure compatibility with existing systems while providing enhanced\nperformance. The backend's performance was validated through multi-station VLBI\nexperiments, demonstrating its ability to achieve good correlation fringes\ncompared to the customized CDAS2-D system. Furthermore, this platform offers\nflexibility for rapid deployment of additional digital backends, such as those\nfor spectral line observations, showcasing its potential for broader\nastronomical applications.",
        "Precise, accurate neutrino flux predictions for neutrino beam experiments are\ncrucial for physics results. Flux predictions for the Booster Neutrino Beam at\nFermilab were first published in 2009 by the MiniBooNE collaboration. It is no\nlonger possible to run the simulation used to create these predictions due to\noutdated software versions. In this exciting period for the Short-Baseline\nNeutrino Program at Fermilab, with both the far detector (ICARUS) and the near\ndetector (SBND) operating, an updated flux model for the BNB is necessary. For\nthe purpose of using a dynamic, up-to-date beam model, the SBN program has\ncreated a new simulation named G4BNB. This framework contains new features,\nsuch as a full neutrino ancestry to handle hadron production systematics with\nmore precision, and the inclusion of neutral mesons from the proton-beryllium\nscatter to allow the study of exotic BSM scenarios.",
        "The solar wind affects the plasma environment around all solar system bodies.\nA strong solar wind dynamic pressure pushes plasma boundaries closer to these\nobjects. For small objects kinetic effects on scales smaller than an ion\ngyroradius play an important role, and species with various mass-per-charge may\nact differently. In this case the solar wind composition can be important.\nProtons are the dominant ion species in the solar wind; however, sometimes the\ndensity of alpha particles increases significantly. We analyse the effect of\ndifferent solar wind alpha-to-proton ratios on the plasma boundaries of the\ninduced cometary magnetosphere. In addition, we investigate the energy transfer\nbetween the solar wind ions, the cometary ions, and the electromagnetic fields.\nUsing the hybrid model Amitis, we simulate two different alpha-to-proton ratios\nand analyse the resulting plasma structures. We calculate the power density\n(E.J) of all three ion species (solar wind protons and alphas, and cometary\nions) to identify load and generator regions. The integrated 1D power density\nshows the evolution of the power density from the upstream solar wind to\ndownstream of the nucleus. A higher alpha-to-proton ratio leads to a larger\ncomet magnetosphere but weaker magnetic field pile-up. The protons transfer\nenergy to the fields and the cometary ions in the entire upstream region and\nthe pile-up layer. Upstream of the nucleus, alphas are inefficient in\ntransferring energy and can act as a load, especially for low alpha-to-proton\nratios. The transfer of energy from alphas to cometary ions happens further\ndownstream due to their larger inertia. For a multi-species solar wind the mass\nloading and energy transfer upstream of the pile-up layer will be most\nefficient for the species with the lowest inertia, typically protons, since\ndifferent ion gyroradii give different flow patterns for the individual\nspecies.",
        "Bilevel optimization is a key framework in hierarchical decision-making,\nwhere one problem is embedded within the constraints of another. In this work,\nwe propose a control-theoretic approach to solving bilevel optimization\nproblems. Our method consists of two components: a gradient flow mechanism to\nminimize the upper-level objective and a safety filter to enforce the\nconstraints imposed by the lower-level problem. Together, these components form\na safe gradient flow that solves the bilevel problem in a single loop. To\nimprove scalability with respect to the lower-level problem's dimensions, we\nintroduce a relaxed formulation and design a compact variant of the safe\ngradient flow. This variant minimizes the upper-level objective while ensuring\nthe lower-level decision variable remains within a user-defined suboptimality.\nUsing Lyapunov analysis, we establish convergence guarantees for the dynamics,\nproving that they converge to a neighborhood of the optimal solution. Numerical\nexperiments further validate the effectiveness of the proposed approaches. Our\ncontributions provide both theoretical insights and practical tools for\nefficiently solving bilevel optimization problems.",
        "Hypergraph data, which capture multi-way interactions among entities, are\nbecoming increasingly prevalent in the big data eta. Generating new hyperlinks\nfrom an observed, usually high-dimensional hypergraph is an important yet\nchallenging task with diverse applications, such as electronic health record\nanalysis and biological research. This task is fraught with several challenges.\nThe discrete nature of hyperlinks renders many existing generative models\ninapplicable. Additionally, powerful machine learning-based generative models\noften operate as black boxes, providing limited interpretability. Key\nstructural characteristics of hypergraphs, including node degree heterogeneity\nand hyperlink sparsity, further complicate the modeling process and must be\ncarefully addressed. To tackle these challenges, we propose Denoising Diffused\nEmbeddings (DDE), a general generative model architecture for hypergraphs. DDE\nexploits potential low-rank structures in high-dimensional hypergraphs and\nadopts the state-of-the-art diffusion model framework. Theoretically, we show\nthat when true embeddings are accessible, DDE exactly reduces the task of\ngenerating new high-dimensional hyperlinks to generating new low-dimensional\nembeddings. Moreover, we analyze the implications of using estimated embeddings\nin DDE, revealing how hypergraph properties--such as dimensionality, node\ndegree heterogeneity, and hyperlink sparsity--impact its generative\nperformance. Simulation studies demonstrate the superiority of DDE over\nexisting methods, in terms of both computational efficiency and generative\naccuracy. Furthermore, an application to a symptom co-occurrence hypergraph\nderived from electronic medical records uncovers interesting findings and\nhighlights the advantages of DDE.",
        "In this paper we prove a new subconvexity result for the standard L-function\nof a unitary cuspidal automorphic representation $\\pi$ of $\\text{GL}_n$, where\nthe finite set of places $S$ with large conductors is allowed to vary, provided\nthat the local parameters at every place in $S$ satisfy certain uniform growth\ncondition.",
        "This paper introduces an efficient stabilizer-free weak Galerkin (WG) finite\nelement method for solving the three-dimensional quad-curl problem. Leveraging\nbubble functions as a key analytical tool, the method extends the applicability\nof stabilizer-free WG approaches to non-convex elements in finite element\npartitions-a notable advancement over existing methods, which are restricted to\nconvex elements. The proposed method maintains a simple, symmetric, and\npositive definite formulation. It achieves optimal error estimates for the\nexact solution in a discrete norm, as well as an optimal-order $L^2$ error\nestimate for $k>2$ and a sub-optimal order for the lowest order case $k=2$.\nNumerical experiments are presented to validate the method's efficiency and\naccuracy.",
        "This is a continuation of [arXiv:2309.16550] in which we computed the\nasymptotics near $x = \\infty$ of all solutions of the radial Toda equation. In\nthis article, we compute the asymptotics near $x = \\infty$ of all solutions of\na \"partner\" equation. The equations are related in the sense that their\nrespective monodromy data constitute connected components of the same\n\"monodromy manifold\". While all solutions of the radial Toda equation are\nsmooth, those of the partner equation have infinitely many singularities, and\nthis makes the Riemann-Hilbert nonlinear steepest descent method (and the\nasymptotics of solutions) more involved.",
        "Thermomagnetic generation stands out as a promising technology for harvesting\nand converting low-grade waste heat below 100 {\\deg}C. Despite the exponential\ngrowth in research on thermomagnetic materials and prototypes over the last\ndecade, there remains, to unlock the full potential of this technology, a\ncritical gap between fundamental research on materials and the design of\nadvanced devices. In this study, we present the in-operando assessment of\nthermomagnetic performance of three representative Ni,Mn-based Heusler alloys\noptimized for harvesting low-grade waste heat below 373 K. These materials were\ntested under operational conditions using a specially designed laboratory-scale\nprototype of a thermomagnetic motor. The mechanical power output of the motor,\noperated with NiMnIn, NiMnSn and NiMnCuGa alloys, was correlated with the\nmagnetic properties of the materials, highlighting the critical role of the\nmagnetic transition temperature and saturation magnetization in determining the\nefficiency of thermomagnetic energy conversion. Austenitic Heusler alloys were\nconfirmed to be promising thermomagnetic materials due to their highly tunable\nCurie temperature and significant magnetization changes in the 300-360 K\ntemperature range. Among the tested materials, the Ni48Mn36In16 alloy\ndemonstrated the highest thermomagnetic performance, surpassing the benchmark\nmaterial Gd in the 320-340 K range. From an experimental perspective, the\ndeveloped prototype of thermomagnetic motor serves as a flexible test-bench for\nevaluating and comparing the thermomagnetic performance of small amounts (less\nthan 0.3 g) of new materials under variable conditions. Additionally, its\nmodular design facilitates testing and optimization of its various components,\nthereby contributing to the advancement of thermomagnetic motor technology.",
        "There are a few different mechanisms that can cause white dwarf stars to vary\nin brightness, providing opportunities to probe the physics, structures, and\nformation of these compact stellar remnants. The observational characteristics\nof the three most common types of white dwarf variability are summarized:\nstellar pulsations, rotation, and ellipsoidal variations from tidal distortion\nin binary systems. Stellar pulsations are emphasized as the most complex type\nof variability, which also has the greatest potential to reveal the conditions\nof white dwarf interiors.",
        "Reducing space and time overheads of fault-tolerant quantum computation\n(FTQC) has been receiving increasing attention as it is crucial for the\ndevelopment of quantum computers and also plays a fundamental role in\nunderstanding the feasibility and limitations of realizing quantum advantages.\nShorter time overheads are particularly essential for demonstrating quantum\ncomputational speedups without compromising runtime advantages. However,\nsurpassing the conventional polylogarithmic (polylog) scaling of time overheads\nhas remained a significant challenge, since it requires addressing all\npotential bottlenecks, including the nonzero runtime of classical computation\nfor decoding in practical implementations. In this work, we construct a\nprotocol that achieves FTQC with doubly polylog time overhead while maintaining\nthe conventional polylog space overhead. The key to our approach is the\ndevelopment of a highly parallelizable minimum-weight perfect matching (MWPM)\ndecoder, which achieves a polylog parallel runtime in terms of the code size\nwhile providing theoretical guarantees on threshold existence and overhead\nbounds. Our protocol integrates this decoder with a topological-code protocol\nthat incorporates single-shot decoding for efficient syndrome extraction;\nfurthermore, we concatenate this with the concatenated Steane codes to\nguarantee the existence of the threshold while avoiding a backlog problem,\nenabling us to achieve doubly polylog time overheads even when accounting for\nthe decoder's runtime. These results suggest the feasibility of surpassing the\nconventional polylog-time-overhead barrier, opening a new frontier in\nlow-overhead FTQC.",
        "Two-dimensional electrodynamics coupled to Dirac fermions is mapped onto\ntwo-dimensional gravity in the first-order formalism, also including fermions.\nHowever, the resulting fermion-gravity coupling deviates from the conventional\nform, explicitly violating parity and time-reversal symmetries. Additionally,\nthese fermions exhibit an unconventional transformation behavior under $SO(2)$\ntransformations. Furthermore, we analyze the consistency of this mapping at the\nquantum level using the path integral formalism and Becchi-Rouet-Stora-Tyutin\ntechniques. Our findings demonstrate that quantum electrodynamics and quantum\ngravity remain equivalent at the quantum level.",
        "Maintaining wavefront stability while directly imaging exoplanets over long\nexposure times is an ongoing problem in the field of high-contrast imaging.\nRobust and efficient high-order wavefront sensing and control systems are\nrequired for maintaining wavefront stability to counteract mechanical and\nthermal instabilities. Dark zone maintenance (DZM) has been proposed to address\nquasi-static optical aberrations and maintain high levels of contrast for\ncoronagraphic space telescopes. To further experimentally test this approach\nfor future missions, such as the Habitable Worlds Observatory, this paper\nquantifies the differences between the theoretical closed-loop contrast bounds\nand DZM performance on the High-contrast Imager for Complex Aperture\nTelescopes(HiCAT) testbed. The quantification of DZM is achieved by traversing\nimportant parameters of the system, specifically the total direct photon rate\nentering the aperture of the instrument, ranging from $1.85 \\times 10^6$ to\n$1.85 \\times 10^8$ photons per second, and the wavefront error drift rate,\nranging from $\\sigma_{drift}$ = 0.3 - 3 $nm\/\\sqrt{iteration}$, injected via the\ndeformable mirror actuators. This is tested on the HiCAT testbed by injecting\nrandom walk drifts using two Boston Micromachines kilo deformable mirrors\n(DMs). The parameter scan is run on the HiCAT simulator and the HiCAT testbed\nwhere the corresponding results are compared to the model-based theoretical\ncontrast bounds to analyze discrepancies. The results indicate an approximate\none and a half order of magnitude difference between the theoretical bounds and\ntestbed results.",
        "We explore the self-sustained Josephson junction dynamics in dipolar\nsupersolids, predicting the possibility of self-trapping alongside the\nexperimentally observed Josephson oscillations [Biagioni, G. et al., Nature\n629, 773 (2024)]. Using an asymmetric two-mode (ATM) model to describe a\ntriangular dipolar supersolid, validated through Gross-Pitaevskii simulations,\nwe demonstrate that the system's symmetry enables a consistent two-mode mapping\ndespite the presence of seven droplets. Additionally, we show that bringing the\nsystem into rotation preserves its ability to sustain the Josephson junction\ndynamics across its full range, and we assess the robustness of the ATM model\nunder these conditions.",
        "This work introduces a novel deep learning-based approach for gravitational\nwave anomaly detection, aiming to overcome the limitations of traditional\nmatched filtering techniques in identifying unknown waveform gravitational wave\nsignals. We introduce a modified convolutional neural network architecture\ninspired by ResNet that leverages residual blocks to extract high-dimensional\nfeatures, effectively capturing subtle differences between background noise and\ngravitational wave signals. This network architecture learns a high-dimensional\nprojection while preserving discrepancies with the original input, facilitating\nprecise identification of gravitational wave signals. In our experiments, we\nimplement an innovative data augmentation strategy that generates new data by\ncomputing the arithmetic mean of multiple signal samples while retaining the\nkey features of the original signals.\n  In the NSF HDR A3D3: Detecting Anomalous Gravitational Wave Signals\ncompetition, it is honorable for us (group name: easonyan123) to get to the\nfirst place at the end with our model achieving a true negative rate (TNR) of\n0.9708 during development\/validation phase and 0.9832 on an unseen challenge\ndataset during final\/testing phase, the highest among all competitors. These\nresults demonstrate that our method not only achieves excellent generalization\nperformance but also maintains robust adaptability in addressing the complex\nuncertainties inherent in gravitational wave anomaly detection.",
        "In this paper, we extend the classical SIRS\n(Susceptible-Infectious-Recovered-Susceptible) model from mathematical\nepidemiology by incorporating a vaccinated compartment, V, accounting for an\nimperfect vaccine with waning efficacy over time. The SIRSV-model divides the\npopulation into four compartments and introduces periodic re-vaccination for\nwaning immunity. The efficiency of the vaccine is assumed to decay with the\ntime passed since the vaccination. Periodic re-vaccinations are applied to the\npopulation. We develop a partial differential equation (PDE) model for the\ncontinuous vaccination time and a coupled ordinary differential equation (ODE)\nsystem when discretizing the vaccination period. We analyze the equilibria of\nthe ODE model and investigate the linear stability of the disease-free\nequilibrium (DFE). Furthermore, we explore an optimization framework where\nvaccination rate, re-vaccination time, and non-pharmaceutical interventions\n(NPIs) are control variables to minimize infection levels. The optimization\nobjective is defined using different norm-based measures of infected\nindividuals. A numerical analysis of the model's dynamic behavior under varying\ncontrol parameters is conducted using path-following methods. The analysis\nfocuses on the impacts of vaccination strategies and contact limitation\nmeasures. Bifurcation analysis reveals complex behaviors, including\nbistability, fold bifurcations, forward and backward bifurcations, highlighting\nthe need for combined vaccination and contact control strategies to manage\ndisease spread effectively.",
        "For a positive integer $n$ let $\\mathcal{X}_n$ be either the algebra $M_n$ of\n$n \\times n$ complex matrices, the set $N_n$ of all $n \\times n$ normal\nmatrices, or any of the matrix Lie groups $\\mathrm{GL}(n)$, $\\mathrm{SL}(n)$\nand $\\mathrm{U}(n)$. We first give a short and elementary argument that for two\npositive integers $m$ and $n$ there exists a continuous spectrum-shrinking map\n$\\phi : \\mathcal{X}_n \\to M_m$ (i.e.\\ $\\mathrm{sp}(\\phi(X))\\subseteq\n\\mathrm{sp}(X)$ for all $X \\in \\mathcal{X}_n$) if and only if $n$ divides $m$.\nMoreover, in that case we have the equality of characteristic polynomials\n$k_{\\phi(X)}(\\cdot) = k_{X}(\\cdot)^\\frac{m}{n}$ for all $X \\in \\mathcal{X}_n$,\nwhich in particular shows that $\\phi$ preserves spectra. Using this we show\nthat whenever $n \\geq 3$, any continuous commutativity preserving and\nspectrum-shrinking map $\\phi : \\mathcal{X}_n \\to M_n$ is of the form\n$\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$, for some $T\\in\n\\mathrm{GL}(n)$. The analogous results fail for the special unitary group\n$\\mathrm{SU}(n)$, and slightly more elaborate versions hold for the spaces of\nsemisimple elements in either $\\mathrm{GL}(n)$ or $\\mathrm{SL}(n)$, where a\nqualitatively new (and surprising) phenomenon arises: the map sending\n$SNS^{-1}$ to $S^{-1}NS$ for positive invertible $S$ and normal $N$ is also an\nexample. As a consequence, we also recover (a strengthened version of)\n\\v{S}emrl's influential characterization of Jordan automorphisms of $M_n$ via\npreserving properties.",
        "We search for evidence of rotational support by analyzing the thermodynamic\nprofiles of the intracluster medium (ICM) in a sample of nearby, massive galaxy\nclusters. For each object of the XMM-Newton Cluster Outskirts Project (X-COP)\nsample, we present axisymmetric models of rotating ICM with composite\npolytropic distributions, in equilibrium in spherically symmetric dark halos,\nexploring cases both with and without turbulent support in the ICM. The profile\nof rotation velocity and the distribution of turbulent velocity dispersion are\ndescribed with flexible functional forms, consistent with the properties of\nsynthetic clusters formed in cosmological simulations. The models are tuned via\na Markov Chain Monte Carlo algorithm to reproduce the radial profiles of the\nthermodynamic variables as resolved in the XMM-Newton and Planck maps, and to\nbe consistent with the mass distributions estimated either from weak lensing\nobservations (when available) or under the assumption of a \"universal\" value of\nthe baryon fraction. Our models indicate that there is room for non-negligible\nrotation in the ICM of massive clusters, with typical peak rotation speed 300\nkm\/s and peak rotation-velocity-to-velocity-dispersion ratio of 0.3. According\nto our models, the ICM in Abell 2255 can have a rotation speed as high as 500\nkm\/s, corresponding to a rotation-velocity-to-velocity-dispersion ratio of 0.3,\nat a distance of 100 kpc from the center, where the X-ray emissivity is still\nhigh. This makes Abell 2255 a very promising candidate for the presence of\nrotation in the ICM that could be detected with the currently operating XRISM\nobservatory, as we demonstrate computing and analyzing a mock X-ray spectrum.",
        "We study the shape of the flux tube in lattice Yang-Mills theories and in\nparticular its intrinsic width. In the framework of the Effective String Theory\ndescription of the confining flux tube this intrinsic width has no measurable\neffects on the inter-quark static potential, but it can be precisely detected\nlooking at the profile of the flux tube. We address this problem with a set of\nhigh precision simulations in the (2+1) dimensional SU(2) model. We find two\ndifferent behaviours as a function of the temperature. In the low temperature\nregime ($T \\ll T_c$) we find a good agreement with an expression inspired by\nthe dual superconductive model of confinement. In the high temperature regime\n($T \\lesssim T_c$) our data agree with a model based on the Svetitsky-Yaffe\nmapping. All our data in this regime can be described in terms of only one\nlength scale, the intrinsic width, which turns out to be the same scale\nappearing in the confining inter-quark static potential.",
        "In previous studies, the propagation of extreme events across nodes in\nmonolayer networks has been extensively studied. In this work, we extend this\ninvestigation to explore the propagation of extreme events between two distinct\nlayers in a multiplex network. We consider a two-layer network, where one layer\nis globally coupled and exhibits extreme events, while the second layer remains\nuncoupled. The interlayer connections between the layers are either\nunidirectional or bidirectional. We find that unidirectional coupling between\nthe layers can induce extreme events in the uncoupled layer, whereas\nbidirectional coupling tends to mitigate extreme events in the globally coupled\nlayer. To characterize extreme and non-extreme states, we use probability plots\nto identify distinct regions in the parameter space. Additionally, we study the\nrobustness of extreme events emergence by examining various network topologies\nin the uncoupled layer. The mechanism behind the occurrence of extreme events\nis explored, with a particular focus on the transition from asynchronous states\nto a fully synchronized excitable state. For numerical simulations, we use\nnonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical\nbehavior of both coupled and uncoupled layers. Our findings suggest that\nextreme events in the uncoupled layer emerge through the gradual disappearance\nof disorder, accompanied by occasional bursts of synchronized activity. Results\nobtained in this work will serve a starting point in understanding the dynamics\nbehind the propagation of extreme events in real-world networks.",
        "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
        "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
        "The usability of enzymatically induced calcium carbonate precipitation (EICP)\nas a method for altering porous-media properties, soil stabilization, or\nbiocementation depends on our ability to predict the spatial distribution of\nthe precipitated calcium carbonate in porous media. While current REV-scale\nmodels are able to reproduce the main features of laboratory experiments, they\nneglect effects like the formation of preferential flow paths and the\nappearance of multiple polymorphs of calcium carbonate with differing\nproperties. We show that extending an existing EICP model by the conceptual\nassumption of a mobile precipitate, amorphous calcium carbonate (ACC), allows\nfor the formation of preferential flow paths when the initial porosity is\nheterogeneous. We apply sensitivity analysis and Bayesian inference to gain an\nunderstanding of the influence of characteristic parameters of ACC that are\nuncertain or unknown and compare two variations of the model based on different\nformulations of the ACC detachment term to analyse the plausibility of our\nhypothesis. An arbitrary Polynomial Chaos (aPC) surrogate model is trained\nbased on the full model and used to reduce the computational cost of this\nstudy.",
        "In this article, we investigate the properties of neutron stars within the\nbraneworld model, employing six distinct piece-wise polytropic equation of\nstates. These equation of states satisfy observational constraints put by\nGW170817 event and pulsar observations (PSR J0740 and PSR J0030) within general\nrelativity framework. Our primary goal is to assess whether these equation of\nstates, in conjunction with the braneworld framework, can accommodate more\nmassive neutron stars, as suggested by the GW190814 observation, while\nremaining consistent with established observational constraints. The brane\ntension parameter significantly affects the mass-radius and mass-tidal\ndeformability relations, particularly for neutron stars with masses exceeding\nthe canonical value. We establish strong constraints on the brane tension by\ncomparing the canonical neutron star radius and tidal deformability with the\nresults from the braneworld model, a stringent lower bound on the brane\ntension, $\\lambda > 2 \\times 10^ {37} \\, \\text {dyne\/cm} ^2 $. Our results\ndemonstrate that the braneworld model allows for the existence of neutron stars\nwith masses greater than those predicted by General Relativity, in agreement\nwith the GW190814 observation, and highlight the significant role of brane\ntension in shaping the properties of neutron stars.",
        "The complementary field-effect transistors (CFETs), featuring vertically\nstacked n\/p-FETs, enhance integration density and significantly reduce the area\nof standard cells such as static random-access memory (SRAM). However, the\nadvantage of area scaling through CFETs is hindered by the imbalance in N\/P\ntransistor counts (typically 4N\/2P) within SRAM cells. In this work, we propose\na novel 6T-SRAM design using three sets of CFETs, achieved by vertically\nstacking two n-FET pass-gate (PG) transistors via the CFET architecture.\nThrough TCAD simulations, we optimize channel doping concentration and the\nnumber of top\/bottom nanosheets (NS), demonstrating that junctionless\naccumulation mode (JAM) devices outperform inversion mode (IM) devices for PG\nand pull-down (PD) transistors. The proposed design achieves a 37% area\nreduction in SRAM standard cell layout compared to conventional CFET-based\nSRAM. With optimized parameters (n-type doping of \\(1\\times10^{15}\\)\ncm\\(^{-3}\\) and '1B4T' NS configuration), the 3-CFET SRAM exhibits superior\nwrite margin (349.60 mV) and write delay (54.4 ps). This work advances SRAM\ndesign within the CFET framework, offering a scalable solution for\nnext-generation memory technologies.",
        "We evaluate methods for causal representation learning (CRL) on a simple,\nreal-world system where these methods are expected to work. The system consists\nof a controlled optical experiment specifically built for this purpose, which\nsatisfies the core assumptions of CRL and where the underlying causal factors\n(the inputs to the experiment) are known, providing a ground truth. We select\nmethods representative of different approaches to CRL and find that they all\nfail to recover the underlying causal factors. To understand the failure modes\nof the evaluated algorithms, we perform an ablation on the data by substituting\nthe real data-generating process with a simpler synthetic equivalent. The\nresults reveal a reproducibility problem, as most methods already fail on this\nsynthetic ablation despite its simple data-generating process. Additionally, we\nobserve that common assumptions on the mixing function are crucial for the\nperformance of some of the methods but do not hold in the real data. Our\nefforts highlight the contrast between the theoretical promise of the state of\nthe art and the challenges in its application. We hope the benchmark serves as\na simple, real-world sanity check to further develop and validate methodology,\nbridging the gap towards CRL methods that work in practice. We make all code\nand datasets publicly available at github.com\/simonbing\/CRLSanityCheck",
        "The black hole X-ray binary GRS 1915+105 was bright for 26 years since its\ndiscovery and is well-known for its disk instabilities, quasi-periodic\noscillations, and disk wind signatures. We report a long-term spectral-timing\ntracing of this source from mid-2017 until the onset of the \"obscured state\",\nbased on the complete data from the Neutron Star Interior Composition Explorer\n(NICER) and the Insight--Hard X-ray Modulation Telescope (HXMT), whose hard\ncoverage decisively informs the modeling at lower energies. In the soft state\npredating 2018, we observed highly ionized winds. However, in the hard state\nshortly before transitioning into the \"obscured state\" on May 14, 2019 (MJD\n58617), the winds exhibited a discernible reduction in ionization degree ($\\log\n\\xi$), decreasing from above 4 to approximately 3. Our analysis involves the\nmeasurement of the frequencies of the quasi-periodic oscillations and the\nestimation of the properties of the ionized winds and the intensities of\ndifferent spectral components through spectroscopy during the decay phase. We\ndelve into the origin of these infrequently observed warm outflows in the hard\nstate. It is found that the launching radius of the winds in the hard decay\nphase is similar to that in the soft state, indicating the launching mechanism\nof those winds in both states is likely the same. The presence of the ionized\nwinds is preferentially dependent on the periphery of the accretion disk, but\nnot directly related to the corona activities in the center of the binary\nsystem.",
        "Finite mixtures are a broad class of models useful in scenarios where\nobserved data is generated by multiple distinct processes but without explicit\ninformation about the responsible process for each data point. Estimating\nBayesian mixture models is computationally challenging due to issues such as\nhigh-dimensional posterior inference and label switching. Furthermore,\ntraditional methods such as MCMC are applicable only if the likelihoods for\neach mixture component are analytically tractable.\n  Amortized Bayesian Inference (ABI) is a simulation-based framework for\nestimating Bayesian models using generative neural networks. This allows the\nfitting of models without explicit likelihoods, and provides fast inference.\nABI is therefore an attractive framework for estimating mixture models. This\npaper introduces a novel extension of ABI tailored to mixture models. We\nfactorize the posterior into a distribution of the parameters and a\ndistribution of (categorical) mixture indicators, which allows us to use a\ncombination of generative neural networks for parameter inference, and\nclassification networks for mixture membership identification. The proposed\nframework accommodates both independent and dependent mixture models, enabling\nfiltering and smoothing. We validate and demonstrate our approach through\nsynthetic and real-world datasets.",
        "Snapping shrimp produce bubbles that emit light when they collapse. When a\nbubble collapses so strongly that it emits light, the light emission is usually\ncalled sonoluminescence; in the case of the shrimp, it is called\n\"shrimpoluminescence\". The bubble collapses so fast that no heat can escape and\nthe gas trapped in the bubble becomes hot enough to ionize. Light is emitted\nthrough electron-ion bremsstrahlung, electron-atom bremsstrahlung, and\nelectron-ion recombination. In this paper, we study the dynamics of a\nsonoluminescing bubble and learn how to calculate the spectrum of emitted\nlight, allowing us to explain the physical mechanisms of shrimpoluminescence."
      ]
    }
  },
  {
    "id":2411.09469,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"CBAM: Convolutional block attention module",
    "start_abstract":"We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "The accuracy of colposcopic biopsy: Analyses from the placebo arm of the Gardasil clinical trials"
      ],
      "abstract":[
        "We evaluated the overall agreement between colposcopically directed biopsies and definitive excisional specimens within context of three clinical trials. A total 737 women aged 16-45 who had a cervical biopsy taken 6 months before their therapy were included. Per-protocol, colposcopists to also obtain representative immediately therapy. Using adjudicated histological diagnoses, initial same day correlated with surgically excised specimens. The therapy, diagnoses was 42% (weighted kappa = 0.34) (95% CI: 0.29-0.39). underestimation intraepithelial neoplasia grade 2\/3 or adenocarcinoma in situ (CIN2-3\/AIS) CIN3\/AIS 26 42%, respectively. When allowing for one degree variance correlation, 92% CIN2-3\/AIS. specimen 56% 0.41) 0.36-0.47), CIN2-3\/AIS 57%. There significant associations when patients stratified by age, number biopsies, lesion size, presence human papillomavirus (HPV)16\/18 region. Of 178 diagnostic endocervical curettages performed, 14 (7.9%) found any HPV disease. Colposcopic accuracy improved CIN2 grouped as single predictive measure high-grade Colposcopy functioned well allowed one-degree difference surgical histologic interpretations, done practice. Taking more than colposcopic could improve patient management."
      ],
      "categories":[
        "Clinical Trial"
      ]
    },
    "list":{
      "title":[
        "Procrustes Wasserstein Metric: A Modified Benamou-Brenier Approach with\n  Applications to Latent Gaussian Distributions",
        "Quaternionic Quantum Mechanics: the Particles, their q-Potentials and\n  Mathematical Electron Model",
        "Search for continuous gravitational waves from neutron stars in five\n  globular clusters with a phase-tracking hidden Markov model in the third LIGO\n  observing run",
        "Statistical inference for interacting innovation processes and related\n  general results",
        "Detecting Supernova Axions with IAXO",
        "Diagnostic tools for exploring differences in distributional properties\n  between two samples: nonparametric approach",
        "Quantal analysis of the effects of coordinate noncommutativity on\n  bi-dimensional harmonic motion under parametric variations",
        "Infinite-temperature thermostats by energy localization in a\n  nonequilibrium setup",
        "Sequential Quadratic Optimization for Solving Expectation Equality\n  Constrained Stochastic Optimization Problems",
        "p-Summing weighted holomorphic mappings",
        "Probabilistic pulse-position modulation for classical communication on\n  quantum channels",
        "Optical Properties of Aluminium-Doped Zinc Oxide Thin Films Synthesized\n  via AACVD Using Nitrogen as a Carrier Gas",
        "Fractional Subadditivity of Submodular Functions: Equality Conditions\n  and Their Applications",
        "VIRAC2: NIR Astrometry and Time Series Photometry for 500M+ Stars from\n  the VVV and VVVX Surveys",
        "A New Proof of the QNEC",
        "Probing classical and quantum violations of the equivalence of active\n  and passive gravitational mass",
        "On an analogue of the doubling method in coding theory",
        "Relaxation Critical Dynamics in Measurement-induced Phase Transitions",
        "Bogomolov property for Galois representations with big local image",
        "Every projective Oka manifold is subelliptic",
        "T-cell receptor specificity landscape revealed through de novo peptide\n  design",
        "The Response of Farmer Welfares Amidst Food Prices Shock and Inflation\n  in the Province of East Java",
        "Chirality transfer from chiral perovskite to molecular dopants via\n  charge transfer states",
        "Large-scale stochastic simulation of open quantum systems",
        "Log Prismatic Dieudonn\\'{e} theory and its application to Shimura\n  varieties",
        "Temporally symmetry-broken metasurfaces for ultrafast resonance creation\n  and annihilation",
        "The Formation of Globular Clusters",
        "General Constraints on Isocurvature from the CMB and Ly-$\\alpha$ Forest",
        "MIRI-LRS spectrum of a cold exoplanet around a white dwarf: water,\n  ammonia, and methane measurements"
      ],
      "abstract":[
        "We introduce a modified Benamou-Brenier type approach leading to a\nWasserstein type distance that allows global invariance, specifically,\nisometries, and we show that the problem can be summarized to orthogonal\ntransformations. This distance is defined by penalizing the action with a\ncostless movement of the particle that does not change the direction and speed\nof its trajectory. We show that for Gaussian distribution resume to measuring\nthe Euclidean distance between their ordered vector of eigenvalues and we show\na direct application in recovering Latent Gaussian distributions.",
        "In this work we show the quaternionic quantum descriptions of physical\nprocesses from the Planck to macro scale. The results presented here are based\non the concepts of the Cauchy continuum and the elementary cell at the Planck\nscale. The symmetrization of quaternion relations and the postulate of\nquaternion velocity have been crucial in the present development. The momentum\nof the expansion and compression is the consequence of the scalar term in the\nquaternionic deformation potential.",
        "A search is performed for continuous gravitational waves emitted by unknown\nneutron stars in five nearby globular clusters using data from the third Laser\nInterferometer Gravitational-Wave Observatory (LIGO) observing run, over the\nfrequency range $100$--$800\\,\\mathrm{Hz}$. The search uses a hidden Markov\nmodel to track both the frequency and phase of the continuous wave signal from\none coherent segment to the next. It represents the first time that a\nphase-tracking hidden Markov model has been used in a LIGO search. After\napplying vetoes to reject candidates consistent with non-Gaussian artifacts, no\nsignificant candidates are detected. Estimates of the strain sensitivity at\n95\\% confidence $h_{0,\\mathrm{eff}}^{95\\%}$ and corresponding neutron star\nellipticity $\\epsilon^{95\\%}$ are presented. The best strain sensitivity,\n$h_{0,\\mathrm{eff}}^{95\\%} = 2.7 \\times 10^{-26}$ at $211\\,\\mathrm{Hz}$, is\nachieved for the cluster NGC6544.",
        "Given the importance of understanding how different innovation processes\naffect each other, we have introduced a model for a finite system of\ninteracting innovation processes. The present work focuses on the second-order\nasymptotic properties of the model and illustrates how to leverage the\ntheoretical results in order to make statistical inference on the intensity of\nthe interaction. This methodology is presented within a general framework in\nthe supplementary material to ensure its broad applicability across various\ncontexts. We apply the proposed tools to two real data sets (from Reddit and\nGutenberg).",
        "We investigate the potential of IAXO and its intermediate version, BabyIAXO,\nto detect axions produced in core-collapse supernovae (SNe). Our study\ndemonstrates that these experiments have realistic chances of identifying SN\naxions, offering crucial insights into both axion physics and SN dynamics.\nIAXO's sensitivity to SN axions allows for the exploration of regions of the\naxion parameter space inaccessible through solar observations. In addition, in\nthe event of a nearby SN, $d \\sim O(100)$ pc, and sufficiently large axion\ncouplings, $g_{a \\gamma} \\gtrsim 10^{-11} GeV^{-1}$, IAXO could have a chance\nto significantly advance our understanding of axion production in nuclear\nmatter and provide valuable information about the physics of SNe, such as pion\nabundance, the equation of state, and other nuclear processes occurring in\nextreme environments.",
        "This paper reconsiders the problem of testing the equality of two unspecified\ncontinuous distributions. The framework, which we propose, allows for readable\nand insightful data visualisation and helps to understand and quantify how two\ngroups of data differ. We consider a useful weighted rank empirical process on\n(0,1) and utilise a grid-based approach, based on diadic partitions of (0,1),\nto discretize the continuous process and construct local simultaneous\nacceptance regions. These regions help to identify statistically significant\ndeviations from the null model. In addition, the form of the process and its\ndicretization lead to a highly interpretable visualisation of distributional\ndifferences. We also introduce a new two-sample test, explicitly related to the\nvisualisation. Numerical studies show that the new test procedure performs very\nwell. We illustrate the use and diagnostic capabilities of our approach by an\napplication to a known set of neuroscience data.",
        "Influence of coordinate noncommutativity on 2D quantum oscillatory motion,\nwhich undergoes parameter variations, is investigated. We first derive quantum\nsolutions of the system described with time-independent parameters considering\nthe noncommutativity of coordinates as a preliminary step. And then, we extend\nour study, framed with noncommutative phase-space formalism, to obtain relevant\nsolutions of the system with time-dependent parameters. This system, which we\nfocus on, is nonstationary due to variation of parameters in time. While the\nleft and right circular annihilation and creation operators are utilized in the\nquantal management of the basic stationary system, the Schr\\\"odinger equation\nof the nonstationary system is solved using the Lewis-Riesenfeld invariant\ntheory and the invariant-related unitary transformation procedure. The outcome\nof our analysis is useful in understanding the effects of noncommutativity from\nquantum perspectives, especially in conjunction with the impact of parameter\nvariations.",
        "Some lattice models having two conservation laws may display an equilibrium\nphase transition from a homogeneous (positive temperature - PT) to a condensed\n(negative temperature) phase, where a finite fraction of the energy is\nlocalized in a few sites. We study one such stochastic model in an\nout-of-equilibrium setup, where the ends of the lattice chain are attached to\ntwo PT baths. We show that localized peaks may spontaneously emerge, acting as\ninfinite-temperature heat baths. The number $N_b$ of peaks is expected to grow\nin time $t$ as $N_b \\sim \\sqrt{\\ln t}$, as a consequence of an effective\nfreezing of the dynamics. Asymptotically, the chain spontaneously subdivides\ninto three intervals: the two external ones lying inside the PT region; the\nmiddle one characterized by peaks superposed to a background lying along the\ninfinite-temperature line. In the thermodynamic limit, the Onsager formalism\nallows determining the shape of the whole profile.",
        "A sequential quadratic programming method is designed for solving general\nsmooth nonlinear stochastic optimization problems subject to expectation\nequality constraints. We consider the setting where the objective and\nconstraint function values, as well as their derivatives, are not directly\navailable. The algorithm applies an adaptive step size policy and only relies\non objective gradient estimates, constraint function estimates, and constraint\nderivative estimates to update iterates. Both asymptotic and non-asymptotic\nconvergence properties of the algorithm are analyzed. Under reasonable\nassumptions, the algorithm generates a sequence of iterates whose first-order\nstationary measure diminishes in expectation. In addition, we identify the\niteration and sample complexity for obtaining a first-order\n$\\varepsilon$-stationary iterate in expectation. The results of numerical\nexperiments demonstrate the efficiency and efficacy of our proposed algorithm\ncompared to a penalty method and an augmented Lagrangian method.",
        "Given an open subset $U$ of a complex Banach space $E$, a weight $v$ on $U$,\nand a complex Banach space $F$, let $\\mathcal{H}^\\infty_v(U,F)$ denote the\nBanach space of all weighted holomorphic mappings $f\\colon U\\to F$, under the\nweighted supremum norm\n$\\left\\|f\\right\\|_v:=\\sup\\left\\{v(x)\\left\\|f(x)\\right\\|\\colon x\\in U\\right\\}$.\nIn this paper, we introduce and study the class\n$\\Pi_p^{\\mathcal{H}^\\infty_v}(U,F)$ of $p$-summing weighted holomorphic\nmappings. We prove that it is an injective Banach ideal of weighted holomorphic\nmappings which is not generated by composition. Variants for weighted\nholomorphic mappings of Pietsch Domination Theorem, Pietsch Factorization\nTheorem and Maurey Extrapolation Theorem are presented. We also identify the\nspaces of $p$-summing weighted holomorphic mappings from $U$ into $F^*$ under\nthe norm $\\pi^{\\mathcal{H}^\\infty_v}_p$ with the duals of $F$-valued\n$\\mathcal{H}^\\infty_v$-molecules on $U$ under a suitable version\n$d^{\\mathcal{H}^\\infty_v}_{p^*}$ of the Chevet--Saphar tensor norms.",
        "Classical communication over lossy quantum channels is an essential topic in\nquantum information theory, with practical implications for optical-fiber and\nfree-space communications. Multi-phase Hadamard codes, based on coherent-state\nBinary Phase-Shift Keying (BPSK) modulation and decoded using vacuum-or-pulse\n(VP) detectors, offer a promising approach for achieving high communication\nrates while relying only on linear optics and single-photon detectors (SPDs).\nHowever, their performance does not reach the ultimate Holevo limit. In this\nwork, we propose a generalization of Hadamard codes that distributes the signal\nacross multiple modes with optimized probabilities, rather than concentrating\nit in a single mode, dubbed probabilistic pulse-position modulation (PPPM). We\nderive an achievable communication rate, demonstrating that our PPPM can\noutperform traditional Hadamard codes in certain intermediate energy regimes.",
        "The study uses AACVD technology with nitrogen carrier gas to make AZO thin\nfilms through which it determines structural, optical, and morphological\nchanges from 0 to 20 percent aluminum doping. The depositions took place at 400\ndegrees Celsius on soda-lime glass before the samples received an annealing\nprocess at 450 degrees Celsius inside a nitrogen chamber. The X-ray diffraction\nanalysis identified superior crystalline structure in films processed with\nnitrogen gas through their strong signals at the 220, 311 and 222 peaks. The\nincreasing levels of aluminum doping decreased the crystallite dimensions and\nelevated grain boundary concentrations because of intensified crystal tension\nand defective structural formation. The profilometry assessment determined film\nthickness increased mildly from 102 nanometers in undoped ZnO to 115 nanometers\nin 20 percent aluminum-doped ZnO. The presence of nitrogen annealing in the\nfilms led to increased absorbance while the strongest absorbance peaks occurred\nwhen the material contained 5 percent dopants. The bandgap energy expanded\nthrough the change from undoped ZnO with 3.21 electron volts to 3.33 electron\nvolts at 20 percent aluminum doping which matched Burstein-Moss effect results.\nThe optoelectronic devices gain enhanced optical characteristics from the\ndoping levels exceeding 15 percent.",
        "Submodular functions are known to satisfy various forms of fractional\nsubadditivity. This work investigates the conditions for equality to hold\nexactly or approximately in the fractional subadditivity of submodular\nfunctions. We establish that a small gap in the inequality implies that the\nfunction is close to being modular, and that the gap is zero if and only if the\nfunction is modular. We then present natural implications of these results for\nspecial cases of submodular functions, such as entropy, relative entropy, and\nmatroid rank. As a consequence, we characterize the necessary and sufficient\nconditions for equality to hold in Shearer's lemma, recovering a result of\nEllis \\emph{et al.} (2016) as a special case. We leverage our results to\npropose a new multivariate mutual information, which generalizes Watanabe's\ntotal correlation (1960), Han's dual total correlation (1978), and Csisz\\'ar\nand Narayan's shared information (2004), and analyze its properties. Among\nthese properties, we extend Watanabe's characterization of total correlation as\nthe maximum correlation over partitions to fractional partitions. When applied\nto matrix determinantal inequalities for positive definite matrices, our\nresults recover the equality conditions of the classical determinantal\ninequalities of Hadamard, Sz\\'asz, and Fischer as special cases.",
        "We present VIRAC2, a catalogue of positions, proper motions, parallaxes and\n$Z$, $Y$, $J$, $H$, and $K_s$ near-infrared photometric time series of 545 346\n537 unique stars. The catalogue is based on a point spread function fitting\nreduction of nearly a decade of VISTA VVV and VVVX images, which cover\n$560~{\\rm deg}^2$ of the Southern Galactic plane and bulge. The catalogue is\ncomplete at the $>90$ per cent level for $11<K_s~{\\rm mag}<16$ sources, but\nextends to $K_s\\approx{}17.5$ mag in most fields. Astrometric performance for\n$11<K_s~{\\rm mag}<14$ sources is typically $\\approx{}0.37~{\\rm mas~yr}^{-1}$\nper dimension for proper motion, and $1~{\\rm mas}$ for parallax. At $K_s=16$\nthe equivalent values are around $1.5~{\\rm mas~yr}^{-1}$ and $5~{\\rm mas}$.\nThese uncertainties are validated against Gaia DR3 and Hubble Space Telescope\nastrometry. The complete catalogues are available via the ESO archive. We\nperform an initial search of the catalogue for nearby ultracool dwarf\ncandidates. In total we find 26 new sources whose parallaxes place them within\n50 parsecs of the Sun. Among them we find two high-confidence T dwarfs and a\nnumber of other sources that appear to lie close to the L\/T transition.",
        "We give a simplified proof of the quantum null energy condition (QNEC). Our\nproof is based on an explicit formula for the shape derivative of the relative\nentropy, with respect to an entangling cut. It allows bypassing the analytic\ncontinuation arguments of a previous proof by Ceyhan and Faulkner and can be\nused e.g., for defining entropy current fluctuations.",
        "The equivalence of active and passive (EAP) gravitational mass is one of the\nmost fundamental principles of gravity. But in contrast to the usual\nequivalence of inertial and (passive) gravitational mass, the EAP has not\nreceived much attention. Here we revisit this principle and show how it can be\nused to probe quantum gravity in laboratory-based experiments. We first examine\nhow the dynamics under EAP violations affects classical systems and show that\nnew laboratory tests can be performed, to improve over the current experimental\nbounds and to test new manifestations of EAP violations. We then extend the\nanalysis to the quantum domain, where quantized energy contributes to mass and\nthe EAP principle can thus shed light on how quantum source masses would\ngravitate. We show that experiments with cold polar molecules, and future\nexperiments with nuclear atomic clocks, can test the quantum EAP in a regime\nwhere quantum gravity phenomenology could become relevant. Our results open new\nopportunities for fundamental tests of gravity in high-precision laboratory\nexperiments that can shed light on foundational principles of gravity and its\ninterface with quantum theory.",
        "The theories of automorphic forms and self-dual linear codes share many\nremarkable analogies. In both worlds there are functions invariant under an\naction of a group, notions of cusp forms and Hecke operators, also projections\nand lifts between different geni. It is then natural to ask if other important\nautomorphic objects or techniques could be introduced into coding theory. In\nthis article we propose a way to introduce the doubling method, an efficient\ntechnique used to construct and study $L$-functions. As a result, we prove the\nso-called doubling identity, which usually forms a base of many applications.\nHere we use it to solve an analogue of the \"basis problem\". Namely, we express\na cusp form as an explicit linear combination of complete weight enumerators of\nthe same type.",
        "Measurement-induced phase transition (MIPT) describes the nonanalytical\nchange of the entanglement entropy resulting from the interplay between\nmeasurement and unitary evolution. In this paper, we investigate the relaxation\ncritical dynamics near the MIPT for different initial states in a\none-dimensional quantum circuit. Specifically, when the initial state is in the\nvolume-law phase with vanishing measurement probability, we find that the\nhalf-chain entanglement entropy $S$ decays as $S\\propto t^{-1}$ with the\ncoefficients proportional to the size of the system in the short-time stage; In\ncontrast, when the initial state is the product state, $S$ increases with time\nas $S\\propto \\ln{t}$, consistent with previous studies. Despite these\ncontrasting behaviors, we develop a unified scaling form to describe these\nscaling behaviors for different initial states where the off-critical-point\neffects can also be incorporated. This framework offers significant advantages\nfor experimental MIPT detection. Our novel scheme, leveraging relaxation\ndynamical scaling, drastically reduces post-selection overhead, and can\neliminate it completely with trackable classical simulation.",
        "An algebraic extension of the rational numbers is said to have the\n$\\textit{Bogomolov property}$ (B) if the absolute logarithmic Weil height of\nits non-torsion elements is uniformly bounded from below. Given a continuous\nrepresentation $\\rho$ of the absolute Galois group $G_{\\mathbb{Q}}$ of\n${\\mathbb{Q}}$, one says that $\\rho$ has (B) if the field fixed by\n$\\mathrm{ker}\\,\\rho$ has (B). We prove that, if $\\rho:G_{\\mathbb{Q}} \\to\n\\mathrm{GL}_d({\\mathbb{Z}}_p)$ maps an inertia subgroup at $p$ surjectively\nonto an open subgroup of $\\mathrm{GL}_d({\\mathbb{Z}}_p)$, then $\\rho$ has (B).\nMore generally, we show that if the image of a decomposition group at $p$ is\nopen in $\\rho(G_{\\mathbb{Q}})$, and a certain condition on the center of\n$\\rho(G_{\\mathbb{Q}})$ satisfied, then $\\rho$ has (B). In particular, no\nassumption on the modularity of $\\rho$ is needed, contrary to previous work of\nHabegger and Amoroso--Terracini. We provide several examples both in modular\nand non-modular cases. Our methods rely on a result of Sen comparing the\nramification and Lie filtrations on the $p$-adic Lie group\n$\\rho(G_{\\mathbb{Q}})$.",
        "We show that every projective Oka manifold is subelliptic. This solves a\nlong-standing open problem. We present further results concerning the\nrelationship between the Oka property, ellipticity, subellipticity, and a new\nproperty that we call weak ellipticity.",
        "T-cells play a key role in adaptive immunity by mounting specific responses\nagainst diverse pathogens. An effective binding between T-cell receptors (TCRs)\nand pathogen-derived peptides presented on Major Histocompatibility Complexes\n(MHCs) mediate an immune response. However, predicting these interactions\nremains challenging due to limited functional data on T-cell reactivities.\nHere, we introduce a computational approach to predict TCR interactions with\npeptides presented on MHC class I alleles, and to design novel immunogenic\npeptides for specified TCR-MHC complexes. Our method leverages HERMES, a\nstructure-based, physics-guided machine learning model trained on the protein\nuniverse to predict amino acid preferences based on local structural\nenvironments. Despite no direct training on TCR-pMHC data, the implicit\nphysical reasoning in HERMES enables us to make accurate predictions of both\nTCR-pMHC binding affinities and T-cell activities across diverse viral epitopes\nand cancer neoantigens, achieving up to 72% correlation with experimental data.\nLeveraging our TCR recognition model, we develop a computational protocol for\nde novo design of immunogenic peptides. Through experimental validation in\nthree TCR-MHC systems targeting viral and cancer peptides, we demonstrate that\nour designs--with up to five substitutions from the native sequence--activate\nT-cells at success rates of up to 50%. Lastly, we use our generative framework\nto quantify the diversity of the peptide recognition landscape for various\nTCR-MHC complexes, offering key insights into T-cell specificity in both humans\nand mice. Our approach provides a platform for immunogenic peptide and\nneoantigen design, opening new computational paths for T-cell vaccine\ndevelopment against viruses and cancer.",
        "Price uncertainty in food commodities can create uncertainty for farmers and\npotentially negatively impact the level of farmer household well-being. On the\nother hand, the agriculture sector in the province of East Java has greatly\ncontributed to East Java's economy. This paper analyses the response of farmer\nwelfare through farmer exchange values amidst fluctuation shock of food needed\nprices and inflation level in the east java province. The research method of\nthis paper employs the impulse response function of the Bayesian Vector\nAutoregressive (BVAR) model by using time series secondary data from May 2017\nuntil December 2023. This paper finds that the shock that happens to aggregate\nfood prices can increase farmer exchange values even though the shock to the\ninflation level has reduced farmer exchange values and increased aggregate food\nprices.",
        "Chiral perovskites are emerging semiconducting materials with broken symmetry\nthat can selectively absorb and emit circularly polarized light. However, most\nof the chiral perovskites are typically low-dimensional structures with limited\nelectrical conductivity and their light absorption occurs in the UV region. In\nthis work, we find doping 2,3,5,6-Tetrafluoro-7,7,8,8-tetracyanoquinodimethane\n(F4TCNQ) in the chiral perovskite matrix can improve the electrical\nconductivity with an addition of visible light absorption through the emerging\ncharge-transfer electronic states. The new absorption feature exhibits strong\ncircular dichroism adapted from the chiral matrix, which is indicative of a\nchirality transfer from the host to the guest via an electronic coupling. The\ncharge transfer state is validated by transient absorption spectroscopy and\ntheory modeling. Quantum-chemical modeling identifies a strong wave function\noverlap between an electron and a hole of the guest-host in a closely packed\ncrystal configuration forming the charge-transfer absorption state. We then\nintegrate the doped chiral perovskite film in photodetectors and demonstrate a\nselective detection of circularly polarized light both in the UV and visible\nrange. Our results suggest a universal approach of introducing visible photo\nabsorption states to the chiral matrix to broaden the optical active range and\nenhance the conductivity.",
        "Understanding the precise interaction mechanisms between quantum systems and\ntheir environment is crucial for advancing stable quantum technologies,\ndesigning reliable experimental frameworks, and building accurate models of\nreal-world phenomena. However, simulating open quantum systems, which feature\ncomplex non-unitary dynamics, poses significant computational challenges that\nrequire innovative methods to overcome. In this work, we introduce the tensor\njump method (TJM), a scalable, embarrassingly parallel algorithm for\nstochastically simulating large-scale open quantum systems, specifically\nMarkovian dynamics captured by Lindbladians. This method is built on three core\nprinciples where, in particular, we extend the Monte Carlo wave function (MCWF)\nmethod to matrix product states, use a dynamic time-dependent variational\nprinciple (TDVP) to significantly reduce errors during time evolution, and\nintroduce what we call a sampling MPS to drastically reduce the dependence on\nthe simulation's time step size. We demonstrate that this method scales more\neffectively than previous methods and ensures convergence to the Lindbladian\nsolution independent of system size, which we show both rigorously and\nnumerically. Finally, we provide evidence of its utility by simulating\nLindbladian dynamics of XXX Heisenberg models up to a thousand spins using a\nconsumer-grade CPU. This work represents a significant step forward in the\nsimulation of large-scale open quantum systems, with the potential to enable\ndiscoveries across various domains of quantum physics, particularly those where\nthe environment plays a fundamental role, and to both dequantize and facilitate\nthe development of more stable quantum hardware.",
        "We study the log version of the prismatic Dieudonn\\'{e} theory established by\nAnsch\\\"{u}tz-Le Bras. By applying this result to the integral toroidal\ncompactification of a Shimura variety of Hodge type, we extend the prismatic\nrealization, originally constructed by Imai-Kato-Youcis, to the\ncompactification. This extension enables us to prove Lovering's conjecture on\n$p$-adic comparison isomorphisms for Shimura varieties.",
        "Active metasurfaces, compact platforms for nanoscale light manipulation, are\ntransforming technologies like holography, quantum cryptography, and optical\ncomputing. Despite their versatility, tunability in metasurfaces has mainly\nrelied on shifting the resonance wavelength or increasing material losses to\nspectrally detune or quench resonant modes, respectively. However, both methods\nface fundamental limitations, such as limited Q-factor and near-field\nenhancement control and the inability to achieve resonance on\/off switching by\ncompletely coupling and decoupling the mode from the far-field. Here, we\ndemonstrate temporal symmetry-breaking in metasurfaces via ultrafast optical\npumping, marking the first experimental realization of radiative loss-driven\nresonance creation, annihilation, broadening, and sharpening. We introduce\nrestored symmetry-protected bound states in the continuum as a new concept\nwhich are central to the realization of temporal symmetry-breaking. These\nstates arise in metasurfaces with geometrically asymmetric unit cells, where\nthe total dipole moment, composed of two antisymmetric dipoles, cancels out.\nMie-resonant optical absorption within specific regions of the unit cell\nlocally modifies the refractive index, disrupting the balance between the two\ndipole moments. A total dipole moment is thereby created or annihilated, and\nconsequently, the radiative loss is tuned. This enables full control over\ncoupling to incoming light, allowing precise adjustment of the resonance\nlinewidth, near-field enhancement, and resonance amplitude. Our work\nestablishes radiative loss-based active metasurfaces with potential\napplications ranging from high-speed optical and quantum communications to\ntime-crystals and photonic circuits.",
        "Globular clusters (GCs) are among the oldest and most luminous stellar\nsystems in the Universe, offering unique insights into galaxy formation and\nevolution. While the physical processes behind their origin have long remained\nelusive, major theoretical and observational developments in the past decade\nhave led to a new understanding of GCs as the natural outcome of high-pressure\nstar formation in high-redshift galaxies. This review synthesizes recent\nadvancements in our understanding of GC formation and aims to provide a\ncomprehensive point of reference for leveraging the revolutionary capabilities\nof the current and upcoming generation of telescopes. The latest generation of\nGC models combine our understanding of their formation and destruction with\nadvanced galaxy formation simulations. The next decade will provide the\nfirst-ever opportunity to test such models across their full evolutionary\nhistory, from GC formation at high redshift as seen with the James Webb\nTelescope, to snapshots of GC demographics at intermediate redshifts obtained\nwith 30m-class telescopes, and eventually to the well-characterized GC\npopulations observed at the present day. We identify the major questions that\nwe should expect to address this way.",
        "Current cosmological data are well-described by the Lambda-Cold Dark Matter\n($\\Lambda$CDM) model, which assumes adiabatic initial conditions for the\nprimordial density perturbations. This agreement between data and theory\nenables strong constraints on new physics that generates isocurvature\nperturbations. Existing constraints typically assume a simple power law form\nfor the isocurvature power spectrum. However, many new physics scenarios --\nsuch as cosmological phase transitions and gravitational particle production --\ncan deviate from this assumption. To derive general constraints which apply to\na wide variety of new physics scenarios, we consider four types of isocurvature\nmodes (dark matter, baryon, dark radiation and neutrino density isocurvature)\nand parametrize the isocurvature power spectrum using two general forms: a\ndelta function and a broken power law. Using data from the cosmic microwave\nbackground (CMB), baryon acoustic oscillations, the Lyman-$\\alpha$ forest, and\nCMB spectral distortions, we place constraints on the isocurvature power\nspectrum across a wide range of scales, from $10^{-4}\\,\\textrm{Mpc}^{-1}$ to\n$10^{4}\\,\\textrm{Mpc}^{-1}$.",
        "The study of the atmosphere of exoplanets orbiting white dwarfs is a largely\nunexplored field. With WD\\,0806-661\\,b, we present the first deep dive into the\natmospheric physics and chemistry of a cold exoplanet around a white dwarf. We\nobserved WD 0806-661 b using JWST's Mid-InfraRed Instrument Low-Resolution\nSpectrometer (MIRI-LRS), covering the wavelength range from 5 -- 12~$\\mu\n\\rm{m}$, and the Imager, providing us with 12.8, 15, 18 and 21\\,$\\mu$m\nphotometric measurements. We carried the data reduction of those datasets,\ntackling second-order effects to ensure a reliable retrieval analysis. Using\nthe \\textsc{TauREx} retrieval code, we inferred the pressure-temperature\nstructure, atmospheric chemistry, mass, and radius of the planet. The spectrum\nof WD 0806-661 b is shaped by molecular absorption of water, ammonia, and\nmethane, consistent with a cold Jupiter atmosphere, allowing us to retrieve\ntheir abundances. From the mixing ratio of water, ammonia and methane we derive\n$\\rm{C\/O} = 0.34 \\pm 0.06$, $\\rm{C\/N} = 14.4 ^{+2.5}_{-1.8}$ and $\\rm{N\/O} =\n0.023 \\pm 0.004$ and the ratio of detected metals as proxy for metallicity. We\nalso derive upper limits for the abundance of CO and $\\rm{CO_2}$\n($1.2\\cdot10^{-6} \\rm{\\,and\\,} 1.6\\cdot10^{-7}$ respectively), which were not\ndetected by our retrieval models. While our interpretation of WD\\,0806-661\\,b's\natmosphere is mostly consistent with our theoretical understanding, some\nresults -- such as the lack of evidence for water clouds, an apparent increase\nin the mixing ratio of ammonia at low pressure, or the retrieved mass at odds\nwith the supposed age -- remain surprising and require follow-up observational\nand theoretical studies to be confirmed."
      ]
    }
  },
  {
    "id":2411.09469,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"The accuracy of colposcopic biopsy: Analyses from the placebo arm of the Gardasil clinical trials",
    "start_abstract":"We evaluated the overall agreement between colposcopically directed biopsies and definitive excisional specimens within context of three clinical trials. A total 737 women aged 16-45 who had a cervical biopsy taken 6 months before their therapy were included. Per-protocol, colposcopists to also obtain representative immediately therapy. Using adjudicated histological diagnoses, initial same day correlated with surgically excised specimens. The therapy, diagnoses was 42% (weighted kappa = 0.34) (95% CI: 0.29-0.39). underestimation intraepithelial neoplasia grade 2\/3 or adenocarcinoma in situ (CIN2-3\/AIS) CIN3\/AIS 26 42%, respectively. When allowing for one degree variance correlation, 92% CIN2-3\/AIS. specimen 56% 0.41) 0.36-0.47), CIN2-3\/AIS 57%. There significant associations when patients stratified by age, number biopsies, lesion size, presence human papillomavirus (HPV)16\/18 region. Of 178 diagnostic endocervical curettages performed, 14 (7.9%) found any HPV disease. Colposcopic accuracy improved CIN2 grouped as single predictive measure high-grade Colposcopy functioned well allowed one-degree difference surgical histologic interpretations, done practice. Taking more than colposcopic could improve patient management.",
    "start_categories":[
      "Clinical Trial"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b36"
      ],
      "title":[
        "CBAM: Convolutional block attention module"
      ],
      "abstract":[
        "We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Rethinking Benign Overfitting in Two-Layer Neural Networks",
        "Darboux theorem for generalized complex structures on transitive Courant\n  algebroids",
        "Lambda-Fleming-Viot processes arising in logistic\n  Bienaym\\'e-Galton-Watson processes with a large carrying capacity",
        "Effective Field Neural Network",
        "Noncommutative $p$-wave holographic superconductors",
        "Improved Robust Estimation for Erd\\H{o}s-R\\'enyi Graphs: The Sparse\n  Regime and Optimal Breakdown Point",
        "Inference-Time Alignment in Diffusion Models with Reward-Guided\n  Generation: Tutorial and Review",
        "Neural Interpretable Reasoning",
        "DiffusionRenderer: Neural Inverse and Forward Rendering with Video\n  Diffusion Models",
        "Staying Alive: Online Neural Network Maintenance and Systemic Drift",
        "Halilsoy and Chandrasekhar standing gravitational waves in the linear\n  approximation",
        "Local Differences, Global Lessons: Insights from Organisation Policies\n  for International Legislation",
        "Low Complexity Artificial Noise Aided Beam Focusing Design in Near-Field\n  Terahertz Communications",
        "A free parameter depending family of polynomial wavelets on a compact\n  interval",
        "Volume growths versus Sobolev inequalities",
        "TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated\n  Learning",
        "Multi-Hypothesis Prediction for Portfolio Optimization: A Structured\n  Ensemble Learning Approach to Risk Diversification",
        "Constructing families of 3-Selmer companions",
        "Cross Section Measurements of Large Angle Fragments Production in the\n  Interaction of Carbon Ion Beams with Thin Targets",
        "ShapeLib: designing a library of procedural 3D shape abstractions with\n  Large Language Models",
        "Testing the hypothesis of vector X17 boson by D meson, Charmonium, and\n  $\\phi$ meson decays",
        "A differentiable binary microlensing model using adaptive contour\n  integration method",
        "A Landmark-Aided Navigation Approach Using Side-Scan Sonar",
        "Observation of topological prethermal strong zero modes",
        "Topological frustration in twisted, elastic ribbons",
        "V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes",
        "Bridge Frame and Event: Common Spatiotemporal Fusion for High-Dynamic\n  Scene Optical Flow",
        "BAMBI: Developing Baby Language Models for Italian",
        "Hyperbolic free-surface jets"
      ],
      "abstract":[
        "Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) have revealed\na sharp phase transition from benign to harmful overfitting when the\nnoise-to-feature ratio exceeds a threshold-a situation common in long-tailed\ndata distributions where atypical data is prevalent. However, harmful\noverfitting rarely happens in overparameterized neural networks. Further\nexperimental results suggested that memorization is necessary for achieving\nnear-optimal generalization error in long-tailed data distributions (Feldman &\nZhang, 2020). We argue that this discrepancy between theoretical predictions\nand empirical observations arises because previous feature-noise data models\noverlook the heterogeneous nature of noise across different data classes. In\nthis paper, we refine the feature-noise data model by incorporating\nclass-dependent heterogeneous noise and re-examine the overfitting phenomenon\nin neural networks. Through a comprehensive analysis of the training dynamics,\nwe establish test loss bounds for the refined model. Our findings reveal that\nneural networks can leverage \"data noise\", previously deemed harmful, to learn\nimplicit features that improve the classification accuracy for long-tailed\ndata. Experimental validation on both synthetic and real-world datasets\nsupports our theoretical results.",
        "Let E be a transitive Courant algebroid with scalar product of neutral\nsignature. A generalized almost complex structure \\mathcal J on E is a\nskew-symmetric smooth field of endomorphisms of E which squares to minus the\nidentity. We say that \\mathcal J is integrable (or is a generalized complex\nstructure) if the space of sections of its (1,0) bundle is closed under the\nDorfman bracket of E. In this paper we determine, under certain natural\nconditions, the local form of \\mathcal J around regular points. This result is\nanalogous to Gualtieri's Darboux theorem for generalized complex structures on\nmanifolds and extends Wang's description of skew-symmetric left-invariant\ncomplex structures on compact semisimple Lie groups.",
        "We consider a continuous-time Bienaym\\'e-Galton-Watson process with logistic\ncompetition in a regime of weak competition, or equivalently of a large\ncarrying capacity. Individuals reproduce at random times independently of each\nother but die at a rate which increases with the population size. When\nindividuals reproduce, they produce a random number of offspring, drawn\naccording to some probability distribution on the natural integers. We keep\ntrack of the number of descendants of the initial individuals by adding neutral\nmarkers to the individuals, which are inherited by one's offspring. We then\nconsider several scaling limits of the measure-valued process describing the\ndistribution of neutral markers in the population, as well as the population\nsize, when the competition parameter tends to zero. Three regimes emerge,\ndepending on the tail of the offspring distribution. When the offspring\ndistribution admits a second moment (actually a $ 2+\\delta $ moment for some\npositive $ \\delta $), the fluctuations of the population size around its\ncarrying capacity are small and the neutral types asymptotically follow a\nFleming-Viot process. When the offspring distribution has a power-law decay\nwith exponent $ \\alpha \\in (1,2) $, the population size remains most of the\ntime close to its carrying capacity with some (short-lived) fluctuations, and\nthe neutral types evolve in the limit according to a generalised $ \\Lambda\n$-Fleming-Viot process. When the exponent $ \\alpha $ is equal to 1, the time\nscale of the fluctuations changes drastically, as well as the order of\nmagnitude of the population size. In that case the limiting dynamics of the\nneutral markers is given by the dual of the Bolthausen-Sznitman coalescent.",
        "In recent years, with the rapid development of machine learning, physicists\nhave been exploring its new applications in solving or alleviating the curse of\ndimensionality in many-body problems. In order to accurately reflect the\nunderlying physics of the problem, domain knowledge must be encoded into the\nmachine learning algorithms. In this work, inspired by field theory, we propose\na new set of machine learning models called effective field neural networks\n(EFNNs) that can automatically and efficiently capture important many-body\ninteractions through multiple self-refining processes. Taking the classical\n$3$-spin infinite-range model and the quantum double exchange model as case\nstudies, we explicitly demonstrate that EFNNs significantly outperform\nfully-connected deep neural networks (DNNs) and the effective model.\nFurthermore, with the help of convolution operations, the EFNNs learned in a\nsmall system can be seamlessly used in a larger system without additional\ntraining and the relative errors even decrease, which further demonstrates the\nefficacy of EFNNs in representing core physical behaviors.",
        "In this work, we have studied the effects of noncommutative geometry on the\nproperties of p-wave holographic superconductors with massive vector\ncondensates in the probe limit. We have applied the St\\\"{u}rm-Liouville\neigenvalue approach to analyze the model. In this model, we have calculated the\ncritical temperature and the value of the condensation operator for two\ndifferent values of $m^2$. We have also shown how the influence of\nnoncommutative geometry modifies these quantities. Finally, by applying a\nlinearized gauge field perturbation along the boundary direction, we calculated\nthe holographic superconductor's DC conductivity using a self-consistent\napproach and then carrying out a more rigorous analysis. The noncommutative\neffects are also found to be present in the result of DC conductivity. We have\nalso found that just like the commutative case, here the DC conductivity\ndiverges due to the presence of a first order pole in the frequency regime.",
        "We study the problem of robustly estimating the edge density of\nErd\\H{o}s-R\\'enyi random graphs $G(n, d^\\circ\/n)$ when an adversary can\narbitrarily add or remove edges incident to an $\\eta$-fraction of the nodes. We\ndevelop the first polynomial-time algorithm for this problem that estimates\n$d^\\circ$ up to an additive error $O([\\sqrt{\\log(n) \/ n} +\n\\eta\\sqrt{\\log(1\/\\eta)} ] \\cdot \\sqrt{d^\\circ} + \\eta \\log(1\/\\eta))$. Our error\nguarantee matches information-theoretic lower bounds up to factors of\n$\\log(1\/\\eta)$. Moreover, our estimator works for all $d^\\circ \\geq \\Omega(1)$\nand achieves optimal breakdown point $\\eta = 1\/2$.\n  Previous algorithms [AJK+22, CDHS24], including inefficient ones, incur\nsignificantly suboptimal errors. Furthermore, even admitting suboptimal error\nguarantees, only inefficient algorithms achieve optimal breakdown point. Our\nalgorithm is based on the sum-of-squares (SoS) hierarchy. A key ingredient is\nto construct constant-degree SoS certificates for concentration of the number\nof edges incident to small sets in $G(n, d^\\circ\/n)$. Crucially, we show that\nthese certificates also exist in the sparse regime, when $d^\\circ = o(\\log n)$,\na regime in which the performance of previous algorithms was significantly\nsuboptimal.",
        "This tutorial provides an in-depth guide on inference-time guidance and\nalignment methods for optimizing downstream reward functions in diffusion\nmodels. While diffusion models are renowned for their generative modeling\ncapabilities, practical applications in fields such as biology often require\nsample generation that maximizes specific metrics (e.g., stability, affinity in\nproteins, closeness to target structures). In these scenarios, diffusion models\ncan be adapted not only to generate realistic samples but also to explicitly\nmaximize desired measures at inference time without fine-tuning. This tutorial\nexplores the foundational aspects of such inference-time algorithms. We review\nthese methods from a unified perspective, demonstrating that current techniques\n-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,\nand classifier guidance -- aim to approximate soft optimal denoising processes\n(a.k.a. policies in RL) that combine pre-trained denoising processes with value\nfunctions serving as look-ahead functions that predict from intermediate states\nto terminal rewards. Within this framework, we present several novel algorithms\nnot yet covered in the literature. Furthermore, we discuss (1) fine-tuning\nmethods combined with inference-time techniques, (2) inference-time algorithms\nbased on search algorithms such as Monte Carlo tree search, which have received\nlimited attention in current research, and (3) connections between\ninference-time algorithms in language models and diffusion models. The code of\nthis tutorial on protein design is available at\nhttps:\/\/github.com\/masa-ue\/AlignInversePro",
        "We formalize a novel modeling framework for achieving interpretability in\ndeep learning, anchored in the principle of inference equivariance. While the\ndirect verification of interpretability scales exponentially with the number of\nvariables of the system, we show that this complexity can be mitigated by\ntreating interpretability as a Markovian property and employing neural\nre-parametrization techniques. Building on these insights, we propose a new\nmodeling paradigm -- neural generation and interpretable execution -- that\nenables scalable verification of equivariance. This paradigm provides a general\napproach for designing Neural Interpretable Reasoners that are not only\nexpressive but also transparent.",
        "Understanding and modeling lighting effects are fundamental tasks in computer\nvision and graphics. Classic physically-based rendering (PBR) accurately\nsimulates the light transport, but relies on precise scene\nrepresentations--explicit 3D geometry, high-quality material properties, and\nlighting conditions--that are often impractical to obtain in real-world\nscenarios. Therefore, we introduce DiffusionRenderer, a neural approach that\naddresses the dual problem of inverse and forward rendering within a holistic\nframework. Leveraging powerful video diffusion model priors, the inverse\nrendering model accurately estimates G-buffers from real-world videos,\nproviding an interface for image editing tasks, and training data for the\nrendering model. Conversely, our rendering model generates photorealistic\nimages from G-buffers without explicit light transport simulation. Experiments\ndemonstrate that DiffusionRenderer effectively approximates inverse and\nforwards rendering, consistently outperforming the state-of-the-art. Our model\nenables practical applications from a single video input--including relighting,\nmaterial editing, and realistic object insertion.",
        "We present the Subset Extended Kalman Filter (SEKF) as a method to update\npreviously trained model weights online rather than retraining or finetuning\nthem when the system a model represents drifts away from the conditions under\nwhich it was trained. We identify the parameters to be updated using the\ngradient of the loss function and use the SEKF to update only these parameters.\nWe compare finetuning and SEKF for online model maintenance in the presence of\nsystemic drift through four dynamic regression case studies and find that the\nSEKF is able to maintain model accuracy as-well if not better than finetuning\nwhile requiring significantly less time per iteration, and less hyperparameter\ntuning.",
        "Halilsoy and Chandrasekhar cylindrical standing gravitational waves\ncorrespond to two different classes of solutions to the vacuum Einstein\nequations. Both families satisfy the definition of standing gravitational waves\nproposed by Stephani, but only the latter class fulfills the stricter\ndefinition introduced by Chandrasekhar. The aim of this research is to compare\nboth classes of solutions within the linear regime. We discover that the\nlinearized Halilsoy and Chandrasekhar standing waves are gravitational\nanalogues of two different types of electromagnetic polarization standing waves\n- a phenomenon not previously discussed in the literature for exact solutions\nto the Einstein equations",
        "The rapid adoption of AI across diverse domains has led to the development of\norganisational guidelines that vary significantly, even within the same sector.\nThis paper examines AI policies in two domains, news organisations and\nuniversities, to understand how bottom-up governance approaches shape AI usage\nand oversight. By analysing these policies, we identify key areas of\nconvergence and divergence in how organisations address risks such as bias,\nprivacy, misinformation, and accountability. We then explore the implications\nof these findings for international AI legislation, particularly the EU AI Act,\nhighlighting gaps where practical policy insights could inform regulatory\nrefinements. Our analysis reveals that organisational policies often address\nissues such as AI literacy, disclosure practices, and environmental impact,\nareas that are underdeveloped in existing international frameworks. We argue\nthat lessons from domain-specific AI policies can contribute to more adaptive\nand effective AI governance at the global level. This study provides actionable\nrecommendations for policymakers seeking to bridge the gap between local AI\npractices and international regulations.",
        "In this paper, we develop a novel low-complexity artificial noise (AN) aided\nbeam focusing scheme in a near-field terahertz wiretap communication system. In\nthis system, the base station (BS) equipped with a large-scale array transmits\nsignals to a legitimate user, while mitigating information leakage to an\neavesdropper. We formulate an optimization problem to maximize the secrecy rate\nachieved at the legitimate user and solve it by designing the optimal beam\nfocusing and power allocation. Numerical results demonstrate the significant\nperformance improvement achieved by the proposed AN aided beam focusing scheme,\nespecially when the eavesdropper is located closer to the BS than the\nlegitimate user.",
        "On a compact interval, we introduce and study a whole family of wavelets\ndepending on a free parameter that can be suitably modulated to improve\nperformance. Such wavelets arise from de la Vall\\'ee Poussin (VP) interpolation\nat Chebyshev nodes, generalizing previous work by Capobianco and Themistoclakis\nwho considered a special parameter setting. In our construction, both scaling\nand wavelet functions are interpolating polynomials at some Chebyshev zeros of\n1st kind. Contrarily to the classical approach, they are not generated by\ndilations and translations of a single mother function and are naturally\ndefined on the interval $[-1,1]$ to which any other compact interval can be\nreduced. In the paper, we provide a non-standard multiresolution analysis with\nfast (DCT-based) decomposition and reconstruction algorithms. Moreover, we\nstate several theoretical results, particularly on convergence, laying the\nfoundation for future applications.",
        "Our study deals with fine volume growth estimates on metric measures spaces\nsupporting various Sobolev-type inequalities. We first establish the sharp Nash\ninequality on Riemannian manifolds with non-negative Ricci curvature and\npositive asymptotic volume ratio. The proof of the sharpness of this result\nprovides a generic approach to establish a unified volume growth estimate\nwhenever a wide range of Gagliardo-Nirenberg-Sobolev interpolation inequality\nholds on general, not necessarily smooth metric measure spaces without any\ncurvature restriction. This result provides a partial answer to the question of\nLedoux [Ann. Fac. Sci. Toulouse Math., 2000] in a broader setting, as it\nincludes the usual Gagliardo-Nirenberg, Sobolev and Nash inequalities, as well\nas their borderlines, i.e., the logarithmic-Sobolev, Faber-Krahn, Morrey and\nMoser-Trudinger inequalities, respectively. As a first application, we prove\nsharp Gagliardo-Nirenberg-Sobolev interpolation inequalities -- with their\nborderlines -- in the setting of metric measure spaces verifying the\ncurvature-dimension condition ${\\sf CD}(0,N)$ in the sense of\nLott-Sturm-Villani. In addition, the equality cases are also characterized in\nterms of the $N$-volume cone structure of the ${\\sf CD}(0,N)$ space together\nwith the precise profile of extremizers. In addition, we also provide the heat\nkernel estimate with sharp constant on Riemannian manifolds with non-negative\nRicci curvature, answering another question of Bakry, Concordet and Ledoux\n[ESAIM Probab. Statist., 1997].",
        "Federated learning is a computing paradigm that enhances privacy by enabling\nmultiple parties to collaboratively train a machine learning model without\nrevealing personal data. However, current research indicates that traditional\nfederated learning platforms are unable to ensure privacy due to privacy leaks\ncaused by the interchange of gradients. To achieve privacy-preserving federated\nlearning, integrating secure aggregation mechanisms is essential.\nUnfortunately, existing solutions are vulnerable to recently demonstrated\ninference attacks such as the disaggregation attack. This paper proposes\nTAPFed, an approach for achieving privacy-preserving federated learning in the\ncontext of multiple decentralized aggregators with malicious actors. TAPFed\nuses a proposed threshold functional encryption scheme and allows for a certain\nnumber of malicious aggregators while maintaining security and privacy. We\nprovide formal security and privacy analyses of TAPFed and compare it to\nvarious baselines through experimental evaluation. Our results show that TAPFed\noffers equivalent performance in terms of model quality compared to\nstate-of-the-art approaches while reducing transmission overhead by 29%-45%\nacross different model training scenarios. Most importantly, TAPFed can defend\nagainst recently demonstrated inference attacks caused by curious aggregators,\nwhich the majority of existing approaches are susceptible to.",
        "A framework for portfolio allocation based on multiple hypotheses prediction\nusing structured ensemble models is presented. Portfolio optimization is\nformulated as an ensemble learning problem, where each predictor focuses on a\nspecific asset or hypothesis. The portfolio weights are determined by\noptimizing the ensemble's parameters, using an equal-weighted portfolio as the\ntarget, serving as a canonical basis for the hypotheses. Diversity in learning\namong predictors is parametrically controlled, and their predictions form a\nstructured input for the ensemble optimization model. The proposed methodology\nestablishes a link between this source of learning diversity and portfolio risk\ndiversification, enabling parametric control of portfolio diversification prior\nto the decision-making process. Moreover, the methodology demonstrates that the\ndiversity in asset or hypothesis selection, based on predictions of future\nreturns, before and independently of the ensemble learning stage, also\ncontributes to the out-of-sample portfolio diversification. The sets of assets\nwith more diverse but lower average return predictions are preferred over less\ndiverse selections. The methodology enables parametric control of diversity in\nboth the asset selection and learning stages, providing users with significant\ncontrol over out-of-sample portfolio diversification prior to decision-making.\nExperiments validate the hypotheses across one-step and multi-step decisions\nfor all parameter configurations and the structured model variants using equity\nportfolios.",
        "Mazur and Rubin introduced the notion of $n$-Selmer companion elliptic curves\nand gave several examples of pairs of non-isogenous Selmer companions. We\nconstruct two infinite families of pairs of non-isogenous $3$-Selmer\ncompanions, parameterised by $t\\in\\mathbb{Z}$.",
        "The fragmentation cross sections of carbon ion beams with kinetic energies of\n$115 - 353~\\text{MeV\/u}$ impinging on thin targets of graphite (C),\npolyvinyl-toluene (C$_9$H$_{10}$) and PMMA (C$_2$O$_5$H$_8$) have been measured\nat 90$^{\\text{o}}$ and 60$^{\\text{o}}$ at the CNAO particle therapy center\n(Pavia, Italy). The presented measurements are a complete reanalysis by the\nFOOT collaboration of already published elemental cross section on composite\ntargets, in order to refine the analysis, improve the systematic uncertainties\nand show the comparison with the FLUKA Monte Carlo code calculations. In this\nwork, the kinetic energy at production of measured fragments has been\ncompletely redefined, together with the efficiencies computation. The new\nanalysis strategy has been successfully validated against the Monte Carlo cross\nsections. Two detection arms were positioned at two different angles to perform\nthe measurement at 90$^{\\text{o}}$ and 60$^{\\text{o}}$. The fragment species\nhave been identified in charge (Z$_{id}$ = H) and mass (M$_{id}$ = $^1$H,\n$^2$H, $^3$H) combining the information of the deposited energy in thin plastic\nscintillators, of the deposited energy in a thick LYSO crystal and of the\nfragments Time of Flight (ToF) measurement. The ToF was also used to compute\nthe fragments measured kinetic energy. The cross sections are presented as a\nfunction of the fragments kinetic energy at production thanks to an unfolding\ntechnique applied to the data.",
        "Procedural representations are desirable, versatile, and popular shape\nencodings. Authoring them, either manually or using data-driven procedures,\nremains challenging, as a well-designed procedural representation should be\ncompact, intuitive, and easy to manipulate. A long-standing problem in shape\nanalysis studies how to discover a reusable library of procedural functions,\nwith semantically aligned exposed parameters, that can explain an entire shape\nfamily. We present ShapeLib as the first method that leverages the priors of\nfrontier LLMs to design a library of 3D shape abstraction functions. Our system\naccepts two forms of design intent: text descriptions of functions to include\nin the library and a seed set of exemplar shapes. We discover procedural\nabstractions that match this design intent by proposing, and then validating,\nfunction applications and implementations. The discovered shape functions in\nthe library are not only expressive but also generalize beyond the seed set to\na full family of shapes. We train a recognition network that learns to infer\nshape programs based on our library from different visual modalities\n(primitives, voxels, point clouds). Our shape functions have parameters that\nare semantically interpretable and can be modified to produce plausible shape\nvariations. We show that this allows inferred programs to be successfully\nmanipulated by an LLM given a text prompt. We evaluate ShapeLib on different\ndatasets and show clear advantages over existing methods and alternative\nformulations.",
        "The recent ATOMKI experiments provided evidence pointing towards the\nexistence of an X17 boson in the anomalous nuclear transitions of Beryllium-8,\nHelium-4, and Carbon-12. In this work, we consider X17 boson contributions to\nthe previously measured $D$ meson decays which include $D_s^{*+} \\rightarrow\nD_s^+ e^+ e^-$, $D_s^{*+} \\rightarrow D_s^+ \\gamma$, $D^{*0} \\rightarrow D^0\ne^+ e^-$, and $D^{*0} \\rightarrow D^0 \\gamma$, as well as the measured decays\nof $\\psi(2S) \\rightarrow \\eta_c e^+ e^-$, $\\psi(2S) \\rightarrow \\eta_c \\gamma$,\n$\\phi \\rightarrow \\eta e^+ e^-$, and $\\phi \\rightarrow \\eta \\gamma$. Using the\ndata of the above meson decays, we perform a fitting to the coupling parameters\n$\\varepsilon_u, \\varepsilon_c$, and $\\varepsilon_s$ by treating the couplings\n$\\varepsilon_u$ and $\\varepsilon_c$ as independent from each other rather than\nassuming the generation universality $\\varepsilon_u =\\varepsilon_c$. It is\nfound that the above fitting renders $|\\varepsilon_c|=3\\times 10^{-4}$,\n$|\\varepsilon_s|=4\\times 10^{-5}$ and a huge magnitude for $\\varepsilon_u$,\nwhich is in serious tension with $\\left|\\varepsilon_u\\right|$ determined from\nATOMKI measurements.",
        "We present microlux, which is a Jax-based code that can compute the binary\nmicrolensing light curve and its derivatives both efficiently and accurately.\nThe key feature of microlux is the implementation of a modified version of the\nadaptive sampling algorithm that was originally proposed by V. Bozza to account\nfor the finite-source effect most efficiently. The efficiency and accuracy of\nmicrolux have been verified across the relevant parameter space for binary\nmicrolensing. As a differentiable code, microlux makes it possible to apply\ngradient-based algorithms to the search and posterior estimation of the\nmicrolensing modeling. As an example, we use microlux to model a real\nmicrolensing event and infer the model posterior via both Fisher information\nmatrix and Hamiltonian Monte Carlo, neither of which would have been possible\nwithout the access to accurate model gradients.",
        "Cost-effective localization methods for Autonomous Underwater Vehicle (AUV)\nnavigation are key for ocean monitoring and data collection at high resolution\nin time and space. Algorithmic solutions suitable for real-time processing that\nhandle nonlinear measurement models and different forms of measurement\nuncertainty will accelerate the development of field-ready technology. This\npaper details a Bayesian estimation method for landmark-aided navigation using\na Side-scan Sonar (SSS) sensor. The method bounds navigation filter error in\nthe GPS-denied undersea environment and captures the highly nonlinear nature of\nslant range measurements while remaining computationally tractable. Combining a\nnovel measurement model with the chosen statistical framework facilitates the\nefficient use of SSS data and, in the future, could be used in real time. The\nproposed filter has two primary steps: a prediction step using an unscented\ntransform and an update step utilizing particles. The update step performs\nprobabilistic association of sonar detections with known landmarks. We evaluate\nalgorithm performance and tractability using synthetic data and real data\ncollected field experiments. Field experiments were performed using two\ndifferent marine robotic platforms with two different SSS and at two different\nsites. Finally, we discuss the computational requirements of the proposed\nmethod and how it extends to real-time applications.",
        "Symmetry-protected topological phases cannot be described by any local order\nparameter and are beyond the conventional symmetry-breaking paradigm for\nunderstanding quantum matter. They are characterized by topological boundary\nstates robust against perturbations that respect the protecting symmetry. In a\nclean system without disorder, these edge modes typically only occur for the\nground states of systems with a bulk energy gap and would not survive at finite\ntemperatures due to mobile thermal excitations. Here, we report the observation\nof a distinct type of topological edge modes, which are protected by emergent\nsymmetries and persist even up to infinite temperature, with an array of 100\nprogrammable superconducting qubits. In particular, through digital quantum\nsimulation of the dynamics of a one-dimensional disorder-free \"cluster\"\nHamiltonian, we observe robust long-lived topological edge modes over up to 30\ncycles at a wide range of temperatures. By monitoring the propagation of\nthermal excitations, we show that despite the free mobility of these\nexcitations, their interactions with the edge modes are substantially\nsuppressed in the dimerized regime due to an emergent U(1)$\\times$U(1)\nsymmetry, resulting in an unusually prolonged lifetime of the topological edge\nmodes even at infinite temperature. In addition, we exploit these topological\nedge modes as logical qubits and prepare a logical Bell state, which exhibits\npersistent coherence in the dimerized and off-resonant regime, despite the\nsystem being disorder-free and far from its ground state. Our results establish\na viable digital simulation approach to experimentally exploring a variety of\nfinite-temperature topological phases and demonstrate a potential route to\nconstruct long-lived robust boundary qubits that survive to infinite\ntemperature in disorder-free systems.",
        "Topology is an important determinant of the behavior of a great number of\ncondensed-matter systems, but until recently has played a minor role in\nelasticity. We develop a theory for the deformations of a class of twisted\nnon-Euclidean sheets which have a symmetry based on the celebrated Bonnet\nisometry. We show that non-orientability is an obstruction to realizing the\nsymmetry globally, and induces a geometric phase that captures a memory\nanalogous to a previously identified one in 2D metamaterials. However, we show\nthat some of the orientable ribbons also obstruct realizing the symmetry\nglobally. This new obstruction is mediated by the complex interplay between\nstrain, geometry, and topology.",
        "This paper introduces V$^2$Edit, a novel training-free framework for\ninstruction-guided video and 3D scene editing. Addressing the critical\nchallenge of balancing original content preservation with editing task\nfulfillment, our approach employs a progressive strategy that decomposes\ncomplex editing tasks into a sequence of simpler subtasks. Each subtask is\ncontrolled through three key synergistic mechanisms: the initial noise, noise\nadded at each denoising step, and cross-attention maps between text prompts and\nvideo content. This ensures robust preservation of original video elements\nwhile effectively applying the desired edits. Beyond its native video editing\ncapability, we extend V$^2$Edit to 3D scene editing via a\n\"render-edit-reconstruct\" process, enabling high-quality, 3D-consistent edits\neven for tasks involving substantial geometric changes such as object\ninsertion. Extensive experiments demonstrate that our V$^2$Edit achieves\nhigh-quality and successful edits across various challenging video editing\ntasks and complex 3D scene editing tasks, thereby establishing state-of-the-art\nperformance in both domains.",
        "High-dynamic scene optical flow is a challenging task, which suffers spatial\nblur and temporal discontinuous motion due to large displacement in frame\nimaging, thus deteriorating the spatiotemporal feature of optical flow.\nTypically, existing methods mainly introduce event camera to directly fuse the\nspatiotemporal features between the two modalities. However, this direct fusion\nis ineffective, since there exists a large gap due to the heterogeneous data\nrepresentation between frame and event modalities. To address this issue, we\nexplore a common-latent space as an intermediate bridge to mitigate the\nmodality gap. In this work, we propose a novel common spatiotemporal fusion\nbetween frame and event modalities for high-dynamic scene optical flow,\nincluding visual boundary localization and motion correlation fusion.\nSpecifically, in visual boundary localization, we figure out that frame and\nevent share the similar spatiotemporal gradients, whose similarity distribution\nis consistent with the extracted boundary distribution. This motivates us to\ndesign the common spatiotemporal gradient to constrain the reference boundary\nlocalization. In motion correlation fusion, we discover that the frame-based\nmotion possesses spatially dense but temporally discontinuous correlation,\nwhile the event-based motion has spatially sparse but temporally continuous\ncorrelation. This inspires us to use the reference boundary to guide the\ncomplementary motion knowledge fusion between the two modalities. Moreover,\ncommon spatiotemporal fusion can not only relieve the cross-modal feature\ndiscrepancy, but also make the fusion process interpretable for dense and\ncontinuous optical flow. Extensive experiments have been performed to verify\nthe superiority of the proposed method.",
        "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance.",
        "If a body of inviscid fluid is disturbed, it will typically eject a jet of\nfluid. If the effects of gravity and surface tension are negligible, these jets\ntravel in straight lines, with the tips approaching a constant velocity. It has\nbeen observed that these jets can have a broad base, tapering progressively\ntoward the tip, but the mathematical form of their profile has not been\nsuccessfully analysed in earlier works. In this paper, we describe the simplest\ncase, in two dimensions: an infinitely deep body of inviscid fluid, with no\nsurface tension or gravitational forces acting, responds to an impulsive\ndisturbance. We find that, contrary to some earlier suggestions, the jet has a\nhyperbolic profile (away from its tip and its base)."
      ]
    }
  },
  {
    "id":2411.14752,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"\u201cApr\u00e8s Mois, Le D\u00e9luge\u201d: Preparing for the Coming Data Flood in the MRI-Guided Radiotherapy Era",
    "start_abstract":"Magnetic resonance imaging provides a sea of quantitative and semi-quantitative data. While radiation oncologists already navigate pool clinical (semantic) data, the tide will swell with advent hybrid MRI\/linear accelerator devices increasing interest in MRI-guided radiotherapy (MRIgRT), including adaptive MRIgRT. The variety MR sequences (of greater complexity than single parameter Hounsfield unit CT scanning routinely used radiotherapy), workflow fractionation, sheer quantity daily images acquired are challenges for scaling this technology. Biomedical informatics, which is science information biomedicine, can provide helpful insights looming transition. Funneling MRIgRT data into clinically meaningful streams requires committing to flow inter-institutional accessibility interoperability initiatives, standardizing dosimetry methods, streamlining linear workflow, MRI acquisition post-processing, current topic review attempt conceptually ford using informatics approaches as theoretical bridge.",
    "start_categories":[
      "Oncology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation",
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is made publicly available at: https:\/\/github.com\/MIC-DKFZ\/MedNeXt.",
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "How to Minimize the Decoherence Caused by Black Holes",
        "Quantum Monte Carlo Calculations of neutron-$\\alpha$ Scattering via an\n  Integral Relation",
        "Modeling the Impact of Moderate External UV Irradiation on Disk\n  Chemistry",
        "Towards Understanding of Frequency Dependence on Sound Event Detection",
        "No evidence (yet) for increased star-formation efficiency at early times",
        "SAKE: Steering Activations for Knowledge Editing",
        "A Study on the Performance of U-Net Modifications in Retroperitoneal\n  Tumor Segmentation",
        "Extreme mixed-mode oscillatory bursts in the Helmholtz-Duffing\n  oscillator",
        "An Improved NSGA-II with local search for multi-objective\n  energy-efficient flowshop scheduling problem",
        "Partial Actions on Generalized Boolean Algebras with Applications to\n  Inverse Semigroups and Combinatorial $R$-Algebras",
        "DiffCL: A Diffusion-Based Contrastive Learning Framework with Semantic\n  Alignment for Multimodal Recommendations",
        "Pessimistic bilevel optimization approach for decision-focused learning",
        "Ecomap: Sustainability-Driven Optimization of Multi-Tenant DNN Execution\n  on Edge Servers",
        "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning",
        "IPO: Your Language Model is Secretly a Preference Classifier",
        "Flux homomorphism and bilinear form constructed from Shelukhin's\n  quasimorphism",
        "Integration of LLM Quality Assurance into an NLG System",
        "Evaluating the faithfulness of PDF uncertainties in the presence of\n  inconsistent data",
        "Exploring some cosmological aspects of Kaniadakis entropy",
        "Greenberg's conjecture for real quadratic number fields",
        "Stirring supercooled colloidal liquids at the particle scale",
        "Factual Inconsistency in Data-to-Text Generation Scales Exponentially\n  with LLM Size: A Statistical Validation",
        "Microwave-regime demonstration of plasmonic non-reciprocity in a flowing\n  two-dimensional electron gas",
        "\"This could save us months of work\" -- Use Cases of AI and Automation\n  Support in Investigative Journalism",
        "Multi-Scale Conformal Prediction: A Theoretical Framework with Coverage\n  Guarantees",
        "Generic linear convergence for algorithms of non-linear least squares\n  over smooth varieties",
        "Finite Volume Hamiltonian method for two-particle systems containing\n  long-range potential on the lattice",
        "A General Error-Theoretical Analysis Framework for Constructing\n  Compression Strategies",
        "Quantum effects in charge control of semiconductor surfaces as\n  elucidated by ab initio calculations a review"
      ],
      "abstract":[
        "We consider an experimentalist, Alice, who creates a quantum superposition of\na charged or massive body outside of a black hole (or, more generally, in the\npresence of a Killing horizon). It was previously shown that emission of soft\nphotons\/gravitons into the black hole will result in the decoherence of the\ncomponents of the superposition if it is held open for a sufficiently long span\nof time. However, at any finite time, $t_c$, during the process, it is not\nobvious how much decoherence has irrevocably occurred. Equivalently, it is not\nobvious how much information an observer inside the black hole can extract\nabout Alice's superposition prior to time $t_c$. In this paper, we solve for\nthe optimal experimental protocol to be followed by Alice for $t > t_c$ so as\nto minimize the decoherence of the components of her superposition. More\nprecisely, given the entangling radiation that has passed through the horizon\nprior to the cross-section $\\mathcal C$ corresponding to the time $t = t_c$ in\nAlice's lab, we determine the \"optimal purification\" of this radiation beyond\n$\\mathcal C$ such that the global quantum state of the radiation through the\nhorizon has maximal overlap (quantum fidelity) with the Hartle-Hawking or Unruh\nvacuum. Due to the intricate low frequency entanglement structure of the\nquantum field theory vacuum state, we find this optimal purification to be\nnontrivial. In particular, even if Alice has already \"closed\" her superposition\nby bringing the components back together, we find that she can decrease the net\ndecoherence of the components of her superposition somewhat by re-opening it\nand performing further manipulations.",
        "Nuclear physics seeks to describe both bound and unbound states within a\nunified predictive framework. While coordinate-space Quantum Monte Carlo (QMC)\nmethods have successfully computed bound states for systems with $A \\leq 12$,\ntheir application to unbound states remains limited. In this work, we extend\nthe QMC approach to enable a broader range of unbound-state calculations. Our\nmethod infers long-range amplitudes in the wave function from integrals over\nthe short-range interaction region. By evaluating these integrals using Green's\nFunction Monte Carlo wave functions with the Argonne $v_{18}$ potential, we\naccurately reproduce existing results for neutron-alpha scattering. This\napproach provides a systematic pathway for studying more complex nuclear\nsystems, including coupled-channel scattering and the effects of three-nucleon\nforces. It serves as a powerful tool for advancing $\\textit{ab initio}$\ncalculations in nuclear reactions, paving the way for a unified framework that\nconsistently describes both bound and scattering states within a single\ntheoretical approach.",
        "The chemistry within a protoplanetary disk is greatly affected by external\nradiation from the local stellar environment. Previous work has focused on\nextreme radiation fields, representative of the center of something like the\nOrion Nebula Cluster. However, even in such environments, many disks exist at\nthe edges of a cluster where the lower stellar density leads to radiation\nfields weaker by orders of magnitude compared to the center. We present new\nchemical models of a T-Tauri disk in the presence of a moderately increased\ninterstellar radiation field (ISRF). Such an environment has a background UV\nstrength of 10 to 100 times higher than the galactic average ISRF. Moderate\nradiation fields are among the most prevalent disk-harboring environments and\nhave interesting implications for the chemistry of the outer disk radii. We\nfind that the external UV radiation creates an outer ionization front that\nimpacts the cold disk chemistry to varying degrees, depending on outer disk\nstructure. Certain molecules like C$^+$, N$_2$H$^+$, C, and CS are more\nstrongly impacted by the ISRF in their abundance, column density, and\nobservable emission. Other abundant species like HCO$^+$ and CO are less\naffected by the external UV flux in the outer disk under such moderate UV\nconditions. Further, we demonstrate that the chemistry occurring in the inner\ntens of au is relatively unchanged, which suggests that even in moderately\nexternally irradiated disks, the inner disk chemistry may be more similar to\nisolated disks like those in, e.g., the Taurus and Lupus star-forming regions.",
        "In this work, various analysis methods are conducted on frequency-dependent\nmethods on SED to further delve into their detailed characteristics and\nbehaviors on SED. While SED has been rapidly advancing through the adoption of\nvarious deep learning techniques from other pattern recognition fields, these\ntechniques are often not suitable for SED. To address this issue, two\nfrequency-dependent SED methods were previously proposed: FilterAugment, a data\naugmentation randomly weighting frequency bands, and frequency dynamic\nconvolution (FDY Conv), an architecture applying frequency adaptive convolution\nkernels. These methods have demonstrated superior performance in SED, and we\naim to further analyze their detailed effectiveness and characteristics in SED.\nWe compare class-wise performance to find out specific pros and cons of\nFilterAugment and FDY Conv. We apply Gradient-weighted Class Activation Mapping\n(Grad-CAM), which highlights time-frequency region that is more inferred by the\nmodel, on SED models with and without frequency masking and two types of\nFilterAugment to observe their detailed characteristics. We propose simpler\nfrequency dependent convolution methods and compare them with FDY Conv to\nfurther understand which components of FDY Conv affects SED performance.\nLastly, we apply PCA to show how FDY Conv adapts dynamic kernel across\nfrequency dimensions on different sound event classes. The results and\ndiscussions demonstrate that frequency dependency plays a significant role in\nsound event detection and further confirms the effectiveness of frequency\ndependent methods on SED.",
        "Early JWST observations have revealed substantial numbers of galaxies out to\nredshifts as high as $z \\simeq 14$, reflecting a slow evolution of the galaxy\nUV luminosity function (LF) not anticipated by many models of galaxy evolution.\nIt has also been discovered that fairly massive galaxies existed at early\ntimes, a finding again viewed as a challenge to our understanding of early\ngalaxy growth or even ${\\rm \\Lambda}$CDM cosmology. Here we develop and test a\nsimple theoretical model which shows that these observations are unsurprising,\nbut instead are arguably as expected if one assumes a non-evolving halo-mass\ndependent galaxy-formation efficiency consistent with that observed today.\nCrucially, this simple model matches the observed galaxy UV LF over the\nredshift range $z \\simeq 6-13$ and the galaxy stellar mass function (GSMF) at\n$z \\simeq 6-8$. When combined with new constraints on Lyman continuum escape\nand the ionizing photon production efficiency of early galaxies, our model also\npredicts the progress of cosmic hydrogen reionization consistent with current\nobservational constraints. The requirement to fit both the UV LF and the GSMF\nbreaks the degeneracy between mass-light ratio and star-formation efficiency,\nand our model only works if the typical mass-to-light ratio of galaxies\nincreases systematically with redshift beyond $z \\simeq 6$. However, at present\nthis does not require changes to the IMF, cosmic dust, or any other new\nastrophysics. Rather, the current data can be reproduced simply by assuming\never-younger stellar populations consistent with a formation epoch at $z \\simeq\n15$. A key prediction of our model is thus that there should be a more rapid\ndrop-off in the numbers of galaxy number density beyond $z \\simeq 15$, where\none can no longer appeal to ever younger ages to offset the precipitous descent\nof the halo mass function.",
        "As Large Langue Models have been shown to memorize real-world facts, the need\nto update this knowledge in a controlled and efficient manner arises. Designed\nwith these constraints in mind, Knowledge Editing (KE) approaches propose to\nalter specific facts in pretrained models. However, they have been shown to\nsuffer from several limitations, including their lack of contextual robustness\nand their failure to generalize to logical implications related to the fact. To\novercome these issues, we propose SAKE, a steering activation method that\nmodels a fact to be edited as a distribution rather than a single prompt.\nLeveraging Optimal Transport, SAKE alters the LLM behavior over a whole\nfact-related distribution, defined as paraphrases and logical implications.\nSeveral numerical experiments demonstrate the effectiveness of this method:\nSAKE is thus able to perform more robust edits than its existing counterparts.",
        "The retroperitoneum hosts a variety of tumors, including rare benign and\nmalignant types, which pose diagnostic and treatment challenges due to their\ninfrequency and proximity to vital structures. Estimating tumor volume is\ndifficult due to their irregular shapes, and manual segmentation is\ntime-consuming. Automatic segmentation using U-Net and its variants,\nincorporating Vision Transformer (ViT) elements, has shown promising results\nbut struggles with high computational demands. To address this, architectures\nlike the Mamba State Space Model (SSM) and Extended Long-Short Term Memory\n(xLSTM) offer efficient solutions by handling long-range dependencies with\nlower resource consumption. This study evaluates U-Net enhancements, including\nCNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ\nsegmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for\nimproved segmentation. Results highlight xLSTM's efficiency in the U-Net\nframework. The code is publicly accessible on GitHub.",
        "We report a new type of extreme event - extreme irregular mixed-mode\noscillatory burst - appearing in an asymmetric double-welled, driven\nHelmholtz-Duffing oscillator. The interplay of cubic and quadratic\nnonlinearities in the system, along with the external drive, contributes to\nthis type of unusual extreme event. These extreme events are classified using\nthe peak-over threshold method and found to be well-fitted with the generalized\nextreme value distribution. The asymmetry in the depth of one of the potential\nwells allows the oscillator to exhibit rare irregular mixed-mode oscillations\nbetween the two wells, manifesting as extremely irregular mixed-mode\noscillatory bursts. Furthermore, we also find that such an irregular transition\nbetween the two potential wells occurs during the up phase of the periodic\nexternal drive. Importantly, we also find that the system velocity can be\ntracked and utilized as a reliable lead indicator of the occurrence of this\nnovel type of extreme event.",
        "There has been an increasing concern to reduce the energy consumption in\nmanufacturing and other industries. Energy consumption in manufacturing\nindustries is directly related to efficient schedules. The contribution of this\npaper includes: i) a permutation flowshop scheduling problem (PFLSP)\nmathematical model by considering energy consumed by each machine in the\nsystem. ii) an improved non-dominated sorted genetic algorithm with Taguchi\nmethod with further incorporating local search (NSGA-II_LS) is proposed for the\nmulti-objective PFLSP model. iii) solved 90 benchmarks problems of Taillard\n(1993) for the minimisation of flowtime (FT) and energy consumption (EC). The\nperformance of the proposed NSGA_LS algorithm is evaluated on the benchmark\nproblems selected from the published literature Li et. al, (2018). From these\nresults, it is noted that the proposed algorithm performed better on both the\nobjectives i.e., FT and EC minimization in 5 out of 9 cases. On FT objective\nour algorithm performed better in 8 out of 9 cases and on EC objective 5 out of\n9 cases. Overall, the proposed algorithm achieved 47% and 15.44% average\nimprovement in FT and EC minimization respectively on the benchmark problems.\nFrom the results of 90 benchmark problems, it is observed that average\ndifference in FT and EC between two solutions is decreasing as the problem size\nincreases from 5 machines to 10 machines with an exception in one case.\nFurther, it is observed that the performance of the proposed algorithm is\nbetter as the problem size increases in both jobs and machines. These results\ncan act as standard solutions for further research.",
        "We define the notion of a partial action on a generalized Boolean algebra and\nassociate to every such system and commutative unital ring $R$ an $R$-algebra.\nWe prove that every strongly $E^{\\ast}$-unitary inverse semigroup has an\nassociated partial action on the generalized Boolean algebra of compact open\nsets of tight filters in the meet semilattice of idempotents. Using these\ncorrespondences, we associate to every strongly $E^{\\ast}$-unitary inverse\nsemigroup and commutative unital ring $R$ an $R$-algebra, and show that it is\nisomorphic to the Steinberg algebra of the tight groupoid. As an application,\nwe show that there is a natural unitization operation on an inverse semigroup\nthat corresponds to a unitization of the corresponding $R$-algebra. Finally, we\nshow that Leavitt path algebras and labelled Leavitt path algebras can be\nrealized as the $R$-algebra associated to a strongly $E^{\\ast}$-unitary inverse\nsemigroup.",
        "Multimodal recommendation systems integrate diverse multimodal information\ninto the feature representations of both items and users, thereby enabling a\nmore comprehensive modeling of user preferences. However, existing methods are\nhindered by data sparsity and the inherent noise within multimodal data, which\nimpedes the accurate capture of users' interest preferences. Additionally,\ndiscrepancies in the semantic representations of items across different\nmodalities can adversely impact the prediction accuracy of recommendation\nmodels. To address these challenges, we introduce a novel diffusion-based\ncontrastive learning framework (DiffCL) for multimodal recommendation. DiffCL\nemploys a diffusion model to generate contrastive views that effectively\nmitigate the impact of noise during the contrastive learning phase.\nFurthermore, it improves semantic consistency across modalities by aligning\ndistinct visual and textual semantic information through stable ID embeddings.\nFinally, the introduction of the Item-Item Graph enhances multimodal feature\nrepresentations, thereby alleviating the adverse effects of data sparsity on\nthe overall system performance. We conduct extensive experiments on three\npublic datasets, and the results demonstrate the superiority and effectiveness\nof the DiffCL.",
        "The recent interest in contextual optimization problems, where randomness is\nassociated with side information, has led to two primary strategies for\nformulation and solution. The first, estimate-then-optimize, separates the\nestimation of the problem's parameters from the optimization process. The\nsecond, decision-focused optimization, integrates the optimization problem's\nstructure directly into the prediction procedure. In this work, we propose a\npessimistic bilevel approach for solving general decision-focused formulations\nof combinatorial optimization problems. Our method solves an\n$\\varepsilon$-approximation of the pessimistic bilevel problem using a\nspecialized cut generation algorithm. We benchmark its performance on the 0-1\nknapsack problem against estimate-then-optimize and decision-focused methods,\nincluding the popular SPO+ approach. Computational experiments highlight the\nproposed method's advantages, particularly in reducing out-of-sample regret.",
        "Edge computing systems struggle to efficiently manage multiple concurrent\ndeep neural network (DNN) workloads while meeting strict latency requirements,\nminimizing power consumption, and maintaining environmental sustainability.\nThis paper introduces Ecomap, a sustainability-driven framework that\ndynamically adjusts the maximum power threshold of edge devices based on\nreal-time carbon intensity. Ecomap incorporates the innovative use of\nmixed-quality models, allowing it to dynamically replace computationally heavy\nDNNs with lighter alternatives when latency constraints are violated, ensuring\nservice responsiveness with minimal accuracy loss. Additionally, it employs a\ntransformer-based estimator to guide efficient workload mappings. Experimental\nresults using NVIDIA Jetson AGX Xavier demonstrate that Ecomap reduces carbon\nemissions by an average of 30% and achieves a 25% lower carbon delay product\n(CDP) compared to state-of-the-art methods, while maintaining comparable or\nbetter latency and power efficiency.",
        "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.",
        "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences.",
        "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
        "In this paper, we present a system that uses a Large Language Model (LLM) to\nperform grammar and spelling correction as a component of Quality Assurance\n(QA) for texts generated by NLG systems, which is important for text production\nin real-world scenarios. Evaluating the results of the system on\nwork-in-progress sports news texts in three languages, we show that it is able\nto deliver acceptable corrections.",
        "We critically assess the robustness of uncertainties on parton distribution\nfunctions (PDFs) determined using neural networks from global sets of\nexperimental data collected from multiple experiments. We view the\ndetermination of PDFs as an inverse problem, and we study the way the neural\nnetwork model tackles it when inconsistencies between input datasets are\npresent. We use a closure test approach, in which the regression model is\napplied to artificial data produced from a known underlying truth, to which the\noutput of the model can be compared and its accuracy can be assessed in a\nstatistically reliable way. We explore various phenomenologically relevant\nscenarios in which inconsistencies arise due to incorrect estimation of\ncorrelated systematic uncertainties. We show that the neural network generally\ncorrects for the inconsistency except in cases of extreme uncertainty\nunderestimation. When the inconsistency is not corrected, we propose and\nvalidate a procedure to detect inconsistencies.",
        "Adopting the modifications induced by the Kaniadakis entropy on the Friedmann\nequations, we explore some relevant aspects of this cosmological scenario at\nthe background level. We analyze the constraint imposed on the parameter $K$\nobtained from the accelerated cosmic expansion condition, and we also study the\nrole of such a parameter as a cosmological constant.",
        "We compute the $3$-class groups $A_n$ of the fields $F_n$ in the cyclotomic\n$\\mathbf{Z}_3$-extensions of the real quadratic fields of discriminant\n$f<100,000$. In all cases the orders of $A_n$ remain bounded as $n$ goes to\ninfinity. This is in agreement with Greenberg's conjecture.",
        "We study the decay of tangential velocity profiles with distance from a local\ndisturbance in hard-sphere colloidal suspensions as the colloidal glass\ntransition is approached. The disturbance, generated by a dimer of\nsuperparamagnetic particles rotated by an external magnetic field, enables a\nprecise characterization of the system's response through confocal microscopy\nand tracking of individual particle dynamics. The tangential velocity profiles\nexhibit nearly exponential decay with distance. As particle density increases\ntoward the colloidal glass transition, the characteristic length scale derived\nfrom exponential fits grows. We also observe that the colloidal particles slip\nagainst the rotating dimer, with less slip in samples which are closer to the\nglass transition.",
        "Monitoring factual inconsistency is essential for ensuring trustworthiness in\ndata-to-text generation (D2T). While large language models (LLMs) have\ndemonstrated exceptional performance across various D2T tasks, previous studies\non scaling laws have primarily focused on generalization error through power\nlaw scaling to LLM size (i.e., the number of model parameters). However, no\nresearch has examined the impact of LLM size on factual inconsistency in D2T.\nIn this paper, we investigate how factual inconsistency in D2T scales with LLM\nsize by exploring two scaling laws: power law and exponential scaling. To\nrigorously evaluate and compare these scaling laws, we employ a statistical\nvalidation framework consisting of three key stages: predictive performance\nestimation, goodness-of-fit assessment, and comparative analysis. For a\ncomprehensive empirical study, we analyze three popular LLM families across\nfive D2T datasets, measuring factual inconsistency inversely using four\nstate-of-the-art consistency metrics. Our findings, based on exhaustive\nempirical results and validated through our framework, reveal that, contrary to\nthe widely assumed power law scaling, factual inconsistency in D2T follows an\nexponential scaling with LLM size.",
        "The speed of a plasmonic wave in the presence of electron drift in a\nconductor depends on the wave's propagation direction, with the wave traveling\nalong the drift (`forward wave') faster than the wave traveling against the\ndrift (`backward wave'). Phenomena related to this plasmonic non-reciprocity --\nwhich is relatively more pronounced in two-dimensional conductors than in bulk\nconductors and could lead to solid-state device applications -- have been\nstudied in THz and optical spectral regimes. Here we demonstrate the plasmonic\nnon-reciprocity at microwave frequencies (10 $\\sim$ 50 GHz). Concretely, we\nconduct, at 4K, a microwave network analysis on a gated GaAs two-dimensional\nelectron gas with electron drift (i.e., DC current), directly measuring out\nforward and backward wave speeds via their propagation phase delays. We\nresolve, for example, forward and backward wave speeds of $4.26 \\times 10^{-3}\n\\pm 8.97 \\times 10^{-6}$ (normalized to the speed of light). Sufficient\nconsistency between the electron drift speed obtained from the microwave\nmeasurement and that alternatively estimated by a DC transport theory further\nconfirms the non-reciprocity. We conclude this paper with a discussion on how\nto enhance the non-reciprocity for real-world applications, where degeneracy\npressure would play an important role.",
        "As the capabilities of Large Language Models (LLMs) expand, more researchers\nare studying their adoption in newsrooms. However, much of the research focus\nremains broad and does not address the specific technical needs of\ninvestigative journalists. Therefore, this paper presents several applied use\ncases where automation and AI intersect with investigative journalism. We\nconducted a within-subjects user study with eight investigative journalists. In\ninterviews, we elicited practical use cases using a speculative design approach\nby having journalists react to a prototype of a system that combines LLMs and\nProgramming-by-Demonstration (PbD) to simplify data collection on numerous\nwebsites. Based on user reports, we classified the journalistic processes into\ndata collecting and reporting. Participants indicated they utilize automation\nto handle repetitive tasks like content monitoring, web scraping,\nsummarization, and preliminary data exploration. Following these insights, we\nprovide guidelines on how investigative journalism can benefit from AI and\nautomation.",
        "We propose a multi-scale extension of conformal prediction, an approach that\nconstructs prediction sets with finite-sample coverage guarantees under minimal\nstatistical assumptions. Classic conformal prediction relies on a single notion\nof conformity, overlooking the multi-level structures that arise in\napplications such as image analysis, hierarchical data exploration, and\nmulti-resolution time series modeling. In contrast, the proposed framework\ndefines a distinct conformity function at each relevant scale or resolution,\nproducing multiple conformal predictors whose prediction sets are then\nintersected to form the final multi-scale output. We establish theoretical\nresults confirming that the multi-scale prediction set retains the marginal\ncoverage guarantees of the original conformal framework and can, in fact, yield\nsmaller or more precise sets in practice. By distributing the total miscoverage\nprobability across scales in proportion to their informative power, the method\nfurther refines the set sizes. We also show that dependence between scales can\nlead to conservative coverage, ensuring that the actual coverage exceeds the\nnominal level. Numerical experiments in a synthetic classification setting\ndemonstrate that multi-scale conformal prediction achieves or surpasses the\nnominal coverage level while generating smaller prediction sets compared to\nsingle-scale conformal methods.",
        "In applications, a substantial number of problems can be formulated as\nnon-linear least squares problems over smooth varieties. Unlike the usual least\nsquares problem over a Euclidean space, the non-linear least squares problem\nover a variety can be challenging to solve and analyze, even if the variety\nitself is simple. Geometrically, this problem is equivalent to projecting a\npoint in the ambient Euclidean space onto the image of the given variety under\na non-linear map. It is the singularities of the image that make both the\ncomputation and the analysis difficult. In this paper, we prove that under some\nmild assumptions, these troublesome singularities can always be avoided. This\nenables us to establish a linear convergence rate for iterative sequences\ngenerated by algorithms satisfying some standard assumptions. We apply our\ngeneral results to the low-rank partially orthogonal tensor approximation\nproblem. As a consequence, we obtain the linear convergence rate for a\nclassical APD-ALS method applied to a generic tensor, without any further\nassumptions.",
        "We propose a systematic method to block-diagonalize the finite volume\neffective Hamiltonian for two-particle systems with arbitrary spin in both the\nrest and moving frame. The framework is convenient and efficient for addressing\nthe left-hand cut issue arising from long-range potential, which are\nchallenging in the framework of standard L\\\"uscher formula. Furthermore, the\nmethod provides a foundation for further extension to three-particle systems.\nWe first benchmark our method by examining several toy models, demonstrating\nits consistency with standard L\\\"uscher formula in the absence of long-range\npotential. In the presence of long-range potential, we investigate and resolve\nthe effects and issues of left-hand cut. As a realistic application, we\ncalculate the finite volume spectra of isoscalar $D\\bar{D}^*$ system, where the\nwell-known exotic state $\\chi_{c1}(3872)$ is observed. The results are\nqualitatively consistent with the lattice QCD calculation, highlighting the\nreliability and potential application of our framework to the study of other\nexotic states in hadron physics.",
        "The exponential growth in parameter size and computational complexity of deep\nmodels poses significant challenges for efficient deployment. The core problem\nof existing compression methods is that different layers of the model have\nsignificant differences in their tolerance to compression levels. For instance,\nthe first layer of a model can typically sustain a higher compression level\ncompared to the last layer without compromising performance. Thus, the key\nchallenge lies in how to allocate compression levels across layers in a way\nthat minimizes performance loss while maximizing parameter reduction. To\naddress this challenge, we propose a Compression Error Theory (CET) framework,\ndesigned to determine the optimal compression level for each layer. Taking\nquantization as an example, CET leverages differential expansion and algebraic\ngeometry to reconstruct the quadratic form of quantization error as ellipsoids\nand hyperbolic paraboloids, and utilizes their geometric structures to define\nan error subspace. To identify the error subspace with minimal performance\nloss, by performing orthogonal decomposition of the geometric space, CET\ntransforms the optimization process of the error subspace into a complementary\nproblem. The final theoretical analysis shows that constructing the\nquantization subspace along the major axis results in minimal performance\ndegradation. Through experimental verification of the theory, CET can greatly\nretain performance while compressing. Specifically, on the ResNet-34 model, CET\nachieves nearly 11$\\times$ parameter compression while even surpassing\nperformance comparable to the original model.",
        "Recent progress in the investigations of the charge role in semiconductor\nsurfaces is reviewed."
      ]
    }
  },
  {
    "id":2411.14752,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation",
    "start_abstract":"There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is made publicly available at: https:\/\/github.com\/MIC-DKFZ\/MedNeXt.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "\u201cApr\u00e8s Mois, Le D\u00e9luge\u201d: Preparing for the Coming Data Flood in the MRI-Guided Radiotherapy Era"
      ],
      "abstract":[
        "Magnetic resonance imaging provides a sea of quantitative and semi-quantitative data. While radiation oncologists already navigate pool clinical (semantic) data, the tide will swell with advent hybrid MRI\/linear accelerator devices increasing interest in MRI-guided radiotherapy (MRIgRT), including adaptive MRIgRT. The variety MR sequences (of greater complexity than single parameter Hounsfield unit CT scanning routinely used radiotherapy), workflow fractionation, sheer quantity daily images acquired are challenges for scaling this technology. Biomedical informatics, which is science information biomedicine, can provide helpful insights looming transition. Funneling MRIgRT data into clinically meaningful streams requires committing to flow inter-institutional accessibility interoperability initiatives, standardizing dosimetry methods, streamlining linear workflow, MRI acquisition post-processing, current topic review attempt conceptually ford using informatics approaches as theoretical bridge."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Design of resilient structures by randomization and bistability",
        "Numerical action for endomorphisms",
        "Mapping Parameter Correlations in Spinning Binary Black Hole Mergers",
        "Quasi-aperiodic grain boundary phases of {\\Sigma}5 tilt grain boundaries\n  in refractory metals",
        "Spiral Spin Liquid State in the Corrugated Honeycomb Lattice of\n  CaMn$_2$P$_2$",
        "Perfect Transfer of Entanglement and One-Way Quantum Steering via\n  Parametric Frequency Converter in a Two-mode Cavity Magnomechanical System",
        "Multiband dispersion and warped vortices of strongly-interacting photons",
        "Experimental Demonstration of an Optical Neural PDE Solver via On-Chip\n  PINN Training",
        "A parametric study of the pipeline hammer phenomenon in plastic Bingham\n  slurry flows using the finite element method",
        "Four-point correlators with BPS bound states in AdS$_3$ and AdS$_5$",
        "Emergent fractals in hBN-encapsulated graphene based supermoir\\'e\n  structures and their experimental signatures",
        "Multi-component altermagnet: A general approach to generating\n  multi-component structures with two-dimensional altermagnetism",
        "First differential measurement of the single $\\mathbf{\\pi}^+$ production\n  cross section in neutrino neutral-current scattering",
        "On the Impacts of Halo Model Implementations in Sunyaev-Zeldovich\n  Cross-Correlation Analyses",
        "Critical $(P_5,W_4)$-Free Graphs",
        "A Statistical Theory of Contrastive Pre-training and Multimodal\n  Generative AI",
        "On invex functions with same {\\eta} in single and multivalued nonsmooth\n  optimization with Clarke's subdifferential",
        "Tensor meson transition form factors in holographic QCD and the muon\n  $g-2$",
        "Anticipated backward stochastic Volterra integral equations and their\n  applications to nonzero-sum stochastic differential games",
        "From trees to traits: A review of advances in PhyloG2P methods and\n  future directions",
        "Strain-Induced Optical and Molecular Transformations in PET Films for\n  Organic Electronic Applications",
        "Micro Text Classification Based on Balanced Positive-Unlabeled Learning",
        "Effects of electron correlation on resonant Edelstein and\n  inverse-Edelstein effects",
        "Growing black-hole hair in nonminimally coupled biscalar gravity",
        "A symmetry-protected topological optical lattice clock",
        "Darwinian spreading and quality thinning in even-aged boreal forest\n  stands",
        "Decay of solutions of nonlinear Dirac equations",
        "Projective crossed modules in semi-abelian categories",
        "Melting of devil's staircases in the long-range Dicke-Ising model"
      ],
      "abstract":[
        "This paper examines various ways of improving the impact resilience of\nprotective structures. Such structures' purpose is to dissipate an impact's\nenergy while avoiding cracking and failure. We have tested the reaction of\nplane elastic-brittle lattices to an impulse. Four topologies are compared:\nperiodic triangular, square, and hexagonal topologies, and aperiodic Penrose\ntopology. Then, structures with random variations of the links' stiffness, node\npositions, and random holes are compared. Combinations of these random factors\nare also considered, as well as the resilience of bistable elastic-brittle\nlattices with sacrificial links. Several parameters are introduced to measure\nthe structural resilience of the compared designs: (i) the amount of dissipated\nimpact energy, (ii) the size of broken clusters of links, and (iii) the spread\nof damage. The results suggest new routes for rationally designing protective\nstructures using nonperiodic topology, bistability, and structural randomness.\nIn particular, we find that some quantities of interest can be maximized by\ntuning the randomized design appropriately -- for example, randomly removing\n8\\% of links maximizes energy dissipation. We also find that randomization of\nbistable lattices can offer superior energy dissipation while reducing the\nconnectivity between broken clusters of links.",
        "Let $f: X\\to X$ be a surjective endomorphism of a projective variety of\ndimension $d$. The aim of this paper is to study the action of $f$ on the\nnumerical group of divisors. For exmaple, I proved that $f$ is cohomologically\nhyperbolic if and only if it is quasi-amplified; and it is amplified if and\nonly if every subsystem of $(X,f)$ is cohomologically hyperbolic. For the\nproofs, I introduced a notion of spectrum in linear algebra for an open and\nsaliant invariant cone. I also introduce a notion of generated (positive)\ncycles as an algebraic analogy of (positive) closed current.",
        "The spins of black holes in binaries measured with gravitational waves\nprovide insights about the formation, evolution, and dynamics of these systems.\nThe imprint of spin in the inspiral, where the black holes are well-separated,\nis understood through analytic equations for the binary dynamics. During the\nmerger phase, the binary dynamics can only be studied with numerical relativity\nsimulations. Though such simulations provide an exact solution (to within\nnumerical error), the imprint of the full six spin degrees of freedom on the\nsignal is not transparent. In the absence of analytic expressions for the\nmerger, here we propose a waveform-based approach. Leveraging a neural network\nto efficiently calculate mismatches between waveforms, we identify regions in\nthe parameter space of spins and mass ratio that result in low mismatches and\nthus similar waveforms. We map these regions with a Gaussian fit, thus\nidentifying correlations between the mass ratio and spins and quantifying their\nstrength. For low-mass, inspiral-dominated systems, we recover the known\nphysical imprint: larger aligned spins are correlated with more equal masses as\nthey have opposite effects on the inspiral length. For high-mass,\nmerger-dominated signals, a qualitatively similar correlation is present,\nthough its shape is altered and strength decreases with increasing total mass.\nCorrelations between in-plane spins and mass ratio follow a similar trend, with\ntheir shape and strength altered as the mass increases. Waveform-based\ncorrelation mapping can motivate effective spin parameters and reveal the\nimprint of spins on signals for which no simple analytic descriptions exist.",
        "We report new quasi-aperiodic, ground-state structures and phase transitions\nin $\\Sigma5$ tilt grain boundaries (GBs) in body-centered cubic (BCC)\nrefractory metals Nb, Ta, Mo, and W. $\\Sigma5$ tilt GBs have been extensively\ninvestigated over the past several decades, with their ground-state structure\n-- composed of kite-shaped structural units -- previously thought to be well\nunderstood. By performing a rigorous GB structure search that optimizes the\nnumber of atoms in the boundary core, we predict new quasi-aperiodic \"split\nkite\" phases analogous to those previously found in GBs in face-centered cubic\nmetals. Our results suggest that complex aperiodic phases of GBs appear to be a\ngeneral phenomenon. Density functional theory calculations yield similar GB\nenergies and structures for kite and split kite phases in all BCC metals\nstudied. Phase-contrast image simulations of split kites show better agreement\nwith experimental observations, offering an alternative explanation for\nprevious microscopy results and motivating future atomically resolved imaging\nof the GB structure.",
        "CaMn$_2$P$_2$ exemplifies the realization of a frustrated $J_1$-$J_2$-$J_3$\nHeisenberg model of a corrugated honeycomb magnetic lattice. Previous studies\nshow that below the N\\'eel temperature ($T_{\\rm N}$), the system forms a\ncycloidal $6\\times 6$ $ab$-plane magnetic unit cell that conforms with various\nmagnetic space groups. Here, we present single-crystal neutron-diffraction\nstudies across expansive reciprocal-space volumes, confirming the cycloidal\nmagnetic structure while uncovering further distinctive features. We find\nevidence for three magnetic domains, the analysis of which narrows the possible\nmagnetic model structures. At $T_{\\rm N}$, the insulator exhibits a sharp phase\ntransition, above which the spin structure transforms into a spiral spin liquid\nstate, evident via a continuous ring of scattering with degenerate wavevectors\ncorresponding to a collection of short-range spiral spin configurations. These\ndegenerate states emerge as thermal fluctuations effectively reduce the $J_3$\ninteraction. The integration of experimental, theoretical, and real-space\nsimulation results reveals the intricate balance of exchange interactions\n($J_1$-$J_2$-$J_3$) that stabilizes the ground-state magnetic structure and\ndrives the emergence of a sought-after $U$(1)-symmetric spiral spin-liquid\nstate with easy-plane anisotropy above the transition temperature.",
        "We study the effects of a parametric frequency converter in a two-mode cavity\nsystem where one of the cavity mode is coupled with yttrium iron garnet (YIG)\nvia magnetic dipole interaction. Parametric frequency converter acts as a\nnonlinear source for enhanced entanglement among all bipartitions and\nasymmetrical quantum steering. The behavior of the two types of quantum\ncorrelations are shown to be dependent on parametric coupling and the\nassociated phase factor. We show that cavity-cavity entanglement and\ncavity-phonon entanglement (cavity-magnon entanglement) decreases (increases)\nwith the increase of the parametric phase factor {\\phi}. In addition, generated\nentanglements in the present system have shown to be more robust against the\nthermal effects, with the inclusion of the parametric converter as compared\nwith the bare cavity case. Another intriguing finding is the asymmetric one-way\nsteering, where we notice that magnon and phonon modes can steer the indirectly\ncoupled cavity modes, yet the steering in swapped direction is not observed. It\nis of great interest that the perfect transfer of entanglement and quantum\nsteering is achieved among different modes by adjusting the system's\nparameters. In fact, our protocol for these transferring processes suggests a\ndifferent approach to the processing and storage of quantum information.",
        "We study quantum correlations between interacting photons realized through\nco-propagating Rydberg polaritons. We show that the evolution of the $n$-photon\nwavefunction is governed by a multiband dispersion featuring one massive mode\nand $n-1$ degenerate modes, such as two Dirac cones for $n=3$. The band\nstructure exhibits an $n$-fold rotational symmetry, including an $n$-fold\nwarped light cone, in contrast to the single-band, parabolic approximation\noften assumed for interacting polaritons. For three photons, the dispersion\nbreaks the symmetry between a photon pair propagating ahead versus behind a\nsingle photon. We confirm these findings experimentally by measuring the\nthree-photon phase and intensity correlation functions, revealing trigonal\nwarping of the quantum vortex-ring generated by the three-photon interactions.\nOur analytical results are supported by rigorous numerical modeling that fully\naccounts for the photon propagation before, inside, and after the finite atomic\nmedium. These findings advance the understanding of multi-photon interactions\nand the development of future multi-photon control tools.",
        "Partial differential equation (PDE) is an important math tool in science and\nengineering. This paper experimentally demonstrates an optical neural PDE\nsolver by leveraging the back-propagation-free on-photonic-chip training of\nphysics-informed neural networks.",
        "We do a numerical study of the transient phenomenon in pipelines transporting\nplastic Bingham slurry flows, using a lowest-order finite element method (FEM).\nWhile most pipeline hammer studies focus on Newtonian fluids, the transient\ndynamics in Bingham fluids remains elusive and poorly afforded, even though\nthis case has significant impact in industry like mining. A detailed parametric\nstudy asses the effects of the slurry yield stress and the valve closure times\non both, pressure and velocity distributions along the pipeline, using an\nadaptive friction model to account for turbulent slurries. Results reveal that\nyield stress enhances flow resistance and accelerates pressure peak\nattenuation, underscoring the damping role of Bingham rheology compared to\nNewtonian flows. These insights emphasize the need for advanced FEM-based\nschemes in non-Newtonian shockwave modeling, with implications for industrial\npipeline design and operational safety.",
        "We consider heavy-heavy-light-light (HHLL) correlators in AdS\/CFT, focussing\non the D1D5 CFT$_2$ and the ${\\cal N}= 4$ super Yang-Mills theory. Out of the\nlightest $1\/2$-BPS operator in the spectrum, $O$, we construct a particular\nheavy operator $O_H$ given by a coherent superposition of multi-particle\noperators $O^n$, and study the HHLL correlator. When $n$ is of order of the\ncentral charge, we show that the bulk equation that computes our boundary HHLL\ncorrelators is always a Heun equation. By assuming that the form of the\ncorrelator can be continued to the regime where $n$ is ${\\mathcal O}(1)$, we\nfirst reproduce the known single-particle four-point correlators for $n=1$ and\nthen predict new results for the multi-particle correlators $\\langle O^n O^n O\nO\\rangle$. Explicit expressions can be written entirely in terms of $n$-loop\nladder integrals and their derivatives, and we provide them for $n=2$ and $n=3$\nboth in position and in Mellin space. Focussing on the AdS$_5$ case, we study\nthe OPE expansion of these multi-particle correlators and show that several\nconsistency relations with known CFT data are non-trivially satisfied. Finally,\nwe extract new CFT data for double and triple-particle long operators.",
        "Supermoir\\'e structures (SMS), formed by overlapping moir\\'e-patterns in van\nder Waals heterostructures, display complex behaviour that lacks a\ncomprehensive low-energy theoretical description. We demonstrate that these\nstructures can form emergent fractals under specific conditions and identify\nthe parameter space where this occurs in hexagonal trilateral SMS. This\nfractality enables a reliable calculation of low-energy band counts, which are\ncrucial for understanding both single-particle and correlation effects. Using\nan effective Hamiltonian that includes in- and out-of-plane lattice relaxation,\nwe analyze SMS in hBN-encapsulated single and bilayer graphene. We prescribe\nmethods to experimentally verify these fractals and extract their fractal\ndimension through angle-resolved photoemission spectroscopy (ARPES) and\nscanning tunneling microscopy (STM).",
        "Altermagnetism, as an unconventional antiferromagnetism, exhibits\ncollinear-compensated magnetic order in real space and spin-splitting band\nstructure in reciprocal space. In this work, we propose a general approach to\ngenerating multi-component structures with two-dimensional altermagnetism,\nbased on symmetry analysis. Specifically, by analyzing the space group of the\ncrystal structures and their subgroups, we systematically categorize equivalent\natomic positions and arrange them into orbits based on symmetry operations.\nChemical elements are then allowed to occupy all atomic positions on these\norbits, generating candidate structures with specific symmetries. We present a\ngeneral technique for generating collinear-compensated magnetic order,\ncharacterized by the symmetrical interconnection between opposite-spin\nsublattices, and employ first-principles calculations to determine magnetic\nground states of multi-component materials. This approach integrates symmetry\nanalysis with the screening of altermagnetic configurations to evaluate the\nlikelihood of candidates possessing altermagnetism. To verify the methodology,\nwe provide examples of previously unreported 2D altermagnets, such as\nCr2Si2S3Se3, Fe2P2S3Se3, and V2O2BrI3, and evaluate their dynamical stability\nby calculating the phonon spectrum. The results demonstrate the feasibility of\nour approach in generating stable multi-component structures with\ntwo-dimensional altermagnetism. Our research has significantly enriched the\ncandidate materials for 2D altermagnet and provided a reference for\nexperimental synthesis.",
        "Since its first observation in the 1970s, neutrino-induced neutral-current\nsingle positive pion production (NC1$\\pi^+$) has remained an elusive and poorly\nunderstood interaction channel. This process is a significant background in\nneutrino oscillation experiments and studying it further is critical for the\nphysics program of next-generation accelerator-based neutrino oscillation\nexperiments. In this Letter we present the first double-differential\ncross-section measurement of NC1$\\pi^+$ interactions using data from the ND280\ndetector of the T2K experiment collected in $\\nu$-beam mode. We compare the\nresults on a hydrocarbon target to the predictions of several neutrino\ninteraction generators and final-state interaction models. While model\npredictions agree with the differential results, the data shows a weak\npreference for a cross-section normalization approximately 30\\% higher than\npredicted by most models studied in this Letter.",
        "Statistical studies of the circumgalactic medium (CGM) using\nSunyaev-Zeldovich (SZ) observations offer a promising method of studying the\ngas properties of galaxies and the astrophysics that govern their evolution.\nForward modeling profiles from theory and simulations allows them to be refined\ndirectly off of data, but there are currently significant differences between\nthe thermal SZ (tSZ) observations of the CGM and the predicted tSZ signal.\nWhile these discrepancies could be inherent, they could also be the result of\ndecisions in the forward modeling used to build statistical measures off of\ntheory. In order to see effects of this, we compare an analysis utilizing halo\noccupancy distributions (HODs) implemented in halo models to simulate the\ngalaxy distribution against a previous studies which weighted their results off\nof the CMASS galaxy sample, which contains nearly one million galaxies, mainly\ncentrals of group sized halos, selected for relatively uniform stellar mass\nacross redshifts between $0.4<z<0.7$. We review some of the implementation\ndifferences that can account for changes, such as miscentering,\none-halo\/two-halo cutoff radii, and mass ranges, all of which will need to be\ngiven the proper attention in future high-signal-to-noise studies. We find that\nour more thorough model predicts a signal $\\sim 25\\%$ stronger than the one\nfrom previous studies on the exact same sample, resulting in a $33\\%$ improved\nfit for non-dust-contaminated angular scales. Additionally, we find that\nmodifications that change the satellite fraction even by just a few percents,\nsuch as editing the halo mass range and certain HOD parameters, result in\nstrong changes in the final signal. Although significant, this discrepancy from\nthe modeling choices is not large enough to completely account for the existing\ndisagreements between simulations and measurements.",
        "A graph $G$ is $k$-vertex-critical if $\\chi(G) = k$ but $\\chi(G-v)<k$ for all\n$v \\in V(G)$. A graph is $(H_1,H_2)$-free if it contains no induced subgraph\nisomorphic to $H_1$ nor $H_2$. A $W_4$ is the graph consisting of a $C_4$ plus\nan additional vertex adjacent to all the vertices of the $C_4$.\n  We show that there are finitely many $k$-vertex-critical $(P_5,W_4)$-free\ngraphs for all $k \\ge 1$ and we characterize all $5$-vertex-critical\n$(P_5,W_4)$-free graphs. Our results imply the existence of a polynomial-time\ncertifying algorithm to decide the $k$-colorability of $(P_5,W_4)$-free graphs\nfor each $k \\ge 1$ where the certificate is either a $k$-coloring or a\n$(k+1)$-vertex-critical induced subgraph.",
        "Multi-modal generative AI systems, such as those combining vision and\nlanguage, rely on contrastive pre-training to learn representations across\ndifferent modalities. While their practical benefits are widely acknowledged, a\nrigorous theoretical understanding of the contrastive pre-training framework\nremains limited. This paper develops a theoretical framework to explain the\nsuccess of contrastive pre-training in downstream tasks, such as zero-shot\nclassification, conditional diffusion models, and vision-language models. We\nintroduce the concept of approximate sufficient statistics, a generalization of\nthe classical sufficient statistics, and show that near-minimizers of the\ncontrastive pre-training loss are approximately sufficient, making them\nadaptable to diverse downstream tasks. We further propose the Joint Generative\nHierarchical Model for the joint distribution of images and text, showing that\ntransformers can efficiently approximate relevant functions within this model\nvia belief propagation. Building on this framework, we derive sample complexity\nguarantees for multi-modal learning based on contrastive pre-trained\nrepresentations. Numerical simulations validate these theoretical findings,\ndemonstrating the strong generalization performance of contrastively\npre-trained transformers in various multi-modal tasks.",
        "In this paper, a finite family of nonsmooth locally Lipschitz continuous\nfunctions that are invex with respect to the same function {\\eta} are\ncharacterized in terms of their scalarized counterparts.",
        "Despite the prominence of tensor mesons in photon-photon collisions, until\nrecently their contribution to the hadronic light-by-light (HLBL) scattering\npart of the anomalous magnetic moment of the muon has been estimated at the\nlevel of only a few $10^{-12}$, with an almost negligible contribution to the\nerror budget of the Standard Model prediction. A recent reanalysis within the\ndispersive approach has found that after resolving the issue of kinematic\nsingularities in previous approaches, a larger result is obtained, a few\n$10^{-11}$, and with opposite sign as in previous results, when a simple quark\nmodel for the transition form factors is employed. In this paper, we present\nthe first complete evaluation of tensor meson contributions within a hard-wall\nmodel in holographic QCD, which reproduces surprisingly well mass, two-photon\nwidth, and the observed singly virtual transition form factors of the dominant\n$f_2(1270)$, requiring only that the energy-momentum tensor correlator is\nmatched to the leading OPE result of QCD. Due to a second structure function\nthat is absent in the quark model, the result for $a_\\mu$ turns out to be\npositive instead of negative, and also with a magnitude of a few $10^{-11}$. We\ndiscuss both pole and non-pole contributions arising from tensor meson\nexchanges in the holographic HLBL amplitude, finding that keeping all\ncontributions improves dramatically the convergence of a sum over excited\ntensor mesons and avoids unnaturally large contributions from the first few\nexcited modes at low energies. Moreover, we find that the infinite tower of\ntensor mesons permits to fill the gap in the symmetric longitudinal\nshort-distance constraint on the HLBL amplitude left by the contribution of\naxial vector mesons. Total $a_\\mu^\\mathrm{Tensor}$ contribution: $+12.4\\times\n10^{-11}$; with an $F_\\rho$ fit this is reduced slightly to $+11.1\\times\n10^{-11}$.",
        "Motivated by the stochastic differential game problem, in this paper we\nintroduce and study a more general class of anticipated backward stochastic\nVolterra integral equations (anticipated BSVIEs, for short) whose generator\nincludes both pointwise time-advanced functions and average time-advanced\nfunctions. The well-posedness and the comparison theorem of anticipated BSVIEs\nare established, and some regularity results of adapted M-solutions are proved\nby applying Malliavin calculus, which cover the previous results for BSVIEs.\nMoreover, by virtue of the duality principle, we present the maximum principle\nfor the nonzero-sum differential game system of stochastic delay Volterra\nintegral equations (SDVIEs, for short) for the first time. As one of the\napplications of the theorem, a Nash equilibrium point of the linear-quadratic\ndifferential game problem of SDVIEs is obtained. It should be pointed out that\noptimal controls of stochastic Volterra integral equations can be regarded as a\nspecial case of our game problem.",
        "Mapping genotypes to phenotypes (G2P) is a fundamental goal in biology. So\ncalled PhyloG2P methods are a relatively new set of tools that leverage\nreplicated evolution in phylogenetically independent lineages to identify\ngenomic regions associated with traits of interest. Here, we review recent\ndevelopments in PhyloG2P methods, focusing on three key areas: methods based on\nreplicated amino acid substitutions, methods detecting changes in evolutionary\nrates, and methods analysing gene duplication and loss. We discuss how the\ndefinition and measurement of traits impacts the utility of these methods,\narguing that focusing on simple rather than compound traits will lead to more\nmeaningful genotype-phenotype associations. We advocate for the use of methods\nthat work with continuous traits directly rather than collapsing them to binary\nrepresentations. We examine the strengths and limitations of different\napproaches to modeling genetic replication, highlighting the importance of\nexplicit modeling of evolutionary processes. Finally, we outline promising\nfuture directions, including the integration of population-level variation, as\nwell as epigenetic and environmental information. No one method is likely to\nidentify all genomic regions of interest, so we encourage users to apply\nmultiple methods that are capable of detecting a wide range of associations.\nThe overall aim of this review is to provide practitioners a roadmap for\nunderstanding and applying PhyloG2P methods.",
        "Poly(ethylene terephthalate) (PET) films are widely used in flexible\nelectronics and optoelectronics, where their mechanical durability and optical\nperformance under strain are essential for device reliability. This study\ninvestigates the impact of applied mechanical strain on the optical and\nmolecular properties of PET at room temperature,using UV-Vis absorption and\nRaman spectroscopy. The work explores how varying strain levels, from 0%\n(unstretched) to 30%, affect the transparency, vibrational modes, and molecular\nreorganization within PET films. UV-Vis absorbance measurements reveal that\nstrain induces significant changes in the light transmission properties of PET,\nparticularly in the visible range, and increases absorption in the UVA and\nvisible region by up to 100%. Raman spectra indicate that strain levels higher\nthan 5% lead to irreversible shifts of vibrational lines, accompanied by an\nincrease of their full width at half maximum (FWHM), suggesting molecular\nreorientation and crystallinity changes. The phonon mode coupled with C-O\nstretching [O-CH2] shows the strongest response to applied mechanical stress.\nThis study provides a comprehensive understanding of strain-induced optical and\nstructural alterations in PET, with implications for improving the mechanical\nand optical performance of PET-based devices in strainsensitive applications,\nsuch as organic solar cells (OSCs), organic light-emitting diodes (OLEDs), and\nflexible sensors.",
        "In real-world text classification tasks, negative texts often contain a\nminimal proportion of negative content, which is especially problematic in\nareas like text quality control, legal risk screening, and sensitive\ninformation interception. This challenge manifests at two levels: at the macro\nlevel, distinguishing negative texts is difficult due to the high similarity\nbetween coarse-grained positive and negative samples; at the micro level, the\nissue stems from extreme class imbalance and a lack of fine-grained labels. To\naddress these challenges, we propose transforming the coarse-grained\npositive-negative (PN) classification task into an imbalanced fine-grained\npositive-unlabeled (PU) classification problem, supported by theoretical\nanalysis. We introduce a novel framework, Balanced Fine-Grained\nPositive-Unlabeled (BFGPU) learning, which features a unique PU learning loss\nfunction that optimizes macro-level performance amidst severe imbalance at the\nmicro level. The framework's performance is further boosted by rebalanced\npseudo-labeling and threshold adjustment. Extensive experiments on both public\nand real-world datasets demonstrate the effectiveness of BFGPU, which\noutperforms other methods, even in extreme scenarios where both macro and micro\nlevels are highly imbalanced.",
        "Spin-orbit coupling in systems with broken inversion symmetry gives rise to\nthe Edelstein effect, which is the induced spin polarization in response to an\napplied electric field or current, and the inverse Edelstein effect, which is\nthe induced electric current in response to an oscillatory magnetic field or\nspin polarization. At the same time, an interplay between spin-orbit coupling\nand electron-electron interaction leads to a special type of collective\nexcitations -- chiral-spin modes -- which are oscillations of spin polarization\nin the absence of a magnetic field. As a result, both Edelstein and inverse\nEdelstein effects exhibit resonances at the frequencies of spin-chiral\ncollective modes. Here, we present a detailed study of the effect of electron\ncorrelation on the Edelstein and inverse Edelstein effects in a single-valley\ntwo-dimensional electron gas and a multi-valley Dirac system with\nproximity-induced spin-orbit coupling. While the chiral-spin modes involve both\nin-plane and out-of-plane oscillations of spins, we show that only the in-plane\nmodes are responsible for the above resonances. In the multi-valley system,\nelectron correlation splits the in-plane modes into two. We also study the\nspectral weight distribution between the two resonances over a large parameter\nspace of intra- and inter-valley interactions.",
        "Black holes offer a unique laboratory for fundamental physics and are crucial\nfor probing theories beyond Einstein's theory of General Relativity. In this\npaper, we consider 4D effective field theories with scalar fields. We focus on\naxi-dilaton gravity, a quadratic gravity theory with two kinetically coupled\nscalar fields, an axion and a dilaton. To evolve these fields around black\nholes, we introduce Canuda-AxiDil, the first open-source, parameterized\nnumerical relativity code for quadratic and bi-scalar gravity. Using this code,\nwe perform single black hole simulations to show the dynamical formation of\naxion and dilaton hairs. Through these simulations, we measure the impact of\nblack-hole spin and curvature coupling strength on the axion and dilaton, and\nshow that a kinetic coupling between the fields increases the observed\ndeviations from General Relativity. Furthermore, we simulate the axion and\ndilaton fields around a binary black hole coalescence demonstrating the growth\nof axion hair during the inspiral and the production of radiative modes for\nboth fields.",
        "We theoretically propose a tunable implementation of symmetry-protected\ntopological phases in a synthetic superlattice, taking advantage of the long\ncoherence time and exquisite spectral resolutions offered by gravity-tilted\noptical lattice clocks. We describe a protocol similar to Rabi spectroscopy\nthat can be used to probe the distinct topological properties of our system. We\nthen demonstrate how the sensitivity of clocks and interferometers can be\nimproved by the topological robustness to unwanted experimental imperfections.\nThe proposed implementation opens a path to exploit the unique opportunities\noffered by symmetry-protected topological phases in state-of-the-art quantum\nsensors.",
        "Darwinian spreading of vigor, in addition to quality distribution, is\nintroduced in a tree growth model. The size of any tree, within an even-aged\nstand, is taken as a measure of an inherited productive capacity, and then\ncombined with quality thinning. For sparse cultivation density, the result is\nforestry without commercial thinnings; large trees cannot be removed since they\nare the most productive, neither small trees because the unit price of\nharvesting would be large. For large cultivation density, thinning from above\nbecomes combined with quality thinning of large trees. Darwinian spreading of\ngrowth rate may correlate with quality. Quality thinning may enhance growth\nrate. Such effects combined, large trees still become removed but quality\nthinning becomes implemented in almost all diameter classes. At financial\nmaturity, Darwinian spreading treated with quality thinning, the weighted mean\nbreast height diameter is in the vicinity of 20 cm within stands of high\nplanting density, and somewhat higher with low planting density. Quality\ncorrelating with Darwinian spreading of growth, and growth rate correlating\nwith observed quality, trees grow bigger, the maturity size becoming closer to\n25 cm - bigger than in earlier studies without tree-size - dependency of vigor.",
        "We study the long-time behavior of small and large solutions to a broad class\nof nonlinear Dirac-type equations. Our results are classified in 1D massless\nand massive cases, 3D general and $n$ dimensional in generality. In the 1D\nmassless case we prove that any globally defined solution converges to zero as\ntime tends to infinity, within a spatial region expanding at a rate\nproportional to $ t \\log^{-2} t$. This result holds without assumptions on the\nsmallness of initial data or specific power of nonlinearity, ruling out the\nexistence of standing breather-like or solitary wave structures in this regime.\nIn the 1D massive case, solitary waves are known to exist. Introducing new\nvirial identities adapted to the Dirac's distinctive algebra, we prove that\nthere are ``holomorphic'' odd nonlinearities under which globally defined small\nodd solutions decay to zero on spatial compact sets as time tends to infinity.\nThis result is extended to the 3D case under boundedness of the $H^1$ norm but\nwithout requiring the parity condition on the data, giving decay proofs for an\nimportant class of nonlinear Dirac models, and opening the door to the future\nuse of virial identities to prove asymptotic stability of well-chosen Dirac\nsolitary waves.\n  Finally, in higher dimensions $ n \\geq 1$, we prove the $L^2$ decay for\nglobal solutions of nonlinear Dirac equations in the ``exterior light-cone''\nregion. This confirms the non-existence of breathers and other solutions\npropagating faster than the speed of light. Our proofs rely on carefully\nconstructed weighted virial identities.",
        "We characterize projective objects in the category of internal crossed\nmodules within any semi-abelian category. When this category forms a variety of\nalgebras, the internal crossed modules again constitute a semi-abelian variety,\nensuring the existence of free objects, and thus of enough projectives. We show\nthat such a variety is not necessarily Schreier, but satisfies the so-called\nCondition (P) -- meaning the class of projectives is closed under protosplit\nsubobjects -- if and only if the base variety satisfies this condition. As a\nconsequence, the non-additive left chain-derived functors of the connected\ncomponents functor are well defined in this context.",
        "We present quantum phase diagrams for the antiferromagnetic long-range Ising\nmodel with a linear coupling to a single bosonic mode on the square and\ntriangular lattice. For zero coupling, the ground-state magnetization forms a\ndevil's staircase structure of magnetization plateaux as a function of a\nlongitudinal field. Apart from a paramagnetic superradiant phase with a finite\nphoton density at strong light-matter couplings, the long-range interactions\nlead to a plethora of intermediate phases that break the translational symmetry\nand have a finite photon density at the same time. We apply an adaption of the\nunit-cell-based mean-field calculations, which captures all possible magnetic\nunit cells up to a chosen extent. Further, we exploit an exact mapping of the\nnon-superradiant phases to an effective Dicke model to calculate upper bounds\nfor phase transitions towards superradiant phases. Finally, to treat quantum\nfluctuations in a quantitative fashion, we employ a generalized wormhole\nquantum Monte Carlo algorithm. We discuss how these three methods are used in a\ncooperative fashion. In the calculated phase diagrams we see several features\narising from the long-range interactions: The devil's staircases of distinct\nmagnetically ordered normal phases and non-trivial magnetically ordered\nsuperradiant phases beyond the findings for nearest-neighbor interactions.\nExamples are a superradiant phase with a three-sublattice magnetic order on the\nsquare lattice and the superradiant Wigner crystal with four sites per unit\ncell on the triangular lattice. We find the transition between normal and\nsuperradiant phases with the same (different) magnetic order to be of second\norder with Dicke universality (first order). Further, between superradiant\nphases we find first-order phase transitions, besides specially highlighted\nregimes for which we find indications for second-order behavior."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"RNA-Seq: a revolutionary tool for transcriptomics",
    "start_abstract":"RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "White-Box Transformers via Sparse Rate Reduction"
      ],
      "abstract":[
        "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Condensation of lighter-than-physical pions in QCD",
        "The Cygnus Allscale Survey of Chemistry and Dynamical Environments:\n  CASCADE. IV. Unveiling the hidden structures in DR18",
        "Gravitational Vacuum Condensate Stars in the Effective Theory of Gravity",
        "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space",
        "Fast-RF-Shimming: Accelerate RF Shimming in 7T MRI using Deep Learning",
        "Polynomial algebra from the Lie algebra reduction chain\n  $\\mathfrak{su}(4) \\supset \\mathfrak{su}(2) \\times \\mathfrak{su}(2)$: The\n  supermultiplet model",
        "Asteroseismology of the red giant companions to Gaia BH2 and BH3",
        "Unification of stochastic matrices and quantum operations for N-level\n  systems",
        "Asymptotics for EBLUPs within crossed mixed effect models",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Transport Characteristics and Modelling of ST40 Hot Ion Plasmas",
        "Robust Body Composition Analysis by Generating 3D CT Volumes from\n  Limited 2D Slices",
        "A Doubly-Dispersive MIMO Channel Model Parametrized with Stacked\n  Intelligent Metasurfaces",
        "GraphRAG under Fire",
        "3D\/2D Registration of Angiograms using Silhouette-based Differentiable\n  Rendering",
        "The Co-Moving Velocity and Projective Transformations",
        "Low-Rank Compression for IMC Arrays",
        "Solving nonograms using Neural Networks",
        "Data-augmented Learning of Geodesic Distances in Irregular Domains\n  through Soner Boundary Conditions",
        "Poisson representations for tree-indexed Markov chains",
        "LatteReview: A Multi-Agent Framework for Systematic Review Automation\n  Using Large Language Models",
        "3D-grids are not transducible from planar graphs",
        "Discovery of Staircase delta Scuti Variables",
        "Search Trees on Trees via LP",
        "Contrastive Learning for Cold Start Recommendation with Adaptive Feature\n  Fusion",
        "Metis Observations of Alfv\\'{e}nic Outflows Driven by Interchange\n  Reconnection in a Pseudostreamer",
        "Comment on \"Generic machine learning inference on heterogeneous\n  treatment effects in randomized experiments.\"",
        "On counting numerical semigroups by maximum primitive and Wilf's\n  conjecture",
        "Asymptotic regularity of graded family of ideals"
      ],
      "abstract":[
        "We report on the results of the 2+1 flavour QCD simulations at nonzero\nisospin chemical potential performed at half the physical light quark mass. At\nlow temperatures and large isospin chemical potential Bose-Einstein\nCondensation (BEC) occurs, creating a pion condensed phase, separated from the\nhadronic and quark-gluon plasma phases by the BEC transition line. For physical\nquark masses, the section of this line between the hadronic and BEC phases was\nfound to be almost perfectly vertical, i.e. aligned with the temperature axis.\nWe show that for lighter than physical pions, this section remains vertical,\nand approaches the axis of vanishing chemical potential linearly with the pion\nmass, giving a prediction of the phase diagram in the chiral limit.",
        "The Cygnus-X complex is a massive, nearby (1.4 kpc) star-forming region with\nseveral OB associations. As part of the Cygnus Allscale Survey of Chemistry and\nDynamical Environments (CASCADE) program, we carried out 3.6 millimeter (mm)\ncontinuum and spectral line high-resolution observations ($\\sim$ 3 - 4$''$)\ntoward DR18, covering several molecular species with the Northern Extended\nMillimeter Array (NOEMA) and the Institut de Radioastronomie Millim\\'etrique\n(IRAM) 30m telescope. In addition, multi-wavelength archival datasets were used\nto provide a comprehensive analysis of the region. A comparison of the 3.6mm\nand 6 cm continuum emission confirms that a B2 star (DR18-05) shapes the\ncometary HII region in the DR18 cavity, with ionized gas escaping toward the\nOB2 association. On the other hand, the extended 3.6mm and 6 cm continuum\nemission are likely to trace photoevaporating ionized gas from ultraviolet\nradiation from the Cyg OB2 association, not from DR18-05. The shell structure\naround DR18-05 indicates photodissociation regions (PDRs) formed by the\nexpanding HII region and photo-erosion from DR18-05 and OB2 stars. We also\nidentified 18 compact cores with N$_2$H$^+$ emission, half of which are\ngravitationally bound and mostly located in colder regions behind the PDRs. The\nSiO emission is found only in PDRs, with narrow-line widths ( 0.8 - 2.0 km\ns$^{-1}$) and lower abundances (X(SiO) $\\sim$ 5$\\times$10$^{-11}$ -\n1$\\times$10$^{-10}$). Comparing with the UV irradiated shock models, we suggest\nthat the SiO emission partially encompassing the HII region arises from the\nmolecular gas region, marginally compressed by low-velocity shocks with $\\sim$\n5 km s$^{-1}$, irradiated by external UV radiation (G$_{\\rm 0} \\sim 10^{2} -\n10^{3}$), as they traverse through a medium with $n_{\\rm H} \\sim 10^{4}$ to\n10$^5$ cm$^{-3}$.",
        "The low energy effective theory of gravity comprises two elements of quantum\ntheory joined to classical general relativity. The first is the quantum\nconformal anomaly, which is responsible for macroscopic correlations on light\ncones and a stress tensor that can strongly modify the classical geometry at\nblack hole horizons. The second is the formulation of vacuum energy as\n$\\Lambda_{\\rm eff}\\!\\propto\\! F^2$ in terms of an exact $4$-form abelian gauge\nfield strength $F\\!=\\!dA$. When $A$ is identified with the Chern-Simons\n$3$-form of the Euler class, defined in terms of the spin connection, a $J\\cdot\nA$ interaction is generated by the conformal anomaly of massless fermions. Due\nto the extreme blueshifting of local frequencies in the near-horizon region of\na `black hole,' the lightest fermions of the Standard Model can be treated as\nmassless there, contributing to the anomaly and providing a $3$-current source\n$J$ for the `Maxwell' equation $d\\ast F = \\ast J$. In this phase boundary\nregion, torsion is activated, and $F$ can change rapidly. The Schwarzschild\nblack hole horizon is thereby replaced by a surface, with a positive surface\ntension and $\\mathbb{R}\\otimes \\mathbb{S}^2$ worldtube topology, separating\nregions of differing vacuum energy. The result is a gravitational vacuum\ncondensate star, a cold, compact, horizonless object with a $p_{_V}\\!=\\! -\n\\rho_{_V}$ zero entropy, non-singular de Sitter interior and thin quantum phase\nboundary layer at the Schwarzschild radius $2GM\/c^2$.",
        "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the Euclid photometric survey will allow for precise\nstudies of galaxy clustering from a single survey, over a large range of\nredshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts to\nextract the baryon acoustic oscillation signal (BAO) from the Flagship galaxy\nmock catalogue with a tomographic approach to constrain the evolution of the\nUniverse and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}\/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference.",
        "Ultrahigh field (UHF) Magnetic Resonance Imaging (MRI) provides a high\nsignal-to-noise ratio (SNR), enabling exceptional spatial resolution for\nclinical diagnostics and research. However, higher fields introduce challenges\nsuch as transmit radiofrequency (RF) field inhomogeneities, which result in\nuneven flip angles and image intensity artifacts. These artifacts degrade image\nquality and limit clinical adoption. Traditional RF shimming methods, including\nMagnitude Least Squares (MLS) optimization, mitigate RF field inhomogeneity but\nare time-intensive and often require the presence of the patient. Recent\nmachine learning methods, such as RF Shim Prediction by Iteratively Projected\nRidge Regression and other deep learning architectures, offer alternative\napproaches but face challenges such as extensive training requirements, limited\ncomplexity, and practical data constraints. This paper introduces a holistic\nlearning-based framework called Fast RF Shimming, which achieves a 5000-fold\nspeedup compared to MLS methods. First, random-initialized Adaptive Moment\nEstimation (Adam) derives reference shimming weights from multichannel RF\nfields. Next, a Residual Network (ResNet) maps RF fields to shimming outputs\nwhile incorporating a confidence parameter into the loss function. Finally, a\nNon-uniformity Field Detector (NFD) identifies extreme non-uniform outcomes.\nComparative evaluations demonstrate significant improvements in both speed and\npredictive accuracy. The proposed pipeline also supports potential extensions,\nsuch as the integration of anatomical priors or multi-echo data, to enhance the\nrobustness of RF field correction. This approach offers a faster and more\nefficient solution to RF shimming challenges in UHF MRI.",
        "The supermultiplet model, based on the reduction chain $\\mathfrak{su}(4)\n\\supset \\mathfrak{su}(2) \\times \\mathfrak{su}(2)$, is revisited through the\nlens of commutants within universal enveloping algebras of Lie algebras. From\nthis analysis, a collection of twenty polynomials up to degree nine emerges\nfrom the commutant associated with the $\\mathfrak{su}(2) \\times\n\\mathfrak{su}(2)$ subalgebra. This study is conducted in the Poisson\n(commutative) framework using the Lie-Poisson bracket associated with the dual\nof the Lie algebra under consideration. As the main result, we obtain the\npolynomial Poisson algebra generated by these twenty linearly independent and\nindecomposable polynomials, with five elements being central. This incorporates\npolynomial expansions up to degree seventeen in the Lie algebra generators. We\nfurther discuss additional algebraic relations among these polynomials,\nexplicitly detailing some of the lower-order ones. As a byproduct of these\nresults, we also show that the recently introduced 'grading method' turns out\nto be essential for deriving the Poisson bracket relations when the degree of\nthe expansions becomes so high that standard approaches are no longer\napplicable, due to computational limitations. These findings represent a\nfurther step toward the systematic exploration of polynomial algebras relevant\nto nuclear models.",
        "The stellar companions in the binary black hole systems Gaia BH2 and BH3,\nboth of which are $\\alpha$-enhanced red giant branch stars, are expected to\nshow normal modes with the characteristic signature of convectively-driven\nsolar-like oscillations. We investigate this using photometry from the TESS\nmission and find such a signal for Gaia BH2. For Gaia BH2, we measure a power\nexcess frequency of $\\nu_{\\rm max}=60.15\\pm0.57$ $\\mu$Hz and a large separation\nof $\\Delta\\nu=5.99\\pm0.03$ $\\mu$Hz, yielding a mass of $1.19^{+0.08}_{-0.08}$\nM$_\\odot$, which is in agreement with spectroscopically derived parameters.\nSeismic modeling favors an age for the red giant of $5.03^{+2.58}_{-3.05}$ Gyr,\nstrongly suggesting that it is a young, $\\alpha$-enriched giant star, which are\nthought to arise from a binary accretion or merger scenario. Ground-based\nphotometry of Gaia BH2 spanning 8 years indicates a photometric period of\n$398\\pm5$ d, which we tentatively attribute to rotation. If this rotation is\nphysical, it can not be explained solely by evolutionary spin-down or magnetic\nbraking, and implies that the red giant underwent some tidal forcing mechanism.\nSuggestively, this period is close to the pseudo-synchronous spin period of\nP$_\\text{spin}=428\\pm1$ days derived from the binary orbit. For Gaia BH3, we\nare unable to identify an asteroseismic signal in the TESS data despite\npredicting that the amplitude of the signal should lie well above the measured\nnoise level. We discuss a number of scenarios for why this signal may not be\nvisible.",
        "The time evolution of the one-point probability distribution of stochastic\nprocesses and quantum processes for $N$-level systems has been unified. Hence,\nquantum states and quantum operations can be regarded as generalizations of the\none-point probability vectors and stochastic matrices, respectively. It has\nalso been proven that completely positive divisibility (CP-divisibility) for\nquantum operations is the natural extension of the Chapman-Kolmogorov equation.\nIt is thus shown that CP-divisibility is a necessary but insufficient condition\nfor a quantum process to be specified as Markovian. The main results have been\nillustrated through a dichotomic Markov process.",
        "In this article, we derive the joint asymptotic distribution of empirical\nbest linear unbiased predictors (EBLUPs) for individual and cell-level random\neffects in a crossed mixed effect model. Under mild conditions (which include\nmoment conditions instead of normality for the random effects and model\nerrors), we demonstrate that as the sizes of rows, columns, and, when we\ninclude interactions, cells simultaneously increase to infinity, the\ndistribution of the differences between the EBLUPs and the random effects\nsatisfy central limit theorems. These central limit theorems mean the EBLUPs\nasymptotically follow the convolution of the true random effect distribution\nand a normal distribution. Moreover, our results enable simple asymptotic\napproximations and estimators for the mean squared error (MSE) of the EBLUPs,\nwhich in turn facilitates the construction of asymptotic prediction intervals\nfor the unobserved random effects. We show in simulations that our simple\nestimator of the MSE of the EBLUPs works very well in finite samples. Finally,\nwe illustrate the use of the asymptotic prediction intervals with an analysis\nof movie rating data.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "In this paper, the turbulent transport properties of ST40 hot ion plasmas are\nexamined and fully predictive time evolving modelling of a hot ion plasma pulse\nwas performed. Understanding turbulent transport on spherical tokamaks (STs) is\nchallenging due to their unique geometry characteristics. ST40 hot ion plasmas\nare typically unstable to ion scale Trapped Electron Modes (TEMs) and\nUbiquitous Modes (UMs), driven from the kinetic response of trapped particles\nand passing ions, and electron scale Electron Temperature Gradient Modes (ETGs)\nat the edge of the plasma. A comparison between the linear unstable modes of\nthe gyro-kinetic code GS2 and the gyro-fluid code TGLF showed that both models\nagree to a satisfactory level. However, some discrepancy was observed at the\ncore of the plasma where a large fraction of beams ions exists, and\nelectromagnetic effects are potentially important. Turbulent fluxes were also\nobserved to be somewhat overpredicted with TGLF. The core heat ion transport is\nobserved to be close to neoclassical levels due to turbulence suppression from\nhigh rotation and fast ion stabilisation, while the edge region is dominated by\nanomalous transport in both ions and electrons. As a result, enhanced energy\nconfinement is observed in those plasmas driven by the reduced turbulent core\nregion and the confined beam ions. Fully predictive simulations using the ASTRA\ntransport solver coupled with SPIDER, NUBEAM, NCLASS and TGLF together with a\nnovel reduced scrape of layer (SOL) model for the simulation of the last closed\nflux surface (LCFS) boundary conditions was attempted. Agreement in global\nquantities but also kinetic profiles between the predictive and interpretative\nmodelling as well as experimental measurements was observed.",
        "Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%.",
        "Introduced with the advent of statistical wireless channel models for high\nmobility communications and having a profound role in communication-centric\n(CC) integrated sensing and communications (ISAC), the doubly-dispersive (DD)\nchannel structure has long been heralded as a useful tool enabling the capture\nof the most important fading effects undergone by an arbitrary time-domain\ntransmit signal propagating through some medium. However, the incorporation of\nthis model into multiple-input multiple-output (MIMO) system setups, relying on\nthe recent paradigm-shifting transceiver architecture based on stacked\nintelligent metasurfaces (SIM), in an environment with reconfigurable\nintelligent surfaces (RISs) remains an open problem due to the many intricate\ndetails that have to be accounted for. In this paper, we fill this gap by\nintroducing a novel DD MIMO channel model that incorporates an arbitrary number\nof RISs in the ambient, as well as SIMs equipping both the transmitter and\nreceiver. We then discuss how the proposed metasurfaces-parametrized DD (MPDD)\nchannel model can be seamlessly applied to waveforms that are known to perform\nwell in DD environments, namely, orthogonal frequency division multiplexing\n(OFDM), orthogonal time frequency space (OTFS), and affine frequency division\nmultiplexing (AFDM), with each having their own inherent advantages and\ndisadvantages. An illustrative application of the programmable functionality of\nthe proposed model is finally presented to showcase its potential for boosting\nthe performance of the aforementioned waveforms. Our numerical results indicate\nthat the design of waveforms suitable to mitigating the effects of DD channels\nis significantly impacted by the emerging SIM technology.",
        "GraphRAG advances retrieval-augmented generation (RAG) by structuring\nexternal knowledge as multi-scale knowledge graphs, enabling language models to\nintegrate both broad context and granular details in their reasoning. While\nGraphRAG has demonstrated success across domains, its security implications\nremain largely unexplored. To bridge this gap, this work examines GraphRAG's\nvulnerability to poisoning attacks, uncovering an intriguing security paradox:\ncompared to conventional RAG, GraphRAG's graph-based indexing and retrieval\nenhance resilience against simple poisoning attacks; meanwhile, the same\nfeatures also create new attack surfaces. We present GRAGPoison, a novel attack\nthat exploits shared relations in the knowledge graph to craft poisoning text\ncapable of compromising multiple queries simultaneously. GRAGPoison employs\nthree key strategies: i) relation injection to introduce false knowledge, ii)\nrelation enhancement to amplify poisoning influence, and iii) narrative\ngeneration to embed malicious content within coherent text. Empirical\nevaluation across diverse datasets and models shows that GRAGPoison\nsubstantially outperforms existing attacks in terms of effectiveness (up to 98%\nsuccess rate) and scalability (using less than 68% poisoning text). We also\nexplore potential defensive measures and their limitations, identifying\npromising directions for future research.",
        "We present a method for 3D\/2D registration of Digital Subtraction Angiography\n(DSA) images to provide valuable insight into brain hemodynamics and\nangioarchitecture. Our approach formulates the registration as a pose\nestimation problem, leveraging both anteroposterior and lateral DSA views and\nemploying differentiable rendering. Preliminary experiments on real and\nsynthetic datasets demonstrate the effectiveness of our method, with both\nqualitative and quantitative evaluations highlighting its potential for\nclinical applications. The code is available at\nhttps:\/\/github.com\/taewoonglee17\/TwoViewsDSAReg.",
        "In a string of recent papers starting with (Transport in Porous Media, 125,\n  565 (2018)), a theory of immiscible two-phase flow in porous media based on\n  Euler homogeneity of the total volumetric flow rate has been investigated.\nThe\n  thermodynamic-like theory has an associated statistical mechanics based on a\n  maximum entropy principle. A quantity called the co-moving velocity connects\n  the equations of state of the intensive thermodynamic velocities and the\n  physical seepage velocities of two flowing fluids. The obtained relations\nhave\n  a structure that can be interpreted using affine- and projective geometry.\nThe\n  co-moving velocity can be expressed as a transformation of the saturation\n  using projective duality of points and lines. One obtains an exact\n  constitutive relation depending on a projective invariant, the cross-ratio,\n  which allows the co-moving velocity to be expressed in terms of a simple\n  steady-state advection equation. A kinematic view of the\n  velocity relations is presented, modeled by a well-known non-trivial geometry\nwhich turns\n  out to be pseudo-Euclidean. The cross-ratio determines a hyperbolic angle in\n  this space, and can be parametrized in terms of three numbers using a linear\n  fractional transformation. Knowing these parameters, the pore velocity and\n  the derivative of the pore velocity with respect to the saturation, an\n  approximation for the co-moving velocity can be obtained for a range of\napplied\n  pressures, viscosity ratios and surface tensions. The parametrization is\n  demonstrated using data from a dynamic pore network model and\n  relative permeability data from the literature. This paper only considers the\n  pore areas as extensive variables, however, the geometric principles are\n  general, and the same idea could potentially be used in other systems.",
        "In this study, we address the challenge of low-rank model compression in the\ncontext of in-memory computing (IMC) architectures. Traditional pruning\napproaches, while effective in model size reduction, necessitate additional\nperipheral circuitry to manage complex dataflows and mitigate dislocation\nissues, leading to increased area and energy overheads. To circumvent these\ndrawbacks, we propose leveraging low-rank compression techniques, which, unlike\npruning, streamline the dataflow and seamlessly integrate with IMC\narchitectures. However, low-rank compression presents its own set of\nchallenges, namely i) suboptimal IMC array utilization and ii) compromised\naccuracy. To address these issues, we introduce a novel approach i) employing\nshift and duplicate kernel (SDK) mapping technique, which exploits idle IMC\ncolumns for parallel processing, and ii) group low-rank convolution, which\nmitigates the information imbalance in the decomposed matrices. Our\nexperimental results demonstrate that our proposed method achieves up to 2.5x\nspeedup or +20.9% accuracy boost over existing pruning techniques.",
        "Nonograms are logic puzzles in which cells in a grid must be colored or left\nblank according to the numbers that are located in its headers. In this study,\nwe analyze different techniques to solve this type of logical problem using an\nHeuristic Algorithm, Genetic Algorithm, and Heuristic Algorithm with Neural\nNetwork. Furthermore, we generate a public dataset to train the neural\nnetworks. We published this dataset and the code of the algorithms. Combination\nof the heuristic algorithm with a neural network obtained the best results.\nFrom state of the art review, no previous works used neural network to solve\nnonograms, nor combined a network with other algorithms to accelerate the\nresolution process.",
        "Geodesic distances play a fundamental role in robotics, as they efficiently\nencode global geometric information of the domain. Recent methods use neural\nnetworks to approximate geodesic distances by solving the Eikonal equation\nthrough physics-informed approaches. While effective, these approaches often\nsuffer from unstable convergence during training in complex environments. We\npropose a framework to learn geodesic distances in irregular domains by using\nthe Soner boundary condition, and systematically evaluate the impact of data\nlosses on training stability and solution accuracy. Our experiments demonstrate\nthat incorporating data losses significantly improves convergence robustness,\nreducing training instabilities and sensitivity to initialization. These\nfindings suggest that hybrid data-physics approaches can effectively enhance\nthe reliability of learning-based geodesic distance solvers with sparse data.",
        "In~\\cite{fgs}, the class of Poisson representable processes was introduced.\nSeveral well-known processes were shown not to belong to this class, with\nexamples including both the Curie Weiss model and the Ising model on $\n\\mathbb{Z}^2 $ for certain choices of parameters. Curiously, it was also shown\nthat all positively associated $ \\{ 0,1 \\}$-valued Markov chains do belong to\nthis class. In this paper, we interpolate between Markov chains and Ising\nmodels by considering tree-indexed Markov chains. In particular, we show that\nfor any finite tree that is not a path, whether or not the corresponding\ntree-indexed Markov chain is representable always depends on the parameters.\nMoreover, we give an example of a family of infinite trees such that the\ncorresponding tree-indexed Markov chains are representable for some non-trivial\nparameters. In addition, we give alternative proofs and arguments and also\nstrengthen several of the results in~\\cite{fgs}.",
        "Systematic literature reviews and meta-analyses are essential for\nsynthesizing research insights, but they remain time-intensive and\nlabor-intensive due to the iterative processes of screening, evaluation, and\ndata extraction. This paper introduces and evaluates LatteReview, a\nPython-based framework that leverages large language models (LLMs) and\nmulti-agent systems to automate key elements of the systematic review process.\nDesigned to streamline workflows while maintaining rigor, LatteReview utilizes\nmodular agents for tasks such as title and abstract screening, relevance\nscoring, and structured data extraction. These agents operate within\norchestrated workflows, supporting sequential and parallel review rounds,\ndynamic decision-making, and iterative refinement based on user feedback.\nLatteReview's architecture integrates LLM providers, enabling compatibility\nwith both cloud-based and locally hosted models. The framework supports\nfeatures such as Retrieval-Augmented Generation (RAG) for incorporating\nexternal context, multimodal reviews, Pydantic-based validation for structured\ninputs and outputs, and asynchronous programming for handling large-scale\ndatasets. The framework is available on the GitHub repository, with detailed\ndocumentation and an installable package.",
        "We prove that the class of 3D-grids is cannot be transduced from planar\ngraphs, and more generally, from any class of graphs of bounded Euler genus. To\nprove our result, we introduce a new structural tool called slice\ndecompositions, and show that every graph class transducible from a class of\ngraphs of bounded Euler genus is a perturbation of a graph class that admits\nslice decompositions.",
        "Analysis of the previously classified delta Scuti variable star MW\nCamelopardalis using data from the Transiting Exoplanet Survey Telescope\nsparked a deeper inquiry due to the unexpected patterns within the target's\nobserved-calculated graph. From the shape of the O-C diagram we have designed\nthese objects as Staircase delta Scuti. The pattern was found to be replicated\nin the O-C graphs of seven additional targets. The objects are TIC 17931346,\nTIC 44845403, TIC 123580083, TIC 173503902, TIC 302394816, TIC 194944219, and\nTIC 396465600. The Q value for the targets, their position in the delta Scuti\nLeavitt Law, and location in the instability strip would show these objects to\nbe low mass, fundamental pulsators, near the red edge of the instability strip.\nWe also discuss the impact this phenomenon could have on the analysis of all\npulsating variable stars.",
        "We consider the problem of computing optimal search trees on trees (STTs).\nSTTs generalize binary search trees (BSTs) in which we search nodes in a path\n(linear order) to search trees that facilitate search over general tree\ntopologies. Golinsky proposed a linear programming (LP) relaxation of the\nproblem of computing an optimal static STT over a given tree topology. He used\nthis LP formulation to compute an STT that is a $2$-approximation to an optimal\nSTT, and conjectured that it is, in fact, an extended formulation of the\nconvex-hull of all depths-vectors of STTs, and thus always gives an optimal\nsolution. In this work we study this LP approach further. We show that the\nconjecture is false and that Golinsky's LP does not always give an optimal\nsolution. To show this we use what we call the ``normals method''. We use this\nmethod to enumerate over vertices of Golinsky's polytope for all tree\ntopologies of no more than 8 nodes. We give a lower bound on the integrality\ngap of the LP and on the approximation ratio of Golinsky's rounding method. We\nfurther enumerate several research directions that can lead to the resolution\nof the question whether one can compute an optimal STT in polynomial time.",
        "This paper proposes a cold start recommendation model that integrates\ncontrastive learning, aiming to solve the problem of performance degradation of\nrecommendation systems in cold start scenarios due to the scarcity of user and\nitem interaction data. The model dynamically adjusts the weights of key\nfeatures through an adaptive feature selection module and effectively\nintegrates user attributes, item meta-information, and contextual features by\ncombining a multimodal feature fusion mechanism, thereby improving\nrecommendation performance. In addition, the model introduces a contrastive\nlearning mechanism to enhance the robustness and generalization ability of\nfeature representation by constructing positive and negative sample pairs.\nExperiments are conducted on the MovieLens-1M dataset. The results show that\nthe proposed model significantly outperforms mainstream recommendation methods\nsuch as Matrix Factorization, LightGBM, DeepFM, and AutoRec in terms of HR,\nNDCG, MRR, and Recall, especially in cold start scenarios. Ablation experiments\nfurther verify the key role of each module in improving model performance, and\nthe learning rate sensitivity analysis shows that a moderate learning rate is\ncrucial to the optimization effect of the model. This study not only provides a\nnew solution to the cold start problem but also provides an important reference\nfor the application of contrastive learning in recommendation systems. In the\nfuture, this model is expected to play a role in a wider range of scenarios,\nsuch as real-time recommendation and cross-domain recommendation.",
        "This study presents observations of a large pseudostreamer solar eruption\nand, in particular, the post-eruption relaxation phase, as captured by Metis\nonboard the Solar Orbiter on October 12, 2022, during its perihelion passage.\nUtilizing total brightness data, we observe the outward propagation of helical\nfeatures up to 3 solar radii along a radial column that appears to correspond\nto the stalk of the pseudostreamer. The helical structures persisted for more\nthan 3 hours following a jet-like coronal mass ejection associated with a polar\ncrown prominence eruption. A notable trend is revealed: the inclination of\nthese features decreases as their polar angle and height increase.\nAdditionally, we measured their helix pitch. Despite a 2-minute time cadence\nlimiting direct correspondence among filamentary structures in consecutive\nframes, we find that the Metis helical structure may be interpreted as a\nconsequence of twist (nonlinear torsional Alfv\\'{e}n waves) and plasma\nliberated by interchange reconnection. A comparison was performed of the helix\nparameters as outlined by fine-scale outflow features with those obtained from\nsynthetic white-light images derived from the high-resolution\nmagnetohydrodynamics simulation of interchange reconnection in a pseudostreamer\ntopology by Wyper et al. (2022). A remarkable similarity between the\nsimulation-derived images and the observations was found. We conjecture that\nthese Metis observations may represent the upper end in spatial and energy\nscale of the interchange reconnection process that has been proposed recently\nas the origin of the Alfv\\'{e}nic solar wind.",
        "We analyze the split-sample robust inference (SSRI) methodology proposed by\nChernozhukov, Demirer, Duflo, and Fernandez-Val (CDDF) for quantifying\nuncertainty in heterogeneous treatment effect estimation. While SSRI\neffectively accounts for randomness in data splitting, its computational cost\ncan be prohibitive when combined with complex machine learning (ML) models. We\npresent an alternative randomization inference (RI) approach that maintains\nSSRI's generality without requiring repeated data splitting. By leveraging\ncross-fitting and design-based inference, RI achieves valid confidence\nintervals while significantly reducing computational burden. We compare the two\nmethods through simulation, demonstrating that RI retains statistical\nefficiency while being more practical for large-scale applications.",
        "We introduce a new way of counting numerical semigroups, namely by their\nmaximum primitive, and show its relation with the counting of numerical\nsemigroups by their Frobenius number. For any positive integer $n$, let $A_{n}$\ndenote the number of numerical semigroups whose maximum primitive is $n$, and\nlet $N_{n}$ denote the number of numerical semigroups whose Frobenius number is\n$n$. We show that the sequences $(A_{n})$ and $(N_{n})$ are M\\\"obius transforms\nof one another. We also establish that almost all numerical semigroups with\nlarge enough maximum primitive satisfy Wilf's conjecture. A crucial step in the\nproof is a result of independent interest: a numerical semigroup $S$ with\nmultiplicity $\\mathrm{m}$ such that $|S\\cap (\\mathrm{m},2 \\mathrm{m})|\\geq\n\\sqrt{3\\mathrm{m}}$ satisfies Wilf's conjecture.",
        "We show that the asymptotic regularity of a graded family $(I_n)_{n \\ge 0}$\nof homogeneous ideals in a standard graded algebra, i.e., the limit\n$\\lim\\limits_{n \\rightarrow \\infty} \\text{reg } I_n\/n$, exists in several\ncases; for example, when the family $(I_n)_{n \\ge 0}$ consists of artinian\nideals, or Cohen-Macaulay ideals of the same codimension, or when its Rees\nalgebra is Noetherian. Many applications, including simplifications and\ngeneralizations of previously known results on symbolic powers and integral\nclosures of powers of homogeneous ideals, are discussed. We provide a\ncombinatorial interpretation of the asymptotic regularity in terms of the\nassociated Newton--Okounkov body in various situations. We give a negative\nanswer to the question of whether the limits $\\lim\\limits_{n \\rightarrow\n\\infty} \\text{reg } (I_1^n + \\dots + I_p^n)\/n$ and $\\lim\\limits_{n \\rightarrow\n\\infty} \\text{reg } (I_1^n \\cap \\cdots \\cap I_p^n)\/n$ exist, for $p \\ge 2$ and\nhomogeneous ideals $I_1, \\dots, I_p$. We also examine ample evidence supporting\na negative answer to the question of whether the asymptotic regularity of the\nfamily of symbolic powers of a homogeneous ideal always exists. Our work\npresents explicit Gr\\\"obner basis construction for ideals of the type $Q^n +\n(f^k)$, where $Q$ is a monomial ideal, $f$ is a polynomial in the polynomial\nring in 4 variables over a field of characteristic 2."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"White-Box Transformers via Sparse Rate Reduction",
    "start_abstract":"In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "RNA-Seq: a revolutionary tool for transcriptomics"
      ],
      "abstract":[
        "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow\n  in Shallow Linear Networks",
        "Selecting Optimal Sampling Rate for Stable Super-Resolution",
        "A Time-Resolved High-Resolution Spectroscopic Analysis of Ionized\n  Calcium and Dynamical Processes in the Ultra-Hot Jupiter HAT-P-70 b",
        "Determining the Density of the Sun with Neutrinos",
        "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
        "Covering half-grids with lines and planes",
        "Non-collapsing volume estimate for local K\\\"ahler metrics in big\n  cohomology classes",
        "Multi-view biclustering via non-negative matrix tri-factorisation",
        "Robust high-order low-rank BUG integrators based on explicit Runge-Kutta\n  methods",
        "Effective range expansion with the left-hand cut and its application to\n  the $T_{cc}(3875)$",
        "Efficient LP warmstarting for linear modifications of the constraint\n  matrix",
        "Quantum Characterization, Verification, and Validation",
        "Incorporating Backreaction in One-Loop Corrections in Ultra-Slow-Roll\n  Inflation",
        "Precise constraint on properties of neutron stars through new universal\n  relations and astronomical observations",
        "Uniform mean estimation via generic chaining",
        "Microstate Geometries",
        "Benchmarking Quantum Reinforcement Learning",
        "Random batch sum-of-Gaussians algorithm for molecular dynamics\n  simulations of Yukawa systems in three dimensions",
        "On relationships between symmetric and non-symmetric cone separation\n  based on Bishop-Phelps separating cones in real normed spaces",
        "Equivariant Machine Learning Interatomic Potentials with Global Charge\n  Redistribution",
        "Polytopal discontinuous Galerkin methods for low-frequency\n  poroelasticity coupled to unsteady Stokes flow",
        "On The Very Bright Dropouts Selected Using the James Webb Space\n  Telescope NIRCam Instrument",
        "Liquidity-adjusted Return and Volatility, and Autoregressive Models",
        "The THESAN-ZOOM project: Star-formation efficiencies in high-redshift\n  galaxies",
        "Uniqueness of asymptotically conical shrinking gradient K\\\"ahler-Ricci\n  solitons",
        "Sampling Theory for Function Approximation with Numerical Redundancy",
        "The cones of g-vectors",
        "Embedding of Tree Tensor Networks into Shallow Quantum Circuits",
        "Fractional modelling of COVID-19 transmission incorporating asymptomatic\n  and super-spreader individuals"
      ],
      "abstract":[
        "We study the gradient descent (GD) dynamics of a depth-2 linear neural\nnetwork with a single input and output. We show that GD converges at an\nexplicit linear rate to a global minimum of the training loss, even with a\nlarge stepsize -- about $2\/\\textrm{sharpness}$. It still converges for even\nlarger stepsizes, but may do so very slowly. We also characterize the solution\nto which GD converges, which has lower norm and sharpness than the gradient\nflow solution. Our analysis reveals a trade off between the speed of\nconvergence and the magnitude of implicit regularization. This sheds light on\nthe benefits of training at the ``Edge of Stability'', which induces additional\nregularization by delaying convergence and may have implications for training\nmore complex models.",
        "We investigate the recovery of nodes and amplitudes from noisy frequency\nsamples in spike train signals, also known as the super-resolution (SR)\nproblem. When the node separation falls below the Rayleigh limit, the problem\nbecomes ill-conditioned. Admissible sampling rates, or decimation parameters,\nimprove the conditioning of the SR problem, enabling more accurate recovery. We\npropose an efficient preprocessing method to identify the optimal sampling\nrate, significantly enhancing the performance of SR techniques.",
        "We present the first transmission spectroscopy study of an exoplanet\natmosphere with the high-resolution mode of the new Gemini High-resolution\nOptical SpecTrograph (GHOST) instrument at the Gemini South Observatory. We\nobserved one transit of HAT-P-70 b - an ultra-hot Jupiter with an inflated\nradius - and made a new detection of the infrared Ca II triplet in its\ntransmission spectrum. The depth of the strongest line implies that a\nsubstantial amount of Ca II extends to at least 47% above the bulk planetary\nradius. The triplet lines are blueshifted between ~ 3 to 5 km\/s, indicative of\nstrong dayside-to-nightside winds common on highly irradiated gas giants.\nComparing the transmission spectrum with atmospheric models that incorporate\nnon-local thermodynamic equilibrium effects suggests that the planetary mass is\nlikely between 1 to 2 $M_{\\rm J}$, much lighter than the upper limit previously\nderived from radial velocity measurements. Importantly, thanks to the the high\nsignal-to-noise ratio achieved by GHOST\/Gemini South, we are able to measure\nthe temporal variation of these signals. Absorption depths and velocity offsets\nof the individual Ca II lines remain mostly consistent across the transit,\nexcept for the egress phases, where weaker absorption and stronger blueshifts\nare observed, highlighting the atmospheric processes within the trailing limb\nalone. Our study demonstrates the ability of GHOST to make time-resolved\ndetections of individual spectral lines, providing valuable insights into the\n3D nature of exoplanet atmospheres by probing different planetary longitudes as\nthe tidally locked planet rotates during the transit.",
        "The discovery of solar neutrinos confirmed that the inner workings of the Sun\ngenerally match our theoretical understanding of the fusion process. Solar\nneutrinos have also played a role in discovering that neutrinos have mass and\nthat they oscillate. We combine the latest solar neutrino data along with other\noscillation data from reactors to determine the Sun's density profile. We\nderive constraints given the current data and show the anticipated improvements\nwith more reactor neutrino data from JUNO constraining the true oscillation\nparameters and more solar neutrino data from DUNE which should provide a\ncrucial measurement of $hep$ neutrinos.",
        "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
        "We study hyperplane covering problems for finite grid-like structures in\n$\\mathbb{R}^d$. We call a set $\\mathcal{C}$ of points in $\\mathbb{R}^2$ a\nconical grid if the line $y = a_i$ intersects $\\mathcal{C}$ in exactly $i$\npoints, for some $a_1 > \\cdots > a_n \\in \\mathbb{R}$. We prove that the number\nof lines required to cover every point of such a grid at least $k$ times is at\nleast $nk\\left(1-\\frac{1}{e}-O(\\frac{1}{n}) \\right)$. If the grid $\\mathcal{C}$\nis obtained by cutting an $m \\times n$ grid of points into a half along one of\nthe diagonals, then we prove the lower bound of\n$mk\\left(1-e^{-\\frac{n}{m}}-O(\\frac{n}{m^2})\\right)$.\n  Motivated by the Alon-F\\\"uredi theorem on hyperplane coverings of grids that\nmiss a point and its multiplicity variations, we study the problem of finding\nthe minimum number of hyperplanes required to cover every point of an $n \\times\n\\cdots \\times n$ half-grid in $\\mathbb{R}^d$ at least $k$ times while missing a\npoint $P$. For almost all such half-grids, with $P$ being the corner point, we\nprove asymptotically sharp upper and lower bounds for the covering number in\ndimensions $2$ and $3$. For $k = 1$, $d = 2$, and an arbitrary $P$, we\ndetermine this number exactly by using the polynomial method bound for grids.",
        "We prove a uniform local non-collapsing volume estimate for a large family of\nK\\\"ahler metrics in the big cohomology classes. The key ingredient is a\ngeneralization of a mixed energy estimate for functions in the complex Sobolev\nspace to the setting of big cohomology classes.",
        "Multi-view data is ever more apparent as methods for production, collection\nand storage of data become more feasible both practically and fiscally.\nHowever, not all features are relevant to describe the patterns for all\nindividuals. Multi-view biclustering aims to simultaneously cluster both rows\nand columns, discovering clusters of rows as well as their view-specific\nidentifying features. A novel multi-view biclustering approach based on\nnon-negative matrix factorisation is proposed (ResNMTF). Demonstrated through\nextensive experiments on both synthetic and real datasets, ResNMTF successfully\nidentifies both overlapping and non-exhaustive biclusters, without pre-existing\nknowledge of the number of biclusters present, and is able to incorporate any\ncombination of shared dimensions across views. Further, to address the lack of\na suitable bicluster-specific intrinsic measure, the popular silhouette score\nis extended to the bisilhouette score. The bisilhouette score is demonstrated\nto align well with known extrinsic measures, and proves useful as a tool for\nhyperparameter tuning as well as visualisation.",
        "In this work, we propose high-order basis-update & Galerkin (BUG) integrators\nbased on explicit Runge-Kutta methods for large-scale matrix differential\nequations. These dynamical low-rank integrators are high-order extensions of\nthe BUG integrator and are constructed by performing one time-step of the\nfirst-order BUG integrator at each stage of the Runge-Kutta method. In this\nway, the resulting Runge-Kutta BUG integrator is robust to the presence of\nsmall singular values and does not involve backward time-integration steps. We\nprovide an error bound, which shows that the Runge-Kutta BUG integrator retains\nthe order of convergence of the associated Runge-Kutta method until the error\nreaches a plateau corresponding to the low-rank truncation and which vanishes\nas the rank increases. This error bound is finally validated numerically on\nthree different test cases. The results demonstrate the high-order convergence\nof the Runge-Kutta BUG integrator and its superior accuracy compared to other\nlow-rank integrators proposed in the literature.",
        "The validity range of the widely used traditional effective range expansion\ncan be severely limited by the presence of a left-hand cut near the\ntwo-particle threshold. Such a left-hand cut emerges in two-particle scattering\nprocesses involving either a light particle exchange in the $t$-channel or a\nparticle exchange with a mass slightly heavier than the mass difference of the\ntwo particles in the $u$-channel, which occurs in a wide range of physical\nsystems. We propose a new parameterization for the low-energy scattering\namplitude that incorporates these left-hand cuts arising from particle exchange\ndiagrams. This parameterization extends the convergence radius of the effective\nrange expansion beyond the branch point of the left-hand cut and is applicable\nto a broad range of systems. The parameterization enables the extraction of\ncoupling strengths between the exchange particle and the scattering particles,\nand reveals amplitude zeros resulting from the interplay between short- and\nlong-range interactions. We demonstrate the effectiveness of this new\nparameterization through its application to $DD^*$ scattering with meson masses\nobtained in a lattice QCD calculation.",
        "We consider the problem of computing the optimal solution and objective of a\nlinear program under linearly changing linear constraints. More specifically,\nwe want to compute the optimal solution of a linear optimization where the\nconstraint matrix linearly depends on a paramater that can take p different\nvalues. Based on the information given by a precomputed basis, we present three\nefficient LP warm-starting algorithms. Each algorithm is either based on the\neigenvalue decomposition, the Schur decomposition, or a tweaked eigenvalue\ndecomposition to evaluate the optimal solution and optimal objective of these\nproblems. The three algorithms have an overall complexity O(m^3 + pm^2) where m\nis the number of constraints of the original problem and p the number of values\nof the parameter that we want to evaluate. We also provide theorems related to\nthe optimality conditions to verify when a basis is still optimal and a local\nbound on the objective.",
        "Quantum characterization, verification, and validation (QCVV) is a set of\ntechniques to probe, describe, and assess the behavior of quantum bits\n(qubits), quantum information-processing registers, and quantum computers. QCVV\nprotocols probe and describe the effects of unwanted decoherence so that it can\nbe eliminated or mitigated. They can be usefully divided into characterization\ntechniques that estimate predictive models for a device's behavior from data,\nand benchmarking techniques that assess overall performance of a device. In\nthis introductory article, we briefly summarize the history of QCVV, introduce\nthe mathematical models and metrics upon which it relies, and then summarize\nthe foundational fields of tomography, randomized benchmarking, and holistic\nbenchmarks. We conclude with brief descriptions of (and references to) advanced\ntopics including gate set tomography, phase estimation, Pauli noise learning,\ncharacterization of mid-circuit measurements and non-Markovianity, classical\nshadows, verification and certification, and logical qubit assessment.",
        "We investigate the one-loop quantum correction to the power spectrum of\nprimordial curvature perturbations in the ultra-slow-roll (USR) inflationary\nscenario, incorporating the backreaction effect from curvature perturbations.\nIn the spatially-flat gauge, we expand the background inflaton field up to\nsecond order and identify the one-loop level backreaction term in the action.\nUtilizing a gauge transformation, we derive the comoving curvature interaction\nHamiltonian in the presence of the backreaction term and calculate the one-loop\ncorrection using the in-in formalism. Our results reveal that the one-loop\nsuper-horizon corrections previously reported in the literature are canceled by\nthe backreaction contributions. This finding underscores the importance of\naccounting for the backreaction effects in the analysis of quantum corrections\nduring USR inflation.",
        "In view of the great uncertainty of the equation of state (EOS) of\nhigh-density nuclear matter, establishing EOS-independent universal relations\nbetween global properties of neutron stars provides a practical way to\nconstrain the unobservable or difficult-to-observe properties through\nastronomical observations. It is common to construct universal relations\nbetween EOS-dependent properties (e.g., moment of inertia, tidal deformation,\netc.) or combined properties (e.g., compactness). Improving the precision of\nthe universal relations may provide stricter constraint on the properties of\nneutron star. We find that in 3-dimensional space with mass and radius as the\nbase coordinates, the points corresponding to a certain property of neutron\nstar described by different EOSs are almost located in the same surface. Thus\nthe universal relation between the property and the stellar mass-radius can be\nexpressed through describing the surface.\n  It is shown that the resulting universal relations have higher precisions. As\nan example, we construct high-precision universal relations for the moment of\ninertia, the $f$-mode frequency, and the dimensionless tidal deformation\nrespect to the mass-radius. As the observational data of neutron star mass and\nradius from NICER grows in data and accuracy, these universal relations allow\nfor more precise constraints on the unobservable or difficult-to-observe\nproperties.",
        "We introduce an empirical functional $\\Psi$ that is an optimal uniform mean\nestimator: Let $F\\subset L_2(\\mu)$ be a class of mean zero functions, $u$ is a\nreal valued function, and $X_1,\\dots,X_N$ are independent, distributed\naccording to $\\mu$. We show that under minimal assumptions, with $\\mu^{\\otimes\nN}$ exponentially high probability, \\[ \\sup_{f\\in F} |\\Psi(X_1,\\dots,X_N,f) -\n\\mathbb{E} u(f(X))| \\leq c R(F) \\frac{ \\mathbb{E} \\sup_{f\\in F } |G_f| }{\\sqrt\nN}, \\] where $(G_f)_{f\\in F}$ is the gaussian processes indexed by $F$ and\n$R(F)$ is an appropriate notion of `diameter' of the class $\\{u(f(X)) : f\\in\nF\\}$.\n  The fact that such a bound is possible is surprising, and it leads to the\nsolution of various key problems in high dimensional probability and high\ndimensional statistics. The construction is based on combining Talagrand's\ngeneric chaining mechanism with optimal mean estimation procedures for a single\nreal-valued random variable.",
        "We review the 20-year history of the Microstate Geometry Programme and the\nessential role that supergravity has played, and will continue to play, in the\ndescription of black-hole microstructure.",
        "Quantum Reinforcement Learning (QRL) has emerged as a promising research\nfield, leveraging the principles of quantum mechanics to enhance the\nperformance of reinforcement learning (RL) algorithms. However, despite its\ngrowing interest, QRL still faces significant challenges. It is still uncertain\nif QRL can show any advantage over classical RL beyond artificial problem\nformulations. Additionally, it is not yet clear which streams of QRL research\nshow the greatest potential. The lack of a unified benchmark and the need to\nevaluate the reliance on quantum principles of QRL approaches are pressing\nquestions. This work aims to address these challenges by providing a\ncomprehensive comparison of three major QRL classes: Parameterized Quantum\nCircuit based QRL (PQC-QRL) (with one policy gradient (QPG) and one Q-Learning\n(QDQN) algorithm), Free Energy based QRL (FE-QRL), and Amplitude Amplification\nbased QRL (AA-QRL). We introduce a set of metrics to evaluate the QRL\nalgorithms on the widely applicable benchmark of gridworld games. Our results\nprovide a detailed analysis of the strengths and weaknesses of the QRL classes,\nshedding light on the role of quantum principles in QRL and paving the way for\nfuture research in this field.",
        "Yukawa systems have drawn widespread interest across various applications. In\nthis paper, we introduce a novel random batch sum-of-Gaussians (RBSOG)\nalgorithm for molecular dynamics simulations of 3D Yukawa systems with periodic\nboundary conditions. We develop a sum-of-Gaussians (SOG) decomposition of the\nYukawa kernel, dividing the interactions into near-field and far-field\ncomponents. The near-field component, singular but compactly supported in a\nlocal domain, is calculated directly. The far-field component, represented as a\nsum of smooth Gaussians, is treated using the random batch approximation in\nFourier space with an adaptive importance sampling strategy to reduce the\nvariance of force calculations. Unlike the traditional Ewald decomposition,\nwhich introduces discontinuities and significant truncation error at the\ncutoff, the SOG decomposition achieves high-order smoothness and accuracy near\nthe cutoff, allowing for efficient and energy-stable simulations. Additionally,\nby avoiding the use of the fast Fourier transform, our method achieves optimal\nO(N) complexity while maintaining high parallel scalability. Finally, unlike\nprevious random batch approaches, the proposed adaptive importance sampling\nstrategy achieves nearly optimal variance reduction across the regime of the\ncoupling parameters. Rigorous theoretical analyses are presented. We validate\nthe performance of RBSOG method through simulations of one-component plasma\nunder weak and strong coupling conditions, using up to 10^6 particles and 1024\nCPU cores. As a practical application in fusion ignition, we simulate\nhigh-temperature, high-density deuterium-\\alpha mixtures to study the energy\nexchange between deuterium and high-energy \\alpha particles. The RBSOG method\ncan be readily extended to other dielectric response functions, offering a\npromising approach for large-scale simulations.",
        "In this paper, we study relationships between symmetric and non-symmetric\nseparation of (not necessarily convex) cones by using separating cones of\nBishop-Phelps type in real normed spaces. Besides extending some known results\nfor the non-symmetric cone separation approach, we propose a new symmetric cone\nseparation approach and establish cone separation results for it by using some\ncone separation results obtained for the non-symmetric cone separation approach\ntwice (by swapping the roles of the cones). In addition to specifically\nemphasizing the results for the convex case, we also present some existence\nresults for (bounded) convex bases of convex cones. Finally, we highlight some\napplications of symmetric and non-symmetric cone separation in optimization.",
        "Machine learning interatomic potentials (MLIPs) provide a computationally\nefficient alternative to quantum mechanical simulations for predicting material\nproperties. Message-passing graph neural networks, commonly used in these\nMLIPs, rely on local descriptor-based symmetry functions to model atomic\ninteractions. However, such local descriptor-based approaches struggle with\nsystems exhibiting long-range interactions, charge transfer, and compositional\nheterogeneity. In this work, we develop a new equivariant MLIP incorporating\nlong-range Coulomb interactions through explicit treatment of electronic\ndegrees of freedom, specifically global charge distribution within the system.\nThis is achieved using a charge equilibration scheme based on predicted atomic\nelectronegativities. We systematically evaluate our model across a range of\nbenchmark periodic and non-periodic datasets, demonstrating that it outperforms\nboth short-range equivariant and long-range invariant MLIPs in energy and force\npredictions. Our approach enables more accurate and efficient simulations of\nsystems with long-range interactions and charge heterogeneity, expanding the\napplicability of MLIPs in computational materials science.",
        "We focus on the numerical analysis of a polygonal discontinuous Galerkin\nscheme for the simulation of the exchange of fluid between a deformable\nsaturated poroelastic structure and an adjacent free-flow channel. We\nspecifically address wave phenomena described by the low-frequency Biot model\nin the poroelastic region and unsteady Stokes flow in the open channel,\npossibly an isolated cavity or a connected fracture system. The coupling at the\ninterface between the two regions is realized by means of transmission\nconditions expressing conservation laws. The spatial discretization hinges on\nthe weak form of the two-displacement poroelasticity system and a stress\nformulation of the Stokes equation with weakly imposed symmetry. We present a\ncomplete stability analysis for the proposed semi-discrete formulation and\nderive a-priori hp-error estimates.",
        "The selection of candidate high-redshift galaxies using the dropout technique\ntargeting the Lyman-break signature sometimes results in very bright objects,\nwhich would be too luminous to be easily explained if they are indeed at the\nexpected redshifts. Here we present a systematic study of very bright dropouts\nselected through successive bands of the NIRCam instrument onboard the James\nWebb Space Telescope (JWST). Using the public NIRCam data in four blank fields\nover 500~arcmin$^2$, 300 such objects were found. They have magnitudes in F356W\n$<25.1$~mag or $<26.0$~mag depending on the dropout passband, and the vast\nmajority of them ($>80\\%$) have very red F115W$-$F356W colors $> 2.0$~mag,\nwhich make them qualify as ``extremely red objects'' (EROs). We focus on the\n137 objects that also have mid-IR observations from the JWST MIRI instrument.\nThe analysis of their spectral energy distributions shows that these very\nbright dropouts are dominated by low-redshift ($z\\sim 1$--4) galaxies ($\\gtrsim\n67\\%$). However, a non-negligible fraction ($\\gtrsim 7\\%$) could be at high\nredshifts. Seven of our objects have secure spectroscopic redshifts from the\nJWST NIRSpec identifications, and the results confirm this picture: while six\nare low-redshift galaxies at $z\\approx 3$, one is a known galaxy at $z=8.679$\nrecovered in our sample. If more objects from our sample are confirmed to be at\nhigh redshifts, they could pose a severe challenge in explaining their\nproperties, such as the extremely high star formation rates and stellar masses.",
        "We construct liquidity-adjusted return and volatility using purposely\ndesigned liquidity metrics (liquidity jump and liquidity diffusion) that\nincorporate additional liquidity information. Based on these measures, we\nintroduce a liquidity-adjusted ARMA-GARCH framework to address the limitations\nof traditional ARMA-GARCH models, which are not effectively in modeling\nilliquid assets with high liquidity variability, such as cryptocurrencies. We\ndemonstrate that the liquidity-adjusted model improves model fit for\ncryptocurrencies, with greater volatility sensitivity to past shocks and\nreduced volatility persistence of erratic past volatility. Our model is\nvalidated by the empirical evidence that the liquidity-adjusted mean-variance\n(LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.",
        "Recent JWST observations hint at unexpectedly intense cosmic star-formation\nin the early Universe, often attributed to enhanced star-formation efficiencies\n(SFEs). Here, we analyze the SFE in THESAN-ZOOM, a novel zoom-in\nradiation-hydrodynamic simulation campaign of high-redshift ($z \\gtrsim 3$)\ngalaxies employing a state-of-the-art galaxy formation model resolving the\nmultiphase interstellar medium (ISM). The halo-scale SFE ($\\epsilon^{\\ast}_{\\rm\nhalo}$) - the fraction of baryons accreted by a halo that are converted to\nstars - follows a double power-law dependence on halo mass, with a mild\nredshift evolution above $M_{\\rm halo} \\gtrsim 10^{9.5}\\,{\\rm M}_{\\odot}$. The\npower-law slope is roughly $1\/3$ at large halo masses, consistent with\nexpectations when gas outflows are momentum-driven. At lower masses, the slope\nis roughly $2\/3$ and is more aligned with the energy-driven outflow scenario.\n$\\epsilon^{\\ast}_{\\rm halo}$ is a factor of $2-3$ larger than commonly assumed\nin empirical galaxy-formation models at $M_{\\rm halo} \\lesssim 10^{11}\\,{\\rm\nM}_{\\odot}$. On galactic (kpc) scales, the Kennicutt-Schmidt (KS) relation of\nneutral gas is universal in THESAN-ZOOM, following $\\Sigma_{\\rm SFR} \\propto\n\\Sigma_{\\rm gas}^2$, indicative of a turbulent energy balance in the ISM\nmaintained by stellar feedback. The rise of $\\epsilon^{\\ast}_{\\rm halo}$ with\nhalo mass can be traced primarily to increasing gas surface densities in\nmassive galaxies, while the underlying KS relation and neutral, star-forming\ngas fraction remain unchanged. Although the increase in $\\epsilon^{\\ast}_{\\rm\nhalo}$ with redshift is relatively modest, it is sufficient to explain the\nlarge observed number density of UV-bright galaxies at $z \\gtrsim 12$. However,\nreproducing the brightest sources at $M_{\\rm UV} \\lesssim -21$ may require\nextrapolating the SFE beyond the halo mass range directly covered by\nTHESAN-ZOOM.",
        "We show that, up to biholomorphism, a given noncompact complex manifold only\nadmits one shrinking gradient K\\\"ahler-Ricci soliton with Ricci curvature\ntending to zero at infinity. Our result does not require fixing the asymptotic\ndata of the metric, nor fixing the soliton vector field. The method used to\nprove the uniqueness of the soliton vector field can be applied more widely,\nfor example to show that conical Calabi-Yau metrics on a given complex manifold\nare unique up to biholomorphism. We also use it to prove that if two polarized\nFano fibrations, as introduced by Sun-Zhang, are biholomorphic and their\nvertices agree, then they are isomorphic as algebraic varieties.",
        "The study of numerical rounding errors is often greatly simplified in the\nanalytical treatment of mathematical problems, or even entirely separated from\nit. In sampling theory, for instance, it is standard to assume the availability\nof an orthonormal basis for computations, ensuring that numerical errors are\nnegligible. In reality, however, this assumption is often unmet. In this paper,\nwe discard it and demonstrate the advantages of integrating numerical insights\nmore deeply into sampling theory. To clearly pinpoint when the numerical\nphenomena play a significant role, we introduce the concept of numerical\nredundancy. A set of functions is numerically redundant if it spans a\nlower-dimensional space when analysed numerically rather than analytically.\nThis property makes it generally impossible to compute the best approximation\nof a function in its span using finite precision. In contrast,\n$\\ell^2$-regularized approximations are computable and, therefore, form the\nfoundation of many practical methods. Regularization generally reduces accuracy\ncompared to the best approximation, but our analysis shows that there is a\nbenefit: it also significantly reduces the amount of data needed for accurate\napproximation. Furthermore, we present a constructive method for optimally\nselecting data points for $L^2$-approximations, explicitly accounting for the\neffects of regularization. The results are illustrated for two common scenarios\nthat lead to numerical redundancy: (1) approximations on irregular domains and\n(2) approximations that incorporate specific features of the function to be\napproximated. In doing so, we obtain new results on random sampling for Fourier\nextension frames. Finally, we establish that regularization is implicit in\nnumerical orthogonalization of a numerically redundant set, indicating that its\nanalysis cannot be bypassed in a much broader range of methods.",
        "This paper studies the wall and chamber structure of algebras via generic\ndecompositions of g-vectors. Specifically, we examine points outside the\nchambers of the wall and chamber structure of ($\\tau$-tilting infinite)\nfinite-dimensional algebras. We demonstrate that the cones of g-vectors are\nboth rational and simplicial. Moreover, we show that the open cone of a given\ng-vector and the interior of its TF-equivalence class coincide if and only if\nthey are of the same dimension. Furthermore, we establish that g-vectors\nsatisfy the ray condition when sufficiently far from the origin. These results\nallow us to generalize several findings by Asai and Iyama regarding\nTF-equivalence classes of g-vectors.",
        "Variational Quantum Algorithms (VQAs) are being highlighted as key quantum\nalgorithms for demonstrating quantum advantage on Noisy Intermediate-Scale\nQuantum (NISQ) devices, which are limited to executing shallow quantum circuits\nbecause of noise. However, the barren plateau problem, where the gradient of\nthe loss function becomes exponentially small with system size, hinders this\ngoal. Recent studies suggest that embedding tensor networks into quantum\ncircuits and initializing the parameters can avoid the barren plateau. Yet,\nembedding tensor networks into quantum circuits is generally difficult, and\nmethods have been limited to the simplest structure, Matrix Product States\n(MPSs). This study proposes a method to embed Tree Tensor Networks (TTNs),\ncharacterized by their hierarchical structure, into shallow quantum circuits.\nTTNs are suitable for representing two-dimensional systems and systems with\nlong-range correlations, which MPSs are inadequate for representing. Our\nnumerical results show that embedding TTNs provides better initial quantum\ncircuits than MPS. Additionally, our method has a practical computational\ncomplexity, making it applicable to a wide range of TTNs. This study is\nexpected to extend the application of VQAs to two-dimensional systems and those\nwith long-range correlations, which have been challenging to utilize.",
        "The COVID-19 pandemic has presented unprecedented challenges worldwide,\nnecessitating effective modelling approaches to understand and control its\ntransmission dynamics. In this study, we propose a novel approach that\nintegrates asymptomatic and super-spreader individuals in a single\ncompartmental model. We highlight the advantages of utilizing incommensurate\nfractional order derivatives in ordinary differential equations, including\nincreased flexibility in capturing disease dynamics and refined memory effects\nin the transmission process. We conduct a qualitative analysis of our proposed\nmodel, which involves determining the basic reproduction number and analysing\nthe disease-free equilibrium's stability. By fitting the proposed model with\nreal data from Portugal and comparing it with existing models, we demonstrate\nthat the incorporation of supplementary population classes and fractional\nderivatives significantly improves the model's goodness of fit. Sensitivity\nanalysis further provides valuable insights for designing effective strategies\nto mitigate the spread of the virus."
      ]
    }
  }
]