[
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"The future of digital health with federated learning",
    "start_abstract":"Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Advances and Open Problems in Federated Learning"
      ],
      "abstract":[
        "The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Metering Error Estimation of Fast-Charging Stations Using Charging Data\n  Analytics",
        "Discovering Directly-Follows Graph Model for Acyclic Processes",
        "The Impact of Artificial Intelligence on Emergency Medicine: A Review of\n  Recent Advances",
        "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
        "Physical Layer Design for Ambient IoT",
        "Nice and precise $K^*(892) \\to K\\pi$ branching fractions",
        "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency",
        "A plethora of long-range neutrino interactions probed by DUNE and T2HK",
        "SPRI: Aligning Large Language Models with Context-Situated Principles",
        "Discrete Markov Probabilistic Models",
        "Estimation of the generalized Laplace distribution and its projection\n  onto the circle",
        "Efficient Parallel Scheduling for Sparse Triangular Solvers",
        "Electron spin dynamics guide cell motility",
        "Towards More Trustworthy Deep Code Models by Enabling\n  Out-of-Distribution Detection",
        "Real-Time LiDAR Point Cloud Compression and Transmission for\n  Resource-constrained Robots",
        "How Well Can AI Build SD Models?",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model",
        "A Unified Understanding and Evaluation of Steering Methods",
        "Pure $\\epsilon$-equilibrium in random games",
        "$B \\to \\rho \\ell \\bar{\\nu}$ resonance form factors from $B \\to \\pi\\pi\n  \\ell \\bar{\\nu}$ in lattice QCD",
        "Linear, nested, and quadratic ordered measures: Computation and\n  incorporation into optimization problems",
        "Fingerprint Matrix Concept for Detecting, Localizing and Characterizing\n  Targets in Complex Media",
        "Partitions with prescribed sum of reciprocals: asymptotic bounds",
        "The Role of Mobile and Social Media Services in Enhancing Freedom of\n  Expression: Opportunities, Challenges, and Prospects for Local Platform\n  Development in Uganda's Digital Ecosystem",
        "The Power of Perturbation under Sampling in Solving Extensive-Form Games",
        "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense",
        "A Randomised Approach to Distributed Sorting",
        "RITHMS : An advanced stochastic framework for the simulation of\n  transgenerational hologenomic data"
      ],
      "abstract":[
        "Accurate electric energy metering (EEM) of fast charging stations (FCSs),\nserving as critical infrastructure in the electric vehicle (EV) industry and as\nsignificant carriers of vehicle-to-grid (V2G) technology, is the cornerstone\nfor ensuring fair electric energy transactions. Traditional on-site\nverification methods, constrained by their high costs and low efficiency,\nstruggle to keep pace with the rapid global expansion of FCSs. In response,\nthis paper adopts a data-driven approach and proposes the measuring performance\ncomparison (MPC) method. By utilizing the estimation value of state-of-charge\n(SOC) as a medium, MPC establishes comparison chains of EEM performance of\nmultiple FCSs. Therefore, the estimation of EEM errors for FCSs with high\nefficiency is enabled. Moreover, this paper summarizes the interfering factors\nof estimation results and establishes corresponding error models and\nuncertainty models. Also, a method for discriminating whether there are EEM\nperformance defects in FCSs is proposed. Finally, the feasibility of MPC method\nis validated, with results indicating that for FCSs with an accuracy grade of\n2\\%, the discriminative accuracy exceeds 95\\%. The MPC provides a viable\napproach for the online monitoring of EEM performance for FCSs, laying a\nfoundation for a fair and just electricity trading market.",
        "Process mining is the common name for a range of methods and approaches aimed\nat analysing and improving processes. Specifically, methods that aim to derive\nprocess models from event logs fall under the category of process discovery.\nWithin the range of processes, acyclic processes form a distinct category. In\nsuch processes, previously performed actions are not repeated, forming chains\nof unique actions. However, due to differences in the order of actions,\nexisting process discovery methods can provide models containing cycles even if\na process is acyclic. This paper presents a new process discovery algorithm\nthat allows to discover acyclic DFG models for acyclic processes. A model is\ndiscovered by partitioning an event log into parts that provide acyclic DFG\nmodels and merging them while avoiding the formation of cycles. The resulting\nalgorithm was tested both on real-life and artificial event logs. Absence of\ncycles improves model visual clarity and precision, also allowing to apply\ncycle-sensitive methods or visualisations to the model.",
        "Artificial Intelligence (AI) is revolutionizing emergency medicine by\nenhancing diagnostic processes and improving patient outcomes. This article\nprovides a review of the current applications of AI in emergency imaging\nstudies, focusing on the last five years of advancements. AI technologies,\nparticularly machine learning and deep learning, are pivotal in interpreting\ncomplex imaging data, offering rapid, accurate diagnoses and potentially\nsurpassing traditional diagnostic methods. Studies highlighted within the\narticle demonstrate AI's capabilities in accurately detecting conditions such\nas fractures, pneumothorax, and pulmonary diseases from various imaging\nmodalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to\npredict clinical outcomes like mechanical ventilation needs illustrates its\npotential in crisis resource optimization. Despite these advancements, the\nintegration of AI into clinical practice presents challenges such as data\nprivacy, algorithmic bias, and the need for extensive validation across diverse\nsettings. This review underscores the transformative potential of AI in\nemergency settings, advocating for a future where AI and clinical expertise\nsynergize to elevate patient care standards.",
        "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
        "There is a growing demand for ultra low power and ultra low complexity\ndevices for applications which require maintenance-free and battery-less\noperation. One way to serve such applications is through backscatter devices,\nwhich communicate using energy harvested from ambient sources such as radio\nwaves transmitted by a reader. Traditional backscatter devices, such as RFID,\nare limited by range, interference, low connection density, and security\nissues. To address these problems, the Third Generation Partnership Project\n(3GPP) has started working on Ambient IoT (A-IoT). For the realization of A-IoT\ndevices, various aspects ranging from physical layer design, to the protocol\nstack, to the device architecture should be standardized. In this paper, we\nprovide an overview of the standardization efforts on the physical layer design\nfor A-IoT devices. The various physical channels and signals are discussed,\nfollowed by link level simulations to compare the performance of various\nconfigurations of reader to device and device to reader channels.",
        "Although discovered more than sixty years ago, direct measurement of the\n$K^*(892) \\to K\\pi$ branching fractions is a formidable challenge that has not\nbeen attempted. Typically they are assumed to obey the isospin limit in\nhundreds of particle data measurements. We show that an abundance of recent\namplitude analyses and other data, however, enables recovery of the ratios\n$\\mathcal{B}(K^{*+} \\to K^+ \\pi^0)\/\\mathcal{B}(K^{*+} \\to K_S^0 \\pi^+)$ and\n$4\\mathcal{B}(K^{*0} \\to K_S^0 \\pi^0)\/\\mathcal{B}(K^{*0} \\to K^+ \\pi^-)$ at\n$\\sim 5\\%$ precision.",
        "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%.",
        "The next-generation neutrino oscillation experiments would be sensitive to\nthe new neutrino interactions that would strengthen the search for physics\nbeyond the Standard Model. In this context, we explore the capabilities of the\ntwo leading future long-baseline neutrino oscillation experiments, DUNE and\nT2HK, to search for new flavor-dependent neutrino interactions with electrons,\nprotons, and neutrons that could potentially modify neutrino flavor\ntransitions. We forecast their sensitivities in the context of long-range\nneutrino interactions mediated by a neutral vector boson lighter than\n$10^{-10}$ eV and sourced by the vast amount of nearby and distant matter in\nthe Earth, Moon, Sun, Milky Way, and local Universe. For the first time, we\nexplore a plethora of $U(1)^\\prime$ symmetries inducing the new interactions\nbuilt from the combination of lepton and baryon numbers. We find that in all\ncases, DUNE and T2HK may constrain or discover the existence of new long-range\nneutrino interaction, and in some favorable cases, may identify the new\n$U(1)^\\prime$ symmetry responsible for it. In this short proceeding, we only\nsummarize the prospects of constraining the new interaction in case of all our\ncandidate $U(1)^\\prime$ symmetries, which have been discussed in JHEP 09 (2024)\n055.",
        "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https:\/\/github.com\/honglizhan\/SPRI-public.",
        "This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel\nalgorithm for discrete data generation. The algorithm operates in the space of\nbits $\\{0,1\\}^d$, where the noising process is a continuous-time Markov chain\nthat can be sampled exactly via a Poissonian clock that flips labels uniformly\nat random. The time-reversal process, like the forward noise process, is a jump\nprocess, with its intensity governed by a discrete analogue of the classical\nscore function. Crucially, this intensity is proven to be the conditional\nexpectation of a function of the forward process, strengthening its theoretical\nalignment with score-based generative models while ensuring robustness and\nefficiency. We further establish convergence bounds for the algorithm under\nminimal assumptions and demonstrate its effectiveness through experiments on\nlow-dimensional Bernoulli-distributed datasets and high-dimensional binary\nMNIST data. The results highlight its strong performance in generating discrete\nstructures. This work bridges theoretical foundations and practical\napplications, advancing the development of effective and theoretically grounded\ndiscrete generative modeling.",
        "The generalized Laplace (GL) distribution, which falls in the larger family\nof generalized hyperbolic distributions, provides a versatile model to deal\nwith a variety of applications thanks to its shape parameters. The elliptically\nsymmetric GL admits a polar representation that can be used to yield a circular\ndistribution, which we call \\emph{projected} GL distribution. The latter does\nnot appear to have been considered yet in practical applications. In this\narticle, we explore an easy-to-implement maximum likelihood estimation strategy\nbased on Gaussian quadrature for the scale-mixture representation of the GL and\nits projection onto the circle. A simulation study is carried out to benchmark\nthe fitting routine against alternative estimation methods to assess its\nfeasibility, while the projected GL model is contrasted with other popular\ncircular distributions.",
        "We develop and analyze new scheduling algorithms for solving sparse\ntriangular linear systems (SpTRSV) in parallel. Our approach, which we call\nbarrier list scheduling, produces highly efficient synchronous schedules for\nthe forward- and backward-substitution algorithm. Compared to state-of-the-art\nbaselines HDagg and SpMP, we achieve a $3.24\\times$ and $1.45\\times$\ngeometric-mean speed-up, respectively. We achieve this by obtaining an up to\n$11\\times$ geometric-mean reduction in the number of synchronization barriers\nover HDagg, whilst maintaining a balanced workload, and by applying a matrix\nreordering step for locality. We show that our improvements are consistent\nacross a variety of input matrices and hardware architectures.",
        "Diverse organisms exploit the geomagnetic field (GMF) for migration.\nMigrating birds employ an intrinsically quantum mechanical mechanism for\ndetecting the geomagnetic field: absorption of a blue photon generates a\nradical pair whose two electrons precess at different rates in the magnetic\nfield, thereby sensitizing cells to the direction of the GMF. In this work,\nusing an in vitro injury model, we discovered a quantum-based mechanism of\ncellular migration. Specifically, we show that migrating cells detect the GMF\nvia an optically activated, electron spin-based mechanism. Cell injury provokes\nacute emission of blue photons, and these photons sensitize muscle progenitor\ncells to the magnetic field. We show that the magnetosensitivity of muscle\nprogenitor cells is (a) activated by blue light, but not by green or red light,\nand (b) disrupted by the application of an oscillatory field at the frequency\ncorresponding to the energy of the electron-spin\/magnetic field interaction. A\ncomprehensive analysis of protein expression reveals that the ability of blue\nphotons to promote cell motility is mediated by activation of calmodulin\ncalcium sensors. Collectively, these data suggest that cells possess a\nlight-dependent magnetic compass driven by electron spin dynamics.",
        "Numerous machine learning (ML) models have been developed, including those\nfor software engineering (SE) tasks, under the assumption that training and\ntesting data come from the same distribution. However, training and testing\ndistributions often differ, as training datasets rarely encompass the entire\ndistribution, while testing distribution tends to shift over time. Hence, when\nconfronted with out-of-distribution (OOD) instances that differ from the\ntraining data, a reliable and trustworthy SE ML model must be capable of\ndetecting them to either abstain from making predictions, or potentially\nforward these OODs to appropriate models handling other categories or tasks.\n  In this paper, we develop two types of SE-specific OOD detection models,\nunsupervised and weakly-supervised OOD detection for code. The unsupervised OOD\ndetection approach is trained solely on in-distribution samples while the\nweakly-supervised approach utilizes a tiny number of OOD samples to further\nenhance the detection performance in various OOD scenarios. Extensive\nexperimental results demonstrate that our proposed methods significantly\noutperform the baselines in detecting OOD samples from four different scenarios\nsimultaneously and also positively impact a main code understanding task.",
        "LiDARs are widely used in autonomous robots due to their ability to provide\naccurate environment structural information. However, the large size of point\nclouds poses challenges in terms of data storage and transmission. In this\npaper, we propose a novel point cloud compression and transmission framework\nfor resource-constrained robotic applications, called RCPCC. We iteratively fit\nthe surface of point clouds with a similar range value and eliminate redundancy\nthrough their spatial relationships. Then, we use Shape-adaptive DCT (SA-DCT)\nto transform the unfit points and reduce the data volume by quantizing the\ntransformed coefficients. We design an adaptive bitrate control strategy based\non QoE as the optimization goal to control the quality of the transmitted point\ncloud. Experiments show that our framework achieves compression rates of\n40$\\times$ to 80$\\times$ while maintaining high accuracy for downstream\napplications. our method significantly outperforms other baselines in terms of\naccuracy when the compression rate exceeds 70$\\times$. Furthermore, in\nsituations of reduced communication bandwidth, our adaptive bitrate control\nstrategy demonstrates significant QoE improvements. The code will be available\nat https:\/\/github.com\/HITSZ-NRSL\/RCPCC.git.",
        "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Advances in natural language processing and large language models have\nsparked growing interest in modeling DNA, often referred to as the \"language of\nlife\". However, DNA modeling poses unique challenges. First, it requires the\nability to process ultra-long DNA sequences while preserving single-nucleotide\nresolution, as individual nucleotides play a critical role in DNA function.\nSecond, success in this domain requires excelling at both generative and\nunderstanding tasks: generative tasks hold potential for therapeutic and\nindustrial applications, while understanding tasks provide crucial insights\ninto biological mechanisms and diseases. To address these challenges, we\npropose HybriDNA, a decoder-only DNA language model that incorporates a hybrid\nTransformer-Mamba2 architecture, seamlessly integrating the strengths of\nattention mechanisms with selective state-space models. This hybrid design\nenables HybriDNA to efficiently process DNA sequences up to 131kb in length\nwith single-nucleotide resolution. HybriDNA achieves state-of-the-art\nperformance across 33 DNA understanding datasets curated from the BEND, GUE,\nand LRB benchmarks, and demonstrates exceptional capability in generating\nsynthetic cis-regulatory elements (CREs) with desired properties. Furthermore,\nwe show that HybriDNA adheres to expected scaling laws, with performance\nimproving consistently as the model scales from 300M to 3B and 7B parameters.\nThese findings underscore HybriDNA's versatility and its potential to advance\nDNA research and applications, paving the way for innovations in understanding\nand engineering the \"language of life\".",
        "Steering methods provide a practical approach to controlling large language\nmodels by applying steering vectors to intermediate activations, guiding\noutputs toward desired behaviors while avoiding retraining. Despite their\ngrowing importance, the field lacks a unified understanding and consistent\nevaluation across tasks and datasets, hindering progress. This paper introduces\na unified framework for analyzing and evaluating steering methods, formalizing\ntheir core principles and offering theoretical insights into their\neffectiveness. Through comprehensive empirical evaluations on multiple-choice\nand open-ended text generation tasks, we validate these insights, identifying\nkey factors that influence performance and demonstrating the superiority of\ncertain methods. Our work bridges theoretical and practical perspectives,\noffering actionable guidance for advancing the design, optimization, and\ndeployment of steering methods in LLMs.",
        "We show that for any $\\epsilon>0$ the probability that a randomly drawn game\nhas a pure $\\epsilon$-equilibrium goes to 1 as the number of agents gets large.\nThis contrasts sharply with the known fact that if $\\epsilon = 0$, that is, for\npure Nash equilibrium, the probability is asymptotically $1- 1\/e\\approx 0.63$.",
        "The decay $B \\to \\rho \\ell \\bar{\\nu}$ is an attractive process for\ndetermining the magnitude of the smallest CKM matrix element, $|V_{ub}|$, and\ncan provide new insights into the origin of the long-standing\nexclusive-inclusive discrepancy in determinations of this Standard-Model\nparameter. This requires a nonperturbative QCD calculation of the $B \\to \\rho$\nform factors $V$, $A_0$, $A_1$, and $A_{12}$. The unstable nature of the $\\rho$\nresonance has prevented precise lattice QCD calculations of these form factors\nto date. Here, we present the first lattice QCD calculation of the $B \\to \\rho$\nform factors in which the $\\rho$ is treated properly as a resonance in $P$-wave\n$\\pi \\pi$ scattering. To this end, we use the Lellouch-L\\\"uscher finite-volume\nformalism to compute the $B \\to \\pi \\pi$ form factors as a function of both\nmomentum transfer and $\\pi \\pi$ invariant mass, and then analytically continue\nto the $\\rho$ resonance pole. This calculation is performed with $2+1$\ndynamical quark flavors at a pion mass of approximately 320 MeV, and\ndemonstrates a clear path toward results at the physical point.",
        "In this paper we address a unified mathematical optimization framework to\ncompute a wide range of measures used in most operations research and data\nscience contexts. The goal is to embed such metrics within general optimization\nmodels allowing their efficient computation. We assess the usefulness of this\napproach applying it to three different families of measures, namely linear,\nnested, and quadratic ordered measures. Computational results are reported\nshowing the efficiency and accuracy of our methods as compared with standard\nimplementations in numerical software packages. Finally, we illustrate this\nmethodology by computing a number of optimal solutions with respect to\ndifferent metrics on three well-known linear and combinatorial optimization\nproblems: scenario analysis in linear programming, the traveling salesman and\nthe weighted multicover set problem.",
        "As waves propagate through a complex medium, they undergo multiple scattering\nevents. This phenomenon is detrimental to imaging, as it causes a full blurring\nof the image beyond a transport mean free path. Here, we show how to detect,\nlocalize, and characterize any scattering target through the reflection matrix\nof the complex medium in which this target is embedded and thus hidden from\ndirect view. More precisely, we introduce a fingerprint operator that contains\nthe specific signature of the target with respect to its environment. Applied\nto the recorded reflection matrix, this operator provides a likelihood index of\nthe target in any given state, despite the scattering fog induced by the\nsurrounding environment. This state can be the target position for localization\npurposes, its shape for characterization, or any other parameter that\ninfluences the target response. Our concept is versatile and broadly applicable\nto different type of waves for which multi-element technology allows a\nreflection matrix to be measured. We demonstrate this here explicitly by\nperforming different proof-of-concept experiments with ultrasound on targets\nburied inside a strongly scattering granular suspension, on lesion markers for\nclinical applications, and on the architecture of muscle tissue.",
        "In $1963$ Graham proved that every positive integer $n \\ge 78$ can be written\nas a sum of distinct positive integers $a_1, a_2, \\ldots, a_r$ for which\n$\\frac{1}{a_1} + \\frac{1}{a_2} + \\ldots + \\frac{1}{a_r}$ is equal to $1$. In\nthe same paper he managed to further generalize this, and showed that for all\npositive rationals $\\alpha$ and all positive integers $m$, there exists an\n$n_{\\alpha, m}$ such that every positive integer $n \\ge n_{\\alpha, m}$ has a\npartition with distinct parts, all larger than or equal to $m$, and such that\nthe sum of reciprocals is equal to $\\alpha$. No attempt was made to estimate\nthe quantity $n_{\\alpha, m}$, however. With $n_{\\alpha} := n_{\\alpha, 1}$, in\nthis paper we provide near-optimal upper bounds on $n_{\\alpha}$ and $n_{\\alpha,\nm}$, as well as bounds on the cardinality of the set $\\{\\alpha : n_{\\alpha} \\le\nn\\}$.",
        "Utilizing mobile and social media platforms is a transformative approach to\nenhancing freedom of expression and fostering digital engagement. However,\nUganda's digital ecosystem faces challenges such as restrictive legislation,\nfinancial barriers, and the absence of localized platforms tailored to cultural\ncontexts. This study employed a mixed-methods approach to explore how these\nplatforms influence public discourse, activism, and civic participation while\nhighlighting opportunities for local innovation. The research further\nidentified the critical need for regulatory reforms, investments in digital\nliteracy, and collaborative efforts to develop sustainable and culturally\nrelevant platforms, ensuring a more inclusive and empowered digital society.\n  Keywords: Freedom of Expression, Mobile Services, Social Media Platforms,\nLocal Digital Innovation, Uganda's Digital Ecosystem",
        "This paper investigates how perturbation does and does not improve the\nFollow-the-Regularized-Leader (FTRL) algorithm in imperfect-information\nextensive-form games. Perturbing the expected payoffs guarantees that the FTRL\ndynamics reach an approximate equilibrium, and proper adjustments of the\nmagnitude of the perturbation lead to a Nash equilibrium (\\textit{last-iterate\nconvergence}). This approach is robust even when payoffs are estimated using\nsampling -- as is the case for large games -- while the optimistic approach\noften becomes unstable. Building upon those insights, we first develop a\ngeneral framework for perturbed FTRL algorithms under \\textit{sampling}. We\nthen empirically show that in the last-iterate sense, the perturbed FTRL\nconsistently outperforms the non-perturbed FTRL. We further identify a\ndivergence function that reduces the variance of the estimates for perturbed\npayoffs, with which it significantly outperforms the prior algorithms on Leduc\npoker (whose structure is more asymmetric in a sense than that of the other\nbenchmark games) and consistently performs smooth convergence behavior on all\nthe benchmark games.",
        "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs.",
        "We introduce and analyse a new, extremely simple, randomised sorting\nalgorithm:\n  - choose a pair of indices $\\{i, j\\}$ according to some distribution $q$;\n  - sort the elements in positions $i$ and $j$ of the array in ascending order.\n  Choosing $q_{\\{i,j\\}} \\propto 1\/|j - i|$ yields an order-$n (\\log n)^2$\nsorting time. We call it the harmonic sorter.\n  The sorter trivially parallelises in the asynchronous setting, yielding a\nlinear speed-up. We also exhibit a low-communication, synchronous version with\na linear speed-up.\n  We compare and contrast this algorithm with other sorters, and discuss some\nof its benefits, particularly its robustness and amenability to parallelisation\nand distributed computing.",
        "A holobiont is made up of a host organism together with its microbiota. In\nthe context of animal breeding, as the holobiont can be viewed as the single\nunit upon which selection operates, integrating microbiota data into genomic\nprediction models may be a promising approach to improve predictions of\nphenotypic and genetic values. Nevertheless, there is a paucity of hologenomic\ntransgenerational data to address this hypothesis, and thus to fill this gap,\nwe propose a new simulation framework. Our approach, an R Implementation of a\nTransgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source\npackage, builds upon the MoBPS package and incorporates distinctive\ncharacteristics of the microbiota, notably vertical and horizontal transmission\nas well as modulation due to the environment and host genetics. In addition,\nRITHMS can account for a variety of selection strategies and is adaptable to\ndifferent genetic architectures. We simulated transgenerational hologenomic\ndata using RITHMS under a wide variety of scenarios, varying heritability,\nmicrobiability, and microbiota heritability. We found that simulated data\naccurately reflected expected characteristics, notably based on microbial\ndiversity metrics, correlation between taxa, modulation of vertical and\nhorizontal transmission, response to environmental effects and the evolution of\nphenotypic values depending on selection strategy. Our results support the\nrelevance of our simulation framework and illustrate its possible use for\nbuilding a selection index balancing genetic gain and microbial diversity.\nRITHMS is an advanced, flexible tool for generating transgenerational\nhologenomic data that incorporate the complex interplay between genetics,\nmicrobiota and environment."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b2",
    "start_title":"Meeting Strangers and Friends of Friends: How Random Are Social Networks?",
    "start_abstract":"We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)",
    "start_categories":[
      "q-fin.EC"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "An agent-based spatial urban social network generator: A case study of beijing, china"
      ],
      "abstract":[
        "This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective."
      ],
      "categories":[
        "cs.CE"
      ]
    },
    "list":{
      "title":[
        "Ferroelectric Properties of van der Waals Chalcogenides: DFT perspective",
        "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies",
        "Contact process for the spread of knowledge",
        "Site-Decorated Model for Unconventional Frustrated Magnets: Ultranarrow\n  Phase Crossover and Spin Reversal Transition",
        "Path-dependency and emergent computing under vectorial driving",
        "Enhancing the De-identification of Personally Identifiable Information\n  in Educational Data",
        "Multi-compartment diffusion-relaxation MR signal representation in the\n  spherical 3D-SHORE basis",
        "Advancing ATLAS DCS Data Analysis with a Modern Data Platform",
        "Non-linear Partition of Unity method",
        "Network fault costs based on minimum leaf spanning trees",
        "Study of long-term spectral evolution and X-ray and Gamma-ray\n  correlation of blazars seen by HAWC",
        "Optimized Relay Lens Design For High-Resolution Image Transmission In\n  Military Target Detection Systems",
        "Reinforcement Learning in Strategy-Based and Atari Games: A Review of\n  Google DeepMinds Innovations",
        "Detection of Somali-written Fake News and Toxic Messages on the Social\n  Media Using Transformer-based Language Models",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "Landau-level composition of bound exciton states in magnetic field",
        "Foundations of Digital Circuits: Denotation, Operational, and Algebraic\n  Semantics",
        "AI-based Identity Fraud Detection: A Systematic Review",
        "On stability of exponentially subelliptic harmonic maps",
        "Numerical evaluation of Gaussian mixture entropy",
        "A Homology Theory for the Semimodules of Radical Submodules",
        "Annotating Scientific Uncertainty: A comprehensive model using\n  linguistic patterns and comparison with existing approaches",
        "Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge\n  Consistency Guided Score Distillation",
        "Incorporating Backreaction in One-Loop Corrections in Ultra-Slow-Roll\n  Inflation",
        "Several-variable Kronecker limit formula over global function fields",
        "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
        "LSM Trees in Adversarial Environments",
        "Pulsar scattering as a probe for structures in the interstellar medium",
        "Orbital Signatures of Density Wave Transition in La3Ni2O7-delta and\n  La2PrNi2O7-delta RP-Nickelates Probed via in-situ X-ray Absorption Near-edge\n  Spectroscopy"
      ],
      "abstract":[
        "Layered materials with non-centrosymmetric stacking order are attracting\nincreasing interest due to the presence of ferroelectric polarization, which is\ndictated by weak interlayer hybridization of atomic orbitals. Here, we use\ndensity functional theory modelling to systematically build a library of van\nder Waals chalcogenides that exhibit substantial ferroelectric polarization.\nFor the most promising materials, we also analyse the pressure dependence of\nthe ferroelectric effect and charge accumulation of photo-induced electrons and\nholes at surfaces and internal twin boundaries in thin films of such materials.",
        "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), tensor processing units (TPUs), and\nfield-programmable gate arrays (FPGAs). The goal is to inspire further research\nwith a contemporary guide on optimizing ViTs for efficient deployment on edge\ndevices.",
        "This paper is concerned with a natural variant of the contact process\nmodeling the spread of knowledge on the integer lattice. Each site is\ncharacterized by its knowledge, measured by a real number ranging from 0 =\nignorant to 1 = omniscient. Neighbors interact at rate $\\lambda$, which results\nin both neighbors attempting to teach each other a fraction $\\mu$ of their\nknowledge, and individuals die at rate one, which results in a new individual\nwith no knowledge. Starting with a single omniscient site, our objective is to\nstudy whether the total amount of knowledge on the lattice converges to zero\n(extinction) or remains bounded away from zero (survival). The process dies out\nwhen $\\lambda \\leq \\lambda_c$ and\/or $\\mu = 0$, where $\\lambda_c$ denotes the\ncritical value of the contact process. In contrast, we prove that, for all\n$\\lambda > \\lambda_c$, there is a unique phase transition in the direction of\n$\\mu$, and for all $\\mu > 0$, there is a unique phase transition in the\ndirection of $\\lambda$. Our proof of survival relies on block constructions\nshowing more generally convergence of the knowledge to infinity, while our\nproof of extinction relies on martingale techniques showing more generally an\nexponential decay of the knowledge.",
        "The site-decorated Ising model is introduced to advance the understanding and\nexperimental realization of the recently discovered one-dimensional\nfinite-temperature ultranarrow phase crossover in an external magnetic field,\nwhile mitigating the geometric complexities of traditional bond-decorated\nmodels. Furthermore, although higher-dimensional Ising models in an external\nfield remain unsolved, an exact solution for a novel spin-reversal transition\n-- driven by an exotic, hidden ``half-ice, half-fire'' state induced by site\ndecoration -- is derived. This transition, triggered by a slight variation in\ntemperature or magnetic field even in the weak-field limit, offers a promising\nroute toward energy-efficient applications such as data storage and processing.\nThe results establish site decoration as a compelling new avenue for materials\nand device design, particularly in systems such as mixed $d$-$f$ compounds,\noptical lattices, and neural networks.",
        "The sequential response of frustrated materials - ranging from crumpled\nsheets and amorphous media to metamaterials - reveals their memory effects and\nemergent computational potential. Despite their spatial extension, most studies\nrely on a single global stimulus, such as compression, effectively reducing the\nproblem to scalar driving. Here, we introduce vectorial driving by applying\nmultiple spatially localized stimuli to explore path-dependent, sequential\nresponses. We uncover a wealth of phenomena absent in scalar driving, including\nnon-Abelian responses, mixed-mode behavior, and chiral loop transients. We find\nthat such path dependencies arise from elementary motifs linked to fold\nsingularities, which connect triplets of states - ancestor, descendant, and\nsibling; and develop a general framework using pt-graphs to describe responses\nunder any vectorial driving protocol. Leveraging binarized vectorial driving,\nwe establish a natural connection to computation, showing that a single sample\ncan encode multiple sequential Boolean circuits, which are selectable by\ndriving strength and reprogrammable via additional inputs. Finally, we\nintroduce graph-based motifs to manage the complexity of high-dimensional\ndriving. Our work paves the way for strategies to explore, harness, and\nunderstand complex materials and memory, while advancing embodied intelligence\nand in-materia computing.",
        "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https:\/\/github.com\/AnonJD\/PrivacyAI",
        "Modelling the diffusion-relaxation magnetic resonance (MR) signal obtained\nfrom multi-parametric sequences has recently gained immense interest in the\ncommunity due to new techniques significantly reducing data acquisition time. A\npreferred approach for examining the diffusion-relaxation MR data is to follow\nthe continuum modelling principle that employs kernels to represent the tissue\nfeatures, such as the relaxations or diffusion properties. However,\nconstructing reasonable dictionaries with predefined signal components depends\non the sampling density of model parameter space, thus leading to a geometrical\nincrease in the number of atoms per extra tissue parameter considered in the\nmodel. That makes estimating the contributions from each atom in the signal\nchallenging, especially considering diffusion features beyond the\nmono-exponential decay.\n  This paper presents a new Multi-Compartment diffusion-relaxation MR signal\nrepresentation based on the Simple Harmonic Oscillator-based Reconstruction and\nEstimation (MC-SHORE) representation, compatible with scattered acquisitions.\nThe proposed technique imposes sparsity constraint on the solution via the\n$\\ell_1$ norm and enables the estimation of the microstructural measures, such\nas the return-to-the-origin probability, and the orientation distribution\nfunction, depending on the compartments considered in a single voxel. The\nprocedure has been verified with in silico and in vivo data and enabled the\napproximation of the diffusion-relaxation MR signal more accurately than\nsingle-compartment non-Gaussian representations and multi-compartment\nmono-exponential decay techniques, maintaining a low number of atoms in the\ndictionary. Ultimately, the MC-SHORE procedure allows for separating\nintra-\/extra-axonal and free water contributions from the signal, thus reducing\nthe partial volume effect observable in the boundaries of the tissues.",
        "This paper presents a modern and scalable framework for analyzing Detector\nControl System (DCS) data from the ATLAS experiment at CERN. The DCS data,\nstored in an Oracle database via the WinCC OA system, is optimized for\ntransactional operations, posing challenges for large-scale analysis across\nextensive time periods and devices. To address these limitations, we developed\na data pipeline using Apache Spark, CERN's Hadoop service, and the CERN SWAN\nplatform. This framework integrates seamlessly with Python notebooks, providing\nan accessible and efficient environment for data analysis using\nindustry-standard tools. The approach has proven effective in troubleshooting\nData Acquisition (DAQ) links for the ATLAS New Small Wheel (NSW) detector,\ndemonstrating the value of modern data platforms in enabling detector experts\nto quickly identify and resolve critical issues.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We study the fault-tolerance of networks from both the structural and\ncomputational point of view using the minimum leaf number of the corresponding\ngraph $G$, i.e. the minimum number of leaves of the spanning trees of $G$, and\nits vertex-deleted subgraphs. We investigate networks that are leaf-guaranteed,\ni.e. which satisfy a certain stability condition with respect to minimum leaf\nnumbers and vertex-deletion. Next to this, our main notion is the so-called\nfault cost, which is based on the number of vertices that have different\ndegrees in minimum leaf spanning trees of the network and its vertex-deleted\nsubgraphs. We characterise networks with vanishing fault cost via\nleaf-guaranteed graphs and describe, for any given network $N$, leaf-guaranteed\nnetworks containing $N$. We determine for all non-negative integers $k \\le 8$\nexcept $1$ the smallest network with fault cost $k$. We also give a detailed\ntreatment of the fault cost $1$ case, prove that there are infinitely many\n$3$-regular networks with fault cost $3$, and show that for any non-negative\ninteger $k$ there exists a network with fault cost exactly $k$.",
        "The HAWC Observatory collected 6 years of extensive data, providing an ideal\nplatform for long-term monitoring of blazars in the Very High Energy (VHE)\nband, without bias towards specific flux states. HAWC continuously monitors\nblazar activity at TeV energies, focusing on sources with a redshift of {z \\lt\n0.3}, based on the Third Fermi-LAT Catalog of High-Energy sources. We\nspecifically focused our analysis on Mrk 421 and Mrk 501, as they are the\nbrightest blazars observed by the HAWC Observatory. With a dataset of 2143\ndays, this work significantly extends the monitoring previously published,\nwhich was based on 511 days of observation. By utilizing HAWC data for the VHE\n{\\gamma}-ray emission in the 300 GeV to 100 TeV energy range, in conjunction\nwith Swift-XRT data for the 0.3 to 10 keV X-ray emission, we aim to explore\npotential correlations between these two bands. For Mrk 501, we found evidence\nof a long-term correlation. Additionally, we identified a period in the light\ncurve where the flux was very low for more than two years. On the other hand,\nour analysis of Mrk 421 measured a strong linear correlation for\nquasi-simultaneous observations collected by HAWC and Swift-XRT. This result is\nconsistent with a linear dependence and a multiple-zone synchrotron\nself-Compton model to explain the X-ray and the {\\gamma}-ray emission. Finally,\nas suggested by previous findings, we confirm a harder-when-brighter behavior\nin the spectral evolution of the flux properties for Mrk 421. These findings\ncontribute to the understanding of blazar emissions and their underlying\nmechanisms.",
        "The design and performance analysis of relay lenses that provide\nhigh-performance image transmission for target acquisition and tracking in\nmilitary optical systems. Relay lenses are critical components for clear and\nlossless image transmission over long distances. In this study, the optical\nperformance of a relay lens system designed and optimized using ZEMAX software\nis investigated in detail. The analysis focuses on important optical properties\nsuch as modulation transfer function (MTF), spot diagrams, Seidel diagram,\nfield curvature and distortion. The results show that the lens has significant\npotential in military applications for target detection and tracking with high\nresolution and low aberration.",
        "Reinforcement Learning (RL) has been widely used in many applications,\nparticularly in gaming, which serves as an excellent training ground for AI\nmodels. Google DeepMind has pioneered innovations in this field, employing\nreinforcement learning algorithms, including model-based, model-free, and deep\nQ-network approaches, to create advanced AI models such as AlphaGo, AlphaGo\nZero, and MuZero. AlphaGo, the initial model, integrates supervised learning\nand reinforcement learning to master the game of Go, surpassing professional\nhuman players. AlphaGo Zero refines this approach by eliminating reliance on\nhuman gameplay data, instead utilizing self-play for enhanced learning\nefficiency. MuZero further extends these advancements by learning the\nunderlying dynamics of game environments without explicit knowledge of the\nrules, achieving adaptability across various games, including complex Atari\ngames. This paper reviews the significance of reinforcement learning\napplications in Atari and strategy-based games, analyzing these three models,\ntheir key innovations, training processes, challenges encountered, and\nimprovements made. Additionally, we discuss advancements in the field of\ngaming, including MiniZero and multi-agent models, highlighting future\ndirections and emerging AI models from Google DeepMind.",
        "The fact that everyone with a social media account can create and share\ncontent, and the increasing public reliance on social media platforms as a news\nand information source bring about significant challenges such as\nmisinformation, fake news, harmful content, etc. Although human content\nmoderation may be useful to an extent and used by these platforms to flag\nposted materials, the use of AI models provides a more sustainable, scalable,\nand effective way to mitigate these harmful contents. However, low-resourced\nlanguages such as the Somali language face limitations in AI automation,\nincluding scarce annotated training datasets and lack of language models\ntailored to their unique linguistic characteristics. This paper presents part\nof our ongoing research work to bridge some of these gaps for the Somali\nlanguage. In particular, we created two human-annotated social-media-sourced\nSomali datasets for two downstream applications, fake news \\& toxicity\nclassification, and developed a transformer-based monolingual Somali language\nmodel (named SomBERTa) -- the first of its kind to the best of our knowledge.\nSomBERTa is then fine-tuned and evaluated on toxic content, fake news and news\ntopic classification datasets. Comparative evaluation analysis of the proposed\nmodel against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc)\ndemonstrated that SomBERTa consistently outperformed these comparators in both\nfake news and toxic content classification tasks while achieving the best\naverage accuracy (87.99%) across all tasks. This research contributes to Somali\nNLP by offering a foundational language model and a replicable framework for\nother low-resource languages, promoting digital and AI inclusivity and\nlinguistic diversity.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "We present a theory that studies the state composition of a bound exciton in\nmagnetic field. Using a basis set made of products of free electron and hole\nwavefunctions in Landau gauge, we derive a secular equation which shows the\nrelation between Landau levels (LLs) of the electron and hole when a bound\nexciton is formed. Focusing on excitons in the light cone, we establish a\nscattering selection rule for the interaction of an electron in LL $n_\\text{e}$\nwith a hole in LL $n_\\text{h}$. We solve the resulting secular equation and\nidentify a simple pairing law, $n_\\text{e} = n_\\text{h} + l$, which informs us\non the construction of a bound exciton state with magnetic quantum number $l$,\nand on the interaction of the exciton magnetic moment with magnetic field. We\nobtain good agreement between theory results and recent measurements of the\ndiamagnetic shifts of exciton states in WSe$_2$ monolayers.",
        "This thesis details a project to define a fully compositional theory of\nsynchronous sequential circuits built from primitive components, motivated by\napplying techniques successfully used in programming languages to hardware.\n  The first part of the thesis defines the syntactic foundations of sequential\ncircuit morphisms, and then builds three different semantic theories:\ndenotational, operational and algebraic. We characterise the denotational\nsemantics of sequential circuits as certain causal stream functions, as well as\nproviding a link to existing circuit methodologies by mapping between circuit\nmorphisms, stream functions and Mealy machines. The operational semantics is\ndefined as a strategy for applying some global transformations followed by\nlocal reductions to demonstrate how a circuit processes a value, leading to a\nnotion of observational equivalence. The algebraic semantics consists of\nequations for bringing circuits into a pseudo-normal form, and then encoding\nbetween different state sets. This part of the thesis concludes with a\ndiscussion of some novel applications, such as those for using partial\nevaluation for digital circuits.\n  While mathematically rigorous, the categorical string diagram formalism is\nnot suited for reasoning computationally. The second part of this thesis\ndetails an extension of string diagram rewriting with hypergraphs so that it is\ncompatible with the traced comonoid structure present in the category of\ndigital circuits. We identify the properties that characterise cospans of\nhypergraphs corresponding to traced comonoid terms, and demonstrate how to\nidentify rewriting contexts valid for rewriting modulo traced comonoid\nstructure. We apply the graph rewriting framework to fixed point operators as\nwell as the operational semantics from the first part, and present a new\nhardware description language based on these theoretical developments.",
        "With the rapid development of digital services, a large volume of personally\nidentifiable information (PII) is stored online and is subject to cyberattacks\nsuch as Identity fraud. Most recently, the use of Artificial Intelligence (AI)\nenabled deep fake technologies has significantly increased the complexity of\nidentity fraud. Fraudsters may use these technologies to create highly\nsophisticated counterfeit personal identification documents, photos and videos.\nThese advancements in the identity fraud landscape pose challenges for identity\nfraud detection and society at large. There is a pressing need to review and\nunderstand identity fraud detection methods, their limitations and potential\nsolutions. This research aims to address this important need by using the\nwell-known systematic literature review method. This paper reviewed a selected\nset of 43 papers across 4 major academic literature databases. In particular,\nthe review results highlight the two types of identity fraud prevention and\ndetection methods, in-depth and open challenges. The results were also\nconsolidated into a taxonomy of AI-based identity fraud detection and\nprevention methods including key insights and trends. Overall, this paper\nprovides a foundational knowledge base to researchers and practitioners for\nfurther research and development in this important area of digital identity\nfraud.",
        "In this paper, we study the stability problem of exponentially subelliptic\nharmonic maps from sub-Riemannian manifolds to Riemannian manifolds. We derive\nthe rst and second variation formulas for exponentially subelliptic harmonic\nmaps, and apply these formulas to prove that if the target manifold has\nnonpositive curvature, the exponentially subelliptic harmonic map is stable.\nFurther, we obtain the instability of exponentially subelliptic harmonic maps\nwhen the target manifold is a sphere.",
        "We develop an approximation method for the differential entropy\n$h(\\mathbf{X})$ of a $q$-component Gaussian mixture in $\\mathbb{R}^n$. We\nprovide two examples of approximations using our method denoted by\n$\\bar{h}^{\\mathrm{Taylor}}_{C,m}(\\mathbf{X})$ and\n$\\bar{h}^{\\mathrm{Polyfit}}_{C,m}(\\mathbf{X})$. We show that\n$\\bar{h}^{\\mathrm{Taylor}}_{C,m}(\\mathbf{X})$ provides an easy to compute lower\nbound to $h(\\mathbf{X})$, while $\\bar{h}^{\\mathrm{Polyfit}}_{C,m}(\\mathbf{X})$\nprovides an accurate and efficient approximation to $h(\\mathbf{X})$.\n$\\bar{h}^{\\mathrm{Polyfit}}_{C,m}(\\mathbf{X})$ is more accurate than known\nbounds, and conjectured to be much more resilient than the approximation of [5]\nin high dimensions.",
        "Let $R$ be a commutative ring with identity, and let $\\R(R)$ denote the\nsemiring of radical ideals of $R$. The radical functor $\\R$, from the category\nof $R$-modules $R{-}\\boldsymbol{\\sf{Mod}}$ to the category of\n$\\R(R)$-semimodules $\\R(R){-}\\boldsymbol{\\sf{Semod}}$, maps any complex\n$\\M=(M_n, f_n)_{n\\geq 0}$ of $R$-modules to a complex $\\R(\\M)=(\\R(M_n),\n\\R(f_n))_{n\\geq 0}$ of $\\R(R)$-semimodules, where $\\R(M_n)$ consists of radical\nsubmodules of $M_n$, and the $\\R(R)$-semimodule homomorphisms\n$\\R(f_n):\\R(M_n)\\rightarrow \\R(M_{n-1})$ are defined by\n$\\R(f_n)(N)=\\rad(f_n(N))$. The $n$-th radical homology of the complex\n$(\\R(M_n), \\R(f_n))_{n\\geq 0}$, denoted $H_n(\\R(\\M))$, consists of radical\nsubmodules $N$ of $M_n$ such that $f_n(N)$ is contained in the radical of the\nzero submodule of $M_{n-1}$, and two such radical submodules are equivalent\nunder the Bourne relation modulo the image of $\\R(f_{n+1})$. $H_n(\\R(-))$ is\nregarded as a covariant functor from the category\n$\\boldsymbol{\\sf{Ch}}(R{-}\\boldsymbol{\\sf{Mod}})$ of chain complexes of\n$R$-modules to $\\R(R){-}\\boldsymbol{\\sf{Semod}}$, which acts identically on any\npair of homotopic maps of complexes of $R$-modules. In particular, if $\\M$ and\n$\\M'$ are homotopically equivalent, then $H_n(\\R(\\M))$ and $H_n(\\R(\\M'))$ are\nisomorphic $\\R(R)$-semimodules. We provide conditions under which $H_n(\\R(-))$\ninduces a long exact sequence of radical homology modules for any short exact\nsequence of complexes of $R$-modules, and satisfies the naturality condition\nfor exact homology sequences. Finally, we introduce a projective resolution for\nan $R$-module $M$ based on $\\R(R)$-semimodules and give conditions under which\nsuch a projective resolution exists and is unique up to a homotopy.",
        "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages.",
        "We present Acc3D to tackle the challenge of accelerating the diffusion\nprocess to generate 3D models from single images. To derive high-quality\nreconstructions through few-step inferences, we emphasize the critical issue of\nregularizing the learning of score function in states of random noise. To this\nend, we propose edge consistency, i.e., consistent predictions across the high\nsignal-to-noise ratio region, to enhance a pre-trained diffusion model,\nenabling a distillation-based refinement of the endpoint score function.\nBuilding on those distilled diffusion models, we propose an adversarial\naugmentation strategy to further enrich the generation detail and boost overall\ngeneration quality. The two modules complement each other, mutually reinforcing\nto elevate generative performance. Extensive experiments demonstrate that our\nAcc3D not only achieves over a $20\\times$ increase in computational efficiency\nbut also yields notable quality improvements, compared to the\nstate-of-the-arts.",
        "We investigate the one-loop quantum correction to the power spectrum of\nprimordial curvature perturbations in the ultra-slow-roll (USR) inflationary\nscenario, incorporating the backreaction effect from curvature perturbations.\nIn the spatially-flat gauge, we expand the background inflaton field up to\nsecond order and identify the one-loop level backreaction term in the action.\nUtilizing a gauge transformation, we derive the comoving curvature interaction\nHamiltonian in the presence of the backreaction term and calculate the one-loop\ncorrection using the in-in formalism. Our results reveal that the one-loop\nsuper-horizon corrections previously reported in the literature are canceled by\nthe backreaction contributions. This finding underscores the importance of\naccounting for the backreaction effects in the analysis of quantum corrections\nduring USR inflation.",
        "We establish Kronecker-type first and second limit formulas for\n\"non-holomorphic\" and \"Jacobi-type\" Eisenstein series over global function\nfields in the several-variable setting. Our main theorem demonstrates that the\nderivatives of these Eisenstein series can be understood as averaged integrals\nof certain period quantities along the associated \"Heegner cycles\" on Drinfeld\nmodular varieties. A key innovation lies in our use of the Berkovich analytic\nstructure of the Drinfeld period domains, which enables the parametrization of\nthe Heegner cycles in question by Euclidean \"parallelepiped\" regions. This\napproach also facilitates a unified and streamlined formulation and proof of\nour results. Finally, we apply these formulas to provide period interpretations\nof the \"Kronecker terms\" of Dedekind-Weil zeta functions and Dirichlet\n$L$-functions associated with ring and ray class characters.",
        "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
        "The Log Structured Merge (LSM) Tree is a popular choice for key-value stores\nthat focus on optimized write throughput while maintaining performant,\nproduction-ready read latencies. To optimize read performance, LSM stores rely\non a probabilistic data structure called the Bloom Filter (BF). In this paper,\nwe focus on adversarial workloads that lead to a sharp degradation in read\nperformance by impacting the accuracy of BFs used within the LSM store. Our\nevaluation shows up to $800\\%$ increase in the read latency of lookups for\npopular LSM stores. We define adversarial models and security definitions for\nLSM stores. We implement adversary resilience into two popular LSM stores,\nLevelDB and RocksDB. We use our implementations to demonstrate how performance\ndegradation under adversarial workloads can be mitigated.",
        "Due to the inhomogeneity of electron number density, radio waves emitted by\npulsars undergo scattering as they pass through the interstellar medium (ISM).\nHowever, a connection between large-scale pulsar scattering data and the\nstructure of the Galactic ISM has yet to be established. In this paper, we\nexplore the capability of pulsar scattering time data in discovering structures\nin the ISM. Using a large dataset of scattering time measurements for 473\npulsars, we fit the pulsar reduced scattering intensity as a function of\nGalactic latitude and distance, constructing a smooth model of the Galactic\npulsar scattering distribution. By comparing this smooth distribution with\nobservational data, we identify two ISM structures responsible for pulsar\nscattering, one is associated with the Vela supernova remnant region within the\nGum Nebula, while the other is a newly discovered structure -- a distant\nsuperbubble, G38, located at a distance of 2.3 kpc with a size of ~50 pc.\nAnalysis of the correlation coefficient of the pulsar scattering distribution\nshows that the correlation is dominated by structures smaller than 0.15 kpc --\nthe closest separation approachable by the current dataset. As measurements of\nthe pulsar scattering time continue to increase in the future, they can\npotentially become an independent tool for exploring structures in the ISM.",
        "The report of superconductivity (SC) with Tc~80 K in bilayer\nRuddlesden-Popper (RP) nickelate La3Ni2O7-delta have sparked considerable\ninvestigations on its normal state properties and SC mechanism under pressure\nand at low temperature. It is believed that the density wave (DW) at ~150 K\nplays an important role in SC emergence, but its nature remains largely\nunderexplored. Here, we utilized temperature-dependent in-situ Ni K-edge X-ray\nAbsorption Near-edge Spectroscopy (XANES) to probe the Ni-3d\/4p electronic\nstates of La3Ni2O7-delta and La2PrNi2O7-delta samples down to 4.8 K, enabling\nus to witness the evolution of both in-plane d_(x^2-y^2)\/p_x (p_y) and\nout-of-plane d_(3z^2-r^2)\/p_z orbitals of NiO6 octahedron across the DW\ntransition. Main edge energy associated with Ni 4p orbital shows an anomalous\ndecline near DW transition, signifying the occurrence of lattice distortions as\na hallmark of charge density wave. Below DW transition, the enlarged crystal\nfield splitting (CFS) indicates an enhanced NiO6 octahedral distortion.\nIntriguingly, magnetic Pr substituents could activate the mutual interplay of\nd_(x^2-y^2) and d_(3z^2-r^2) orbitals. We discussed its relevance to the\nfavored bulk SC in the pressurized polycrystalline La2PrNi2O7-delta than\npristine."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"An agent-based spatial urban social network generator: A case study of beijing, china",
    "start_abstract":"This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective.",
    "start_categories":[
      "cs.CE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Meeting Strangers and Friends of Friends: How Random Are Social Networks?"
      ],
      "abstract":[
        "We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)"
      ],
      "categories":[
        "q-fin.EC"
      ]
    },
    "list":{
      "title":[
        "Reducing Circuit Depth in Quantum State Preparation for Quantum\n  Simulation Using Measurements and Feedforward",
        "Insights from leptohadronic modelling of the brightest blazar flare",
        "Ultra-cold neutrons in qBounce experiments as laboratory for test of\n  chameleon field theories and cosmic acceleration",
        "Twenty years of Ne\\v{s}et\\v{r}il's classification programme of Ramsey\n  classes",
        "The algebraic and geometric classification of Jordan superalgebras",
        "Flipped Rotating Axion Non-minimally Coupled to Gravity: Baryogenesis\n  and Dark Matter",
        "Euclid Quick Data Release (Q1). The Strong Lensing Discovery Engine D --\n  Double-source-plane lens candidates",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Can supermassive stars form in protogalaxies due to internal\n  Lyman-Werner feedback?",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Elasticity of a Freely Jointed Chain with Quenched Disorder",
        "Robust Cislunar Low-Thrust Trajectory Optimization under Uncertainties\n  via Sequential Covariance Steering",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "Quantile-Based Randomized Kaczmarz for Corrupted Tensor Linear Systems",
        "Free Perpetuities I: Existence, Subordination and Tail Asymptotics",
        "Coupled Hierarchical Structure Learning using Tree-Wasserstein Distance",
        "Gravitational lensing and shadows in the toron solution of Einstein's\n  equations using ray tracing methods",
        "The Thermodynamic Cost of Ignorance: Thermal State Preparation with One\n  Ancilla Qubit",
        "The external version of a subclassical logic",
        "Magnetic moment of electrons in systems with spin-orbit coupling",
        "Graph-Theoretic Analysis of $n$-Replica Time Evolution in the Brownian\n  Gaussian Unitary Ensemble",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "On finitely many base $q$ expansions",
        "Polarization agnostic continuous variable quantum key distribution",
        "Broadband Absorption in Cadmium Telluride Thin-Film Solar Cells via\n  Composite Light Trapping Techniques",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "A Detection of Circumgalactic Dust at Megaparsec Scales with Maximum\n  Likelihood Estimation",
        "A quantum monomer-dimer model on Penrose tilings",
        "Phase space geometry of collective spin systems: Scaling and Fractality"
      ],
      "abstract":[
        "Reducing circuit depth and identifying an optimal trade-off between circuit\ndepth and width is crucial for successful quantum computation. In this context,\nmid-circuit measurement and feedforward have been shown to significantly reduce\nthe depth of quantum circuits, particularly in implementing logical gates. By\nleveraging these techniques, we propose several parallelization strategies that\nreduce quantum circuit depth at the expense of increasing width in preparing\nvarious quantum states relevant to quantum simulation. With measurements and\nfeedforward, we demonstrate that utilizing unary encoding as a bridge between\ntwo quantum states substantially reduces the circuit depth required for\npreparing quantum states, such as sparse quantum states and sums of Slater\ndeterminants within the first quantization framework, while maintaining an\nefficient circuit width. Additionally, we show that a coordinate Bethe ansatz,\ncharacterized by its high degree of freedom in its phase, can be\nprobabilistically prepared in a constant-depth quantum circuit using\nmeasurements and feedforward. We anticipate that our study will contribute to\nthe reduction of circuit depth in initial state preparation, particularly for\nquantum simulation, which is a critical step toward achieving quantum\nadvantage.",
        "The blazar 3C 454.3 experienced a major flare in November 2010 making it the\nbrightest $\\gamma$-ray source in the sky of the Fermi-LAT. We obtain seven\ndaily consecutive spectral-energy distributions (SEDs) of the flare in the\ninfra-red, optical, ultra-violet, X-ray and $\\gamma$-ray bands with publicly\navailable data. We simulate the physical conditions in the blazar and show that\nthe observed SEDs are well reproduced in the framework of a \"standing feature\"\nwhere the position of the emitting region is almost stationary, located beyond\nthe outer radius of the broad-line region and into which fresh blobs of\nrelativistically moving magnetized plasma are continuously injected. Meanwhile,\na model with a single \"moving blob\" does not describe the data well. We obtain\na robust upper limit to the amount of high-energy protons in the jet of 3C\n454.3 from the electromagnetic SED. We construct a neutrino light curve of 3C\n454.3 and estimate the expected neutrino yield at energies $\\geq 100$ TeV for\n3C 454.3 to be up to $6 \\times 10^{-3}$ $\\nu_{\\mu}$ per year. Finally, we\nextrapolate our model findings to the light curves of all Fermi-LAT\nflat-spectrum radio quasars. We find that next-generation neutrino telescopes\nare expected to detect approximately one multimessenger ($\\gamma + \\nu_{\\mu}$)\nflare per year from bright blazars with neutrino peak energy in the hundreds\nTeV -- hundreds PeV energy range and show that the electromagnetic flare peak\ncan precede the neutrino arrival by months to years.",
        "The accelerating expansion of the Universe, attributed to dark energy, has\nspurred interest in theories involving scalar fields such as chameleon field\ntheories. These fields, which couple to matter with density-dependent effective\nmass, offer a promising explanation for cosmic acceleration. Experiments\nleveraging ultra-cold neutrons (UCNs) provide an innovative approach to testing\nthese theories. The existence of a chameleon field, being responsible for the\ncurrent phase of cosmic acceleration, is investigated by analysing a free fall\nof ultra-cold neutrons from the gap between two mirrors after their bouncing\nbetween these two mirrors. We analyse a deformation of the wave functions of\nthe quantum gravitational states of ultra-cold neutrons, induced by a chameleon\nfield, and find a new upper bound $\\beta\\leq6.5\\times10^8$ on the\nchameleon-matter coupling constant $\\beta$ from the unitarity condition. This\nresult refines previous estimates and highlights the potential of ultra-cold\nneutron experiments as laboratories for exploring scalar field theories and\nfundamental physics.",
        "In the 1970s, structural Ramsey theory emerged as a new branch of\ncombinatorics. This development came with the isolation of the concepts of the\n$\\mathbf{A}$-Ramsey property and Ramsey class. Following the influential\nNe\\v{s}et\\v{r}il-R\\\"{o}dl theorem, several Ramsey classes have been identified.\nIn the 1980s Ne\\v{s}et\\v{r}il, inspired by a seminar of Lachlan, discovered a\ncrucial connection between Ramsey classes and Fra\\\"{\\i}ss\\'{e} classes and, in\nhis 1989 paper, connected the classification programme of homogeneous\nstructures to structural Ramsey theory. In 2005, Kechris, Pestov, and\nTodor\\v{c}evi\\'{c} revitalized the field by connecting Ramsey classes to\ntopological dynamics. This breakthrough motivated Ne\\v{s}et\\v{r}il to propose a\nprogram for classifying Ramsey classes. We review the progress made on this\nprogram in the past two decades, list open problems, and discuss recent\nextensions to new areas, namely the extension property for partial\nautomorphisms (EPPA), and big Ramsey structures.",
        "We give the algebraic and geometric classification of complex\nfour-dimensional Jordan superalgebras. In particular, we describe all\nirreducible components in the corresponding varieties.",
        "We demonstrate that the co-genesis of baryon asymmetry and dark matter can be\nachieved through the rotation of an axion-like particle, driven by a flip in\nthe vacuum manifold's direction at the end of inflation. This can occur if the\naxion has a periodic non-minimal coupling to gravity, while preserving the\ndiscrete shift symmetry. In non-oscillating inflation models, after inflation\nthere is typically a period of kination (with $w = 1$). In this case, it is\nshown that the vacuum manifold of the axion is flipped and the axion begins\nrotating in field space, because it can slide across the decreasing potential\nbarrier as in Ricci reheating. Such a rotating axion can generate the baryon\nasymmetry of the Universe through spontaneous baryogenesis, while at later\nepochs it can oscillate as dark matter. The period of kination makes the\nprimordial gravitational waves (GW) generated during inflation sharply\nblue-tilted which constrains the parameter space due to GW overproduction,\nwhile being testable by next generation CMB experiments. As a concrete example,\nwe show that such a cogenesis of baryon asymmetry and dark matter can be\nrealized for the axion as the Majoron in the Type-I seesaw setup, predicting\nmass ranges for the Majoron below sub eVs, with right-handed neutrino mass\nabove $\\mathcal{O}(10^{8})$ GeV. We also show that in order to avoid\nfragmentation of the axion condensate during the rotation, we require the\nnon-minimal coupling \\mbox{$\\xi \\sim (f\/m_P)^2 $} or somewhat larger, where $f$\nis the axion decay constant.",
        "Strong gravitational lensing systems with multiple source planes are powerful\ntools for probing the density profiles and dark matter substructure of the\ngalaxies. The ratio of Einstein radii is related to the dark energy equation of\nstate through the cosmological scaling factor $\\beta$. However, galaxy-scale\ndouble-source-plane lenses (DSPLs) are extremely rare. In this paper, we report\nthe discovery of four new galaxy-scale double-source-plane lens candidates in\nthe Euclid Quick Release 1 (Q1) data. These systems were initially identified\nthrough a combination of machine learning lens-finding models and subsequent\nvisual inspection from citizens and experts. We apply the widely-used {\\tt\nLensPop} lens forecasting model to predict that the full \\Euclid survey will\ndiscover 1700 DSPLs, which scales to $6 \\pm 3$ DSPLs in 63 deg$^2$, the area of\nQ1. The number of discoveries in this work is broadly consistent with this\nforecast. We present lens models for each DSPL and infer their $\\beta$ values.\nOur initial Q1 sample demonstrates the promise of \\Euclid to discover such rare\nobjects.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "Population III stars are possible precursors to early massive and\nsupermassive black holes (BHs). The presence of soft UV Lyman Werner (LW)\nbackground radiation can suppress Population III star formation in minihalos\nand allow them to form in pristine atomic cooling halos. In the absence of\nmolecular hydrogen ($\\rm H_2$) cooling, atomic-cooling halos enable rapid\ncollapse with suppressed fragmentation. High background LW fluxes from\npreceding star-formation have been proposed to dissociate $\\rm H_2$. This flux\ncan be supplemented by LW radiation from one or more Population III star(s) in\nthe same halo, reducing the necessary background level. Here we consider\natomic-cooling halos in which multiple protostellar cores form close to one\nanother nearly simultaneously. We assess whether the first star's LW radiation\ncan dissociate nearby $\\rm H_2$, enabling the prompt formation of a second,\nsupermassive star (SMS) from warm, atomically-cooled gas. We use a set of\nhydrodynamical simulations with the code ENZO, with identical LW backgrounds\ncentered on a halo with two adjacent collapsing gas clumps. When an additional\nlarge local LW flux is introduced, we observe immediate reductions in both the\naccretion rates and the stellar masses that form within these clumps. While the\nLW flux reduces the $\\text{H}_2$ fraction and increases the gas temperature,\nthe halo core's potential well is too shallow to promptly heat the gas to\n$\\gtrsim$ 1000 K and increase the accretion rate onto the second protostar. We\nconclude that internal LW feedback inside atomic-cooling halos is unlikely to\nfacilitate the formation of SMSs or massive BH seeds.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a simple theoretical model, the Freely Jointed Chain with\nquenched hinges (qFJC), which captures the quenched disorder in the local\nbending stiffness of the polymer. In this article, we analyze the tensile\nelasticity of the qFJC in the Gibbs (fixed-force) ensemble. For finite-size\nsystems, we obtain a recurrence relation of the exact free energy, which allows\nus to calculate the exact force-extension relation numerically for an arbitrary\nsize of the system. In the thermodynamic limit, when $L({\\rm contour\n\\;length})\\gg L_p({\\rm persistence \\;length})$, we obtain a framework to deal\nwith quenched disorder in the polymer configuration. This allows us to obtain\nthe response function for the discrete and continuous qFJC in the thermodynamic\nlimit. It turns out that the extension of the continuous qFJC can be cast in a\nsimple form. Furthermore, we have applied our analysis to rod-coil multiblock\ncopolymers.",
        "Spacecraft operations are influenced by uncertainties such as dynamics\nmodeling, navigation, and maneuver execution errors. Although mission design\nhas traditionally incorporated heuristic safety margins to mitigate the effect\nof uncertainties, particularly before\/after crucial events, it is yet unclear\nwhether this practice will scale in the cislunar region, which features locally\nchaotic nonlinear dynamics and involves frequent lunar flybys. This paper\napplies chance-constrained covariance steering and sequential convex\nprogramming to simultaneously design an optimal trajectory and trajectory\ncorrection policy that can probabilistically guarantee safety constraints under\nthe assumed physical\/navigational error models. The results show that the\nproposed method can effectively control the state uncertainty in a highly\nnonlinear environment and provide a trajectory with better local stability\nproperties than a trajectory designed without considering uncertainties. The\nframework allows faster computation and lossless covariance propagation\ncompared to existing methods, enabling a rapid and accurate comparison of\n$\\Delta V_{99}$ costs for different uncertainty parameters. We demonstrate the\nalgorithm on several transfers in the Earth-Moon Circular Restricted Three Body\nProblem.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "The reconstruction of tensor-valued signals from corrupted measurements,\nknown as tensor regression, has become essential in many multi-modal\napplications such as hyperspectral image reconstruction and medical imaging. In\nthis work, we address the tensor linear system problem $\\mathcal{A}\n\\mathcal{X}=\\mathcal{B}$, where $\\mathcal{A}$ is a measurement operator,\n$\\mathcal{X}$ is the unknown tensor-valued signal, and $\\mathcal{B}$ contains\nthe measurements, possibly corrupted by arbitrary errors. Such corruption is\ncommon in large-scale tensor data, where transmission, sensory, or storage\nerrors are rare per instance but likely over the entire dataset and may be\narbitrarily large in magnitude. We extend the Kaczmarz method, a popular\niterative algorithm for solving large linear systems, to develop a Quantile\nTensor Randomized Kaczmarz (QTRK) method robust to large, sparse corruptions in\nthe observations $\\mathcal{B}$. This approach combines the tensor Kaczmarz\nframework with quantile-based statistics, allowing it to mitigate adversarial\ncorruptions and improve convergence reliability. We also propose and discuss\nthe Masked Quantile Randomized Kaczmarz (mQTRK) variant, which selectively\napplies partial updates to handle corruptions further. We present convergence\nguarantees, discuss the advantages and disadvantages of our approaches, and\ndemonstrate the effectiveness of our methods through experiments, including an\napplication for video deblurring.",
        "We study the free analogue of the classical affine fixed-point (or\nperpetuity) equation\n  \\[\n  \\mathbb{X} \\stackrel{d}{=} \\mathbb{A}^{1\/2}\\mathbb{X}\\,\\mathbb{A}^{1\/2} +\n\\mathbb{B},\n  \\] where $\\mathbb{X}$ is assumed to be $*$-free from the pair\n$(\\mathbb{A},\\mathbb{B})$, with $\\mathbb{A}\\ge 0$ and\n$\\mathbb{B}=\\mathbb{B}^*$. Our analysis covers both the subcritical regime,\nwhere $\\tau(\\mathbb{A})<1$, and the critical case $\\tau(\\mathbb{A})=1$, in\nwhich the solution $\\mathbb{X}$ is necessarily unbounded. When\n$\\tau(\\mathbb{A})=1$, we prove that the series defining $\\mathbb{X}$ converges\nbilaterally almost uniformly (and almost uniformly under additional tail\nassumptions), while the perpetuity fails to have higher moments even if all\nmoments of $\\mathbb{A}$ and $\\mathbb{B}$ exist.\n  Our approach relies on a detailed study of the asymptotic behavior of moments\nunder free multiplicative convolution, which reveals a markedly different\nbehavior from the classical setting. By employing subordination techniques for\nnon-commutative random variables, we derive precise asymptotic estimates for\nthe tail of the distributions of $\\mathbb{X}$ in both one-sided and symmetric\ncases. Interestingly, in the critical case, the free perpetuity exhibits a\npower-law tail behavior that mirrors the phenomenon observed in the celebrated\nKesten's theorem.",
        "In many applications, both data samples and features have underlying\nhierarchical structures. However, existing methods for learning these latent\nstructures typically focus on either samples or features, ignoring possible\ncoupling between them. In this paper, we introduce a coupled hierarchical\nstructure learning method using tree-Wasserstein distance (TWD). Our method\njointly computes TWDs for samples and features, representing their latent\nhierarchies as trees. We propose an iterative, unsupervised procedure to build\nthese sample and feature trees based on diffusion geometry, hyperbolic\ngeometry, and wavelet filters. We show that this iterative procedure converges\nand empirically improves the quality of the constructed trees. The method is\nalso computationally efficient and scales well in high-dimensional settings.\nOur method can be seamlessly integrated with hyperbolic graph convolutional\nnetworks (HGCN). We demonstrate that our method outperforms competing\napproaches in sparse approximation and unsupervised Wasserstein distance\nlearning on several word-document and single-cell RNA-sequencing datasets. In\naddition, integrating our method into HGCN enhances performance in link\nprediction and node classification tasks.",
        "We present a numerical and analytical study of the so-called `toron' solution\nof the stationary axisymmetric Einstein equations in vacuum expressed in terms\nof elliptic functions. The asymptotic behavior of this solution coincides with\nthe one of the NUT solution, i.e., it has a `gravimagnetic' mass known as the\nNUT parameter while the ordinary mass vanishes. The physical properties of this\nspacetime are studied via ray tracing. The results are compared to known\ngeodesic flows in Schwarzschild, Kerr and NUT spacetimes to discuss\nsimilarities and differences, with a particular emphasis on the comparison of\nNUT and toron spacetimes.",
        "In this work we investigate a model of thermalization wherein a single\nancillary qubit randomly interacts with the system to be thermalized. This not\nonly sheds light on the emergence of Gibbs states in nature, but also provides\na routine for preparing arbitrary thermal states on a digital quantum computer.\nFor desired $\\beta$ and random interaction $G$ the routine boils down to time\nindependent Hamiltonian simulation and is represented by the channel $\\Phi :\n\\rho \\mapsto \\mathbb{E}_G {\\rm Tr}_{\\rm Env} \\left[ e^{-i(H + \\alpha G)t}\n\\left(\\rho \\otimes \\frac{e^{-\\beta H_E}}{\\mathcal{Z}}\\right) e^{i (H + \\alpha\nG)t} \\right]$. We rigorously prove that these dynamics reduce to a Markov chain\nprocess in the weak-coupling regime with the thermal state as the approximate\nfixed point. We upper bound the total simulation time required in terms of the\nMarkov chain spectral gap $\\lambda_\\star$, which we compute exactly in the\nground state limit. These results are independent of any eigenvalue knowledge\nof the system, but we are further able to show that with knowledge of\neigenvalue differences $\\lambda_S(i) - \\lambda_S(j)$, then the total simulation\ntime is dramatically reduced. The ratio of the complete ignorance simulation\ncost to the perfect knowledge simulation cost scales as $\\widetilde{O}\n\\left({\\frac{\\|{H_S}\\|^7}{\\delta_{\\rm min}^7 \\epsilon^{3.5}\n\\lambda_\\star(\\beta)^{3.5}}}\\right)$, where $\\delta_{\\min}$ is related to the\neigenvalue differences of the system. Additionally, we provide more specific\nresults for single qubit and harmonic oscillator systems as well as numeric\nexperiments with hydrogen chains. In addition to the algorithmic merits, these\nresults can be viewed as broad extensions of the Repeated Interactions model to\ngeneric Hamiltonians with unknown interactions, giving a complete picture of\nthe thermalization process for quantum systems.",
        "A three-valued logic L is subclassical when it is defined by a single matrix\nhaving the classical two-element matrix as a subreduct. In this case, the\nlanguage of L can be expanded with special unary connectives, called external\noperators. The resulting logic L^e is the external version of L, a notion\noriginally introduced by D. Bochvar in 1938 with respect to his weak Kleene\nlogic. In this paper we study the semantic properties of the external version\nof a three-valued subclassical logic L. We determine sufficient and necessary\nconditions to turn a model of L into a model of L^e . Moreover, we establish\nsome distinctive semantic properties of L^e.",
        "Magnetic effects originating from spin-orbit coupling (SOC) have been\nattracting major attention. However, SOC contributions to the electron magnetic\nmoment operator are conventionally disregarded. In this work, we analyze\nrelativistic contributions to the latter operator, including those of the\nSOC-type: in vacuum, for the semiconductor 8 band Kane model, and for an\narbitrary system with two spectral branches. In this endeavor, we introduce a\nnotion of relativistic corrections to the operation\n$\\partial\/\\partial\\boldsymbol B$, where $\\boldsymbol B$ is an external magnetic\nfield. We highlight the difference between the magnetic moment and $-\\partial\nH\/\\partial\\boldsymbol B$, where $H$ is the system Hamiltonian. We suggest to\ncall this difference the abnormal magnetic moment. We demonstrate that the\nconventional splitting of the total magnetic moment into the spin and orbital\nparts becomes ambiguous when relativistic corrections are taken into account.\nThe latter also jeopardize the ``modern theory of orbital magnetization'' in\nits standard formulation. We derive a linear response Kubo formula for the\nkinetic magnetoelectric effect projected to individual branches of a two branch\nsystem. This allows us, in particular, to identify a source of this effect that\nstems from noncommutation of the position and $\\partial\/\\partial\\boldsymbol B$\noperators' components. This is an analog of the contribution to the Hall\nconductivity from noncommuting components of the position operator. We also\nreport several additional observations related to the electron magnetic moment\noperator in systems with SOC and other relativistic corrections.",
        "In this paper, we investigate the $n$-replica time evolution operator\n$\\mathcal{U}_n(t)\\equiv e^{\\mathcal{L}_nt} $ for the Brownian Gaussian Unitary\nEnsemble (BGUE) using a graph-theoretic approach. We examine the moments of the\ngenerating operator $\\mathcal{L}_n$, which governs the Euclidean time evolution\nwithin an auxiliary $D^{2n}$-dimensional Hilbert space, where $D$ represents\nthe dimension of the Hilbert space for the original system. Explicit\nrepresentations for the cases of $n = 2$ and $n = 3$ are derived, emphasizing\nthe role of graph categorization in simplifying calculations. Furthermore, we\npresent a general approach to streamline the calculation of time evolution for\narbitrary $n$, supported by a detailed example of $n = 4$. Our results\ndemonstrate that the $n$-replica framework not only facilitates the evaluation\nof various observables but also provides valuable insights into the\nrelationship between Brownian disordered systems and quantum information\ntheory.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "Given some integer $m \\geq 3$, we find the first explicit collection of\ncountably many intervals in $(1,2)$ such that for any $q$ in one of these\nintervals, the set of points with exactly $m$ base $q$ expansions is nonempty\nand moreover has positive Hausdorff dimension. Our method relies on an\napplication of a theorem proved by Falconer and Yavicoli, which guarantees that\nthe intersection of a family of compact subsets of $\\mathbb{R}^d$ has positive\nHausdorff dimension under certain conditions.",
        "We introduce a polarization agnostic method for Gaussian-modulated coherent\nstate (GCMS) continuous-variable quantum key distribution (CVQKD). Due to the\nrandom and continuous nature of the GCMS protocol, Alice, the transmitter, can\nencode two distinct quadratures in each of two orthogonal polarization modes,\nsuch that Bob, the receiver, measures valid GCMS quadratures in a single\npolarization mode even when polarization changes occur during transmission.\nThis method does not require polarization correction in the optical domain,\ndoes not require monitoring both polarization modes, reduces loss by\neliminating optical components, and avoids the noise injected by polarization\ncorrection algorithms.",
        "Composite light-trapping structures offer a promising approach to achieving\nbroadband absorption and high efficiency in thin-film solar cells (TFSCs) in\norder to accelerate sustainable energy solutions. As the leading material in\nthin-film solar technology, cadmium telluride (CdTe) faces challenges from\nsurface reflective losses across the solar spectrum and weak absorption in the\nnear-infrared (NIR) range. This computational study addresses these limitations\nby employing a dual light trapping technique: the top surfaces of both the CdS\nand CdTe layers are tapered as nanocones (NCs), while germanium (Ge) spherical\nnanoparticles (NPs) are embedded within the CdTe absorber layer to enhance\nbroadband absorption. Numerical simulations using Finite-Difference Time Domain\n(FDTD) and other methods are used to optimize the parameters and configurations\nof both nanostructures, aiming to achieve peak optoelectronic performance. The\nresults show that a short-circuit current density ($J_{sc}$) of 35.38 mA\/$cm^2$\nand a power conversion efficiency (PCE) of 27.76% can be achieved with optimal\nnanocone (NC) texturing and spherical Ge nanoparticle (NP) configurations, a\n45.45% and 80.72% increase compared to baseline structure in $J_{sc}$ and PCE\nrespectively. To understand the enhancement mechanisms, the study includes\nanalyses using diffraction grating theory and Mie theory. Fabricability of\nthese structures is also evaluated. Furthermore, an additional study on the\neffects of incident angle variation and polarization change demonstrates that\nthe optimal structure is robust under practical conditions, maintaining\nconsistent performance.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "One of the more surprising astrophysical discoveries of the last decade has\nbeen the presence of enormous quantities of dust at megaparsec distances from\ngalaxies, which has important implications for galaxy evolution, the\ncircumgalactic and intergalactic medium, and observational cosmology. In this\nwork, we present a novel method for studying these vast halos of circumgalactic\ndust: a maximum-likelihood estimator for dust-induced extinction of background\ngalaxies. This estimator can accommodate a broad range of archival photometric\ndata and can incorporate different dust reddening prescriptions, making it\napplicable to diverse galaxy types and redshifts. We apply the estimator to the\nredMaGiC catalog of luminous red galaxies, selected for their tight dispersion\nin color and well-constrained photometric redshifts, and measure the resulting\nextinction as a function of projected distance from WISExSuperCOSMOS and\nredMaGiC foreground galaxies. We detect significant dust-induced extinction\nprofiles extending to at least 1 megaparsec from galactic disks, with\nnoticeable differences between star-forming and quiescent galaxies:\nstar-forming galaxies exhibit a pronounced rise in extinction within the inner\n50 kiloparsecs and a steep decline beyond 1 megaparsec, while the quiescent\ngalaxies host little dust in the inner halo but have detectable extinction out\nto 30 megaparsecs. We test the robustness of our results using star catalogs\nand inverted foreground and background samples and find no evidence for\nsignificant systematic error. Our approach provides a powerful tool for\nstudying the interplay between circumgalactic dust, galaxy evolution, and\nlarge-scale structure, with potential applications in a number of astrophysical\nsubfields.",
        "We define a quantum monomer-dimer model in the space of maximal dimer\ncoverings of quasicrystalline Penrose tilings. Since Penrose tilings do not\nadmit perfect dimer coverings, as shown by F. Flicker et al., PRX 10, 011005\n(2020), monomers are necessarily present in our model. The model features a\nfrustration-free Rokhsar-Kivelson (RK) point where the ground state is a\nuniform superposition of all the exponentially many maximal dimer coverings,\ndespite the presence of a finite density of monomers. We map our model to a\n$\\mathbb{Z}_2$ gauge theory with matter and calculate various correlators to\ncharacterize the phase of the system at the RK point using classical Monte\nCarlo calculations. Specifically, we compute the dimer-dimer and vison-vison\ncorrelators, as well as open Wilson lines and closed Wilson loops corresponding\nto the monomers and the visons. We find that both the dimer-dimer and\nvison-vison correlators decay exponentially with distance. The open Wilson\nlines and closed Wilson loops decay exponentially with the same correlation\nlength, indicating that the gauge theory is in the confined phase, which\nimplies that the system is likely in an ordered phase.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network",
    "start_abstract":"Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice"
      ],
      "abstract":[
        "Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073"
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "A variant of \\v{S}emrl's preserver theorem for singular matrices",
        "Optimal domain of Volterra operators in Korenblum spaces",
        "On general versions of the Petty projection inequality",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "Representation Theorems for Convex Expectations and Semigroups on Path\n  Space",
        "Homotopical Entropy",
        "Metastability and Ostwald Step Rule in the Crystallisation of Diamond\n  and Graphite from Molten Carbon",
        "Multiwavelength Variability Analysis of the Blazar PKS 0727-11: A\n  $\\sim$168 Days Quasi-periodic Oscillation in Gamma-ray",
        "Chirality, Nonreciprocity and Symmetries for a Giant Atom",
        "The period-index problem for hyper-K\\\"ahler varieties via\n  hyperholomorphic bundles",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Benchmarking ANN extrapolations of the ground-state energies and radii\n  of Li isotopes",
        "On the Curvature and Topology of Compact Stationary Spacetimes",
        "Global Convergence and Rate Analysis of the Steepest Descent Method for\n  Uncertain Multiobjective Optimization via a Robust Optimization Approach",
        "Topological Invariants in Invasion Percolation",
        "A refined functorial universal tangle invariant",
        "Mosaic-skeleton approximation is all you need for Smoluchowski equations",
        "Searching for single production of vector-like quarks decaying into $Wb$\n  at a future muon-proton collider",
        "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints",
        "Measurement-based Simulation of Geometric Gates in Topological Qubits on\n  NISQ Devices",
        "The Complex Magnetic Field of the Extreme Galactic Center: PRIMA Science\n  Potential",
        "Dynamical behavior and bifurcation analysis for a theoretical model of\n  dengue fever transmission with incubation period and delayed recovery",
        "Facial structure of copositive and completely positive cones over a\n  second-order cone",
        "KLiNQ: Knowledge Distillation-Assisted Lightweight Neural Network for\n  Qubit Readout on FPGA",
        "$B\\to K\\sf{+}invisible$, dark matter, and $CP$ violation in hyperon\n  decays",
        "Chern numbers on positive vector bundles and combinatorics",
        "On the $L_2$-discrepancy of Latin hypercubes",
        "Quantum Cosmology and the Age of the Universe",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery"
      ],
      "abstract":[
        "For positive integers $1 \\leq k \\leq n$ let $M_n$ be the algebra of all $n\n\\times n$ complex matrices and $M_n^{\\le k}$ its subset consisting of all\nmatrices of rank at most $k$. We first show that whenever $k>\\frac{n}{2}$, any\ncontinuous spectrum-shrinking map $\\phi : M_n^{\\le k} \\to M_n$ (i.e.\n$\\mathrm{sp}(\\phi(X)) \\subseteq \\mathrm{sp}(X)$ for all $X \\in M_n^{\\le k}$)\neither preserves characteristic polynomials or takes only nilpotent values.\nMoreover, for any $k$ there exists a real analytic embedding of $M_n^{\\le k}$\ninto the space of $n\\times n$ nilpotent matrices for all sufficiently large\n$n$. This phenomenon cannot occur when $\\phi$ is injective and either $k > n -\n\\sqrt{n}$ or the image of $\\phi$ is contained in $M_n^{\\le k}$. We then\nestablish a main result of the paper -- a variant of \\v{S}emrl's preserver\ntheorem for $M_n^{\\le k}$: if $n \\geq 3$, any injective continuous map $\\phi\n:M_n^{\\le k} \\to M_n^{\\le k}$ that preserves commutativity and shrinks spectrum\nis of the form $\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$,\nfor some invertible matrix $T\\in M_n$. Moreover, when $k=n-1$, which\ncorresponds to the set of singular $n\\times n$ matrices, this result extends to\nmaps $\\phi$ which take values in $M_n$. Finally, we discuss the\nindispensability of assumptions in our main result.",
        "The aim of this article is to study the largest domain space $[T,X]$,\nwhenever it exists, of a given continuous linear operator $T\\colon X\\to X$,\nwhere $X\\subseteq H(\\mathbb{D})$ is a Banach space of analytic functions on the\nopen unit disc $\\mathbb{D}\\subseteq \\mathbb{C}$. That is, $[T,X]\\subseteq\nH(\\mathbb{D})$ is the \\textit{largest} Banach space of analytic functions\ncontaining $X$ to which $T$ has a continuous, linear, $X$-valued extension\n$T\\colon [T,X]\\to X$. The class of operators considered consists of generalized\nVolterra operators $T$ acting in the Korenblum growth Banach spaces\n$X:=A^{-\\gamma}$, for $\\gamma>0$. Previous studies dealt with the classical\nCes\\`aro operator $T:=C$ acting in the Hardy spaces $H^p$, $1\\leq p<\\infty$,\n\\cite{CR}, \\cite{CR1}, in $A^{-\\gamma}$, \\cite{ABR-R}, and more recently,\ngeneralized Volterra operators $T$ acting in $X:=H^p$, \\cite{BDNS}.",
        "The classical Petty projection inequality is an affine isoperimetric\ninequality which constitutes a cornerstone in the affine geometry of convex\nbodies. By extending the polar projection body to an inter-dimensional\noperator, Petty's inequality was generalized to the so-called $(L_p,Q)$\nsetting, where $Q$ is an $m$-dimensional compact convex set. In this work, we\nfurther extend the $(L_p,Q)$ Petty projection inequality to the broader realm\nof rotationally invariant measures with concavity properties, namely, those\nwith $\\gamma$-concave density (for $\\gamma\\geq-1\/nm$). Moreover, when $p=1$,\nand motivated by a contemporary empirical reinterpretation of Petty's result,\nwe explore empirical analogues of this inequality.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
        "We present a \"homotopification\" of fundamental concepts from information\ntheory. Using homotopy type theory, we define homotopy types that behave\nanalogously to probability spaces, random variables, and the exponentials of\nShannon entropy and relative entropy. The original analytic theories emerge\nthrough homotopy cardinality, which maps homotopy types to real numbers and\ngeneralizes the cardinality of sets.",
        "The crystallisation of carbon from the melt under extreme conditions is\nhighly relevant to earth and planetary science, materials manufacturing, and\nnuclear fusion research. The thermodynamic conditions near the\ngraphite-diamond-liquid (GDL) triple point are especially of interest for\ngeological and technological applications, but high-pressure flash heating\nexperiments aiming to resolve this region of the phase diagram of carbon\nexhibit large discrepancies. Experimental challenges are often related to the\npersistence of metastable crystalline or glassy phases, superheated crystals,\nor supercooled liquids. A deeper understanding of the crystallisation kinetics\nof diamond and graphite is crucial for effectively interpreting the outcomes of\nthese experiments. Here, we reveal the microscopic mechanisms of diamond and\ngraphite nucleation from liquid carbon through molecular simulations with\nfirst-principles machine learning potentials. Our simulations accurately\nreproduce the experimental phase diagram of carbon in the region around the GDL\ntriple point and show that liquid carbon crystallises spontaneously upon\ncooling at constant pressure. Surprisingly, metastable graphite crystallises in\nthe domain of diamond thermodynamic stability at pressures above the triple\npoint. Furthermore, whereas diamond crystallises through a classical nucleation\npathway, graphite follows a two-step process in which low-density fluctuations\nforego ordering. Calculations of the nucleation rates of the two competing\nphases confirm this result and reveal a manifestation of Ostwald's step rule\nwhere the strong metastability of graphite hinders the transformation to the\nstable diamond phase. Our results provide a new key to interpreting melting and\nrecrystallisation experiments and shed light on nucleation kinetics in\npolymorphic materials with deep metastable states.",
        "We performed variability analysis of the multiwavelength light curves for the\nflat-spectrum radio quasar PKS 0727-11. Using the generalized Lomb-Scargle\nperiodogram, we identified a possible quasi-periodic oscillation (QPO) of\n$\\sim$ 168.6 days (persisted for 6 cycles, with a significance of $3.8\\sigma$)\nin the gamma-ray light curve during the flare period (MJD 54687-55738). It is\nthe first time that periodic variations have been detected in this source, and\nfurther supported by other methods: weighted wavelet $z$-transform, phase\ndispersion minimization, REDFIT, autoregressive integrated moving average\nmodel, and structure function analysis. Cross-correlation analysis shows that\nthere is a strong correlation between multi-band light variations, indicating\nthat gamma-ray and radio flares may originate from the same disturbance, and\nthe distance between the emission regions of gamma-ray and radio flares is\ncalculated based on the time lag. We demonstrate that QPO arising from the\nnon-ballistic helical jet motion driven by the orbital motion in a supermassive\nbinary black hole is a plausible physical explanation. In this scenario, the\nestimated mass of the primary black hole is\n$M\\sim3.66\\times10^8-5.79\\times10^{9}M_\\odot$.",
        "Chiral and nonreciprocal quantum devices are crucial for signal routing and\nprocessing in a quantum network. In this work, we study the chirality and\nnonreciprocity of a giant atom coupled to a one-dimensional waveguide. We\nclarify that the chiral emission of the giant atom is not directly related to\nthe time-reversal symmetry breaking but to the mirror-symmetry breaking. We\npropose a passive scheme to realize the chiral emission of a giant atom without\nbreaking time-reversal symmetry by extending the legs of the giant atom. We\nfind the time-reversal symmetry breaking via nonuniform coupling phases is\nartificial and thus cannot result in nonreciprocal single-photon scattering for\nthe giant atom. The nonreciprocity of the giant atom can be obtained by the\nexternal dissipation of the giant atom that truly breaks the time-reversal\nsymmetry. Our work clarifies the roles of symmetries in the chirality and\nnonreciprocity of giant-atom systems and paves the way for the design of\non-chip functional devices with superconducting giant atoms.",
        "We prove new bounds for the period-index problem for hyper-K\\\"ahler varieties\nof $K3^{[n]}$-type using projectively hyperholomorphic bundles constructed by\nMarkman. We show that $\\mathrm{dim}(X)$ is a bound for any $X$ of\n$K3^{[n]}$-type. We also show that the bound can be reduced to\n$\\frac{1}{2}\\mathrm{dim}(X)$, as conjectured by Huybrechts, when the Picard\nrank of $X$ is at least 3.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "We present a comparison of model-space extrapolation methods for No-Core\nShell Model calculations of ground-state energies and root-mean-square radii in\nLi isotopes. In particular, we benchmark the latest machine learning tools\nagainst widely used exponential and infrared extrapolations for energies and\ncrossing point estimates for radii. Our findings demonstrate that machine\nlearning-based approaches provide reliable predictions with robust statistical\nuncertainties for both observables even in small model spaces. These\npredictions are compatible with established exponential and IR extrapolations\nof energies and mark a notable improvement over conventional radius estimates.",
        "Using the result of Petersen $\\&$ Wink '21, we find obstructions to the\ncurvature and topology of compact Lorentzian manifolds admitting a unit-length\ntimelike Killing vector field.",
        "In this article, we extend our previous work (Applicable Analysis, 2024, pp.\n1-25) on the steepest descent method for uncertain multiobjective optimization\nproblems. While that study established local convergence, it did not address\nglobal convergence and the rate of convergence of the steepest descent\nalgorithm. To bridge this gap, we provide rigorous proofs for both global\nconvergence and the linear convergence rate of the steepest descent algorithm.\nGlobal convergence analysis strengthens the theoretical foundation of the\nsteepest descent method for uncertain multiobjective optimization problems,\noffering deeper insights into its efficiency and robustness across a broader\nclass of optimization problems. These findings enhance the method's practical\napplicability and contribute to the advancement of robust optimization\ntechniques.",
        "Based on bond percolation theory, a method is presented here to calculate the\nrelationship between capillary pressure and saturation in porous media from\nfirst principles. The governing equations are formulated on the undirected\ngraph of the pore network. The graph is a simplified mathematical object that\naccounts for the topology of the pore structure. Thus, the calculation is\nextremely computationally efficient since it is mesh-free and voxel-free. Two\ntopological invariants are identified: The bond percolation threshold and the\nresidual saturation. Bond percolation theory is used to obtain a closed-form\npressure-saturation relation in terms of the geometry of the pores (pore throat\ndistribution) and material parameters (contact angle and interfacial tension),\nuniversal exponents, and topological invariants, based on scaling relations.",
        "The universal invariant with respect to a given ribbon Hopf algebra is a\ntangle invariant that dominates all the Reshetikhin-Turaev invariants built\nfrom the representation theory of the algebra. We construct a canonical strict\nmonoidal functor that encodes the universal invariant of upwards tangles and\nrefines the Kerler-Kauffman-Radford functorial invariant. Moreover, this\nfunctor preserves the braiding, twist and the open trace, the latter being a\nmild modification of Joyal-Street-Verity's notion of trace in a balanced\ncategory. We construct this functor using the more flexible XC-algebras, a\nclass which contains both ribbon Hopf algebras and endomorphism algebras of\nrepresentation of these.",
        "In this work we demonstrate a surprising way of exploitation of the\nmosaic--skeleton approximations for efficient numerical solving of aggregation\nequations with many applied kinetic kernels. The complexity of the evaluation\nof the right-hand side with $M$ nonlinear differential equations basing on the\nuse of the mosaic-skeleton approximations is $\\mathcal{O}(M \\log^2 M)$\noperations instead of $\\mathcal{O}(M^2)$ for the straightforward computation.\nThe class of kernels allowing to make fast and accurate computations via our\napproach is wider than analogous set of kinetic coefficients for effective\ncalculations with previously developed algorithms. This class covers the\naggregation problems arising in modelling of sedimentation, supersonic effects,\nturbulent flows, etc. We show that our approach makes it possible to study the\nsystems with $M=2^{20}$ nonlinear equations within a modest computing time.",
        "In this work, we investigate the single production of the Vector-Like Quarks\n(VLQs) $T$ and $Y$, with charge $+2\/3$e and $-4\/3$e, respectively, both\ndecaying into $Wb$ at a future $\\mu p$ collider with $\\sqrt{s}=5.29, 6.48$, and\n9.16 TeV. We focus on two final states, where the $W$ boson decays leptonically\nor hadronically (in the latter case, it is highly boosted, leading to a fat\n$W$-jet). By performing a detailed signal-to-background analysis in presence of\ndetector simulations, the $5\\sigma$ discovery prospects and 95\\% CL exclusion\nlimits are mapped over parameter space regions of the model accommodating such\nnew heavy quarks, assuming an integrated luminosity of 100 fb$^{-1}$ at each of\nthe aforementioned energies.",
        "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars.",
        "While the adiabatic exchange of Majorana zero modes (MZMs) enables a\nnon-universal set of geometrically protected gates, realising an experimental\nimplementation of MZM braiding remains challenging. In an alternative proposal,\ncharge-parity measurement of two neighboring MZMs supports braiding by\nteleportation. Moreover, owing to the lack of definitive evidence of MZMs in\nsemiconducting systems, there have been several simulations of MZMs on NISQ\ndevices which more naturally lend themselves to braiding. In this work,\nmeasurement-based braiding about MZM Y-junctions are simulated by multi-qubit\nPauli-parity measurements of a logical qubit. Logical single-qubit geometric\n$S^{(\\dagger)}$-gates and entangling two-qubit gates is shown using\ntwo-physical-qubit joint measurements alone, whilst partial phase rotations\nsuch as a $T^{(\\dagger)}$-gates require at least one three-qubit joint\nmeasurement. These relatively small scale circuits offer both novel\nmeasurement-based geometric gates as well as a measurement-based demonstration\nof quantum Hamiltonian simulation.",
        "The Central Molecular Zone (CMZ) of the Galactic Center (GC) region of the\nMilky Way contains a substantial fraction of the molecular mass of the Galaxy\n>10e7 solar masses yet exhibits an order of magnitude lower star formation\nefficiency (SFE) than expected given the high densities found in this region.\nThere are multiple possible explanations for the depressed SFE in the CMZ, like\nfeedback, strong turbulence, longer free-fall timescales, and high magnetic\nfield strengths. It is currently unclear which of these mechanisms is the\ndominant inhibitor of star formation in the CMZ. It is important to understand\nthe star formation process in the extreme environment of the CMZ because it is\nthe only Galactic nuclear region we are able to study at high spatial\nresolutions with current observatories. One way to determine the relative\nimportance of the different SFE inhibiting mechanisms is through multi-spatial\nand multi-frequency polarimetric observations of the CMZ. Such observations\nwill provide insight into the behavior of the magnetic field in this unique\nenvironment. These observations will complement radio observations of\nnon-thermal structures revealing the magnetic field morphology and\npolarization. The PRobe far--Infrared Mission for Astrophysics (PRIMA) will be\nuniquely capable of contributing to such explorations by providing unique\nresolutions and frequencies for polarimetric observations. The PRIMAger\ninstrument will yield polarimetric observations covering the wavelength range\n80 -- 261 um with beam sizes ranging from 11 -- 28'', capabilities that\ncomplement existing and upcoming observatories.",
        "As offered by the World Health Organisation (WHO), close to half of the\npopulation in the world's resides in dengue-risk zones. Dengue viruses are\ntransmitted to individuals by Aedes mosquito species infected bite (Ae.\nAlbopictus of Ae. aegypti). These mosquitoes can transmit other viruses,\nincluding Zika and Chikungunya. In this research, a mathematical model is\nformulated to reflect different time delays considered in both extrinsic and\nintrinsic incubation periods, as well as in the recovery periods of infectious\nindividuals. Preliminary results for the non-delayed model including positivity\nand boundedness of solutions, non-dimensionalization and equalibria analysis\nare presented. The threshold parameter (reproduction number) of the model is\nobtained via next generation matrix schemes. The stability analysis of the\nmodel revealed that various dynamical behaviour can be observed depending on\ndelay parameters, where in particular the effect of delay in the recovery time\nof infectious individuals may lead to substantial changes in the dynamics. The\nideas presented in this paper can be applied to other infectious diseases,\nproviding qualitative evaluations for understanding time delays influencing the\ntransmission dynamics.",
        "We classify the faces of copositive and completely positive cones over a\nsecond-order cone and investigate their dimension and exposedness properties.\nThen we compute two parameters related to chains of faces of both cones. At the\nend, we discuss some possible extensions of the results with a view toward\nanalyzing the facial structure of general copositive and completely positive\ncones.",
        "Superconducting qubits are among the most promising candidates for building\nquantum information processors. Yet, they are often limited by slow and\nerror-prone qubit readout -- a critical factor in achieving high-fidelity\noperations. While current methods, including deep neural networks, enhance\nreadout accuracy, they typically lack support for mid-circuit measurements\nessential for quantum error correction, and they usually rely on large,\nresource-intensive network models. This paper presents KLiNQ, a novel qubit\nreadout architecture leveraging lightweight neural networks optimized via\nknowledge distillation. Our approach achieves around a 99% reduction in model\nsize compared to the baseline while maintaining a qubit-state discrimination\naccuracy of 91%. KLiNQ facilitates rapid, independent qubit-state readouts that\nenable mid-circuit measurements by assigning a dedicated, compact neural\nnetwork for each qubit. Implemented on the Xilinx UltraScale+ FPGA, our design\ncan perform the discrimination within 32ns. The results demonstrate that\ncompressed neural networks can maintain high-fidelity independent readout while\nenabling efficient hardware implementation, advancing practical quantum\ncomputing.",
        "Recently the Belle II Collaboration has reported a measurement of the $B^+\\to\nK^+\\nu\\bar\\nu$ rate that is higher than the standard-model expectation. Since\nthe emitted neutrinos are unobserved, the excess could be due to the $B^+$\ndecaying into a $K^+$ and a dark-matter pair. We entertain this possibility in\na two-Higgs-doublet model supplemented with a real singlet scalar boson acting\nas the dark matter. This model also accommodates strangeness-changing\ninteractions providing new sources of $CP$ violation which can affect hyperon\nand kaon nonleptonic transitions. We find that the resulting $CP$ violation in\nthe hyperon sector can be significant, reaching the current empirical bounds,\nafter taking into account constraints from kaon mixing and decay and from\ndark-matter relic-density data and direct searches including the Migdal effect.\nWe demonstrate that the hyperon and kaon processes are complementary probes of\nthis new-physics scenario. Its prediction for sizable hyperon $CP$ violation is\npotentially testable in ongoing experiments, such as BESIII, Belle II, and\nLHCb, and in next-generation ones like PANDA and at the Super Tau Charm\nFacility.",
        "Combinatorial ideas are developed in this article to study Chern numbers on\nample and numerically effective vector bundles. An effective lower bound for\nChern numbers of ample vector bundles is established, which makes some progress\ntowards a long-standing question. Along this line we prove that Chern numbers\non nef vector bundles obey reverse dominance ordering, which improves upon some\nclassical and recent results. We propose a simultaneous positivity question on\n(signed) Chern numbers of compact complex or K\\\"{a}hler manifolds whose\n(co)tangent bundles are semipositive in various senses, and show that it holds\ntrue for compact homogeneous complex manifolds.",
        "We investigate $L_2$-discrepancies of what we call weak Latin hypercubes. In\nthis case it turns out that there is a precise equivalence between the extreme\nand periodic $L_2$-discrepancy which follows from a much broader result about\ngeneralized energies for weighted point sets.\n  Motivated by this we study the asymptotics of the optimal $L_2$-discrepancy\nof weak Latin hypercubes. We determine asymptotically tight bounds for $d \\geq\n3$ and even the precise (dimension dependent) constant in front of the\ndominating term for $d \\geq 4$.",
        "In this article I study how the problem of time of canonical approaches to\nquantum gravity affects the simple minisuperspace models used in quantum\ncosmology. I follow some authors who have argued that this issue makes the\nquantization of general relativity problematic to conclude that the same\napplies in the case of quantum cosmology. In particular, I argue that temporal\nstructures are lost in quantization and that this is a problem, as they encode\npart of the empirical content of classical cosmology, such as the age of the\nuniverse.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice",
    "start_abstract":"Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network"
      ],
      "abstract":[
        "Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
        "Theory-to-Practice Gap for Neural Networks and Neural Operators",
        "Quantitative Derivation of the Two-Component Gross-Pitaevskii Equation\n  with Uniform-in-Time Convergence Rate",
        "Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation\n  Problems",
        "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models",
        "Characterizing Data Visualization Literacy: a Systematic Literature\n  Review",
        "Geodesic Diffusion Models for Medical Image-to-Image Generation",
        "Langevin Monte-Carlo Provably Learns Depth Two Neural Nets at Any Size\n  and Data",
        "Optimisation of space-time periodic eigenvalues",
        "The least balanced graphs and trees",
        "One Stack, Diverse Vehicles: Checking Safe Portability of Automated\n  Driving Software",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Diffusion on Graph: Augmentation of Graph Structure for Node\n  Classification",
        "Prophet Inequalities for Bandits, Cabinets, and DAGs",
        "Fast Debiasing of the LASSO Estimator",
        "Accelerated Medicines Development using a Digital Formulator and a\n  Self-Driving Tableting DataFactory",
        "A Novel Spatiotemporal Correlation Anomaly Detection Method Based on\n  Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in\n  Wireless Sensor Network",
        "Para-Holomorphic Algebroids and Para-Complex Connections",
        "FeatPCA: A feature subspace based principal component analysis technique\n  for enhancing clustering of single-cell RNA-seq data",
        "Seeing World Dynamics in a Nutshell",
        "Augmenting Image Annotation: A Human-LMM Collaborative Framework for\n  Efficient Object Selection and Label Generation",
        "Generalized Simple Graphical Rules for Assessing Selection Bias",
        "DBSCAN in domains with periodic boundary conditions",
        "Joint Cell Selection and Resource Allocation Games with Backhaul\n  Constraints",
        "Toward Foundation Models for Online Complex Event Detection in CPS-IoT:\n  A Case Study",
        "UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance",
        "You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from\n  Video Demonstrations",
        "Robust Probabilistic Model Checking with Continuous Reward Domains",
        "KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model"
      ],
      "abstract":[
        "Recent advancements in Neural Audio Codec (NAC) models have inspired their\nuse in various speech processing tasks, including speech enhancement (SE). In\nthis work, we propose a novel, efficient SE approach by leveraging the\npre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE\nmethods, which process discrete speech tokens using Language Models (LMs), we\nperform SE within the continuous embedding space of the pretrained NAC, which\nis highly compressed along the time dimension for efficient representation. Our\nlightweight SE model, optimized through an embedding-level loss, delivers\nresults comparable to SE baselines trained on larger datasets, with a\nsignificantly lower real-time factor of 0.005. Additionally, our method\nachieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer\nin a simulated cloud-based audio transmission environment. This work highlights\na new, efficient NAC-based SE solution, particularly suitable for cloud\napplications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting\/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.",
        "This work studies the sampling complexity of learning with ReLU neural\nnetworks and neural operators. For mappings belonging to relevant approximation\nspaces, we derive upper bounds on the best-possible convergence rate of any\nlearning algorithm, with respect to the number of samples. In the\nfinite-dimensional case, these bounds imply a gap between the parametric and\nsampling complexities of learning, known as the \\emph{theory-to-practice gap}.\nIn this work, a unified treatment of the theory-to-practice gap is achieved in\na general $L^p$-setting, while at the same time improving available bounds in\nthe literature. Furthermore, based on these results the theory-to-practice gap\nis extended to the infinite-dimensional setting of operator learning. Our\nresults apply to Deep Operator Networks and integral kernel-based neural\noperators, including the Fourier neural operator. We show that the\nbest-possible convergence rate in a Bochner $L^p$-norm is bounded by\nMonte-Carlo rates of order $1\/p$.",
        "We derive the time-dependent two-component Gross-Pitaevskii equation as an\neffective description of the dynamics of a dilute two-component Bose gas near\nits ground state, which exhibits a two-component mixture Bose-Einstein\ncondensate, in the Gross-Pitaevskii limit regime. Our main result establishes a\nuniform-in-time bound on the convergence rate between the many-body dynamics\nand the effective description, explicitly quantified in terms of the particle\nnumber $N$. This improves upon the works of Michelangeli and Olgliati [73, 85]\nby providing a sharper, $N$-dependent, time-independent convergence rate. Our\napproach also extends the framework of Benedikter, de Oliveira, and Schlein\n[10] to the multi-component Bose gas setting. More specifically, we develop the\nnecessary Bogoliubov theory to analyze the dynamics of multi-component Bose\ngases in the Gross-Pitaevskii regime.",
        "We consider a learning-augmented framework for NP-hard permutation problems.\nThe algorithm has access to predictions telling, given a pair $u,v$ of\nelements, whether $u$ is before $v$ or not in an optimal solution. Building on\nthe work of Braverman and Mossel (SODA 2008), we show that for a class of\noptimization problems including scheduling, network design and other graph\npermutation problems, these predictions allow to solve them in polynomial time\nwith high probability, provided that predictions are true with probability at\nleast $1\/2+\\epsilon$. Moreover, this can be achieved with a parsimonious access\nto the predictions.",
        "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps:\/\/github.com\/LuckyGirl-XU\/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.",
        "With the advent of the data era, and of new, more intelligent interfaces for\nsupporting decision making, there is a growing need to define, model and assess\nhuman ability and data visualizations usability for a better encoding and\ndecoding of data patterns. Data Visualization Literacy (DVL) is the ability of\nencoding and decoding data into and from a visual language. Although this\nability and its measurement are crucial for advancing human knowledge and\ndecision capacity, they have seldom been investigated, let alone\nsystematically. To address this gap, this paper presents a systematic\nliterature review comprising 43 reports on DVL, analyzed using the PRISMA\nmethodology. Our results include the identification of the purposes of DVL, its\nsatellite aspects, the models proposed, and the assessments designed to\nevaluate the degree of DVL of people. Eventually, we devise many research\ndirections including, among the most challenging, the definition of a\n(standard) unifying construct of DVL.",
        "Diffusion models transform an unknown data distribution into a Gaussian prior\nby progressively adding noise until the data become indistinguishable from pure\nnoise. This stochastic process traces a path in probability space, evolving\nfrom the original data distribution (considered as a Gaussian with near-zero\nvariance) to an isotropic Gaussian. The denoiser then learns to reverse this\nprocess, generating high-quality samples from random Gaussian noise. However,\nstandard diffusion models, such as the Denoising Diffusion Probabilistic Model\n(DDPM), do not ensure a geodesic (i.e., shortest) path in probability space.\nThis inefficiency necessitates the use of many intermediate time steps, leading\nto high computational costs in training and sampling. To address this\nlimitation, we propose the Geodesic Diffusion Model (GDM), which defines a\ngeodesic path under the Fisher-Rao metric with a variance-exploding noise\nscheduler. This formulation transforms the data distribution into a Gaussian\nprior with minimal energy, significantly improving the efficiency of diffusion\nmodels. We trained GDM by continuously sampling time steps from 0 to 1 and\nusing as few as 15 evenly spaced time steps for model sampling. We evaluated\nGDM on two medical image-to-image generation tasks: CT image denoising and MRI\nimage super-resolution. Experimental results show that GDM achieved\nstate-of-the-art performance while reducing training time by a 50-fold compared\nto DDPM and 10-fold compared to Fast-DDPM, with 66 times faster sampling than\nDDPM and a similar sampling speed to Fast-DDPM. These efficiency gains enable\nrapid model exploration and real-time clinical applications. Our code is\npublicly available at: https:\/\/github.com\/mirthAI\/GDM-VE.",
        "In this work, we will establish that the Langevin Monte-Carlo algorithm can\nlearn depth-2 neural nets of any size and for any data and we give\nnon-asymptotic convergence rates for it. We achieve this via showing that under\nTotal Variation distance and q-Renyi divergence, the iterates of Langevin Monte\nCarlo converge to the Gibbs distribution of Frobenius norm regularized losses\nfor any of these nets, when using smooth activations and in both classification\nand regression settings. Most critically, the amount of regularization needed\nfor our results is independent of the size of the net. This result combines\nseveral recent observations, like our previous papers showing that two-layer\nneural loss functions can always be regularized by a certain constant amount\nsuch that they satisfy the Villani conditions, and thus their Gibbs measures\nsatisfy a Poincare inequality.",
        "The goal of this paper is to provide a qualitative analysis of the\noptimisation of space-time periodic principal eigenvalues. Namely, considering\na fixed time horizon $T$ and the $d$-dimensional torus $\\mathbb{T}^d$, let, for\nany $m\\in L^\\infty((0,T)\\times\\mathbb{T}^d)$, $\\lambda(m)$ be the principal\neigenvalue of the operator $\\partial_t-\\Delta-m$ endowed with (time-space)\nperiodic boundary conditions. The main question we set out to answer is the\nfollowing: how to choose $m$ so as to minimise $\\lambda(m)$? This question\nstems from population dynamics. We prove that in several cases it is always\nbeneficial to rearrange $m$ with respect to time in a symmetric way, which is\nthe first comparison result for the rearrangement in time of parabolic\nequations. Furthermore, we investigate the validity (or lack thereof) of\nTalenti inequalities for the rearrangement in time of parabolic equations. The\nnumerical simulations which illustrate our results were obtained by developing\na framework within which it is possible to optimise criteria with respect to\nfunctions having a prescribed rearrangement (or distribution function).",
        "Given a connected graph, the principal eigenvector of the adjacency matrix\n(often called the Perron vector) can be used to assign positive weights to the\nvertices. A natural way to measure the homogeneousness of this vector is by\nconsidering the ratio of its $\\ell^1$ and $\\ell^2$ norms.\n  It is easy to see that the most balanced graphs in this sense (i.e., the ones\nwith the largest ratio) are the regular graphs. What about the least balanced\ngraphs with the smallest ratio? It was conjectured by R\\\"ucker, R\\\"ucker and\nGutman that, for any given $n \\geq 6$, among $n$-vertex connected graphs the\nsmallest ratio is achieved by the complete graph $K_4$ with a single path\n$P_{n-4}$ attached to one of its vertices. In this paper we confirm this\nconjecture.\n  We also verify the analogous conjecture for trees: for any given $n \\geq 8$,\namong $n$-vertex trees the smallest ratio is achieved by the star graph $S_5$\nwith a path $P_{n-5}$ attached to its central vertex.",
        "Integrating an automated driving software stack into vehicles with variable\nconfiguration is challenging, especially due to different hardware\ncharacteristics. Further, to provide software updates to a vehicle fleet in the\nfield, the functional safety of every affected configuration has to be ensured.\nThese additional demands for dependability and the increasing hardware\ndiversity in automated driving make rigorous automatic analysis essential. This\npaper addresses this challenge by using formal portability checking of adaptive\ncruise controller code for different vehicle configurations. Given a formal\nspecification of the safe behavior, models of target configurations are\nderived, which capture relevant effects of sensors, actuators and computing\nplatforms. A corresponding safe set is obtained and used to check if the\ndesired behavior is achievable on all targets. In a case study, portability\nchecking of a traditional and a neural network controller are performed\nautomatically within minutes for each vehicle hardware configuration. The check\nprovides feedback for necessary adaptations of the controllers, thus, allowing\nrapid integration and testing of software or parameter changes.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "Graph diffusion models have recently been proposed to synthesize entire\ngraphs, such as molecule graphs. Although existing methods have shown great\nperformance in generating entire graphs for graph-level learning tasks, no\ngraph diffusion models have been developed to generate synthetic graph\nstructures, that is, synthetic nodes and associated edges within a given graph,\nfor node-level learning tasks. Inspired by the research in the computer vision\nliterature using synthetic data for enhanced performance, we propose Diffusion\non Graph (DoG), which generates synthetic graph structures to boost the\nperformance of GNNs. The synthetic graph structures generated by DoG are\ncombined with the original graph to form an augmented graph for the training of\nnode-level learning tasks, such as node classification and graph contrastive\nlearning (GCL). To improve the efficiency of the generation process, a Bi-Level\nNeighbor Map Decoder (BLND) is introduced in DoG. To mitigate the adverse\neffect of the noise introduced by the synthetic graph structures, a low-rank\nregularization method is proposed for the training of graph neural networks\n(GNNs) on the augmented graphs. Extensive experiments on various graph datasets\nfor semi-supervised node classification and graph contrastive learning have\nbeen conducted to demonstrate the effectiveness of DoG with low-rank\nregularization. The code of DoG is available at\nhttps:\/\/github.com\/Statistical-Deep-Learning\/DoG.",
        "A decisionmaker faces $n$ alternatives, each of which represents a potential\nreward. After investing costly resources into investigating the alternatives,\nthe decisionmaker may select one, or more generally a feasible subset, and\nobtain the associated reward(s). The objective is to maximize the sum of\nrewards minus total costs invested. We consider this problem under a general\nmodel of an alternative as a \"Markov Search Process,\" a type of undiscounted\nMarkov Decision Process on a finite acyclic graph. Even simple cases generalize\nNP-hard problems such as Pandora's Box with nonobligatory inspection.\n  Despite the apparently adaptive and interactive nature of the problem, we\nprove optimal prophet inequalities for this problem under a variety of\ncombinatorial constraints. That is, we give approximation algorithms that\ninteract with the alternatives sequentially, where each must be fully explored\nand either selected or else discarded before the next arrives. In particular,\nwe obtain a computationally efficient $\\frac{1}{2}-\\epsilon$ prophet inequality\nfor Combinatorial Markov Search subject to any matroid constraint. This result\nimplies incentive-compatible mechanisms with constant Price of Anarchy for\nserving single-parameter agents when the agents strategically conduct\nindependent, costly search processes to discover their values.",
        "In high-dimensional sparse regression, the \\textsc{Lasso} estimator offers\nexcellent theoretical guarantees but is well-known to produce biased estimates.\nTo address this, \\cite{Javanmard2014} introduced a method to ``debias\" the\n\\textsc{Lasso} estimates for a random sub-Gaussian sensing matrix\n$\\boldsymbol{A}$. Their approach relies on computing an ``approximate inverse\"\n$\\boldsymbol{M}$ of the matrix $\\boldsymbol{A}^\\top \\boldsymbol{A}\/n$ by\nsolving a convex optimization problem. This matrix $\\boldsymbol{M}$ plays a\ncritical role in mitigating bias and allowing for construction of confidence\nintervals using the debiased \\textsc{Lasso} estimates. However the computation\nof $\\boldsymbol{M}$ is expensive in practice as it requires iterative\noptimization. In the presented work, we re-parameterize the optimization\nproblem to compute a ``debiasing matrix\" $\\boldsymbol{W} :=\n\\boldsymbol{AM}^{\\top}$ directly, rather than the approximate inverse\n$\\boldsymbol{M}$. This reformulation retains the theoretical guarantees of the\ndebiased \\textsc{Lasso} estimates, as they depend on the \\emph{product}\n$\\boldsymbol{AM}^{\\top}$ rather than on $\\boldsymbol{M}$ alone. Notably, we\nprovide a simple, computationally efficient, closed-form solution for\n$\\boldsymbol{W}$ under similar conditions for the sensing matrix\n$\\boldsymbol{A}$ used in the original debiasing formulation, with an additional\ncondition that the elements of every row of $\\boldsymbol{A}$ have uncorrelated\nentries. Also, the optimization problem based on $\\boldsymbol{W}$ guarantees a\nunique optimal solution, unlike the original formulation based on\n$\\boldsymbol{M}$. We verify our main result with numerical simulations.",
        "Pharmaceutical tablet formulation and process development, traditionally a\ncomplex and multi-dimensional decision-making process, necessitates extensive\nexperimentation and resources, often resulting in suboptimal solutions. This\nstudy presents an integrated platform for tablet formulation and manufacturing,\nbuilt around a Digital Formulator and a Self-Driving Tableting DataFactory. By\ncombining predictive modelling, optimisation algorithms, and automation, this\nsystem offers a material-to-product approach to predict and optimise critical\nquality attributes for different formulations, linking raw material attributes\nto key blend and tablet properties, such as flowability, porosity, and tensile\nstrength. The platform leverages the Digital Formulator, an in-silico\noptimisation framework that employs a hybrid system of models - melding\ndata-driven and mechanistic models - to identify optimal formulation settings\nfor manufacturability. Optimised formulations then proceed through the\nself-driving Tableting DataFactory, which includes automated powder dosing,\ntablet compression and performance testing, followed by iterative refinement of\nprocess parameters through Bayesian optimisation methods. This approach\naccelerates the timeline from material characterisation to development of an\nin-specification tablet within 6 hours, utilising less than 5 grams of API, and\nmanufacturing small batch sizes of up to 1,440 tablets with augmented and mixed\nreality enabled real-time quality control within 24 hours. Validation across\nmultiple APIs and drug loadings underscores the platform's capacity to reliably\nmeet target quality attributes, positioning it as a transformative solution for\naccelerated and resource-efficient pharmaceutical development.",
        "Attention-based transformers have played an important role in wireless sensor\nnetwork (WSN) timing anomaly detection due to their ability to capture\nlong-term dependencies. However, there are several issues that must be\naddressed, such as the fact that their ability to capture long-term\ndependencies is not completely reliable, their computational complexity levels\nare high, and the spatiotemporal features of WSN timing data are not\nsufficiently extracted for detecting the correlation anomalies of multinode WSN\ntiming data. To address these limitations, this paper proposes a WSN anomaly\ndetection method that integrates frequency-domain features with dynamic graph\nneural networks (GNN) under a designed self-encoder reconstruction framework.\nFirst, the discrete wavelet transform effectively decomposes trend and seasonal\ncomponents of time series to solve the poor long-term reliability of\ntransformers. Second, a frequency-domain attention mechanism is designed to\nmake full use of the difference between the amplitude distributions of normal\ndata and anomalous data in this domain. Finally, a multimodal fusion-based\ndynamic graph convolutional network (MFDGCN) is designed by combining an\nattention mechanism and a graph convolutional network (GCN) to adaptively\nextract spatial correlation features. A series of experiments conducted on\npublic datasets and their results demonstrate that the anomaly detection method\ndesigned in this paper exhibits superior precision and recall than the existing\nmethods do, with an F1 score of 93.5%, representing an improvement of 2.9% over\nthat of the existing models.",
        "The goal of this paper is to develop the theory of Courant algebroids with\nintegrable para-Hermitian vector bundle structures by invoking the theory of\nLie bialgebroids. We consider the case where the underlying manifold has an\nalmost para-complex structure, and use this to define a notion of\npara-holomorphic algebroid. We investigate connections on para-holomorphic\nalgebroids and determine an appropriate sense in which they can be\npara-complex. Finally, we show through a series of examples how the theory of\nexact para-holomorphic algebroids with a para-complex connection is a\ngeneralization of both para-K\\\"{a}hler geometry and the theory of Poisson-Lie\ngroups.",
        "Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to\nanalyze gene expression at the cellular level. By providing data on gene\nexpression for each individual cell, scRNA-seq generates large datasets with\nthousands of genes. However, handling such high-dimensional data poses\ncomputational challenges due to increased complexity. Dimensionality reduction\nbecomes crucial for scRNA-seq analysis. Various dimensionality reduction\nalgorithms, including Principal Component Analysis (PCA), Uniform Manifold\nApproximation and Projection (UMAP), and t-Distributed Stochastic Neighbor\nEmbedding (t-SNE), are commonly used to address this challenge. These methods\ntransform the original high-dimensional data into a lower-dimensional\nrepresentation while preserving relevant information. In this paper we propose\n{\\methodname}. Instead of applying dimensionality reduction directly to the\nentire dataset, we divide it into multiple subspaces. Within each subspace, we\napply dimension reduction techniques, and then merge the reduced data.\n{\\methodname} offers four variations for subspacing. Our experimental results\ndemonstrate that clustering based on subspacing yields better accuracy than\nworking with the full dataset. Across a variety of scRNA-seq datasets,\n{\\methodname} consistently outperforms existing state-of-the-art clustering\ntools.",
        "We consider the problem of efficiently representing casually captured\nmonocular videos in a spatially- and temporally-coherent manner. While existing\napproaches predominantly rely on 2D\/2.5D techniques treating videos as\ncollections of spatiotemporal pixels, they struggle with complex motions,\nocclusions, and geometric consistency due to absence of temporal coherence and\nexplicit 3D structure. Drawing inspiration from monocular video as a projection\nof the dynamic 3D world, we explore representing videos in their intrinsic 3D\nform through continuous flows of Gaussian primitives in space-time. In this\npaper, we propose NutWorld, a novel framework that efficiently transforms\nmonocular videos into dynamic 3D Gaussian representations in a single forward\npass. At its core, NutWorld introduces a structured spatial-temporal aligned\nGaussian (STAG) representation, enabling optimization-free scene modeling with\neffective depth and flow regularization. Through comprehensive experiments, we\ndemonstrate that NutWorld achieves high-fidelity video reconstruction quality\nwhile enabling various downstream applications in real-time. Demos and code\nwill be available at https:\/\/github.com\/Nut-World\/NutWorld.",
        "Traditional image annotation tasks rely heavily on human effort for object\nselection and label assignment, making the process time-consuming and prone to\ndecreased efficiency as annotators experience fatigue after extensive work.\nThis paper introduces a novel framework that leverages the visual understanding\ncapabilities of large multimodal models (LMMs), particularly GPT, to assist\nannotation workflows. In our proposed approach, human annotators focus on\nselecting objects via bounding boxes, while the LMM autonomously generates\nrelevant labels. This human-AI collaborative framework enhances annotation\nefficiency by reducing the cognitive and time burden on human annotators. By\nanalyzing the system's performance across various types of annotation tasks, we\ndemonstrate its ability to generalize to tasks such as object recognition,\nscene description, and fine-grained categorization. Our proposed framework\nhighlights the potential of this approach to redefine annotation workflows,\noffering a scalable and efficient solution for large-scale data labeling in\ncomputer vision. Finally, we discuss how integrating LMMs into the annotation\npipeline can advance bidirectional human-AI alignment, as well as the\nchallenges of alleviating the \"endless annotation\" burden in the face of\ninformation overload by shifting some of the work to AI.",
        "Selection bias is a major obstacle toward valid causal inference in\nepidemiology. Over the past decade, several simple graphical rules based on\ncausal diagrams have been proposed as the sufficient identification conditions\nfor addressing selection bias and recovering causal effects. However, these\nsimple graphical rules are usually coupled with specific identification\nstrategies and estimators. In this article, we show two important cases of\nselection bias that cannot be addressed by these simple rules and their\nestimators: one case where selection is a descendant of a collider of the\ntreatment and the outcome, and the other case where selection is affected by\nthe mediator. To address selection bias in these two cases, we construct\nidentification formulas by the g-computation and the inverse probability\nweighting (IPW) methods based on single-world intervention graphs (SWIGs). They\nare generalized to recover the average treatment effect by adjusting for\npost-treatment upstream causes of selection. We propose two IPW estimators and\ntheir variance estimators to recover the average treatment effect in the\npresence of selection bias in these two cases. We conduct simulation studies to\nverify the performance of the estimators when the traditional crude\nselected-sample analysis returns erroneous contradictory conclusions to the\ntruth.",
        "Many scientific problems involve data that is embedded in a space with\nperiodic boundary conditions. This can for instance be related to an inherent\ncyclic or rotational symmetry in the data or a spatially extended periodicity.\nWhen analyzing such data, well-tailored methods are needed to obtain efficient\napproaches that obey the periodic boundary conditions of the problem. In this\nwork, we present a method for applying a clustering algorithm to data embedded\nin a periodic domain based on the DBSCAN algorithm, a widely used unsupervised\nmachine learning method that identifies clusters in data. The proposed method\ninternally leverages the conventional DBSCAN algorithm for domains with open\nboundaries, such that it remains compatible with all optimized implementations\nfor neighborhood searches in open domains. In this way, it retains the same\noptimized runtime complexity of $O(N\\log N)$. We demonstrate the workings of\nthe proposed method using synthetic data in one, two and three dimensions and\nalso apply it to a real-world example involving the clustering of bubbles in a\nturbulent flow. The proposed approach is implemented in a ready-to-use Python\npackage that we make publicly available.",
        "In this work we study the problem of user association and resource allocation\nto maximize the proportional fairness of a wireless network with limited\nbackhaul capacity. The optimal solution of this problem requires solving a\nmixed integer non-linear programming problem which generally cannot be solved\nin real time. We propose instead to model the problem as a potential game,\nwhich decreases dramatically the computational complexity and obtains a user\nassociation and resource allocation close to the optimal solution.\nAdditionally, the use of a game-theoretic approach allows an efficient\ndistribution of the computational burden among the computational resources of\nthe network.",
        "Complex events (CEs) play a crucial role in CPS-IoT applications, enabling\nhigh-level decision-making in domains such as smart monitoring and autonomous\nsystems. However, most existing models focus on short-span perception tasks,\nlacking the long-term reasoning required for CE detection. CEs consist of\nsequences of short-time atomic events (AEs) governed by spatiotemporal\ndependencies. Detecting them is difficult due to long, noisy sensor data and\nthe challenge of filtering out irrelevant AEs while capturing meaningful\npatterns. This work explores CE detection as a case study for CPS-IoT\nfoundation models capable of long-term reasoning. We evaluate three approaches:\n(1) leveraging large language models (LLMs), (2) employing various neural\narchitectures that learn CE rules from data, and (3) adopting a neurosymbolic\napproach that integrates neural models with symbolic engines embedding human\nknowledge. Our results show that the state-space model, Mamba, which belongs to\nthe second category, outperforms all methods in accuracy and generalization to\nlonger, unseen sensor traces. These findings suggest that state-space models\ncould be a strong backbone for CPS-IoT foundation models for long-span\nreasoning tasks.",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet code generation remains a major challenge. Current\napproaches for obtaining high-quality code data primarily focus on (i)\ncollecting large-scale pre-training data and (ii) synthesizing instruction data\nthrough prompt engineering with powerful models. While pre-training data faces\nquality consistency issues, instruction-based synthesis suffers from limited\ninstruction diversity and inherent biases of LLMs. To address this gap, we\nintroduce UnitCoder, a systematic pipeline leveraging model-generated unit\ntests to both guide and validate the code generation process. Combined with\nlarge-scale package-based retrieval from pre-training corpus, we generate a\ndataset of 500K+ verifiable programs containing diverse API calls. Evaluations\non multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that\nmodels fine-tuned on our synthetic data exhibit consistent performance\nimprovements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and\n28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work\npresents a scalable approach that leverages model-generated unit tests to guide\nthe synthesis of high-quality code data from pre-training corpora,\ndemonstrating the potential for producing diverse and high-quality\npost-training data at scale. All code and data will be released\n(https:\/\/github.com).",
        "Bimanual robotic manipulation is a long-standing challenge of embodied\nintelligence due to its characteristics of dual-arm spatial-temporal\ncoordination and high-dimensional action spaces. Previous studies rely on\npre-defined action taxonomies or direct teleoperation to alleviate or\ncircumvent these issues, often making them lack simplicity, versatility and\nscalability. Differently, we believe that the most effective and efficient way\nfor teaching bimanual manipulation is learning from human demonstrated videos,\nwhere rich features such as spatial-temporal positions, dynamic postures,\ninteraction states and dexterous transitions are available almost for free. In\nthis work, we propose the YOTO (You Only Teach Once), which can extract and\nthen inject patterns of bimanual actions from as few as a single binocular\nobservation of hand movements, and teach dual robot arms various complex tasks.\nFurthermore, based on keyframes-based motion trajectories, we devise a subtle\nsolution for rapidly generating training demonstrations with diverse variations\nof manipulated objects and their locations. These data can then be used to\nlearn a customized bimanual diffusion policy (BiDP) across diverse scenes. In\nexperiments, YOTO achieves impressive performance in mimicking 5 intricate\nlong-horizon bimanual tasks, possesses strong generalization under different\nvisual and spatial conditions, and outperforms existing visuomotor imitation\nlearning methods in accuracy and efficiency. Our project link is\nhttps:\/\/hnuzhy.github.io\/projects\/YOTO.",
        "Probabilistic model checking traditionally verifies properties on the\nexpected value of a measure of interest. This restriction may fail to capture\nthe quality of service of a significant proportion of a system's runs,\nespecially when the probability distribution of the measure of interest is\npoorly represented by its expected value due to heavy-tail behaviors or\nmultiple modalities. Recent works inspired by distributional reinforcement\nlearning use discrete histograms to approximate integer reward distribution,\nbut they struggle with continuous reward space and present challenges in\nbalancing accuracy and scalability. We propose a novel method for handling both\ncontinuous and discrete reward distributions in Discrete Time Markov Chains\nusing moment matching with Erlang mixtures. By analytically deriving\nhigher-order moments through Moment Generating Functions, our method\napproximates the reward distribution with theoretically bounded error while\npreserving the statistical properties of the true distribution. This detailed\ndistributional insight enables the formulation and robust model checking of\nquality properties based on the entire reward distribution function, rather\nthan restricting to its expected value. We include a theoretical foundation\nensuring bounded approximation errors, along with an experimental evaluation\ndemonstrating our method's accuracy and scalability in practical model-checking\nproblems.",
        "Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)",
    "start_abstract":"Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence.",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"
      ],
      "abstract":[
        "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of\n  Vision-Language Models",
        "Velocity-free task-space regulator for robot manipulators with external\n  disturbances",
        "Learning from Active Human Involvement through Proxy Value Propagation",
        "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance",
        "EU-Nets: Enhanced, Explainable and Parsimonious U-Nets",
        "HoneypotNet: Backdoor Attacks Against Model Extraction",
        "Integral gains for non-autonomous Wazewski systems",
        "Multi-agent Auto-Bidding with Latent Graph Diffusion Models",
        "The Large-Scale Structure of Entanglement in Quantum Many-body Systems",
        "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
        "Equivariant localization in Batalin-Vilkovisky formalism",
        "Insights into dendritic growth mechanisms in batteries: A combined\n  machine learning and computational study",
        "General Stability Estimates in NonLocal Traffic Models for Several\n  Populations",
        "A non-D-continuum with weakly infinite-dimensional closed\n  set-aposyndetic Whitney levels",
        "We Can't Understand AI Using our Existing Vocabulary",
        "Evaluation of CGRA Toolchains",
        "FedSA: A Unified Representation Learning via Semantic Anchors for\n  Prototype-based Federated Learning",
        "SWIPTNet: A Unified Deep Learning Framework for SWIPT based on GNN and\n  Transfer Learning",
        "Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields",
        "Accurate 3D Grapevine Structure Extraction from High-Resolution Point\n  Clouds",
        "System-level Analysis of Dual-Mode Networked Sensing: ISAC Integration &\n  Coordination Gains",
        "Pseudo-cones and measure transport",
        "Infrastructure for AI Agents",
        "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining",
        "ProgCo: Program Helps Self-Correction of Large Language Models",
        "Global Solvability for the Compressible Hookean Viscoelastic Fluids with\n  a Free Boundary in Some Classes of Large Data",
        "Graded Courrent PDL",
        "An Efficient Approach to Fractional Analysis for Non-Linear Coupled\n  Thermo-Elastic Systems",
        "Options-Aware Dense Retrieval for Multiple-Choice query Answering"
      ],
      "abstract":[
        "Test-time adaptation (TTA) is crucial in maintaining Vision-Language Models\n(VLMs) performance when facing real-world distribution shifts, particularly\nwhen the source data or target labels are inaccessible. Existing TTA methods\nrely on CLIP's output probability distribution for feature evaluation, which\ncan introduce biases under domain shifts. This misalignment may cause features\nto be misclassified due to text priors or incorrect textual associations. To\naddress these limitations, we propose Bidirectional Prototype-Reward\nco-Evolution (BPRE), a novel TTA framework for VLMs that integrates feature\nquality assessment with prototype evolution through a synergistic feedback\nloop. BPRE first employs a Multi-Dimensional Quality-Aware Reward Module to\nevaluate feature quality and guide prototype refinement precisely. The\ncontinuous refinement of prototype quality through Prototype-Reward Interactive\nEvolution will subsequently enhance the computation of more robust\nMulti-Dimensional Quality-Aware Reward Scores. Through the bidirectional\ninteraction, the precision of rewards and the evolution of prototypes mutually\nreinforce each other, forming a self-evolving cycle. Extensive experiments are\nconducted across 15 diverse recognition datasets encompassing natural\ndistribution shifts and cross-dataset generalization scenarios. Results\ndemonstrate that BPRE consistently achieves superior average performance\ncompared to state-of-the-art methods across different model architectures, such\nas ResNet-50 and ViT-B\/16. By emphasizing comprehensive feature evaluation and\nbidirectional knowledge refinement, BPRE advances VLM generalization\ncapabilities, offering a new perspective on TTA.",
        "This paper addresses the problem of task-space robust regulation of robot\nmanipulators subject to external disturbances. A velocity-free control law is\nproposed by combining the internal model principle and the passivity-based\noutput-feedback control approach. The developed output-feedback controller\nensures not only asymptotic convergence of the regulation error but also\nsuppression of unwanted external step\/sinusoidal disturbances. The potential of\nthe proposed method lies in its simplicity, intuitively appealing, and simple\ngain selection criteria for synthesis of multi-joint robot manipulator control\nsystems.",
        "Learning from active human involvement enables the human subject to actively\nintervene and demonstrate to the AI agent during training. The interaction and\ncorrective feedback from human brings safety and AI alignment to the learning\nprocess. In this work, we propose a new reward-free active human involvement\nmethod called Proxy Value Propagation for policy optimization. Our key insight\nis that a proxy value function can be designed to express human intents,\nwherein state-action pairs in the human demonstration are labeled with high\nvalues, while those agents' actions that are intervened receive low values.\nThrough the TD-learning framework, labeled values of demonstrated state-action\npairs are further propagated to other unlabeled data generated from agents'\nexploration. The proxy value function thus induces a policy that faithfully\nemulates human behaviors. Human-in-the-loop experiments show the generality and\nefficiency of our method. With minimal modification to existing reinforcement\nlearning algorithms, our method can learn to solve continuous and discrete\ncontrol tasks with various human control devices, including the challenging\ntask of driving in Grand Theft Auto V. Demo video and code are available at:\nhttps:\/\/metadriverse.github.io\/pvp",
        "Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail. Please visit https:\/\/arc2avatar.github.io for more\nresources.",
        "In this study, we propose MHEX+, a framework adaptable to any U-Net\narchitecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets,\nwhich enhance explainability and uncertainty estimation, addressing the\nlimitations of traditional U-Net models while improving performance and\nstability. A key innovation is the Equivalent Convolutional Kernel, which\nunifies consecutive convolutional layers, boosting interpretability. For\nuncertainty estimation, we propose the collaboration gradient approach,\nmeasuring gradient consistency across decoder layers. Notably, EU-Nets achieve\nan average accuracy improvement of 1.389\\% and a variance reduction of 0.83\\%\nacross all networks and datasets in our experiments, requiring fewer than 0.1M\nparameters.",
        "Model extraction attacks are one type of inference-time attacks that\napproximate the functionality and performance of a black-box victim model by\nlaunching a certain number of queries to the model and then leveraging the\nmodel's predictions to train a substitute model. These attacks pose severe\nsecurity threats to production models and MLaaS platforms and could cause\nsignificant monetary losses to the model owners. A body of work has proposed to\ndefend machine learning models against model extraction attacks, including both\nactive defense methods that modify the model's outputs or increase the query\noverhead to avoid extraction and passive defense methods that detect malicious\nqueries or leverage watermarks to perform post-verification. In this work, we\nintroduce a new defense paradigm called attack as defense which modifies the\nmodel's output to be poisonous such that any malicious users that attempt to\nuse the output to train a substitute model will be poisoned. To this end, we\npropose a novel lightweight backdoor attack method dubbed HoneypotNet that\nreplaces the classification layer of the victim model with a honeypot layer and\nthen fine-tunes the honeypot layer with a shadow model (to simulate model\nextraction) via bi-level optimization to modify its output to be poisonous\nwhile remaining the original performance. We empirically demonstrate on four\ncommonly used benchmark datasets that HoneypotNet can inject backdoors into\nsubstitute models with a high success rate. The injected backdoor not only\nfacilitates ownership verification but also disrupts the functionality of\nsubstitute models, serving as a significant deterrent to model extraction\nattacks.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "This paper proposes a diffusion-based auto-bidding framework that leverages\ngraph representations to model large-scale auction environments. In such\nsettings, agents must dynamically optimize bidding strategies under constraints\ndefined by key performance indicator (KPI) metrics, all while operating in\ncompetitive environments characterized by uncertain, sparse, and stochastic\nvariables. To address these challenges, we introduce a novel approach combining\nlearnable graph-based embeddings with a planning-based latent diffusion model\n(LDM). By capturing patterns and nuances underlying the interdependence of\nimpression opportunities and the multi-agent dynamics of the auction\nenvironment, the graph representation enable expressive computations regarding\nauto-bidding outcomes. With reward alignment techniques, the LDM's posterior is\nfine-tuned to generate auto-bidding trajectories that maximize KPI metrics\nwhile satisfying constraint thresholds. Empirical evaluations on both\nreal-world and synthetic auction environments demonstrate significant\nimprovements in auto-bidding performance across multiple common KPI metrics, as\nwell as accuracy in forecasting auction outcomes.",
        "We show that the thermodynamic limit of a many-body system can reveal\nentanglement properties that are hard to detect in finite-size systems --\nsimilar to how phase transitions only sharply emerge in the thermodynamic\nlimit. The resulting operational entanglement properties are in one-to-one\ncorrespondence with abstract properties of the local observable algebras that\nemerge in the thermodynamic limit. These properties are insensitive to finite\nperturbations and hence describe the \\emph{large-scale structure of\nentanglement} of many-body systems. We formulate and discuss the emerging\nstructures and open questions, both for gapped and gapless many-body systems.\nIn particular, we show that every gapped phase of matter, even the trivial one,\nin $D\\geq 2$ dimensions contains models with the strongest possible bipartite\nlarge-scale entanglement. Conversely, we conjecture the existence of\ntopological phases of matter, where all representatives have the strongest form\nof entanglement.",
        "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
        "We derive equivariant localization formulas of Atiyah--Bott and cohomological\nfield theory types in the Batalin-Vilkovisky formalism and discuss their\napplications in Poisson geometry and quantum field theory.",
        "In recent years, researchers have increasingly sought batteries as an\nefficient and cost-effective solution for energy storage and supply, owing to\ntheir high energy density, low cost, and environmental resilience. However, the\nissue of dendrite growth has emerged as a significant obstacle in battery\ndevelopment. Excessive dendrite growth during charging and discharging\nprocesses can lead to battery short-circuiting, degradation of electrochemical\nperformance, reduced cycle life, and abnormal exothermic events. Consequently,\nunderstanding the dendrite growth process has become a key challenge for\nresearchers. In this study, we investigated dendrite growth mechanisms in\nbatteries using a combined machine learning approach, specifically a\ntwo-dimensional artificial convolutional neural network (CNN) model, along with\ncomputational methods. We developed two distinct computer models to predict\ndendrite growth in batteries. The CNN-1 model employs standard convolutional\nneural network techniques for dendritic growth prediction, while CNN-2\nintegrates additional physical parameters to enhance model robustness. Our\nresults demonstrate that CNN-2 significantly enhances prediction accuracy,\noffering deeper insights into the impact of physical factors on dendritic\ngrowth. This improved model effectively captures the dynamic nature of dendrite\nformation, exhibiting high accuracy and sensitivity. These findings contribute\nto the advancement of safer and more reliable energy storage systems.",
        "We prove global existence, uniqueness and $\\L1$ stability of solutions to\ngeneral systems of nonlocal conservation laws modeling multiclass vehicular\ntraffic. Each class follows its own speed law and has specific effects on the\nother classes' speeds. Moreover, general explicit dependencies of the speed\nlaws on space and time are allowed. Solutions are proved to depend continuously\n-- in suitable norms -- on all terms appearing in the equations, as well as on\nthe initial data. Numerical simulations show the relevance and the effects of\nthe nonlocal terms.",
        "In this paper, we introduce the new class of continua; weakly\ninfinite-dimensional closed set-aposyndetic continua. With this notion, we show\nthat there exists a non-D-continuum such that each positive Whitney level of\nthe hyperspace of the continuum is a weakly infinite-dimensional closed\nset-aposyndetic continuum. This result strengthens those of van Douwen and\nGoodykoontz [2], Illanes [7], and the main result of Illanes et al. [9].",
        "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.",
        "Increasing demands for computing power also propel the need for\nenergy-efficient SoC accelerator architectures. One class for such accelerators\nare so-called processor arrays, which typically integrate a two-dimensional\nmesh of interconnected processing elements (PEs). Such arrays are specifically\ndesigned to accelerate the execution of multidimensional nested loops by\nexploiting the intrinsic parallelism of such loops. Coarse-grained\nreconfigurable arrays (CGRAs) belong to this class of accelerator\narchitectures. In this work, we analyze four toolchains for mapping loop\nprograms onto CGRAs and compare the resulting mappings wrt. performance, i.e.,\nlatency. While most toolchains succeed in simpler kernels like general matrix\nmultiplication, some struggle to find valid mappings for more complex loops\nlike a triangular solver. Furthermore, we observe that the considered CGRA\nmappers generally tend to underutilize the available PEs.",
        "Prototype-based federated learning has emerged as a promising approach that\nshares lightweight prototypes to transfer knowledge among clients with data\nheterogeneity in a model-agnostic manner. However, existing methods often\ncollect prototypes directly from local models, which inevitably introduce\ninconsistencies into representation learning due to the biased data\ndistributions and differing model architectures among clients. In this paper,\nwe identify that both statistical and model heterogeneity create a vicious\ncycle of representation inconsistency, classifier divergence, and skewed\nprototype alignment, which negatively impacts the performance of clients. To\nbreak the vicious cycle, we propose a novel framework named Federated Learning\nvia Semantic Anchors (FedSA) to decouple the generation of prototypes from\nlocal representation learning. We introduce a novel perspective that uses\nsimple yet effective semantic anchors serving as prototypes to guide local\nmodels in learning consistent representations. By incorporating semantic\nanchors, we further propose anchor-based regularization with margin-enhanced\ncontrastive learning and anchor-based classifier calibration to correct feature\nextractors and calibrate classifiers across clients, achieving intra-class\ncompactness and inter-class separability of prototypes while ensuring\nconsistent decision boundaries. We then update the semantic anchors with these\nconsistent and discriminative prototypes, which iteratively encourage clients\nto collaboratively learn a unified data representation with robust\ngeneralization. Extensive experiments under both statistical and model\nheterogeneity settings show that FedSA significantly outperforms existing\nprototype-based FL methods on various classification tasks.",
        "This paper investigates the deep learning based approaches for simultaneous\nwireless information and power transfer (SWIPT). The quality-of-service (QoS)\nconstrained sum-rate maximization problems are, respectively, formulated for\npower-splitting (PS) receivers and time-switching (TS) receivers and solved by\na unified graph neural network (GNN) based model termed SWIPT net (SWIPTNet).\nTo improve the performance of SWIPTNet, we first propose a single-type output\nmethod to reduce the learning complexity and facilitate the satisfaction of QoS\nconstraints, and then, utilize the Laplace transform to enhance input features\nwith the structural information. Besides, we adopt the multi-head attention and\nlayer connection to enhance feature extracting. Furthermore, we present the\nimplementation of transfer learning to the SWIPTNet between PS and TS\nreceivers. Ablation studies show the effectiveness of key components in the\nSWIPTNet. Numerical results also demonstrate the capability of SWIPTNet in\nachieving near-optimal performance with millisecond-level inference speed which\nis much faster than the traditional optimization algorithms. We also show the\neffectiveness of transfer learning via fast convergence and expressive\ncapability improvement.",
        "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps:\/\/tkreiman.github.io\/projects\/mlff_distribution_shifts\/.",
        "Accurate 3D modelling of grapevines is crucial for precision viticulture,\nparticularly for informed pruning decisions and automated management\ntechniques. However, the intricate structure of grapevines poses significant\nchallenges for traditional skeletonization algorithms. This paper presents an\nadaptation of the Smart-Tree algorithm for 3D grapevine modelling, addressing\nthe unique characteristics of grapevine structures. We introduce a graph-based\nmethod for disambiguating skeletonization. Our method delineates individual\ncane skeletons, which are crucial for precise analysis and management. We\nvalidate our approach using annotated real-world grapevine point clouds,\ndemonstrating improvement of 15.8% in the F1 score compared to the original\nSmart-Tree algorithm. This research contributes to advancing 3D grapevine\nmodelling techniques, potentially enhancing both the sustainability and\nprofitability of grape production through more precise and automated\nviticulture practices",
        "This paper characterizes integration and coordination gains in dense\nmillimeter-wave ISAC networks through a dual-mode framework that combines\nmonostatic and multistatic sensing. A comprehensive system-level analysis is\nconducted, accounting for base station (BS) density, power allocation, antenna\nmisalignment, radar cross-section (RCS) fluctuations, clutter, bistatic\ngeometry, channel fading, and self-interference cancellation (SIC) efficiency.\nUsing stochastic geometry, coverage probabilities and ergodic rates for sensing\nand communication are derived, revealing tradeoffs among BS density, beamwidth,\nand power allocation. It is shown that the communication performance sustained\nreliable operation despite the overlaid sensing functionality. In contrast, the\nresults reveal the foundational role of spatial sensing diversity, driven by\nthe dual-mode operation, to compensate for the weak sensing reflections and\nvulnerability to imperfect SIC along with interference and clutter. To this\nend, we identify a system transition from monostatic to multistatic-dominant\nsensing operation as a function of the SIC efficiency. In the latter case,\nusing six multistatic BSs instead of a single bistatic receiver improved\nsensing coverage probability by over 100%, highlighting the coordination gain.\nMoreover, comparisons with pure communication networks confirm substantial\nintegration gain. Specifically, dual-mode networked sensing with four\ncooperative BSs can double throughput, while multistatic sensing alone improves\nthroughput by over 50%.",
        "A recent result on the Gauss image problem for pseudo-cones can be\ninterpreted as a measure transport, performed by the reverse radial Gauss map\nof a pseudo-cone. We find a cost function that is minimized by this transport\nmap, and we prove an analogue of Rockafellar's characterization of the\nsubdifferentials of convex functions.",
        "Increasingly many AI systems can plan and execute interactions in open-ended\nenvironments, such as making phone calls or buying online goods. As developers\ngrow the space of tasks that such AI agents can accomplish, we will need tools\nboth to unlock their benefits and manage their risks. Current tools are largely\ninsufficient because they are not designed to shape how agents interact with\nexisting institutions (e.g., legal and economic systems) or actors (e.g.,\ndigital service providers, humans, other AI agents). For example, alignment\ntechniques by nature do not assure counterparties that some human will be held\naccountable when a user instructs an agent to perform an illegal action. To\nfill this gap, we propose the concept of agent infrastructure: technical\nsystems and shared protocols external to agents that are designed to mediate\nand influence their interactions with and impacts on their environments. Agent\ninfrastructure comprises both new tools and reconfigurations or extensions of\nexisting tools. For example, to facilitate accountability, protocols that tie\nusers to agents could build upon existing systems for user authentication, such\nas OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue\nthat agent infrastructure will be similarly indispensable to ecosystems of\nagents. We identify three functions for agent infrastructure: 1) attributing\nactions, properties, and other information to specific agents, their users, or\nother actors; 2) shaping agents' interactions; and 3) detecting and remedying\nharmful actions from agents. We propose infrastructure that could help achieve\neach function, explaining use cases, adoption, limitations, and open questions.\nMaking progress on agent infrastructure can prepare society for the adoption of\nmore advanced agents.",
        "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.",
        "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
        "Recently Jiang-Jiang established a global (in time) existence result for\nunique strong solutions of the two-dimensional (2D) free-boundary problem of an\nincompressible Hookean viscoelastic fluid, the rest state of which is defined\nin a slab, in some classes of large data [28]. In particular, Jiang-Jiang's\nmathematical result shows that, if the initial free boundary is flat, the way\nthe elastic deformation under the large elasticity coefficient $\\kappa$ acts on\nthe free boundary prevents the natural tendency of the fluid to form\nsingularities, even when the initial velocity is properly large. However it is\nnot clear whether their result can be extended to the corresponding 3D case. In\nthis paper, we further find a similar result in the 3D stratified (immiscible)\ncompressible Hookean viscoelastic fluids in an infinite slab with two\nrestrictive conditions: that the elasticity coefficients of two fluids are\nequal, and that the initial density functions satisfy the asymptotic stability\ncondition in Lagrangian coordinates. These two restrictive conditions in the\ncompressible case contribute us to avoid the essential obstacles that would be\nfaced in the extension of Jiang-Jiang's result from two dimensions to our 3D\ncase. In addition, we can further obtain a new result regarding the vanishing\nphenomena of the nonlinear interactions of solutions with the fixed initial\nvelocity and the initial zero perturbation deformation. Such a new result\nroughly presents that the solutions of the problem considered by us can be\napproximated by the ones of a linear problem for sufficiently large $\\kappa$.",
        "Propositional Dynamic Logic, PDL, is a modal logic designed to formalize the\nreasoning about programs. By extending accessibility between states to states\nand state sets, concurrent propositional dynamic logic CPDL, is introduced to\ninclude concurrent programs due to Peleg and Goldblatt. We study a many-valued\ngeneralization of CPDL where the satisfiability and the reachability relation\nbetween states and state sets are graded over a finite {\\L}ukasiewicz chain.\nFinitely-valued dynamic logic has been shown to be useful in formalizing\nreasoning about program behaviors under uncertainty. We obtain completeness\nresults for all finitely valued PDL.",
        "Nonlinear thermoelastic systems play a crucial role in understanding thermal\nconductivity, stresses, elasticity, and temperature interactions. This research\nfocuses on finding solutions to these systems in their fractional forms, which\nis a significant aspect of the study. We consider various proposed models\nrelated to fractional thermoelasticity and derive results through sophisticated\nmethodologies. Numerical simulations are conducted for both fractional and\ninteger order thermoelastic coupled systems, with results presented in tables\nand graphs. The graphs indicate a close correspondence between the approximate\nand exact solutions. The solutions obtained demonstrate convergence for both\nfractional and integer order problems, ensuring accurate modeling. Furthermore,\nthe tables confirm that greater accuracy can be achieved by increasing the\nnumber of terms in the series of solutions.",
        "Long-context multiple-choice question answering tasks require robust\nreasoning over extensive text sources. Since most of the pre-trained\ntransformer models are restricted to processing only a few hundred words at a\ntime, successful completion of such tasks often relies on the identification of\nevidence spans, such as sentences, that provide supporting evidence for\nselecting the correct answer. Prior research in this domain has predominantly\nutilized pre-trained dense retrieval models, given the absence of supervision\nto fine-tune the retrieval process. This paper proposes a novel method called\nOptions Aware Dense Retrieval (OADR) to address these challenges. ORDA uses an\ninnovative approach to fine-tuning retrieval by leveraging query-options\nembeddings, which aim to mimic the embeddings of the oracle query (i.e., the\nquery paired with the correct answer) for enhanced identification of supporting\nevidence. Through experiments conducted on the QuALITY benchmark dataset, we\ndemonstrate that our proposed model surpasses existing baselines in terms of\nperformance and accuracy."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation",
    "start_abstract":"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)"
      ],
      "abstract":[
        "Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence."
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "Efficient sampling approaches based on generalized Golub-Kahan methods\n  for large-scale hierarchical Bayesian inverse problems",
        "Optimal Insurance under Endogenous Default and Background Risk",
        "Optimizing confidence in negative-partial-transpose-based entanglement\n  criteria",
        "On almost Gallai colourings in complete graphs",
        "Stabilization of an unstable reaction-diffusion PDE with input delay\n  despite state and input quantization",
        "Rigidity in a Fixed Number Field and a Directional $p$-Adic Littlewood\n  Conjecture for Algebraic Vectors",
        "Enhancing finite-difference based derivative-free optimization methods\n  with machine learning",
        "New Representations of Catalan's Constant, Apery's Constant and the\n  Euler Numbers Obtained from the Half Hyperbolic Secant Distribution",
        "The COSMOS-Web deep galaxy group catalog up to $z=3.7$",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Explaining the Unexplainable: A Systematic Review of Explainable AI in\n  Finance",
        "A new transcendence measure for the values of the exponential function\n  at algebraic arguments",
        "Sensitivity analysis of path-dependent options in an incomplete market\n  with pathwise functional Ito calculus",
        "Scattering resonances and pairing in a Rabi-coupled Fermi gas",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry",
        "Spatiotemporal Gaussian Optimization for 4D Cone Beam CT Reconstruction\n  from Sparse Projections",
        "Partial domination of middle graphs",
        "Lattice Boltzmann simulation reveals supercritical bifurcation in flow\n  mode transitions of power-law fluids in the four-roll mill",
        "Generic Structural Stability for $2 \\times 2$ Systems of Hyperbolic\n  Conservation Laws",
        "Analysis of the data on differential cross sections and spin density\n  matrix elements for $\\gamma p \\to \\rho^0 p$",
        "Ensemble Knowledge Distillation for Machine Learning Interatomic\n  Potentials",
        "Numerical verification of the Collatz conjecture for billion digit\n  random numbers",
        "Functional central limit theorem for the subgraph count of the voter\n  model on dynamic random graphs",
        "Multimaterial topology optimization for finite strain elastoplasticity:\n  theory, methods, and applications",
        "Exploring the wettability of liquid iron on refractory oxides with\n  sessile drop technique and density-functional derived Hamaker constants",
        "Characterizations of the semi-harmonious and harmonious quasi-projection\n  pairs on Hilbert $C^*$-modules",
        "Unveiling What Makes Saturn Ring: Quantifying the Amplitudes of Saturn's\n  Planetary Normal-Mode Oscillations and Trends in C-Ring Properties Using\n  Kronoseismology (VII)",
        "On the location of zeros of a quaternion polynomial",
        "Pauli Network Circuit Synthesis with Reinforcement Learning"
      ],
      "abstract":[
        "Uncertainty quantification for large-scale inverse problems remains a\nchallenging task. For linear inverse problems with additive Gaussian noise and\nGaussian priors, the posterior is Gaussian but sampling can be challenging,\nespecially for problems with a very large number of unknown parameters (e.g.,\ndynamic inverse problems) and for problems where computation of the square root\nand inverse of the prior covariance matrix are not feasible. Moreover, for\nhierarchical problems where several hyperparameters that define the prior and\nthe noise model must be estimated from the data, the posterior distribution may\nno longer be Gaussian, even if the forward operator is linear. Performing\nlarge-scale uncertainty quantification for these hierarchical settings requires\nnew computational techniques. In this work, we consider a hierarchical Bayesian\nframework where both the noise and prior variance are modeled as\nhyperparameters. Our approach uses Metropolis-Hastings independence sampling\nwithin Gibbs where the proposal distribution is based on generalized\nGolub-Kahan based methods. We consider two proposal samplers, one that uses a\nlow rank approximation to the conditional covariance matrix and another that\nuses a preconditioned Lanczos method. Numerical examples from seismic imaging,\ndynamic photoacoustic tomography, and atmospheric inverse modeling demonstrate\nthe effectiveness of the described approaches.",
        "This paper studies an optimal insurance problem for a utility-maximizing\nbuyer of insurance, subject to the seller's endogenous default and background\nrisk. An endogenous default occurs when the buyer's contractual indemnity\nexceeds the seller's available reserve, which is random due to the background\nrisk. We obtain an analytical solution to the optimal contract for two types of\ncontracts, differentiated by whether their indemnity functions depend on the\nseller's background risk. The results shed light on the joint effect of the\nseller's default and background risk on the buyer's insurance demand.",
        "A key requirement of any separable quantum state is that its density matrix\nhas a positive partial transpose. For continuous bipartite quantum states,\nviolation of this condition may be tested via the hierarchy of\nnegative-partial-transpose (NPT) based entanglement criteria introduced by\nShchukin and Vogel [Phys. Rev. Lett. 95, 230502 (2005)]. However, a procedure\nfor selecting the optimal NPT-based criterion is currently lacking. Here, we\ndevelop a framework to select the optimal criterion by determining the level of\nconfidence of criteria within the Shchukin and Vogel hierarchy for finite\nmeasurement number, environmental noise, and the optimal allocation of\nmeasurement resources. To demonstrate the utility of our approach, we apply our\nstatistical framework to prominent example Gaussian and non-Gaussian states,\nincluding the two-mode squeezed vacuum state, the quanta-subtracted two-mode\nsqueezed vacuum state, and the two-mode Schr\\\"odinger-cat state. Beyond\nbipartite inseparability tests, our framework can be applied to any Hermitian\nmatrix constructed of observable moments and thus can be utilized for a wide\nvariety of other nonclassicality criteria and multi-mode entanglement tests.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "We solve the global asymptotic stability problem of an unstable\nreaction-diffusion Partial Differential Equation (PDE) subject to input delay\nand state quantization developing a switched predictor-feedback law. To deal\nwith the input delay, we reformulate the problem as an actuated transport PDE\ncoupled with the original reaction-diffusion PDE. Then, we design a quantized\npredictor-based feedback mechanism that employs a dynamic switching strategy to\nadjust the quantization range and error over time. The stability of the\nclosed-loop system is proven properly combining backstepping with a small-gain\napproach and input-to-state stability techniques, for deriving estimates on\nsolutions, despite the quantization effect and the system's instability. We\nalso extend this result to the input quantization case.",
        "Let $X_n$ be the space of unimodular lattices in $\\RR^n$ and let $A$ be the\nfull diagonal group in $\\on{SL}_n(\\RR)$. It is known that compact $A$-orbits\noriginate from moduls in totally real degree $n$ number fields. Our first\nresult shows that for a natural family of compact orbits $(Ax_k)_k$ all\noriginating from a fixed number field $K$, every weak limit of the Haar\nmeasures on those orbits $m_{Ax_k}$ must contain the Haar measure $m_{X_n}$ as\nan ergodic component. This result generalizes certain aspects of the work by\nAka and Shapira in \\cite{Shapira-Aka} to arbitrary dimensions, as well as\nelements from Shapira-Zheng in \\cite{shapira2021translates}.\n  For every vector $\\overline \\alpha\\in \\RR^n$ and for every rational\napproximation $(\\overline p,q)\\in \\RR^n\\times\\RR$ we can associate the\ndisplacement vector $q\\alpha-\\overline p$. We focus on algebraic vectors,\nnamely $\\overline \\alpha=(\\alpha_1,\\dots,\\alpha_n)$ such that $1, \\alpha_1,\n\\dots, \\alpha_n$ span a rank $n$ number field. For these vectors, we\ninvestigate the size of their displacements as well as the distribution of\ntheir directions. We establish that algebraic vector $\\overline \\alpha$ satisfy\nthe $p$-adic Littlewood Conjecture. Namely, we prove that \\begin{equation}\n  \\liminf_{k \\to \\infty} \\left( k \\|k\\|_p \\right)^{1\/n} \\| k (\\alpha_1, \\dots,\n\\alpha_n) \\|_\\infty = 0. \\end{equation} Additionally, we classify all limiting\ndistributions, with a special weighting, of the sequence of directions of the\ndefects in the $\\varepsilon$-approximations of $(\\alpha_1, \\dots, \\alpha_n)$.\nEach such limiting measure is expressed as the pushforward of an algebraic\nmeasure on $X_n$ to the sphere.\n  Our proof relies on estimates of the asymptotic orders of units in fixed\nnumber fields modulo families of natural numbers and on rigidity results from\n\\cite{ELMV1}.",
        "Derivative-Free Optimization (DFO) involves methods that rely solely on\nevaluations of the objective function. One of the earliest strategies for\ndesigning DFO methods is to adapt first-order methods by replacing gradients\nwith finite-difference approximations. The execution of such methods generates\na rich dataset about the objective function, including iterate points, function\nvalues, approximate gradients, and successful step sizes. In this work, we\npropose a simple auxiliary procedure to leverage this dataset and enhance the\nperformance of finite-difference-based DFO methods. Specifically, our procedure\ntrains a surrogate model using the available data and applies the gradient\nmethod with Armijo line search to the surrogate until it fails to ensure\nsufficient decrease in the true objective function, in which case we revert to\nthe original algorithm and improve our surrogate based on the new available\ninformation. As a proof of concept, we integrate this procedure with the\nderivative-free method proposed in (Optim. Lett. 18: 195--213, 2024). Numerical\nresults demonstrate significant performance improvements, particularly when the\napproximate gradients are also used to train the surrogates.",
        "New expressions and bounds for Catalan's and Apery's constants, derived from\nthe half hyperbolic secant distribution, are presented. These constants are\nobtained by using expressions for the Lorenz curve, the Gini and Theil indices,\nconvolutions and a mixture of distributions, among other approaches. The new\nexpressions are presented both in terms of integral (simple and double)\nrepresentation and also as an interesting series representation. Some of these\nfeatures are well known, while others are new. In addition, some integral\nrepresentations of Euler's numbers are obtained.",
        "Galaxy groups with $M_{tot} \\lesssim 10^{14}$ $M_\\odot$ and up to a few tens\nof members are the most common galaxy environment, marking the transition\nbetween field and massive clusters. Identifying groups plays a crucial role in\nunderstanding structure formation and galaxy evolution. Modern deep surveys\nallow us to build well-characterized samples of groups up to the regime where\nstructures were taking shape. We aimed to build the largest deep catalog of\ngalaxy groups to date over the COSMOS-Web field effective area of 0.45 deg$^2$,\nleveraging the deep high quality data of the new COSMOS-Web photometric catalog\nresulted from the James Webb Space Telescope observations of the COSMOS-Web\nfield. We performed the group search with the AMICO algorithm, a linear matched\nfilter based on an analytical model for the group signal. AMICO has already\nbeen tested in wide and deep field surveys, including COSMOS data up to $z=2$.\nIn this work, we tested the algorithm performances at even higher redshift and\nsearched for protocluster cores at $z>2$. We compiled a list of known\nprotoclusters in COSMOS at $2 \\leq z \\leq 3.7$, matched them with our\ndetections and studied the clustering of the detected cores. We estimated\npurity and completeness of our sample by creating data-driven mocks with the\nSinFoniA code and linked signal-to-noise to purity. We detected 1678 groups in\nthe COSMOS-Web field up to $z=3.7$, including lists of members extending nearly\ntwo magnitudes deeper than the previous AMICO-COSMOS catalog. 756 groups were\ndetected with purity of 80\\%. More than 500 groups have their redshift\nconfirmed by assigning spectroscopic counterparts. This group catalog offers a\nunique opportunity to explore galaxy evolution in different environments\nspanning $\\sim$12 Gyr and to study groups, from the least rich population to\nthe formation of the most massive clusters.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.",
        "Let $P\\in \\mathbb Z[X]\\setminus\\{0\\}$ be of degree $\\delta\\ge 1$ and usual\nheight $H\\ge 1$, and let $\\alpha\\in \\overline{\\mathbb Q}^*$ be of degree $d\\ge\n2$. Mahler proved in 1931 the following transcendence measure for $e^\\alpha$:\nfor any $\\varepsilon\\&gt;0$, there exists $c\\&gt;0$ such that $\\vert\nP(e^\\alpha)\\vert\\&gt;c\/H^{\\mu(d,\\delta)+\\varepsilon}$ where the exponent\n$\\mu(d,\\delta)=(4d^2-2d)\\delta+2d-1$. Zheng obtained a better result in 1991\nwith $\\mu(d,\\delta)=(4d^2-2d)\\delta-1$. In this paper, we provide a new\nexplicit exponent $\\mu(d,\\delta)$ which improves on Zheng's transcendence\nmeasure for all $\\delta\\ge 2$ and all $d\\ge 2$. When $\\delta=1$, we recover his\nbound for all $d\\ge 2$, which had in fact already been obtained by Kappe in\n1966. Our improvement rests upon the optimization of an accessory parameter in\nSiegel's classical determinant method applied to Hermite-Pad{\\'e} approximants\nto powers of the exponential function.",
        "Functional It^o calculus is based on an extension of the classical It^o\ncalculus to functionals depending on the entire past evolution of the\nunderlying paths and not only on its current value. The calculus builds on\nFollmer's deterministic proof of the It^o formula, see [3], and a notion of\npathwise functional derivatives introduced by [5]. There are no smoothness\nassumptions required on the functionals, however, they are required to possess\ncertain directional derivatives which may be computed pathwise, see [6, 9, 8].\nUsing functional It^o calculus and the notion of quadratic variation, we derive\nthe functional It^o formula along with the Feynman-Kac formula for functional\nprocesses. Furthermore, we express the Greeks for path-dependent options as\nexpectations, which can be efficiently computed numerically using Monte Carlo\nsimulations. We illustrate these results by applying the formulae to digital\noptions within the Black-Scholes model framework.",
        "We investigate the possibility of using a Rabi drive to tune the interactions\nin an atomic Fermi gas. Specifically, we consider the scenario where two\nfermion species (spins) are Rabi coupled and interacting with a third uncoupled\nspecies. Using an exact calculation within a minimal low-energy model, we\nderive analytical expressions for the effective scattering length and effective\nrange that characterize the collisions between a Rabi-dressed atom and an atom\nfrom the third species. In particular, we find that new scattering resonances\nemerge in the Rabi-coupled system, which we demonstrate are linked to the\nexistence of hybrid two-body bound states. Furthermore, we show via a\ngeneralized Thouless criterion that the scattering properties have a direct\nimpact on the superfluid transitions in the Rabi-coupled Fermi gas. The\npresence of Rabi-induced resonances thus has implications for the investigation\nof many-body physics with driven atomic gases.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.",
        "In image-guided radiotherapy (IGRT), four-dimensional cone-beam computed\ntomography (4D-CBCT) is critical for assessing tumor motion during a patients\nbreathing cycle prior to beam delivery. However, generating 4D-CBCT images with\nsufficient quality requires significantly more projection images than a\nstandard 3D-CBCT scan, leading to extended scanning times and increased imaging\ndose to the patient. To address these limitations, there is a strong demand for\nmethods capable of reconstructing high-quality 4D-CBCT images from a 1-minute\n3D-CBCT acquisition. The challenge lies in the sparse sampling of projections,\nwhich introduces severe streaking artifacts and compromises image quality. This\npaper introduces a novel framework leveraging spatiotemporal Gaussian\nrepresentation for 4D-CBCT reconstruction from sparse projections, achieving a\nbalance between streak artifact reduction, dynamic motion preservation, and\nfine detail restoration. Each Gaussian is characterized by its 3D position,\ncovariance, rotation, and density. Two-dimensional X-ray projection images can\nbe rendered from the Gaussian point cloud representation via X-ray\nrasterization. The properties of each Gaussian were optimized by minimizing the\ndiscrepancy between the measured projections and the rendered X-ray\nprojections. A Gaussian deformation network is jointly optimized to deform\nthese Gaussian properties to obtain a 4D Gaussian representation for dynamic\nCBCT scene modeling. The final 4D-CBCT images are reconstructed by voxelizing\nthe 4D Gaussians, achieving a high-quality representation that preserves both\nmotion dynamics and spatial detail. The code and reconstruction results can be\nfound at https:\/\/github.com\/fuyabo\/4DGS_for_4DCBCT\/tree\/main",
        "For any graph $G=(V,E)$, a subset $S\\subseteq V$ is called {\\it an isolating\nset} of $G$ if $V\\setminus N_G[S]$ is an independent set of $G$, where\n$N_G[S]=S\\cup N_G(S)$, and {\\it the isolation number} of $G$, denoted by\n$\\iota(G)$, is the size of a smallest isolating set of $G$. In this article, we\nshow that the isolation number of the middle graph of $G$ is equal to the size\nof a smallest maximal matching of $G$.",
        "The four-roll mill has been traditionally viewed as a device generating\nsimple extensional flow with a central stagnation point. Our systematic\ninvestigation using a two-relaxation-time regularized lattice Boltzmann\n(TRT-RLB) model reveals unexpected richness in the flow physics, identifying\ntwo previously unreported supercritical bifurcation modes: a quadrifoliate\nvortex mode featuring four symmetrical counter-rotating vortices, and a\ndumbbell-shaped quad-vortex mode where vortices detach from but remain\nsymmetric about the stagnation point. The numerical framework, representing the\nfirst successful extension of TRT-RLB method to power-law fluid dynamics,\nenables comprehensive mapping of flow characteristics across Reynolds numbers\n($1 \\leq Re \\leq 50$), power-law indices ($0.7 \\leq n \\leq 1.3$), and geometric\nconfigurations. The transition from quadrifoliate vortex mode exhibits distinct\npathways depending on the power-law index: at relatively small $n$, the flow\nundergoes a direct supercritical bifurcation to simple extensional flow, while\nat relatively large $n$, it evolves through an intermediate dumbbell-shaped\nstate. Among geometric parameters, the roller radius $r$ emerges as the\ndominant factor controlling bifurcation points and vortex dimensions, whereas\nthe roller-container gap $\\delta$ exerts minimal influence on flow regimes. The\ntransitions between flow modes can be precisely characterized through the\nevolution of vortex dimensions and velocity gradients at the stagnation point,\nproviding quantitative criteria for flow regime identification. These findings\nenrich our fundamental understanding of bifurcation phenomena in extensional\ndevices and provide quantitative guidelines for achieving desired flow patterns\nin four-roll mill applications.",
        "This paper presents a proof of generic structural stability for Riemann\nsolutions to $2 \\times 2$ system of hyperbolic conservation laws in one spatial\nvariable, without diffusive terms. This means that for almost every left and\nright state, shocks and rarefaction solutions of the same type are preserved\nvia perturbations of the flux functions. The main assumptions for this proof\ninvolve standard assumptions on strict hyperbolicity and genuine non-linearity,\na technical assumption on directionality of rarefaction curve, and the regular\nmanifold (submersion) assumption motivated by concepts in differential\ntopology. We show that the structural stability of the Riemann solutions is\nrelated to the transversality of the Hugoniot loci and rarefaction curves in\nthe state space. The regular manifold assumption is required to invoke a\nvariant of a theorem from differential topology, Thom's parametric\ntransversality theorem, to illustrate the genericity of transversality of these\ncurves. This in turn implies the genericity of structural stability. We then\nillustrate the applications of this theorem to two examples: the p-system and a\n$2 \\times 2$ system governing the evolution of gravity-driven monodisperse\nparticle-laden thin films. In particular, we illustrate how one can verify all\nthe above assumptions for the former, and apply the theorem to different\nnumerical and physical aspects of the system governing the latter.",
        "The newly published data on spin density matrix elements from the GlueX\nCollaboration, along with the previously released differential cross-section\ndata from the CLAS Collaboration and the other two experiments for the $\\gamma\np \\to \\rho^0 p$ reaction, are systematically investigated using an effective\nLagrangian approach within the tree-level Born approximation. The model\ncombines contributions from $t$-channel meson exchanges ($\\pi$, $\\eta$, and\n$f_2$), $s$-channel nucleon ($N$) and nucleon resonance ($N^\\ast$) exchanges,\n$u$-channel $N$ exchange, and a generalized contact term to construct the\nscattering amplitudes. Regge propagators are employed for $t$-channel\namplitudes to incorporate the contributions from mesons with various spins\nlying on the same trajectories. The analysis shows that the background\ncontributions, dominated by the $f_2$-trajectory exchange, provide a\nsatisfactory description of the data in the high-energy and forward-angle\nregions. The inclusion of specific nucleon resonances, such as\n$N(2100)1\/2^{+}$, $N(2060)5\/2^{-}$, or $\\Delta(2000)5\/2^{+}$, significantly\nimproves the description of the differential cross-section data at\nnear-perpendicular scattering angles in the low-energy region. Predictions of\nphoton beam and target nucleon asymmetries are provided, offering valuable\ninsights to discriminate reaction mechanisms when corresponding data become\navailable in the future.",
        "Machine learning interatomic potentials (MLIPs) are a promising tool to\naccelerate atomistic simulations and molecular property prediction. The quality\nof MLIPs strongly depends on the quantity of available training data as well as\nthe quantum chemistry (QC) level of theory used to generate that data. Datasets\ngenerated with high-fidelity QC methods, such as coupled cluster, are typically\nrestricted to small molecules and may be missing energy gradients. With this\nlimited quantity of data, it is often difficult to train good MLIP models. We\npresent an ensemble knowledge distillation (EKD) method to improve MLIP\naccuracy when trained to energy-only datasets. In our EKD approach, first,\nmultiple teacher models are trained to QC energies and then used to generate\natomic forces for all configurations in the dataset. Next, a student MLIP is\ntrained to both QC energies and to ensemble-averaged forces generated by the\nteacher models. We apply this workflow on the ANI-1ccx dataset which consists\nof organic molecules with configuration energies computed at the coupled\ncluster level of theory. The resulting student MLIPs achieve new\nstate-of-the-art accuracy on the out-of-sample COMP6 benchmark and improved\nstability for molecular dynamics simulations. The EKD approach for MLIP is\nbroadly applicable for chemical, biomolecular and materials science\nsimulations.",
        "The Collatz conjecture, also known as the 3n+1 problem, is one of the most\npopular open problems in number theory. In this note, an algorithm for the\nverification of the Collatz conjecture is presented that works on a standard PC\nfor numbers with up to ten billion decimal places.",
        "In this paper we consider two-opinion voter models on dynamic random graphs,\nin which the joint dynamics of opinions and graphs acts as one-way feedback,\ni.e., edges appear and disappear over time depending on the opinions of the two\nconnected vertices, while the opinion dynamics does not depend on the edge\nprocess. Our goal is to investigate the joint evolution of the entries of a\nvoter subgraph count vector, i.e., vector of subgraphs where each vertex has a\nspecific opinion, in the regime that the number of vertices grows large. The\nmain result of this paper is a functional central limit theorem. In particular,\nwe prove that, under a proper centering and scaling, the joint functional of\nthe vector of subgraph counts converges to a specific multidimensional Gaussian\nprocess.",
        "Plasticity is inherent to many engineering materials such as metals. While it\ncan degrade the load-carrying capacity of structures via material yielding, it\ncan also protect structures through plastic energy dissipation. To fully\nharness plasticity, here we present the theory, method, and application of a\ntopology optimization framework that simultaneously optimizes structural\ngeometries and material phases to customize the stiffness, strength, and\nstructural toughness of designs experiencing finite strain elastoplasticity.\nThe framework accurately predicts structural responses by employing a rigorous,\nmechanics-based elastoplasticity theory that ensures isochoric plastic flow. It\nalso effectively identifies optimal material phase distributions using a\ngradient-based optimizer, where gradient information is obtained via a reversed\nadjoint method to address history dependence, along with automatic\ndifferentiation to compute the complex partial derivatives. We demonstrate the\nframework by optimizing a range of 2D and 3D elastoplastic structures,\nincluding energy-dissipating dampers, load-carrying beams, impact-resisting\nbumpers, and cold working profiled sheets. These optimized multimaterial\nstructures reveal important mechanisms for improving design performance under\nlarge deformation, such as the transition from kinematic to isotropic hardening\nwith increasing displacement amplitudes and the formation of twisted regions\nthat concentrate stress, enhancing plastic energy dissipation. Through the\nsuperior performance of these optimized designs, we demonstrate the framework's\neffectiveness in tailoring elastoplastic responses across various spatial\nconfigurations, material types, hardening behaviors, and combinations of\ncandidate materials. This work offers a systematic approach for optimizing\nnext-generation multimaterial structures with elastoplastic behaviors under\nlarge deformations.",
        "The macroscopic interactions of liquid iron and solid oxides, such as\nalumina, calcia, magnesia, silica, and zirconia manifest the behavior and\nefficiency of high-temperature metallurgical processes. The oxides serve dual\nroles, both as components of refractory materials in submerged entry nozzles\nand also as significant constituents of non-metallic inclusions in the melt. It\nis therefore crucial to understand the physicochemical interplay between the\nliquid and the oxides in order to address the nozzle clogging challenges, and\nthereby optimize cast iron and steel production. This paper presents a\nmethodology for describing these interactions by combining the materials'\ndielectric responses, computed within the density functional theory, with the\nCasimir-Lifshitz dispersion forces to generate the Hamaker constants. The\napproach provides a comprehensive understanding of the wettability of iron\nagainst these refractory oxides, revealing the complex relation between\nmolecular and macroscopic properties. Our theoretically determined crystalline\nstructures are confirmed by room-temperature X-ray diffraction, and the contact\nangles of liquid iron on the oxides are validated with a sessile drop system at\nthe temperature 1823 K. For comparison, we also present the wettability of the\noxides by a liquid tin-bismuth alloy. The findings are essential in advancing\nthe fundamental understanding of interfacial interactions in metallurgical\nscience, and are also pivotal in driving the development of more efficient and\nreliable steelmaking processes.",
        "For each adjointable idempotent $Q$ on a Hilbert $C^*$-module $H$, a specific\nprojection $m(Q)$ called the matched projection of $Q$ was introduced recently\ndue to the characterization of the minimum value among all the distances from\nprojections to $Q$. Inspired by the relationship between $m(Q)$ and $Q$,\nanother term called the quasi-projection pair $(P,Q)$ was also introduced\nrecently, where $P$ is a projection on $H$ satisfying $Q^*=(2P-I)Q(2P-I)$, in\nwhich $Q^*$ is the adjoint operator of the idempotent $Q$ and $I$ is the\nidentity operator on $H$. This paper aims to make systematical\ncharacterizations of the semi-harmonious and harmonious quasi-projection pairs\non Hilbert $C^*$-modules, and meanwhile to provide examples illustrating the\nnon-triviality of the associated characterizations.",
        "Certain spiral density waves in Saturn's rings are generated through\nresonances with planetary normal modes, making them valuable probes of Saturn's\ninternal structure. Previous research has primarily focused on the rotation\nrates of these waves. However, other characteristics of these waves also\ncontain valuable information about the planet's interior. In this work, we\ninvestigate the amplitudes of the waves across the C-ring by analyzing high\nsignal-to-noise profiles derived from phase-corrected averages of occultation\nprofiles obtained by Cassini's Visual and Infrared Mapping Spectrometer (VIMS).\nBy fitting these wave profiles to linear density wave models, we estimate the\nring's surface mass density, mass extinction coefficient and effective\nkinematic viscosity at 34 locations in the C-ring, as well as the amplitude of\nthe gravitational potential perturbations associated with 6 satellite\nresonances and 28 planetary normal mode resonances.\n  Our estimates of the C-ring's mass extinction coefficient, indicate that the\ntypical particle mass density is around 0.3 g\/cm^3 interior to 84,000 km, but\ncan get as low as 0.03 g\/cm^3 exterior to 84,000 km. We also find the ring's\nviscosity is reduced in the outer C-ring, which is consistent with the\nexceptionally high porosity of the particles in this region. Meanwhile, we find\nthe amplitudes of Saturn's normal modes are complex functions of frequency, l\nand m, implying that multiple factors influence how efficiently these modes are\nexcited. This analysis identified two primary sources of these normal-mode\noscillations: a deep source located close to Saturn's core, and a shallow\nsource residing near the surface.",
        "In this paper, we are concerned with the problem of locating the zeros of\npolynomials of a quaternionic variable with quaternionic coefficients. We\nderive some new Cauchy bounds for the zeros of a polynomial by virtue of\nmaximum modulus theorem. Our results will generalise some recently proved\nresults about the distribution of zeros of a quaternionic polynomial.",
        "We introduce a Reinforcement Learning (RL)-based method for re-synthesis of\nquantum circuits containing arbitrary Pauli rotations alongside Clifford\noperations. By collapsing each sub-block to a compact representation and then\nsynthesizing it step-by-step through a learned heuristic, we obtain circuits\nthat are both shorter and compliant with hardware connectivity constraints. We\nfind that the method is fast enough and good enough to work as an optimization\nprocedure: in direct comparisons on 6-qubit random Pauli Networks against\nstate-of-the-art heuristic methods, our RL approach yields over 2x reduction in\ntwo-qubit gate count, while executing in under 10 milliseconds per circuit. We\nfurther integrate the method into a collect-and-re-synthesize pipeline, applied\nas a Qiskit transpiler pass, where we observe average improvements of 20% in\ntwo-qubit gate count and depth, reaching up to 60% for many instances, across\nthe Benchpress benchmark. These results highlight the potential of RL-driven\nsynthesis to significantly improve circuit quality in realistic, large-scale\nquantum transpilation workloads."
      ]
    }
  },
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm",
    "start_abstract":"The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A method for real-time optimal heliostat aiming strategy generation via deep learning"
      ],
      "abstract":[
        "Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Uncovering the Iceberg in the Sea: Fundamentals of Pulse Shaping and\n  Modulation Design for Random ISAC Signals",
        "Efficient Image Restoration via Latent Consistency Flow Matching",
        "Quantum Computer Controlled by Superconducting Digital Electronics at\n  Millikelvin Temperature",
        "ESPARGOS: An Ultra Low-Cost, Realtime-Capable Multi-Antenna WiFi Channel\n  Sounder",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Graph Neural Network Flavor Tagger and measurement of\n  $\\mathrm{sin}2\\beta$ at Belle II",
        "Remining Hard Negatives for Generative Pseudo Labeled Domain Adaptation",
        "Channel Resolvability Using Multiplicative Weight Update Algorithm",
        "DEFEND: A Large-scale 1M Dataset and Foundation Model for Tobacco\n  Addiction Prevention",
        "Which Code Statements Implement Privacy Behaviors in Android\n  Applications?",
        "Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image\n  Models?",
        "A Survey on the Optimization of Large Language Model-based Agents",
        "Algorithmic Clustering based on String Compression to Extract P300\n  Structure in EEG Signals",
        "Deep Reinforcement Learning based Triggering Function for Early\n  Classifiers of Time Series",
        "Decentralized Online Ensembles of Gaussian Processes for Multi-Agent\n  Systems",
        "On the convergence of split exponential integrators for semilinear\n  parabolic problems",
        "Voting Scheme to Strengthen Localization Security in Randomly Deployed\n  Wireless Sensor Networks",
        "Memristor Applications: Nanodevices Redefining Technological Landscapes",
        "Mean-Field Limits for Nearly Unstable Hawkes Processes",
        "GI-SLAM: Gaussian-Inertial SLAM",
        "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap",
        "Image-Space Gridding for Nonrigid Motion-Corrected MR Image\n  Reconstruction",
        "Refining local-type primordial non-Gaussianity: Sharpened $b_\\phi$\n  constraints through bias expansion",
        "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems",
        "A Heliocentric-orbiting Objects Processing System (HOPS) for the Wide\n  Field Survey Telescope: Architecture, Processing Workflow, and Preliminary\n  Results",
        "Learning Fair Policies for Infectious Diseases Mitigation using Path\n  Integral Control",
        "Accelerating Linear Recurrent Neural Networks for the Edge with\n  Unstructured Sparsity",
        "Position: Open and Closed Large Language Models in Healthcare",
        "AdaPTS: Adapting Univariate Foundation Models to Probabilistic\n  Multivariate Time Series Forecasting"
      ],
      "abstract":[
        "Integrated Sensing and Communications (ISAC) is expected to play a pivotal\nrole in future 6G networks. To maximize time-frequency resource utilization, 6G\nISAC systems must exploit data payload signals, that are inherently random, for\nboth communication and sensing tasks. This paper provides a comprehensive\nanalysis of the sensing performance of such communication-centric ISAC signals,\nwith a focus on modulation and pulse shaping design to reshape the statistical\nproperties of their auto-correlation functions (ACFs), thereby improving the\ntarget ranging performance. We derive a closed-form expression for the\nexpectation of the squared ACF of random ISAC signals, considering arbitrary\nmodulation bases and constellation mappings within the Nyquist pulse shaping\nframework. The structure is metaphorically described as an ``iceberg hidden in\nthe sea\", where the ``iceberg'' represents the squared mean of the ACF of\nrandom ISAC signals, that is determined by the pulse shaping filter, and the\n``sea level'' characterizes the corresponding variance, caused by the\nrandomness of the data payload. Our analysis shows that, for QAM\/PSK\nconstellations with Nyquist pulse shaping, Orthogonal Frequency Division\nMultiplexing (OFDM) achieves the lowest ranging sidelobe level across all lags.\nBuilding on these insights, we propose a novel Nyquist pulse shaping design to\nenhance the sensing performance of random ISAC signals. Numerical results\nvalidate our theoretical findings, showing that the proposed pulse shaping\nsignificantly reduces ranging sidelobes compared to conventional root-raised\ncosine (RRC) pulse shaping, thereby improving the ranging performance.",
        "Recent advances in generative image restoration (IR) have demonstrated\nimpressive results. However, these methods are hindered by their substantial\nsize and computational demands, rendering them unsuitable for deployment on\nedge devices. This work introduces ELIR, an Efficient Latent Image Restoration\nmethod. ELIR operates in latent space by first predicting the latent\nrepresentation of the minimum mean square error (MMSE) estimator and then\ntransporting this estimate to high-quality images using a latent consistency\nflow-based model. Consequently, ELIR is more than 4x faster compared to the\nstate-of-the-art diffusion and flow-based approaches. Moreover, ELIR is also\nmore than 4x smaller, making it well-suited for deployment on\nresource-constrained edge devices. Comprehensive evaluations of various image\nrestoration tasks show that ELIR achieves competitive results, effectively\nbalancing distortion and perceptual quality metrics while offering improved\nefficiency in terms of memory and computation.",
        "Current superconducting quantum computing platforms face significant scaling\nchallenges, as individual signal lines are required for control of each qubit.\nThis wiring overhead is a result of the low level of integration between\ncontrol electronics at room temperature and qubits operating at millikelvin\ntemperatures, which raise serious doubts among technologists about whether\nutility-scale quantum computers can be built. A promising alternative is to\nutilize cryogenic, superconducting digital control electronics that coexist\nwith qubits. Here, we report the first multi-qubit system integrating this\ntechnology. The system utilizes digital demultiplexing, breaking the linear\nscaling of control lines to number of qubits. We also demonstrate single-qubit\nfidelities above 99%, and up to 99.9%. This work is a critical step forward in\nrealizing highly scalable chip-based quantum computers.",
        "Multi-antenna channel sounding is a technique for measuring the propagation\ncharacteristics of electromagnetic waves that is commonly employed for\nparameterizing channel models. Channel sounders are usually custom-built from\nmany Software Defined Radio receivers, making them expensive to procure and\ndifficult to operate, which constrains the set of users to a few specialized\nscientific institutions and industrial research laboratories. Recent\ndevelopments in Joint Communications and Sensing (JCaS) extend the possible\nuses of channel data to applications like human activity recognition, human\npresence detection, user localization and wireless Channel Charting, all of\nwhich are of great interest to security researchers, experts in industrial\nautomation and others. However, due to a lack of affordable, easy-to-use and\ncommercially available multi-antenna channel sounders, those scientific\ncommunities can be hindered by their lack of access to wireless channel\nmeasurements. To lower the barrier to entry for channel sounding, we develop an\nultra low-cost measurement hardware platform based on mass-produced WiFi chips,\nwhich is easily affordable to research groups and even hobbyists.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "We present GFlaT, a new algorithm that uses a graph-neural-network to\ndetermine the flavor of neutral B mesons produced in $\\mathrm{\\Upsilon(4S)}$\ndecays. We evaluate its performance using $B$ decays to flavor-specific\nhadronic final states reconstructed in a $362$ $\\mathrm{fb}^{-1}$ sample of\nelectron-positron collisions recorded at the $\\mathrm{\\Upsilon(4S)}$ resonance\nwith the Belle II detector at the SuperKEKB collider. We achieve an effective\ntagging efficiency of $(37.40 \\pm 0.43 \\pm 0.36) \\%$, where the first\nuncertainty is statistical and the second systematic, which is $18\\%$ better\nthan the previous Belle II algorithm. Demonstrating the algorithm, we use $B^0\n\\to J\/\\psi K_\\mathrm{S}^0$ decays to measure the direct and mixing-induced CP\nviolation parameters, $C = (-0.035 \\pm 0.026 \\pm 0.013)$ and $S = (0.724 \\pm\n0.035 \\pm 0.014)$, from which we obtain $\\beta = (23.2 \\pm 1.5 \\pm\n0.6)^{\\circ}$.",
        "Dense retrievers have demonstrated significant potential for neural\ninformation retrieval; however, they exhibit a lack of robustness to domain\nshifts, thereby limiting their efficacy in zero-shot settings across diverse\ndomains. A state-of-the-art domain adaptation technique is Generative Pseudo\nLabeling (GPL). GPL uses synthetic query generation and initially mined hard\nnegatives to distill knowledge from cross-encoder to dense retrievers in the\ntarget domain. In this paper, we analyze the documents retrieved by the\ndomain-adapted model and discover that these are more relevant to the target\nqueries than those of the non-domain-adapted model. We then propose refreshing\nthe hard-negative index during the knowledge distillation phase to mine better\nhard negatives. Our remining R-GPL approach boosts ranking performance in 13\/14\nBEIR datasets and 9\/12 LoTTe datasets. Our contributions are (i) analyzing hard\nnegatives returned by domain-adapted and non-domain-adapted models and (ii)\napplying the GPL training with and without hard-negative re-mining in LoTTE and\nBEIR datasets.",
        "We study the channel resolvability problem, which is used to prove strong\nconverse of identification via channel. Channel resolvability has been solved\nby only random coding in the literature. We prove channel resolvability using\nthe multiplicative weight update algorithm. This is the first approach to\nchannel resolvability using non-random coding.",
        "While tobacco advertising innovates at unprecedented speed, traditional\nsurveillance methods remain frozen in time, especially in the context of social\nmedia. The lack of large-scale, comprehensive datasets and sophisticated\nmonitoring systems has created a widening gap between industry advancement and\npublic health oversight. This paper addresses this critical challenge by\nintroducing Tobacco-1M, a comprehensive dataset of one million tobacco product\nimages with hierarchical labels spanning 75 product categories, and DEFEND, a\nnovel foundation model for tobacco product understanding. Our approach\nintegrates a Feature Enhancement Module for rich multimodal representation\nlearning, a Local-Global Visual Coherence mechanism for detailed feature\ndiscrimination, and an Enhanced Image-Text Alignment strategy for precise\nproduct characterization. Experimental results demonstrate DEFEND's superior\nperformance, achieving 83.1% accuracy in product classification and 73.8% in\nvisual question-answering tasks, outperforming existing methods by significant\nmargins. Moreover, the model exhibits robust zero-shot learning capabilities\nwith 45.6% accuracy on novel product categories. This work provides regulatory\nbodies and public health researchers with powerful tools for monitoring\nemerging tobacco products and marketing strategies, potentially revolutionizing\napproaches to tobacco control and public health surveillance.",
        "A \"privacy behavior\" in software is an action where the software uses\npersonal information for a service or a feature, such as a website using\nlocation to provide content relevant to a user. Programmers are required by\nregulations or application stores to provide privacy notices and labels\ndescribing these privacy behaviors. Although many tools and research prototypes\nhave been developed to help programmers generate these notices by analyzing the\nsource code, these approaches are often fairly coarse-grained (i.e., at the\nlevel of whole methods or files, rather than at the statement level). But this\nis not necessarily how privacy behaviors exist in code. Privacy behaviors are\nembedded in specific statements in code. Current literature does not examine\nwhat statements programmers see as most important, how consistent these views\nare, or how to detect them. In this paper, we conduct an empirical study to\nexamine which statements programmers view as most-related to privacy behaviors.\nWe find that expression statements that make function calls are most associated\nwith privacy behaviors, while the type of privacy label has little effect on\nthe attributes of the selected statements. We then propose an approach to\nautomatically detect these privacy-relevant statements by fine-tuning three\nlarge language models with the data from the study. We observe that the\nagreement between our approach and participants is comparable to or higher than\nan agreement between two participants. Our study and detection approach can\nhelp programmers understand which statements in code affect privacy in mobile\napplications.",
        "Text-to-Image (T2I) models have recently gained significant attention due to\ntheir ability to generate high-quality images and are consequently used in a\nwide range of applications. However, there are concerns about the gender bias\nof these models. Previous studies have shown that T2I models can perpetuate or\neven amplify gender stereotypes when provided with neutral text prompts.\nResearchers have proposed automated gender bias uncovering detectors for T2I\nmodels, but a crucial gap exists: no existing work comprehensively compares the\nvarious detectors and understands how the gender bias detected by them deviates\nfrom the actual situation. This study addresses this gap by validating previous\ngender bias detectors using a manually labeled dataset and comparing how the\nbias identified by various detectors deviates from the actual bias in T2I\nmodels, as verified by manual confirmation. We create a dataset consisting of\n6,000 images generated from three cutting-edge T2I models: Stable Diffusion XL,\nStable Diffusion 3, and Dreamlike Photoreal 2.0. During the human-labeling\nprocess, we find that all three T2I models generate a portion (12.48% on\naverage) of low-quality images (e.g., generate images with no face present),\nwhere human annotators cannot determine the gender of the person. Our analysis\nreveals that all three T2I models show a preference for generating male images,\nwith SDXL being the most biased. Additionally, images generated using prompts\ncontaining professional descriptions (e.g., lawyer or doctor) show the most\nbias. We evaluate seven gender bias detectors and find that none fully capture\nthe actual level of bias in T2I models, with some detectors overestimating bias\nby up to 26.95%. We further investigate the causes of inaccurate estimations,\nhighlighting the limitations of detectors in dealing with low-quality images.\nBased on our findings, we propose an enhanced detector...",
        "With the rapid development of Large Language Models (LLMs), LLM-based agents\nhave been widely adopted in various fields, becoming essential for autonomous\ndecision-making and interactive tasks. However, current work typically relies\non prompt design or fine-tuning strategies applied to vanilla LLMs, which often\nleads to limited effectiveness or suboptimal performance in complex\nagent-related environments. Although LLM optimization techniques can improve\nmodel performance across many general tasks, they lack specialized optimization\ntowards critical agent functionalities such as long-term planning, dynamic\nenvironmental interaction, and complex decision-making. Although numerous\nrecent studies have explored various strategies to optimize LLM-based agents\nfor complex agent tasks, a systematic review summarizing and comparing these\nmethods from a holistic perspective is still lacking. In this survey, we\nprovide a comprehensive review of LLM-based agent optimization approaches,\ncategorizing them into parameter-driven and parameter-free methods. We first\nfocus on parameter-driven optimization, covering fine-tuning-based\noptimization, reinforcement learning-based optimization, and hybrid strategies,\nanalyzing key aspects such as trajectory data construction, fine-tuning\ntechniques, reward function design, and optimization algorithms. Additionally,\nwe briefly discuss parameter-free strategies that optimize agent behavior\nthrough prompt engineering and external knowledge retrieval. Finally, we\nsummarize the datasets and benchmarks used for evaluation and tuning, review\nkey applications of LLM-based agents, and discuss major challenges and\npromising future directions. Our repository for related references is available\nat https:\/\/github.com\/YoungDubbyDu\/LLM-Agent-Optimization.",
        "P300 is an Event-Related Potential widely used in Brain-Computer Interfaces,\nbut its detection is challenging due to inter-subject and temporal variability.\nThis work introduces a clustering methodology based on Normalized Compression\nDistance (NCD) to extract the P300 structure, ensuring robustness against\nvariability. We propose a novel signal-to-ASCII transformation to generate\ncompression-friendly objects, which are then clustered using a hierarchical\ntree-based method and a multidimensional projection approach. Experimental\nresults on two datasets demonstrate the method's ability to reveal relevant\nP300 structures, showing clustering performance comparable to state-of-the-art\napproaches. Furthermore, analysis at the electrode level suggests that the\nmethod could assist in electrode selection for P300 detection. This\ncompression-driven clustering methodology offers a complementary tool for EEG\nanalysis and P300 identification.",
        "Early Classification of Time Series (ECTS) has been recognized as an\nimportant problem in many areas where decisions have to be taken as soon as\npossible, before the full data availability, while time pressure increases.\nNumerous ECTS approaches have been proposed, based on different triggering\nfunctions, each taking into account various pieces of information related to\nthe incoming time series and\/or the output of a classifier. Although their\nperformances have been empirically compared in the literature, no studies have\nbeen carried out on the optimality of these triggering functions that involve\n``man-tailored'' decision rules. Based on the same information, could there be\nbetter triggering functions? This paper presents one way to investigate this\nquestion by showing first how to translate ECTS problems into Reinforcement\nLearning (RL) ones, where the very same information is used in the state space.\nA thorough comparison of the performance obtained by ``handmade'' approaches\nand their ``RL-based'' counterparts has been carried out. A second question\ninvestigated in this paper is whether a different combination of information,\ndefining the state space in RL systems, can achieve even better performance.\nExperiments show that the system we describe, called \\textsc{Alert},\nsignificantly outperforms its state-of-the-art competitors on a large number of\ndatasets.",
        "Flexible and scalable decentralized learning solutions are fundamentally\nimportant in the application of multi-agent systems. While several recent\napproaches introduce (ensembles of) kernel machines in the distributed setting,\nBayesian solutions are much more limited. We introduce a fully decentralized,\nasymptotically exact solution to computing the random feature approximation of\nGaussian processes. We further address the choice of hyperparameters by\nintroducing an ensembling scheme for Bayesian multiple kernel learning based on\nonline Bayesian model averaging. The resulting algorithm is tested against\nBayesian and frequentist methods on simulated and real-world datasets.",
        "Splitting the exponential-like $\\varphi$ functions, which typically appear in\nexponential integrators, is attractive in many situations since it can\ndramatically reduce the computational cost of the procedure. However, depending\non the employed splitting, this can result in order reduction. The aim of this\npaper is to analyze different such split approximations. We perform the\nanalysis for semilinear problems in the abstract framework of commuting\nsemigroups and derive error bounds that depend, in particular, on whether the\nvector (to which the $\\varphi$ functions are applied) satisfies appropriate\nboundary conditions. We then present the convergence analysis for two split\nversions of a second-order exponential Runge--Kutta integrator in the context\nof analytic semigroups, and show that one suffers from order reduction while\nthe other does not. Numerical results for semidiscretized parabolic PDEs\nconfirm the theoretical findings.",
        "This work aspires to provide a trustworthy solution for target localization\nin adverse environments, where malicious nodes, capable of manipulating\ndistance measurements (i.e., performing spoofing attacks), are present, thus\nhindering accurate localization. Besides localization, its other goal is to\nidentify (detect) which of the nodes participating in the process are\nmalicious. This problem becomes extremely important with the forthcoming\nexpansion of IoT and smart cities applications, that depend on accurate\nlocalization, and the presence of malicious attackers can represent serious\nsecurity threats if not taken into consideration. This is the case with most\nexisting localization systems which makes them highly vulnerable to spoofing\nattacks. In addition, existing methods that are intended for adversarial\nsettings consider very specific settings or require additional knowledge about\nthe system model, making them only partially secure. Therefore, this work\nproposes a novel voting scheme based on clustering and weighted central mass to\nsecurely solve the localization problem and detect attackers. The proposed\nsolution has two main phases: 1) Choosing a cluster of suitable points of\ninterest by taking advantage of the problem geometry to assigning votes in\norder to localize the target, and 2) Attacker detection by exploiting the\nlocation estimate and basic statistics. The proposed method is assessed in\nterms of localization accuracy, success in attacker detection, and\ncomputational complexity in different settings. Computer simulations and\nreal-world experiments corroborate the effectiveness of the proposed scheme\ncompared to state-of-the-art methods, showing that it can accomplish an error\nreduction of $30~\\%$ and is capable of achieving almost perfect attacker\ndetection rate when the ratio between attacker intensity and noise standard\ndeviation is significant.",
        "A memristor, a two-terminal nanodevice, has garnered substantial attention in\nrecent years due to its distinctive properties and versatile applications.\nThese nanoscale components, characterized by their simplicity of manufacture,\nscalability in small dimensions, nonvolatile memory capabilities, and\nadaptability to low-power platforms, offer a wealth of opportunities for\ntechnological innovation. Memristors hold great promise in diverse fields,\nranging from advanced memory devices and neuromorphic computing to\nenergy-efficient circuits and more. As we delve into this report, our aim is to\nprovide a succinct but thorough exploration of the expanding landscape of\nmemristor applications. Through the meticulous examination of scholarly\nliterature, we systematically documented pivotal research milestones. By\npreserving historical consistency in our approach, we aim to unveil the\nintricate spectrum of possibilities that memristors offer, according to which\nthey can revolutionize and enhance various domains of electronics and\ncomputing.",
        "In this paper, we establish general scaling limits for nearly unstable Hawkes\nprocesses in a mean-field regime by extending the method introduced by Jaisson\nand Rosenbaum. Under a mild asymptotic criticality condition on the\nself-exciting kernels $\\{\\phi^n\\}$, specifically $\\|\\phi^n\\|_{L^1} \\to 1$, we\nfirst show that the scaling limits of these Hawkes processes are necessarily\nstochastic Volterra diffusions of affine type. Moreover, we establish a\npropagation of chaos result for Hawkes systems with mean-field interactions,\nhighlighting three distinct regimes for the limiting processes, which depend on\nthe asymptotics of $n(1-\\|\\phi^n\\|_{L^1})^2$. These results provide a\nsignificant generalization of the findings by Delattre, Fournier and Hoffmann.",
        "3D Gaussian Splatting (3DGS) has recently emerged as a powerful\nrepresentation of geometry and appearance for dense Simultaneous Localization\nand Mapping (SLAM). Through rapid, differentiable rasterization of 3D\nGaussians, many 3DGS SLAM methods achieve near real-time rendering and\naccelerated training. However, these methods largely overlook inertial data,\nwitch is a critical piece of information collected from the inertial\nmeasurement unit (IMU). In this paper, we present GI-SLAM, a novel\ngaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking\nmodule and a realistic 3D Gaussian-based scene representation for mapping. Our\nmethod introduces an IMU loss that seamlessly integrates into the deep learning\nframework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the\naccuracy, robustness and efficiency of camera tracking. Moreover, our SLAM\nsystem supports a wide range of sensor configurations, including monocular,\nstereo, and RGBD cameras, both with and without IMU integration. Our method\nachieves competitive performance compared with existing state-of-the-art\nreal-time methods on the EuRoC and TUM-RGBD datasets.",
        "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps:\/\/github.com\/YuanchenBei\/Awesome-Cold-Start-Recommendation.",
        "Motion remains a major challenge in magnetic resonance (MR) imaging,\nparticularly in free-breathing cardiac MR imaging, where data are acquired over\nmultiple heartbeats at varying respiratory phases. We adopt a model-based\napproach for nonrigid motion correction, addressing two challenges: (a) motion\nrepresentation and (b) motion estimation. For motion representation, we derive\nimage-space gridding by adapting the nonuniform fast Fourier transform (NUFFT)\nto represent and compute nonrigid motion, which provides an exact\nforward-adjoint pair of linear operators. We then introduce nonrigid SENSE\noperators that incorporate nonrigid motion into the multi-coil MR acquisition\nmodel. For motion estimation, we employ both low-resolution 3D image-based\nnavigators (iNAVs) and high-resolution 3D self-navigating image-based\nnavigators (self-iNAVs). During each heartbeat, data are acquired along two\ntypes of non-Cartesian trajectories: a subset of a high-resolution trajectory\nthat sparsely covers 3D k-space, followed by a full low-resolution trajectory.\nWe reconstruct 3D iNAVs for each heartbeat using the full low-resolution data,\nwhich are then used to estimate bulk motion and identify the respiratory phase\nof each heartbeat. By combining data from multiple heartbeats within the same\nrespiratory phase, we reconstruct high-resolution 3D self-iNAVs, allowing\nestimation of nonrigid respiratory motion. For each respiratory phase, we\nconstruct the nonrigid SENSE operator, reformulating the nonrigid\nmotion-corrected reconstruction as a standard regularized inverse problem. In a\npreliminary study, the proposed method enhanced sharpness of the coronary\narteries and improved image quality in non-cardiac regions, outperforming\ntranslational motion-corrected reconstruction.",
        "Local-type primordial non-Gaussianity (PNG), predicted by many non-minimal\nmodels of inflation, creates a scale-dependent contribution to the power\nspectrum of large-scale structure (LSS) tracers. Its amplitude is characterized\nby the product $b_\\phi f_{\\rm NL}^{\\rm loc}$, where $b_\\phi$ is an\nastrophysical parameter dependent on the properties of the tracer. However,\n$b_\\phi$ exhibits significant secondary dependence on halo concentration and\nother astrophysical properties, which may bias and weaken the constraints on\n$f_{\\rm NL}^{\\rm loc}$. In this work, we demonstrate that incorporating\nknowledge of the relation between Lagrangian bias parameters and $b_\\phi$ can\nsignificantly enhance PNG constraints. We employ the Hybrid Effective Field\nTheory (HEFT) approach at the field-level and a linear regression model to seek\na connection between the bias parameters and $b_{\\phi}$ for halo and galaxy\nsamples, constructed using the \\textsc{AbacusSummit} simulation suite and\nmimicking the luminous red galaxies (LRGs) and quasi-stellar objects (QSOs) of\nthe Dark Energy Spectroscopic Instrument (DESI) survey. For the fixed-mass halo\nsamples, our full bias model reduces the uncertainty by more than 70\\%, with\nmost of that improvement coming from $b_\\nabla$, which we find to be an\nexcellent proxy for concentration. For the galaxy samples, our model reduces\nthe uncertainty on $b_\\phi$ by 80\\% for all tracers. By adopting\nLagrangian-bias informed priors on the parameter $b_\\phi$, future analyses can\nthus constrain $f_{\\rm NL}^{\\rm loc}$ with less bias and smaller errors.",
        "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems.",
        "Wide-field surveys have markedly enhanced the discovery and study of solar\nsystem objects (SSOs). The 2.5-meter Wide Field Survey Telescope (WFST)\nrepresents the foremost facility dedicated to optical time-domain surveys in\nthe northern hemisphere. To fully exploit WFST's capabilities for SSO\ndetection, we have developed a heliocentric-orbiting objects processing system\n(HOPS) tailored for identifying these objects. This system integrates\nHelioLinC3D, an algorithm well suited for the WFST survey cadence,\ncharacterized by revisiting the same sky field twice on the majority of nights.\nIn this paper, we outline the architecture and processing flow of our SSO\nprocessing system. The application of the system to the WFST pilot survey data\ncollected between March and May 2024 demonstrates exceptional performance in\nterms of both temporal efficiency and completeness. A total of 658,489\nobservations encompassing 38,520 known asteroids have been documented, and 241\nnewly discovered asteroids have been assigned provisional designations. In\nparticular, 27% of these new discoveries were achieved using merely two\nobservations per night on three nights. The preliminary results not only\nilluminate the effectiveness of integrating HelioLinC3D within the SSO\nprocessing system, but also emphasize the considerable potential contributions\nof WFST to the field of solar system science.",
        "Infectious diseases pose major public health challenges to society,\nhighlighting the importance of designing effective policies to reduce economic\nloss and mortality. In this paper, we propose a framework for sequential\ndecision-making under uncertainty to design fairness-aware disease mitigation\npolicies that incorporate various measures of unfairness. Specifically, our\napproach learns equitable vaccination and lockdown strategies based on a\nstochastic multi-group SIR model. To address the challenges of solving the\nresulting sequential decision-making problem, we adopt the path integral\ncontrol algorithm as an efficient solution scheme. Through a case study, we\ndemonstrate that our approach effectively improves fairness compared to\nconventional methods and provides valuable insights for policymakers.",
        "Linear recurrent neural networks enable powerful long-range sequence modeling\nwith constant memory usage and time-per-token during inference. These\narchitectures hold promise for streaming applications at the edge, but\ndeployment in resource-constrained environments requires hardware-aware\noptimizations to minimize latency and energy consumption. Unstructured sparsity\noffers a compelling solution, enabling substantial reductions in compute and\nmemory requirements--when accelerated by compatible hardware platforms. In this\npaper, we conduct a scaling study to investigate the Pareto front of\nperformance and efficiency across inference compute budgets. We find that\nhighly sparse linear RNNs consistently achieve better efficiency-performance\ntrade-offs than dense baselines, with 2x less compute and 36% less memory at\niso-accuracy. Our models achieve state-of-the-art results on a real-time\nstreaming task for audio denoising. By quantizing our sparse models to\nfixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic\nchip for real-time processing, we translate model compression into tangible\ngains of 42x lower latency and 149x lower energy consumption compared to a\ndense model on an edge GPU. Our findings showcase the transformative potential\nof unstructured sparsity, paving the way for highly efficient recurrent neural\nnetworks in real-world, resource-constrained environments.",
        "This position paper analyzes the evolving roles of open-source and\nclosed-source large language models (LLMs) in healthcare, emphasizing their\ndistinct contributions and the scientific community's response to their\ndevelopment. Due to their advanced reasoning capabilities, closed LLMs, such as\nGPT-4, have dominated high-performance applications, particularly in medical\nimaging and multimodal diagnostics. Conversely, open LLMs, like Meta's LLaMA,\nhave gained popularity for their adaptability and cost-effectiveness, enabling\nresearchers to fine-tune models for specific domains, such as mental health and\npatient communication.",
        "Pre-trained foundation models (FMs) have shown exceptional performance in\nunivariate time series forecasting tasks. However, several practical challenges\npersist, including managing intricate dependencies among features and\nquantifying uncertainty in predictions. This study aims to tackle these\ncritical limitations by introducing adapters; feature-space transformations\nthat facilitate the effective use of pre-trained univariate time series FMs for\nmultivariate tasks. Adapters operate by projecting multivariate inputs into a\nsuitable latent space and applying the FM independently to each dimension.\nInspired by the literature on representation learning and partially stochastic\nBayesian neural networks, we present a range of adapters and\noptimization\/inference strategies. Experiments conducted on both synthetic and\nreal-world datasets confirm the efficacy of adapters, demonstrating substantial\nenhancements in forecasting accuracy and uncertainty quantification compared to\nbaseline methods. Our framework, AdaPTS, positions adapters as a modular,\nscalable, and effective solution for leveraging time series FMs in multivariate\ncontexts, thereby promoting their wider adoption in real-world applications. We\nrelease the code at https:\/\/github.com\/abenechehab\/AdaPTS."
      ]
    }
  },
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A method for real-time optimal heliostat aiming strategy generation via deep learning",
    "start_abstract":"Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm"
      ],
      "abstract":[
        "The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "A hybrid pressure formulation of the face-centred finite volume method\n  for viscous laminar incompressible flows",
        "Chemically-Accurate Prediction of the Ionisation Potential of Helium\n  Using a Quantum Processor",
        "Testing the QCD formation time with reconstructed parton splittings",
        "Cauchy Random Features for Operator Learning in Sobolev Space",
        "Grid-based exoplanet atmospheric mass loss predictions through neural\n  network",
        "Hyper-neutron stars from an ab initio calculation",
        "The Method of ${\\cal M}_{n}$-Extension: The KdV Equation",
        "Stronger Constraints on Primordial Black Holes as Dark Matter Derived\n  from the Thermal Evolution of the Intergalactic Medium over the Last Twelve\n  Billion Years",
        "Impulsive mixing of stellar populations in dwarf spheroidal galaxies",
        "A Flux-Tunable cavity for Dark matter detection",
        "Apparent teleportation of indistinguishable particles",
        "Long Lived Quasinormal Modes of Regular and Extreme Black Holes",
        "Positive Feedback: How a Synergy Between the Streaming Instability and\n  Dust Coagulation Forms Planetesimals",
        "Signs of Non-Monotonic Finite-Volume Corrections to $g_A$",
        "Metamaterials that learn to change shape",
        "Nonequilibrium Green's Function Formalism Applicable to Discrete\n  Impurities in Semiconductor Nanostructures",
        "Dark matter spiral arms in Milky Way-like halos",
        "Man-in-the-Middle Attacks Targeting Quantum Cryptography",
        "Ground States for the NLS on graphs with an attractive potential",
        "Non-local functionals, total variation, and Gamma-convergence with\n  respect to area-strict convergence",
        "Representation Theorems for Convex Expectations and Semigroups on Path\n  Space",
        "Totally bounded ultrametric spaces and locally finite trees",
        "Forecasting the Volatility of Energy Transition Metals",
        "Searching for continuous gravitational waves from highly deformed\n  compact objects with DECIGO",
        "Settling the no-$(k+1)$-in-line problem when $k$ is not small",
        "A Quantum Good Authentication Protocol",
        "Future collider sensitivities to $\\nu$SMEFT interactions",
        "On a theorem of Harder"
      ],
      "abstract":[
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "This work presents a hybrid pressure face-centred finite volume (FCFV) solver\nto simulate steady-state incompressible Navier-Stokes flows. The method\nleverages the robustness, in the incompressible limit, of the hybridisable\ndiscontinuous Galerkin paradigm for compressible and weakly compressible flows\nto derive the formulation of a novel, low-order face-based discretisation. The\nincompressibility constraint is enforced in a weak sense, by introducing an\ninter-cell mass flux defined in terms of a new, hybrid variable, representing\nthe pressure at the cell faces. This results in a new hybridisation strategy\nwhere cell variables (velocity, pressure and deviatoric strain rate tensor) are\nexpressed as a function of velocity and pressure at the barycentre of the cell\nfaces. The hybrid pressure formulation provides first-order convergence of all\nvariables, including the stress, independently of cell type, stretching and\ndistortion. Numerical benchmarks of Navier-Stokes flows at low and moderate\nReynolds numbers, in two and three dimensions, are presented to evaluate\naccuracy and robustness of the method. In particular, the hybrid pressure\nformulation outperforms the FCFV method when convective effects are relevant,\nachieving accurate predictions on significantly coarser meshes.",
        "Quantum computers have the potential to revolutionise our understanding of\nthe microscopic behaviour of materials and chemical processes by enabling\nhigh-accuracy electronic structure calculations to scale more efficiently than\nis possible using classical computers. Current quantum computing hardware\ndevices suffer from the dual challenges of noise and cost, which raises the\nquestion of what practical value these devices might offer before full fault\ntolerance is achieved and economies of scale enable cheaper access. Here we\nexamine the practical value of noisy quantum computers as tools for\nhigh-accuracy electronic structure, by using a Quantinuum ion-trap quantum\ncomputer to predict the ionisation potential of helium. By combining a series\nof techniques suited for use with current hardware including qubit-efficient\nencoding coupled with chemical insight, low-cost variational optimisation with\nhardware-adapted quantum circuits, and moments-based corrections, we obtain an\nionisation potential of 24.5536 (+0.0011, -0.0005) eV, which agrees with the\nexperimentally measured value to within true chemical accuracy, and with high\nstatistical confidence. The methods employed here can be generalised to predict\nother properties and expand our understanding of the value that might be\nprovided by near-term quantum computers.",
        "In high-energy elementary collisions the space-time ordering of parton\nbranching processes is not accessible experimentally. In contrast, in heavy-ion\ncollisions, parton showers interact with a spatially extended dense medium.\nThis sets a reference length scale with respect to which the space-time\nordering may be analysed. Here, we explore the possibility of identifying\nexperimental signatures of the QCD formation time, $\\tau_f$, on the level of a\nsingle parton splitting. Since heavy flavour offers an additional handle on\ntracing the propagation of individual quarks through the medium, we focus on\nthe $g\\to c\\bar{c}$ splitting. Combining adapted versions of the\nCambridge-Aachen and FlavourCone jet finding algorithms with grooming\ntechniques, we show how the kinematics of such splittings can be reconstructed\nwith high fidelity using either final state partons or hadrons, and how the\nformation time distribution of parton splittings can be constructed therefrom.\nMedium modification leads to a characteristic modification of this $\\tau_f$\ndistribution. This effect can be used to construct experimentally-accessible\nratios of $\\tau_f$ distributions, in which the sensitivity of the medium\nmodification to the QCD formation time becomes measurable.",
        "Operator learning is the approximation of operators between infinite\ndimensional Banach spaces using machine learning approaches. While most\nprogress in this area has been driven by variants of deep neural networks such\nas the Deep Operator Network and Fourier Neural Operator, the theoretical\nguarantees are often in the form of a universal approximation property.\nHowever, the existence theorems do not guarantee that an accurate operator\nnetwork is obtainable in practice. Motivated by the recent kernel-based\noperator learning framework, we propose a random feature operator learning\nmethod with theoretical guarantees and error bounds. The random feature method\ncan be viewed as a randomized approximation of a kernel method, which\nsignificantly reduces the computation requirements for training. We provide a\ngeneralization error analysis for our proposed random feature operator learning\nmethod along with comprehensive numerical results. Compared to kernel-based\nmethod and neural network methods, the proposed method can obtain similar or\nbetter test errors across benchmarks examples with significantly reduced\ntraining times. An additional advantages it that our implementation is simple\nand does require costly computational resources, such as GPU.",
        "The fast and accurate estimation of planetary mass-loss rates is critical for\nplanet population and evolution modelling. We use machine learning (ML) for\nfast interpolation across an existing large grid of hydrodynamic upper\natmosphere models, providing mass-loss rates for any planet inside the grid\nboundaries with superior accuracy compared to previously published\ninterpolation schemes. We consider an already available grid comprising about\n11000 hydrodynamic upper atmosphere models for training and generate an\nadditional grid of about 250 models for testing purposes. We develop the ML\ninterpolation scheme (dubbed \"atmospheric Mass Loss INquiry frameworK\"; MLink)\nusing a Dense Neural Network, further comparing the results with what was\nobtained employing classical approaches (e.g. linear interpolation and radial\nbasis function-based regression). Finally, we study the impact of the different\ninterpolation schemes on the evolution of a small sample of carefully selected\nsynthetic planets. MLink provides high-quality interpolation across the entire\nparameter space by significantly reducing both the number of points with large\ninterpolation errors and the maximum interpolation error compared to previously\navailable schemes. For most cases, evolutionary tracks computed employing MLink\nand classical schemes lead to comparable planetary parameters at\nGyr-timescales. However, particularly for planets close to the top edge of the\nradius gap, the difference between the predicted planetary radii at a given age\nof tracks obtained employing MLink and classical interpolation schemes can\nexceed the typical observational uncertainties. Machine learning can be\nsuccessfully used to estimate atmospheric mass-loss rates from model grids\npaving the way to explore future larger and more complex grids of models\ncomputed accounting for more physical processes.",
        "The equation of state (EoS) of neutron matter plays a decisive role to\nunderstand the neutron star properties and the gravitational waves from neutron\nstar mergers. At sufficient densities, the appearance of hyperons generally\nsoftens the EoS, leading to a reduction in the maximum mass of neutron stars\nwell below the observed values of about 2 solar masses. Even though repulsive\nthree-body forces are known to solve this so-called ``hyperon puzzle'', so far\nperforming \\textit{ab initio} calculations with a substantial number of\nhyperons for neutron star properties has remained elusive. Starting from the\nnewly developed auxiliary field quantum Monte Carlo algorithm to simulate\nhyper-neutron matter (HNM) without any sign oscillations, we derive three\ndistinct EoSs by employing the state-of-the-art Nuclear Lattice Effective Field\nTheory. We include $N\\Lambda$, $\\Lambda\\Lambda$ two-body forces, $NN\\Lambda$,\nand $N\\Lambda\\Lambda$ three-body forces. Consequently, we determine essential\nastrophysical quantities such as the neutron star mass, radius, tidal\ndeformability, and the universal $I$-Love-$Q$ relation. The maximum mass,\nradius and tidal deformability of a $1.4M_\\odot$ neutron star are predicted to\nbe $2.17(1)(1)~M_\\odot$, $R_{1.4M\\odot}=13.10(1)(7)~$km, and\n$\\Lambda_{1.4M_\\odot}=597(5)(18)$, respectively, based on our most realistic\nEoS. These predictions are in good agreement with the latest astrophysical\nconstraints derived from observations of massive neutron stars, gravitational\nwaves, and joint mass-radius measurements. Also, for the first time in\n\\textit{ab initio} calculations, we investigate both non-rotating and rotating\nneutron star configurations. The results indicate that the impact of rotational\ndynamics on the maximum mass is small, regardless of whether hyperons are\npresent in the EoS or not.",
        "In this work we generalize ${\\cal M}_{2}$-extension that has been introduced\nrecently. For illustration we use the KdV equation. We present five different\n${\\cal M}_{3}$-extensions of the KdV equation and their recursion operators. We\ngive a compact form of ${\\cal M}_{n}$-extension of the KdV equation and\nrecursion operator of the coupled KdV system. The method of ${\\cal\nM}_{n}$-extension can be applied to any integrable scalar equation to obtain\nintegrable multi-field system of equations. We also present unshifted and\nshifted nonlocal reductions of an example of ${\\cal M}_{3}$-extension of KdV.",
        "Primordial black holes (PBHs) have been explored as potential dark matter\ncandidates, with various astrophysical observations placing upper limits on the\nfraction $f_\\mathrm{PBH}$ of dark matter in the form of PBHs. However, a\nlargely underutilized probe of PBH abundance is the temperature of the\nintergalactic medium (IGM), inferred from the thermal broadening of absorption\nlines in the Lyman-$\\alpha$ forest of quasar spectra. PBHs inject energy into\nthe IGM via Hawking radiation, altering its thermal evolution. In this work, we\nconstrain this energy injection by self-consistently modeling its interplay\nwith the cosmological ultraviolet background from galaxies and supermassive\nblack holes. Leveraging IGM temperature measurements spanning the past twelve\nbillion years ($z \\sim 0$ to $6$), we derive one of the most stringent\nconstraints on PBH-induced heating from light PBHs within the mass range\n$10^{15}\\unicode{x2013}10^{17}$ g. Specifically, for $M_\\mathrm{PBH} = 10^{16}$\ng, we find $f_\\mathrm{PBH} < 5 \\times 10^{-5}$ at 95% confidence, with the\nbound scaling approximately as $M_\\mathrm{PBH}^{4}$ at other masses. Our\ninclusion of helium reionization and low-redshift temperature measurements\nstrengthens previous IGM-based PBH constraints by an order of magnitude or\nmore. Compared to other existing limits, our result is among the strongest,\nsecond only to the constraints from the 511 keV line from the Galactic Centre,\nbut with distinct systematics. More broadly, this study highlights the IGM\nthermal history as a powerful and independent probe of beyond-standard-model\nphysics.",
        "We study the response of mono-energetic stellar populations with initially\nisotropic kinematics to impulsive and adiabatic changes to an underlying dark\nmatter potential. Half-light radii expand and velocity dispersions decrease as\nenclosed dark matter is removed. The details of this expansion and cooling\ndepend on the time scale on which the underlying potential changes. In the\nadiabatic regime, the product of half-light radius and average velocity\ndispersion is conserved. We show that the stellar populations maintain\ncentrally isotropic kinematics throughout their adiabatic evolution, and their\ndensities can be approximated by a family of analytical radial profiles.\nMetallicity gradients within the galaxy flatten as dark matter is slowly\nremoved. In the case of strong impulsive perturbations, stellar populations\ndevelop power-law-like density tails with radially biased kinematics. We show\nthat the distribution of stellar binding energies within the dark matter halo\nsubstantially widens after an impulsive perturbation, no matter the sign of the\nperturbation. This allows initially energetically separated stellar populations\nto mix, to the extent that previously chemo-dynamically distinct populations\nmay masquerade as a single population with large metallicity and energy spread.\nFinally, we show that in response to an impulsive perturbation, stellar\npopulations that are deeply embedded in cored dark matter halos undergo a\nseries of damped oscillations before reaching a virialised equilibrium state,\ndriven by inefficient phase mixing in the harmonic potentials of cored halos.\nThis slow return to equilibrium adds substantial systematic uncertainty to\ndynamical masses estimated from Jeans modeling or the virial theorem.",
        "Developing a dark matter detector with wide mass tunability is an immensely\ndesirable property, yet it is challenging due to maintaining strong\nsensitivity. Resonant cavities for dark matter detection have traditionally\nemployed mechanical tuning, moving parts around to change electromagnetic\nboundary conditions. However, these cavities have proven challenging to operate\nin sub-Kelvin cryogenic environments due to differential thermal contraction,\nlow heat capacities, and low thermal conductivities. Instead, we develop an\nelectronically tunable cavity architecture by coupling a superconducting 3D\nmicrowave cavity with a DC flux tunable SQUID. With a flux delivery system\nengineered to maintain high coherence in the cavity, we perform a hidden-photon\ndark matter search below the quantum-limited threshold. A microwave photon\ncounting technique is employed through repeated quantum non-demolition\nmeasurements using a transmon qubit. With this device, we perform a\nhidden-photon search with a dark count rate of around 64 counts\/s and constrain\nthe kinetic mixing angle to ${\\varepsilon}< 4\\times 10^{-13}$ in a tunable band\nfrom 5.672 GHz to 5.694 GHz. By coupling multimode tunable cavities to the\ntransmon, wider hidden-photon searching ranges are possible.",
        "Teleportation, introduced in science fiction literature, is an instantaneous\nchange of the position of a microscopic object. Two teleportation-like\nphenomena were predicted by quantum mechanics: quantum teleportation and,\nrecently, quantum particle teleportation. The former is investigated\nexperimentally and has applications in quantum communication and computing.\n  Here, we introduced the third teleportation-like phenomenon - an apparent\nteleportation. It seems to be a natural consequence of elementary particles and\nantiparticles of the Standard Model being indistinguishable. We give an example\nof a process leading to the apparent teleportation within a toy model of\nboson-like particles. It utilizes the local transport of particles and\nantiparticles and the local creation and annihilation of particle-antiparticle\npairs. Furthermore, we suggest a method to observe the apparent teleportation\nin nucleus-nucleus collisions at properly selected collision energy. The method\nrequires the measurement of correlations between momenta of charm and anticharm\nhadrons in collisions with a single $c\\bar{c}$ pair being produced. The\nultimate prediction following the apparent teleportation hypothesis is the\nuncorrelated emission of charm and anticharm hadrons. It can be tested by\ncontemporary experiments.\n  Observing the apparent teleportation would uncover the basic transport\nproperties of indistinguishable particles. In particular, the apparent\nteleportation may explain the rapid thermalisation of the system created in\ncollisions of two atomic nuclei. Theoretical and experimental efforts are\nneeded to observe the apparent teleportation processes and study their\nproperties.",
        "Recently, black hole models in a nonlinear modification of the Maxwell\nelectrodynamics were suggested, possessing simultaneously properties of an\nextreme charge and regularity (Bronnikov K. A., Phys. Rev. D, 110 (2024)\n024021). We study quasinormal modes of a massive scalar field around such black\nholes and show that they are characterized by a comparatively small damping\nrate, indicating the possible existence of arbitrarily long-lived quasinormal\nmodes, called quasi-resonances.",
        "One of the most important open questions in planet formation is how dust\ngrains in a protoplanetary disk manage to overcome growth barriers and form the\n$\\sim$100km planet building blocks that we call planetesimals. There appears to\nbe a gap between the largest grains that can be produce by coagulation, and the\nsmallest grains that are needed for the streaming instability (SI) to form\nplanetesimals. Here we explore a novel hypothesis: That dust coagulation and\nthe SI work in tandem. That they form a feedback loop where each one boosts the\naction of the other to bridge the gap between dust grains and planetesimals. We\ndevelop a semi-analytical model of dust concentration due to the SI, and an\nanalytic model of how the SI affects the fragmentation and radial drift\nbarriers. We then combine those to model our proposed feedback loop. In the\nfragmentation-limited regime, we find a powerful synergy between the SI and\ndust growth that drastically increases both grain sizes and densities. We find\nthat a midplane dust-to-gas ratio of $\\epsilon \\ge 0.3$ is a sufficient\ncondition for the feedback loop to reach the planetesimal-forming region for\nturbulence values $10^{-4} \\le \\alpha \\le 10^{-3}$ and grain sizes $0.01 \\le\n{\\rm St} \\le 0.1$. In contrast, the drift-limited regime only shows grain\ngrowth, without significant dust accumulation. Planet formation in the\ndrift-limited portion of the disk may require other processes (particle traps)\nto halt radial drift.",
        "We study finite-volume (FV) corrections to determinations of $g_A$ via\nlattice quantum chromodynamics (QCD) using analytic results and numerical\nanalysis. We observe that $SU(2)$ Heavy Baryon Chiral Perturbation Theory does\nnot provide an unambiguous prediction for the sign of the FV correction, which\nis not surprising when one also considers large-$N_c$ constraints on the axial\ncouplings. We further show that non-monotonic FV corrections are naturally\nallowed when one considers either including explicit $\\Delta$-resonance degrees\nof freedom or one works to higher orders in the chiral expansion. We\ninvestigate the potential impact of these FV corrections with a precision study\nof $g_A$ using models of FV corrections that are monotonic and non-monotonic.\nUsing lattice QCD data that is approximately at the 1% level of precision, we\ndo not see significant evidence of non-monotonic corrections. Looking forward\nto the next phase of lattice QCD calculations, we estimate that calculations\nthat are between the 0.1%-1%-level of precision may be sensitive to these FV\nartifacts. Finally, we present an update of the CalLat prediction of $g_A$ in\nthe isospin limit with sub-percent precision, $g_A^{\\rm QCD} = 1.2674(96)$.",
        "Learning to change shape is a fundamental strategy of adaptation and\nevolution of living organisms, from bacteria and cells to tissues and animals.\nHuman-made materials can also exhibit advanced shape morphing capabilities, but\nlack the ability to learn. Here, we build metamaterials that can learn complex\nshape-changing responses using a contrastive learning scheme. By being shown\nexamples of the target shape changes, our metamaterials are able to learn those\nshape changes by progressively updating internal learning degrees of freedom --\nthe local stiffnesses. Unlike traditional materials that are designed once and\nfor all, our metamaterials have the ability to forget and learn new shape\nchanges in sequence, to learn multiple shape changes that break reciprocity,\nand to learn multistable shape changes, which in turn allows them to perform\nreflex gripping actions and locomotion. Our findings establish metamaterials as\nan exciting platform for physical learning, which in turn opens avenues for the\nuse of physical learning to design adaptive materials and robots.",
        "A new theoretical framework for the nonequilibrium Green's function (NEGF)\nscheme is presented to account for the discrete nature of impurities doped in\nsemiconductor nanostructures. The short-range part of impurity potential is\nincluded as scattering potential in the self-energy due to spatially localized\nimpurity scattering, and the long-range part of impurity potential is treated\nas the self-consistent Hartree potential by coupling with the Poisson equation.\nThe position-dependent impurity scattering rate under inhomogeneous impurity\nprofiles is systematically derived so that its physical meaning is clarified.\nThe position dependence of the scattering rate turns out to be represented by\nthe `center of mass' coordinates in the Wigner coordinates, rather than the\nreal-space coordinates. Consequently, impurity scattering is intrinsically\nnonlocal in space. The proposed framework is applied to cylindrical thin wires\nunder the quasi-one-dimensional (quasi-1D) approximation. We show explicitly\nhow the discrete nature of impurities affects the transport properties such as\nelectrostatic potential, local density of states, carrier density, scattering\nrates, and mobility.",
        "The coupling between the dark matter (DM) halo and the stellar disc is a key\nfactor in galactic evolution. While the interaction between structures like the\nGalactic bar and DM halos has been explored (e.g. slowing down of the bar due\nto dynamical friction), the effect of spiral arms on the DM halo distribution\nhas received limited attention. We analyze a suite of simulations featuring\nstrong stellar spiral arms, ranging in complexity from test-particle models to\nfully cosmological hydrodynamical simulations. Using Fourier transforms, we\ncharacterize the phase and amplitude of the stellar spirals at different times\nand radii. We then apply the same methodology to DM particles near the stellar\ndisc and compare trends in Fourier coefficients and phases between the two\ncomponents. We detect a clear spiral arm signal in the DM distribution,\ncorrelated with the stellar spirals, confirming the reaction of the halo. The\nstrength of the DM spirals consistently measures around 10\\% of that of the\nstellar spiral arms. In the $N$-body simulation, the DM spiral persistently\ntrails the stellar spiral arm by approximately $10^\\circ$. A strong spiral\nsignal of a few km\\,s$^{-1}$ appears in the radial, azimuthal, and vertical\nvelocities of halo particles, distinct from the stellar kinematic signature. In\na test-particle simulation with an analytical spiral potential (omitting\nself-gravity), we reproduce a similar density and kinematic response, showing\nthat the test-particle halo responds in the same way as the $N$-body halo.\nFinally, we also find the rest of the simulations, indicating that the\ndynamical signatures of the forced response in the DM halo are independent of\nthe dynamical origin of the stellar spiral arms. We reveal the ubiquitous\npresence of DM spiral arms in Milky Way-like galaxies, driven by a forced\nresponse to the stellar spiral potential. (ABR)",
        "The development of the Willow quantum chip by Google has sparked significant\ninterest in quantum computing, ushering in a new wave of advancements in the\nfield. As quantum computing technology continues to mature, secure quantum\ncommunication has garnered increasing attention. To establish secure\ncommunication, several quantum key distribution (QKD) protocols have been\nproposed, such as the BB84 protocol, which leverages the principles of quantum\nsuperposition and other quantum properties to ensure secure transmission.\nHowever, existing QKD protocols may face vulnerabilities under certain\nconditions. This study proposes two types of man-in-the-middle (MITM) attack\ntechniques and demonstrates their potential to compromise quantum cryptography\nthrough practical case studies. Furthermore, this study proposes strategies to\ncounteract these MITM attacks and proposes methods to enhance the security of\nquantum cryptographic systems. The findings offer valuable insights for the\nfuture implementation and deployment of secure quantum communication systems.",
        "We consider the subcritical nonlinear Schr\\\"odinger equation on quantum\ngraphs with an attractive potential supported in the compact core, and\ninvestigate the existence and the nonexistence of Ground States, defined as\nminimizers of the energy at fixed $L^2$-norm, or mass. We finally reach the\nfollowing picture: for small and large mass there are Ground States. Moreover,\naccording to the metric features of the compact core of the graph and to the\nstrength of the potential, there may be a region of intermediate masses for\nwhich there are no Ground States. The study was originally inspired by the\nresearch on quantum waveguides, in which the curvature of a thin tube induces\nan effective attractive potential.",
        "We study a class of non-local functionals that was introduced by\nBrezis--Seeger--Van Schaftingen--Yung, and can be used to characterize the\ntotal variation of functions. We establish the $\\Gamma$-limit of these\nfunctionals with respect to area-strict convergence.",
        "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
        "We investigate the interrelations between the metric properties, order\nproperties and combinatorial properties of the set of balls in totally bounded\nultrametric space. In particular, the Gurvich-Vyalyi representation of finite,\nultrametric spaces by monotone rooted trees is generalized to the case of\ntotally bounded ultrametric spaces. It is shown that such spaces have isometric\ncompletions if and only if their labeled representing trees are isomorphic. We\ncharacterize up to isomorphism the representing trees of these spaces and, up\nto order isomorphism, the posets of open balls in such spaces.",
        "The transition to a cleaner energy mix, essential for achieving net-zero\ngreenhouse gas emissions by 2050, will significantly increase demand for metals\ncritical to renewable energy technologies. Energy Transition Metals (ETMs),\nincluding copper, lithium, nickel, cobalt, and rare earth elements, are\nindispensable for renewable energy generation and the electrification of global\neconomies. However, their markets are characterized by high price volatility\ndue to supply concentration, low substitutability, and limited price\nelasticity. This paper provides a comprehensive analysis of the price\nvolatility of ETMs, a subset of Critical Raw Materials (CRMs). Using a\ncombination of exploratory data analysis, data reduction, and visualization\nmethods, we identify key features for accurate point and density forecasts. We\nevaluate various volatility models, including Generalized Autoregressive\nConditional Heteroskedasticity (GARCH) and Stochastic Volatility (SV) models,\nto determine their forecasting performance. Our findings reveal significant\nheterogeneity in ETM volatility patterns, which challenge standard groupings by\ndata providers and geological classifications. The results contribute to the\nliterature on CRM economics and commodity volatility, offering novel insights\ninto the complex dynamics of ETM markets and the modeling of their returns and\nvolatilities.",
        "Searches for continuous gravitational waves from isolated compact objects and\nthose in binary systems aim to detect non-axisymmetric, deformed neutron stars\nat particular locations in the Galaxy or all-sky. However, a large fraction of\nknown pulsars have rotational frequencies that lie outside the audio frequency\nband, rendering current detectors insensitive to these pulsars. In this work,\nwe show that DECIGO, a future space-based deci-hertz gravitational-wave\ninterferometer, will be sensitive to severely deformed compact objects, e.g.\nhybrid stars, neutron stars, or magnetars. We estimate the number of possible\ncompact objects that could be detected with such high deformations, both via\ntheir individual continuous gravitational-wave emission and the stochastic\ngravitational-wave background created by a superposition of gravitational waves\nfrom the $\\sim 10^8$ compact objects in the Galaxy. Furthermore, we show that\nthe existence of such compact objects could be probed across a wide parameter\nspace at a fraction of the computational cost of current searches for isolated\ncompact objects and those in binary systems. For known pulsars, we will be able\nto both beat the spin-down limit and probe the Brans-Dicke modified theory of\ngravity parameter $\\zeta<1$ for approximately 85% of known pulsars with $f_{\\rm\ngw}<10$ Hz, the latter of which is currently only possible for $O(10)$ pulsars.\nDECIGO will thus open a new window to probe highly deformed compact objects and\nover half of the known pulsars, both of which are currently inaccessible to\nground-based detectors.",
        "What is the maximum number of points that can be selected from an $n \\times\nn$ square lattice such that no $k+1$ of them are in a line? This has been asked\nmore than $100$ years ago for $k=2$ and it remained wide open ever since. In\nthis paper, we prove the precise answer is $kn$, provided that\n$k>C\\sqrt{n\\log{n}}$ for an absolute constant $C$. The proof relies on\ncarefully constructed bi-uniform random bipartite graphs and concentration\ninequalities.",
        "This article presents a novel network protocol that incorporates a quantum\nphotonic channel for symmetric key distribution, a Dilithium signature to\nreplace factor-based public key cryptography for enhanced authentication,\nsecurity, and privacy. The protocol uses strong hash functions to hash original\nmessages and verify heightened data integrity at the destination. This Quantum\nGood Authentication Protocol (QGP) provides high-level security provided by the\ntheory of quantum mechanics. QGP also has the advantage of quantum-resistant\ndata protection that prevents current digital computer and future quantum\ncomputer attacks.\n  QGP transforms the Transmission Control Protocol\/Internet Protocol (TCP\/IP)\nby adding a quantum layer at the bottom of Open Systems Interconnection (OSI)\nmodel (layer 0) and modifying the top layer (layer 7) with Dilithium\nsignatures, thus improving the security of the original OSI model. In addition,\nQGP incorporates strong encryption, hardware-based quantum channels,\npost-quantum signatures, and secure hash algorithms over a platform of\ndecryptors, switches, routers, and network controllers to form a testbed of the\nnext-generation, secure quantum internet. The experiments presented here show\nthat QGP provides secure authentication and improved security and privacy and\ncan be adopted as a new protocol for the next-generation quantum Internet.",
        "The discovery of neutrino oscillations and masses provides strong motivation\nto extend the Standard Model by including right-handed neutrinos, which lead to\nheavy neutrino states that could exist at the electroweak scale. These states\nmay also be influenced by new high-scale, weakly interacting physics.\nIncorporating right-handed neutrinos into an effective field theory framework\n-- the $\\nu$SMEFT -- offers a systematic approach to study the phenomenology of\nheavy neutrinos in current and upcoming experiments. In this work, we present\nthe first prospective 95\\% exclusion plots achievable at a future lepton\ncollider operating at a center-of-mass energy of $\\sqrt{s}=0.5 ~\\rm{TeV}$ for\nwhat we term the agnostic $\\nu$SMEFT scenario. This study focuses on the\nhigh-mass regime where the heavy neutrino $N$ decays promptly into leptons and\njets. Specifically, we analyze the processes $e^+e^- \\to \\nu N \\to \\nu \\mu^{-}\n\\mu^{+} \\nu$ and $e^+e^- \\to \\nu N \\to \\nu \\mu^{-} \\mathrm{j} \\mathrm{j}$,\nderiving the exclusion regions in the $\\frac{\\alpha}{\\Lambda^2}$ vs. $m_N$\nparameter space. When compared to prospective limits for the LHeC, we find that\nthe semi-leptonic process with final jets in a lepton collider offers the\ngreatest sensitivity, even with a straightforward cut-based analysis. The\nexpected bounds are as stringent as those considered in recent studies for the\nlow-mass regime where the $N$ may be long-lived and detectable via displaced\ndecay searches, both at the LHC and future colliders.",
        "We prove that for any simply connected isotropic reductive group G over a\nDedekind domain D, any Zariski-locally trivial principal G-bundle over D is\ntrivial. The corresponding result for quasi-split groups was proved in 1967 by\nG. Harder."
      ]
    }
  },
  {
    "id":2411.12897,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Mapping Tree Species Using Advanced Remote Sensing Technologies: A State-of-the-Art Review and Perspective",
    "start_abstract":"Timely and accurate information on tree species (TS) is crucial for developing strategies sustainable management conservation of artificial natural forests. Over the last four decades, advances in remote sensing technologies have made TS classification possible. Since many studies topic been conducted their comprehensive results novel findings published literature, it necessary to conduct an updated review status, trends, potentials, challenges recommend future directions. The will provide overview various optical light detection ranging (LiDAR) sensors; present assess current techniques\/methods for, a general trend method development in, classification; identify limitations In this review, several concluding remarks were made. They include following: (1) A large group using high-resolution satellite, airborne multi-\/hyperspectral imagery, LiDAR data. (2) \u201cmultiple\u201d was observed. (3) Machine learning methods including deep models demonstrated be significant improving accuracy. (4) Recently, unmanned aerial vehicle- (UAV-) based sensors caught interest researchers practitioners topic-related research applications. addition, three directions recommended, refining categories methods, data fusion algorithms or processing chains, exploring new spectral unmixing automatically extract map from satellite hyperspectral",
    "start_categories":[
      "Remote Sensing Technologies"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Morphological transformation and spatial-logical aggregation for tree species classification using hyperspectral imagery"
      ],
      "abstract":[
        "Hyperspectral image (HSI) consists of abundant spectral and spatial characteristics, which contribute to a more accurate identification of materials and land covers. However, most existing methods of hyperspectral image analysis primarily focus on spectral knowledge or coarse-grained spatial information while neglecting the fine-grained morphological structures. In the classification task of complex objects, spatial morphological differences can help to search for the boundary of fine-grained classes, e.g., forestry tree species. Focusing on subtle traits extraction, a spatial-logical aggregation network (SLA-NET) is proposed with morphological transformation for tree species classification. The morphological operators are effectively embedded with the trainable structuring elements, which contributes to distinctive morphological representations. We evaluate the classification performance of the proposed method on two tree species datasets, and the results demonstrate that the proposed SLA-NET significantly outperforms the other state-of-the-art classifiers."
      ],
      "categories":[
        "Evolutionary Biology"
      ]
    },
    "list":{
      "title":[
        "Runge type approximation results for spaces of smooth Whitney jets",
        "Design Considerations in Offline Preference-based RL",
        "Increasing the p-Selmer rank by twisting",
        "Copilot Arena: A Platform for Code LLM Evaluation in the Wild",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "4bit-Quantization in Vector-Embedding for RAG",
        "ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised\n  Pretraining Networks for Retinal OCT Classification",
        "Impact of Electric Spatially Discordant Alternans on Cardiac Magnetic\n  Field",
        "Supersymmetric Higher-Spin Gauge Theories in any $d$ and their Coupling\n  Constants within BRST Formalism",
        "How Collective Intelligence Emerges in a Crowd of People Through Learned\n  Division of Labor: A Case Study",
        "A real-time battle situation intelligent awareness system based on\n  Meta-learning & RNN",
        "Nonlinear optical metasurfaces empowered by bound-states in the\n  continuum",
        "Parametric Hypersensitivity and Transport in the Steady-State\n  Open-System Holstein Model",
        "GeoWarp: Warped spatial processes for inferring subsea sediment\n  properties",
        "On Fairness of Unified Multimodal Large Language Model for Image\n  Generation",
        "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances\n  Rare Disease Diagnosis from Clinical Notes",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided\n  Feedback",
        "Improved estimates of statistical properties in some non-uniformly\n  hyperbolic dynamical systems",
        "Diffusion Restoration Adapter for Real-World Image Restoration",
        "Grokking Explained: A Statistical Phenomenon",
        "Power Ramp-Rate Control via Power Regulation for Storageless\n  Grid-Connected Photovoltaic Systems",
        "Adaptive Grasping of Moving Objects in Dense Clutter via Global-to-Local\n  Detection and Static-to-Dynamic Planning",
        "On 1-regular and 1-uniform metric measure spaces",
        "Measuring ultrafast laser pulses using a single-shot amplitude swing\n  implementation",
        "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
        "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
        "LET measurements and simulation modelling of the charged particle field\n  for the Clatterbridge ocular proton therapy beamline",
        "Configuration of Single Giant Planet Systems Generating `Oumuamua-Like\n  Interstellar Asteroids"
      ],
      "abstract":[
        "We prove Runge type approximation results for linear partial differential\noperators with constant coefficients on spaces of smooth Whitney jets. Among\nothers, we characterize when for a constant coefficient linear partial\ndifferential operator $P(D)$ and for closed subsets $F_1\\subset F_2$ of\n$\\mathbb{R}^d$ the restrictions to $F_1$ of smooth Whitney jets $f$ on $F_2$\nsatisfying $P(D)f=0$ on $F_2$ are dense in the space of smooth Whitney jets on\n$F_1$ satisfying the same partial differential equation on $F_1$. For elliptic\noperators we give a geometric evaluation of this characterization.\nAdditionally, for differential operators with a single characteristic\ndirection, like parabolic operators, we give a sufficient geometric condition\nfor the above density to hold. Under mild additional assumptions on $\\partial\nF_1$ and for $F_2=\\mathbb{R}^d$ this sufficient conditions is also necessary.\nAs an application of our work, we characterize those open subsets $\\Omega$ of\nthe complex plane satisfying $\\Omega=\\operatorname{int}\\overline{\\Omega}$ for\nwhich the set of holomorphic polynomials are dense in $A^\\infty(\\Omega)$, under\nthe mild additional hypothesis that $\\overline{\\Omega}$ satisfies the strong\nregularity condition. Furthermore, for the wave operator in one spatial\nvariable, a simple sufficient geometric condition on $F_1,\nF_2\\subset\\mathbb{R}^2$ is given for the above density to hold. For the special\ncase of $F_2=\\mathbb{R}^2$ this sufficient condition is also necessary under\nmild additional hypotheses on $F_1$.",
        "Offline algorithms for Reinforcement Learning from Human Preferences (RLHF),\nwhich use only a fixed dataset of sampled responses given an input, and\npreference feedback among these responses, have gained increasing prominence in\nthe literature on aligning language models. In this paper, we study how the\ndifferent design choices made in methods such as DPO, IPO, SLiC and many\nvariants influence the quality of the learned policy, from a theoretical\nperspective. Our treatment yields insights into the choices of loss function,\nthe policy which is used to normalize log-likelihoods, and also the role of the\ndata sampling policy. Notably, our results do not rely on the standard\nreparameterization-style arguments used to motivate some of the algorithms in\nthis family, which allows us to give a unified treatment to a broad class of\nmethods. We also conduct a small empirical study to verify some of the\ntheoretical findings on a standard summarization benchmark.",
        "In this paper, we study the $p$-Selmer groups in the family of $p$-twists of\nan elliptic curve $E$ over a number field $K$. We prove that if $E\/K$ is an\nelliptic curve over a number field $K$, and if $d$ is congruent to the\ndimension of the Selmer group of $E\/K$ modulo $2$ and is greater than that\ndimension, then there exist infinitely many characters $\\chi \\in\n\\text{Hom}(G_K, \\mu_p)$ such that $\\text{dim}_{\\mathbb{F}_p}(\\text{Sel}_p(E\/K,\n\\chi)) = d$ under certain conditions.",
        "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Retrieval-augmented generation (RAG) is a promising technique that has shown\ngreat potential in addressing some of the limitations of large language models\n(LLMs). LLMs have two major limitations: they can contain outdated information\ndue to their training data, and they can generate factually inaccurate\nresponses, a phenomenon known as hallucinations. RAG aims to mitigate these\nissues by leveraging a database of relevant documents, which are stored as\nembedding vectors in a high-dimensional space. However, one of the challenges\nof using high-dimensional embeddings is that they require a significant amount\nof memory to store. This can be a major issue, especially when dealing with\nlarge databases of documents. To alleviate this problem, we propose the use of\n4-bit quantization to store the embedding vectors. This involves reducing the\nprecision of the vectors from 32-bit floating-point numbers to 4-bit integers,\nwhich can significantly reduce the memory requirements. Our approach has\nseveral benefits. Firstly, it significantly reduces the memory storage\nrequirements of the high-dimensional vector database, making it more feasible\nto deploy RAG systems in resource-constrained environments. Secondly, it speeds\nup the searching process, as the reduced precision of the vectors allows for\nfaster computation. Our code is available at\nhttps:\/\/github.com\/taeheej\/4bit-Quantization-in-Vector-Embedding-for-RAG",
        "Optical Coherence Tomography (OCT) is a non-invasive imaging modality\nessential for diagnosing various eye diseases. Despite its clinical\nsignificance, developing OCT-based diagnostic tools faces challenges, such as\nlimited public datasets, sparse annotations, and privacy concerns. Although\ndeep learning has made progress in automating OCT analysis, these challenges\nremain unresolved. To address these limitations, we introduce the Vision\nTransformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a\nnovel framework designed to enhance feature extraction and improve diagnostic\naccuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,\nSelf-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining\nphase leverages the OCTMNIST dataset (97,477 unlabeled images across four\ndisease classes) with data augmentation to create dual-augmented views. A\nVision Transformer (ViT-Base) backbone extracts features, while a negative\ncosine similarity loss aligns feature representations. Pretraining is conducted\nover 50 epochs with a learning rate of 0.0001 and momentum of 0.999.\nFine-tuning is performed on a stratified 5.129% subset of OCTMNIST using\n10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of\n0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming\nexisting SSP-based methods.",
        "Spatially discordant alternans (SDA) play a crucial role in cardiac\narrhythmogenesis by creating steep repolarization gradients facilitating\nconduction block and reentry. While traditionally studied using electrical\nindicators, this work provides a novel perspective by characterizing SDA\nthrough their magnetic field signatures. Using a one-dimensional cardiac fiber\nmodel, we demonstrate that magnetic field measurements effectively detect SDA\nand temperature dependent changes in cardiac action potentials, offering a\nnon-invasive alternative to conventional electrophysiological metrics. Our\nresults reveal that the spatial organization of SDA is mirrored in the magnetic\nfield distribution, with SDA nodes clearly identifiable via spatial mapping.\nNotably, magnetic restitution curves exhibit a distinct pattern from APD-based\nindicators, closely following the dynamics of the action potential upstroke.\nThese findings establish the cardiac magnetic field as a powerful diagnostic\ntool for detecting SDA, opening new avenues for biomagnetic monitoring of\narrhythmic risk.",
        "Nonlinear field equations for the supersymmetric higher-spin gauge theory\ndescribing totally symmetric bosonic and fermionic massless fields along with\nhook-type bosonic fields of all spins in any space-time dimension are\npresented. One of the novel features of the proposed formalism is that the\n$osp(1,2)$ invariance and factorisation conditions are formulated within the\nBRST formalism, that greatly simplifies the form of nonlinear HS equations. To\nmatch the list of vertices found by Metsaev, higher-spin gauge theory is\nanticipated to possess an infinite number of independent coupling constants. A\nconjecture that these coupling constants result from the locality restrictions\non the elements of the factorisation ideal is put forward.",
        "This paper investigates the factors fostering collective intelligence (CI)\nthrough a case study of *LinYi's Experiment, where over 2000 human players\ncollectively controll an avatar car. By conducting theoretical analysis and\nreplicating observed behaviors through numerical simulations, we demonstrate\nhow self-organized division of labor (DOL) among individuals fosters the\nemergence of CI and identify two essential conditions fostering CI by\nformulating this problem into a stability problem of a Markov Jump Linear\nSystem (MJLS). These conditions, independent of external stimulus, emphasize\nthe importance of both elite and common players in fostering CI. Additionally,\nwe propose an index for emergence of CI and a distributed method for estimating\njoint actions, enabling individuals to learn their optimal social roles without\nglobal action information of the whole crowd.",
        "In modern warfare, real-time and accurate battle situation analysis is\ncrucial for making strategic and tactical decisions. The proposed real-time\nbattle situation intelligent awareness system (BSIAS) aims at meta-learning\nanalysis and stepwise RNN (recurrent neural network) modeling, where the former\ncarries out the basic processing and analysis of battlefield data, which\nincludes multi-steps such as data cleansing, data fusion, data mining and\ncontinuously updates, and the latter optimizes the battlefield modeling by\nstepwise capturing the temporal dependencies of data set. BSIAS can predict the\npossible movement from any side of the fence and attack routes by taking a\nsimulated battle as an example, which can be an intelligent support platform\nfor commanders to make scientific decisions during wartime. This work delivers\nthe potential application of integrated BSIAS in the field of battlefield\ncommand & analysis engineering.",
        "Optical bound-states in the continuum (BICs) have greatly enriched the field\nof nonlinear optics with novel ways to control and manipulate light-matter\ninteraction at the nanoscale. This has been made possible by their unique\nphysical properties, including effective confinement of light, non-trivial\ntopological features, and robustness upon the propagation of the optical field\nboth in the real and momentum space. Regarding the exploration of nonlinear\noptical response in various photonic nanostructures supporting BICs, particular\nattention has been paid to optical metasurfaces, chiefly due to their ability\nto control the light flow at subwavelength scale, design and fabrication\nflexibility, and convenient phase-matching conditions. In this review, we\noutline and discuss recent advances in metasurface-based frequency conversion\nprocesses utilizing the versatile physics of BICs, with a particular emphasis\non the main physics background pertaining to nonlinear optical phenomena and\noptics of BICs, as well as state-of-the-art functionalities enabled by\nBIC-driven nonlinear metasurfaces. These applications include harmonic\ngeneration, harmonic chiroptical effects, generation of complex quantum states,\nand broadband terahertz generation. In addition, several emerging research\nfields and the existing challenges of photonic nanodevices relying on BICs are\ndiscussed.",
        "We demonstrate that the nonequilibrium steady state (NESS) of an open-system\nHolstein model with linear bias displays extreme sensitivity to the closed\nsystem parameters. This sensitivity is shown to correspond to avoided crossings\nin the closed system spectrum, as previously demonstrated in the Rabi model. We\nthen develop a kinetic model to analyze the effects of environmental parameters\non NESS hypersensitivity. This reveals that hypersensitivity only exists in\nintermediate environmental parameter regimes, a prediction that is verified\nnumerically. The inherent spatial character of the Holstein model offers a\nnatural connection to transport, revealing that transport properties in the\nsteady-state regime can be optimized by simultaneously coordinating the closed-\nand open-system parameters.",
        "For offshore structures like wind turbines, subsea infrastructure, pipelines,\nand cables, it is crucial to quantify the properties of the seabed sediments at\na proposed site. However, data collection offshore is costly, so analysis of\nthe seabed sediments must be made from measurements that are spatially sparse.\nAdding to this challenge, the structure of the seabed sediments exhibits both\nnonstationarity and anisotropy. To address these issues, we propose GeoWarp, a\nhierarchical spatial statistical modeling framework for inferring the 3-D\ngeotechnical properties of subsea sediments. GeoWarp decomposes the seabed\nproperties into a region-wide vertical mean profile (modeled using B-splines),\nand a nonstationary 3-D spatial Gaussian process. Process nonstationarity and\nanisotropy are accommodated by warping space in three dimensions and by\nallowing the process variance to change with depth. We apply GeoWarp to\nmeasurements of the seabed made using cone penetrometer tests (CPTs) at six\nsites on the North West Shelf of Australia. We show that GeoWarp captures the\ncomplex spatial distribution of the sediment properties, and produces realistic\n3-D simulations suitable for downstream engineering analyses. Through\ncross-validation, we show that GeoWarp has predictive performance superior to\nother state-of-the-art methods, demonstrating its value as a tool in offshore\ngeotechnical engineering.",
        "Unified multimodal large language models (U-MLLMs) have demonstrated\nimpressive performance in visual understanding and generation in an end-to-end\npipeline. Compared with generation-only models (e.g., Stable Diffusion),\nU-MLLMs may raise new questions about bias in their outputs, which can be\naffected by their unified capabilities. This gap is particularly concerning\ngiven the under-explored risk of propagating harmful stereotypes. In this\npaper, we benchmark the latest U-MLLMs and find that most exhibit significant\ndemographic biases, such as gender and race bias. To better understand and\nmitigate this issue, we propose a locate-then-fix strategy, where we audit and\nshow how the individual model component is affected by bias. Our analysis shows\nthat bias originates primarily from the language model. More interestingly, we\nobserve a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias\nappears minimal, but generation bias remains substantial. Thus, we propose a\nnovel balanced preference model to balance the demographic distribution with\nsynthetic data. Experiments demonstrate that our approach reduces demographic\nbias while preserving semantic fidelity. We hope our findings underscore the\nneed for more holistic interpretation and debiasing strategies of U-MLLMs in\nthe future.",
        "Background: Several studies show that large language models (LLMs) struggle\nwith phenotype-driven gene prioritization for rare diseases. These studies\ntypically use Human Phenotype Ontology (HPO) terms to prompt foundation models\nlike GPT and LLaMA to predict candidate genes. However, in real-world settings,\nfoundation models are not optimized for domain-specific tasks like clinical\ndiagnosis, yet inputs are unstructured clinical notes rather than standardized\nterms. How LLMs can be instructed to predict candidate genes or disease\ndiagnosis from unstructured clinical notes remains a major challenge. Methods:\nWe introduce RAG-driven CoT and CoT-driven RAG, two methods that combine\nChain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze\nclinical notes. A five-question CoT protocol mimics expert reasoning, while RAG\nretrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in\nMan). We evaluated these approaches on rare disease datasets, including 5,980\nPhenopacket-derived notes, 255 literature-based narratives, and 220 in-house\nclinical notes from Childrens Hospital of Philadelphia. Results: We found that\nrecent foundations models, including Llama 3.3-70B-Instruct and\nDeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2\nand GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both\noutperform foundation models in candidate gene prioritization from clinical\nnotes; in particular, both methods with DeepSeek backbone resulted in a top-10\ngene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT\nworks better for high-quality notes, where early retrieval can anchor the\nsubsequent reasoning steps in domain-specific evidence, while CoT-driven RAG\nhas advantage when processing lengthy and noisy notes.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
        "Building upon previous works by Young, Chernov-Zhang and\nBruin-Melbourne-Terhesiu, we present a general scheme to improve bounds on the\nstatistical properties (in particular, decay of correlations, and rates in the\nalmost sure invariant principle) for a class of non-uniformly hyperbolic\ndynamical systems. Specifically, for systems with polynomial, yet summable\nmixing rates, our method removes logarithmic factors of earlier arguments,\nresulting in essentially optimal bounds. Applications include Wojtkowski's\nsystem of two falling balls, dispersing billiards with flat points and\nBunimovich's flower-shaped billiard tables.",
        "Diffusion models have demonstrated their powerful image generation\ncapabilities, effectively fitting highly complex image distributions. These\nmodels can serve as strong priors for image restoration. Existing methods often\nutilize techniques like ControlNet to sample high quality images with low\nquality images from these priors. However, ControlNet typically involves\ncopying a large part of the original network, resulting in a significantly\nlarge number of parameters as the prior scales up. In this paper, we propose a\nrelatively lightweight Adapter that leverages the powerful generative\ncapabilities of pretrained priors to achieve photo-realistic image restoration.\nThe Adapters can be adapt to both denoising UNet and DiT, and performs\nexcellent.",
        "Grokking, or delayed generalization, is an intriguing learning phenomenon\nwhere test set loss decreases sharply only after a model's training set loss\nhas converged. This challenges conventional understanding of the training\ndynamics in deep learning networks. In this paper, we formalize and investigate\ngrokking, highlighting that a key factor in its emergence is a distribution\nshift between training and test data. We introduce two synthetic datasets\nspecifically designed to analyze grokking. One dataset examines the impact of\nlimited sampling, and the other investigates transfer learning's role in\ngrokking. By inducing distribution shifts through controlled imbalanced\nsampling of sub-categories, we systematically reproduce the phenomenon,\ndemonstrating that while small-sampling is strongly associated with grokking,\nit is not its cause. Instead, small-sampling serves as a convenient mechanism\nfor achieving the necessary distribution shift. We also show that when classes\nform an equivariant map, grokking can be explained by the model's ability to\nlearn from similar classes or sub-categories. Unlike earlier work suggesting\nthat grokking primarily arises from high regularization and sparse data, we\ndemonstrate that it can also occur with dense data and minimal hyper-parameter\ntuning. Our findings deepen the understanding of grokking and pave the way for\ndeveloping better stopping criteria in future training processes.",
        "Photovoltaic Power Ramp-Rate Control (PRRC) constitutes a key ancillary\nservice for future power systems. Although its implementation through the\ninstallation of storage systems or irradiance sensors has been widely\ninvestigated, fewer studies have explored the power curtailment approach. The\nlatter lacks efficiency, as it voluntarily produces power discharges, yet it is\na cost-effective solution in terms of capital expenditures. This paper proposes\na novel storageless and sensorless photovoltaic PRRC for grid-connected\napplications in which the photovoltaic power, rather than the voltage, is the\ncontrolled magnitude. The aforementioned contribution makes the effective\ntracking of the power ramp-rate limit possible compared to the existing methods\nin the literature. The method is assisted by a real-time curve-fitting\nalgorithm that estimates the Maximum Power Point while operating suboptimally.\nThus, no direct temperature or irradiance measurement systems are needed. The\nvalidation of the proposed PRRC strategy has been tested by simulation and\ncompared to another approach available in the literature, considering\nreal-field highly variable irradiance data. Experimental validation of the\nproposed strategy has been performed in real time via Controller\nHardware-in-the-Loop.",
        "Robotic grasping is facing a variety of real-world uncertainties caused by\nnon-static object states, unknown object properties, and cluttered object\narrangements. The difficulty of grasping increases with the presence of more\nuncertainties, where commonly used learning-based approaches struggle to\nperform consistently across varying conditions. In this study, we integrate the\nidea of similarity matching to tackle the challenge of grasping novel objects\nthat are simultaneously in motion and densely cluttered using a single RGBD\ncamera, where multiple uncertainties coexist. We achieve this by shifting\nvisual detection from global to local states and operating grasp planning from\nstatic to dynamic scenes. Notably, we introduce optimization methods to enhance\nplanning efficiency for this time-sensitive task. Our proposed system can adapt\nto various object types, arrangements and movement speeds without the need for\nextensive training, as demonstrated by real-world experiments. Videos are\navailable at https:\/\/youtu.be\/sdC50dx-xp8?si=27oVr4dhG0rqN_tT.",
        "A metric measure space $(X,\\mu)$ is 1-regular if \\[0< \\lim_{r\\to 0}\n\\frac{\\mu(B(x,r))}{r}<\\infty\\] for $\\mu$-a.e $x\\in X$. We give a complete\ngeometric characterisation of the rectifiable and purely unrectifiable part of\na 1-regular measure in terms of its tangent spaces.\n  A special instance of a 1-regular metric measure space is a 1-uniform space\n$(Y,\\nu)$, which satisfies $\\nu(B(y,r))=r$ for all $y\\in Y$ and $r>0$. We prove\nthat there are exactly three 1-uniform metric measure spaces.",
        "Single-shot characterization techniques are crucial when dealing with\nshot-to-shot pulse-shape fluctuations (e.g., unstable laser systems,\nhigh-power, or with low repetition rate) since the scanning configurations\ncannot measure single pulses. The demand for simple setups that can be easily\nadapted to a wide variety of experimental conditions is continuously rising. In\nthis work, we propose a single-shot implementation of amplitude swing,\nmaintaining the compactness, versatility, and robustness of the scanning\nversions of this technique. First, we theoretically study the proposed\nimplementation, based on a pair of uniaxial wedges. Then, we present the\nretrieval ptychographic algorithm. Finally, we experimentally demonstrate the\nsetup by comparing the single-shot and scanning traces and their retrieved\npulses. In sum, we provide the ultrafast science community with a simple and\nversatile setup capable of measuring single laser pulses, which is necessary\nfor characterizing fluctuating pulse trains, meeting the current increasing\ndemand.",
        "In this work, we study offline reinforcement learning (RL) with zero-shot\ngeneralization property (ZSG), where the agent has access to an offline dataset\nincluding experiences from different environments, and the goal of the agent is\nto train a policy over the training environments which performs well on test\nenvironments without further interaction. Existing work showed that classical\noffline RL fails to generalize to new, unseen environments. We propose\npessimistic empirical risk minimization (PERM) and pessimistic proximal policy\noptimization (PPPO), which leverage pessimistic policy evaluation to guide\npolicy learning and enhance generalization. We show that both PERM and PPPO are\ncapable of finding a near-optimal policy with ZSG. Our result serves as a first\nstep in understanding the foundation of the generalization phenomenon in\noffline reinforcement learning.",
        "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https:\/\/github.com\/zsq259\/Plan-over-Graph.",
        "Proton therapy can achieve a highly targeted treatment by utilising the\nadvantageous dosimetric characteristics of the Bragg Peak. Protons traversing\nthrough a material will deposit their maximum energy at the Bragg Peak through\nionisation and other interactions, transferring minimal excess dose to\nsurrounding tissue and organs. This rate of energy loss is also quantified by\nthe linear energy transfer (LET), which is indicative of radiation quality and\nradiobiological effects. However it is a challenging physical quantity to\nmeasure, as characterisation of radiation fields and the impact of LET on\ntreatment requires advanced tools and technology. The MiniPIX-Timepix is a\nminiaturised, hybrid semiconductor pixel detector capable of high resolution\nspectrometric tracking, enabling wide-range detection of the deposited energy,\nposition and direction of single particles. Experimental measurements were\nperformed at a clinical facility, the Clatterbridge Cancer Centre which houses\na 60 MeV ocular proton therapy beamline. A realistic end-to-end model of the\nfacility was developed in the Monte Carlo code TOPAS (TOol for PArticle\nSimulation) and was used to simulate the experimental conditions. The detector\nwas held at 45$^{\\circ}$ and 60$^{\\circ}$ perpendicular to the beam, and placed\ndownstream of various thickness Polymethyl methacrylate (PMMA) blocks to\nacquire data along the dose deposition depth. Empirical cluster data providing\ntrack length and the energy deposition distributions were used to obtain the\nLET spectra. The determined values for the LET in silicon and dose averaged LET\nacross the BP show general agreement with simulated results, supporting the\napplicability of the TOPAS CCC model. This work explores the capability of the\nMiniPIX detector to measure physical quantities to resolve the LET, and\ndiscusses experimental considerations and further possibilities.",
        "The first discovered interstellar small object, `Oumuamua (1I\/2017 U1),\npresents unique physical properties of extremely elongated geometric shape and\ndual characteristics of an asteroid and a comet. These properties suggest a\npossible origin through tidal fragmentation, which posits that `Oumuamua was\nproduced through intensive tidal fragmentation during a close encounter with a\nstar or a white dwarf, resulting in its shape and ejection from its natal\nsystem. According to this mechanism, a high initial orbit eccentricity and a\nsmall pericentre of the parent body are necessary to produce `Oumuamua-like\nobjects. To verify whether this mechanism can occur in single giant planet\nsystems, we conduct long-term numerical simulations of systems with a low-mass\n($0.5M_\\odot$) host star and a giant planet in this study. We determine that an\neccentric orbit ($e_\\mathrm{p}\\sim0.2$) and a Jupiter-mass ($M_\\mathrm{p}\\sim\nM_\\mathrm{J}$) of the planet appears to be optimal to generate sufficient\nperturbations for the production of `Oumuamua-like objects. When the planetary\nsemi-major axis $a_\\mathrm{p}$ increases, the proportion of planetesimals\nejected beyond the system $P(\\mathrm{ej})$ increases accordingly, while the\npossibilities of ejected planetesimals undergoing stellar tidal fragmentation\n$P(\\mathrm{tidal}|\\mathrm{ej})$ remains relatively constant at $\\sim0.6\\%$.\nFocusing on stellar tidal fragmentation alone, the ratio of extremely elongated\ninterstellar objects to all interstellar objects is $P_\\mathrm{e}\\sim3\\%$."
      ]
    }
  },
  {
    "id":2411.12897,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Morphological transformation and spatial-logical aggregation for tree species classification using hyperspectral imagery",
    "start_abstract":"Hyperspectral image (HSI) consists of abundant spectral and spatial characteristics, which contribute to a more accurate identification of materials and land covers. However, most existing methods of hyperspectral image analysis primarily focus on spectral knowledge or coarse-grained spatial information while neglecting the fine-grained morphological structures. In the classification task of complex objects, spatial morphological differences can help to search for the boundary of fine-grained classes, e.g., forestry tree species. Focusing on subtle traits extraction, a spatial-logical aggregation network (SLA-NET) is proposed with morphological transformation for tree species classification. The morphological operators are effectively embedded with the trainable structuring elements, which contributes to distinctive morphological representations. We evaluate the classification performance of the proposed method on two tree species datasets, and the results demonstrate that the proposed SLA-NET significantly outperforms the other state-of-the-art classifiers.",
    "start_categories":[
      "Evolutionary Biology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Mapping Tree Species Using Advanced Remote Sensing Technologies: A State-of-the-Art Review and Perspective"
      ],
      "abstract":[
        "Timely and accurate information on tree species (TS) is crucial for developing strategies sustainable management conservation of artificial natural forests. Over the last four decades, advances in remote sensing technologies have made TS classification possible. Since many studies topic been conducted their comprehensive results novel findings published literature, it necessary to conduct an updated review status, trends, potentials, challenges recommend future directions. The will provide overview various optical light detection ranging (LiDAR) sensors; present assess current techniques\/methods for, a general trend method development in, classification; identify limitations In this review, several concluding remarks were made. They include following: (1) A large group using high-resolution satellite, airborne multi-\/hyperspectral imagery, LiDAR data. (2) \u201cmultiple\u201d was observed. (3) Machine learning methods including deep models demonstrated be significant improving accuracy. (4) Recently, unmanned aerial vehicle- (UAV-) based sensors caught interest researchers practitioners topic-related research applications. addition, three directions recommended, refining categories methods, data fusion algorithms or processing chains, exploring new spectral unmixing automatically extract map from satellite hyperspectral"
      ],
      "categories":[
        "Remote Sensing Technologies"
      ]
    },
    "list":{
      "title":[
        "Runge type approximation results for spaces of smooth Whitney jets",
        "Design Considerations in Offline Preference-based RL",
        "Increasing the p-Selmer rank by twisting",
        "Copilot Arena: A Platform for Code LLM Evaluation in the Wild",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "4bit-Quantization in Vector-Embedding for RAG",
        "ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised\n  Pretraining Networks for Retinal OCT Classification",
        "Impact of Electric Spatially Discordant Alternans on Cardiac Magnetic\n  Field",
        "Supersymmetric Higher-Spin Gauge Theories in any $d$ and their Coupling\n  Constants within BRST Formalism",
        "How Collective Intelligence Emerges in a Crowd of People Through Learned\n  Division of Labor: A Case Study",
        "A real-time battle situation intelligent awareness system based on\n  Meta-learning & RNN",
        "Nonlinear optical metasurfaces empowered by bound-states in the\n  continuum",
        "Parametric Hypersensitivity and Transport in the Steady-State\n  Open-System Holstein Model",
        "GeoWarp: Warped spatial processes for inferring subsea sediment\n  properties",
        "On Fairness of Unified Multimodal Large Language Model for Image\n  Generation",
        "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances\n  Rare Disease Diagnosis from Clinical Notes",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided\n  Feedback",
        "Improved estimates of statistical properties in some non-uniformly\n  hyperbolic dynamical systems",
        "Diffusion Restoration Adapter for Real-World Image Restoration",
        "Grokking Explained: A Statistical Phenomenon",
        "Power Ramp-Rate Control via Power Regulation for Storageless\n  Grid-Connected Photovoltaic Systems",
        "Adaptive Grasping of Moving Objects in Dense Clutter via Global-to-Local\n  Detection and Static-to-Dynamic Planning",
        "On 1-regular and 1-uniform metric measure spaces",
        "Measuring ultrafast laser pulses using a single-shot amplitude swing\n  implementation",
        "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
        "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
        "LET measurements and simulation modelling of the charged particle field\n  for the Clatterbridge ocular proton therapy beamline",
        "Configuration of Single Giant Planet Systems Generating `Oumuamua-Like\n  Interstellar Asteroids"
      ],
      "abstract":[
        "We prove Runge type approximation results for linear partial differential\noperators with constant coefficients on spaces of smooth Whitney jets. Among\nothers, we characterize when for a constant coefficient linear partial\ndifferential operator $P(D)$ and for closed subsets $F_1\\subset F_2$ of\n$\\mathbb{R}^d$ the restrictions to $F_1$ of smooth Whitney jets $f$ on $F_2$\nsatisfying $P(D)f=0$ on $F_2$ are dense in the space of smooth Whitney jets on\n$F_1$ satisfying the same partial differential equation on $F_1$. For elliptic\noperators we give a geometric evaluation of this characterization.\nAdditionally, for differential operators with a single characteristic\ndirection, like parabolic operators, we give a sufficient geometric condition\nfor the above density to hold. Under mild additional assumptions on $\\partial\nF_1$ and for $F_2=\\mathbb{R}^d$ this sufficient conditions is also necessary.\nAs an application of our work, we characterize those open subsets $\\Omega$ of\nthe complex plane satisfying $\\Omega=\\operatorname{int}\\overline{\\Omega}$ for\nwhich the set of holomorphic polynomials are dense in $A^\\infty(\\Omega)$, under\nthe mild additional hypothesis that $\\overline{\\Omega}$ satisfies the strong\nregularity condition. Furthermore, for the wave operator in one spatial\nvariable, a simple sufficient geometric condition on $F_1,\nF_2\\subset\\mathbb{R}^2$ is given for the above density to hold. For the special\ncase of $F_2=\\mathbb{R}^2$ this sufficient condition is also necessary under\nmild additional hypotheses on $F_1$.",
        "Offline algorithms for Reinforcement Learning from Human Preferences (RLHF),\nwhich use only a fixed dataset of sampled responses given an input, and\npreference feedback among these responses, have gained increasing prominence in\nthe literature on aligning language models. In this paper, we study how the\ndifferent design choices made in methods such as DPO, IPO, SLiC and many\nvariants influence the quality of the learned policy, from a theoretical\nperspective. Our treatment yields insights into the choices of loss function,\nthe policy which is used to normalize log-likelihoods, and also the role of the\ndata sampling policy. Notably, our results do not rely on the standard\nreparameterization-style arguments used to motivate some of the algorithms in\nthis family, which allows us to give a unified treatment to a broad class of\nmethods. We also conduct a small empirical study to verify some of the\ntheoretical findings on a standard summarization benchmark.",
        "In this paper, we study the $p$-Selmer groups in the family of $p$-twists of\nan elliptic curve $E$ over a number field $K$. We prove that if $E\/K$ is an\nelliptic curve over a number field $K$, and if $d$ is congruent to the\ndimension of the Selmer group of $E\/K$ modulo $2$ and is greater than that\ndimension, then there exist infinitely many characters $\\chi \\in\n\\text{Hom}(G_K, \\mu_p)$ such that $\\text{dim}_{\\mathbb{F}_p}(\\text{Sel}_p(E\/K,\n\\chi)) = d$ under certain conditions.",
        "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Retrieval-augmented generation (RAG) is a promising technique that has shown\ngreat potential in addressing some of the limitations of large language models\n(LLMs). LLMs have two major limitations: they can contain outdated information\ndue to their training data, and they can generate factually inaccurate\nresponses, a phenomenon known as hallucinations. RAG aims to mitigate these\nissues by leveraging a database of relevant documents, which are stored as\nembedding vectors in a high-dimensional space. However, one of the challenges\nof using high-dimensional embeddings is that they require a significant amount\nof memory to store. This can be a major issue, especially when dealing with\nlarge databases of documents. To alleviate this problem, we propose the use of\n4-bit quantization to store the embedding vectors. This involves reducing the\nprecision of the vectors from 32-bit floating-point numbers to 4-bit integers,\nwhich can significantly reduce the memory requirements. Our approach has\nseveral benefits. Firstly, it significantly reduces the memory storage\nrequirements of the high-dimensional vector database, making it more feasible\nto deploy RAG systems in resource-constrained environments. Secondly, it speeds\nup the searching process, as the reduced precision of the vectors allows for\nfaster computation. Our code is available at\nhttps:\/\/github.com\/taeheej\/4bit-Quantization-in-Vector-Embedding-for-RAG",
        "Optical Coherence Tomography (OCT) is a non-invasive imaging modality\nessential for diagnosing various eye diseases. Despite its clinical\nsignificance, developing OCT-based diagnostic tools faces challenges, such as\nlimited public datasets, sparse annotations, and privacy concerns. Although\ndeep learning has made progress in automating OCT analysis, these challenges\nremain unresolved. To address these limitations, we introduce the Vision\nTransformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a\nnovel framework designed to enhance feature extraction and improve diagnostic\naccuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,\nSelf-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining\nphase leverages the OCTMNIST dataset (97,477 unlabeled images across four\ndisease classes) with data augmentation to create dual-augmented views. A\nVision Transformer (ViT-Base) backbone extracts features, while a negative\ncosine similarity loss aligns feature representations. Pretraining is conducted\nover 50 epochs with a learning rate of 0.0001 and momentum of 0.999.\nFine-tuning is performed on a stratified 5.129% subset of OCTMNIST using\n10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of\n0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming\nexisting SSP-based methods.",
        "Spatially discordant alternans (SDA) play a crucial role in cardiac\narrhythmogenesis by creating steep repolarization gradients facilitating\nconduction block and reentry. While traditionally studied using electrical\nindicators, this work provides a novel perspective by characterizing SDA\nthrough their magnetic field signatures. Using a one-dimensional cardiac fiber\nmodel, we demonstrate that magnetic field measurements effectively detect SDA\nand temperature dependent changes in cardiac action potentials, offering a\nnon-invasive alternative to conventional electrophysiological metrics. Our\nresults reveal that the spatial organization of SDA is mirrored in the magnetic\nfield distribution, with SDA nodes clearly identifiable via spatial mapping.\nNotably, magnetic restitution curves exhibit a distinct pattern from APD-based\nindicators, closely following the dynamics of the action potential upstroke.\nThese findings establish the cardiac magnetic field as a powerful diagnostic\ntool for detecting SDA, opening new avenues for biomagnetic monitoring of\narrhythmic risk.",
        "Nonlinear field equations for the supersymmetric higher-spin gauge theory\ndescribing totally symmetric bosonic and fermionic massless fields along with\nhook-type bosonic fields of all spins in any space-time dimension are\npresented. One of the novel features of the proposed formalism is that the\n$osp(1,2)$ invariance and factorisation conditions are formulated within the\nBRST formalism, that greatly simplifies the form of nonlinear HS equations. To\nmatch the list of vertices found by Metsaev, higher-spin gauge theory is\nanticipated to possess an infinite number of independent coupling constants. A\nconjecture that these coupling constants result from the locality restrictions\non the elements of the factorisation ideal is put forward.",
        "This paper investigates the factors fostering collective intelligence (CI)\nthrough a case study of *LinYi's Experiment, where over 2000 human players\ncollectively controll an avatar car. By conducting theoretical analysis and\nreplicating observed behaviors through numerical simulations, we demonstrate\nhow self-organized division of labor (DOL) among individuals fosters the\nemergence of CI and identify two essential conditions fostering CI by\nformulating this problem into a stability problem of a Markov Jump Linear\nSystem (MJLS). These conditions, independent of external stimulus, emphasize\nthe importance of both elite and common players in fostering CI. Additionally,\nwe propose an index for emergence of CI and a distributed method for estimating\njoint actions, enabling individuals to learn their optimal social roles without\nglobal action information of the whole crowd.",
        "In modern warfare, real-time and accurate battle situation analysis is\ncrucial for making strategic and tactical decisions. The proposed real-time\nbattle situation intelligent awareness system (BSIAS) aims at meta-learning\nanalysis and stepwise RNN (recurrent neural network) modeling, where the former\ncarries out the basic processing and analysis of battlefield data, which\nincludes multi-steps such as data cleansing, data fusion, data mining and\ncontinuously updates, and the latter optimizes the battlefield modeling by\nstepwise capturing the temporal dependencies of data set. BSIAS can predict the\npossible movement from any side of the fence and attack routes by taking a\nsimulated battle as an example, which can be an intelligent support platform\nfor commanders to make scientific decisions during wartime. This work delivers\nthe potential application of integrated BSIAS in the field of battlefield\ncommand & analysis engineering.",
        "Optical bound-states in the continuum (BICs) have greatly enriched the field\nof nonlinear optics with novel ways to control and manipulate light-matter\ninteraction at the nanoscale. This has been made possible by their unique\nphysical properties, including effective confinement of light, non-trivial\ntopological features, and robustness upon the propagation of the optical field\nboth in the real and momentum space. Regarding the exploration of nonlinear\noptical response in various photonic nanostructures supporting BICs, particular\nattention has been paid to optical metasurfaces, chiefly due to their ability\nto control the light flow at subwavelength scale, design and fabrication\nflexibility, and convenient phase-matching conditions. In this review, we\noutline and discuss recent advances in metasurface-based frequency conversion\nprocesses utilizing the versatile physics of BICs, with a particular emphasis\non the main physics background pertaining to nonlinear optical phenomena and\noptics of BICs, as well as state-of-the-art functionalities enabled by\nBIC-driven nonlinear metasurfaces. These applications include harmonic\ngeneration, harmonic chiroptical effects, generation of complex quantum states,\nand broadband terahertz generation. In addition, several emerging research\nfields and the existing challenges of photonic nanodevices relying on BICs are\ndiscussed.",
        "We demonstrate that the nonequilibrium steady state (NESS) of an open-system\nHolstein model with linear bias displays extreme sensitivity to the closed\nsystem parameters. This sensitivity is shown to correspond to avoided crossings\nin the closed system spectrum, as previously demonstrated in the Rabi model. We\nthen develop a kinetic model to analyze the effects of environmental parameters\non NESS hypersensitivity. This reveals that hypersensitivity only exists in\nintermediate environmental parameter regimes, a prediction that is verified\nnumerically. The inherent spatial character of the Holstein model offers a\nnatural connection to transport, revealing that transport properties in the\nsteady-state regime can be optimized by simultaneously coordinating the closed-\nand open-system parameters.",
        "For offshore structures like wind turbines, subsea infrastructure, pipelines,\nand cables, it is crucial to quantify the properties of the seabed sediments at\na proposed site. However, data collection offshore is costly, so analysis of\nthe seabed sediments must be made from measurements that are spatially sparse.\nAdding to this challenge, the structure of the seabed sediments exhibits both\nnonstationarity and anisotropy. To address these issues, we propose GeoWarp, a\nhierarchical spatial statistical modeling framework for inferring the 3-D\ngeotechnical properties of subsea sediments. GeoWarp decomposes the seabed\nproperties into a region-wide vertical mean profile (modeled using B-splines),\nand a nonstationary 3-D spatial Gaussian process. Process nonstationarity and\nanisotropy are accommodated by warping space in three dimensions and by\nallowing the process variance to change with depth. We apply GeoWarp to\nmeasurements of the seabed made using cone penetrometer tests (CPTs) at six\nsites on the North West Shelf of Australia. We show that GeoWarp captures the\ncomplex spatial distribution of the sediment properties, and produces realistic\n3-D simulations suitable for downstream engineering analyses. Through\ncross-validation, we show that GeoWarp has predictive performance superior to\nother state-of-the-art methods, demonstrating its value as a tool in offshore\ngeotechnical engineering.",
        "Unified multimodal large language models (U-MLLMs) have demonstrated\nimpressive performance in visual understanding and generation in an end-to-end\npipeline. Compared with generation-only models (e.g., Stable Diffusion),\nU-MLLMs may raise new questions about bias in their outputs, which can be\naffected by their unified capabilities. This gap is particularly concerning\ngiven the under-explored risk of propagating harmful stereotypes. In this\npaper, we benchmark the latest U-MLLMs and find that most exhibit significant\ndemographic biases, such as gender and race bias. To better understand and\nmitigate this issue, we propose a locate-then-fix strategy, where we audit and\nshow how the individual model component is affected by bias. Our analysis shows\nthat bias originates primarily from the language model. More interestingly, we\nobserve a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias\nappears minimal, but generation bias remains substantial. Thus, we propose a\nnovel balanced preference model to balance the demographic distribution with\nsynthetic data. Experiments demonstrate that our approach reduces demographic\nbias while preserving semantic fidelity. We hope our findings underscore the\nneed for more holistic interpretation and debiasing strategies of U-MLLMs in\nthe future.",
        "Background: Several studies show that large language models (LLMs) struggle\nwith phenotype-driven gene prioritization for rare diseases. These studies\ntypically use Human Phenotype Ontology (HPO) terms to prompt foundation models\nlike GPT and LLaMA to predict candidate genes. However, in real-world settings,\nfoundation models are not optimized for domain-specific tasks like clinical\ndiagnosis, yet inputs are unstructured clinical notes rather than standardized\nterms. How LLMs can be instructed to predict candidate genes or disease\ndiagnosis from unstructured clinical notes remains a major challenge. Methods:\nWe introduce RAG-driven CoT and CoT-driven RAG, two methods that combine\nChain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze\nclinical notes. A five-question CoT protocol mimics expert reasoning, while RAG\nretrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in\nMan). We evaluated these approaches on rare disease datasets, including 5,980\nPhenopacket-derived notes, 255 literature-based narratives, and 220 in-house\nclinical notes from Childrens Hospital of Philadelphia. Results: We found that\nrecent foundations models, including Llama 3.3-70B-Instruct and\nDeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2\nand GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both\noutperform foundation models in candidate gene prioritization from clinical\nnotes; in particular, both methods with DeepSeek backbone resulted in a top-10\ngene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT\nworks better for high-quality notes, where early retrieval can anchor the\nsubsequent reasoning steps in domain-specific evidence, while CoT-driven RAG\nhas advantage when processing lengthy and noisy notes.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
        "Building upon previous works by Young, Chernov-Zhang and\nBruin-Melbourne-Terhesiu, we present a general scheme to improve bounds on the\nstatistical properties (in particular, decay of correlations, and rates in the\nalmost sure invariant principle) for a class of non-uniformly hyperbolic\ndynamical systems. Specifically, for systems with polynomial, yet summable\nmixing rates, our method removes logarithmic factors of earlier arguments,\nresulting in essentially optimal bounds. Applications include Wojtkowski's\nsystem of two falling balls, dispersing billiards with flat points and\nBunimovich's flower-shaped billiard tables.",
        "Diffusion models have demonstrated their powerful image generation\ncapabilities, effectively fitting highly complex image distributions. These\nmodels can serve as strong priors for image restoration. Existing methods often\nutilize techniques like ControlNet to sample high quality images with low\nquality images from these priors. However, ControlNet typically involves\ncopying a large part of the original network, resulting in a significantly\nlarge number of parameters as the prior scales up. In this paper, we propose a\nrelatively lightweight Adapter that leverages the powerful generative\ncapabilities of pretrained priors to achieve photo-realistic image restoration.\nThe Adapters can be adapt to both denoising UNet and DiT, and performs\nexcellent.",
        "Grokking, or delayed generalization, is an intriguing learning phenomenon\nwhere test set loss decreases sharply only after a model's training set loss\nhas converged. This challenges conventional understanding of the training\ndynamics in deep learning networks. In this paper, we formalize and investigate\ngrokking, highlighting that a key factor in its emergence is a distribution\nshift between training and test data. We introduce two synthetic datasets\nspecifically designed to analyze grokking. One dataset examines the impact of\nlimited sampling, and the other investigates transfer learning's role in\ngrokking. By inducing distribution shifts through controlled imbalanced\nsampling of sub-categories, we systematically reproduce the phenomenon,\ndemonstrating that while small-sampling is strongly associated with grokking,\nit is not its cause. Instead, small-sampling serves as a convenient mechanism\nfor achieving the necessary distribution shift. We also show that when classes\nform an equivariant map, grokking can be explained by the model's ability to\nlearn from similar classes or sub-categories. Unlike earlier work suggesting\nthat grokking primarily arises from high regularization and sparse data, we\ndemonstrate that it can also occur with dense data and minimal hyper-parameter\ntuning. Our findings deepen the understanding of grokking and pave the way for\ndeveloping better stopping criteria in future training processes.",
        "Photovoltaic Power Ramp-Rate Control (PRRC) constitutes a key ancillary\nservice for future power systems. Although its implementation through the\ninstallation of storage systems or irradiance sensors has been widely\ninvestigated, fewer studies have explored the power curtailment approach. The\nlatter lacks efficiency, as it voluntarily produces power discharges, yet it is\na cost-effective solution in terms of capital expenditures. This paper proposes\na novel storageless and sensorless photovoltaic PRRC for grid-connected\napplications in which the photovoltaic power, rather than the voltage, is the\ncontrolled magnitude. The aforementioned contribution makes the effective\ntracking of the power ramp-rate limit possible compared to the existing methods\nin the literature. The method is assisted by a real-time curve-fitting\nalgorithm that estimates the Maximum Power Point while operating suboptimally.\nThus, no direct temperature or irradiance measurement systems are needed. The\nvalidation of the proposed PRRC strategy has been tested by simulation and\ncompared to another approach available in the literature, considering\nreal-field highly variable irradiance data. Experimental validation of the\nproposed strategy has been performed in real time via Controller\nHardware-in-the-Loop.",
        "Robotic grasping is facing a variety of real-world uncertainties caused by\nnon-static object states, unknown object properties, and cluttered object\narrangements. The difficulty of grasping increases with the presence of more\nuncertainties, where commonly used learning-based approaches struggle to\nperform consistently across varying conditions. In this study, we integrate the\nidea of similarity matching to tackle the challenge of grasping novel objects\nthat are simultaneously in motion and densely cluttered using a single RGBD\ncamera, where multiple uncertainties coexist. We achieve this by shifting\nvisual detection from global to local states and operating grasp planning from\nstatic to dynamic scenes. Notably, we introduce optimization methods to enhance\nplanning efficiency for this time-sensitive task. Our proposed system can adapt\nto various object types, arrangements and movement speeds without the need for\nextensive training, as demonstrated by real-world experiments. Videos are\navailable at https:\/\/youtu.be\/sdC50dx-xp8?si=27oVr4dhG0rqN_tT.",
        "A metric measure space $(X,\\mu)$ is 1-regular if \\[0< \\lim_{r\\to 0}\n\\frac{\\mu(B(x,r))}{r}<\\infty\\] for $\\mu$-a.e $x\\in X$. We give a complete\ngeometric characterisation of the rectifiable and purely unrectifiable part of\na 1-regular measure in terms of its tangent spaces.\n  A special instance of a 1-regular metric measure space is a 1-uniform space\n$(Y,\\nu)$, which satisfies $\\nu(B(y,r))=r$ for all $y\\in Y$ and $r>0$. We prove\nthat there are exactly three 1-uniform metric measure spaces.",
        "Single-shot characterization techniques are crucial when dealing with\nshot-to-shot pulse-shape fluctuations (e.g., unstable laser systems,\nhigh-power, or with low repetition rate) since the scanning configurations\ncannot measure single pulses. The demand for simple setups that can be easily\nadapted to a wide variety of experimental conditions is continuously rising. In\nthis work, we propose a single-shot implementation of amplitude swing,\nmaintaining the compactness, versatility, and robustness of the scanning\nversions of this technique. First, we theoretically study the proposed\nimplementation, based on a pair of uniaxial wedges. Then, we present the\nretrieval ptychographic algorithm. Finally, we experimentally demonstrate the\nsetup by comparing the single-shot and scanning traces and their retrieved\npulses. In sum, we provide the ultrafast science community with a simple and\nversatile setup capable of measuring single laser pulses, which is necessary\nfor characterizing fluctuating pulse trains, meeting the current increasing\ndemand.",
        "In this work, we study offline reinforcement learning (RL) with zero-shot\ngeneralization property (ZSG), where the agent has access to an offline dataset\nincluding experiences from different environments, and the goal of the agent is\nto train a policy over the training environments which performs well on test\nenvironments without further interaction. Existing work showed that classical\noffline RL fails to generalize to new, unseen environments. We propose\npessimistic empirical risk minimization (PERM) and pessimistic proximal policy\noptimization (PPPO), which leverage pessimistic policy evaluation to guide\npolicy learning and enhance generalization. We show that both PERM and PPPO are\ncapable of finding a near-optimal policy with ZSG. Our result serves as a first\nstep in understanding the foundation of the generalization phenomenon in\noffline reinforcement learning.",
        "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https:\/\/github.com\/zsq259\/Plan-over-Graph.",
        "Proton therapy can achieve a highly targeted treatment by utilising the\nadvantageous dosimetric characteristics of the Bragg Peak. Protons traversing\nthrough a material will deposit their maximum energy at the Bragg Peak through\nionisation and other interactions, transferring minimal excess dose to\nsurrounding tissue and organs. This rate of energy loss is also quantified by\nthe linear energy transfer (LET), which is indicative of radiation quality and\nradiobiological effects. However it is a challenging physical quantity to\nmeasure, as characterisation of radiation fields and the impact of LET on\ntreatment requires advanced tools and technology. The MiniPIX-Timepix is a\nminiaturised, hybrid semiconductor pixel detector capable of high resolution\nspectrometric tracking, enabling wide-range detection of the deposited energy,\nposition and direction of single particles. Experimental measurements were\nperformed at a clinical facility, the Clatterbridge Cancer Centre which houses\na 60 MeV ocular proton therapy beamline. A realistic end-to-end model of the\nfacility was developed in the Monte Carlo code TOPAS (TOol for PArticle\nSimulation) and was used to simulate the experimental conditions. The detector\nwas held at 45$^{\\circ}$ and 60$^{\\circ}$ perpendicular to the beam, and placed\ndownstream of various thickness Polymethyl methacrylate (PMMA) blocks to\nacquire data along the dose deposition depth. Empirical cluster data providing\ntrack length and the energy deposition distributions were used to obtain the\nLET spectra. The determined values for the LET in silicon and dose averaged LET\nacross the BP show general agreement with simulated results, supporting the\napplicability of the TOPAS CCC model. This work explores the capability of the\nMiniPIX detector to measure physical quantities to resolve the LET, and\ndiscusses experimental considerations and further possibilities.",
        "The first discovered interstellar small object, `Oumuamua (1I\/2017 U1),\npresents unique physical properties of extremely elongated geometric shape and\ndual characteristics of an asteroid and a comet. These properties suggest a\npossible origin through tidal fragmentation, which posits that `Oumuamua was\nproduced through intensive tidal fragmentation during a close encounter with a\nstar or a white dwarf, resulting in its shape and ejection from its natal\nsystem. According to this mechanism, a high initial orbit eccentricity and a\nsmall pericentre of the parent body are necessary to produce `Oumuamua-like\nobjects. To verify whether this mechanism can occur in single giant planet\nsystems, we conduct long-term numerical simulations of systems with a low-mass\n($0.5M_\\odot$) host star and a giant planet in this study. We determine that an\neccentric orbit ($e_\\mathrm{p}\\sim0.2$) and a Jupiter-mass ($M_\\mathrm{p}\\sim\nM_\\mathrm{J}$) of the planet appears to be optimal to generate sufficient\nperturbations for the production of `Oumuamua-like objects. When the planetary\nsemi-major axis $a_\\mathrm{p}$ increases, the proportion of planetesimals\nejected beyond the system $P(\\mathrm{ej})$ increases accordingly, while the\npossibilities of ejected planetesimals undergoing stellar tidal fragmentation\n$P(\\mathrm{tidal}|\\mathrm{ej})$ remains relatively constant at $\\sim0.6\\%$.\nFocusing on stellar tidal fragmentation alone, the ratio of extremely elongated\ninterstellar objects to all interstellar objects is $P_\\mathrm{e}\\sim3\\%$."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation",
    "start_abstract":"Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators.",
    "start_categories":[
      "physics.acc-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Accelerating cavity fault prediction using deep learning at Jefferson laboratory"
      ],
      "abstract":[
        "Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
        "LexPro-1.0 Technical Report",
        "Observer-Based Data-Driven Consensus Control for Nonlinear Multi-Agent\n  Systems against DoS and FDI attacks",
        "Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive\n  Impairment",
        "Generative AI & Changing Work: Systematic Review of Practitioner-led\n  Work Transformations through the Lens of Job Crafting",
        "Learning Code-Edit Embedding to Model Student Debugging Behavior",
        "Monochromatic graph decompositions and monochromatic piercing inspired\n  by anti-Ramsey colorings",
        "Select2Drive: Pragmatic Communications for Real-Time Collaborative\n  Autonomous Driving",
        "Determination of the density in the linear elastic wave equation",
        "Semicustom Frontend VLSI Design and Analysis of a 32-bit Brent-Kung\n  Adder in Cadence Suite",
        "DiffCLIP: Differential Attention Meets CLIP",
        "Splitting algorithms for paraxial and It\\^o-Schr\\\"odinger models of wave\n  propagation in random media",
        "Numerical homological regularities over positively graded algebras",
        "Provable Benefits of Task-Specific Prompts for In-context Learning",
        "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of\n  Frozen Language Models",
        "Bidirectionalization For The Common People",
        "Ambiguity Function Analysis and Optimization of Frequency-Hopping MIMO\n  Radar with Movable Antennas",
        "vS-Graphs: Integrating Visual SLAM and Situational Graphs through\n  Multi-level Scene Understanding",
        "Dynamic Noise Preference Optimization for LLM Self-Improvement via\n  Synthetic Data",
        "A Review on Geometry and Surface Inspection in 3D Concrete Printing",
        "Closing a Source Complexity Gap between Chapel and HPX",
        "Super-Linear Speedup by Generalizing Runtime Repeated Recursion\n  Unfolding in Prolog",
        "Notes on Khovanov homology",
        "ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed\n  Visual-Ability Families",
        "Orbit recovery from invariants of low degree in representations of\n  finite groups",
        "A low-PAPR Pilot Design and Optimization for OTFS Modulation",
        "Guided SAM: Label-Efficient Part Segmentation",
        "The Building Blocks of Classical Nonparametric Two-Sample Testing\n  Procedures: Statistically Equivalent Blocks",
        "Anderson localized states for the nonlinear Maryland model on\n  $\\mathbb{Z}^d$"
      ],
      "abstract":[
        "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
        "In this report, we introduce our first-generation reasoning model,\nLexPro-1.0, a large language model designed for the highly specialized Chinese\nlegal domain, offering comprehensive capabilities to meet diverse realistic\nneeds. Existing legal LLMs face two primary challenges. Firstly, their design\nand evaluation are predominantly driven by computer science perspectives,\nleading to insufficient incorporation of legal expertise and logic, which is\ncrucial for high-precision legal applications, such as handling complex\nprosecutorial tasks. Secondly, these models often underperform due to a lack of\ncomprehensive training data from the legal domain, limiting their ability to\neffectively address real-world legal scenarios. To address this, we first\ncompile millions of legal documents covering over 20 types of crimes from 31\nprovinces in China for model training. From the extensive dataset, we further\nselect high-quality for supervised fine-tuning, ensuring enhanced relevance and\nprecision. The model further undergoes large-scale reinforcement learning\nwithout additional supervision, emphasizing the enhancement of its reasoning\ncapabilities and explainability. To validate its effectiveness in complex legal\napplications, we also conduct human evaluations with legal experts. We develop\nfine-tuned models based on DeepSeek-R1-Distilled versions, available in three\ndense configurations: 14B, 32B, and 70B.",
        "Existing data-driven control methods generally do not address False Data\nInjection (FDI) and Denial-of-Service (DoS) attacks simultaneously. This letter\nintroduces a distributed data-driven attack-resilient consensus problem under\nboth FDI and DoS attacks and proposes a data-driven consensus control\nframework, consisting of a group of comprehensive attack-resilient observers.\nThe proposed group of observers is designed to estimate FDI attacks, external\ndisturbances, and lumped disturbances, combined with a DoS attack compensation\nmechanism. A rigorous stability analysis of the approach is provided to ensure\nthe boundedness of the distributed neighborhood estimation consensus error. The\neffectiveness of the approach is validated through numerical examples involving\nboth leaderless consensus and leader-follower consensus, demonstrating\nsignificantly improved resilient performance compared to existing data-driven\ncontrol approaches.",
        "Existing methods for analyzing linguistic content from picture descriptions\nfor assessment of cognitive-linguistic impairment often overlook the\nparticipant's visual narrative path, which typically requires eye tracking to\nassess. Spatio-semantic graphs are a useful tool for analyzing this narrative\npath from transcripts alone, however they are limited by the need for manual\ntagging of content information units (CIUs). In this paper, we propose an\nautomated approach for estimation of spatio-semantic graphs (via automated\nextraction of CIUs) from the Cookie Theft picture commonly used in\ncognitive-linguistic analyses. The method enables the automatic\ncharacterization of the visual semantic path during picture description.\nExperiments demonstrate that the automatic spatio-semantic graphs effectively\ndifferentiate between cognitively impaired and unimpaired speakers. Statistical\nanalyses reveal that the features derived by the automated method produce\ncomparable results to the manual method, with even greater group differences\nbetween clinical groups of interest. These results highlight the potential of\nthe automated approach for extracting spatio-semantic features in developing\nclinical speech models for cognitive impairment assessment.",
        "Widespread integration of Generative AI tools is transforming white-collar\nwork, reshaping how workers define their roles, manage their tasks, and\ncollaborate with peers. This has created a need to develop an overarching\nunderstanding of common worker-driven patterns around these transformations. To\nfill this gap, we conducted a systematic literature review of 23 studies from\nthe ACM Digital Library that focused on workers' lived-experiences and\npractitioners with GenAI. Our findings reveal that while many professionals\nhave delegated routine tasks to GenAI to focus on core responsibilities, they\nhave also taken on new forms of AI managerial labor to monitor and refine GenAI\noutputs. Additionally, practitioners have restructured collaborations,\nsometimes bypassing traditional peer and subordinate interactions in favor of\nGenAI assistance. These shifts have fragmented cohesive tasks into piecework\ncreating tensions around role boundaries and professional identity. Our\nanalysis suggests that current frameworks, like job crafting, need to evolve to\naddress the complexities of GenAI-driven transformations.",
        "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.",
        "Anti-Ramsey theory was initiated in 1975 by Erd\\H{o}s, Simonovits and S\\'os,\ninspiring hundreds of publications since then. The present work is the third\nand last piece of our trilogy in which we introduce a far-reaching\ngeneralization via the following two functions for any graph $G$ and family\n${\\cal F}$ of graphs:\n  If $K_2 \\in {\\cal F}$, let $f(n,G|{\\cal F})$ be the smallest integer $k$ such\nthat every edge coloring of $K_n$ with at least $k$ colors forces a copy of $G$\nin which all color classes are members of ${\\cal F}$.\n  If $K_2 \\notin {\\cal F}$, let $g(n,G|{\\cal F})$ be the largest integer $k$\nfor which there exists an edge coloring of $K_n$ using exactly $k$ colors, such\nthat every copy of $G$ contains an induced color class which is a member of\n${\\cal F}$.\n  We develop methods suitable for deriving asymptotically tight results for the\n$f$-function and the $g$-function for many combinations of $G$ and ${\\cal F}$.\n  The preceding parts of the trilogy are arXiv: 2405.19812 and 2408.04257,\npublished in Discrete Applied Math. Vol. 363 and Mathematics Vol. 12:23,\nrespectively.",
        "Vehicle-to-Everything communications-assisted Autonomous Driving (V2X-AD) has\nwitnessed remarkable advancements in recent years, with pragmatic\ncommunications (PragComm) emerging as a promising paradigm for real-time\ncollaboration among vehicles and other agents.Simultaneously, extensive\nresearch has explored the interplay between collaborative perception and\ndecision-making in end-to-end driving frameworks.In this work, we revisit the\ncollaborative driving problem and propose the Select2Drive framework to\noptimize the utilization of limited computational and communication\nresources.Particularly, to mitigate cumulative latency in perception and\ndecision-making, Select2Drive introduces Distributed Predictive Perception\n(DPP) by formulating an active prediction paradigm and simplifies\nhigh-dimensional semantic feature prediction into computation cost-efficient,\nmotion-aware reconstruction. Given the \"less is more\" principle that a\nbroadened perceptual horizon possibly confuses the decision module rather than\ncontributing to it, Select2Drive utilizes Area-of-Importance-based PragComm\n(APC) to prioritize the communications of critical regions, thus boosting both\ncommunication efficiency and decision-making efficacy. Empirical evaluations on\nthe V2Xverse dataset and CARLA driving simulator demonstrate that Select2Drive\nachieves a 11.31% (resp. 7.69%) improvement in offline perception tasks under\nlimited bandwidth (resp. pose error conditions). Moreover, it delivers at most\n14.68% and 31.76% enhancement in closed-loop driving scores and route\ncompletion rates, particularly in scenarios characterized by dense traffic and\nhigh-speed dynamics.",
        "We study the inverse boundary value problem for the linear elastic wave\nequation in three-dimensional isotropic medium. We show that both the Lam\\'e\nparameters and the density can be uniquely recovered from the boundary\nmeasurements under the strictly convex foliation condition.",
        "Adders are fundamental components in digital circuits, playing a crucial role\nin arithmetic operations within computing systems and many other applications.\nThis paper focuses on the design and simulation of a 32-bit Brent-Kung parallel\nprefix adder, which is recognized for its efficient carry propagation and\nlogarithmic delay characteristics. The Brent-Kung architecture balances\ncomputational speed and hardware complexity, making it suitable for high-speed\ndigital applications. The design is implemented using Verilog HDL and simulated\nusing Cadence Design Suite tools, including NCLaunch and Genus, to evaluate its\nperformance in terms of scalability, speed, and functional working. Comparative\nanalysis with traditional adder architectures highlights the advantages of the\nBrent-Kung adder for modern digital systems.",
        "We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps:\/\/github.com\/hammoudhasan\/DiffCLIP.",
        "This paper introduces a full discretization procedure to solve wave beam\npropagation in random media modeled by a paraxial wave equation or an\nIt\\^o-Schr\\\"odinger stochastic partial differential equation. This method bears\nsimilarities with the phase screen method used routinely to solve such\nproblems. The main axis of propagation is discretized by a centered splitting\nscheme with step $\\Delta z$ while the transverse variables are treated by a\nspectral method after appropriate spatial truncation. The originality of our\napproach is its theoretical validity even when the typical wavelength $\\theta$\nof the propagating signal satisfies $\\theta\\ll\\Delta z$. More precisely, we\nobtain a convergence of order $\\Delta z$ in mean-square sense while the errors\non statistical moments are of order $(\\Delta z)^2$ as expected for standard\ncentered splitting schemes. This is a surprising result as splitting schemes\ntypically do not converge when $\\Delta z$ is not the smallest scale of the\nproblem. The analysis is based on equations satisfied by statistical moments in\nthe It\\^o-Schr\\\"odinger case and on integral (Duhamel) expansions for the\nparaxial model. Several numerical simulations illustrate and confirm the\ntheoretical findings.",
        "We study numerical regularities for complexes over noncommutative noetherian\nlocally finite $\\mathbb{N}$-graded algebras $A$ such as CM (cm)-regularity, Tor\n(tor)-regularity (Ext (ext)-regularity) and Ex (ex)-regularity, which are the\nsupremum or infimum degrees of some associated canonical complexes. We show\nthat for any right bounded complex $X$ with finitely generated cohomologies,\nthe supremum degree of $R\\underline{\\text{Hom}}_A(X, A_0)$ coincides with the\nopposite of the infimum degree of $X$ if $A_0$ is semisimple. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the\nCM-regularity of $X$ coincides with the supremum degree of\n$R\\underline{\\text{Hom}}_A(A_0,X)$ for any left bounded complex $X$ with\nfinitely generated cohomologies.\n  Several inequalities concerning the numerical regularities and the supremum\nor infimum degree of derived Hom or derived tensor complexes are given for\nnoncommutative noetherian locally finite $\\mathbb{N}$-graded algebras. Some of\nthese are generalizations of J\\o rgensen's results on the inequalities between\nthe CM-regularity and Tor-regularity, some are new even in the connected graded\ncase. Conditions are given under which the inequalities become equalities by\nestablishing two technical lemmas.\n  Following Kirkman, Won and Zhang, we also use the numerical AS-regularity\n(resp. little AS-regularity) to study Artin-Schelter regular property\n(finite-dimensional property) for noetherian $\\mathbb{N}$-graded algebras. We\nprove that the numerical AS-regularity of $A$ is zero if and only if that $A$\nis an $\\mathbb{N}$-graded AS-regular algebra under some mild conditions, which\ngeneralizes a result of Dong-Wu and a result of Kirkman-Won-Zhang. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the little\nAS-regularity of $A$ is zero if and only if $A$ is finite-dimensional.",
        "The in-context learning capabilities of modern language models have motivated\na deeper mathematical understanding of sequence models. A line of recent work\nhas shown that linear attention models can emulate projected gradient descent\niterations to implicitly learn the task vector from the data provided in the\ncontext window. In this work, we consider a novel setting where the global task\ndistribution can be partitioned into a union of conditional task distributions.\nWe then examine the use of task-specific prompts and prediction heads for\nlearning the prior information associated with the conditional task\ndistribution using a one-layer attention model. Our results on loss landscape\nshow that task-specific prompts facilitate a covariance-mean decoupling where\nprompt-tuning explains the conditional mean of the distribution whereas the\nvariance is learned\/explained through in-context learning. Incorporating\ntask-specific head further aids this process by entirely decoupling estimation\nof mean and variance components. This covariance-mean perspective similarly\nexplains how jointly training prompt and attention weights can provably help\nover fine-tuning after pretraining.",
        "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https:\/\/github.com\/fairyshine\/Chain-of-Tools .",
        "This paper presents an innovative approach to applying bidirectional\ntransformations (BX) in practice. To introduce BX to a wider audience of\ntechnologists, engineers, and researchers, we have chosen to use C# to develop\nBifrons - a library of BX lenses that replaces domain-specific programming\nlanguages (DSL) in practical use. The proposed approach simplifies the\nimplementation effort for two-way transformations by using simple symmetric\nlenses as the initial design pattern. It ensures correctness within reason by\nproviding a simple lens-testing framework. We demonstrate the usability of BX\nlenses in a realistic scenario by using Bifrons to perform a case study\nexperiment synchronizing data from two structurally and technologically\nheterogeneous databases.",
        "In this paper, we propose a movable antenna (MA)-enabled frequency-hopping\n(FH) multiple-input multiple-output (MIMO) radar system and investigate its\nsensing resolution. Specifically, we derive the expression of the ambiguity\nfunction and analyze the relationship between its main lobe width and the\ntransmit antenna positions. In particular, the optimal antenna distribution to\nachieve the minimum main lobe width in the angular domain is characterized. We\ndiscover that this minimum width is related to the antenna size, the antenna\nnumber, and the target angle. Meanwhile, we present lower bounds of the\nambiguity function in the Doppler and delay domains, and show that the impact\nof the antenna size on the radar performance in these two domains is very\ndifferent from that in the angular domain. Moreover, the performance\nenhancement brought by MAs exhibits a certain trade-off between the main lobe\nwidth and the side lobe peak levels. Therefore, we propose to balance between\nminimizing the side lobe levels and narrowing the main lobe of the ambiguity\nfunction by optimizing the antenna positions. To achieve this goal, we propose\na low-complexity algorithm based on the Rosen's gradient projection method, and\nshow that its performance is very close to the baseline. Simulation results are\npresented to validate the theoretical analysis on the properties of the\nambiguity function, and demonstrate that MAs can reduce the main lobe width and\nsuppress the side lobe levels of the ambiguity function, thereby enhancing\nradar performance.",
        "Current Visual Simultaneous Localization and Mapping (VSLAM) systems often\nstruggle to create maps that are both semantically rich and easily\ninterpretable. While incorporating semantic scene knowledge aids in building\nricher maps with contextual associations among mapped objects, representing\nthem in structured formats like scene graphs has not been widely addressed,\nencountering complex map comprehension and limited scalability. This paper\nintroduces visual S-Graphs (vS-Graphs), a novel real-time VSLAM framework that\nintegrates vision-based scene understanding with map reconstruction and\ncomprehensible graph-based representation. The framework infers structural\nelements (i.e., rooms and corridors) from detected building components (i.e.,\nwalls and ground surfaces) and incorporates them into optimizable 3D scene\ngraphs. This solution enhances the reconstructed map's semantic richness,\ncomprehensibility, and localization accuracy. Extensive experiments on standard\nbenchmarks and real-world datasets demonstrate that vS-Graphs outperforms\nstate-of-the-art VSLAM methods, reducing trajectory error by an average of\n3.38% and up to 9.58% on real-world data. Furthermore, the proposed framework\nachieves environment-driven semantic entity detection accuracy comparable to\nprecise LiDAR-based frameworks using only visual features. A web page\ncontaining more media and evaluation outcomes is available on\nhttps:\/\/snt-arg.github.io\/vsgraphs-results\/.",
        "Although LLMs have achieved significant success, their reliance on large\nvolumes of human-annotated data has limited their potential for further\nscaling. In this situation, utilizing self-generated synthetic data has become\ncrucial for fine-tuning LLMs without extensive human annotation. However,\ncurrent methods often fail to ensure consistent improvements across iterations,\nwith performance stagnating after only minimal updates. To overcome these\nchallenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO\nemploys a dynamic sample labeling mechanism to construct preference pairs for\ntraining and introduces controlled, trainable noise into the preference\noptimization process. Our approach effectively prevents stagnation and enables\ncontinuous improvement. In experiments with Zephyr-7B, DNPO consistently\noutperforms existing methods, showing an average performance boost of 2.6%\nacross multiple benchmarks. Additionally, DNPO shows a significant improvement\nin model-generated data quality, with a 29.4% win-loss rate gap compared to the\nbaseline in GPT-4 evaluations. This highlights its effectiveness in enhancing\nmodel performance through iterative refinement.",
        "Given the substantial growth in the use of additive manufacturing in\nconstruction (AMC), it is necessary to ensure the quality of printed specimens\nwhich can be much more complex than conventionally manufactured parts. This\nstudy explores the various aspects of geometry and surface quality control for\n3D concrete printing (3DCP), with a particular emphasis on deposition-based\nmethods, namely extrusion and shotcrete 3D printing (SC3DP). A comprehensive\noverview of existing quality control (QC) methods and strategies is provided\nand preceded by an in-depth discussion. Four categories of data capture\ntechnologies are investigated and their advantages and limitations in the\ncontext of AMC are discussed. Additionally, the effects of environmental\nconditions and objects' properties on data capture are also analyzed. The study\nextends to automated data capture planning methods for different sensors.\nFurthermore, various quality control strategies are explored across different\nstages of the fabrication cycle of the printed object including: (i) During\nprinting, (ii) Layer-wise, (iii) Preassembly, and (iv) Assembly. In addition to\nreviewing the methods already applied in AMC, we also address various research\ngaps and future trends and highlight potential methodologies from adjacent\ndomains that could be transferred to AMC.",
        "A previous case study measured performance vs source-code complexity across\nmultiple languages. The case study identified Chapel and HPX provide similar\nperformance and code complexity. This paper is the result of initial steps\ntoward closing the source-code complexity gap between Chapel and HPX by using a\nsource-to-source compiler. The investigation assesses the single-machine\nperformance of both Chapel and Chplx applications across Arm and x86.",
        "Runtime repeated recursion unfolding was recently introduced as a\njust-in-time program transformation strategy that can achieve super-linear\nspeedup. So far, the method was restricted to single linear direct recursive\nrules in the programming language Constraint Handling Rules (CHR). In this\ncompanion paper, we generalize the technique to multiple recursion and to\nmultiple recursive rules and provide an implementation of the generalized\nmethod in the logic programming language Prolog.\n  The basic idea of the approach is as follows: When a recursive call is\nencountered at runtime, the recursive rule is unfolded with itself and this\nprocess is repeated with each resulting unfolded rule as long as it is\napplicable to the current call. In this way, more and more recursive steps are\ncombined into one recursive step. Then an interpreter applies these rules to\nthe call starting from the most unfolded rule. For recursions which have\nsufficiently simplifyable unfoldings, a super-linear can be achieved, i.e. the\ntime complexity is reduced.\n  We implement an unfolder, a generalized meta-interpreter and a novel\nround-robin rule processor for our generalization of runtime repeated recursion\nunfolding with just ten clauses in Prolog. We illustrate the feasibility of our\ntechnique with worst-case time complexity estimates and benchmarks for some\nbasic classical algorithms that achieve a super-linear speedup.",
        "These are expository lecture notes from a graduate topics course taught by\nthe author on Khovanov homology and related invariants. Major topics include\nthe Jones polynomial, Khovanov homology, Bar-Natan's cobordism category,\napplications of Khovanov homology, some spectral sequences, Khovanov stable\nhomotopy type, and skein lasagna modules. Topological and algebraic exposition\nare sprinkled throughout as needed.",
        "We introduce ArtInsight, a novel AI-powered system to facilitate deeper\nengagement with child-created artwork in mixed visual-ability families.\nArtInsight leverages large language models (LLMs) to craft a respectful and\nthorough initial description of a child's artwork, and provides: creative\nAI-generated descriptions for a vivid overview, audio recording to capture the\nchild's own description of their artwork, and a set of AI-generated questions\nto facilitate discussion between blind or low-vision (BLV) family members and\ntheir children. Alongside ArtInsight, we also contribute a new rubric to score\nAI-generated descriptions of child-created artwork and an assessment of\nstate-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family\nmembers and their children, and as a case study with one BLV child therapist.\nOur findings highlight a preference for ArtInsight's longer,\nartistically-tailored descriptions over those generated by existing BLV AI\ntools. Participants highlighted the creative description and audio recording\ncomponents as most beneficial, with the former helping ``bring a picture to\nlife'' and the latter centering the child's narrative to generate context-aware\nAI responses. Our findings reveal different ways that AI can be used to support\nart engagement, including before, during, and after interaction with the child\nartist, as well as expectations that BLV adults and their sighted children have\nabout AI-powered tools.",
        "Motivated by applications to equivariant neural networks and cryo-electron\nmicroscopy we consider the problem of recovering the generic orbit in a\nrepresentation of a finite group from invariants of low degree. The main result\nproved here is that invariants of degree at most three separate generic orbits\nin the regular representation of a finite group defined over any infinite\nfield. This answers a question posed in a 2023 ACHA paper of Bandeira et. al.\nWe also discuss this problem for subregular representations of the dihedral and\nsymmetric groups.",
        "Orthogonal time frequency space (OTFS) modulation has been proposed recently\nas a new waveform in the context of doubly-selective multi-path channels. This\narticle proposes a novel pilot design that improves OTFS spectral efficiency\n(SE) while reducing its peak-to-average power ratio (PAPR). Instead of adopting\nan embedded data-orthogonal pilot for channel estimation, our scheme relies on\nChu sequences superimposed to data symbols. We optimize the construction by\ninvestigating the best energy split between pilot and data symbols. Two\nequalizers, and an iterative channel estimation and equalization procedure are\nconsidered. We present extensive numerical results of relevant performance\nmetrics, including the normalized mean squared error of the estimator, bit\nerror rate, PAPR and SE. Our results show that, while the embedded pilot scheme\nestimates the channel more accurately, our approach yields a better tradeoff by\nachieving much higher spectral efficiency and lower PAPR.",
        "Localizing object parts precisely is essential for tasks such as object\nrecognition and robotic manipulation. Recent part segmentation methods require\nextensive training data and labor-intensive annotations. Segment-Anything Model\n(SAM) has demonstrated good performance on a wide range of segmentation\nproblems, but requires (manual) positional prompts to guide it where to\nsegment. Furthermore, since it has been trained on full objects instead of\nobject parts, it is prone to over-segmentation of parts. To address this, we\npropose a novel approach that guides SAM towards the relevant object parts. Our\nmethod learns positional prompts from coarse patch annotations that are easier\nand cheaper to acquire. We train classifiers on image patches to identify part\nclasses and aggregate patches into regions of interest (ROIs) with positional\nprompts. SAM is conditioned on these ROIs and prompts. This approach, termed\n`Guided SAM', enhances efficiency and reduces manual effort, allowing effective\npart segmentation with minimal labeled data. We demonstrate the efficacy of\nGuided SAM on a dataset of car parts, improving the average IoU on state of the\nart models from 0.37 to 0.49 with annotations that are on average five times\nmore efficient to acquire.",
        "Statistically equivalent blocks are not frequently considered in the context\nof nonparametric two-sample hypothesis testing. Despite the limited exposure,\nthis paper shows that a number of classical nonparametric hypothesis tests can\nbe derived on the basis of statistically equivalent blocks and their\nfrequencies. Far from a moot historical point, this allows for a more unified\napproach in considering the many two-sample nonparametric tests based on ranks,\nsigns, placements, order statistics, and runs. Perhaps more importantly, this\napproach also allows for the easy extension of many univariate nonparametric\ntests into arbitrarily high dimensions that retain all null properties\nregardless of dimensionality and are invariant to the scaling of the\nobservations. These generalizations do not require depth functions or the\nexplicit use of spatial signs or ranks and may be of use in various areas such\nas life-testing and quality control. In the manuscript, an overview of\nstatistically equivalent blocks and tests based on these blocks are provided.\nThis is followed by reformulations of some popular univariate tests and\ngeneralizations to higher dimensions. Comments comparing proposed methods to\nthose based on spatial signs and ranks are offered along with some conclusions.",
        "In this paper, we investigate Anderson localization for a nonlinear\nperturbation of the Maryland model\n$H=\\varepsilon\\Delta+\\cot\\pi(\\theta+j\\cdot\\alpha)\\delta_{j,j'}$ on\n$\\mathbb{Z}^d$. Specifically, if $\\varepsilon,\\delta$ are sufficiently small,\nwe construct a large number of time quasi-periodic and space exponentially\ndecaying solutions (i.e., Anderson localized states) for the equation\n$i\\frac{\\partial u}{\\partial t}=Hu+\\delta|u|^{2p}u$ with a Diophantine\n$\\alpha$. Our proof combines eigenvalue estimates of the Maryland model with\nthe Craig-Wayne-Bourgain method, which originates from KAM theory for\nHamiltonian PDEs."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Accelerating cavity fault prediction using deep learning at Jefferson laboratory",
    "start_abstract":"Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation"
      ],
      "abstract":[
        "Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators."
      ],
      "categories":[
        "physics.acc-ph"
      ]
    },
    "list":{
      "title":[
        "An exposition on the supersimplicity of certain expansions of the\n  additive group of the integers",
        "Evaluating the resilience of ESG investments in European Markets during\n  turmoil periods",
        "A new local time-decoupled squared Wasserstein-2 method for training\n  stochastic neural networks to reconstruct uncertain parameters in dynamical\n  systems",
        "A Comprehensive Framework for Statistical Inference in Measurement\n  System Assessment Studies",
        "A Packaging Method for ALPIDE Integration Enabling Flexible and\n  Low-Material-Budget Designs",
        "FairUDT: Fairness-aware Uplift Decision Trees",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "Kauffman bracket skein module of the $(3,3,3,3)$-pretzel link exterior",
        "Breakdown of broken-symmetry approach to exchange interaction",
        "The $D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $ decay from a $D_{s1}$ molecular\n  perspective",
        "Gravitational waves and primordial black holes from the T-model\n  inflation with Gauss-Bonnet correction",
        "Inverse Gaussian Distribution, Introduction and\n  Applications:Comprehensive Analysis of Power Plant Performance: A Study of\n  Combined Cycle and Nuclear Power Plant",
        "Personalized Convolutional Dictionary Learning of Physiological Time\n  Series",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A Comparison of Strategies to Embed Physics-Informed Neural Networks in\n  Nonlinear Model Predictive Control Formulations Solved via Direct\n  Transcription",
        "Corona algebras and strongly self-absorbing $\\mathrm{C}^{\\ast}$-dynamics",
        "Monolithic On-Chip Phononic Chiral Anomalous Bulk States on LiNbO3\n  Thin-films",
        "Discovery of a large magnetic nonlinear Hall effect in an altermagnet",
        "A Variable Coefficient Free Boundary Problem for $L^p$-solvability of\n  Parabolic Dirichlet Problems in Graph Domains",
        "High temperature surface state in Kondo insulator U$_3$Bi$_4$Ni$_3$",
        "Measurable Improvement in Multi-Qubit Readout Using a Kinetic Inductance\n  Traveling Wave Parametric Amplifier",
        "Behaviour of Newton Polygon over polynomial composition",
        "LAMOST Reveals Long-lived Protoplanetary Disks",
        "Towards Spectral Convergence of Locally Linear Embedding on Manifolds\n  with Boundary",
        "Unveiling the kinematics of a central region in the triple AGN host NGC\n  7733-7734 interacting group",
        "Crossover from BKT to first-order transition induced by higher-order\n  terms in 2D XY models",
        "Species scale associated with Weinberg operator and bound on Majorana\n  neutrino mass",
        "Mechanical Torque Disruption of Dust Grains Induced by Supernova Shock\n  Waves",
        "Quantifying the Upper Limit of Backflash Attack in Quantum Key\n  Distribution"
      ],
      "abstract":[
        "In this short note, we present a self-contained exposition of the\nsupersimplicity of certain expansions of the additive group of the integers,\nsuch as adding a generic predicate (due to Chatzidakis and Pillay), a predicate\nfor the square-free integers (due to Bhardwaj and Tran) or a predicate for the\nprime integers (due to Kaplan and Shelah, assuming Dickson's conjecture).",
        "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
        "In this work, we propose and analyze a new local time-decoupled squared\nWasserstein-2 method for reconstructing the distribution of unknown parameters\nin dynamical systems. Specifically, we show that a stochastic neural network\nmodel, which can be effectively trained by minimizing our proposed local\ntime-decoupled squared Wasserstein-2 loss function, is an effective model for\napproximating the distribution of uncertain model parameters in dynamical\nsystems. Through several numerical examples, we showcase the effectiveness of\nour proposed method in reconstructing the distribution of parameters in\ndifferent dynamical systems.",
        "Measurement system analysis aims to quantify the variability in data\nattributable to the measurement system and evaluate its contribution to overall\ndata variability. This paper conducts a rigorous theoretical investigation of\nthe statistical methods used in such analyses, focusing on variance components\nand other critical parameters. While established techniques exist for\nsingle-variable cases, a systematic theoretical exploration of their properties\nhas been largely overlooked. This study addresses this gap by examining\nestimators for variance components and other key parameters in measurement\nsystem assessment, analyzing their statistical properties, and providing new\ninsights into their reliability, performance, and applicability.",
        "This work presents a novel solution for the packaging of ALPIDE chips that\nfacilitates non-planar assembly with a minimal material budget. This solution\nrepresents a technological advancement based on methodologies developed for the\nALICE ITS1 and the STAR tracker two decades ago. The core of this approach\ninvolves the use of flexible cables composed of aluminum and polyimide, with\nthicknesses on the order of tens of micrometers. These cables are connected to\nthe sensors using single-point Tape Automated Bonding (spTAB), which replaces\nthe traditional wire bonding technique that is suboptimal for curved\nintegrations. The spTAB bonding is achieved by creating openings in the\npolyimide layer, allowing aluminum wires to remain free-standing, which are\nthen connected to the sensor using pressure and ultrasonic energy. Extending\nthis concept, we have applied this approach to entire printed circuit boards\n(PCBs), resulting in a fully flexible packaging solution maintaining an\nultra-low material budget. This work introduces a prototype utilizing this\nmethod to bond an ALPIDE chip, proposing it as a viable option for future\ndesigns necessitating flexible packaging for both the chip and associated\nelectronics. The overall workflow, comprising microfabrication and assembly, is\ncarried out at the Fondazione Bruno Kessler and INFN TIFPA laboratories and\nwill be detailed to elucidate our procedures and demonstrate the applicability\nof our solution in future experimental setups. The proposed packaging features\na flexible PCB constructed from three stacked layers, each containing 20 $\\mu$m\nthick aluminum features and a 25 $\\mu$m thick polyimide substrate. These layers\ninclude a ground layer, a signal layer (encompassing both digital and analog\nsignals), and a local bonding layer (which substitutes wire bonding).",
        "Training data used for developing machine learning classifiers can exhibit\nbiases against specific protected attributes. Such biases typically originate\nfrom historical discrimination or certain underlying patterns that\ndisproportionately under-represent minority groups, such as those identified by\ntheir gender, religion, or race. In this paper, we propose a novel approach,\nFairUDT, a fairness-aware Uplift-based Decision Tree for discrimination\nidentification. FairUDT demonstrates how the integration of uplift modeling\nwith decision trees can be adapted to include fair splitting criteria.\nAdditionally, we introduce a modified leaf relabeling approach for removing\ndiscrimination. We divide our dataset into favored and deprived groups based on\na binary sensitive attribute, with the favored dataset serving as the treatment\ngroup and the deprived dataset as the control group. By applying FairUDT and\nour leaf relabeling approach to preprocess three benchmark datasets, we achieve\nan acceptable accuracy-discrimination tradeoff. We also show that FairUDT is\ninherently interpretable and can be utilized in discrimination detection tasks.\nThe code for this project is available https:\/\/github.com\/ara-25\/FairUDT",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "We show that the Kauffman bracket skein module of the $(3,3,3,3)$-pretzel\nlink exterior over $\\mathbb{Q}(q^{\\frac{1}{2}})$ is not finitely generated as a\nmodule over $\\mathbb{Q}(q^{\\frac{1}{2}})[t_1,t_2]$, where $t_1,t_2$ are the\nmeridians of two components. This disproves a finiteness conjecture proposed in\n2021.",
        "Broken-symmetry (BS) approaches are widely employed to evaluate Heisenberg\nexchange parameters, primarily in combination with DFT calculations. For many\nmagnetic materials, BS-DFT calculations give reasonable estimations of exchange\nparameters although systematic failures have also been reported. While the\nlatter were attributed to deficiencies of approximate exchange-correlation\nfunctional, we prove here by treating a simple model system that the\nbroken-symmetry methodology has serious problems. Detailed analysis clarifies\nthe intrinsic issue with the broken-symmetry treatment of low-spin states. It\nshows, in particular, that the error in the BS calculation of exchange\nparameter scales with the degree of covalency between the magnetic and the\nbridging orbitals. As a possible tool to overcome this intrinsic drawback of\nsingle-determinant BS approaches, we propose their extension to a minimal\nmulticonfigurational version.",
        "We conduct a theoretical study of the $ D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $\ndecay from the perspective that the $D_{s1}$ is a molecular state, built mostly\nfrom the $D^* K$ and $D_s^* \\eta$ components. The $D^*$ and $D_s^*$ mesons are\nallowed to decay into two pseudoscalars, with one of them merging with the\nother pseudoscalar that forms the $D_{s1}$ state, ultimately leading to the\n$\\pi^+ \\pi^- D_s$ final state. This results in a triangle diagram mechanism\nwhere all theoretical ingredients are well-known, leading to a free parameter\nframework. We evaluate the mass distributions of particle pairs and find good\nagreement with the experimental distributions of a recent LHCb experiment,\nproviding strong support to the molecular picture of the $D_{s1}(2460)$ state.\nWe also discuss the role played by the scalar mesons $f_0(500)$ and $f_0(980)$,\nat odds with the interpretation of the experimental analysis.",
        "Recently, the worldwide Pulsar Timing Array (PTA) collaborations detected a\nstochastic gravitational wave(GW) background in the nanohertz range, which may\noriginate from the early universe's inflationary phase. So in this work, we\ninvestigated induce GWs in the T-model inflation with Gauss-Bonnet coupling.\nConsider the scenario of traversing a domain wall in moduli space, we take the\ncoupling coefficient to be an approximately step function. Within suitable\nparameter regions, the model exhibits de Sitter fixed points, which allows\ninflation to undergo an ultra-slow-roll phase, which causes the power spectrum\nto exhibit a peak. Such a peak can induce nanohertz GWs, which provids an\nexplanation for the PTA observational data. Furthermore, we consider the case\nof multiple domain wall crossings, and adopting a double-step coupling\nfunction. In this case, the resulting GW spectrum has two peaks with\nfrequencies around \\(10^{-8} \\,\\text{Hz}\\) and \\(10^{-2}\\,\\text{Hz}\\),\nrespectively. Which can be observed by the PTA and the space GW detectors\nsimultaneously.Additionally, the reentry of the power spectrum peaks into the\nhorizon leads to the collapse into primordial black holes (PBHs). We calculate\nthe abundance of PBHs and found that the masses is in the range of \\(10^{-14}\n\\sim 10^{-13} M_\\odot\\) and around \\(10^{-2} M_\\odot\\) , which constitute\nsignificant components of the current dark matter.",
        "This paper presents a comprehensive analysis of power plant performance using\nthe inverse Gaussian (IG) distribution framework. We combine theoretical\nfoundations with practical applications, focusing on both combined cycle and\nnuclear power plant contexts. The study demonstrates the advantages of the IG\ndistribution in modeling right-skewed industrial data, particularly in power\ngeneration. Using the UCI Combined Cycle Power Plant Dataset, we establishthe\nsuperiority of IG-based models over traditional approaches through rigorous\nstatistical testing and model validation. The methodology developed here\nextends naturally to nuclear power plant applications, where similar\nstatistical patterns emerge in operational data. Our findings suggest that\nIG-based models provide more accurate predictions and better capture the\nunderlying physical processes in power generation systems.",
        "Human physiological signals tend to exhibit both global and local structures:\nthe former are shared across a population, while the latter reflect\ninter-individual variability. For instance, kinetic measurements of the gait\ncycle during locomotion present common characteristics, although idiosyncrasies\nmay be observed due to biomechanical disposition or pathology. To better\nrepresent datasets with local-global structure, this work extends Convolutional\nDictionary Learning (CDL), a popular method for learning interpretable\nrepresentations, or dictionaries, of time-series data. In particular, we\npropose Personalized CDL (PerCDL), in which a local dictionary models local\ninformation as a personalized spatiotemporal transformation of a global\ndictionary. The transformation is learnable and can combine operations such as\ntime warping and rotation. Formal computational and statistical guarantees for\nPerCDL are provided and its effectiveness on synthetic and real human\nlocomotion data is demonstrated.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "This study aims to benchmark candidate strategies for embedding neural\nnetwork (NN) surrogates in nonlinear model predictive control (NMPC)\nformulations that are subject to systems described with partial differential\nequations and that are solved via direct transcription (i.e., simultaneous\nmethods). This study focuses on the use of physics-informed NNs and\nphysics-informed convolutional NNs as the internal (surrogate) models within\nthe NMPC formulation. One strategy embeds NN models as explicit algebraic\nconstraints, leveraging the automatic differentiation (AD) of an algebraic\nmodelling language (AML) to evaluate the derivatives. Alternatively, the solver\ncan be provided with derivatives computed external to the AML via the AD\nroutines of the machine learning environment the NN is trained in. The three\nnumerical experiments considered in this work reveal that replacing mechanistic\nmodels with NN surrogates may not always offer computational advantages when\nsmooth activation functions are used in conjunction with a local nonlinear\nsolver (e.g., Ipopt), even with highly nonlinear systems. Moreover, in this\ncontext, the external function evaluation of the NN surrogates often\noutperforms the embedding strategies that rely on explicit algebraic\nconstraints, likely due to the difficulty in initializing the auxiliary\nvariables and constraints introduced by explicit algebraic reformulations.",
        "This article concerns the structure of $\\mathrm{C}^{\\ast}$-algebraic group\nactions induced on corona algebras from a given $\\sigma$-unital\n$\\mathrm{C}^{\\ast}$-dynamical system over a locally compact group $G$. We prove\nthat such actions satisfy the so-called dynamical folding property, which\ngeneralizes a fundamental property observed for corona algebras in works of\nManuilov--Thomsen and Phillips--Weaver. We then focus on corona actions induced\nfrom $G$-$\\mathrm{C}^{\\ast}$-dynamics that are assumed to absorb a given\nstrongly self-absorbing and unitarily regular $G$-action $\\gamma$. It is proved\nthat these corona actions are $\\gamma$-saturated, which is a stronger property\nthan being separably $\\gamma$-stable. Conversely, if one assumes that the\nunderlying $\\mathrm{C}^{\\ast}$-dynamics absorbs the trivial action on the\ncompact operators, then $\\gamma$-saturation of the corona action is equivalent\nto the original action being $\\gamma$-absorbing. These results are a dynamical\nversion of recent work by Farah and the third-named author.",
        "Phononic materials are crucial for developing efficient, robust mechanical\nwaveguides with strong transport properties, enabling advances in sensing,\nsignal processing, energy harvesting, and microfluidics. A key motivation is\ntheir integration into monolithic systems for on-chip applications. While\ntopological phononic materials developed in the past decade offer\nunidirectional edge states immune to backscattering, their integration requires\nlarge volumes to control localized small volumes' transport properties,\nlimiting their efficiency and application in modern phononic circuits. The\nrecently introduced chiral anomalous bulk states (CABSs) combine the advantages\nof topological materials with innovative boundary designs, overcoming\ntransmission limitations and ensuring full material utilization for superior\nwave propagation. Here, we present the first on-chip monolithic CABS device\nintegrated on a suspended LiNbO3 thin film. This breakthrough enables the\ncreation of phononic waveguides with unmatched unidirectionality, low loss, and\nhigh transmission efficiency, seamlessly integrated with broadband\npiezoelectric transducers, and showcasing their potential for high-fidelity,\nbroad-bandwidth microwave signal transmission. Additionally, we exploit the\nslow-wave characteristics of CABSs for delay lines and high-density signal\nprocessing. Tailoring wave propagation through boundary engineering opens a new\nparadigm for phononic\/photonic device design, with implications across\nmicroelectronics, high-frequency communications, radar, and advanced sensing\ntechnologies. The work sets the stage for the future development of highly\nscalable, multifunctional, and robust phononic systems, unlocking new avenues\nfor integrated acoustic technologies.",
        "Since Edwin Halls groundbreaking discovery of the Hall effect in 1879,\nmagnetism, spin, and quantization have been expanding the scope of Hall\neffects, continuously driving transformative progress in science and\ntechnology. Among them, the latest nonlinear Hall effect (NLHE), where\nlongitudinal electric field tunes quantum geometry to generate nonlinear Hall\nvoltage, attracts wide attention as a sensitive probe of topological phases\nacross a wide range of materials. Here, we report a new Hall effect member: the\nmagnetic nonlinear Hall effect (MNLHE), characterized by a quadratic Hall\nconductivity dependence on magnetic field, rather than electric field as in\nNLHE. This finding relies on an altermagnet, Mn5Si3 thin film, whose\nalternating-sign Berry curvatures ensure higher-order MNLHE clearly\ndistinguishable from the first-order anomalous Hall effect. The observed\nquadratic dependence originates from chiral next-nearest-neighbor hopping\nprocesses that acquire magnetic-exchange-driven Zeeman energies and\nHaldane-like chiral flux phases. Remarkably, this MNLHE is non-analytic, as\nreversing the magnetic field flips the alternating spin-splitting bands and\nreverses the hopping chirality, which is absent in traditional NLHE. Beyond\noffering a distinctive transport fingerprint for altermagnet Mn5Si3 thin film,\nthis MNLHE is large and unsaturated up to 60 T, providing opportunities for\npulsed high-field sensing technologies in both fundamental researches and\nengineering applications.",
        "We investigate variable coefficient analogs of a recent work of Bortz,\nHofmann, Martell and Nystr\\\"om [BHMN25]. In particular, we show that if\n$\\Omega$ is the region above the graph of a Lip(1,1\/2) (parabolic Lipschitz)\nfunction and $L$ is a parabolic operator in divergence form \\[L = \\partial_t -\n\\text{div} A \\nabla\\] with $A$ satisfying an $L^1$ Carleson condition on its\nspatial and time derivatives, then the $L^p$-solvability of the Dirichlet\nproblem for $L$ and $L^*$ implies that the graph function has a half-order time\nderivative in BMO. Equivalently, the graph is parabolic uniformly rectifiable.\n  In the case of $A$ symmetric, we only require that the Dirichlet problem for\n$L$ is solvable, which requires us to adapt a clever integration by parts\nargument by Lewis and Nystr\\\"om. A feature of the present work is that we must\novercome the lack of translation invariance in our equation, which is a\nfundamental tool in similar works, including [BHMN25].",
        "The resurgence of interest in Kondo insulators has been driven by two major\nmysteries: the presence of metallic surface states and the observation of\nquantum oscillations. To further explore these mysteries, it is crucial to\ninvestigate another similar system beyond the two existing ones, SmB$_6$ and\nYbB$_{12}$. Here, we address this by reporting on a Kondo insulator,\nU$_3$Bi$_4$Ni$_3$. Our transport measurements reveal that a surface state\nemerges below 250 K and dominates transport properties below 150 K, which is\nwell above the temperature scale of SmB$_6$ and YbB$_{12}$. At low\ntemperatures, the surface conductivity is about one order of magnitude higher\nthan the bulk. The robustness of the surface state indicates that it is\ninherently protected. The similarities and differences between\nU$_3$Bi$_4$Ni$_3$ and the other two Kondo insulators will provide valuable\ninsights into the nature of metallic surface states in Kondo insulators and\ntheir interplay with strong electron correlations.",
        "Increasing the size and complexity of quantum information systems requires\nhighly-multiplexed readout architectures, as well as amplifier chains operating\nnear the quantum limit (QL) of added noise. While documented prior efforts in\nKITWPA integration in quantum systems are scarce, in this work we demonstrate\nintegration of a KI-TWPA with a multiplexed-qubit device. To quantify the\nsystem noise improvement we perform an ac Stark shift calibration to precisely\ndetermine noise power levels on-chip (at each cavity's reference plane) and the\ntotal system gain. We then characterize the qubit state measurement fidelity\nand the corresponding signal-to-noise ratio (SNR). To conduct the most faithful\nmeasurement of the benefits offered by the KI-TWPA we perform these\nmeasurements for readout chains where the high electron mobility transistor\n(HEMT) amplifier is the first-stage amplifier (FSA) - with none of the external\nhardware required to operate the KI-TWPA - and with the KI-TWPA as the FSA.\nWhile some readout cavities fall outside the KI-TWPA bandwidth, for those\ninside the bandwidth we demonstrate a maximum improvement in the state\nmeasurement SNR by a factor of 1.45, and increase the fidelity from 96.2% to\n97.8%. These measurements demonstrate a system noise below 5 quanta referenced\non-chip and we bound the KI-TWPA excess noise to be below 4 quanta for the six\ncavities inside its bandwidth. These results show a promising path forward for\nrealizing quantum-limited readout chains in large qubit systems using a single\nparametric amplifier.",
        "In this paper, we study the structure of Newton polygons for compositions of\npolynomials over the rationals. We establish sufficient conditions under which\nthe successive vertices of the Newton polygon of the composition $ g(f^n(x)) $\nwith respect to a prime $ p $ can be explicitly described in terms of the\nNewton polygon of the polynomial $ g(x) $. Our results provide deeper insights\ninto how the Newton polygon of a polynomial evolves under iteration and\ncomposition, with applications to the study of dynamical irreducibility,\neventual stability, non-monogenity of tower of number fields, etc.",
        "While both observations and theories demonstrate that protoplanetary disks\nare not expected to live much longer than $\\sim$10 Myr, several examples of\nprolonged disks have been observed in the past. In this work, we perform a\nsystematic search for aged YSOs still surrounded by protoplanetary disks in the\nM star catalog from the LAMOST archive. We identify 14 sources older than 10\nMyr, still surrounded by protoplanetary disks and with ongoing accretion\nactivities, significantly improving the census of the category known as the\nPeter Pan disks. The stellar parameters, variability and accretion properties\nof these objects, as well as their spatial distribution, are investigated.\nNearly all of these objects are distributed far away from nearby associations\nand star forming regions, but show evidence of being members of open clusters.\nInvestigating the correlation between mass accretion rates and stellar masses,\nwe find these long-lived disks accrete at systematically lower levels, compared\nto their younger counterparts with similar stellar masses. Studying the\nevolution of mass accretion rates with stellar ages, we find these aged disks\nfollow similar trend as young ones.",
        "We study the eigenvalues and eigenfunctions of a differential operator that\ngoverns the asymptotic behavior of the unsupervised learning algorithm known as\nLocally Linear Embedding when a large data set is sampled from an interval or\ndisc. In particular, the differential operator is of second order, mixed-type,\nand degenerates near the boundary. We show that a natural regularity condition\non the eigenfunctions imposes a consistent boundary condition and use the\nFrobenius method to estimate pointwise behavior. We then determine the limiting\nsequence of eigenvalues analytically and compare them to numerical predictions.\nFinally, we propose a variational framework for determining eigenvalues on\nother compact manifolds.",
        "We present a detailed study of the interacting triple active galactic nuclear\nsystem NGC 7733-34, focusing on stellar kinematics, ionised gas characteristics\nand star formation within the central region and stellar bars of both galaxies.\nWe performed a comprehensive analysis using archival data from MUSE, HST\/ACS,\nand DECaLS, complemented by observations from UVIT and IRSF. We identified a\ndisc-like bulge in both NGC 7733 and NGC 7734 through 2-D decomposition. A\ncentral nuclear structure, with a semi-major axis of $\\sim$1.113 kpc, was\ndetected in NGC 7733 via photometric and kinematic analysis, confirmed by the\nstrong anti-correlation between $V\/\\sigma$ and $h_{3}$, indicative of circular\norbits in the centre. NGC 7734 lacks a distinct nuclear structure. The presence\nof disc-like bulge results in an anti-correlation between $V\/\\sigma$ and\n$h_{3}$ along with diffuse light. However, it does show higher central velocity\ndispersion, possibly attributed to an interaction with a smaller clump, which\nis likely a fourth galaxy within the system. Both galaxies demonstrate ongoing\nstar formation, evidenced by $FUV$ and $H\\alpha$ observations. NGC 7734 shows\nrecent star formation along its bar, while NGC 7733 experiences bar quenching.\nThe star formation rate (SFR) analysis of NGC 7734 reveals that the bar\nregion's SFR dominates the galaxy's overall SFR. Conversely, in NGC 7733, the\nlack of star formation along the bar and the presence of a Seyfert 2 active\ngalactic nuclei at the galaxy centre leave the possibility of a connection\nbetween both facts. However, it does not affect the galaxy's overall star\nformation. Our findings provide valuable insights into the stellar and gas\nkinematics, star formation processes, and active galactic nuclear feedback\nmechanisms in interacting galaxies hosting stellar bars.",
        "We study phase transitions in $XY$ models, generalized by inclusion of $n$\nhigher-order pairwise interactions of equal strength, by Monte Carlo\nsimulation. It is found that by adding new terms the\nBerezinskii-Kosterlitz-Thouless (BKT) transition, observed in the standard $XY$\nmodel, gradually changes to the first-order phase transition. We determine the\ncritical number of terms for which the first-order transition appears as\n$n_c=6$. It is also found that for $n=5$ the transition is pseudo-first-order\nbut it becomes true first-order if the couplings are allowed to increase. In\ngeneral, a more rapid increase of the coupling intensity supports the\nfirst-order transition, however, a too fast increase may result in splitting of\nthe single transition to multiple transitions. Consequently, the minimal number\nof the terms required for the change of the BKT phase transition to first order\nin the present model with arbitrary couplings is estimated to be $2 < n_c \\leq\n5$.",
        "When states in a tower like the Kaluza-Klein or the string tower couple to\nanother state through the irrelevant operators of the same type, their\ncontributions to the loop corrections of the relevant or the marginal operators\nare not negligible, threatening the perturbativity. This can be avoided\nprovided the cutoff scale is lower than the species scale associated with the\nirrelevant operator. We apply this to towers of states associated with the\nneutrino which couple to the Higgs through the Weinberg operator, the\ndimension-5 irrelevant operator generating the Majorana neutrino mass.\nRequiring the `Majorana species scale', the species scale associated with the\nWeinberg operator, to be below the gravitational species scale, one finds the\nlower bound on the Majorana neutrino mass determined by the species number. The\nFestina-Lente bound also gives the lower bound on the Majorana neutrino mass,\nbut it is not so stringent. Meanwhile, even if the neutrino mass is of the\nDirac type at the renormalizable level, the Majorana mass term still can be\nwritten in the effective field theory action so far as the Weinberg operator is\nnot forbidden. Even if the Majorana neutrino mass is larger than the Dirac one,\nso far as there are sufficient degrees of freedom with mass smaller than the\nscale of the cosmological constant, the observation of the Majorana nature of\nthe neutrino may not contradict to quantum gravity constraints which rules out\nthe neutrino mass purely given by the Majorana type.",
        "The feedback from massive stars drives the evolution of interstellar dust\ngrains by altering their physical properties via a number of radiative and\nmechanical processes. Through these interactions, interstellar grains can\nachieve high rotational velocities due to unbalanced torques, potentially\nleading to their disruption. Mechanical torque disruption occurs when gas-grain\ncollisions, induced by the passage of shocks, spin grains to critical\nrotational velocities. This study aims to investigate the effects of stochastic\nmechanical torque disruption on both pre-existent and supernova-condensed dust\ngrains located within wind-blown bubbles. The impact of mechanical torque\ndisruption on supernova-condensed dust and dust grains in wind-blown bubbles is\ninvestigated through post-processing of three-dimensional hydrodynamical\nsimulation outputs. The associated timescale is then compared to those of\nkinetic sputtering and grain shattering. Before the supernova explosion, dust\ngrain disruption timescales within wind-driven bubbles are on the order of\nmillions of years due to the low-density environment. The timescales for\nmechanical torque disruption (METD) are longer than those for kinetic\nsputtering and comparable to those of grain shattering, primarily due the high\ngrain drift velocities typical of these regions.",
        "Quantum Key Distribution (QKD) theoretically provides information-theoretic\nsecurity based on physical laws. However, imperfections in practice lead to the\npossibility of quantum hacking on the QKD implementation, especially the\npassive attacks that are difficult to be detected. In this paper, we study\nexperimentally and theoretically the upper limit of a backflash attack, as one\nof the vital passive attacks, on a fiber-based QKD system. We experimentally\ndemonstrate the backflash attack on a full equipped fiber-based QKD receiver to\nshow its feasibility and limited distinguish ratio of decoding. More\nimportantly, we have developed a simulation model to analyze the maximum\ndistinguish ratio of decoding can be achieved considering the wide-spectrum\nfeature of backflash photons, which indicates that Eve can extract effective\nkey information from 95.7% of the backflash photons. Consequently, the secure\nkey rate of the decoy-state BB84 QKD system under backflash attack is\ncalculated. This work provides a general methodology to comprehensively\nevaluate the effect of the backflash attack on a QKD system."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A New Kind of Science",
    "start_abstract":"3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics.",
    "start_categories":[
      "cs.FL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory"
      ],
      "abstract":[
        "Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore."
      ],
      "categories":[
        "cs.NA"
      ]
    },
    "list":{
      "title":[
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces",
        "Data-driven continuation of patterns and their bifurcations",
        "Spaces of subgroups of toral groups",
        "Geometric properties for a certain subclass of normalized harmonic\n  mappings",
        "A new tail bound for the sum of bounded independent random variables",
        "SUSY transformation as the coupler of non-interacting systems",
        "Joint State-Parameter Estimation for the Reduced Fracture Model via the\n  United Filter",
        "Recent advances in high-dimensional quantum frequency combs",
        "Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Propagation of optical solitons in the dielectric medium of a liquid\n  csystal",
        "ContinuouSP: Generative Model for Crystal Structure Prediction with\n  Invariance and Continuity",
        "Phase space geometry of collective spin systems: Scaling and Fractality",
        "Flat band driven itinerant magnetism in the Co-pnictides\n  (La,Ca)Co$_2$(As,P)$_2$",
        "Semi-analytical Engineering of Strongly Driven Nonlinear Systems Beyond\n  Floquet and Perturbation Theory"
      ],
      "abstract":[
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given.",
        "Patterns and nonlinear waves, such as spots, stripes, and rotating spirals,\narise prominently in many natural processes and in reaction-diffusion models.\nOur goal is to compute boundaries between parameter regions with different\nprevailing patterns and waves. We accomplish this by evolving randomized\ninitial data to full patterns and evaluate feature functions, such as the\nnumber of connected components or their area distribution, on their sublevel\nsets. The resulting probability measure on the feature space, which we refer to\nas pattern statistics, can then be compared at different parameter values using\nthe Wasserstein distance. We show that arclength predictor-corrector\ncontinuation can be used to trace out transition and bifurcation curves in\nparameter space by maximizing the distance of the pattern statistics. The\nutility of this approach is demonstrated through a range of examples involving\nhomogeneous states, spots, stripes, and spiral waves.",
        "We study the space of conjugacy classes of subgroups of a compact Lie group G\nwhose identity component is a torus, and consider how various invariants of\nsubgroups behave as sheaves over this space. This feeds in to the author's\nprogramme to give algebraic models of rational G-equivariant cohomology\ntheories.\n  The methods are illustrated by making the outcome explicit for all toral\nsubgroups of compact connected rank 2 groups.",
        "Let $\\mathcal{H}$ be the class of harmonic functions $f=h+\\overline{g}$ in\nthe unit disk $\\mathbb{D}:=\\{z\\in\\mathbb{C}:|z|<1\\}$, where $h$ and $g$ are\nanalytic in $\\mathbb{D}$ with the normalization $h(0)=g(0)=h'(0)-1=0$. Let\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$ denote the subclass of $\\mathcal{H}$ in\n$\\mathbb{D}$ satisfying $\\text{Re}\\left((1-\\alpha)h'(z)+\\alpha\nzh''(z)\\right)>-M+\\left|(1-\\alpha)g'(z)+\\alpha zg''(z)\\right|$ with $g'(0)=0$,\n$M>0$ and $\\alpha\\in(0,1]$. In this paper, we investigate some fundamental\nproperties of functions belonging to the class\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$, including coefficient bounds, growth\nestimates, convexity, starlikeness, convex combinations, and convolution.",
        "We construct a new tail bound for the sum of independent random variables for\nsituations in which the expected value of the sum is known and each random\nvariable lies within a specified interval, which may be different for each\nvariable. This new bound can be computed by solving a two-dimensional convex\noptimization problem. Simulations demonstrate that the new bound is often\nsubstantially tighter than Hoeffding's inequality for cases in which both\nbounds are applicable.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately.",
        "In this paper, we introduce an effective United Filter method for jointly\nestimating the solution state and physical parameters in flow and transport\nproblems within fractured porous media. Fluid flow and transport in fractured\nporous media are critical in subsurface hydrology, geophysics, and reservoir\ngeomechanics. Reduced fracture models, which represent fractures as\nlower-dimensional interfaces, enable efficient multi-scale simulations.\nHowever, reduced fracture models also face accuracy challenges due to modeling\nerrors and uncertainties in physical parameters such as permeability and\nfracture geometry. To address these challenges, we propose a United Filter\nmethod, which integrates the Ensemble Score Filter (EnSF) for state estimation\nwith the Direct Filter for parameter estimation. EnSF, based on a score-based\ndiffusion model framework, produces ensemble representations of the state\ndistribution without deep learning. Meanwhile, the Direct Filter, a recursive\nBayesian inference method, estimates parameters directly from state\nobservations. The United Filter combines these methods iteratively: EnSF\nestimates are used to refine parameter values, which are then fed back to\nimprove state estimation. Numerical experiments demonstrate that the United\nFilter method surpasses the state-of-the-art Augmented Ensemble Kalman Filter,\ndelivering more accurate state and parameter estimation for reduced fracture\nmodels. This framework also provides a robust and efficient solution for\nPDE-constrained inverse problems with uncertainties and sparse observations.",
        "High-dimensional entanglement in qudit states offers a promising pathway\ntowards the realization of practical, large-scale quantum systems that are\nhighly controllable. These systems can be leveraged for various applications,\nincluding advanced quantum information processing, secure communications,\ncomputation, and metrology. In this context, quantum frequency combs have a\ncrucial role as they inherently support multiple modes in both temporal and\nfrequency domains, while preserving a single spatial mode. The multiple\ntemporal and frequency modes of quantum frequency combs facilitate the\ngeneration, characterization, and control of high-dimensional time-frequency\nentanglement in extensive quantum systems. In this review article, we provide\nan overview of recent technological advancements in high-dimensional\nenergy-time entangled quantum frequency combs. We explore how these\ntime-frequency qudits, achieved using scalable telecommunications-wavelength\ncomponents, can empower the creation of large-scale quantum states. Advances in\nquantum frequency combs can unlock new capabilities and versatility for\npromising developments in quantum science and technology.",
        "We consider the problem of how many samples from a Gaussian multi-index model\nare required to weakly reconstruct the relevant index subspace. Despite its\nincreasing popularity as a testbed for investigating the computational\ncomplexity of neural networks, results beyond the single-index setting remain\nelusive. In this work, we introduce spectral algorithms based on the\nlinearization of a message passing scheme tailored to this problem. Our main\ncontribution is to show that the proposed methods achieve the optimal\nreconstruction threshold. Leveraging a high-dimensional characterization of the\nalgorithms, we show that above the critical threshold the leading eigenvector\ncorrelates with the relevant index subspace, a phenomenon reminiscent of the\nBaik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix\ntheory. Supported by numerical experiments and a rigorous theoretical\nframework, our work bridges critical gaps in the computational limits of weak\nlearnability in multi-index model.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Aim. Implement a stochastic representation of the wave function for a pair of\nentangled soliton functions in a liquid crystal. Show the applicability of a\nspecial soliton representation of quantum mechanics for modeling real entangled\nsystems. Methodology. The central place in the study is occupied by the method\nof mathematical modeling. As part of the calculation of stochastics by the\nmethod of abstraction and concretization, a detailed mathematical apparatus is\ngiven, adapted to the real physical case. A qualitative analysis of the\nbehavior of the material during the propagation of soliton pulses in it is\ncarried out. Results. The main value of the stochastic theory for a system of\nentangled solitons lies in the possibility of modeling the entangled states of\nreal systems - photons. In the framework of this work, the optical 1D envelopes\nof solitons in a nematic liquid crystal are considered in approximation to the\nconditions of a real physical problem. Research implications. The theoretical\nand\/or practical significance lies in the fundamental possibility of modeling\nreal entangled systems based on the constructed stochastic model of entangled\nsolitons and subsequent creation of special applications on its basis. In\nparticular, there will be a prospect of applying quantum teleportation to the\nproblem of propagation of quantum computing for use among the components of\nquantum computing networks.",
        "The discovery of new materials using crystal structure prediction (CSP) based\non generative machine learning models has become a significant research topic\nin recent years. In this paper, we study invariance and continuity in the\ngenerative machine learning for CSP. We propose a new model, called\nContinuouSP, which effectively handles symmetry and periodicity in crystals. We\nclearly formulate the invariance and the continuity, and construct a model\nbased on the energy-based model. Our preliminary evaluation demonstrates the\neffectiveness of this model with the CSP task.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework.",
        "Flat bands can induce strong electron correlation effects that help stabilize\nboth magnetic and superconducting states. Here, we carry out angle-resolved\nphotoemission spectroscopy and density functional theory calculations to study\nthe electronic structure of the Co-pnictides CaCo$_2$As$_2$ and LaCo$_2$P$_2$.\nWe find that, while the $k_z$ Fermi topology of ferromagnetic LaCo$_2$P$_2$ is\nmarkedly 2-dimensional, antiferromagnetic CaCo$_2$As$_2$ develops a 3D Fermi\nsurface described by a $zig-zag$-like band dispersion perpendicular to the\nCo-As plane. Furthermore, the magnetism is driven by the electronic\ncorrelations of the flat bands with $d_{xy}$ and $d_{z^2}$ orbital character at\nthe Fermi level. Our results link the electronic dimensionality and the\nmagnetic order, and emphasize the critical role of the As-As and P-P bond\nstrength along the $c$-direction to understand the electronic band structure\nand the rich phase diagram of transition metal pnictides.",
        "Strongly driven nonlinear systems are frequently encountered in physics, yet\ntheir accurate control is generally challenging due to the intricate dynamics.\nIn this work, we present a non-perturbative, semi-analytical framework for\ntailoring such systems. The key idea is heuristically extending the Floquet\ntheory to nonlinear differential equations using the Harmonic Balance method.\nAdditionally, we establish a novel constrained optimization technique inspired\nby the Lagrange multiplier method. This approach enables accurate engineering\nof effective potentials across a broader parameter space, surpassing the\nlimitations of perturbative methods. Our method offers practical\nimplementations in diverse experimental platforms, facilitating nonclassical\nstate generation, versatile bosonic quantum simulations, and solving complex\noptimization problems across quantum and classical applications."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory",
    "start_abstract":"Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore.",
    "start_categories":[
      "cs.NA"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A New Kind of Science"
      ],
      "abstract":[
        "3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics."
      ],
      "categories":[
        "cs.FL"
      ]
    },
    "list":{
      "title":[
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces",
        "Data-driven continuation of patterns and their bifurcations",
        "Spaces of subgroups of toral groups",
        "Geometric properties for a certain subclass of normalized harmonic\n  mappings",
        "A new tail bound for the sum of bounded independent random variables",
        "SUSY transformation as the coupler of non-interacting systems",
        "Joint State-Parameter Estimation for the Reduced Fracture Model via the\n  United Filter",
        "Recent advances in high-dimensional quantum frequency combs",
        "Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Propagation of optical solitons in the dielectric medium of a liquid\n  csystal",
        "ContinuouSP: Generative Model for Crystal Structure Prediction with\n  Invariance and Continuity",
        "Phase space geometry of collective spin systems: Scaling and Fractality",
        "Flat band driven itinerant magnetism in the Co-pnictides\n  (La,Ca)Co$_2$(As,P)$_2$",
        "Semi-analytical Engineering of Strongly Driven Nonlinear Systems Beyond\n  Floquet and Perturbation Theory"
      ],
      "abstract":[
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given.",
        "Patterns and nonlinear waves, such as spots, stripes, and rotating spirals,\narise prominently in many natural processes and in reaction-diffusion models.\nOur goal is to compute boundaries between parameter regions with different\nprevailing patterns and waves. We accomplish this by evolving randomized\ninitial data to full patterns and evaluate feature functions, such as the\nnumber of connected components or their area distribution, on their sublevel\nsets. The resulting probability measure on the feature space, which we refer to\nas pattern statistics, can then be compared at different parameter values using\nthe Wasserstein distance. We show that arclength predictor-corrector\ncontinuation can be used to trace out transition and bifurcation curves in\nparameter space by maximizing the distance of the pattern statistics. The\nutility of this approach is demonstrated through a range of examples involving\nhomogeneous states, spots, stripes, and spiral waves.",
        "We study the space of conjugacy classes of subgroups of a compact Lie group G\nwhose identity component is a torus, and consider how various invariants of\nsubgroups behave as sheaves over this space. This feeds in to the author's\nprogramme to give algebraic models of rational G-equivariant cohomology\ntheories.\n  The methods are illustrated by making the outcome explicit for all toral\nsubgroups of compact connected rank 2 groups.",
        "Let $\\mathcal{H}$ be the class of harmonic functions $f=h+\\overline{g}$ in\nthe unit disk $\\mathbb{D}:=\\{z\\in\\mathbb{C}:|z|<1\\}$, where $h$ and $g$ are\nanalytic in $\\mathbb{D}$ with the normalization $h(0)=g(0)=h'(0)-1=0$. Let\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$ denote the subclass of $\\mathcal{H}$ in\n$\\mathbb{D}$ satisfying $\\text{Re}\\left((1-\\alpha)h'(z)+\\alpha\nzh''(z)\\right)>-M+\\left|(1-\\alpha)g'(z)+\\alpha zg''(z)\\right|$ with $g'(0)=0$,\n$M>0$ and $\\alpha\\in(0,1]$. In this paper, we investigate some fundamental\nproperties of functions belonging to the class\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$, including coefficient bounds, growth\nestimates, convexity, starlikeness, convex combinations, and convolution.",
        "We construct a new tail bound for the sum of independent random variables for\nsituations in which the expected value of the sum is known and each random\nvariable lies within a specified interval, which may be different for each\nvariable. This new bound can be computed by solving a two-dimensional convex\noptimization problem. Simulations demonstrate that the new bound is often\nsubstantially tighter than Hoeffding's inequality for cases in which both\nbounds are applicable.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately.",
        "In this paper, we introduce an effective United Filter method for jointly\nestimating the solution state and physical parameters in flow and transport\nproblems within fractured porous media. Fluid flow and transport in fractured\nporous media are critical in subsurface hydrology, geophysics, and reservoir\ngeomechanics. Reduced fracture models, which represent fractures as\nlower-dimensional interfaces, enable efficient multi-scale simulations.\nHowever, reduced fracture models also face accuracy challenges due to modeling\nerrors and uncertainties in physical parameters such as permeability and\nfracture geometry. To address these challenges, we propose a United Filter\nmethod, which integrates the Ensemble Score Filter (EnSF) for state estimation\nwith the Direct Filter for parameter estimation. EnSF, based on a score-based\ndiffusion model framework, produces ensemble representations of the state\ndistribution without deep learning. Meanwhile, the Direct Filter, a recursive\nBayesian inference method, estimates parameters directly from state\nobservations. The United Filter combines these methods iteratively: EnSF\nestimates are used to refine parameter values, which are then fed back to\nimprove state estimation. Numerical experiments demonstrate that the United\nFilter method surpasses the state-of-the-art Augmented Ensemble Kalman Filter,\ndelivering more accurate state and parameter estimation for reduced fracture\nmodels. This framework also provides a robust and efficient solution for\nPDE-constrained inverse problems with uncertainties and sparse observations.",
        "High-dimensional entanglement in qudit states offers a promising pathway\ntowards the realization of practical, large-scale quantum systems that are\nhighly controllable. These systems can be leveraged for various applications,\nincluding advanced quantum information processing, secure communications,\ncomputation, and metrology. In this context, quantum frequency combs have a\ncrucial role as they inherently support multiple modes in both temporal and\nfrequency domains, while preserving a single spatial mode. The multiple\ntemporal and frequency modes of quantum frequency combs facilitate the\ngeneration, characterization, and control of high-dimensional time-frequency\nentanglement in extensive quantum systems. In this review article, we provide\nan overview of recent technological advancements in high-dimensional\nenergy-time entangled quantum frequency combs. We explore how these\ntime-frequency qudits, achieved using scalable telecommunications-wavelength\ncomponents, can empower the creation of large-scale quantum states. Advances in\nquantum frequency combs can unlock new capabilities and versatility for\npromising developments in quantum science and technology.",
        "We consider the problem of how many samples from a Gaussian multi-index model\nare required to weakly reconstruct the relevant index subspace. Despite its\nincreasing popularity as a testbed for investigating the computational\ncomplexity of neural networks, results beyond the single-index setting remain\nelusive. In this work, we introduce spectral algorithms based on the\nlinearization of a message passing scheme tailored to this problem. Our main\ncontribution is to show that the proposed methods achieve the optimal\nreconstruction threshold. Leveraging a high-dimensional characterization of the\nalgorithms, we show that above the critical threshold the leading eigenvector\ncorrelates with the relevant index subspace, a phenomenon reminiscent of the\nBaik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix\ntheory. Supported by numerical experiments and a rigorous theoretical\nframework, our work bridges critical gaps in the computational limits of weak\nlearnability in multi-index model.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Aim. Implement a stochastic representation of the wave function for a pair of\nentangled soliton functions in a liquid crystal. Show the applicability of a\nspecial soliton representation of quantum mechanics for modeling real entangled\nsystems. Methodology. The central place in the study is occupied by the method\nof mathematical modeling. As part of the calculation of stochastics by the\nmethod of abstraction and concretization, a detailed mathematical apparatus is\ngiven, adapted to the real physical case. A qualitative analysis of the\nbehavior of the material during the propagation of soliton pulses in it is\ncarried out. Results. The main value of the stochastic theory for a system of\nentangled solitons lies in the possibility of modeling the entangled states of\nreal systems - photons. In the framework of this work, the optical 1D envelopes\nof solitons in a nematic liquid crystal are considered in approximation to the\nconditions of a real physical problem. Research implications. The theoretical\nand\/or practical significance lies in the fundamental possibility of modeling\nreal entangled systems based on the constructed stochastic model of entangled\nsolitons and subsequent creation of special applications on its basis. In\nparticular, there will be a prospect of applying quantum teleportation to the\nproblem of propagation of quantum computing for use among the components of\nquantum computing networks.",
        "The discovery of new materials using crystal structure prediction (CSP) based\non generative machine learning models has become a significant research topic\nin recent years. In this paper, we study invariance and continuity in the\ngenerative machine learning for CSP. We propose a new model, called\nContinuouSP, which effectively handles symmetry and periodicity in crystals. We\nclearly formulate the invariance and the continuity, and construct a model\nbased on the energy-based model. Our preliminary evaluation demonstrates the\neffectiveness of this model with the CSP task.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework.",
        "Flat bands can induce strong electron correlation effects that help stabilize\nboth magnetic and superconducting states. Here, we carry out angle-resolved\nphotoemission spectroscopy and density functional theory calculations to study\nthe electronic structure of the Co-pnictides CaCo$_2$As$_2$ and LaCo$_2$P$_2$.\nWe find that, while the $k_z$ Fermi topology of ferromagnetic LaCo$_2$P$_2$ is\nmarkedly 2-dimensional, antiferromagnetic CaCo$_2$As$_2$ develops a 3D Fermi\nsurface described by a $zig-zag$-like band dispersion perpendicular to the\nCo-As plane. Furthermore, the magnetism is driven by the electronic\ncorrelations of the flat bands with $d_{xy}$ and $d_{z^2}$ orbital character at\nthe Fermi level. Our results link the electronic dimensionality and the\nmagnetic order, and emphasize the critical role of the As-As and P-P bond\nstrength along the $c$-direction to understand the electronic band structure\nand the rich phase diagram of transition metal pnictides.",
        "Strongly driven nonlinear systems are frequently encountered in physics, yet\ntheir accurate control is generally challenging due to the intricate dynamics.\nIn this work, we present a non-perturbative, semi-analytical framework for\ntailoring such systems. The key idea is heuristically extending the Floquet\ntheory to nonlinear differential equations using the Harmonic Balance method.\nAdditionally, we establish a novel constrained optimization technique inspired\nby the Lagrange multiplier method. This approach enables accurate engineering\nof effective potentials across a broader parameter space, surpassing the\nlimitations of perturbative methods. Our method offers practical\nimplementations in diverse experimental platforms, facilitating nonclassical\nstate generation, versatile bosonic quantum simulations, and solving complex\noptimization problems across quantum and classical applications."
      ]
    }
  },
  {
    "id":2411.06414,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"An EEG-based brain-computer interface for real-time multi-task robotic control",
    "start_abstract":"The Brain Computer Interface (BCI) is the communication between human brain and computer. Electroencephalogram (EEG) one of biomedical signals which can be obtained by attaching electrodes to scalp. Some EEG related applications developed help disabled people, such as based wheelchair or robotic arm. A hybrid BCI real-time control system proposed a multi-tasks robot. In this system, sliding window online data segmentation strategy segment training data, enable learn dynamic features when subject's state transfer from rest task execution state. achieve ensure continuity executing actions. addition, Common Spatial Pattern (CSP) better extract spatial these continuous actions that multiple commands are accurately classified. experiment, three subjects' collected, trained tested performance reliability system. records robot's spending time, moving distance, number objects pushing down. Experimental results given show feasibility Compared remote controller, similar performance. Thus, able robot in environment used develop robot-aided arm methods on neurological rehabilitation principles for stroke injury patients.",
    "start_categories":[
      "cs.RO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Braincomputer interfaces for communication and control"
      ],
      "abstract":[
        "For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world \u2013 a brain\u2013computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or \u2018locked in\u2019, with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10\u201325 bits\/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers. With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances."
      ],
      "categories":[
        "Neurophysiology"
      ]
    },
    "list":{
      "title":[
        "Calibration-free measurement of the phonon temperature around a single\n  emitter",
        "Stability, growth, and doping of In$_{2}$(Si, Ge)$_{2}$O$_{7}$ as\n  promising n-type wide-gap semiconductors",
        "Critical Dynamics of Spin Boson Model",
        "Modulation of the galactic cosmic ray spectrum in an anisotropic\n  diffusion approach",
        "Non(anti)Commutative Superspace, Baker-Campbell-Hausdorff Closed Forms,\n  and Dirac-K\\\"ahler Twisted Supersymmetry",
        "Quantifying the generation of negatively charged boron vacancies in\n  He-ion irradiated hexagonal boron nitride",
        "Quorum sensing and absorbing phase transitions in colloidal active\n  matter",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "An Unconventional Ultra-Sub-Wavelength Receiving Nano-Antenna Activated\n  by ac Spin Pumping and the ac Inverse Spin Hall Effect",
        "Structure-Preserving Neural Ordinary Differential Equations for Stiff\n  Systems",
        "Connectivity and matching extendability of optimal $1$-embedded graphs\n  on the torus",
        "Forcing, genericity and CBERS",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "(FAPP) Infinity Does Macroscopic Irreversibility From Microscopic\n  Reversibility",
        "SeqSee: A schema-based approach to spectral sequence visualization",
        "Multi-Modality Representation Learning for Antibody-Antigen Interactions\n  Prediction",
        "On the role of true and false chirality in producing parity violating\n  energy differences",
        "Miniaturized liquid metal composite circuits with energy harvesting\n  coils for battery-free bioelectronics and optogenetics",
        "Multideterminantal measures",
        "On the Mordell-Weil rank and $2$-Selmer group of a family of elliptic\n  curves",
        "Gradient-based Explanations for Deep Learning Survival Models",
        "Parameter Choices for Sparse Multi-Parameter Regularization with the\n  $\\ell_1$ Norm",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Influence of Chemistry and Topography on the Wettability of Copper",
        "Principal component analysis for 5\/2 fractional quantum Hall states",
        "Simulating inverse patchy colloid models",
        "Formalising the intentional stance 2: a coinductive approach",
        "Searching for axion dark matter gegenschein of the Vela supernova\n  remnant with FAST",
        "Generalized network autoregressive modelling of longitudinal networks\n  with application to presidential elections in the USA"
      ],
      "abstract":[
        "The emission properties of a localized solid-state emitter are strongly\ninfluenced by its environment. The coupling to acoustic phonons impacts the\ncoherence of the emitter and its temperature dependence, and also results in\nthe apparition of phonon sidebands besides the sharp zero-phonon line. Here, we\npresent a method for measuring the absolute temperature of a localized emitter\nby directly plotting the ratio of the Stokes and anti-Stokes components of the\nphonon sideband as a function of the shift from the zero-phonon line. This\napproach requires no calibration and knowledge of the system, making it\napplicable to a wide range of emitters and materials. We validate the method\nusing a CdSe quantum dot in a ZnSe nanowire. We thus show that the quantum dot\nis significantly heated under non-resonant excitation when increasing the\nincident power at low temperature and is ascribed to the drop in thermal\nconductivity at these temperatures.",
        "In this paper we investigate, computationally and experimentally, the phase\nstability, electronic structure properties, and the propensity for n-type\ndoping of In$_{2}$X$_{2}$O$_{7}$ (X=Si, Ge) ternary oxides. This family of\nmaterials contains promising novel wide-gap semiconductors based on their\nestimated high $n$-type Baliga figures of merit and acceptable thermal\nconductivity for power electronics applications. Here, we find that both\nIn$_{2}$Si$_{2}$O$_{7}$ and In$_{2}$Ge$_{2}$O$_{7}$ to be n-type dopable, with\nZr providing between 10$^{16}$ and above 10$^{21}$ cm$^{-3}$ net donor\nconcentrations under O-poor conditions, depending on the chemistry, structure\n(ground-state thorvetite or high-pressure pyrochlore) and synthesis\ntemperature. Initial thin-film growth and annealing leads to polycrystalline\nIn$_{2}$Ge$_{2}$O$_{7}$ thin films in thorvetite structure with band gap over 4\neV, and confirms Zr doping predictions by achieving electron concentrations at\n10$^{14}$-10$^{16}$ cm$^{-3}$ under O-rich condition. While future epitaxial\ngrowth development is still needed, this study establishes\nIn$_{2}$X$_{2}$O$_{7}$ as promising n-type wide-gap semiconductors for power\nelectronic applications.",
        "In this work, we study the low-energy properties of the spin-boson model\n(SBM), which describes the dynamics of a 1\/2 spin associated with a thermostat\ncharacterized by a power law spectral density, $f(\\omega)\\propto |\\omega|^s$.\nThe theoretical description is constructed in the Schwinger--Keldysh technique,\nbased on the representation of the 1\/2-spin by Majorana fermions. We study the\ncritical dynamics of the system near the quantum phase transition by\nconstructing and analyzing the system of renormalization group equations. Our\ntheoretical approach is more universal, contrary to the one based on quantum\nclassical mapping, since it is applicable for $0<s\\leq 1$. We show that in both\nthe ohmic case $s=1$, and subohmic case $0<s<1$, the second order quantum phase\ntransition is observed in the model considered, and the critical magnetization\nexponent agrees with the exact hyperscaling result, $1\/\\delta=(1-s)\/(1+s)$.\nFurthermore, we obtain the dependence of the critical value of the spin-boson\ncoupling constant on the temperature of the bosonic thermal bath.",
        "We introduce a novel diffusion model for the propagation of cosmic rays (CRs)\nthat incorporates an anisotropic diffusion tensor of a general form within a\nrealistically modeled large-scale Galactic magnetic field. The parameters of\nthe model are consistent with the contemporary understanding of the large-scale\nGalactic magnetic field structure and the dynamics of small-scale turbulent CR\npropagation. The paper demonstrates the modulation of spectra of Galactic\ncosmic rays (GCRs) in the magnetic rigidity range of 1 - 30 PV (the CR knee)\nand explores the spatial variation of this phenomenon. The observed modulation\nof the spectrum is explained by changes in the leakage mechanism.",
        "Starting from an elementary calculation of super Lie group elements\nassociating with non(anti)-commutative Grassmann parameters, we derive several\nclosed expressions of Baker-Campbell-Hausdorff (BCH) formula which represent\nmultiplication properties of super Lie group elements in the corresponding\nsuperspace. We then show that parametrization of superspace in general may\nbecome infinite dimensional due to the presence of non(anti)commutativity. We\nshow that a Dirac-K\\\"ahler Twisted SUSY Algebra (also referred to as Marcus\nB-type Twisted SUSY Algebra or Geometric Langlands Twisted SUSY Algebra) with a\ncertain type of deformation, which we call an exponential deformation, may\ncircumvent this problem. We also provide, in terms of gauge covariantization of\nthe SUSY algebra, a geometric understanding of the exponential deformation, and\nsee that the framework constructed in this paper may serve as a\nnon(anti)commutative superspace framework providing the gauge covariant link\nformulation of twisted super Yang-Mills on a lattice.",
        "Hexagonal boron nitride (hBN) hosts luminescent defects possessing spin\nqualities compatible with quantum sensing protocols at room temperature.\nVacancies, in particular, are readily obtained via exposure to high-energy ion\nbeams. While the defect creation mechanism via such irradiation is well\nunderstood, the occurrence rate of optically active negatively charged\nvacancies ($V_B^-$) is an open question. In this work, we exploit focused\nhelium ions to systematically generate optically active vacancy defects in hBN\nflakes at varying density. By comparing the density-dependent spin splitting\nmeasured by magnetic resonance to calculations based on a microscopic charge\nmodel, in which we introduce a correction term due to a constant background\ncharge, we are able to quantify the number of $V_B^-$ defects generated by the\nion irradiation. We find that only a small fraction (0.2%) of all vacancies is\nin the optically active, negatively charged state. Our results provide a\nprotocol for measuring the generation efficiency of $V_B^-$, which is necessary\nfor understanding and optimizing luminescent centers in hBN.",
        "Unlike biological active matter that constantly adapt to their environment,\nthe motors of synthetic active particles are typically agnostic to their\nsurroundings and merely operate at constant force. Here, we design colloidal\nactive rods capable of modulating their inner activity in response to crowding,\nthereby enforcing a primitive form of quorum sensing interactions. Through\nexperiments, simulations and theory we elucidate the impact of these\ninteractions on the phase behavior of isotropic active matter. We demonstrate\nthat, when conditioned to density, motility regulation can either lead to an\nabsorbing phase transition, where all particles freeze their dynamics, or to\natypical phase separation, where flat interfaces supporting a net pressure drop\nare in mechanical equilibrium. Fully active and fully arrested particles can\nthen form heterogeneous patterns ruled by the competition between quorum\nsensing and mechanical interactions. Beyond the specifics of motile colloids,\nwe expect our findings to apply broadly to adaptive active matter assembled\nfrom living or synthetic units.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "We report an extreme sub-wavelength unconventional receiving antenna. It\nconsists of an array of nanomagnets connected to heavy metal nanostrips.\nIncident electromagnetic (EM) radiation generates intrinsic and extrinsic spin\nwaves in the nanomagnets, which pump spin into the heavy metal nanostrips at\ntheir own frequencies giving rise to a polychromatic alternating voltage across\nthe latter owing to the ac inverse spin Hall effect. This implements a\nreceiving nano-antenna. We demonstrate its operation at two different EM wave\nfrequencies of 1.5 GHz and 2.4 GHz - the latter being the Bluetooth and Wi-Fi\nfrequency. We measure the receiving gain at 2.4 GHz to be approximately -9 db.\nThe free space radiated wavelength \"lambda\" at 2.4 GHz is 12.5 cm while the\nantenna area A is merely 160 micron^2, making the ratio A\/lambda^2 =\n0.97x10^-8. This antenna's receiving gain should be very poor because of the\ntiny size. Yet the measured gain is more than 4000 times larger than the\ntheoretical limit for a conventional antenna of this size at this wavelength\nbecause of the unconventional operating principle.",
        "Neural ordinary differential equations (NODEs) are an effective approach for\ndata-based modeling of dynamical systems arising from simulations and\nexperiments. One of the major shortcomings of NODEs, especially when coupled\nwith explicit integrators, is its long-term stability, which impedes their\nefficiency and robustness when encountering stiff problems. In this work we\npresent a structure-preserving NODE approach, which integrates with a linear\nand nonlinear split and an exponential integrator, latter of which is an\nexplicit integrator with stability properties comparable to implicit methods.\nWe demonstrate that our model has advantages in both learning and deployment\nover standard explicit or even implicit NODE methods. The long-time stability\nis further enhanced by the Hurwitz matrix decomposition that constrains the\nspectrum of the linear operator, therefore stabilizing the linearized dynamics.\nWhen combined with a Lipschitz-controlled neural network treatment for the\nnonlinear operator, we show the nonlinear dynamics of the NODE are provably\nstable in the sense of Lyapunov. For high-dimensional data, we further rely on\nan autoencoder performing dimension reduction and Higham's algorithm for the\nmatrix-free application of the matrix exponential on a vector. We demonstrate\nthe effectiveness of the proposed NODE approach in various examples, including\nthe Grad-13 moment equations and the Kuramoto-Sivashinky equation.",
        "In this paper, we discuss optimal $1$-toroidal graphs (abbreviated as O1TG),\nwhich are drawn on the torus so that every edge crosses another edge at most\nonce, and has $n$ vertices and exactly $4n$ edges. We first consider\nconnectivity of O1TGs, and give the characterization of O1TGs having\nconnectivity exactly $k$ for each $k\\in \\{4, 5, 6, 8\\}$. In our argument, we\nalso show that there exists no O1TG having connectivity exactly $7$.\nFurthermore, using the result above, we discuss extendability of matchings, and\ngive the characterization of $1$-, $2$- and $3$-extendable O1TGs in turn.",
        "In this paper we continue the study of equivalence of generics filters\nstarted by Smythe in [Smy22]. We fully characterize those forcing posets for\nwhich the corresponding equivalence of generics is smooth using the purely\ntopological property of condensation. Next we leverage our characterization to\nshow that there are non-homogeneous forcing for which equivalence of generics\nis not smooth. Then we prove hyperfiniteness in the case of Prikry forcing and\nsome additional results addressing the problem whether generic equivalence for\nCohen forcing is hyperfinite.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "Infinity is central to deriving macroscopic irreversibility from reversible\nmicroscopic laws across mathematics, theoretical computer science and physics.\nIn analysis, infinite processes -- such as Dedekind cuts and Cauchy sequences\n-- construct real numbers as equivalence classes of rational approximations,\nbridging discrete rationals to the continuous real line. In quantum mechanics,\ninfinite tensor products model nested measurements, where sectorization\npartitions the Hilbert space into equivalence classes, reconciling unitary\nevolution with wavefunction collapse. In statistical mechanics, macrostates\nemerge as equivalence classes of microstates sharing identical macroscopic\nproperties, providing the statistical basis for thermodynamic irreversibility\ndespite reversible dynamics. Equivalence relations formalize\nFor-All-Practical-Purposes (FAPP) indistinguishability, reflecting operational\nlimits on precision and observation. Together, these examples reveal a unified\nframework where infinity and equivalence underpin emergent macroscopic behavior\nfrom microscopic reversibility.",
        "We present SeqSee, a software system that addresses spectral sequence\nvisualization through a schema-based approach. By introducing a standardized\nJSON schema as an intermediate representation, SeqSee decouples the\nmathematical computations of spectral sequences from their visualizations. We\ndemonstrate the system through a case study of the classical and C-motivic\nAdams spectral sequences.",
        "While deep learning models play a crucial role in predicting antibody-antigen\ninteractions (AAI), the scarcity of publicly available sequence-structure\npairings constrains their generalization. Current AAI methods often focus on\nresidue-level static details, overlooking fine-grained structural\nrepresentations of antibodies and their inter-antibody similarities. To tackle\nthis challenge, we introduce a multi-modality representation approach that\nintegates 3D structural and 1D sequence data to unravel intricate\nintra-antibody hierarchical relationships. By harnessing these representations,\nwe present MuLAAIP, an AAI prediction framework that utilizes graph attention\nnetworks to illuminate graph-level structural features and normalized adaptive\ngraph convolution networks to capture inter-antibody sequence associations.\nFurthermore, we have curated an AAI benchmark dataset comprising both\nstructural and sequence information along with interaction labels. Through\nextensive experiments on this benchmark, our results demonstrate that MuLAAIP\noutperforms current state-of-the-art methods in terms of predictive\nperformance. The implementation code and dataset are publicly available at\nhttps:\/\/github.com\/trashTian\/MuLAAIP for reproducibility.",
        "In this work we tackle the problem of showing which type of influences can\nlift the degeneracy between truly and falsely chiral systems, showing that only\nwhen both systems and influences are both truly (falsely) chiral, a parity\nviolating energy difference between left- and right-handed systems can be\nproduced. In particular, after considering the enantiomers of a chiral molecule\nas paradigmatic truly chiral systems, we rigorously show, under a quantum field\ntheoretically approach, that only a truly chiral influence such as the\n$Z^{0}$-mediated electroweak interaction can lift the degeneracy between\nenantiomers. On the contrary, we explicitly show that a falsely chiral\ninfluence, such as an axion-mediated interaction in chiral molecules, can not\nlift the aforementioned degeneracy. These results extend Barron's seminal ideas\n[L. D. Barron, True and false chirality and parity violation, Chem. Phys. Lett\n{\\bf 123}, 423 (1986)] to a quantum field theory-based approach.",
        "Over the past years, rapid progress has been made on soft-matter electronics\nfor wearable and implantable devices, for bioelectronics and optogenetics.\nLiquid Metal (LM) based electronics were especially popular, due to their\nlong-term durability, when subject to repetitive strain cycles. However, one\nmajor limitation has been the need for tethering bioelectronics circuits to\nexternal power, or the use of rigid bulky batteries. This has motivated a\ngrowing interest in wireless energy transfer, which demands circuit\nminiaturization. However, miniaturization of LM circuits is challenging due to\nlow LM-substrate adhesion, LM smearing, and challenges on\nmicrochip-interfacing. In this article, we address these challenges by\nhigh-resolution laser-assisted micropatterning of biphasic LM composites and\nvapor-assisted LM microchip soldering. Through development of a search\nalgorithm for optimization of the biphasic ink coil performance, we designed\nand implemented micro coils with trace spacing of 50 {\\mu}m that can harvest a\nsignificant amount of energy (178 mW\/cm2) through near field inductive\ncoupling. We show miniaturized soft-matter circuits with integrated SMD chips\nsuch as NFC chips, capacitors, and LEDs that are implemented in a few minutes\nthrough laser patterning, and vaporassisted soldering. In the context of\noptogenetics, where lightweight, miniaturized systems are needed to provide\noptical stimulation, soft coils stand out in terms of their improved\nconformability and flexibility. Thus, this article explores the applications of\nsoft coils in wearable and implantable devices, with a specific focus on their\nuse in optogenetics.",
        "We define multideterminantal probability measures, a family of probability\nmeasures on $[k]^n$ where $[k]=\\{1,2,\\dots,k\\}$, generalizing determinantal\nmeasures (which correspond to the case $k=2$). We give examples coming from the\npositive Grassmannian, from the dimer model and from the spanning tree model.\n  We also define and completely characterize determinantal probability measures\non the permutation group $S_n$.",
        "We consider the parametric family of elliptic curves over $\\mathbb{Q}$ of the\nform $E_{m} : y^{2} = x(x - n_{1})(x - n_{2}) + t^{2}$, where $n_{1}$, $n_{2}$\nand $t$ are particular polynomial expressions in an integral variable $m$. In\nthis paper, we investigate the torsion group $E_{m}(\\mathbb{Q})_{\\rm{tors}}$, a\nlower bound for the Mordell-Weil rank $r({E_{m}})$ and the $2$-Selmer group\n${\\rm{Sel}}_{2}(E_{m})$ under certain conditions on $m$. This extends the\nprevious works done in this direction, which are mostly concerned with the\nMordell-Weil ranks of various parametric families of elliptic curves.",
        "Deep learning survival models often outperform classical methods in\ntime-to-event predictions, particularly in personalized medicine, but their\n\"black box\" nature hinders broader adoption. We propose a framework for\ngradient-based explanation methods tailored to survival neural networks,\nextending their use beyond regression and classification. We analyze the\nimplications of their theoretical assumptions for time-dependent explanations\nin the survival setting and propose effective visualizations incorporating the\ntemporal dimension. Experiments on synthetic data show that gradient-based\nmethods capture the magnitude and direction of local and global feature\neffects, including time dependencies. We introduce GradSHAP(t), a\ngradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and\nSurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply\nthese methods to medical data with multi-modal inputs, revealing relevant\ntabular features and visual patterns, as well as their temporal dynamics.",
        "This paper introduces a multi-parameter regularization approach using the\n$\\ell_1$ norm, designed to better adapt to complex data structures and problem\ncharacteristics while offering enhanced flexibility in promoting sparsity in\nregularized solutions. As data volumes grow, sparse representations of learned\nfunctions become critical for reducing computational costs during function\noperations. We investigate how the selection of multiple regularization\nparameters influences the sparsity of regularized solutions. Specifically, we\ncharacterize the relationship between these parameters and the sparsity of\nsolutions under transform matrices, enabling the development of an iterative\nscheme for selecting parameters that achieve prescribed sparsity levels.\nSpecial attention is given to scenarios where the fidelity term is\nnon-differentiable, and the transform matrix lacks full row rank. In such\ncases, the regularized solution, along with two auxiliary vectors arising in\nthe sparsity characterization, are essential components of the multi-parameter\nselection strategy. To address this, we propose a fixed-point proximity\nalgorithm that simultaneously determines these three vectors. This algorithm,\ncombined with our sparsity characterization, forms the basis of a practical\nmulti-parameter selection strategy. Numerical experiments demonstrate the\neffectiveness of the proposed approach, yielding regularized solutions with\nboth predetermined sparsity levels and satisfactory approximation accuracy.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "To understand the complex interplay of topography and surface chemistry in\nwetting, fundamental studies investigating both parameters are needed. Due to\nthe sensitivity of wetting to miniscule changes in one of the parameters it is\nimperative to precisely control the experimental approach. A profound\nunderstanding of their influence on wetting facilitates a tailored design of\nsurfaces with unique functionality. We present a multi-step study: The\ninfluence of surface chemistry is analyzed by determining the adsorption of\nvolatile carbonous species (A) and by sputter deposition of metallic copper and\ncopper oxides on flat copper substrates (B). A precise surface topography is\ncreated by laser processing. Isotropic topography is created by ps laser\nprocessing (C), and hierarchical anisotropic line patterns are produced by\ndirect laser interference patterning (DLIP) with different pulse durations (D).\nOur results reveal that the long-term wetting response of polished copper\nsurfaces stabilizes with time despite ongoing accumulation of hydrocarbons and\nis dominated by this adsorption layer over the oxide state of the substrate\n(Cu, CuO, Cu2O). The surfaces' wetting response can be precisely tuned by\ntailoring the topography via laser processing. The sub-pattern morphology of\nprimary line-like patterns showed great impact on the static contact angle,\nwetting anisotropy, and water adhesion. An increased roughness inside the\npattern valleys combined with a minor roughness on the peaks favors\nair-inclusions, isotropic hydrophobicity, and low water adhesion. Increasing\nthe aspect ratio showed to enhance air-inclusions and hydrophobicity despite\nincreased peak roughness while time dependent wetting transitions were\nobserved.",
        "For the special single-layer fractional quantum Hall system with a filling\nfactor of 5\/2, which has an even denominator, this paper uses principal\ncomponent analysis (PCA) to study its behavior under the breaking of\nparticle-hole symmetry. By introducing a model three-body potential to\nrepresent the mechanism of particle-hole symmetry breaking, the paper finds\nthat the 5\/2 system evolves into two types of special topological quantum\nstates with non-Abelian statistics as the strength and direction of the\nthree-body potential vary. The transition points of these states correspond to\nthe particle-hole symmetric pure Coulomb interaction system. Our results\nvalidate the applicability of machine learning as a new research tool in\nfractional quantum Hall systems. Furthermore, machine learning directly\nanalyzes the raw wave functions, without relying on prior empirical theoretical\nassumptions and models, making it applicable to a broader range of fractional\nquantum Hall systems experiencing phase transitions due to particle-hole\nsymmetry breaking.",
        "Nano- to micro-sized particles with differently charged surface areas exhibit\ncomplex interaction patterns, characterized by both opposite-charge attraction\nand like-charge repulsion. While several successful models have been proposed\nin the literature to describe directional attraction, models accounting for\nboth directional attraction and directional repulsion are much less numerous\nand often tailored to specific microscopic systems. Here we present a simple\nand versatile patchy model, where the interaction energy of a pair of particles\nis a sum of interactions between sites of different types located within the\nparticle volume. We implement different formulations of this model in both a\nself-developed Monte Carlo code and the widely used LAMMPS Molecular Dynamics\nsimulation software, providing basic toolkits for both simulation methods and,\nin the latter case, for different algorithms. By comparing physical observables\nand code performances, we discuss the different models, methods, and\nalgorithms, offering insights into optimization strategies and tricks of trade.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Axions are one of the leading dark matter candidates. If we are embedded in a\nMilky Way dark matter halo comprised of axions, their stimulated decay would\nenable us to observe a counterimage (``axion gegenschein\") with a frequency\nequal to half the axion mass in the opposite direction of a bright radio\nsource. This spectral line emission will be broadened to $\\Delta \\nu\/\\nu \\sim\n\\sigma_d\/c \\sim 10^{-3}$ due to the velocity dispersion of dark matter,\n$\\sigma_d$. In this pilot study, we perform the first search for the expected\naxion gegenschein image of Vela supernova remnant (SNR) with 26.4 hours of\neffective ON-OFF data from the Five-hundred-meter Aperture Spherical radio\nTelescope (FAST) L-band (1.0 - 1.5~GHz) 19-beam receiver. Our null detection\nlimits the axion-photon coupling strength to be $g_{a\\gamma\\gamma} \\lesssim 2\n\\times 10^{-10} \\mathrm{GeV}^{-1}$ in the mass ranges of $8.7\\,\\mu\\mathrm{eV}\n\\leq m_a \\leq 9.44\\,\\mu\\mathrm{eV}$ and $10.85\\,\\mu\\mathrm{eV} \\leq m_a \\leq\n12.01\\,\\mu\\mathrm{eV} $. These results provide a stronger constraint on\n$g_{a\\gamma\\gamma}$ in this axion mass range than the current limits obtained\nby the direct search of axion decay signal from galaxy clusters which uses FAST\nobservations, but is a factor of $\\sim 3$ times weaker than the current CAST\nlimit.Based on our observation strategy, data processing methods, and results,\nthe expected sensitivity will reach $\\sim 10^{-11}\\mathrm{GeV}^{-1}$ with $\\sim\n2000$ hours of observation in the future.",
        "Longitudinal networks are becoming increasingly relevant in the study of\ndynamic processes characterised by known or inferred community structure.\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\nframework for exploiting the underlying network and multivariate time series.\nWe introduce the community-$\\alpha$ GNAR model with interactions that exploits\nprior knowledge or exogenous variables for analysing interactions within and\nbetween communities, and can describe serial correlation in longitudinal\nnetworks. We derive new explicit finite-sample error bounds that validate\nanalysing high-dimensional longitudinal network data with GNAR models, and\nprovide insights into their attractive properties. We further illustrate our\napproach by analysing the dynamics of $\\textit{Red, Blue}$ and $\\textit{Swing}$\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\na time series of length twelve on 51 time series (US states and Washington DC).\nOur analysis connects network autocorrelation to eight-year long terms,\nhighlights a possible change in the system after the 2016 election, and a\ndifference in behaviour between $\\textit{Red}$ and $\\textit{Blue}$ states."
      ]
    }
  },
  {
    "id":2411.06414,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"QEEGNet: Quantum Machine Learning for Enhanced Electroencephalography\n  Encoding",
    "start_abstract":"Electroencephalography (EEG) is a critical tool in neuroscience and clinical practice for monitoring analyzing brain activity. Traditional neural network models, such as EEGNet, have achieved considerable success decoding EEG signals but often struggle with the complexity high dimensionality of data. Recent advances quantum computing present new opportunities to enhance machine learning models through (QML) techniques. In this paper, we introduce Quantum-EEGNet (QEEGNet), novel hybrid that integrates classical EEGNet architecture improve encoding analysis, forward-looking approach, acknowledging results might not always surpass traditional methods it shows its potential. QEEGNet incorporates layers within network, allowing capture more intricate patterns data potentially offering computational advantages. We evaluate on benchmark dataset, BCI Competition IV 2a, demonstrating consistently outperforms most subjects other robustness noise. Our highlight significant potential quantum-enhanced networks suggesting directions both research practical applications field.",
    "start_categories":[
      "cs.RO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Braincomputer interfaces for communication and control"
      ],
      "abstract":[
        "For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world \u2013 a brain\u2013computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or \u2018locked in\u2019, with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10\u201325 bits\/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers. With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances."
      ],
      "categories":[
        "Neurophysiology"
      ]
    },
    "list":{
      "title":[
        "Calibration-free measurement of the phonon temperature around a single\n  emitter",
        "Stability, growth, and doping of In$_{2}$(Si, Ge)$_{2}$O$_{7}$ as\n  promising n-type wide-gap semiconductors",
        "Critical Dynamics of Spin Boson Model",
        "Modulation of the galactic cosmic ray spectrum in an anisotropic\n  diffusion approach",
        "Non(anti)Commutative Superspace, Baker-Campbell-Hausdorff Closed Forms,\n  and Dirac-K\\\"ahler Twisted Supersymmetry",
        "Quantifying the generation of negatively charged boron vacancies in\n  He-ion irradiated hexagonal boron nitride",
        "Quorum sensing and absorbing phase transitions in colloidal active\n  matter",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "An Unconventional Ultra-Sub-Wavelength Receiving Nano-Antenna Activated\n  by ac Spin Pumping and the ac Inverse Spin Hall Effect",
        "Structure-Preserving Neural Ordinary Differential Equations for Stiff\n  Systems",
        "Connectivity and matching extendability of optimal $1$-embedded graphs\n  on the torus",
        "Forcing, genericity and CBERS",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "(FAPP) Infinity Does Macroscopic Irreversibility From Microscopic\n  Reversibility",
        "SeqSee: A schema-based approach to spectral sequence visualization",
        "Multi-Modality Representation Learning for Antibody-Antigen Interactions\n  Prediction",
        "On the role of true and false chirality in producing parity violating\n  energy differences",
        "Miniaturized liquid metal composite circuits with energy harvesting\n  coils for battery-free bioelectronics and optogenetics",
        "Multideterminantal measures",
        "On the Mordell-Weil rank and $2$-Selmer group of a family of elliptic\n  curves",
        "Gradient-based Explanations for Deep Learning Survival Models",
        "Parameter Choices for Sparse Multi-Parameter Regularization with the\n  $\\ell_1$ Norm",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Influence of Chemistry and Topography on the Wettability of Copper",
        "Principal component analysis for 5\/2 fractional quantum Hall states",
        "Simulating inverse patchy colloid models",
        "Formalising the intentional stance 2: a coinductive approach",
        "Searching for axion dark matter gegenschein of the Vela supernova\n  remnant with FAST",
        "Generalized network autoregressive modelling of longitudinal networks\n  with application to presidential elections in the USA"
      ],
      "abstract":[
        "The emission properties of a localized solid-state emitter are strongly\ninfluenced by its environment. The coupling to acoustic phonons impacts the\ncoherence of the emitter and its temperature dependence, and also results in\nthe apparition of phonon sidebands besides the sharp zero-phonon line. Here, we\npresent a method for measuring the absolute temperature of a localized emitter\nby directly plotting the ratio of the Stokes and anti-Stokes components of the\nphonon sideband as a function of the shift from the zero-phonon line. This\napproach requires no calibration and knowledge of the system, making it\napplicable to a wide range of emitters and materials. We validate the method\nusing a CdSe quantum dot in a ZnSe nanowire. We thus show that the quantum dot\nis significantly heated under non-resonant excitation when increasing the\nincident power at low temperature and is ascribed to the drop in thermal\nconductivity at these temperatures.",
        "In this paper we investigate, computationally and experimentally, the phase\nstability, electronic structure properties, and the propensity for n-type\ndoping of In$_{2}$X$_{2}$O$_{7}$ (X=Si, Ge) ternary oxides. This family of\nmaterials contains promising novel wide-gap semiconductors based on their\nestimated high $n$-type Baliga figures of merit and acceptable thermal\nconductivity for power electronics applications. Here, we find that both\nIn$_{2}$Si$_{2}$O$_{7}$ and In$_{2}$Ge$_{2}$O$_{7}$ to be n-type dopable, with\nZr providing between 10$^{16}$ and above 10$^{21}$ cm$^{-3}$ net donor\nconcentrations under O-poor conditions, depending on the chemistry, structure\n(ground-state thorvetite or high-pressure pyrochlore) and synthesis\ntemperature. Initial thin-film growth and annealing leads to polycrystalline\nIn$_{2}$Ge$_{2}$O$_{7}$ thin films in thorvetite structure with band gap over 4\neV, and confirms Zr doping predictions by achieving electron concentrations at\n10$^{14}$-10$^{16}$ cm$^{-3}$ under O-rich condition. While future epitaxial\ngrowth development is still needed, this study establishes\nIn$_{2}$X$_{2}$O$_{7}$ as promising n-type wide-gap semiconductors for power\nelectronic applications.",
        "In this work, we study the low-energy properties of the spin-boson model\n(SBM), which describes the dynamics of a 1\/2 spin associated with a thermostat\ncharacterized by a power law spectral density, $f(\\omega)\\propto |\\omega|^s$.\nThe theoretical description is constructed in the Schwinger--Keldysh technique,\nbased on the representation of the 1\/2-spin by Majorana fermions. We study the\ncritical dynamics of the system near the quantum phase transition by\nconstructing and analyzing the system of renormalization group equations. Our\ntheoretical approach is more universal, contrary to the one based on quantum\nclassical mapping, since it is applicable for $0<s\\leq 1$. We show that in both\nthe ohmic case $s=1$, and subohmic case $0<s<1$, the second order quantum phase\ntransition is observed in the model considered, and the critical magnetization\nexponent agrees with the exact hyperscaling result, $1\/\\delta=(1-s)\/(1+s)$.\nFurthermore, we obtain the dependence of the critical value of the spin-boson\ncoupling constant on the temperature of the bosonic thermal bath.",
        "We introduce a novel diffusion model for the propagation of cosmic rays (CRs)\nthat incorporates an anisotropic diffusion tensor of a general form within a\nrealistically modeled large-scale Galactic magnetic field. The parameters of\nthe model are consistent with the contemporary understanding of the large-scale\nGalactic magnetic field structure and the dynamics of small-scale turbulent CR\npropagation. The paper demonstrates the modulation of spectra of Galactic\ncosmic rays (GCRs) in the magnetic rigidity range of 1 - 30 PV (the CR knee)\nand explores the spatial variation of this phenomenon. The observed modulation\nof the spectrum is explained by changes in the leakage mechanism.",
        "Starting from an elementary calculation of super Lie group elements\nassociating with non(anti)-commutative Grassmann parameters, we derive several\nclosed expressions of Baker-Campbell-Hausdorff (BCH) formula which represent\nmultiplication properties of super Lie group elements in the corresponding\nsuperspace. We then show that parametrization of superspace in general may\nbecome infinite dimensional due to the presence of non(anti)commutativity. We\nshow that a Dirac-K\\\"ahler Twisted SUSY Algebra (also referred to as Marcus\nB-type Twisted SUSY Algebra or Geometric Langlands Twisted SUSY Algebra) with a\ncertain type of deformation, which we call an exponential deformation, may\ncircumvent this problem. We also provide, in terms of gauge covariantization of\nthe SUSY algebra, a geometric understanding of the exponential deformation, and\nsee that the framework constructed in this paper may serve as a\nnon(anti)commutative superspace framework providing the gauge covariant link\nformulation of twisted super Yang-Mills on a lattice.",
        "Hexagonal boron nitride (hBN) hosts luminescent defects possessing spin\nqualities compatible with quantum sensing protocols at room temperature.\nVacancies, in particular, are readily obtained via exposure to high-energy ion\nbeams. While the defect creation mechanism via such irradiation is well\nunderstood, the occurrence rate of optically active negatively charged\nvacancies ($V_B^-$) is an open question. In this work, we exploit focused\nhelium ions to systematically generate optically active vacancy defects in hBN\nflakes at varying density. By comparing the density-dependent spin splitting\nmeasured by magnetic resonance to calculations based on a microscopic charge\nmodel, in which we introduce a correction term due to a constant background\ncharge, we are able to quantify the number of $V_B^-$ defects generated by the\nion irradiation. We find that only a small fraction (0.2%) of all vacancies is\nin the optically active, negatively charged state. Our results provide a\nprotocol for measuring the generation efficiency of $V_B^-$, which is necessary\nfor understanding and optimizing luminescent centers in hBN.",
        "Unlike biological active matter that constantly adapt to their environment,\nthe motors of synthetic active particles are typically agnostic to their\nsurroundings and merely operate at constant force. Here, we design colloidal\nactive rods capable of modulating their inner activity in response to crowding,\nthereby enforcing a primitive form of quorum sensing interactions. Through\nexperiments, simulations and theory we elucidate the impact of these\ninteractions on the phase behavior of isotropic active matter. We demonstrate\nthat, when conditioned to density, motility regulation can either lead to an\nabsorbing phase transition, where all particles freeze their dynamics, or to\natypical phase separation, where flat interfaces supporting a net pressure drop\nare in mechanical equilibrium. Fully active and fully arrested particles can\nthen form heterogeneous patterns ruled by the competition between quorum\nsensing and mechanical interactions. Beyond the specifics of motile colloids,\nwe expect our findings to apply broadly to adaptive active matter assembled\nfrom living or synthetic units.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "We report an extreme sub-wavelength unconventional receiving antenna. It\nconsists of an array of nanomagnets connected to heavy metal nanostrips.\nIncident electromagnetic (EM) radiation generates intrinsic and extrinsic spin\nwaves in the nanomagnets, which pump spin into the heavy metal nanostrips at\ntheir own frequencies giving rise to a polychromatic alternating voltage across\nthe latter owing to the ac inverse spin Hall effect. This implements a\nreceiving nano-antenna. We demonstrate its operation at two different EM wave\nfrequencies of 1.5 GHz and 2.4 GHz - the latter being the Bluetooth and Wi-Fi\nfrequency. We measure the receiving gain at 2.4 GHz to be approximately -9 db.\nThe free space radiated wavelength \"lambda\" at 2.4 GHz is 12.5 cm while the\nantenna area A is merely 160 micron^2, making the ratio A\/lambda^2 =\n0.97x10^-8. This antenna's receiving gain should be very poor because of the\ntiny size. Yet the measured gain is more than 4000 times larger than the\ntheoretical limit for a conventional antenna of this size at this wavelength\nbecause of the unconventional operating principle.",
        "Neural ordinary differential equations (NODEs) are an effective approach for\ndata-based modeling of dynamical systems arising from simulations and\nexperiments. One of the major shortcomings of NODEs, especially when coupled\nwith explicit integrators, is its long-term stability, which impedes their\nefficiency and robustness when encountering stiff problems. In this work we\npresent a structure-preserving NODE approach, which integrates with a linear\nand nonlinear split and an exponential integrator, latter of which is an\nexplicit integrator with stability properties comparable to implicit methods.\nWe demonstrate that our model has advantages in both learning and deployment\nover standard explicit or even implicit NODE methods. The long-time stability\nis further enhanced by the Hurwitz matrix decomposition that constrains the\nspectrum of the linear operator, therefore stabilizing the linearized dynamics.\nWhen combined with a Lipschitz-controlled neural network treatment for the\nnonlinear operator, we show the nonlinear dynamics of the NODE are provably\nstable in the sense of Lyapunov. For high-dimensional data, we further rely on\nan autoencoder performing dimension reduction and Higham's algorithm for the\nmatrix-free application of the matrix exponential on a vector. We demonstrate\nthe effectiveness of the proposed NODE approach in various examples, including\nthe Grad-13 moment equations and the Kuramoto-Sivashinky equation.",
        "In this paper, we discuss optimal $1$-toroidal graphs (abbreviated as O1TG),\nwhich are drawn on the torus so that every edge crosses another edge at most\nonce, and has $n$ vertices and exactly $4n$ edges. We first consider\nconnectivity of O1TGs, and give the characterization of O1TGs having\nconnectivity exactly $k$ for each $k\\in \\{4, 5, 6, 8\\}$. In our argument, we\nalso show that there exists no O1TG having connectivity exactly $7$.\nFurthermore, using the result above, we discuss extendability of matchings, and\ngive the characterization of $1$-, $2$- and $3$-extendable O1TGs in turn.",
        "In this paper we continue the study of equivalence of generics filters\nstarted by Smythe in [Smy22]. We fully characterize those forcing posets for\nwhich the corresponding equivalence of generics is smooth using the purely\ntopological property of condensation. Next we leverage our characterization to\nshow that there are non-homogeneous forcing for which equivalence of generics\nis not smooth. Then we prove hyperfiniteness in the case of Prikry forcing and\nsome additional results addressing the problem whether generic equivalence for\nCohen forcing is hyperfinite.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "Infinity is central to deriving macroscopic irreversibility from reversible\nmicroscopic laws across mathematics, theoretical computer science and physics.\nIn analysis, infinite processes -- such as Dedekind cuts and Cauchy sequences\n-- construct real numbers as equivalence classes of rational approximations,\nbridging discrete rationals to the continuous real line. In quantum mechanics,\ninfinite tensor products model nested measurements, where sectorization\npartitions the Hilbert space into equivalence classes, reconciling unitary\nevolution with wavefunction collapse. In statistical mechanics, macrostates\nemerge as equivalence classes of microstates sharing identical macroscopic\nproperties, providing the statistical basis for thermodynamic irreversibility\ndespite reversible dynamics. Equivalence relations formalize\nFor-All-Practical-Purposes (FAPP) indistinguishability, reflecting operational\nlimits on precision and observation. Together, these examples reveal a unified\nframework where infinity and equivalence underpin emergent macroscopic behavior\nfrom microscopic reversibility.",
        "We present SeqSee, a software system that addresses spectral sequence\nvisualization through a schema-based approach. By introducing a standardized\nJSON schema as an intermediate representation, SeqSee decouples the\nmathematical computations of spectral sequences from their visualizations. We\ndemonstrate the system through a case study of the classical and C-motivic\nAdams spectral sequences.",
        "While deep learning models play a crucial role in predicting antibody-antigen\ninteractions (AAI), the scarcity of publicly available sequence-structure\npairings constrains their generalization. Current AAI methods often focus on\nresidue-level static details, overlooking fine-grained structural\nrepresentations of antibodies and their inter-antibody similarities. To tackle\nthis challenge, we introduce a multi-modality representation approach that\nintegates 3D structural and 1D sequence data to unravel intricate\nintra-antibody hierarchical relationships. By harnessing these representations,\nwe present MuLAAIP, an AAI prediction framework that utilizes graph attention\nnetworks to illuminate graph-level structural features and normalized adaptive\ngraph convolution networks to capture inter-antibody sequence associations.\nFurthermore, we have curated an AAI benchmark dataset comprising both\nstructural and sequence information along with interaction labels. Through\nextensive experiments on this benchmark, our results demonstrate that MuLAAIP\noutperforms current state-of-the-art methods in terms of predictive\nperformance. The implementation code and dataset are publicly available at\nhttps:\/\/github.com\/trashTian\/MuLAAIP for reproducibility.",
        "In this work we tackle the problem of showing which type of influences can\nlift the degeneracy between truly and falsely chiral systems, showing that only\nwhen both systems and influences are both truly (falsely) chiral, a parity\nviolating energy difference between left- and right-handed systems can be\nproduced. In particular, after considering the enantiomers of a chiral molecule\nas paradigmatic truly chiral systems, we rigorously show, under a quantum field\ntheoretically approach, that only a truly chiral influence such as the\n$Z^{0}$-mediated electroweak interaction can lift the degeneracy between\nenantiomers. On the contrary, we explicitly show that a falsely chiral\ninfluence, such as an axion-mediated interaction in chiral molecules, can not\nlift the aforementioned degeneracy. These results extend Barron's seminal ideas\n[L. D. Barron, True and false chirality and parity violation, Chem. Phys. Lett\n{\\bf 123}, 423 (1986)] to a quantum field theory-based approach.",
        "Over the past years, rapid progress has been made on soft-matter electronics\nfor wearable and implantable devices, for bioelectronics and optogenetics.\nLiquid Metal (LM) based electronics were especially popular, due to their\nlong-term durability, when subject to repetitive strain cycles. However, one\nmajor limitation has been the need for tethering bioelectronics circuits to\nexternal power, or the use of rigid bulky batteries. This has motivated a\ngrowing interest in wireless energy transfer, which demands circuit\nminiaturization. However, miniaturization of LM circuits is challenging due to\nlow LM-substrate adhesion, LM smearing, and challenges on\nmicrochip-interfacing. In this article, we address these challenges by\nhigh-resolution laser-assisted micropatterning of biphasic LM composites and\nvapor-assisted LM microchip soldering. Through development of a search\nalgorithm for optimization of the biphasic ink coil performance, we designed\nand implemented micro coils with trace spacing of 50 {\\mu}m that can harvest a\nsignificant amount of energy (178 mW\/cm2) through near field inductive\ncoupling. We show miniaturized soft-matter circuits with integrated SMD chips\nsuch as NFC chips, capacitors, and LEDs that are implemented in a few minutes\nthrough laser patterning, and vaporassisted soldering. In the context of\noptogenetics, where lightweight, miniaturized systems are needed to provide\noptical stimulation, soft coils stand out in terms of their improved\nconformability and flexibility. Thus, this article explores the applications of\nsoft coils in wearable and implantable devices, with a specific focus on their\nuse in optogenetics.",
        "We define multideterminantal probability measures, a family of probability\nmeasures on $[k]^n$ where $[k]=\\{1,2,\\dots,k\\}$, generalizing determinantal\nmeasures (which correspond to the case $k=2$). We give examples coming from the\npositive Grassmannian, from the dimer model and from the spanning tree model.\n  We also define and completely characterize determinantal probability measures\non the permutation group $S_n$.",
        "We consider the parametric family of elliptic curves over $\\mathbb{Q}$ of the\nform $E_{m} : y^{2} = x(x - n_{1})(x - n_{2}) + t^{2}$, where $n_{1}$, $n_{2}$\nand $t$ are particular polynomial expressions in an integral variable $m$. In\nthis paper, we investigate the torsion group $E_{m}(\\mathbb{Q})_{\\rm{tors}}$, a\nlower bound for the Mordell-Weil rank $r({E_{m}})$ and the $2$-Selmer group\n${\\rm{Sel}}_{2}(E_{m})$ under certain conditions on $m$. This extends the\nprevious works done in this direction, which are mostly concerned with the\nMordell-Weil ranks of various parametric families of elliptic curves.",
        "Deep learning survival models often outperform classical methods in\ntime-to-event predictions, particularly in personalized medicine, but their\n\"black box\" nature hinders broader adoption. We propose a framework for\ngradient-based explanation methods tailored to survival neural networks,\nextending their use beyond regression and classification. We analyze the\nimplications of their theoretical assumptions for time-dependent explanations\nin the survival setting and propose effective visualizations incorporating the\ntemporal dimension. Experiments on synthetic data show that gradient-based\nmethods capture the magnitude and direction of local and global feature\neffects, including time dependencies. We introduce GradSHAP(t), a\ngradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and\nSurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply\nthese methods to medical data with multi-modal inputs, revealing relevant\ntabular features and visual patterns, as well as their temporal dynamics.",
        "This paper introduces a multi-parameter regularization approach using the\n$\\ell_1$ norm, designed to better adapt to complex data structures and problem\ncharacteristics while offering enhanced flexibility in promoting sparsity in\nregularized solutions. As data volumes grow, sparse representations of learned\nfunctions become critical for reducing computational costs during function\noperations. We investigate how the selection of multiple regularization\nparameters influences the sparsity of regularized solutions. Specifically, we\ncharacterize the relationship between these parameters and the sparsity of\nsolutions under transform matrices, enabling the development of an iterative\nscheme for selecting parameters that achieve prescribed sparsity levels.\nSpecial attention is given to scenarios where the fidelity term is\nnon-differentiable, and the transform matrix lacks full row rank. In such\ncases, the regularized solution, along with two auxiliary vectors arising in\nthe sparsity characterization, are essential components of the multi-parameter\nselection strategy. To address this, we propose a fixed-point proximity\nalgorithm that simultaneously determines these three vectors. This algorithm,\ncombined with our sparsity characterization, forms the basis of a practical\nmulti-parameter selection strategy. Numerical experiments demonstrate the\neffectiveness of the proposed approach, yielding regularized solutions with\nboth predetermined sparsity levels and satisfactory approximation accuracy.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "To understand the complex interplay of topography and surface chemistry in\nwetting, fundamental studies investigating both parameters are needed. Due to\nthe sensitivity of wetting to miniscule changes in one of the parameters it is\nimperative to precisely control the experimental approach. A profound\nunderstanding of their influence on wetting facilitates a tailored design of\nsurfaces with unique functionality. We present a multi-step study: The\ninfluence of surface chemistry is analyzed by determining the adsorption of\nvolatile carbonous species (A) and by sputter deposition of metallic copper and\ncopper oxides on flat copper substrates (B). A precise surface topography is\ncreated by laser processing. Isotropic topography is created by ps laser\nprocessing (C), and hierarchical anisotropic line patterns are produced by\ndirect laser interference patterning (DLIP) with different pulse durations (D).\nOur results reveal that the long-term wetting response of polished copper\nsurfaces stabilizes with time despite ongoing accumulation of hydrocarbons and\nis dominated by this adsorption layer over the oxide state of the substrate\n(Cu, CuO, Cu2O). The surfaces' wetting response can be precisely tuned by\ntailoring the topography via laser processing. The sub-pattern morphology of\nprimary line-like patterns showed great impact on the static contact angle,\nwetting anisotropy, and water adhesion. An increased roughness inside the\npattern valleys combined with a minor roughness on the peaks favors\nair-inclusions, isotropic hydrophobicity, and low water adhesion. Increasing\nthe aspect ratio showed to enhance air-inclusions and hydrophobicity despite\nincreased peak roughness while time dependent wetting transitions were\nobserved.",
        "For the special single-layer fractional quantum Hall system with a filling\nfactor of 5\/2, which has an even denominator, this paper uses principal\ncomponent analysis (PCA) to study its behavior under the breaking of\nparticle-hole symmetry. By introducing a model three-body potential to\nrepresent the mechanism of particle-hole symmetry breaking, the paper finds\nthat the 5\/2 system evolves into two types of special topological quantum\nstates with non-Abelian statistics as the strength and direction of the\nthree-body potential vary. The transition points of these states correspond to\nthe particle-hole symmetric pure Coulomb interaction system. Our results\nvalidate the applicability of machine learning as a new research tool in\nfractional quantum Hall systems. Furthermore, machine learning directly\nanalyzes the raw wave functions, without relying on prior empirical theoretical\nassumptions and models, making it applicable to a broader range of fractional\nquantum Hall systems experiencing phase transitions due to particle-hole\nsymmetry breaking.",
        "Nano- to micro-sized particles with differently charged surface areas exhibit\ncomplex interaction patterns, characterized by both opposite-charge attraction\nand like-charge repulsion. While several successful models have been proposed\nin the literature to describe directional attraction, models accounting for\nboth directional attraction and directional repulsion are much less numerous\nand often tailored to specific microscopic systems. Here we present a simple\nand versatile patchy model, where the interaction energy of a pair of particles\nis a sum of interactions between sites of different types located within the\nparticle volume. We implement different formulations of this model in both a\nself-developed Monte Carlo code and the widely used LAMMPS Molecular Dynamics\nsimulation software, providing basic toolkits for both simulation methods and,\nin the latter case, for different algorithms. By comparing physical observables\nand code performances, we discuss the different models, methods, and\nalgorithms, offering insights into optimization strategies and tricks of trade.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Axions are one of the leading dark matter candidates. If we are embedded in a\nMilky Way dark matter halo comprised of axions, their stimulated decay would\nenable us to observe a counterimage (``axion gegenschein\") with a frequency\nequal to half the axion mass in the opposite direction of a bright radio\nsource. This spectral line emission will be broadened to $\\Delta \\nu\/\\nu \\sim\n\\sigma_d\/c \\sim 10^{-3}$ due to the velocity dispersion of dark matter,\n$\\sigma_d$. In this pilot study, we perform the first search for the expected\naxion gegenschein image of Vela supernova remnant (SNR) with 26.4 hours of\neffective ON-OFF data from the Five-hundred-meter Aperture Spherical radio\nTelescope (FAST) L-band (1.0 - 1.5~GHz) 19-beam receiver. Our null detection\nlimits the axion-photon coupling strength to be $g_{a\\gamma\\gamma} \\lesssim 2\n\\times 10^{-10} \\mathrm{GeV}^{-1}$ in the mass ranges of $8.7\\,\\mu\\mathrm{eV}\n\\leq m_a \\leq 9.44\\,\\mu\\mathrm{eV}$ and $10.85\\,\\mu\\mathrm{eV} \\leq m_a \\leq\n12.01\\,\\mu\\mathrm{eV} $. These results provide a stronger constraint on\n$g_{a\\gamma\\gamma}$ in this axion mass range than the current limits obtained\nby the direct search of axion decay signal from galaxy clusters which uses FAST\nobservations, but is a factor of $\\sim 3$ times weaker than the current CAST\nlimit.Based on our observation strategy, data processing methods, and results,\nthe expected sensitivity will reach $\\sim 10^{-11}\\mathrm{GeV}^{-1}$ with $\\sim\n2000$ hours of observation in the future.",
        "Longitudinal networks are becoming increasingly relevant in the study of\ndynamic processes characterised by known or inferred community structure.\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\nframework for exploiting the underlying network and multivariate time series.\nWe introduce the community-$\\alpha$ GNAR model with interactions that exploits\nprior knowledge or exogenous variables for analysing interactions within and\nbetween communities, and can describe serial correlation in longitudinal\nnetworks. We derive new explicit finite-sample error bounds that validate\nanalysing high-dimensional longitudinal network data with GNAR models, and\nprovide insights into their attractive properties. We further illustrate our\napproach by analysing the dynamics of $\\textit{Red, Blue}$ and $\\textit{Swing}$\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\na time series of length twelve on 51 time series (US states and Washington DC).\nOur analysis connects network autocorrelation to eight-year long terms,\nhighlights a possible change in the system after the 2016 election, and a\ndifference in behaviour between $\\textit{Red}$ and $\\textit{Blue}$ states."
      ]
    }
  },
  {
    "id":2411.06414,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Braincomputer interfaces for communication and control",
    "start_abstract":"For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world \u2013 a brain\u2013computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or \u2018locked in\u2019, with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10\u201325 bits\/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers. With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances.",
    "start_categories":[
      "Neurophysiology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b1",
        "b9"
      ],
      "title":[
        "An EEG-based brain-computer interface for real-time multi-task robotic control",
        "QEEGNet: Quantum Machine Learning for Enhanced Electroencephalography\n  Encoding"
      ],
      "abstract":[
        "The Brain Computer Interface (BCI) is the communication between human brain and computer. Electroencephalogram (EEG) one of biomedical signals which can be obtained by attaching electrodes to scalp. Some EEG related applications developed help disabled people, such as based wheelchair or robotic arm. A hybrid BCI real-time control system proposed a multi-tasks robot. In this system, sliding window online data segmentation strategy segment training data, enable learn dynamic features when subject's state transfer from rest task execution state. achieve ensure continuity executing actions. addition, Common Spatial Pattern (CSP) better extract spatial these continuous actions that multiple commands are accurately classified. experiment, three subjects' collected, trained tested performance reliability system. records robot's spending time, moving distance, number objects pushing down. Experimental results given show feasibility Compared remote controller, similar performance. Thus, able robot in environment used develop robot-aided arm methods on neurological rehabilitation principles for stroke injury patients.",
        "Electroencephalography (EEG) is a critical tool in neuroscience and clinical practice for monitoring analyzing brain activity. Traditional neural network models, such as EEGNet, have achieved considerable success decoding EEG signals but often struggle with the complexity high dimensionality of data. Recent advances quantum computing present new opportunities to enhance machine learning models through (QML) techniques. In this paper, we introduce Quantum-EEGNet (QEEGNet), novel hybrid that integrates classical EEGNet architecture improve encoding analysis, forward-looking approach, acknowledging results might not always surpass traditional methods it shows its potential. QEEGNet incorporates layers within network, allowing capture more intricate patterns data potentially offering computational advantages. We evaluate on benchmark dataset, BCI Competition IV 2a, demonstrating consistently outperforms most subjects other robustness noise. Our highlight significant potential quantum-enhanced networks suggesting directions both research practical applications field."
      ],
      "categories":[
        "cs.RO",
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "A Scalable and Robust Compilation Framework for Emitter-Photonic Graph\n  State",
        "Environmental Influences on Collaboration Network Evolution: A\n  Historical Analysis",
        "Graphs of unbounded linear cliquewidth must transduce all trees",
        "Dynamic Bragg microcavities in collisions of unipolar light pulses of\n  unusual shape in two- and three-level medium",
        "Global Lipschitz and Sobolev estimates for the Monge-Amp\\`ere\n  eigenfunctions of general bounded convex domains",
        "Learning to Retrieve and Reason on Knowledge Graph through Active\n  Self-Reflection",
        "Stress energy momentum in terms of geodesic accelerations and\n  variational tensors including torsion",
        "Relations amongst the distances between $C^{*}$-subalgebras and some\n  canonically associated operator algebras",
        "A posteriori error control for a finite volume scheme for a\n  cross-diffusion model of ion transport",
        "MaSS13K: A Matting-level Semantic Segmentation Benchmark",
        "Nuclear Structure Properties and Stellar Weak Rates for 76Se: Unblocking\n  of the Gamow Teller Strength",
        "Active 6D Pose Estimation for Textureless Objects using Multi-View RGB\n  Frames",
        "Aspects of Artificial Intelligence: Transforming Machine Learning\n  Systems Naturally",
        "Micromotion compensation using dark and bright ions",
        "Connection points on double regular polygons",
        "Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video\n  Understanding",
        "Functional Reactive Programming with Effects, A More Permissive Approach",
        "White's conjecture for matroids and inner projections",
        "Incidence equivalence and the Bloch-Beilinson filtration",
        "Nonperturbative Open Quantum Dynamics Bypass Influence Functional",
        "Improving Medical Waste Classification with Hybrid Capsule Networks",
        "Athermal creep deformation of ultrastable amorphous solids",
        "Convergence analysis of linearized $\\ell_q$ penalty methods for\n  nonconvex optimization with nonlinear equality constraints",
        "OrbID: Identifying Orbcomm Satellite RF Fingerprints",
        "TEMPO: A Python Package for Time Evolution of Pulse Sequences in QuTiP",
        "PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning\n  with Noisy Demonstrations",
        "Ultra-Low-Latency Edge Intelligent Sensing: A Source-Channel Tradeoff\n  and Its Application to Coding Rate Adaptation",
        "Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and\n  LLM-based Code Generation"
      ],
      "abstract":[
        "Quantum graph states are critical resources for various quantum algorithms,\nand also determine essential interconnections in distributed quantum computing.\nThere are two schemes for generating graph states probabilistic scheme and\ndeterministic scheme. While the all-photonic probabilistic scheme has garnered\nsignificant attention, the emitter-photonic deterministic scheme has been\nproved to be more scalable and feasible across several hardware platforms.\n  This paper studies the GraphState-to-Circuit compilation problem in the\ncontext of the deterministic scheme. Previous research has primarily focused on\noptimizing individual circuit parameters, often neglecting the characteristics\nof quantum hardware, which results in impractical implementations.\nAdditionally, existing algorithms lack scalability for larger graph sizes. To\nbridge these gaps, we propose a novel compilation framework that partitions the\ntarget graph state into subgraphs, compiles them individually, and subsequently\ncombines and schedules the circuits to maximize emitter resource utilization.\nFurthermore, we incorporate local complementation to transform graph states and\nminimize entanglement overhead. Evaluation of our framework on various graph\ntypes demonstrates significant reductions in CNOT gates and circuit duration,\nup to 52% and 56%. Moreover, it enhances the suppression of photon loss,\nachieving improvements of up to x1.9.",
        "We analysed two large collaboration networks -- the Microsoft Academic Graph\n(1800-2020) and Internet Movie Database (1900-2020) -- to quantify network\nresponses to major historical events. Our analysis revealed four properties of\nnetwork-environment interaction. First, historical events can influence network\nevolution, with effects persisting far longer than previously recognised; the\nacademic network showed 45\\% declines during World Wars and 90\\% growth during\nLa Belle Epoque. Second, node and edge processes exhibited different\nenvironmental sensitivities; while node addition\/removal tracked historical\nevents, edge formation maintained stable statistical properties even during\nmajor disruptions. Third, different collaboration networks showed distinct\nresponse patterns; academic networks displayed sharp disruptions and rapid\nrecoveries, while entertainment networks showed gradual changes and greater\nresilience. Fourth, both networks developed increasing resilience. Our results\nprovide new insights for modelling network evolution and managing collaborative\nsystems during periods of external disruption.",
        "The Pathwidth Theorem states that if a class of graphs has unbounded\npathwidth, then it contains all trees as graph minors. We prove a similar\nresult for dense graphs: if a class of graphs has unbounded linear cliquewidth,\nthen it can produce all trees via some fixed CMSO transduction.",
        "Unipolar light pulses with a non-zero electric area due to the unidirectional\naction on charged particles can be used for the ultrafast control of the\nproperties of quantum systems. To control atomic properties in an efficient\nway, it is necessary to vary the temporal shape of the pulses used. This has\nled to the problem of obtaining pulses of an unusual shape, such as a\nrectangular one. A number of new phenomena, not possible with conventional\nmulti-cycle pulses, were discovered by analyzing the interaction of such\nunipolar pulses with matter. These include the formation of dynamic\nmicrocavities at each resonant transition of a multilevel medium when such\npulses collide with the medium. In this work, we compare the behavior of\ndynamic microcavities in a two-level and a three-level medium when unipolar\npulses of unusual shape (rectangular) are collided with the medium. We do this\non the basis of the numerical solution of the system for the density matrix of\nthe medium and the wave equation for the electric field. Medium parameters\ncorrespond to atomic hydrogen. It is shown that for rectangular pulses in a\nthree-level medium, the dynamics of the cavities can be very different from the\ntwo-level model, as opposed to pulses of other shapes (e.g. Gaussian shape).\nWhen the third level of the medium is taken into account, the self-induced\ntransparency-like regime disappears. Differences in the dynamics of resonators\nin a three-level medium are revealed when the pulses behave like 2{\\pi} pulses\nof self-induced transparency.",
        "We show that the Monge-Amp\\`ere eigenfunctions of general bounded convex\ndomains are globally Lipschitz. The same result holds for convex solutions to\ndegenerate Monge-Amp\\`ere equations of the form $\\det D^2 u =M|u|^p$ with zero\nboundary condition on general bounded convex domains in ${\\mathbb R}^n$ within\nthe sharp threshold $p>n-2$. As a consequence, we obtain global $W^{2, 1}$\nestimates for these solutions.",
        "Extensive research has investigated the integration of large language models\n(LLMs) with knowledge graphs to enhance the reasoning process. However,\nunderstanding how models perform reasoning utilizing structured graph knowledge\nremains underexplored. Most existing approaches rely on LLMs or retrievers to\nmake binary judgments regarding the utilization of knowledge, which is too\ncoarse. Meanwhile, there is still a lack of feedback mechanisms for reflection\nand correction throughout the entire reasoning path. This paper proposes an\nActive self-Reflection framework for knowledge Graph reasoning ARG, introducing\nfor the first time an end-to-end training approach to achieve iterative\nreasoning grounded on structured graphs. Within the framework, the model\nleverages special tokens to \\textit{actively} determine whether knowledge\nretrieval is necessary, performs \\textit{reflective} critique based on the\nretrieved knowledge, and iteratively reasons over the knowledge graph. The\nreasoning paths generated by the model exhibit high interpretability, enabling\ndeeper exploration of the model's understanding of structured knowledge.\nUltimately, the proposed model achieves outstanding results compared to\nexisting baselines in knowledge graph reasoning tasks.",
        "General relativity and its extensions including torsion identify stress\nenergy momentum as being proportional to the Einstein tensor, thus ensuring\nboth symmetry and conservation. Here we visualize stress energy and momentum by\nidentifying the associated relative fractional accelerations of geodesics\nencoded in the Einstein tensor. This also provides an intuitive explanation for\nthe vanishing divergence of the Einstein tensor. In order to obtain this same\nenergy and momentum for other actions such as that of Dirac theory including\ntorsion, we then review the various stress energy momentum tensors resulting\nfrom the variation of different quantities derived from parallel transport, and\ndetail their interrelationships. This provides an opportunity to revisit some\nclassic material from a geometric point of view, including Einstein-Cartan\ntheory, the Sciama-Kibble formalism, and the Belinfante-Rosenfeld relation,\nwhose derivation in the mostly pluses signature would seem to not be otherwise\nreadily available.",
        "We prove that the Christensen distance (resp., the Kadison-Kastler distance)\nbetween two $C^*$-subalgebras $\\mathcal{A}$ and $\\mathcal{B}$ of a\n$C^*$-algebra $\\mathcal{C}$ is equal to that between their enveloping von\nNeumann algebras $\\mathcal{A}^{**}$ and $\\mathcal{B}^{**}$ (resp., the tensor\nproduct algebras $\\mathcal{A} \\otimes^{\\min} \\mathcal{D}$ and $\\mathcal{B}\n\\otimes^{\\min} \\mathcal{D}$, for any unital commutative $C^*$-algebra\n$\\mathcal{D}$).",
        "We derive a reliable a posteriori error estimate for a cell-centered finite\nvolume scheme approximating a cross-diffusion system modeling ion transport\nthrough nanopores. To this end we derive an abstract stability framework that\nis independent of the numerical scheme and introduce a suitable (conforming)\nreconstruction of the numerical solution. The stability framework relies on\nsome simplifying assumption that coincide with those made in weak uniqueness\nresults for this system. This is the first a posteriori error estimate for a\ncross-diffusion system. Along the way, we derive a pointwise a posteriori error\nestimate for a finite volume scheme approximating the diffusion equation. We\nconduct numerical experiments showing that the error estimator scales with the\nsame order as the true error.",
        "High-resolution semantic segmentation is essential for applications such as\nimage editing, bokeh imaging, AR\/VR, etc. Unfortunately, existing datasets\noften have limited resolution and lack precise mask details and boundaries. In\nthis work, we build a large-scale, matting-level semantic segmentation dataset,\nnamed MaSS13K, which consists of 13,348 real-world images, all at 4K\nresolution. MaSS13K provides high-quality mask annotations of a number of\nobjects, which are categorized into seven categories: human, vegetation,\nground, sky, water, building, and others. MaSS13K features precise masks, with\nan average mask complexity 20-50 times higher than existing semantic\nsegmentation datasets. We consequently present a method specifically designed\nfor high-resolution semantic segmentation, namely MaSSFormer, which employs an\nefficient pixel decoder that aggregates high-level semantic features and\nlow-level texture features across three stages, aiming to produce\nhigh-resolution masks with minimal computational cost. Finally, we propose a\nnew learning paradigm, which integrates the high-quality masks of the seven\ngiven categories with pseudo labels from new classes, enabling MaSSFormer to\ntransfer its accurate segmentation capability to other classes of objects. Our\nproposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark\ntogether with 14 representative segmentation models. We expect that our\nmeticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate\nthe research of high-resolution and high-quality semantic segmentation.\nDatasets and codes can be found at https:\/\/github.com\/xiechenxi99\/MaSS13K.",
        "At finite temperatures ($\\geq 10^7$K), $^{76}$Se is abundant in the core of\nmassive stars and electron capture on $^{76}$Se has a consequential role to\nplay in the dynamics of core collapse. The present work may be classified into\ntwo main categories. In the first phase, we study the nuclear structure\nproperties of $^{76}$Se using the interacting boson model-1 (IBM-1). The IBM-1\ninvestigations include the energy levels, $B(E2)$ values, and the prediction of\nthe geometry. We performed the extended consistent-Q formalism (ECQF)\ncalculation and later the triaxial formalism calculation (constructed by adding\nthe cubic term to the ECQF). The geometry of $^{76}$Se can be envisioned within\nthe formalism of the potential energy surface based on the classical limit of\nthe IBM-1 model.\n  In the second phase, we reconfirm the unblocking of the Gamow-Teller (GT)\nstrength in $^{76}$Se (a test case for nuclei having $N > 40$ and $Z < 40$).\nUsing the deformed pn-QRPA model, we calculate GT transitions, stellar electron\ncapture cross section (within the limit of low momentum transfer), and stellar\nweak rates for $^{76}$Se. The distinguishing feature of our calculation is a\nstate-by-state evaluation of stellar weak rates in a fully microscopic fashion.\nResults are compared with experimental data and previous calculations. The\ncalculated GT distribution fulfills the Ikeda sum rule. Rates for\n$\\beta$-delayed neutrons and emission probabilities are also calculated. Our\nstudy suggests that at high stellar temperatures and low densities, the\n$\\beta^+$-decay on $^{76}$Se should not be neglected and needs to be taken into\nconsideration along with electron capture rates for simulation of presupernova\nevolution of massive stars.",
        "Estimating the 6D pose of textureless objects from RBG images is an important\nproblem in robotics. Due to appearance ambiguities, rotational symmetries, and\nsevere occlusions, single-view based 6D pose estimators are still unable to\nhandle a wide range of objects, motivating research towards multi-view pose\nestimation and next-best-view prediction that addresses these limitations. In\nthis work, we propose a comprehensive active perception framework for\nestimating the 6D poses of textureless objects using only RGB images. Our\napproach is built upon a key idea: decoupling the 6D pose estimation into a\nsequential two-step process can greatly improve both accuracy and efficiency.\nFirst, we estimate the 3D translation of each object, resolving scale and depth\nambiguities inherent to RGB images. These estimates are then used to simplify\nthe subsequent task of determining the 3D orientation, which we achieve through\ncanonical scale template matching. Building on this formulation, we then\nintroduce an active perception strategy that predicts the next best camera\nviewpoint to capture an RGB image, effectively reducing object pose uncertainty\nand enhancing pose accuracy. We evaluate our method on the public ROBI dataset\nas well as on a transparent object dataset that we created. When evaluated\nusing the same camera viewpoints, our multi-view pose estimation significantly\noutperforms state-of-the-art approaches. Furthermore, by leveraging our\nnext-best-view strategy, our method achieves high object pose accuracy with\nsubstantially fewer viewpoints than heuristic-based policies.",
        "In this paper, we study the machine learning elements which we are interested\nin together as a machine learning system, consisting of a collection of machine\nlearning elements and a collection of relations between the elements. The\nrelations we concern are algebraic operations, binary relations, and binary\nrelations with composition that can be reasoned categorically. A machine\nlearning system transformation between two systems is a map between the\nsystems, which preserves the relations we concern. The system transformations\ngiven by quotient or clustering, representable functor, and Yoneda embedding\nare highlighted and discussed by machine learning examples. An adjunction\nbetween machine learning systems, a special machine learning system\ntransformation loop, provides the optimal way of solving problems. Machine\nlearning system transformations are linked and compared by their maps at\n2-cell, natural transformations. New insights and structures can be obtained\nfrom universal properties and algebraic structures given by monads, which are\ngenerated from adjunctions.",
        "Stray electric fields induce excess micromotion in ion traps, limiting\nexperimental performance. We present a new micromotion-compensation technique\nthat utilizes a dark ion in a bright-dark-bright linear ion crystal. Stray\nelectric fields in the radial plane of the trap deform the crystal axially. We\nexploit the mode softening near the transition to the zig-zag configuration to\nincrease our sensitivity dramatically. We corroborate our results with a\nmodified ion-displacement compensation method using a single bright ion. Our\nmodification allows us to compensate stray fields on the 2D radial plane from a\n1D measurement of the ion position on the camera. Both methods require only a\nfixed imaging camera and continuous ion-fluorescence detection. As such, they\ncan be readily implemented in virtually any ion-trapping experiment without\nadditional hardware modifications.",
        "We study connection points on the double regular $n$-gon translation surface,\nfor $n \\geq 7$ odd and its staircase model. For $n \\neq 9$, we provide a large\nfamily of points with coordinates in the trace field that are not connection\npoints. This family includes the central points, and for $n=7$ we conjecture\nthat all the remaining points are connection points. Further, in the case where\n$n \\geq 7$ is a prime number, we provide a constructive proof by exhibiting an\nexplicit separatrix passing through a central point that does not extend to a\nsaddle connection.",
        "Despite advanced token compression techniques, existing multimodal large\nlanguage models (MLLMs) still struggle with hour-long video understanding. In\nthis work, we propose Video-XL-Pro, an efficient method for extremely long\nvideo understanding, built upon Reconstructive Compression of Tokens (ReCoT), a\nlearnable module that leverages self-supervised learning to generate\ncomprehensive and compact video tokens. ReCoT introduces two key components:\n(i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from\nstatic image tokens by learning intra-token relationships, which are then used\nin masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively\nmasks redundant visual tokens to facilitate more effective reconstructive\nlearning. To improve training efficiency in MLLMs fine-tuning, we introduce a\nvideo-specific dataset pruning strategy and design a simple yet Query-aware\nSelector that enables the model to precisely locate query-relevant video\ntokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models\ntrained on larger datasets across multiple long video understanding benchmarks.\nMoreover, it can process over 8K frames on a single A100 GPU while maintaining\nhigh-quality performance.",
        "We introduce a functional reactive programming language that extends\nWORMHOLES, an enhancement of YAMPA with support for effects. Our proposal\nrelaxes the constraint in WORMHOLES that restricts all resources to single-use.\nResources are categorized into two kinds: input\/output resources and internal\nresources. Input\/output resources model interactions with the environment and\nfollow constraints similar to those in WORMHOLES. Internal resources, on the\nother hand, enable communication between program components and can be used\nmultiple times. We demonstrate that programs written in our language can be\ntranslated into equivalent effect-free YAMPA programs, ensuring that our\napproach remains compatible with existing functional reactive paradigms.",
        "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties.",
        "Let $X$ be a smooth projective variety of dimension $d$ over an arbitrary\nbase field $k$ and $CH^n(X)_{\\mathbb Q}$ be the $\\mathbb Q$-vector space of\ncodimension $n$ algebraic cycles of $X$ modulo rational equivalence, $1\\leq n\n\\leq d$. Consider the $\\mathbb Q$-vector subspaces $CH^n(X)_{\\mathbb Q}\n\\supseteq CH^n_{\\mathrm{alg}}(X)_{\\mathbb Q} \\supseteq\nCH^n_{\\mathrm{inc}}(X)_{\\mathbb Q}$ of algebraic cycles which are,\nrespectively, algebraically and incident (in the sense of Griffiths) equivalent\nto zero.\n  Our main result computes $CH^d_{\\mathrm{inc}}(X)_{\\mathbb Q}$ (which\ncoincides with the Albanese kernel $T(X)_{\\mathbb Q}$ when $k$ is algebraically\nclosed) in terms of Voevodsky's triangulated category of motives $DM_k$,\nnamely, we show that $CH^d_{\\mathrm{inc}}(X)_{\\mathbb Q}$ is given by the\nsecond step of the orthogonal filtration $F^{\\bullet}$ on $CH^d(X)_{\\mathbb\nQ}$, i.e. $F^2 CH^d (X)_{\\mathbb Q}= CH^d_{\\mathrm{inc}}(X)_{\\mathbb Q}$. The\northogonal filtration $F^\\bullet$ on $CH^n(X)_{\\mathbb Q}$ was introduced by\nthe first author, and is an unconditionally finite filtration satisfying\nseveral of the properties of the still conjectural Bloch-Beilinson filtration.\n  We also prove that the exterior product and intersection product of algebraic\ncycles algebraically equivalent to zero is contained in the second step of the\northogonal filtration.\n  Furthermore, if we assume that the field $k$ is either finite or the\nalgebraic closure of a finite field, then the main result holds in any\ncodimension, i.e. $F^2 CH^n_{\\mathrm{alg}}(X)_{\\mathbb Q}=\nCH^n_{\\mathrm{inc}}(X)_{\\mathbb Q}$. We also compute in the whole Chow group,\n$CH^n(X)_{\\mathbb Q}$, the second step of the orthogonal filtration $F^2\nCH^n(X)_{\\mathbb Q}$ in terms of the vanishing of several intersection\npairings.",
        "An ordered moment approach to exact open quantum dynamics is presented, which\nbypasses the Feynmann-Vernon influence functional formalism. The hierarchical\nequations of motion are constructed using Wick's contraction, which follows\nspecific orderings of the bath's creation and annihilation operators. Our\napproach moves beyond the traditional influence functional formalism, offering\na more intuitive and direct framework, and extends the applicability of theory\nto nonlinear system--bath coupling scenarios.",
        "The improper disposal and mismanagement of medical waste pose severe\nenvironmental and public health risks, contributing to greenhouse gas emissions\nand the spread of infectious diseases. Efficient and accurate medical waste\nclassification is crucial for mitigating these risks. We explore the\nintegration of capsule networks with a pretrained DenseNet model to improve\nmedical waste classification. To the best of our knowledge, capsule networks\nhave not yet been applied to this task, making this study the first to assess\ntheir effectiveness.\n  A diverse dataset of medical waste images collected from multiple public\nsources, is used to evaluate three model configurations: (1) a pretrained\nDenseNet model as a baseline, (2) a pretrained DenseNet with frozen layers\ncombined with a capsule network, and (3) a pretrained DenseNet with unfrozen\nlayers combined with a capsule network. Experimental results demonstrate that\nincorporating capsule networks improves classification performance, with F1\nscores increasing from 0.89 (baseline) to 0.92 (hybrid model with unfrozen\nlayers). This highlights the potential of capsule networks to address the\nspatial limitations of traditional convolutional models and improve\nclassification robustness.\n  While the capsule-enhanced model demonstrated improved classification\nperformance, direct comparisons with prior studies were challenging due to\ndifferences in dataset size and diversity. Previous studies relied on smaller,\ndomain-specific datasets, which inherently yielded higher accuracy. In\ncontrast, our study employs a significantly larger and more diverse dataset,\nleading to better generalization but introducing additional classification\nchallenges. This highlights the trade-off between dataset complexity and model\nperformance.",
        "We numerically investigate the athermal creep deformation of amorphous\nmaterials having a wide range of stability. The imposed shear stress serves as\nthe control parameter, allowing us to examine the time-dependent transient\nresponse through both the macroscopic strain and microscopic observables. Least\nstable samples exhibit monotonicity in the transient strain rate versus time,\nwhile more stable samples display a pronounced non-monotonic S-shaped curve,\ncorresponding to failure by sharp shear band formation. We identify a diverging\ntimescale associated with the fluidization process and extract the\ncorresponding critical exponents. Our results are compared with predictions\nfrom existing scaling theories relevant to soft matter systems. The numerical\nfindings for stable, brittle-like materials represent a challenge for\ntheoretical descriptions. We monitor the microscopic initiation of shear bands\nduring creep responses. Our study encompasses creep deformation across a\nvariety of materials ranging from ductile soft matter to brittle metallic and\noxide glasses, all within the same numerical framework.",
        "In this paper, we consider nonconvex optimization problems with nonlinear\nequality constraints. We assume that the objective function and the functional\nconstraints are locally smooth. To solve this problem, we introduce a\nlinearized $\\ell_q$ penalty based method, where $q \\in (1,2]$ is the parameter\ndefining the norm used in the construction of the penalty function. Our method\ninvolves linearizing the objective function and functional constraints in a\nGauss-Newton fashion at the current iteration in the penalty formulation and\nintroduces a quadratic regularization. This approach yields an easily solvable\nsubproblem, whose solution becomes the next iterate. By using a novel dynamic\nrule for the choice of the regularization parameter, we establish that the\niterates of our method converge to an $\\epsilon$-first-order solution in\n$\\mathcal{O}(1\/{\\epsilon^{2+ (q-1)\/q}})$ outer iterations. Finally, we put\ntheory into practice and evaluate the performance of the proposed algorithm by\nmaking numerical comparisons with existing methods from literature.",
        "An increase in availability of Software Defined Radios (SDRs) has caused a\ndramatic shift in the threat landscape of legacy satellite systems, opening\nthem up to easy spoofing attacks by low-budget adversaries. Physical-layer\nauthentication methods can help improve the security of these systems by\nproviding additional validation without modifying the space segment. This paper\nextends previous research on Radio Frequency Fingerprinting (RFF) of satellite\ncommunication to the Orbcomm satellite formation. The GPS and Iridium\nconstellations are already well covered in prior research, but the feasibility\nof transferring techniques to other formations has not yet been examined, and\nraises previously undiscussed challenges.\n  In this paper, we collect a novel dataset containing 8992474 packets from the\nOrbcom satellite constellation using different SDRs and locations. We use this\ndataset to train RFF systems based on convolutional neural networks. We achieve\nan ROC AUC score of 0.53 when distinguishing different satellites within the\nconstellation, and 0.98 when distinguishing legitimate satellites from SDRs in\na spoofing scenario. We also demonstrate the possibility of mixing datasets\nusing different SDRs in different physical locations.",
        "TEMPO (Time-dependent Evolution of Multiple Pulse Operations) offers\naccessible and efficient simulations of pulse sequences in Python, using the\nsuite of master equation solvers available in the Quantum Toolbox in Python\n(QuTiP). It enables straightforward definition of pulse sequence structures,\nincluding any underlying time-dependent Hamiltonians and pulse timing\ninformation, and faster simulations of pulse sequence dynamics (compared to\nnaive implementations using QuTiP) while remaining compatible with the existing\ncollection of QuTiP subpackages. Given the ubiquitous use of pulse sequences\nthroughout quantum information\/computing sciences, magnetic resonance studies,\nand quantum metrology, this work has immediate relevance to a wide array of\nresearch applications.",
        "Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen\ntasks but faces challenges in long-horizon environments. Skill-based approaches\ntackle this by decomposing state-action sequences into reusable skills and\nemploying hierarchical decision-making. However, these methods are highly\nsusceptible to noisy offline demonstrations, resulting in unstable skill\nlearning and degraded performance. To overcome this, we propose Prioritized\nRefinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates\nexploration near noisy data to generate online trajectories and combines them\nwith offline data. Through prioritization, PRISM extracts high-quality data to\nlearn task-relevant skills effectively. By addressing the impact of noise, our\nmethod ensures stable skill learning and achieves superior performance in\nlong-horizon tasks, even with noisy and sub-optimal data.",
        "The forthcoming sixth-generation (6G) mobile network is set to merge edge\nartificial intelligence (AI) and integrated sensing and communication (ISAC)\nextensively, giving rise to the new paradigm of edge intelligent sensing\n(EI-Sense). This paradigm leverages ubiquitous edge devices for environmental\nsensing and deploys AI algorithms at edge servers to interpret the observations\nvia remote inference on wirelessly uploaded features. A significant challenge\narises in designing EI-Sense systems for 6G mission-critical applications,\nwhich demand high performance under stringent latency constraints. To tackle\nthis challenge, we focus on the end-to-end (E2E) performance of EI-Sense and\ncharacterize a source-channel tradeoff that balances source distortion and\nchannel reliability. In this work, we establish a theoretical foundation for\nthe source-channel tradeoff by quantifying the effects of source coding on\nfeature discriminant gains and channel reliability on packet loss. Building on\nthis foundation, we design the coding rate control by optimizing the tradeoff\nto minimize the E2E sensing error probability, leading to a low-complexity\nalgorithm for ultra-low-latency EI-Sense. Finally, we validate our theoretical\nanalysis and proposed coding rate control algorithm through extensive\nexperiments on both synthetic and real datasets, demonstrating the sensing\nperformance gain of our approach with respect to traditional\nreliability-centric methods.",
        "Homomorphic encryption (HE) is a core building block in privacy-preserving\nmachine learning (PPML), but HE is also widely known as its efficiency\nbottleneck. Therefore, many GPU-accelerated cryptographic schemes have been\nproposed to improve the performance of HE. However, these methods often require\ncomplex modifications tailored to specific algorithms and are tightly coupled\nwith specific GPU and operating systems. It is interesting to ask how to\ngenerally offer more practical GPU-accelerated cryptographic algorithm\nimplementations. Given the powerful code generation capabilities of large\nlanguage models (LLMs), we aim to explore their potential to automatically\ngenerate practical GPU-friendly algorithm code using CPU-friendly code. In this\npaper, we focus on number theoretic transform (NTT) -- the core mechanism of\nHE. We first develop and optimize a GPU-friendly NTT (GNTT) family that\nexploits PyTorch's fast matrix computation and precomputation, achieving an\napproximately 62x speedup -- a significant boost over existing ones. Then we\nexplore GPU-friendly code generation using various LLMs, including DeepSeek-R1,\nOpenAI o1 and o3-mini. We discover many interesting findings throughout the\nprocess. For instance, somewhat surprisingly, our experiments demonstrate that\nDeepSeek-R1 significantly outperforms OpenAI o3-mini and o1, but still cannot\nbeat our optimized protocol. The findings provide valuable insights for\nturbocharging PPML and enhancing code generation capabilities of LLMs. Codes\nare available at: https:\/\/github.com\/LMPC-Lab\/GenGPUCrypto."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven",
    "start_abstract":"The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      ],
      "abstract":[
        "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Reward Models Identify Consistency, Not Causality",
        "On the Data-Driven Modeling of Price-Responsive Flexible Loads:\n  Formulation and Algorithm",
        "Using Large Language Models for Solving Thermodynamic Problems",
        "Advancing Precision Oncology Through Modeling of Longitudinal and\n  Multimodal Data",
        "Spherical Dense Text-to-Image Synthesis",
        "Expressive Music Data Processing and Generation",
        "Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on\n  Code Naturalness",
        "Language-agnostic, automated assessment of listeners' speech recall\n  using large language models",
        "Beyond Pairwise: Global Zero-shot Temporal Graph Generation",
        "Temporal Context Awareness: A Defense Framework Against Multi-turn\n  Manipulation Attacks on Large Language Models",
        "Dimension of diagonal self-affine measures with exponentially separated\n  projections",
        "Global linearization without hyperbolicity",
        "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models",
        "Design of Bayesian Clinical Trials with Clustered Data and Multiple\n  Endpoints",
        "Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited\n  Communication",
        "A note on the existence of nontrivial zero modes on Riemannian manifolds",
        "Optimal Binary Variable-Length Codes with a Bounded Number of 1's per\n  Codeword: Design, Analysis, and Applications",
        "Improved high-index saddle dynamics for finding saddle points and\n  solution landscape",
        "Adaptive refinement for eigenvalue problems based on an associated\n  source problem",
        "One for All: A General Framework of LLMs-based Multi-Criteria Decision\n  Making on Human Expert Level",
        "High-frequency lead-lag relationships in the Chinese stock index futures\n  market: tick-by-tick dynamics of calendar spreads",
        "Pareto-undominated strategy-proof rules in economies with\n  multidimensional single-peaked preferences",
        "On Quantizing Neural Representation for Variable-Rate Video Coding",
        "Estimating Time Delays between Signals under Mixed Noise Influence with\n  Novel Cross- and Bispectral Methods",
        "Thresholds for the biased Maker-Breaker domination games",
        "Towards Safer Social Media Platforms: Scalable and Performant Few-Shot\n  Harmful Content Moderation Using Large Language Models",
        "ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems",
        "On $p$-adic Siegel--Eisenstein series II: How to avoid the regularity\n  condition for $p$",
        "Social hierarchy shapes foraging decisions"
      ],
      "abstract":[
        "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
        "The flexible loads in power systems, such as interruptible and transferable\nloads, are critical flexibility resources for mitigating power imbalances.\nDespite their potential, accurate modeling of these loads is a challenging work\nand has not received enough attention, limiting their integration into\noperational frameworks. To bridge this gap, this paper develops a data-driven\nidentification theory and algorithm for price-responsive flexible loads\n(PRFLs). First, we introduce PRFL models that capture both static and dynamic\ndecision mechanisms governing their response to electricity price variations.\nSecond, We develop a data-driven identification framework that explicitly\nincorporates forecast and measurement errors. Particularly, we give a\ntheoretical analysis to quantify the statistical impact of such noise on\nparameter estimation. Third, leveraging the bilevel structure of the\nidentification problem, we propose a Bayesian optimization-based algorithm that\nfeatures the scalability to large sample sizes and the ability to offer\nposterior differentiability certificates as byproducts. Numerical tests\ndemonstrate the effectiveness and superiority of the proposed approach.",
        "Large Language Models (LLMs) have made significant progress in reasoning,\ndemonstrating their capability to generate human-like responses. This study\nanalyzes the problem-solving capabilities of LLMs in the domain of\nthermodynamics. A benchmark of 22 thermodynamic problems to evaluate LLMs is\npresented that contains both simple and advanced problems. Five different LLMs\nare assessed: GPT-3.5, GPT-4, and GPT-4o from OpenAI, Llama 3.1 from Meta, and\nle Chat from MistralAI. The answers of these LLMs were evaluated by trained\nhuman experts, following a methodology akin to the grading of academic exam\nresponses. The scores and the consistency of the answers are discussed,\ntogether with the analytical skills of the LLMs. Both strengths and weaknesses\nof the LLMs become evident. They generally yield good results for the simple\nproblems, but also limitations become clear: The LLMs do not provide consistent\nresults, they often fail to fully comprehend the context and make wrong\nassumptions. Given the complexity and domain-specific nature of the problems,\nthe statistical language modeling approach of the LLMs struggles with the\naccurate interpretation and the required reasoning. The present results\nhighlight the need for more systematic integration of thermodynamic knowledge\nwith LLMs, for example, by using knowledge-based methods.",
        "Cancer evolves continuously over time through a complex interplay of genetic,\nepigenetic, microenvironmental, and phenotypic changes. This dynamic behavior\ndrives uncontrolled cell growth, metastasis, immune evasion, and therapy\nresistance, posing challenges for effective monitoring and treatment. However,\ntoday's data-driven research in oncology has primarily focused on\ncross-sectional analysis using data from a single modality, limiting the\nability to fully characterize and interpret the disease's dynamic\nheterogeneity. Advances in multiscale data collection and computational methods\nnow enable the discovery of longitudinal multimodal biomarkers for precision\noncology. Longitudinal data reveal patterns of disease progression and\ntreatment response that are not evident from single-timepoint data, enabling\ntimely abnormality detection and dynamic treatment adaptation. Multimodal data\nintegration offers complementary information from diverse sources for more\nprecise risk assessment and targeting of cancer therapy. In this review, we\nsurvey methods of longitudinal and multimodal modeling, highlighting their\nsynergy in providing multifaceted insights for personalized care tailored to\nthe unique characteristics of a patient's cancer. We summarize the current\nchallenges and future directions of longitudinal multimodal analysis in\nadvancing precision oncology.",
        "Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF. Link to code\nhttps:\/\/github.com\/sdt2i\/spherical-dense-text-to-image",
        "Musical expressivity and coherence are indispensable in music composition and\nperformance, while often neglected in modern AI generative models. In this\nwork, we introduce a listening-based data-processing technique that captures\nthe expressivity in musical performance. This technique derived from Weber's\nlaw reflects the human perceptual truth of listening and preserves musical\nsubtlety and expressivity in the training input. To facilitate musical\ncoherence, we model the output interdependencies among multiple arguments in\nthe music data such as pitch, duration, velocity, etc. in the neural networks\nbased on the probabilistic chain rule. In practice, we decompose the\nmulti-output sequential model into single-output submodels and condition\npreviously sampled outputs on the subsequent submodels to induce conditional\ndistributions. Finally, to select eligible sequences from all generations, a\ntentative measure based on the output entropy was proposed. The entropy\nsequence is set as a criterion to select predictable and stable generations,\nwhich is further studied under the context of informational aesthetic measures\nto quantify musical pleasure and information gain along the music tendency.",
        "Neural code models (NCMs) have demonstrated extraordinary capabilities in\ncode intelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems\nhas garnered increasing attention. In particular, NCMs are often trained on\nlarge-scale data from potentially untrustworthy sources, providing attackers\nwith the opportunity to manipulate them by inserting crafted samples into the\ndata. This type of attack is called a code poisoning attack (also known as a\nbackdoor attack). It allows attackers to implant backdoors in NCMs and thus\ncontrol model behavior, which poses a significant security threat. However,\nthere is still a lack of effective techniques for detecting various complex\ncode poisoning attacks.\n  In this paper, we propose an innovative and lightweight technique for code\npoisoning detection named KillBadCode. KillBadCode is designed based on our\ninsight that code poisoning disrupts the naturalness of code. Specifically,\nKillBadCode first builds a code language model (CodeLM) on a lightweight\n$n$-gram language model. Then, given poisoned data, KillBadCode utilizes CodeLM\nto identify those tokens in (poisoned) code snippets that will make the code\nsnippets more natural after being deleted as trigger tokens. Considering that\nthe removal of some normal tokens in a single sample might also enhance code\nnaturalness, leading to a high false positive rate (FPR), we aggregate the\ncumulative improvement of each token across all samples. Finally, KillBadCode\npurifies the poisoned data by removing all poisoned samples containing the\nidentified trigger tokens. The experimental results on two code poisoning\nattacks and four code intelligence tasks demonstrate that KillBadCode\nsignificantly outperforms four baselines. More importantly, KillBadCode is very\nefficient, with a minimum time consumption of only 5 minutes, and is 25 times\nfaster than the best baseline on average.",
        "Speech-comprehension difficulties are common among older people. Standard\nspeech tests do not fully capture such difficulties because the tests poorly\nresemble the context-rich, story-like nature of ongoing conversation and are\ntypically available only in a country's dominant\/official language (e.g.,\nEnglish), leading to inaccurate scores for native speakers of other languages.\nAssessments for naturalistic, story speech in multiple languages require\naccurate, time-efficient scoring. The current research leverages modern large\nlanguage models (LLMs) in native English speakers and native speakers of 10\nother languages to automate the generation of high-quality, spoken stories and\nscoring of speech recall in different languages. Participants listened to and\nfreely recalled short stories (in quiet\/clear and in babble noise) in their\nnative language. LLM text-embeddings and LLM prompt engineering with semantic\nsimilarity analyses to score speech recall revealed sensitivity to known\neffects of temporal order, primacy\/recency, and background noise, and high\nsimilarity of recall scores across languages. The work overcomes limitations\nassociated with simple speech materials and testing of closed native-speaker\ngroups because recall data of varying length and details can be mapped across\nlanguages with high accuracy. The full automation of speech generation and\nrecall scoring provides an important step towards comprehension assessments of\nnaturalistic speech with clinical applicability.",
        "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, in which event pairs are considered individually,\nleading to computational inefficiency and a lack of global consistency in the\nresulting temporal graph. In this work, we propose a novel zero-shot method for\nTRE that generates a document's complete temporal graph at once, then applies\ntransitive constraints optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\nsignificantly outperforms existing zero-shot approaches while achieving\ncompetitive performance with supervised models.",
        "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security.",
        "Let $ \\mu $ be a self-affine measure associated with a diagonal affine\niterated function system (IFS) $ \\Phi = \\{ (x_{1}, \\ldots, x_{d}) \\mapsto (\nr_{i, 1}x_{1} + t_{i,1}, \\ldots, r_{i,d}x_{d} + t_{i,d}) \\}_{i\\in\\Lambda} $ on\n$ \\mathbb{R}^{d} $ and a probability vector $ p = (p_{i})_{i\\in\\Lambda}$. For $\n1 \\leq j \\leq d $, denote the $ j $-th the Lyapunov exponent by $ \\chi_{j} :=\n\\sum_{i\\in\\Lambda} - p_{i} \\log | r_{i,j} |$, and define the IFS induced by $\n\\Phi $ on the $j$-th coordinate as $ \\Phi_{j} := \\{ x \\mapsto r_{i,j}x +\nt_{i,j}\\}_{i\\in\\Lambda}$. We prove that if $ \\chi_{j_{1}} \\neq \\chi_{j_{2}} $\nfor $ 1 \\leq j_{1} < j_{2} \\leq d $, and $ \\Phi_{j}$ is exponentially separated\nfor $ 1 \\leq j \\leq d $, then the dimension of $ \\mu $ is the minimum of $ d $\nand its Lyapunov dimension. This confirms a conjecture of Rapaport by removing\nthe additional assumption that the linear parts of the maps in $ \\Phi $ are\ncontained in a 1-dimensional subgroup. One of the main ingredients of the proof\ninvolves disintegrating $ \\mu $ into random measures with convolution\nstructure. In the course of the proof, we establish new results on dimension\nand entropy increase for these random measures.",
        "We give a proof of an extension of the Hartman-Grobman theorem to\nnonhyperbolic but asymptotically stable equilibria of vector fields. Moreover,\nthe linearizing topological conjugacy is (i) defined on the entire basin of\nattraction if the vector field is complete, and (ii) a $C^{k\\geq 1}$\ndiffeomorphism on the complement of the equilibrium if the vector field is\n$C^k$ and the underlying space is not $5$-dimensional. We also show that the\n$C^k$ statement in the $5$-dimensional case is equivalent to the\n$4$-dimensional smooth Poincar\\'{e} conjecture.",
        "Recent advances in text-to-image diffusion models enable photorealistic image\ngeneration, but they also risk producing malicious content, such as NSFW\nimages. To mitigate risk, concept erasure methods are studied to facilitate the\nmodel to unlearn specific concepts. However, current studies struggle to fully\nerase malicious concepts implicitly embedded in prompts (e.g., metaphorical\nexpressions or adversarial prompts) while preserving the model's normal\ngeneration capability. To address this challenge, our study proposes TRCE,\nusing a two-stage concept erasure strategy to achieve an effective trade-off\nbetween reliable erasure and knowledge preservation. Firstly, TRCE starts by\nerasing the malicious semantics implicitly embedded in textual prompts. By\nidentifying a critical mapping objective(i.e., the [EoT] embedding), we\noptimize the cross-attention layers to map malicious prompts to contextually\nsimilar prompts but with safe concepts. This step prevents the model from being\noverly influenced by malicious semantics during the denoising process.\nFollowing this, considering the deterministic properties of the sampling\ntrajectory of the diffusion model, TRCE further steers the early denoising\nprediction toward the safe direction and away from the unsafe one through\ncontrastive learning, thus further avoiding the generation of malicious\ncontent. Finally, we conduct comprehensive evaluations of TRCE on multiple\nmalicious concept erasure benchmarks, and the results demonstrate its\neffectiveness in erasing malicious concepts while better preserving the model's\noriginal generation ability. The code is available at:\nhttp:\/\/github.com\/ddgoodgood\/TRCE. CAUTION: This paper includes model-generated\ncontent that may contain offensive material.",
        "In the design of clinical trials, it is essential to assess the design\noperating characteristics (i.e., the probabilities of making correct\ndecisions). Common practice for the evaluation of operating characteristics in\nBayesian clinical trials relies on estimating the sampling distribution of\nposterior summaries via Monte Carlo simulation. It is computationally intensive\nto repeat this estimation process for each design configuration considered,\nparticularly for clustered data that are analyzed using complex,\nhigh-dimensional models. In this paper, we propose an efficient method to\nassess operating characteristics and determine sample sizes for Bayesian trials\nwith clustered data and multiple endpoints. We prove theoretical results that\nenable posterior probabilities to be modelled as a function of the sample size.\nUsing these functions, we assess operating characteristics at a range of sample\nsizes given simulations conducted at only two sample sizes. These theoretical\nresults are also leveraged to quantify the impact of simulation variability on\nour sample size recommendations. The applicability of our methodology is\nillustrated using a current clinical trial with clustered data.",
        "We consider the problem setting in which multiple autonomous agents must\ncooperatively navigate and perform tasks in an unknown,\ncommunication-constrained environment. Traditional multi-agent reinforcement\nlearning (MARL) approaches assume synchronous communications and perform poorly\nin such environments. We propose AsynCoMARL, an asynchronous MARL approach that\nuses graph transformers to learn communication protocols from dynamic graphs.\nAsynCoMARL can accommodate infrequent and asynchronous communications between\nagents, with edges of the graph only forming when agents communicate with each\nother. We show that AsynCoMARL achieves similar success and collision rates as\nleading baselines, despite 26\\% fewer messages being passed between agents.",
        "We prove a necessary criterion for the (non-)existence of nontrivial\nsolutions to the Dirac equation $D\\psi=A \\cdot_{Cl} \\psi$ on Riemannian\nmanifolds that are either closed or of bounded geometry. This generalizes a\nresult of Rupert Frank and Michael Loss on $\\mathbb{R}^n$ where the criterion\nrelates the $L^n$-norm of $A$ to the Sobolev constant on $\\mathbb{R}^n$. On\nRiemannian manifolds the role of the Sobolev constant will be replaced by the\nYamabe invariant. If $n$ is odd, we show that our criterion is sharp on\n$\\mathbb{S}^n$.",
        "In this paper, we consider the problem of constructing optimal average-length\nbinary codes under the constraint that each codeword must contain at most $D$\nones, where $D$ is a given input parameter. We provide an $O(n^2D)$-time\ncomplexity algorithm for the construction of such codes, where $n$ is the\nnumber of codewords. We also describe several scenarios where the need to\ndesign these kinds of codes naturally arises. Our algorithms allow us to\nconstruct both optimal average-length prefix binary codes and optimal\naverage-length alphabetic binary codes. In the former case, our $O(n^2D)$-time\nalgorithm substantially improves on the previously known $O(n^{2+D})$-time\ncomplexity algorithm for the same problem. We also provide a Kraft-like\ninequality for the existence of (optimal) variable-length binary codes, subject\nto the above-described constraint on the number of 1's in each codeword.",
        "We present an improved high-index saddle dynamics (iHiSD) for finding saddle\npoints and constructing solution landscapes, which is a crossover dynamics from\ngradient flow to traditional HiSD such that the Morse theory for gradient flow\ncould be involved. We propose analysis for the reflection manifold in iHiSD,\nand then prove its stable and nonlocal convergence from outside of the region\nof attraction to the saddle point, which resolves the dependence of the\nconvergence of HiSD on the initial value. We then present and analyze a\ndiscretized iHiSD that inherits these convergence properties. Furthermore,\nbased on the Morse theory, we prove that any two saddle points could be\nconnected by a sequence of trajectories of iHiSD. Theoretically, this implies\nthat a solution landscape with a finite number of stationary points could be\ncompletely constructed by means of iHiSD, which partly answers the completeness\nissue of the solution landscape for the first time and indicates the necessity\nof integrating the gradient flow in HiSD. Different methods are compared by\nnumerical experiments to substantiate the effectiveness of the iHiSD method.",
        "We introduce an adaptive finite element scheme for the efficient\napproximation of a (large) collection of eigenpairs of selfadjoint elliptic\noperators in which the adaptive refinement is driven by the solution of a\nsingle source problem -- the so-called landscape problem for the operator --\ninstead of refining based on the computed eigenpairs. Some theoretical\njustification for the approach is provided, and extensive empirical results\nindicate that it can provide an attractive alternative to standard adaptive\nschemes, particularly in the hp-adaptive environment.",
        "Multi-Criteria Decision Making~(MCDM) is widely applied in various fields,\nusing quantitative and qualitative analyses of multiple levels and attributes\nto support decision makers in making scientific and rational decisions in\ncomplex scenarios. However, traditional MCDM methods face bottlenecks in\nhigh-dimensional problems. Given the fact that Large Language Models~(LLMs)\nachieve impressive performance in various complex tasks, but limited work\nevaluates LLMs in specific MCDM problems with the help of human domain experts,\nwe further explore the capability of LLMs by proposing an LLM-based evaluation\nframework to automatically deal with general complex MCDM problems. Within the\nframework, we assess the performance of various typical open-source models, as\nwell as commercial models such as Claude and ChatGPT, on 3 important\napplications, these models can only achieve around 60\\% accuracy rate compared\nto the evaluation ground truth. Upon incorporation of Chain-of-Thought or\nfew-shot prompting, the accuracy rates rise to around 70\\%, and highly depend\non the model. In order to further improve the performance, a LoRA-based\nfine-tuning technique is employed. The experimental results show that the\naccuracy rates for different applications improve significantly to around 95\\%,\nand the performance difference is trivial between different models, indicating\nthat LoRA-based fine-tuned LLMs exhibit significant and stable advantages in\naddressing MCDM tasks and can provide human-expert-level solutions to a wide\nrange of MCDM challenges.",
        "Lead-lag relationships, integral to market dynamics, offer valuable insights\ninto the trading behavior of high-frequency traders (HFTs) and the flow of\ninformation at a granular level. This paper investigates the lead-lag\nrelationships between stock index futures contracts of different maturities in\nthe Chinese financial futures market (CFFEX). Using high-frequency\n(tick-by-tick) data, we analyze how price movements in near-month futures\ncontracts influence those in longer-dated contracts, such as next-month,\nquarterly, and semi-annual contracts. Our findings reveal a consistent pattern\nof price discovery, with the near-month contract leading the others by one\ntick, driven primarily by liquidity. Additionally, we identify a negative\nfeedback effect of the \"lead-lag spread\" on the leading asset, which can\npredict returns of leading asset. Backtesting results demonstrate the\nprofitability of trading based on the lead-lag spread signal, even after\naccounting for transaction costs. Altogether, our analysis offers valuable\ninsights to understand and capitalize on the evolving dynamics of futures\nmarkets.",
        "In the problem of fully allocating a social endowment of perfectly divisible\ncommodities among a group of agents with multidimensional single-peaked\npreferences, we study strategy-proof rules that are not Pareto-dominated by\nother strategy-proof rules. Specifically, we: (i) establish a sufficient\ncondition for a rule to be Pareto-undominated strategy-proof; (ii) introduce a\nbroad class of rules satisfying this property by extending the family of\n\"sequential allotment rules\" to the multidimensional setting; and (iii) provide\na new characterization of the \"multidimensional uniform rule\" involving\nPareto-undominated strategy-proofness. Results (i) and (iii) generalize\nprevious findings that were only applicable to the two-agent case.",
        "This work introduces NeuroQuant, a novel post-training quantization (PTQ)\napproach tailored to non-generalized Implicit Neural Representations for\nvariable-rate Video Coding (INR-VC). Unlike existing methods that require\nextensive weight retraining for each target bitrate, we hypothesize that\nvariable-rate coding can be achieved by adjusting quantization parameters (QPs)\nof pre-trained weights. Our study reveals that traditional quantization\nmethods, which assume inter-layer independence, are ineffective for\nnon-generalized INR-VC models due to significant dependencies across layers. To\naddress this, we redefine variable-rate INR-VC as a mixed-precision\nquantization problem and establish a theoretical framework for sensitivity\ncriteria aimed at simplified, fine-grained rate control. Additionally, we\npropose network-wise calibration and channel-wise quantization strategies to\nminimize quantization-induced errors, arriving at a unified formula for\nrepresentation-oriented PTQ calibration. Our experimental evaluations\ndemonstrate that NeuroQuant significantly outperforms existing techniques in\nvarying bitwidth quantization and compression efficiency, accelerating encoding\nby up to eight times and enabling quantization down to INT2 with minimal\nreconstruction loss. This work introduces variable-rate INR-VC for the first\ntime and lays a theoretical foundation for future research in rate-distortion\noptimization, advancing the field of video coding technology. The materials\nwill be available at https:\/\/github.com\/Eric-qi\/NeuroQuant.",
        "A common problem to signal processing are biases introduced by correlated\nnoise. In Time-Delay Estimation (TDE), which quantifies a time lag between two\nsignals, noise mixing introduces a bias towards zero delay in conventional TDE\nprotocols based on the cross- or bispectrum. Here we propose two novel TDE\napproaches that address these shortcomings: (1) A cross-spectrum based TDE\nprotocol that relies on estimating the periodicity of the phase spectrum rather\nthan its slope, and (2) a bispectrum based TDE analysis, bispectral\nantisymmetrization, which removes contributions from not just Gaussian but all\nindependent sources. In a simulation study, we compare conventional and novel\nTDE protocols and resolve differences in performance with respect to noise\nGaussianity and auto-correlation structure. As a proof-of-concept, we also\nperform TDE analysis on a neural stimulation dataset (n=3). We find that\nantisymmetrization consistently outperforms conventional bispectral TDE methods\nat low signal-to-noise ratios (SNR) and removes spurious zero-delay estimates\nin all mixed-noise environments. TDE based on phase periodicity also improves\nsignal sensitivity compared to conventional cross-spectral methods. These\nobservations are stable with respect to the magnitude of the delay and the\nstatistical properties of the noise.",
        "In the $(a,b)$-biased Maker-Breaker domination game, two players alternately\nselect unplayed vertices in a graph $G$ such that Dominator selects $a$ and\nStaller selects $b$ vertices per move. Dominator wins if the vertices he\nselected during the game form a dominating set of $G$, while Staller wins if\nshe can prevent Dominator from achieving this goal. Given a positive integer\n$b$, Dominator's threshold, $\\textrm{a}_b$, is the minimum $a$ such that\nDominator wins the $(a,b)$-biased game on $G$ when he starts the game.\nSimilarly, $\\textrm{a}'_b$ denotes the minimum $a$ such that Dominator wins\nwhen Staller starts the $(a,b)$-biased game. Staller's thresholds,\n$\\textrm{b}_a$ and $\\textrm{b}'_a$, are defined analogously. It is proved that\nStaller wins the $(k-1,k)$-biased games in a graph $G$ if its order is\nsufficiently large with respect to a function of $k$ and the maximum degree of\n$G$. Along the way, the $\\ell$-local domination number of a graph is\nintroduced. This new parameter is proved to bound Dominator's thresholds\n$\\textrm{a}_\\ell$ and $\\textrm{a}_\\ell'$ from above. As a consequence,\n$\\textrm{a}_1'(G)\\le 2$ holds for every claw-free graph $G$. More specific\nresults are obtained for thresholds in line graphs and Cartesian grids. Based\non the concept of $[1,k]$-factor of a graph $G$, we introduce the star\npartition width $\\sigma(G)$ of $G$, and prove that $\\textrm{a}_1'(G)\\le\n\\sigma(G)$ holds for any nontrivial graph $G$, while\n$\\textrm{a}_1'(G)=\\sigma(G)$ if $G$ is a tree.",
        "The prevalence of harmful content on social media platforms poses significant\nrisks to users and society, necessitating more effective and scalable content\nmoderation strategies. Current approaches rely on human moderators, supervised\nclassifiers, and large volumes of training data, and often struggle with\nscalability, subjectivity, and the dynamic nature of harmful content (e.g.,\nviolent content, dangerous challenge trends, etc.). To bridge these gaps, we\nutilize Large Language Models (LLMs) to undertake few-shot dynamic content\nmoderation via in-context learning. Through extensive experiments on multiple\nLLMs, we demonstrate that our few-shot approaches can outperform existing\nproprietary baselines (Perspective and OpenAI Moderation) as well as prior\nstate-of-the-art few-shot learning methods, in identifying harm. We also\nincorporate visual information (video thumbnails) and assess if different\nmultimodal techniques improve model performance. Our results underscore the\nsignificant benefits of employing LLM based methods for scalable and dynamic\nharmful content moderation online.",
        "Advancements in audio foundation models (FMs) have fueled interest in\nend-to-end (E2E) spoken dialogue systems, but different web interfaces for each\nsystem makes it challenging to compare and contrast them effectively. Motivated\nby this, we introduce an open-source, user-friendly toolkit designed to build\nunified web interfaces for various cascaded and E2E spoken dialogue systems.\nOur demo further provides users with the option to get on-the-fly automated\nevaluation metrics such as (1) latency, (2) ability to understand user input,\n(3) coherence, diversity, and relevance of system response, and (4)\nintelligibility and audio quality of system output. Using the evaluation\nmetrics, we compare various cascaded and E2E spoken dialogue systems with a\nhuman-human conversation dataset as a proxy. Our analysis demonstrates that the\ntoolkit allows researchers to effortlessly compare and contrast different\ntechnologies, providing valuable insights such as current E2E systems having\npoorer audio quality and less diverse responses. An example demo produced using\nour toolkit is publicly available here:\nhttps:\/\/huggingface.co\/spaces\/Siddhant\/Voice_Assistant_Demo.",
        "In a previous paper, the authors showed that two kinds of $p$-adic\nSiegel--Eisenstein series of degree $n$ coincide with classical modular forms\nof weight $k$ for $\\Gamma _0(p)$, under the assumption that $p$ is a regular\nprime. The purpose of this paper is to show that this condition on $p$ can be\nremoved if the degree $n$ is low compared with $k$, namely, $n\\le 2k+1$.",
        "Social foraging is a widespread form of animal foraging in which groups of\nindividuals coordinate their decisions to exploit resources in the environment.\nAnimals show a variety of social structures from egalitarian to hierarchical.\nIn this study, we examine how different forms of social hierarchy shape\nforaging decisions. We developed a mechanistic analytically tractable model to\nstudy the underlying processes of social foraging, tying the microscopic\nindividual to the macroscopic group levels. Based on a stochastic evidence\naccumulation framework, we developed a model of patch-leaving decisions in a\nlarge hierarchical group with leading and following individuals. Across a\nvariety of information sharing mechanisms, we were able to analytically\nquantify emergent collective dynamics. We found that follower-leader dynamics\nthrough observations of leader movements or through counting the number of\nindividuals in a patch confers, for most conditions, a benefit for the\nfollowing individuals by increasing their accuracy in inferring patch richness.\nOn the other hand, misinformation, through the communication of false beliefs\nabout food rewards or patch quality, shows to be detrimental to following\nindividuals, but paradoxically may lead to increased group cohesion. In an era\nwhere there is a huge amount of animal foraging data collected, our model\nprovides a systematic way to conceptualize and understand those data by\nuncovering hidden mechanisms underlying social foraging decisions."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
    "start_abstract":"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven"
      ],
      "abstract":[
        "The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "Nano-topographical changes in latent fingerprint due to degradation over\n  time studied by Atomic force microscopy -- option to set a timeline?",
        "Nonlinear Temperature Sensitivity of Residential Electricity Demand:\n  Evidence from a Distributional Regression Approach",
        "Conformal Prediction and Human Decision Making",
        "On monotone alternating inverse monoids",
        "Oblique rotational axis detection using elliptical optical vortex based\n  on rotational Doppler effect",
        "Sublinear Variational Optimization of Gaussian Mixture Models with\n  Millions to Billions of Parameters",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "Revisiting Frank-Wolfe for Structured Nonconvex Optimization",
        "Cayley unitary elements in group algebras under oriented involutions",
        "Tractable General Equilibrium",
        "Mesoscopic Collective Dynamics in Liquids and the Dual Model",
        "Comprehensive landscape and simple rules for transition-metal Heusler\n  semiconductors",
        "Efficient Truncations of SU($N_c$) Lattice Gauge Theory for Quantum\n  Simulation",
        "Attractors for Singular-Degenerate Porous Medium Type Equations Arising\n  in Models for Biofilm Growth",
        "Topology-preserving discretization for the magneto-frictional equations\n  arising in the Parker conjecture",
        "Deformation theory and Koszul duality for Rota-Baxter systems",
        "Higher-order chiral scalar from boundary reduction of 3d higher-spin\n  gravity",
        "Topological phase diagram and quantum magnetotransport effects in\n  (Pb,Sn)Se quantum wells with magnetic barriers (Pb,Eu)Se",
        "Non-Canonical Crosslinks Confound Evolutionary Protein Structure Models",
        "SymEFT for local tastes of staggered lattice QCD",
        "The Pell sequence and cyclotomic matrices involving squares over finite\n  fields",
        "Graduated orders over completed group rings and conductor formul\\ae",
        "Sterile-neutrino search based on 259 days of KATRIN data",
        "The canonical differential equations of the one-loop-like integrals",
        "Defining the mean turbulent boundary layer thickness based on streamwise\n  velocity skewness",
        "On limiting distributions of Graham, Knuth, Patashnik recurrences",
        "A Foundational Theory for Decentralized Sensory Learning",
        "Models of liquid samples confinement for nanoscale NMR",
        "Binary Bosonic Mixtures with Pair Hopping in Synthetic Dimension: Phase\n  Transitions and Demixing Effects"
      ],
      "abstract":[
        "Latent fingerprints, if present, are crucial in identifying the suspect who\nwas at the crime scene. If there are many latent fingerprints or the suspect is\nfrom the same household, crime investigators may have difficulty identifying\nwhose latent fingerprints are time-related to the crime. Here, we report\nchanges in the nanoscale topography of latent fingerprints, which may serve as\na timeline and could help estimate when the latent fingerprint was imprinted.\nOn the latent fingerprint of an adolescent, we observed a change in\nnano-topography over time, specifically the formation of nano-chain structures\nin space between the imprinted papillary ridges. We consequently compared this\nobservation with the decomposition of the latent fingerprints of a child and\nadult. We observed a significant difference in the time change in\nnano-topography of latent fingerprints of a child, adolescent, and young adult.\nThe nano-topographical changes of latent fingerprints were studied by atomic\nforce microscopy over 70 days. In the case of child's and adolescent's latent\nfingerprints, the first nano-chains were observed already 24 hours after\nimprinting of the latent fingerprint, and the number of nano-chains increased\nsteadily up to 21 days, then we observed that another organic material covered\nthe nano-chains, and they started slowly deteriorating; nevertheless, the\nnano-chains were still present on the 70th day.",
        "We estimate the temperature sensitivity of residential electricity demand\nduring extreme temperature events using the distribution-to-scalar regression\nmodel. Rather than relying on simple averages or individual quantile statistics\nof raw temperature data, we construct distributional summaries, such as\nprobability density, hazard rate, and quantile functions, to retain a more\ncomprehensive representation of temperature variation. This approach not only\nutilizes richer information from the underlying temperature distribution but\nalso enables the examination of extreme temperature effects that conventional\nmodels fail to capture. Additionally, recognizing that distribution functions\nare typically estimated from limited discrete observations and may be subject\nto measurement errors, our econometric framework explicitly addresses this\nissue. Empirical findings from the hazard-to-demand model indicate that\nresidential electricity demand exhibits a stronger nonlinear response to cold\nwaves than to heat waves, while heat wave shocks demonstrate a more pronounced\nincremental effect. Moreover, the temperature quantile-to-demand model produces\nlargely insignificant demand response estimates, attributed to the offsetting\ninfluence of two counteracting forces.",
        "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
        "In this paper, we consider the inverse submonoids $AM_n$ of monotone\ntransformations and $AO_n$ of order-preserving transformations of the\nalternating inverse monoid $AI_n$ on a chain with $n$ elements. We compute the\ncardinalities, describe the Green's structures and the congruences, and\ncalculate the ranks of these two submonoids of $AI_n$.",
        "The rotational Doppler effect (RDE) of structured light carrying orbital\nangular momentum (OAM) has attracted widespread attention for applications in\noptical sensors and OAM spectrum detection. These studies, however, based on\nRDE, are mostly focused on the motion parameters of rotating objects; other\nequally important attitude characteristics, e.g., the tilt angle of the axis of\nrotation, have rarely been considered. We observed an interesting phenomenon in\nthe experiments: the rotational Doppler spectral distribution varies with the\nellipticity of the elliptical optical vortex (EOV) and the tilt angle between\nthe rotational axis and optical axis, which inspired us to wonder if it is\npossible to detect oblique rotational axis or compensate the rotational Doppler\nbroadening effect induced by oblique incidence by utilizing the EOV. Here, we\nreveal the RDE quantitative relationship with tilt angle and ellipticity for\nthe first time and report a novel approach for tilt angle measurement. By\nemploying a series of EOV with periodically varying ellipticity to illuminate a\nrotating object and analyze the time-frequency spectral distribution of\nscattered light associated with ellipticity and tilt angle, the tilt angle can\nbe acquired accurately based on the specific relationship between the tilt\nangle and ellipticity of the EOV. Furthermore, the spectrum broadening effect\narising from oblique incidence in the actual scenario may be addressed through\nour scheme. The method may find applications in industrial manufacturing and\ntarget attitude measurement, and our results provide new insights for obtaining\nmore information about objects.",
        "Gaussian Mixture Models (GMMs) range among the most frequently used machine\nlearning models. However, training large, general GMMs becomes computationally\nprohibitive for datasets with many data points $N$ of high-dimensionality $D$.\nFor GMMs with arbitrary covariances, we here derive a highly efficient\nvariational approximation, which is integrated with mixtures of factor\nanalyzers (MFAs). For GMMs with $C$ components, our proposed algorithm\nsignificantly reduces runtime complexity per iteration from\n$\\mathcal{O}(NCD^2)$ to a complexity scaling linearly with $D$ and remaining\nconstant w.r.t. $C$. Numerical validation of this theoretical complexity\nreduction then shows the following: the distance evaluations required for the\nentire GMM optimization process scale sublinearly with $NC$. On large-scale\nbenchmarks, this sublinearity results in speed-ups of an order-of-magnitude\ncompared to the state-of-the-art. As a proof of concept, we train GMMs with\nover 10 billion parameters on about 100 million images, and observe training\ntimes of approximately nine hours on a single state-of-the-art CPU.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We introduce a new projection-free (Frank-Wolfe) method for optimizing\nstructured nonconvex functions that are expressed as a difference of two convex\nfunctions. This problem class subsumes smooth nonconvex minimization,\npositioning our method as a promising alternative to the classical Frank-Wolfe\nalgorithm. DC decompositions are not unique; by carefully selecting a\ndecomposition, we can better exploit the problem structure, improve\ncomputational efficiency, and adapt to the underlying problem geometry to find\nbetter local solutions. We prove that the proposed method achieves a\nfirst-order stationary point in $O(1\/\\epsilon^2)$ iterations, matching the\ncomplexity of the standard Frank-Wolfe algorithm for smooth nonconvex\nminimization in general. Specific decompositions can, for instance, yield a\ngradient-efficient variant that requires only $O(1\/\\epsilon)$ calls to the\ngradient oracle. Finally, we present numerical experiments demonstrating the\neffectiveness of the proposed method compared to the standard Frank-Wolfe\nalgorithm.",
        "Let $\\mathbf{F}$ be a real extension of $\\mathbb{Q}$, $G$ a finite group and\n$\\mathbf{F}G$ its group algebra. Given both a group homomorphism\n$\\sigma:G\\rightarrow \\{\\pm1\\}$ (called an orientation) and a group involution\n$^\\ast:G \\rightarrow G$ such that $gg^\\ast\\in N=ker(\\sigma)$, an oriented group\ninvolution $\\circledast$ of $\\mathbf{F}G$ is defined by $\\alpha=\\sum_{g\\in\nG}\\alpha_{g}g \\mapsto \\alpha^\\circledast=\\sum_{g\\in\nG}\\alpha_{g}\\sigma(g)g^{\\ast}$. In this paper, in case the involution on $G$ is\nthe classical one, $x\\mapsto x^{-1}$, $\\beta=x+x^{-1}$ is a skew-symmetric\nelement in $\\mathbf{F}G$ such that $1+\\beta$ is invertible, for $x\\in G$ with\n$\\sigma(x)=-1$, we consider Cayley unitary elements built out of $\\beta$. We\nprove that the coefficients of $(1+\\beta)^{-1}$ involve an interesting sequence\nwhich is a Fibonacci-like sequence.",
        "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
        "A microscopic vision is presented of a Dual Model of Liquids from a solid\npicture. Among the novelties of this model is that it provides quantitative\nexpressions of various extensive thermophysical properties. The introduction of\nthe statistical number of excited degrees of freedom (DoF) allows bypassing the\nproblem of other dual models which are sometimes unable to correctly reproduce\nthe expressions for those thermophysical quantities showing deviations due to\nthe activation or deactivation of internal DoF. The interpretation of the\nrelaxation times is given, their Order of Magnitude calculated and the way in\nwhich these times are involved in the different phases of the collective\ndynamics of liquids is discussed. A comparison is provided with results\nobtained in the frame of another phononic model of liquids, as well as with the\npredictions for the viscoelastic transition regions and with systems exhibiting\nkgap. In the last part of the paper, theoretical insights and experiments are\nsuggested as potential directions for future research and development.",
        "Heusler alloys, renowned for their multifunctionality and capacity for vast\nelemental customization, are primarily classified into half-Heusler (XYZ) and\nfull-Heusler (X2YZ) structural types. Typically, the 18-electron half-Heusler\nand the 24-electron full-Heusler alloys are recognized as semiconductors,\nfollowing the Slater-Pauling rule. Semiconductors are desired for many\napplications, but they represent a minor portion compared to the predominantly\nmetallic and half-metallic members of the Heusler family. To broaden the scope\nof Heusler semiconductors, advancements have been made in developing variants\nsuch as double-half Heuslers XX'Y2Z2 and quaternary full Heuslers XX'YZ, which\nincorporate four constituent elements. Recently, vacancy-filling\noff-stoichiometric Heuslers of ternary X1+bYZ (0 <= b <= 1) and quaternary\nXaX'bYZ (1 <= a + b <= 2) have emerged as a more versatile strategy. However,\nthe flexibility associated with off-stoichiometry inevitably leads to\ncomplications, including issues with fractional filling ratios and complex site\noccupations. This work presents a comprehensive landscape of\ntransition-metal-containing Heusler semiconductors, focusing on the\noff-stoichiometric Heuslers but seamlessly encompassing the\ninteger-stoichiometric systems. The structural and electronic properties can be\ntheoretically understood through a few simple rules. Many systems have been\nexperimentally validated, showcasing their potential for applications such as\nthermoelectric converters.",
        "Quantum simulations of lattice gauge theories offer the potential to directly\nstudy the non-perturbative dynamics of quantum chromodynamics, but naive\nanalyses suggest that they require large computational resources. Large $N_c$\nexpansions are performed to order 1\/$N_c$ to simplify the Hamiltonian of pure\nSU($N_c$) lattice gauge theories. A reformulation of the electric basis is\nintroduced with a truncation strategy based on the construction of local Krylov\nsubspaces with plaquette operators. Numerical simulations show that these\ntruncated Hamiltonians are consistent with traditional lattice calculations at\nrelatively small couplings. It is shown that the computational resources\nrequired for quantum simulation of time evolution generated by these\nHamiltonians is 17-19 orders of magnitude smaller than previous approaches.",
        "We investigate the long-time behaviour of solutions of a class of\nsingular-degenerate porous medium type equations in bounded Lipschitz domains\nwith mixed Dirichlet-Neumann boundary conditions. The existence of global\nattractors is shown under very general assumptions. Assuming, in addition, that\nsolutions are globally H\\\"older continuous and the reaction terms satisfy a\nsuitable sign condition in the vicinity of the degeneracy, we also prove the\nexistence of an exponential attractor, which, in turn, yields the finite\nfractal dimension of the global attractor. Moreover, we extend the results for\nscalar equations to systems where the degenerate equation is coupled to a\nsemilinear reaction-diffusion equation. The study of such systems is motivated\nby models for biofilm growth.",
        "The Parker conjecture, which explores whether magnetic fields in perfectly\nconducting plasmas can develop tangential discontinuities during magnetic\nrelaxation, remains an open question in astrophysics. Helicity conservation\nprovides a topological barrier during relaxation, preventing topologically\nnontrivial initial data relaxing to trivial solutions; preserving this\nmechanism discretely over long time periods is therefore crucial for numerical\nsimulation. This work presents an energy- and helicity-preserving finite\nelement discretization for the magneto-frictional system, for investigating the\nParker conjecture. The algorithm preserves a discrete version of the\ntopological barrier and a discrete Arnold inequality. We also discuss\nextensions to domains with nontrivial topology.",
        "This paper investigates Rota-Baxter systems in the sense of Brzezi\\'nski from\nthe perspective of operad theory. The minimal model of the Rota-Baxter system\noperad is constructed, equivalently a concrete construction of its Koszul dual\nhomotopy cooperad is given. The concept of homotopy Rota-Baxter systems and the\n$L_\\infty$-algebra that governs deformations of a Rota-Baxter system are\nderived from the Koszul dual homotopy cooperad. The notion of\ninfinity-Yang-Baxter pairs is introduced, which is a higher-order\ngeneralization of the traditional Yang-Baxter pairs. It is shown that a\nhomotopy Rota-Baxter system structure on the endomorphism algebra of a graded\nspace is equivalent to an associative infinity-Yang-Baxter pair on this graded\nalgebra, thereby generalizing the classical correspondence between Yang-Baxter\npairs and Rota-Baxter systems.",
        "We use a recently proposed covariant procedure to reduce the Chern-Simons\naction of three-dimensional higher-spin gravity to the boundary, resulting in a\nLorentz covariant action for higher-order chiral scalars. After gauge-fixing,\nwe obtain a higher-derivative action generalizing the $s=1$ Floreanini-Jackiw\nand $s=2$ Alekseev-Shatashvili actions to arbitrary spin $s$. For simplicity,\nwe treat the case of general spin at the linearized level, while the full\nnon-linear asymptotic boundary conditions are presented in component form for\nthe $SL(3,\\mathbb R)$ case. Finally, we extend the spin-3 linearized analysis\nto a background with non-trivial higher-spin charge and show that it has a\nricher structure of zero modes.",
        "Despite several theoretical predictions regarding the physics and application\nof quantum wells (QWs) of topological crystalline insulators (TCI), no\nquantized charge transport via helical or chiral edge states has been\nexperimentally demonstrated for such a class of systems. In this study, we\nreport here on a successful growth by molecular beam epitaxy of high\ncrystalline quality Pb$_{1-x}$Sn$_{x}$Se:Bi\/Pb$_{1-y}$Eu$_{y}$Se QWs with $x =\n0.25$ and $y = 0.1$, and on their magnetotransport characterization as a\nfunction of the QW thickness between 10 and 50 nm, temperatures down to 300 mK,\nperpendicular and tilted magnetic fields up to 36 T. The character of weak\nantilocalization magnetoresistance and universal conductance fluctuations\npoints to a notably long phase coherence length. It is argued that a relatively\nlarge magnitude of the dielectric constant of IV-VI compounds suppresses the\ndecoherence by electron-electron scattering. The observation of\nShubnikov-de-Haas oscillations and the quantum Hall effect, together with\nmultiband $k\\cdot p$ modelling, have enabled us to assess valley degeneracies,\nthe magnitude of strain, subbands effective masses, the Berry phases, and the\ntopological phase diagram as a function of the QW thickness. Our results\ndemonstrate that further progress in controlling Sn content, carrier densities,\nand magnetism in Pb$_{1-x}$Sn$_{x}$Se\/Pb$_{1-y}$Eu$_{y}$Se QWs will allow for\nthe exploration of the topologically protected quantized edge transport even in\nthe absence of an external magnetic field. Furthermore, a reduced strength of\nelectron-electron interactions will result in the absence of unpaired localized\nspins in the topological gap and, thus, in a substantially longer topological\nprotection length compared to the quantum spin Hall materials explored so far.",
        "Evolution-based protein structure prediction models have achieved\nbreakthrough success in recent years. However, they struggle to generalize\nbeyond evolutionary priors and on sequences lacking rich homologous data. Here\nwe present a novel, out-of-domain benchmark based on sactipeptides, a rare\nclass of ribosomally synthesized and post-translationally modified peptides\n(RiPPs) characterized by sulfur-to-$\\alpha$-carbon thioether bridges creating\ncross-links between cysteine residues and backbone. We evaluate recent models\non predicting conformations compatible with these cross-links bridges for the\n10 known sactipeptides with elucidated post-translational modifications.\nCrucially, the structures of 5 of them have not yet been experimentally\nresolved. This makes the task a challenging problem for evolution-based models,\nwhich we find exhibit limited performance (0.0% to 19.2% GDT-TS on\nsulfur-to-$\\alpha$-carbon distance). Our results point at the need for\nphysics-informed models to sustain progress in biomolecular structure\nprediction.",
        "The applicability of Symanzik Effective Field Theory (SymEFT) for the\ndescription of lattice artifacts assumes a local formulation of the lattice\ntheory. We discuss the symmetries realised by tastes local in spacetime of\nunrooted staggered quarks, approaching mass-degenerate 4-flavour QCD in the\ncontinuum limit. An outlook on some implications for the asymptotic\nlattice-spacing dependence is given for spectral quantities as well as local\ncomposite fields.",
        "In this paper, by some arithmetic properties of the Pell sequence and some\n$p$-adic tools, we study certain cyclotomic matrices involving squares over\nfinite fields. For example, let $1=s_1,s_2,\\cdots,s_{(q-1)\/2}$ be all the\nnonzero squares over $\\mathbb{F}_{q}$, where $q=p^f$ is an odd prime power with\n$q\\ge7$. We prove that the matrix\n  $$B_q((q-3)\/2)=\\left[\\left(s_i+s_j\\right)^{(q-3)\/2}\\right]_{2\\le i,j\\le\n(q-1)\/2}$$\n  is a singular matrix whenever $f\\ge2$. Also, for the case $q=p$, we show that\n  $$\\det B_p((p-3)\/2)=0\\Leftrightarrow Q_p\\equiv 2\\pmod{p^2\\mathbb{Z}},$$\n  where $Q_p$ is the $p$-th term of the companion Pell sequence\n$\\{Q_i\\}_{i=0}^{\\infty}$ defined by $Q_0=Q_1=2$ and $Q_{i+1}=2Q_i+Q_{i-1}$.",
        "We study graduated orders over completed group rings of $1$-dimensional\nadmissible $p$-adic Lie groups, and verify the equivariant $p$-adic Artin\nconjecture for such orders. Following Jacobinski and Plesken, we obtain a\nformula for the conductor of a graduated order into a self-dual order. We also\nrefine Nickel's central conductor formula by determining a hitherto implicit\nexponent $r_\\chi$.",
        "Neutrinos are the most abundant fundamental matter particles in the Universe\nand play a crucial role in particle physics and cosmology. Neutrino\noscillation, discovered about 25 years ago, reveals that the three known\nspecies mix with each other. Anomalous results from reactor and\nradioactive-source experiments suggest a possible fourth neutrino state, the\nsterile neutrino, which does not interact via the weak force. The KATRIN\nexperiment, primarily designed to measure the neutrino mass via tritium\n$\\beta$-decay, also searches for sterile neutrinos suggested by these\nanomalies. A sterile-neutrino signal would appear as a distortion in the\n$\\beta$-decay energy spectrum, characterized by a discontinuity in curvature\n(kink) related to the sterile-neutrino mass. This signature, which depends only\non the shape of the spectrum rather than its absolute normalization, offers a\nrobust, complementary approach to reactor experiments. KATRIN examined the\nenergy spectrum of 36 million tritium $\\beta$-decay electrons recorded in 259\nmeasurement days within the last 40 electronvolt below the endpoint. The\nresults exclude a substantial part of the parameter space suggested by the\ngallium anomaly and challenge the Neutrino-4 claim. Together with other\nneutrino-disappearance experiments, KATRIN probes sterile-to-active mass\nsplittings from a fraction of an electron-volt squared to several hundred\nelectron-volts squared, excluding light sterile neutrinos with mixing angles\nabove a few percent.",
        "Recently, a new approach for high loop integrals has been proposed in\n\\cite{Huang:2024nij}, where the whole parameter integration has been divided\ninto two parts: a one-loop-like integration and the remaining parameter\nintegration. In this paper, we systematically study the one-loop-like\nintegrals. We establish the IBP relations for the integral family and show how\nto complete the reduction. We find the canonical master integrals and write\ndown the corresponding canonical differential equations.",
        "A new statistical definition for the mean turbulent boundary layer thickness\nis introduced, based on the identification of the point where streamwise\nvelocity skewness changes sign in the outermost region of the boundary layer.\nThis definition is motivated by the phenomenology of streamwise velocity\nfluctuations near the turbulent\/non-turbulent interface, whose local\ncharacteristics are shown to be universal for turbulent boundary layers under\nlow freestream turbulence conditions (e.g., with or without pressure gradients,\nsurface roughness, etc.). This approach provides a turbulent boundary layer\nthickness that is consistent with previous definitions, such as those based on\nReynolds shear stress or `composite' mean velocity profiles, while being\nindependent of arbitrary thresholds and applicable to past single-point\nmeasurements. Two methods are proposed for estimating the turbulent boundary\nlayer thickness using this definition: one based on simple linear interpolation\nand the other on fitting a generalised Fourier model to the outer skewness\nprofile. The robustness and limitations of these methods are demonstrated\nthrough analysis of several published experimental and numerical datasets,\nwhich cover a range of canonical and non-canonical turbulent boundary layers.\nThese datasets vary in wall-normal resolution and measurement noise,\nparticularly in the critical turbulent\/non-turbulent interface region.",
        "Graham, Knuth and Patashnik in their book Concrete Mathematics called for\ndevelopment of a general theory of the solutions of recurrences defined by\n$$\\left|{ n\\atop k}\\right|=(\\alpha n+\\beta k+\\gamma)\\left|{n-1\\atop\nk}\\right|+(\\alpha' n+\\beta' k+\\gamma')\\left|{n-1\\atop k-1}\\right|+I_{n=k=0}$$\nfor $0\\le k\\le n$ and six parameters\n$\\alpha,\\beta,\\gamma,\\alpha'\\beta',\\gamma'$. Since then, a number of authors\ninvestigated various properties of the solutions of these recurrences. In this\nnote we consider a probabilistic aspect, namely we consider the limiting\ndistributions of sequences of integer valued random variables naturally\nassociated with the solutions of such recurrences. We will give a complete\ndescription of the limiting behavior when $\\alpha'=0$ and the remaining five\nparameters are non--negative.",
        "In both neuroscience and artificial intelligence, popular functional\nframeworks and neural network formulations operate by making use of extrinsic\nerror measurements and global learning algorithms. Through a set of conjectures\nbased on evolutionary insights on the origin of cellular adaptive mechanisms,\nwe reinterpret the core meaning of sensory signals to allow the brain to be\ninterpreted as a negative feedback control system, and show how this could lead\nto local learning algorithms without the need for global error correction\nmetrics. Thereby, a sufficiently good minima in sensory activity can be the\ncomplete reward signal of the network, as well as being both necessary and\nsufficient for biological learning to arise. We show that this method of\nlearning was likely already present in the earliest unicellular life forms on\nearth. We show evidence that the same principle holds and scales to\nmulticellular organisms where it in addition can lead to division of labour\nbetween cells. Available evidence shows that the evolution of the nervous\nsystem likely was an adaptation to more effectively communicate intercellular\nsignals to support such division of labour. We therefore propose that the same\nlearning principle that evolved already in the earliest unicellular life forms,\ni.e. negative feedback control of externally and internally generated sensor\nsignals, has simply been scaled up to become a fundament of the learning we see\nin biological brains today. We illustrate diverse biological settings, from the\nearliest unicellular organisms to humans, where this operational principle\nappears to be a plausible interpretation of the meaning of sensor signals in\nbiology, and how this relates to current neuroscientific theories and findings.",
        "Diffusion is a prominent source of noise affecting nuclear magnetic resonance\nat the nanometer scale (nano-NMR), preventing high resolution studies of\nunpolarized liquid samples. Actively managing diffusion noise through, for\nexample, sample confinement, is likely to unveil alternative noise sources\nwhich so far have been disregarded, as they occur in longer time-scales and\nare, consequently, masked by diffusion. These secondary noise sources could\ndiminish the advantages provided by sample confinement but, on the other hand,\nthey can provide with valuable information about the behavior of the sample and\nits interactions. In this article, we study for the first time and in detail\ntwo noise models for confined nano-NMR, namely, surface interactions and\nporosity, and discuss their implications for typical nano-NMR experiments.",
        "We employ the cluster Gutzwiller mean-field method to investigate the\nground-state phase diagrams and demixing effects in binary boson mixtures with\npair hopping in synthetic dimensions. Our study reveals two novel interspecies\npaired superfluid phases: the paired super-counter-fluid (PSCF) phase,\nfeaturing pairs of two particles of one species and two holes of the other, and\nthe SCF* phase, which combines PSCF and super-counter-fluid (SCF) orders. These\nphases provide new insights into XY ferromagnet states from a pseudo-spin\nperspective, with SCF* and PSCF states corresponding to different XY\nferromagnet phases depending on particle filling. We also identify a quantum\nquadruple critical point in the interexchange asymmetric case. Importantly, we\ndemonstrate that the mixed-demixed critical point is phase-dependent due to\npairing hopping, differing from normal two-component bosonic systems.\nExperimental schemes to observe these novel phases are proposed."
      ]
    }
  }
]