[
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey",
    "start_abstract":"Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Medical diffusion on a budget: textual inversion for medical image generation"
      ],
      "abstract":[
        "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Leveraging 13C NMR spectroscopic data derived from SMILES to predict the\n  functionality of small biomolecules by machine learning: a case study on\n  human Dopamine D1 receptor antagonists",
        "HIV\/AIDS Suppression in North America: Intervention Plans and\n  Cost-Effectiveness of UNAIDS 90-90-90 and 95-95-95 Targets",
        "From FAIR to CURE: Guidelines for Computational Models of Biological\n  Systems",
        "Toxicological Evaluation of Phytochemicals and Heavy Metals in Ficus\n  exasperata Vahl (Sandpaper) Leaves obtained in Birnin Kebbi, Nigeria",
        "Comprehensive Analysis of Bioactive Peptides from Cuminum cyminum L.\n  Seeds: Sequence Identification and Pharmacological Evaluation",
        "Modeling and Optimization of Insulin Injection for Type-1 Diabetes\n  Mellitus Management",
        "Co-Translational mRNA Decay in Plants: Recent advances and future\n  directions",
        "Reproductive system and interaction with fauna in a Mediterranean\n  Pyrophite shrub",
        "A new perspective on brain stimulation interventions: Optimal stochastic\n  tracking control of brain network dynamics",
        "Data Sharing in the PRIMED Consortium: Design, implementation, and\n  recommendations for future policymaking",
        "Conditional Success of Adaptive Therapy: The Role of Treatment-Holiday\n  Thresholds Revealed by Mathematical Modeling",
        "Toward a General Theory for the Scaling and Universality of Thermal\n  Responses in Biology",
        "Human Genome Book: Words, Sentences and Paragraphs",
        "Bayesian optimisation of poloidal field coil positions in tokamaks",
        "Quantum metric non-linear Hall effect in an antiferromagnetic\n  topological insulator thin-film EuSn2As2",
        "Monotone conservative strategies in data assimilation",
        "Swimming mode determines how well mesoscale swimmers shield their odor\n  in turbulence",
        "On the Picard numbers of moduli spaces of one-dimensional sheaves on\n  surfaces",
        "Regular evolution algebras are closed under subalgebras",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "The unification of gravity and the spin-1 field",
        "Void spin distribution as a powerful probe of $\\sigma_{8}$",
        "Decay of mass for a semilinear heat equation with mixed local-nonlocal\n  operators",
        "PUATE: Semiparametric Efficient Average Treatment Effect Estimation from\n  Treated (Positive) and Unlabeled Units",
        "The TES-based Cryogenic AntiCoincidence Detector of ATHENA X-IFU:\n  Validation of the thermal end-to-end simulator towards the updated\n  Demonstration Model (DM 1.1)",
        "OGBoost: A Python Package for Ordinal Gradient Boosting",
        "Regulation of Algorithmic Collusion, Refined: Testing Pessimistic\n  Calibrated Regret",
        "Spectral synthesis for exponentials in weighted $L^2$-spaces"
      ],
      "abstract":[
        "This study contributes to ongoing research which aims to predict small\nbiomolecule functionality using Carbon-13 Nuclear Magnetic Resonance ($^{13}$C\nNMR) spectrum data and machine learning (ML). The approach was demonstrated\nusing a bioassay on human dopamine D1 receptor antagonists. The Simplified\nMolecular Input Line Entry System (SMILES) notations of compounds in this\nbioassay were extracted and converted into spectroscopic data by software\ndesigned for this purpose. The resulting data was then used for ML with\nscikit-learn algorithms. The ML models were trained by 27,756 samples and\ntested by 5,466. From the estimators K-Nearest neighbor, Decision Tree\nClassifier, Random Forest Classifier, Gradient Boosting Classifier, XGBoost\nClassifier, and Support Vector Classifier, the last performed the best,\nachieving 71.5 % accuracy, 77.4 % precision, 60.6% recall, 68 % F1, 71.5 % ROC,\nand 0.749 cross-validation score with 0.005 standard deviation. The methodology\ncan be applied to predict any functionality of any compound when relevant data\nare available. It was hypothesized also that increasing the number of samples\nwould increase accuracy. In addition to the SMILES $^{13}$C NMR spectrum ML\nmodel, the time- , and cost-efficient CID_SID ML model was developed. This\nmodel allows researchers who have developed a compound and obtained its PubChem\nCID and SID to check whether their compound is also a human dopamine D1\nreceptor antagonist based solely on the PubChem identifiers. The metrics of the\nCID_SID ML model were 80.2% accuracy, 86.3% precision, 70.4% recall, 77.6% F1,\n79.9% ROC, five-fold cross-validation score of 0.8071 with 0.0047 Standard\ndeviation.",
        "This study utilizes mathematical models to assess progress toward achieving\nthe UNAIDS 90-90-90 and 95-95-95 targets aimed at managing and eradicating\nHIV\/AIDS. It contrasts stochastic and deterministic models, focusing on their\nutility in optimizing public health strategies. Stochastic models account for\nreal-world unpredictability, offering more realistic insights compared to\ndeterministic approaches. The 95-95-95 targets aim for 95\\% of people living\nwith HIV to know their status, 95\\% of those diagnosed to receive\nantiretroviral therapy (ART), and 95\\% of those on ART to achieve viral\nsuppression. These benchmarks are critical for reducing transmission and\nimproving health outcomes. This analysis establishes the basic reproduction\nnumber ($R_0$) to guide interventions and examines the stability of\ndisease-free and endemic equilibria, providing a foundation for applying\noptimal control strategies to minimize HIV prevalence effectively and\ncost-efficiently. Moreover, the data for this study was sourced from the\nofficial UNAIDS website, focusing on North America. An innovative feature of\nthis study is the application of the Stochastic method, which enhances model\naccuracy and operational efficiency in simulating HIV transmission under\nvarious interventions. This research offers actionable insights for\npolicymakers and contributes to global efforts to achieve the 95-95-95 targets\nby 2030, advancing the fight against HIV\/AIDS.",
        "Guidelines for managing scientific data have been established under the FAIR\nprinciples requiring that data be Findable, Accessible, Interoperable, and\nReusable. In many scientific disciplines, especially computational biology,\nboth data and models are key to progress. For this reason, and recognizing that\nsuch models are a very special type of 'data', we argue that computational\nmodels, especially mechanistic models prevalent in medicine, physiology and\nsystems biology, deserve a complementary set of guidelines. We propose the CURE\nprinciples, emphasizing that models should be Credible, Understandable,\nReproducible, and Extensible. We delve into each principle, discussing\nverification, validation, and uncertainty quantification for model credibility;\nthe clarity of model descriptions and annotations for understandability;\nadherence to standards and open science practices for reproducibility; and the\nuse of open standards and modular code for extensibility and reuse. We outline\nrecommended and baseline requirements for each aspect of CURE, aiming to\nenhance the impact and trustworthiness of computational models, particularly in\nbiomedical applications where credibility is paramount. Our perspective\nunderscores the need for a more disciplined approach to modeling, aligning with\nemerging trends such as Digital Twins and emphasizing the importance of data\nand modeling standards for interoperability and reuse. Finally, we emphasize\nthat given the non-trivial effort required to implement the guidelines, the\ncommunity moves to automate as many of the guidelines as possible.",
        "Objective: Ficus exasperata Vahl (Sandpaper tree) is extensively used in\nNigeria to treat diseases, but a dearth of documentation about its toxicity\nexists. This information is crucial because pollutants can contaminate\nmedicinal plants. This study determined the heavy metal and phytochemical\ncontent of methanolic leaf extract of F exasperata obtained in Birnin Kebbi,\nNigeria. Material and Methods: The lethality of the plant was also assessed\nusing 70 wild shrimps divided equally into seven groups. Group 1 (negative\ncontrol), groups 2 and 3 (positive controls) were exposed to 500 and 1000ppm of\nformaldehyde, respectively; and groups 4-7 were exposed to 1000, 2000, 4000,\nand 8000ppm of extracts, respectively, for 96 hours. Results: The\nphytochemistry revealed high levels of flavonoids and saponins and moderate\nlevels of tannins and phenols. The heavy metal analysis revealed non-tolerable\nlevels of cadmium, copper, and lead, while zinc was within the tolerable limit.\nThe negative control recorded 10% mortality, 1000 and 2000 ppm (20% each),\n4000ppm (70%), and 8000 ppm (100%). Conclusion: These results inferred safe\ndoses of the plant's extract in low and medium concentrations but toxic and\nfatal at high doses over a period of time. Consumers are advised to seek an\nexpert's guidance before using it.",
        "Cuminum cyminum L. (cumin) is a medicinal and edible plant widely used in\ntraditional Chinese medicine (TCM) for treating various ailments, including\ndiarrhea, abdominal pain, inflammation, asthma, and diabetes. While previous\nresearch has primarily focused on its essential oils, studies on its\nprotein-derived bioactive peptides remain limited. In this study, we employed\nan innovative extraction method to isolate peptides from cumin seeds for the\nfirst time and screened their biological activities, revealing significant\nantimicrobial, antioxidant, and hypoglycemic properties. Guided by bioactivity,\nwe utilized advanced separation and structural identification techniques,\nincluding Matrix-Assisted Laser Desorption\/Ionization Time-of-Flight Mass\nSpectrometry (MALDI-TOF\/TOF MS\/MS), to systematically purify and characterize\ncumin-derived peptides. A total of 479 unique peptide sequences were identified\nusing Mascot software and the SwissProt\/UniProt_Bos databases. Among these, 15\nhighly bioactive peptides were selected for further analysis based on\nbioactivity and toxicity predictions using PeptideRanker and ToxinPred.\nStructural characterization revealed key features, such as {\\alpha}-helices and\n\\b{eta}-sheets, associated with their multifunctional activities. This study\nprovides the first comprehensive analysis of bioactive peptides from Cuminum\ncyminum L. seeds, elucidating their potential as antimicrobial, antioxidant,\nand hypoglycemic agents. These findings not only clarify the pharmacological\nbasis of cumin's traditional uses but also lay a theoretical foundation for the\ndevelopment of novel therapeutic agents from this medicinal plant.",
        "Diabetes mellitus is a global health crisis characterized by poor blood sugar\nregulation, impacting millions of people worldwide and leading to severe\ncomplications and mortality. Although Type 1 Diabetes Mellitus (T1DM) has a\nlower number of cases compared to other forms of diabetes, it is often\ndiagnosed at a young age and requires lifelong exogenous insulin\nadministration. In this paper, we focus on understanding the interaction of\ninsulin and glucose molecules within the subcutaneous layer, which is crucial\nfor blood sugar control in T1DM patients. Specifically, we propose a\ncomprehensive model to characterize the insulin-glucose system within the\nsubcutaneous layer, incorporating a multicellular molecular communication\nsystem. We then divide the T1DM system into insulin and glucose subsystems and\nderive the end-to-end expression for insulin-glucose interaction in the\nsubcutaneous layer. We further validate the insulin-glucose interaction\nanalysis with an agent-based simulator. As effectively managing postprandial\nglucose levels is crucial for individuals with T1DM to safeguard their overall\nhealth and avert short-term and long-term complications, we also derive the\noptimal insulin administration time based on the derived glucose response via\nthe Lagrange multiplier and gradient descent ascent method. This allows us to\nexplore the impact of different types of insulin and dietary management on\nblood sugar levels. Simulation results confirm the correctness of our proposed\nmodel and the effectiveness of our optimized effective time window for\ninjecting insulin in individuals with T1DM.",
        "Tight regulation of messenger RNA (mRNA) stability is essential to ensure\naccurate gene expression in response to developmental and environmental cues.\nmRNA stability is controlled by mRNA decay pathways, which have traditionally\nbeen proposed to occur independently of translation. However, the recent\ndiscovery of a co-translational mRNA decay pathway (also known as CTRD) reveals\nthat mRNA translation and decay can be coupled. While being translated, a mRNA\ncan be targeted for degradation. This pathway was first described in yeast and\nrapidly identified in several plant species. This review explores recent\nadvances in our understanding of CTRD in plants, emphasizing its regulation and\nits importance for development and stress response. The different metrics used\nto assess CTRD activity are also presented. Furthermore, this review outlines\nfuture directions to explore the importance of mRNA decay in maintaining mRNA\nhomeostasis in plants.",
        "The ULEX model, in its present state, involves the study of the biomass and\nthe population of the shrub Ulex parviflorus Pourret, but while being a dynamic\nmodel, it is static in the sense that it does not imply the appearance of new\nspecimens of this plant. As a complement to the ULEX model in its two dynamic\nand spatial aspects, and with the idea of extending the model, the authors have\nintroduced from a biological and statistical point of view four characteristics\nof this species, flowering, pollination, fructification, taking special\ninterest in the role played by the pollinators (bees) and dispersion of seeds.",
        "Network control theory (NCT) has recently been utilized in neuroscience to\nfacilitate our understanding of brain stimulation effects. A particularly\nuseful branch of NCT is optimal control, which focuses on applying theoretical\nand computational principles of control theory to design optimal strategies to\nachieve specific goals in neural processes. However, most existing research\nfocuses on optimally controlling brain network dynamics from the original state\nto a target state at a specific time point. In this paper, we present the first\ninvestigation of introducing optimal stochastic tracking control strategy to\nsynchronize the dynamics of the brain network to a target dynamics rather than\nto a target state at a specific time point. We utilized fMRI data from healthy\ngroups, and cases of stroke and post-stroke aphasia. For all participants, we\nutilized a gradient descent optimization method to estimate the parameters for\nthe brain network dynamic system. We then utilized optimal stochastic tracking\ncontrol techniques to drive original unhealthy dynamics by controlling a\ncertain number of nodes to synchronize with target healthy dynamics. Results\nshow that the energy associated with optimal stochastic tracking control is\nnegatively correlated with the intrinsic average controllability of the brain\nnetwork system, while the energy of the optimal state approaching control is\nsignificantly related to the target state value. For a 100-dimensional brain\nnetwork system, controlling the five nodes with the lowest tracking energy can\nachieve relatively acceptable dynamics control effects. Our results suggest\nthat stochastic tracking control is more aligned with the objective of brain\nstimulation interventions, and is closely related to the intrinsic\ncharacteristics of the brain network system, potentially representing a new\ndirection for future brain network optimal control research.",
        "Sharing diverse genomic and other biomedical datasets is critical to advance\nscientific discoveries and their equitable translation to improve human health.\nHowever, data sharing remains challenging in the context of legacy datasets,\nevolving policies, multi-institutional consortium science, and international\nstakeholders. The NIH-funded Polygenic Risk Methods in Diverse Populations\n(PRIMED) Consortium was established to improve the performance of polygenic\nrisk estimates for a broad range of health and disease outcomes with global\nimpacts. Improving polygenic risk score performance across genetically diverse\npopulations requires access to large, diverse cohorts. We report on the design\nand implementation of data sharing policies and procedures developed in PRIMED\nto aggregate and analyze data from multiple, heterogeneous sources while\nadhering to existing data sharing policies for each integrated dataset. We\ndescribe two primary data sharing mechanisms: coordinated dbGaP applications\nand a Consortium Data Sharing Agreement, as well as provide alternatives when\nindividual-level data cannot be shared within the Consortium (e.g., federated\nanalyses). We also describe technical implementation of Consortium data sharing\nin the NHGRI Analysis Visualization and Informatics Lab-space (AnVIL) cloud\nplatform, to share derived individual-level data, genomic summary results, and\nmethods workflows with appropriate permissions. As a Consortium making\nsecondary use of pre-existing data sources, we also discuss challenges and\npropose solutions for release of individual- and summary-level data products to\nthe broader scientific community. We make recommendations for ongoing and\nfuture policymaking with the goal of informing future consortia and other\nresearch activities.",
        "Adaptive therapy (AT) improves cancer treatment by controlling the\ncompetition between sensitive and resistant cells through treatment holidays.\nThis study highlights the critical role of treatment-holiday thresholds in AT\nfor tumors composed of drug-sensitive and resistant cells. Using a\nLotka-Volterra model, the research compares AT with maximum tolerated dose\ntherapy and intermittent therapy, showing that AT's success largely depends on\nthe threshold at which treatment is paused and resumed, as well as on the\ncompetition between sensitive and resistant cells. Three scenarios of\ncomparison between AT and other therapies are identified: uniform-decline,\nconditional-improve, and uniform-improve, illustrating that optimizing the\ntreatment-holiday threshold is crucial for AT effectiveness. Tumor composition,\nincluding initial tumor burden and the proportion of resistant cells,\ninfluences outcomes. Adjusting threshold values enables AT to suppress\nresistant subclones, preserving sensitive cells, ultimately improving\nprogression-free survival. These findings emphasize the importance of\npersonalized treatment strategies potentially enhancing long-term therapeutic\noutcomes.",
        "We developed a theory showing that under appropriate normalizations and\nrescalings, temperature response curves show a remarkably regular behavior and\nfollow a general, universal law. The impressive universality of temperature\nresponse curves remained hidden due to various curve-fitting models not\nwell-grounded in first principles. In addition, this framework has the\npotential to explain the origin of different scaling relationships in thermal\nperformance in biology, from molecules to ecosystems. Here, we summarize the\nbackground, principles and assumptions, predictions, implications, and possible\nextensions of this theory.",
        "Since the completion of the human genome sequencing project in 2001,\nsignificant progress has been made in areas such as gene regulation editing and\nprotein structure prediction. However, given the vast amount of genomic data,\nthe segments that can be fully annotated and understood remain relatively\nlimited. If we consider the genome as a book, constructing its equivalents of\nwords, sentences, and paragraphs has been a long-standing and popular research\ndirection. Recently, studies on transfer learning in large language models have\nprovided a novel approach to this challenge.Multilingual transfer ability,\nwhich assesses how well models fine-tuned on a source language can be applied\nto other languages, has been extensively studied in multilingual pre-trained\nmodels. Similarly, the transfer of natural language capabilities to \"DNA\nlanguage\" has also been validated. Building upon these findings, we first\ntrained a foundational model capable of transferring linguistic capabilities\nfrom English to DNA sequences. Using this model, we constructed a vocabulary of\nDNA words and mapped DNA words to their English equivalents.Subsequently, we\nfine-tuned this model using English datasets for paragraphing and sentence\nsegmentation to develop models capable of segmenting DNA sequences into\nsentences and paragraphs. Leveraging these models, we processed the GRCh38.p14\nhuman genome by segmenting, tokenizing, and organizing it into a \"book\"\ncomprised of genomic \"words,\" \"sentences,\" and \"paragraphs.\" Additionally,\nbased on the DNA-to-English vocabulary mapping, we created an \"English version\"\nof the genomic book. This study offers a novel perspective for understanding\nthe genome and provides exciting possibilities for developing innovative tools\nfor DNA search, generation, and analysis.",
        "The tokamak is a world-leading concept for producing sustainable energy via\nmagnetically-confined nuclear fusion. Identifying where to position the magnets\nwithin a tokamak, specifically the poloidal field (PF) coils, is a design\nproblem which requires balancing a number of competing economic, physical, and\nengineering objectives and constraints. In this paper, we show that\nmulti-objective Bayesian optimisation (BO), an iterative optimisation technique\nutilising probabilistic machine learning models, can effectively explore this\ncomplex design space and return several optimal PF coil sets. These solutions\nspan the Pareto front, a subset of the objective space that optimally satisfies\nthe specified objective functions. We outline an easy-to-use BO framework and\ndemonstrate that it outperforms alternative optimisation techniques while using\nsignificantly fewer computational resources. Our results show that BO is a\npromising technique for fusion design problems that rely on computationally\ndemanding high-fidelity simulations.",
        "The quantum geometric structure of electrons introduces fundamental insights\ninto understanding quantum effects in materials. One notable manifestation is\nthe non-linear Hall effect (NLHE), which has drawn considerable interest for\nits potential to overcome the intrinsic limitations of semiconductor diodes at\nlow input power and high frequency. In this study, we investigate NLHE stemming\nfrom the real part of the quantum geometric tensor, specifically the quantum\nmetric, in an antiferromagnetic topological material, EuSn2As2, using density\nfunctional theory. Our calculations predict a remarkable NLHE arising from a\nsymmetry-protected, single Type-II surface Dirac cone in the\neven-numbered-layer two-dimensional slab thin-film, yielding a non-linear Hall\nconductivity exceeding 20 mA\/V2-an order of magnitude larger than previously\nreported. This single Dirac band dispersion represents the simplest model for\ngenerating NLHE, positioning the EuSn2As2 thin-film as a hydrogen atom for NLHE\nsystems. Additionally, we observe NLHE from band-edge states near the Fermi\nlevel. Our findings also reveal that 30% phosphorus (P) doping can double the\nnon-linear Hall conductivity. With its substantial and tunable NLHE, EuSn2As2\nthin-films present promising applications in antiferromagnetic spintronics and\nrectification devices.",
        "This paper studies whether numerically preserving monotonic properties can\noffer modelling advantages in data assimilation, particularly when the signal\nor data is a realization of a stochastic partial differential equation (SPDE)\nor partial differential equation (PDE) with a monotonic property. We\ninvestigate the combination of stochastic Strong Stability Preserving (SSP)\ntime-stepping, nonlinear solving strategies and data assimilation. Experimental\nresults indicate that a particle filter whose ensemble members are solved\nmonotonically can increase forecast skill when the reference data (not\nnecessarily observations) also has a monotone property. Additionally, more\nadvanced techniques used to avoid the degeneracy of the filter\n(tempering-jittering) are shown to be compatible with a conservative monotone\napproach.",
        "Marine organisms manipulate their surrounding flow through their swimming\ndynamics, which affects the transport of their own odor cues. We demonstrate by\ndirect numerical simulations how a group of mesoscale swimmers immersed in a\nturbulent flow alters the shape of the odor plume they release in the water.\nOdor mixing is enhanced by increased velocity fluctuations and a\nswimmer-induced flow circulation which widen the odor plume at close range\nwhile speeding up dilution of the chemical trace. Beyond a short-range increase\nin the likelihood of being detected, swimming considerably reduces detections\nwith effects that can persist at distances of the order of ten times the size\nof the group or more. We find that puller-like swimmers are more effective at\nolfactory shielding than pusher-like swimmers. We trace this difference back to\nthe dynamics at the swimmer location, which tends to trap odor at the source\nfor pushers and to dilute it for pullers. Olfactory shielding is robust to\nchanges in the conditions, and is more pronounced for weak turbulent Reynolds\nnumbers and large swimmer Reynolds numbers. Our results suggest that olfactory\nshielding may play a role in the emergence of different swimming modalities by\nmarine organisms.",
        "Motivated by asymptotic phenomena of moduli spaces of higher rank stable\nsheaves on algebraic surfaces, we study the Picard number of the moduli space\nof one-dimensional stable sheaves supported in a sufficiently positive divisor\nclass on a surface. We give an asymptotic lower bound of the Picard number in\ngeneral. In some special cases, we show that this lower bound is attained based\non the geometry of moduli spaces of stable pairs and relative Hilbert schemes\nof points. Additionally, we discuss several related questions and provide\nexamples where the asymptotic irreducibility of the moduli space fails,\nhighlighting a notable distinction from the higher rank case.",
        "The main goal of this note is to show that subalgebras of regular evolution\nalgebras are themselves evolution algebras. This allows us to assume, without\nloss of generality, that every subalgebra in the regular setting has a basis\nconsisting of vectors with disjoint supports. Finally, we use this result to\ncharacterise the existence of codimension-one subalgebras in regular evolution\nalgebras.",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "Unifying the massive spin-1 field with gravity requires the implementation of\na regular vector field that satisfies the spin-1 Proca equation and is a\nfundamental part of the spacetime metric. That vector field is one of the pair\nof vectors in the line element field (\\textbf{X},-\\textbf{X}), which is\nparamount to the existence of all Lorentzian metrics and Modified General\nRelativity (MGR). Symmetrization of the spin-1 Klein-Gordon equation in a\ncurved Lorentzian spacetime introduces the Lie derivative of the metric along\nthe flow of one of the regular vectors in the line element field. The Proca\nequation in curved spacetime can then be described geometrically in terms of\nthe line element vector, the Lie derivative of the Lorentzian metric, and the\nRicci tensor, which unifies gravity and the spin-1 field. Related issues\nconcerning charge conservation and the Lorenz constraint, singularities in a\nspherically symmetric curved spacetime, and geometrical implications of MGR to\nquantum theory are discussed. A geometrical unification of gravity with quantum\nfield theory is presented.",
        "We present a numerical proof of the concept that the void spin distributions\ncan in principle provide a tight constraint on the amplitude of matter density\nfluctuation on the scale of $8\\,h^{-1}{\\rm Mpc}$ ($\\sigma_{8}$) without being\nseverely deteriorated by the degeneracies of $\\sigma_{8}$ with cold dark matter\ndensity parameter multiplied by the dimensionless Hubble parameter square\n($\\Omega_{\\rm cdm}h^{2}$), total neutrino mass ($M_{\\nu}$) and dark energy\nequation of state ($w$). Applying the Void-Finder algorithm to a total of $15$\nAbacusSummit $N$-body simulations of $15$ different cosmological models, we\nidentify the giant voids to measure their spins, defined as the magnitudes of\nrescaled specific angular momenta of void halos. The $15$ cosmologies include\nthe Planck $\\Lambda$CDM and $14$ non-Planck models, each of which differs among\none another only in one of $\\{\\sigma_{8},\\ \\Omega_{\\rm cdm}h^{2},\\ M_{\\nu},\\\nw\\}$. The probability density distribution of void spins is determined for each\nmodel and found to be well approximated by the generalized Gamma distribution\nwith two characteristic parameters, $k$ and $\\theta$. It turns out that the\nbest-fit values of $k$ and $\\theta$ exhibit very sensitive dependences only on\n$\\sigma_{8}$, being almost insensitive to $\\Omega_{\\rm cdm}h^{2}$, $M_{\\nu}$,\n$w$. This exclusive $\\sigma_{8}$-dependence of the void spin distributions is\nconfirmed to be robust against the variation of the mass and number cuts of\nvoid halos. We also test an observational feasibility of estimating the void\nspins from real data on the galaxy redshifts.",
        "In this paper, we are concerned with the Cauchy problem for the\nreaction-diffusion equation $\\partial_t u+t^\\beta\\mathcal{L} u= - h(t)u^p$\nposed on $\\mathbb{R}^N$, driven by the mixed local-nonlocal operator\n$\\mathcal{L}=-\\Delta+(-\\Delta)^{\\alpha\/2}$, $\\alpha\\in(0,2)$, and supplemented\nwith a nonnegative integrable initial data, where $p>1$, $\\beta\\geq 0$, and\n$h:(0,\\infty)\\to(0,\\infty)$ is a locally integrable function. We study the\nlarge time behavior of non-negative solutions and show that the nonlinear term\ndetermines the large time asymptotic for $p\\leq 1+{\\alpha}\/{N(\\beta+1)},$ while\nthe classical\/anomalous diffusion effects win if $p>1+{\\alpha}\/{N(\\beta+1)}$.",
        "The estimation of average treatment effects (ATEs), defined as the difference\nin expected outcomes between treatment and control groups, is a central topic\nin causal inference. This study develops semiparametric efficient estimators\nfor ATE estimation in a setting where only a treatment group and an unknown\ngroup-comprising units for which it is unclear whether they received the\ntreatment or control-are observable. This scenario represents a variant of\nlearning from positive and unlabeled data (PU learning) and can be regarded as\na special case of ATE estimation with missing data. For this setting, we derive\nsemiparametric efficiency bounds, which provide lower bounds on the asymptotic\nvariance of regular estimators. We then propose semiparametric efficient ATE\nestimators whose asymptotic variance aligns with these efficiency bounds. Our\nfindings contribute to causal inference with missing data and weakly supervised\nlearning.",
        "The Cryogenic AntiCoincidence Detector (CryoAC) is a key element of the X-ray\nIntegral Field Unit (X-IFU) on board the future ATHENA X-ray observatory. It is\na TES-based detector designed to reduce the particle background of the\ninstrument, thereby increasing its sensitivity. The detector design is driven\nby an end-to-end simulator which includes the electro-thermal modelling of the\ndetector and the dynamics of its readout chain. Here, we present the\nmeasurements carried out on the last CryoAC single pixel prototype, namely\nDM127, in order to evaluate the critical thermal parameters of the detector and\nconsequently to tune and validate the CryoAC end-to-end simulator.",
        "This paper introduces OGBoost, a scikit-learn-compatible Python package for\nordinal regression using gradient boosting. Ordinal variables (e.g., rating\nscales, quality assessments) lie between nominal and continuous data,\nnecessitating specialized methods that reflect their inherent ordering. Built\non a coordinate-descent approach for optimization and the latent-variable\nframework for ordinal regression, OGBoost performs joint optimization of a\nlatent continuous regression function (functional gradient descent) and a\nthreshold vector that converts the latent continuous value into discrete class\nprobabilities (classical gradient descent). In addition to the stanadard\nmethods for scikit-learn classifiers, the GradientBoostingOrdinal class\nimplements a \"decision_function\" that returns the (scalar) value of the latent\nfunction for each observation, which can be used as a high-resolution\nalternative to class labels for comparing and ranking observations. The class\nhas the option to use cross-validation for early stopping rather than a single\nholdout validation set, a more robust approach for small and\/or imbalanced\ndatasets. Furthermore, users can select base learners with different underlying\nalgorithms and\/or hyperparameters for use throughout the boosting iterations,\nresulting in a `heterogeneous' ensemble approach that can be used as a more\nefficient alternative to hyperparameter tuning (e.g. via grid search). We\nillustrate the capabilities of OGBoost through examples, using the wine quality\ndataset from the UCI respository. The package is available on PyPI and can be\ninstalled via \"pip install ogboost\".",
        "We study the regulation of algorithmic (non-)collusion amongst sellers in\ndynamic imperfect price competition by auditing their data as introduced by\nHartline et al. [2024].\n  We develop an auditing method that tests whether a seller's pessimistic\ncalibrated regret is low. The pessimistic calibrated regret is the highest\ncalibrated regret of outcomes compatible with the observed data. This method\nrelaxes the previous requirement that a pricing algorithm must use\nfully-supported price distributions to be auditable. This method is at least as\npermissive as any auditing method that has a high probability of failing\nalgorithmic outcomes with non-vanishing calibrated regret. Additionally, we\nstrengthen the justification for using vanishing calibrated regret, versus\nvanishing best-in-hindsight regret, as the non-collusion definition, by showing\nthat even without any side information, the pricing algorithms that only\nsatisfy weaker vanishing best-in-hindsight regret allow an opponent to\nmanipulate them into posting supra-competitive prices. This manipulation cannot\nbe excluded with a non-collusion definition of vanishing best-in-hindsight\nregret.\n  We motivate and interpret the approach of auditing algorithms from their data\nas suggesting a per se rule. However, we demonstrate that it is possible for\nalgorithms to pass the audit by pretending to have higher costs than they\nactually do. For such scenarios, the rule of reason can be applied to bound the\nrange of costs to those that are reasonable for the domain.",
        "We prove that for a some natural class of weights $\\K$ and any weighted space\n$$L^2(w) = \\left\\{ f : (-\\pi,\\pi) \\to \\CC \\colon\\int_{-\\pi}^{\\pi} {{|f(t)|^2}\n\\over {w(t)}} dt < \\infty \\right\\},$$ where $w \\in \\K$, there exists a complete\nand minimal system $\\{e^{i\\lambda_nt}\\}_{n\\in \\Z}$ of exponentials, which does\nnot admit spectral synthesis."
      ]
    }
  },
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction",
    "start_abstract":"Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
      ],
      "abstract":[
        "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Stochastic Time to Extinction of an SIQS Epidemic Model with Quiescence",
        "Unsupervised detection and fitness estimation of emerging SARS-CoV-2\n  variants. Application to wastewater samples (ANRS0160)",
        "Functional Correspondences in the Human and Marmoset Visual Cortex\n  During Movie Watching: Insights from Correlation, Redundancy, and Synergy",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Derivation from kinetic theory and 2-D pattern analysis of chemotaxis\n  models for Multiple Sclerosis",
        "Multicellular self-organization in Escherichia coli",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "R3F: An R package for evolutionary dates, rates, and priors using the\n  relative rate framework",
        "Bowel Incision Closure with a Semi-Automated Robot-Assisted Laser Tissue\n  Soldering System",
        "Language modulates vision: Evidence from neural networks and human\n  brain-lesion models",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "FOCUS: First Order Concentrated Updating Scheme",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Equivalence between exponential concentration in quantum machine\n  learning kernels and barren plateaus in variational algorithms",
        "The Marginal Importance of Distortions and Alignment in CASSI systems",
        "Multi-scale physics of cryogenic liquid helium-4: Inverse\n  coarse-graining properties of smoothed particle hydrodynamics",
        "Stability of difference equations with interspecific density dependence,\n  competition, and maturation delays",
        "Design of a quantum diamond microscope with efficient scanning confocal\n  readout",
        "Timing and spectral studies of the Be\/X-ray binary EXO 2030+375 using\n  Insight-HXMT observations",
        "Numerical Study On Temperature Variations Of Superheated Steam Flowing\n  Through A Regulation Valve",
        "Visualizing quantum entanglement in Bose-Einstein condensates without\n  state vectors",
        "Runout of liquefaction-induced tailings dam failure: Influence of\n  earthquake motions and residual strength",
        "Multifunctional Altermagnet with Large Out-of-Plane Piezoelectric\n  Response in Janus V$_{2}$AsBrO Monolayer",
        "On finitary power monoids of linearly orderable monoids",
        "On the role of 5-wave resonances in the nonlinear dynamics of the\n  Fermi-Pasta-Ulam-Tsingou lattice",
        "Influences of accretion flow and dilaton charge on the images of\n  Einstein-Maxwell-dilation black hole"
      ],
      "abstract":[
        "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
        "Repeated waves of emerging variants during the SARS-CoV-2 pandemics have\nhighlighted the urge of collecting longitudinal genomic data and developing\nstatistical methods based on time series analyses for detecting new threatening\nlineages and estimating their fitness early in time. Most models study the\nevolution of the prevalence of particular lineages over time and require a\nprior classification of sequences into lineages. Such process is prone to\ninduce delays and bias. More recently, few authors studied the evolution of the\nprevalence of mutations over time with alternative clustering approaches,\navoiding specific lineage classification. Most of the aforementioned methods\nare however either non parametric or unsuited to pooled data characterizing,\nfor instance, wastewater samples. In this context, we propose an alternative\nunsupervised method for clustering mutations according to their frequency\ntrajectory over time and estimating group fitness from time series of pooled\nmutation prevalence data. Our model is a mixture of observed count data and\nlatent group assignment and we use the expectation-maximization algorithm for\nmodel selection and parameter estimation. The application of our method to time\nseries of SARS-CoV-2 sequencing data collected from wastewater treatment plants\nin France from October 2020 to April 2021 shows its ability to agnostically\ngroup mutations according to their probability of belonging to B.1.160, Alpha,\nBeta, B.1.177 variants with selection coefficient estimates per group in\ncoherence with the viral dynamics in France reported by Nextstrain. Moreover,\nour method detected the Alpha variant as threatening as early as supervised\nmethods (which track specific mutations over time) with the noticeable\ndifference that, since unsupervised, it does not require any prior information\non the set of mutations.",
        "The world of beauty is deeply connected to the visual cortex, as perception\noften begins with vision in both humans and marmosets. Quantifying functional\ncorrespondences in the visual cortex across species can help us understand how\ninformation is processed in the primate visual cortex, while also providing\ndeeper insights into human visual cortex functions through the study of\nmarmosets. In this study, we measured pairwise and beyond pairwise correlation,\nredundancy, and synergy in movie-driven fMRI data across species. Our first key\nfinding was that humans and marmosets exhibited significant overlaps in\nfunctional synergy. Second, we observed that the strongest functional\ncorrespondences between the human peri-entorhinal and entorhinal cortex (PeEc)\nand the occipitotemporal higher-level visual regions in the marmoset during\nmovie watching reflected a functional synergistic relationship. These regions\nare known to correspond to face-selective areas in both species. Third,\nredundancy measures maintained stable high-order hubs, indicating a steady core\nof shared information processing, while synergy measures revealed a dynamic\nshift from low- to high-level visual regions as interaction increased,\nreflecting adaptive integration. This highlights distinct patterns of\ninformation processing across the visual hierarchy. Ultimately, our results\nreveal the marmoset as a compelling model for investigating visual perception,\ndistinguished by its remarkable functional parallels to the human visual\ncortex.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "In this paper, a class of reaction-diffusion equations for Multiple Sclerosis\nis presented. These models are derived by means of a diffusive limit starting\nfrom a proper kinetic description, taking account of the underlying microscopic\ninteractions among cells. At the macroscopic level, we discuss the necessary\nconditions for Turing instability phenomena and the formation of\ntwo-dimensional patterns, whose shape and stability are investigated by means\nof a weakly nonlinear analysis. Some numerical simulations, confirming and\nextending theoretical results, are proposed for a specific scenario.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "The relative rate framework (RRF) can estimate divergence times from branch\nlengths in a phylogeny, which is the theoretical basis of the RelTime method\nfrequently applied, a relaxed clock approach for molecular dating that scales\nwell for large phylogenies. The use of RRF has also enabled the development of\ncomputationally efficient and accurate methods for testing the autocorrelation\nof lineage rates in a phylogeny (CorrTest) and selecting data-driven parameters\nof the birth-death speciation model (ddBD), which can be used to specify priors\nin Bayesian molecular dating. We have developed R3F, an R package implementing\nRRF to estimate divergence times, infer lineage rates, conduct CorrTest, and\nbuild a ddBD tree prior for Bayesian dating in molecular phylogenies. Here, we\ndescribe R3F functionality and explain how to interpret and use its outputs in\nother visualization software and packages, such as MEGA, ggtree, and FigTree.\nUltimately, R3F is intended to enable the dating of the Tree of Life with\ngreater accuracy and precision, which would have important implications for\nstudies of organism evolution, diversification dynamics, phylogeography, and\nbiogeography. Availability and Implementation: The source codes and related\ninstructions for installing and implementing R3F are available from GitHub\n(https:\/\/github.com\/cathyqqtao\/R3F).",
        "Traditional methods for closing gastrointestinal (GI) surgery incisions, like\nsuturing and stapling, present significant challenges, including potentially\nlife-threatening leaks. These techniques, especially in robot-assisted\nminimally invasive surgery (RAMIS), require advanced manual skills. While their\nrepetitive and time-consuming nature makes them suitable candidates for\nautomation, the automation process is complicated by the need for extensive\ncontact with the tissue. Addressing this, we demonstrate a semi-autonomous\ncontactless surgical procedure using our novel Robot-assisted Laser Tissue\nSoldering (RLTS) system on a live porcine bowel. Towards this in-vivo\ndemonstration, we optimized soldering protocols and system parameters in\nex-vivo experiments on porcine bowels and a porcine cadaver. To assess the RLTS\nsystem performance, we compared the pressure at which the anastomosis leaked\nbetween our robotic soldering and manual suturing. With the best setup, we\nadvanced to an in-vivo Heineke Mikulicz closure on small bowel incision in live\npigs and evaluated their healing for two weeks. All pigs successfully\ncompleting the procedure (N=5) survived without leaks and the histology\nindicated mucosal regeneration and fibrous tissue adhesion. This marks the\nfirst in-vivo semi-automated contactless incision closure, paving the way for\nautomating GI surgery incision closure which has the potential to become an\nalternative to traditional methods.",
        "Comparing information structures in between deep neural networks (DNNs) and\nthe human brain has become a key method for exploring their similarities and\ndifferences. Recent research has shown better alignment of vision-language DNN\nmodels, such as CLIP, with the activity of the human ventral occipitotemporal\ncortex (VOTC) than earlier vision models, supporting the idea that language\nmodulates human visual perception. However, interpreting the results from such\ncomparisons is inherently limited due to the \"black box\" nature of DNNs. To\naddress this, we combined model-brain fitness analyses with human brain lesion\ndata to examine how disrupting the communication pathway between the visual and\nlanguage systems causally affects the ability of vision-language DNNs to\nexplain the activity of the VOTC. Across four diverse datasets, CLIP\nconsistently outperformed both label-supervised (ResNet) and unsupervised\n(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,\naligning with the human language network. Analyses of the data of 33 stroke\npatients revealed that reduced white matter integrity between the VOTC and the\nlanguage region in the left angular gyrus was correlated with decreased CLIP\nperformance and increased MoCo performance, indicating a dynamic influence of\nlanguage processing on the activity of the VOTC. These findings support the\nintegration of language modulation in neurocognitive models of human vision,\nreinforcing concepts from vision-language DNN models. The sensitivity of\nmodel-brain similarity to specific brain lesions demonstrates that leveraging\nmanipulation of the human brain is a promising framework for evaluating and\ndeveloping brain-like computer models.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "We formalize a rigorous connection between barren plateaus (BP) in\nvariational quantum algorithms and exponential concentration of quantum kernels\nfor machine learning. Our results imply that recently proposed strategies to\nbuild BP-free quantum circuits can be utilized to construct useful quantum\nkernels for machine learning. This is illustrated by a numerical example\nemploying a provably BP-free quantum neural network to construct kernel\nmatrices for classification datasets of increasing dimensionality without\nexponential concentration.",
        "This paper introduces a differentiable ray-tracing based model that\nincorporates aberrations and distortions to render realistic coded\nhyperspectral acquisitions using Coded-Aperture Spectral Snapshot Imagers\n(CASSI). CASSI systems can now be optimized in order to fulfill simultaneously\nseveral optical design constraints as well as processing constraints. Four\ncomparable CASSI systems with varying degree of optical aberrations have been\ndesigned and modeled. The resulting rendered hyperspectral acquisitions from\neach of these systems are combined with five state-of-the-art hyperspectral\ncube reconstruction processes. These reconstruction processes encompass a\nmapping function created from each system's propagation model to account for\ndistortions and aberrations during the reconstruction process. Our analyses\nshow that if properly modeled, the effects of geometric distortions of the\nsystem and misalignments of the dispersive elements have a marginal impact on\nthe overall quality of the reconstructed hyperspectral data cubes. Therefore,\nrelaxing traditional constraints on measurement conformity and fidelity to the\nscene enables the development of novel imaging instruments, guided by\nperformance metrics applied to the design or the processing of acquisitions. By\nproviding a complete framework for design, simulation and evaluation, this work\ncontributes to the optimization and exploration of new CASSI systems, and more\ngenerally to the computational imaging community.",
        "Our recent numerical studies on cryogenic liquid helium-4 strongly indicate\nthe features of multiscale physics that can be identified using the Landau's\ntwo-fluid model. This study presents the possibility that two-fluid models\nbased on classical and quantum hydrodynamics have a relationship between the\nscale transformation using filtering in large eddy simulations (LES) and the\ninverse scale transformation using smoothed particle hydrodynamics (SPH). We\nshow that the spin angular momentum conservation term, which we previously\nintroduced into the two-fluid model as a quantum mechanical correction,\nformally corresponds to the subgrid-scale (SGS) model, which can be derived\nfrom the scale transformation of the two-fluid model from quantum to classical\nhydrodynamics. Our theoretical analysis shows that solving the two-fluid model\nbased on classical hydrodynamics using SPH can reproduce the microscopic\nfluctuations even at the macroscopic scale because the truncation errors owing\nto the smoothing kernel approximation can substitute the microscopic\nfluctuations. In particular, the fluctuations can be amplified according to the\nsize of the kernel radius at the macroscopic scale. Our further theoretical\nanalysis shows that the Condiff viscosity model can serve as an SGS model and\nincorporate the quantum vortex interactions into the two-fluid model. Our\nresults and discussion provide new insights into the microscopic composition of\nthe cryogenic liquid helium-4 within a multiscale framework. First, a normal\nfluid can be a mixture of inviscid and viscous fluid particles. Second, a flow\nidentified as a normal fluid on the microscopic scale because of the presence\nof molecular viscosity is still classified as an inviscid fluid on the\nhydrodynamic scale because its viscosity is insufficient to produce eddy\nviscosity.",
        "A general system of difference equations is presented for multispecies\ncommunities with density dependent population growth and delayed maturity.\nInterspecific competition, mutualism, predation, commensalism, and amensalism\nare accommodated. A sufficient condition for the local asymptotic stability of\na coexistence equilibrium in this system is then proven. Using this system, the\ngeneralisation of the Beverton-Holt and Leslie-Gower models of competition to\nmultispecies systems with possible maturation delays is presented and shown to\nyield interesting stability properties. The stability of coexistence depends on\nthe relative abundances of the species at the unique interior equilibrium. A\nsufficient condition for local stability is derived that only requires\nintraspecific competition to outweigh interspecific competition. The condition\ndoes not depend on maturation delays. The derived stability properties are used\nto develop a novel estimation approach for the coefficients of interspecific\ncompetition. This approach finds an optimal configuration given the conjecture\nthat coexisting species strive to outcompete competitors and are most likely\nobserved near the stable interior equilibrium with the greatest dampening of\nperturbations. The optimal solution is compared to estimates of niche overlap\nusing an empirical example of malaria mosquito vectors with delayed maturity in\nthe Anopheles gambiae sensu lato species complex.",
        "We introduce the light-sheet confocal quantum diamond microscope (LC-QDM) for\nwidefield 3D quantum sensing with efficient confocal readout. The LC-QDM\nleverages light-sheet illumination and laser scanning confocal methods to\nenable high-resolution, high-speed 3D measurements with nitrogen-vacancy (NV)\ndefects in diamond, combining the best of widefield and confocal modalities in\na single device and eliminating the need for thin-NV-layer diamond chips. We\nperform simulations and measurements of NV initialization and readout times to\nmodel the anticipated performance of the LC-QDM compared to existing QDM\ndesigns. Our findings show that the LC-QDM will provide significant advantages\nfor applications requiring limited laser power.",
        "We report the X-ray spectral and timing analysis of the high mass X-ray\nbinary EXO 2030+375 during the 2021 type-II outburst based on the Insight-HXMT\nobservations. Pulsations can be detected in the energy band of 1-150 keV. The\npulse profile shows energy and luminosity dependence and variability. We\nobserved transitions in the pulse profile shape during the rising and the\ndecaying phase of the outburst. The pulse fraction exhibits an anti-correlation\nwith luminosity and a non-monotonic energy dependence, with a possible dip near\n30 keV during the outburst peak. The hardness-intensity diagrams (7-10 keV\/4-7\nkeV) suggest state transitions during the early and late phases of the\noutburst. These transitions are consistent with the luminosity at which the\npulse profile shape changes occur, revealing the source reaching the critical\nluminosity and transitioning between super-critical and sub-critical accretion\nregimes. We performed the average and phase-resolved spectral analysis, where\nthe flux-resolved average spectra show a stable spectral evolution with\nluminosity. The phase-resolved spectral analysis reveals that the dependence of\nspectral parameters on the pulse phase varies with different luminosities.",
        "Superheated steam is widely employed in various energy systems, particularly\nin power plants, chemical industries, and other applications where\nhigh-temperature and high-pressure steam is essential for efficient energy\nconversion and process control. In these systems, regulation valves are crucial\ncomponents that control the flow of steam, adjusting its pressure and\ntemperature to ensure safe and efficient operation. Accurate understanding and\nprediction of temperature variations within regulation valves are essential for\noptimizing their performance and improving the overall system efficiency. This\nstudy investigates the temperature variations of superheated steam flowing\nthrough a regulation valve using computational fluid dynamics (CFD) simulations\ncombined with Proper Orthogonal Decomposition (POD) techniques. The analysis\nbegins with an examination of the internal flow field parameters, including\ntemperature and pressure, to understand the overall fluid dynamics within the\nvalve. POD is applied to reduce the dimensionality of the CFD results. Singular\nValue Decomposition (SVD) is employed to extract the dominant modes that\ncapture the key flow structures responsible for heat transfer and temperature\nfluctuations. The POD analysis reveals that the most influential modes are\nassociated with regions of high turbulence intensity and significant\ntemperature gradients, which are critical to the thermal performance of the\nsteam flow through the regulation valve. The application of POD to 3D CFD\nresults represents a novel approach, particularly for complex fluid flow models\nsuch as steam flow through regulation valves. The insights gained from this\nstudy have practical implications for the design and optimization of\ntemperature and pressure regulation valves in energy systems, providing a\ntheoretical foundation for enhancing the efficiency and reliability of these\nsystems.",
        "Ring polymer self-consistent field theory is used to calculate the critical\ntemperatures and heat capacities of an ideal Bose gas for an order of magnitude\nmore particles than previously reported. A lambda-transition indicative of\nBose-Einstein condensation is observed as expected. Using a known proof of\nspatial mode entanglement in Bose-Einstein condensates, a relationship between\nboson exchange and quantum entanglement is established. This is done without\nthe use of state vectors, since ring polymer quantum theory uses instead a\nthermal degree of freedom, sometimes called the \"imaginary time\", to map\nclassical statistical mechanics onto non-relativistic quantum mechanics through\nthe theorems of density functional theory. It is shown that quantum phenomena,\nsuch as Bose-Einstein condensation, boson exchange, entanglement and\ncontextuality, can be visualized in terms of merging and separating ring\npolymer threads in thermal-space. A possible extension to fermions is\nmentioned.",
        "This study utilizes a hybrid Finite Element Method (FEM) and Material Point\nMethod (MPM) to investigate the runout of liquefaction-induced flow slide\nfailures. The key inputs to this analysis are the earthquake ground motion,\nwhich induces liquefaction, and the post-liquefaction residual strength. The\ninfluence of these factors on runout is evaluated by subjecting a model of a\ntailings dam to thirty different earthquake motions and by assigning different\nvalues of post-liquefaction residual strength. Ground motions with larger peak\nground accelerations (PGA) generate liquefaction to larger depths, thus\nmobilizing a greater mass of material and resulting in a flow slide with\ngreater runout. However, different ground motions with the same PGA yield\nsignificant variations in the depth of liquefaction, indicating that other\nground motion characteristics (e.g., frequency content) also exert significant\ninfluence over the initiation of liquefaction. Ground motion characteristics of\npeak ground velocity (PGV) and Modified Acceleration Spectrum Intensity (MASI)\nshow a strong correlation to the induced depth of liquefaction because they\ncapture both the intensity and frequency content of the earthquake motion. The\ncomputed runout is directly related to the depth of liquefaction induced by the\nearthquake motion. For dam geometry analyzed, measurable runout occurs when\nliquefaction extends to 10 m depth and the runout is maximized when\nliquefaction extends to about 18 m. Strain-softening of the residual strength\nof the liquefied tailings during runout is shown to substantially increase the\nrunout distance of the flow slide, highlighting the need for additional\nresearch to better characterize the appropriate strength of liquefied materials\nduring flow failures.",
        "Altermagnetism has emerged as a third fundamental category of collinear\nmagnetism, characterized by spin-splitting in symmetry-compensated collinear\nantiferromagnets, opening new frontiers in spintronics and condensed matter\nphysics. Here, based on first-principles calculations, we propose a novel\naltermagnetic semiconductor, the asymmetric Janus V$_2$AsBrO monolayer, which\nexhibits a magnetic easy axis favoring the out-of-plane direction and a\nN\\'{e}el temperature ($T_N$) exceeding room temperature. The system exhibits a\nstrain-tunable piezovalley effect, generating valley polarization under\nuniaxial strain. Notably, hole doping under uniaxial strain generates a net\nmagnetization ($M$) through a piezomagnetic mechanism. Additionally, the broken\ninversion symmetry endows the monolayer with a substantial out-of-plane\npiezoelectric coefficient $d_{31}$ (2.19 pm\/V), presenting broad prospects for\nthe development and design of novel piezoelectric devices. Our findings provide\na promising candidate material for the advancement of 2D multifunctional\ndevices in nanoelectronics, spintronics, valleytronics, and piezoelectrics.",
        "A commutative monoid $M$ is called a linearly orderable monoid if there\nexists a total order on $M$ that is compatible with the monoid operation. The\nfinitary power monoid of a commutative monoid $M$ is the monoid consisting of\nall nonempty finite subsets of $M$ under the so-called sumset. In this paper,\nwe investigate whether certain atomic and divisibility properties ascend from\nlinearly orderable monoids to their corresponding finitary power monoids.",
        "We study the dynamics of the $(\\alpha+\\beta)$ Fermi-Pasta-Ulam-Tsingou\nlattice (FPUT lattice, for short) for an arbitrary number $N$ of interacting\nparticles, in regimes of small enough nonlinearity so that a Birkhoff-Gustavson\ntype of normal form can be found using tools from wave-turbulence theory.\nSpecifically, we obtain the so-called Zakharov equation for $4$-wave resonant\ninteractions and its extension to $5$-wave resonant interactions by Krasitskii,\nbut we introduce an important new feature: even the generic terms in these\nnormal forms contain $resonant$ $interactions$ $only$, via a $unique$ canonical\ntransformation. The resulting normal forms provide an approximation to the\noriginal FPUT lattice that possesses a significant number of exact quadratic\nconservation laws, beyond the quadratic part of the Hamiltonian. We call the\nnew equations \"exact-resonance evolution equations\" and examine their\nproperties: (i) Heisenberg representation's slow evolution allows us to\nimplement numerical methods with large time steps to obtain relevant dynamical\ninformation, such as Lyapunov exponents. (ii) We introduce tests, such as\nconvergence of the normal form transformation and truncation error\nverification, to successfully validate our exact-resonance evolution equations.\n(iii) The systematic construction of new quadratic invariants (via the resonant\ncluster matrix) allows us to use finite-time Lyapunov exponent calculations to\nquantify the level of nonlinearity at which the original FPUT lattice is well\napproximated by the exact-resonance evolution equations. We show numerical\nexperiments in the case $N=9$, but the theory and numerical methods are valid\nfor arbitrary values of $N$. We conclude that, when $3$ divides $N$, at small\nenough nonlinearity the FPUT lattice's dynamics and nontrivial hyperchaos are\ngoverned by $5$-wave resonant interactions.",
        "The characteristics and images of Einstein-Maxwell-Dilaton (EMD) black holes\nare examined in this paper, focusing on their effective potential, photon\ntrajectories, and images with thin and thick accretion disks. We found that the\nshadow and photon sphere radii decrease with increasing dilaton charge. As the\nobservation inclination increases, direct and secondary images become separate,\nwith the direct image appearing hat-shaped. Simulations indicate that the\nbrightness of the shadow and photon ring is higher in static spherical\naccretion flows compared to infalling ones. The study also shows that in thin\ndisk accretion flows, the direct emission predominantly influences observed\nluminosity, with photon ring emission being less significant. Additionally, the\nappearance of black hole images varies with the observer's inclination angle."
      ]
    }
  },
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer",
    "start_abstract":"Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
      ],
      "abstract":[
        "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "Evolution and The Knightian Blindspot of Machine Learning",
        "URECA: The Chain of Two Minimum Set Cover Problems exists behind\n  Adaptation to Shifts in Semantic Code Search",
        "Sensemaking in Novel Environments: How Human Cognition Can Inform\n  Artificial Agents",
        "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "AI-based Identity Fraud Detection: A Systematic Review",
        "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics",
        "AI Generations: From AI 1.0 to AI 4.0",
        "Resource Constrained Pathfinding with A* and Negative Weights",
        "A Combinatorial Identities Benchmark for Theorem Proving via Automated\n  Theorem Generation",
        "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World\n  Multimodal Mathematical Error Detection",
        "Improved Rates of Differentially Private Nonconvex-Strongly-Concave\n  Minimax Optimization",
        "Online Learning of Danger Avoidance for Complex Structures of\n  Musculoskeletal Humanoids and Its Applications",
        "Orientation-dependent transport in junctions formed by $d$-wave\n  altermagnets and $d$-wave superconductors",
        "Assessing the impacts of tradable credit schemes through agent-based\n  simulation",
        "Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting",
        "Quantum Entanglement and Measurement Noise: A Novel Approach to\n  Satellite Node Authentication",
        "CallNavi: A Study and Challenge on Function Calling Routing and\n  Invocation in Large Language Models",
        "Global existence of weak solutions to a cell migration and\n  (de)differentiation model with double haptotaxis in the context of tissue\n  regeneration",
        "An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the\n  SUPERIOT Framework",
        "Decentralized Personalization for Federated Medical Image Segmentation\n  via Gossip Contrastive Mutual Learning",
        "Using cyclic $(f,\\sigma)$-codes over finite chain rings to construct\n  $\\mathbb{Z}_p$- and $\\mathbb{F}_q[\\![t]\\!]$-lattices",
        "MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based\n  Asynchronous Aggregation",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "Utilizing API Response for Test Refinement"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "This paper claims that machine learning (ML) largely overlooks an important\nfacet of general intelligence: robustness to a qualitatively unknown future in\nan open world. Such robustness relates to Knightian uncertainty (KU) in\neconomics, i.e. uncertainty that cannot be quantified, which is excluded from\nconsideration in ML's key formalisms. This paper aims to identify this blind\nspot, argue its importance, and catalyze research into addressing it, which we\nbelieve is necessary to create truly robust open-world AI. To help illuminate\nthe blind spot, we contrast one area of ML, reinforcement learning (RL), with\nthe process of biological evolution. Despite staggering ongoing progress, RL\nstill struggles in open-world situations, often failing under unforeseen\nsituations. For example, the idea of zero-shot transferring a self-driving car\npolicy trained only in the US to the UK currently seems exceedingly ambitious.\nIn dramatic contrast, biological evolution routinely produces agents that\nthrive within an open world, sometimes even to situations that are remarkably\nout-of-distribution (e.g. invasive species; or humans, who do undertake such\nzero-shot international driving). Interestingly, evolution achieves such\nrobustness without explicit theory, formalisms, or mathematical gradients. We\nexplore the assumptions underlying RL's typical formalisms, showing how they\nlimit RL's engagement with the unknown unknowns characteristic of an\never-changing complex world. Further, we identify mechanisms through which\nevolutionary processes foster robustness to novel and unpredictable challenges,\nand discuss potential pathways to algorithmically embody them. The conclusion\nis that the intriguing remaining fragility of ML may result from blind spots in\nits formalisms, and that significant gains may result from direct confrontation\nwith the challenge of KU.",
        "Adaptation is to make model learn the patterns shifted from the training\ndistribution. In general, this adaptation is formulated as the minimum entropy\nproblem. However, the minimum entropy problem has inherent limitation --\nshifted initialization cascade phenomenon. We extend the relationship between\nthe minimum entropy problem and the minimum set cover problem via Lebesgue\nintegral. This extension reveals that internal mechanism of the minimum entropy\nproblem ignores the relationship between disentangled representations, which\nleads to shifted initialization cascade. From the analysis, we introduce a new\nclustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA).\nURECA is an efficient clustering algorithm for the leverage of the\nrelationships between disentangled representations. The update rule of URECA\ndepends on Thresholdly-Updatable Stationary Assumption to dynamics as a\nreleased version of Stationary Assumption. This assumption helps URECA to\ntransport disentangled representations with no errors based on the\nrelationships between disentangled representations. URECA also utilize\nsimulation trick to efficiently cluster disentangled representations. The wide\nrange of evaluations show that URECA achieves consistent performance gains for\nthe few-shot adaptation to diverse types of shifts along with advancement to\nState-of-The-Art performance in CoSQA in the scenario of query shift.",
        "One of the most vital cognitive skills to possess is the ability to make\nsense of objects, events, and situations in the world. In the current paper, we\noffer an approach for creating artificially intelligent agents with the\ncapacity for sensemaking in novel environments. Objectives: to present several\nkey ideas: (1) a novel unified conceptual framework for sensemaking (which\nincludes the existence of sign relations embedded within and across frames);\n(2) interaction among various content-addressable, distributed-knowledge\nstructures via shared attributes (whose net response would represent a\nsynthesized object, event, or situation serving as a sign for sensemaking in a\nnovel environment). Findings: we suggest that attributes across memories can be\nshared and recombined in novel ways to create synthesized signs, which can\ndenote certain outcomes in novel environments (i.e., sensemaking).",
        "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "With the rapid development of digital services, a large volume of personally\nidentifiable information (PII) is stored online and is subject to cyberattacks\nsuch as Identity fraud. Most recently, the use of Artificial Intelligence (AI)\nenabled deep fake technologies has significantly increased the complexity of\nidentity fraud. Fraudsters may use these technologies to create highly\nsophisticated counterfeit personal identification documents, photos and videos.\nThese advancements in the identity fraud landscape pose challenges for identity\nfraud detection and society at large. There is a pressing need to review and\nunderstand identity fraud detection methods, their limitations and potential\nsolutions. This research aims to address this important need by using the\nwell-known systematic literature review method. This paper reviewed a selected\nset of 43 papers across 4 major academic literature databases. In particular,\nthe review results highlight the two types of identity fraud prevention and\ndetection methods, in-depth and open challenges. The results were also\nconsolidated into a taxonomy of AI-based identity fraud detection and\nprevention methods including key insights and trends. Overall, this paper\nprovides a foundational knowledge base to researchers and practitioners for\nfurther research and development in this important area of digital identity\nfraud.",
        "Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences.",
        "This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
        "Constrained pathfinding is a well-studied, yet challenging network\noptimisation problem that can be seen in a broad range of real-world\napplications. Pathfinding with multiple resource limits, which is known as the\nResource Constrained Shortest Path Problem (RCSP), aims to plan a cost-optimum\npath subject to limited usage of resources. Given the recent advances in\nconstrained and multi-criteria search with A*, this paper introduces a new\nresource constrained search framework on the basis of A* to tackle RCSP in\nlarge networks, even in the presence of negative cost and negative resources.\nWe empirically evaluate our new algorithm on a set of large instances and show\nup to two orders of magnitude faster performance compared to state-of-the-art\nRCSP algorithms in the literature.",
        "Large language models (LLMs) have significantly advanced formal theorem\nproving, yet the scarcity of high-quality training data constrains their\ncapabilities in complex mathematical domains. Combinatorics, a cornerstone of\nmathematics, provides essential tools for analyzing discrete structures and\nsolving optimization problems. However, its inherent complexity makes it\nparticularly challenging for automated theorem proving (ATP) for combinatorial\nidentities. To address this, we manually construct LeanComb, combinatorial\nidentities benchmark in Lean, which is, to our knowledge, the first formalized\ntheorem proving benchmark built for combinatorial identities. We develop an\nAutomated Theorem Generator for Combinatorial Identities, ATG4CI, which\ncombines candidate tactics suggested by a self-improving large language model\nwith a Reinforcement Learning Tree Search approach for tactic prediction. By\nutilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K\ncombinatorial identities theorems, each with a complete formal proof in Lean,\nand experimental evaluations demonstrate that models trained on this dataset\ncan generate more effective tactics, thereby improving success rates in\nautomated theorem proving for combinatorial identities.",
        "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection.",
        "In this paper, we study the problem of (finite sum) minimax optimization in\nthe Differential Privacy (DP) model. Unlike most of the previous studies on the\n(strongly) convex-concave settings or loss functions satisfying the\nPolyak-Lojasiewicz condition, here we mainly focus on the\nnonconvex-strongly-concave one, which encapsulates many models in deep learning\nsuch as deep AUC maximization. Specifically, we first analyze a DP version of\nStochastic Gradient Descent Ascent (SGDA) and show that it is possible to get a\nDP estimator whose $l_2$-norm of the gradient for the empirical risk function\nis upper bounded by $\\tilde{O}(\\frac{d^{1\/4}}{({n\\epsilon})^{1\/2}})$, where $d$\nis the model dimension and $n$ is the sample size. We then propose a new method\nwith less gradient noise variance and improve the upper bound to\n$\\tilde{O}(\\frac{d^{1\/3}}{(n\\epsilon)^{2\/3}})$, which matches the best-known\nresult for DP Empirical Risk Minimization with non-convex loss. We also\ndiscussed several lower bounds of private minimax optimization. Finally,\nexperiments on AUC maximization, generative adversarial networks, and temporal\ndifference learning with real-world data support our theoretical analysis.",
        "The complex structure of musculoskeletal humanoids makes it difficult to\nmodel them, and the inter-body interference and high internal muscle force are\nunavoidable. Although various safety mechanisms have been developed to solve\nthis problem, it is important not only to deal with the dangers when they occur\nbut also to prevent them from happening. In this study, we propose a method to\nlearn a network outputting danger probability corresponding to the muscle\nlength online so that the robot can gradually prevent dangers from occurring.\nApplications of this network for control are also described. The method is\napplied to the musculoskeletal humanoid, Musashi, and its effectiveness is\nverified.",
        "We investigate de Gennes-Saint-James states and Josephson effect in hybrid\njunctions based on $d$-wave altermagnet and $d$-wave superconductor. Even\nthough these states are associated to long junctions, we find that the\n$d_{x^{2}-y^{2}}$-altermagnet in a normal metal\/altermagnet\/$d$-wave\nsuperconductor junction forms de Gennes-Saint-James states in a short junction\ndue to an enhanced mismatch between electron and hole wave vectors. As a\nresult, the zero-bias conductance peak vanishes and pronounced resonance spikes\nemerge in the subgap conductance spectra. By contrast, the $d_{xy}$-altermagnet\nonly features de Gennes-Saint-James states in the long junction. Moreover, the\nwell-known features such as V-shape conductance for $d_{x^2-y^2}$ pairings and\nzero-biased conductance peak for $d_{xy}$ pairings are not affected by the\nstrength of $d_{xy}$-altermagnetism in the short junction. We also study the\nJosephson current-phase relation $I(\\varphi)$ of $d$-wave\nsuperconductor\/altermagnet\/$d$-wave superconductor hybrids, where $\\varphi$ is\nthe macroscopic phase difference between two $d$-wave superconductors. In\nsymmetric junctions, we obtain anomalous current phase relation such as a\n$0$-$\\pi$ transition by changing either the orientation or the magnitude of the\naltermagnetic order parameter and dominant higher Josephson harmonics.\nInterestingly, we find the first-order Josephson coupling in an asymmetric\n$d_{x^{2}-y^{2}}$-superconductor\/altermagnet\/$d_{xy}$-superconductor junction\nwhen the symmetry of altermagnetic order parameter is neither\n$d_{x^{2}-y^{2}}$- nor $d_{xy}$-wave. We present the symmetry analysis and\nconclude that the anomalous orientation-dependent current-phase relations are\nascribed to the peculiar feature of the altermagnetic spin-splitting field.",
        "Tradable credit schemes (TCS) have been attracting interest from the\ntransportation research community as an appealing alternative to congestion\npricing, due to the advantages of revenue neutrality and equity. Nonetheless,\nexisting research has largely employed network and market equilibrium\napproaches with simplistic characterizations of transportation demand, supply,\ncredit market operations, and market behavior. Agent- and activity-based\nsimulation affords a natural means to comprehensively assess TCS by more\nrealistically modeling demand, supply, and individual market interactions. We\npropose an integrated simulation framework for modeling a TCS, and implements\nit within the state-of-the-art open-source urban simulation platform\nSimMobility, including: (a) a flexible TCS design that considers multiple trips\nand explicitly accounts for individual trading behaviors; (b) a simulation\nframework that captures the complex interactions between a TCS regulator, the\ntraveler, and the TCS market itself, with the flexibility to test future TCS\ndesigns and relevant mobility models; and (c) a set of simulation experiments\non a large mesoscopic multimodal network combined with a Bayesian Optimization\napproach for TCS optimal design. The experiment results indicate network and\nmarket performance to stabilize over the day-to-day process, showing the\nalignment of our agent-based simulation with the known theoretical properties\nof TCS. We confirm the efficiency of TCS in reducing congestion under the\nadopted market behavioral assumptions and open the door for simulating\ndifferent individual behaviors. We measure how TCS impacts differently the\nlocal network, heterogeneous users, the different travel behaviors, and how\ntesting different TCS designs can avoid negative market trading behaviors.",
        "Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.",
        "In this paper, we introduce a novel authentication scheme for satellite nodes\nbased on quantum entanglement and measurement noise profiles. Our approach\nleverages the unique noise characteristics exhibited by each satellite's\nquantum optical communication system to create a distinctive \"quantum noise\nfingerprint.\" This fingerprint is used for node authentication within a\nsatellite constellation, offering a quantum-safe alternative to traditional\ncryptographic methods. The proposed scheme consists of a training phase, where\neach satellite engages in a training exercise with its neighbors to compile\nnoise profiles, and an online authentication phase, where these profiles are\nused for real-time authentication. Our method addresses the inherent challenges\nof implementing cryptographic-based schemes in space, such as key management\nand distribution, by exploiting the fundamental properties of quantum mechanics\nand the unavoidable imperfections in quantum systems. This approach enhances\nthe security and reliability of satellite communication networks, providing a\nrobust solution to the authentication challenges in satellite constellations.\nWe validated and tested several hypotheses for this approach using IBM System\nOne quantum computers.",
        "Interacting with a software system via a chatbot can be challenging,\nespecially when the chatbot needs to generate API calls, in the right order and\nwith the right parameters, to communicate with the system. API calling in\nchatbot systems poses significant challenges, particularly in complex,\nmulti-step tasks requiring accurate API selection and execution. We contribute\nto this domain in three ways: first, by introducing a novel dataset designed to\nassess models on API function selection, parameter generation, and nested API\ncalls; second, by benchmarking state-of-the-art language models across varying\nlevels of complexity to evaluate their performance in API function generation\nand parameter accuracy; and third, by proposing an enhanced API routing method\nthat combines general-purpose large language models for API selection with\nfine-tuned models for parameter generation and some prompt engineering\napproach. These approaches lead to substantial improvements in handling complex\nAPI tasks, offering practical advancements for real-world API-driven chatbot\nsystems.",
        "We study a model for the spread and (de)differentiation of mesenchymal stem\ncells and chondrocytes in a scaffold whose fibers are coated with hyaluron. The\nchondrocytes produce new extracellular matrix, which, together with hyaluron,\nserves as haptotactic cue for the stem cell migration. We prove global\nexistence of weak solutions of the corresponding cross-diffusion system with\ndouble haptotaxis.",
        "This paper presents a comprehensive analysis of the energy consumption\ncharacteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node\ndeveloped in the initial phase of the SUPERIOT project, focusing on key\noperating states, including Bluetooth Low Energy (BLE) communication,\nNarrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.\nExtensive measurements were conducted to establish a detailed energy profile,\nwhich serves as a benchmark for evaluating the effectiveness of subsequent\noptimizations and future node iterations. To minimize the energy consumption,\nmultiple optimizations were implemented at both the software and hardware\nlevels, achieving a reduction of over 60% in total energy usage through\nsoftware modifications alone. Further improvements were realized by optimizing\nthe E-ink display driving waveform and implementing a very low-power mode for\nnon-communication activities. Based on the measured data, three\nmeasurement-based energy consumption models were developed to characterize the\nenergy behavior of the node under: (i) normal, unoptimized operation, (ii)\nlow-power, software-optimized operation, and (iii) very low-power,\nhardware-optimized operation. These models, validated with new measurement\ndata, achieved an accuracy exceeding 97%, confirming their reliability for\npredicting energy consumption in diverse configurations.",
        "Federated Learning (FL) presents a promising avenue for collaborative model\ntraining among medical centers, facilitating knowledge exchange without\ncompromising data privacy. However, vanilla FL is prone to server failures and\nrarely achieves optimal performance on all participating sites due to\nheterogeneous data distributions among them. To overcome these challenges, we\npropose Gossip Contrastive Mutual Learning (GCML), a unified framework to\noptimize personalized models in a decentralized environment, where Gossip\nProtocol is employed for flexible and robust peer-to-peer communication. To\nmake efficient and reliable knowledge exchange in each communication without\nthe global knowledge across all the sites, we introduce deep contrast mutual\nlearning (DCML), a simple yet effective scheme to encourage knowledge transfer\nbetween the incoming and local models through collaborative training on local\ndata. By integrating DCML with other efforts to optimize site-specific models\nby leveraging useful information from peers, we evaluated the performance and\nefficiency of the proposed method on three publicly available datasets with\ndifferent segmentation tasks. Our extensive experimental results show that the\nproposed GCML framework outperformed both centralized and decentralized FL\nmethods with significantly reduced communication overhead, indicating its\npotential for real-world deployment.",
        "We construct $\\mathbb{Z}_p$-lattices and $\\mathbb{F}_q[\\![t]\\!]$-lattices\nfrom cyclic $(f,\\sigma)$-codes over finite chain rings, employing quotients of\nnatural nonassociative orders and principal left ideals in carefully chosen\nnonassociative algebras. This approach generalizes the classical Construction A\nthat obtains $\\mathbb{Z}$-lattices from linear codes over finite fields or\ncommutative rings to the nonassociative setting. We mostly use proper\nnonassociative cyclic algebras that are defined over field extensions of\n$p$-adic fields. This means we focus on $\\sigma$-constacyclic codes to obtain\n$\\mathbb{Z}_p$-lattices, hence $\\mathbb{Z}_p$-lattice codes. We construct\nlinear maximum rank distance (MRD) codes that are $\\mathbb{Z}_p$-lattice codes\nemploying the left multiplication of a nonassociative algebra over a finite\nchain ring.\n  Possible applications of our constructions include post-quantum cryptography\ninvolving $p$-adic lattices, e.g. learning with errors, building rank-metric\ncodes like MRD-codes, or $p$-adic coset coding, in particular wire-tap coding.",
        "Graph neural networks (GNNs) have become the state of the art for various\ngraph-related tasks and are particularly prominent in heterogeneous graphs\n(HetGs). However, several issues plague this paradigm: first, the difficulty in\nfully utilizing long-range information, known as over-squashing; second, the\ntendency for excessive message-passing layers to produce indistinguishable\nrepresentations, referred to as over-smoothing; and finally, the inadequacy of\nconventional MPNNs to train effectively on large sparse graphs. To address\nthese challenges in deep neural networks for large-scale heterogeneous graphs,\nthis paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),\nwhich enhances the representation of heterogeneous sparse graphs. MAPN consists\nof two primary components: node sequence generation and semantic information\naggregation. Node sequences are initially generated based on meta-paths through\nrandom walks, which serve as the foundation for a spatial state model that\nextracts essential information from nodes at various distances. It then\nasynchronously aggregates semantic information across multiple hops and layers,\neffectively preserving unique node characteristics and mitigating issues\nrelated to deep network degradation. Extensive experiments across diverse\ndatasets demonstrate the effectiveness of MAPN in graph embeddings for various\ndownstream tasks underscoring its substantial benefits for graph representation\nin large sparse heterogeneous graphs.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Artificial Intelligence for Pediatric Ophthalmology",
    "start_abstract":"PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
      ],
      "abstract":[
        "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Inferring collective synchrony observing spiking of one or several\n  neurons",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Solutions of first passage times problems: a biscaling approach",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Incident beam optics optimization for the single crystal neutron\n  diffractometer Pioneer with a polarized beam option",
        "Enhanced Field-Free Perpendicular Magnetization Switching via spin\n  splitting torque in Altermagnetic RuO2-based Heterostructures",
        "Electromagnetic Radiation from High-Energy Nuclear Collisions",
        "First X-ray polarimetric view of a Low-Luminosity Active Galactic\n  Nucleus: the case of NGC 2110",
        "Classifier Weighted Mixture models",
        "Formation of condensations for non-radial solutions to 3-wave kinetic\n  equations",
        "Shifting Attention to You: Personalized Brain-Inspired AI Models",
        "Can wormholes mirror the quasi-normal mode spectrum of Schwarzschild\n  black holes?",
        "The influence of the Hardy potential and a Convection Term on a\n  Nonlinear Degenerate Elliptic Equations",
        "Transit Timing Variations of the Sub-Saturn Exoplanet HAT-P-12b",
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Bayesian Computation in Deep Learning",
        "Anomalies in the electronic, magnetic and thermal behavior near the\n  Invar compositions of Fe-Ni alloys"
      ],
      "abstract":[
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "We study the first-passage time (FPT) problem for widespread recurrent\nprocesses in confined though large systems and present a comprehensive\nframework for characterizing the FPT distribution over many time scales. We\nfind that the FPT statistics can be described by two scaling functions: one\ncorresponds to the solution for an infinite system, and the other describes a\nscaling that depends on system size. We find a universal scaling relationship\nfor the FPT moments $\\langle t^q \\rangle$ with respect to the domain size and\nthe source-target distance. This scaling exhibits a transition at $q_c=\\theta$,\nwhere $\\theta$ is the persistence exponent. For low-order moments with $q<q_c$,\nconvergence occurs towards the moments of an infinite system. In contrast, the\nhigh-order moments, $q>q_c$, can be derived from an infinite density function.\nThe presented uniform approximation, connecting the two scaling functions,\nprovides a description of the first-passage time statistics across all time\nscales. We extend the results to include diffusion in a confining potential in\nthe high-temperature limit, where the potential strength takes the place of the\nsystem's size as the relevant scale. This study has been applied to various\nmediums, including a particle in a box, two-dimensional wedge, fractal\ngeometries, non-Markovian processes and the non-equilibrium process of\nresetting.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Pioneer, a next-generation single-crystal neutron diffractometer, is under\ndevelopment for Oak Ridge National Laboratory's Second Target Station (STS).\nDesigned to address a wide range of scientific questions, Pioneer will deliver\nhomogeneous neutron beams with customizable size and divergence, and provide a\npolarized beam option. This article introduces its incident beam optics,\nhighlighting the optimization methodology and the simulated performance.\nPioneer will utilize a modified elliptical-straight guide for neutron transport\nand deploy slit packages and insertable apertures to control beam size and\ndivergence. The optimized guide geometry matches the\noptimal-and-full-sample-illumination condition, and the beam control system\neffectively filters out unwanted neutrons while preserving the desired ones.\nAdditionally, we have found that polygon-approximated guides provide\nsatisfactory transport efficiency and beam homogeneity, eliminating the need of\ntruly curved guides. To enhance neutronics performance and reduce cost, the\ncoatings of supermirror elements are individually optimized to the lowest\nhalf-integer $m$-values that are sufficient to deliver the desired neutrons.\nAfter evaluating polarizing V-cavities and $^3$He spin filters over the default\npolarized wavelength band of 1.2-5.5~\\AA, we selected a translatable\nmultichannel polarizing V-cavity as the incident beam polarizer. Strategically\nplaced at a location where the beam divergence is low and a large in-guide gap\nhas negligible impact on transport efficiency, the optimized V-cavity achieves\nan average $P^2T$ of approximately 35\\%.",
        "Current-induced spin-orbit torque (SOT) has emerged as a promising method for\nachieving energy-efficient magnetization switching in advanced spintronic\ndevices. However, technological advancement has been inadequate because an\nexternal in-plane magnetic field is required to attain deterministic switching.\nSeveral approaches have been explored to address these challenges. In this\nwork, we explored the potential of a newly emerged altermagnetic material RuO2\nin combination with a Pt layer to achieve both field-free and low-power\nswitching concurrently. We leveraged out-of-plane (OOP) spin polarization via\nthe spin-splitter effect (SSE) in RuO2 for field-free switching (FFS) and\nin-plane spin polarization combined with spin Hall effect (SHE) in Pt for\nenhanced SOT efficiency. We revealed that the effective OOP magnetic field and\nFFS can be maximized by tuning the nominal thickness of the Pt underlayer and\nthe direction of the applied current. We observed a significant enhancement in\nFFS at an optimized Pt thickness of 1.5 nm for an applied current density as\nlow as 2.56e11 A\/m2 at a crystal angle of 90 deg. Our study paves the way for\nenergy-efficient spintronics devices for non-volatile memory, logic circuits,\nand neuromorphic computing.",
        "We highlight some of the developments in the theory and the observation of\nthe electromagnetic radiation, thermal and otherwise, emitted in relativistic\nheavy-ion collisions.",
        "Low-Luminosity Active Galactic Nuclei (LLAGN) provides a unique view of\nComptonization and non-thermal emission from accreting black holes in the\nlow-accretion rate regime. However, to decipher the exact nature of the\nComptonizing corona in LLAGN, its geometry and emission mechanism must be\nunderstood beyond the limits of spectro-timing techniques. Spectro-polarimetry\noffers the potential to break the degeneracies between different coronal\nemission models. Compton-thin LLAGN provide an opportunity for such\nspectro-polarimetric exploration in the 2-8 keV energy range using IXPE. In\nthis work, we carry out a spectro-polarimetric analysis of the first IXPE\nobservation, in synergy with a contemporaneous NuSTAR observation, of an LLAGN:\nNGC 2110. Using 554.4 ks of IXPE data from October 2024, we constrain the 99%\nupper limit on the Polarization Degree (PD) to be less than 8.3% assuming the\ncorresponding Polarization Angle (PA) to be aligned with the radio jet, and\nless than 3.6% if in the perpendicular direction. In the absence of a\nsignificant PD detection, the PA remains formally unconstrained, yet the\npolarization significance contours appear to be aligned with the radio jet,\ntentatively supporting models in which the corona is radially extended in the\nplane of the disk. We also carry out detailed Monte Carlo simulations using\nMONK and STOKES codes to test different coronal models against our results and\ncompare the polarization properties between NGC 2110 and brighter Seyferts.",
        "This paper proposes an extension of standard mixture stochastic models, by\nreplacing the constant mixture weights with functional weights defined using a\nclassifier. Classifier Weighted Mixtures enable straightforward density\nevaluation, explicit sampling, and enhanced expressivity in variational\nestimation problems, without increasing the number of components nor the\ncomplexity of the mixture components.",
        "We consider in this work a $2$-dimensional $3$-wave kinetic equation\ndescribing the dynamics of the thermal cloud outside a Bose-Einstein\nCondensate. We construct global non-radial mild solutions for the equation.\nThose mild solutions are the summation of Dirac masses on circles. We prove\nthat in each spatial direction, either Dirac masses at the origin, which are\nthe so-called Bose-Einstein condensates, can be formed in finite time or the\nsolutions converge to Bose-Einstein condensates as time evolves to infinity. We\nalso describe a dynamics of the formation of the Bose-Einstein condensates\nlatter case. In this case, on each direction, the solutions accumulate around\ncircles close to the origin at growth rates at least linearly in time.",
        "The integration of human and artificial intelligence represents a scientific\nopportunity to advance our understanding of information processing, as each\nsystem offers unique computational insights that can enhance and inform the\nother. The synthesis of human cognitive principles with artificial intelligence\nhas the potential to produce more interpretable and functionally aligned\ncomputational models, while simultaneously providing a formal framework for\ninvestigating the neural mechanisms underlying perception, learning, and\ndecision-making through systematic model comparisons and representational\nanalyses. In this study, we introduce personalized brain-inspired modeling that\nintegrates human behavioral embeddings and neural data to align with cognitive\nprocesses. We took a stepwise approach, fine-tuning the Contrastive\nLanguage-Image Pre-training (CLIP) model with large-scale behavioral decisions,\ngroup-level neural data, and finally, participant-level neural data within a\nbroader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We\nfound that fine-tuning on behavioral data enhances its ability to predict human\nsimilarity judgments while indirectly aligning it with dynamic representations\ncaptured via MEG. To further gain mechanistic insights into the temporal\nevolution of cognitive processes, we introduced a model specifically fine-tuned\non millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in\nenhanced temporal alignment with human neural processing while still showing\nimprovement on behavioral alignment. Finally, we trained individualized models\non participant-specific neural data, effectively capturing individualized\nneural dynamics and highlighting the potential for personalized AI systems.\nThese personalized systems have far-reaching implications for the fields of\nmedicine, cognitive research, human-computer interfaces, and AI development.",
        "Wormholes are exotic compact objects characterized by the absence of\nessential singularities and horizons, acting as slender bridges linking two\ndistinct regions of spacetime. Despite their theoretical significance, they\nremain however undetected, possibly due to their ability to closely mimic the\nobservational properties of black holes. This study explores whether a static\nand spherically symmetric wormhole within General Relativity can reproduce the\nquasi-normal mode spectrum of a Schwarzschild black hole under scalar,\nelectromagnetic, and axial gravitational perturbations, both individually and\nin combination. To address this, we reformulate the wormhole metric components\nusing a near-throat parametrization. Our analysis concentrates on the\nfundamental mode and first overtone, estimated via the\nWentzel-Kramers-Brillouin method. By employing a customized minimization\nstrategy, we demonstrate that within a specific region of the parameter space,\na wormhole can successfully replicate a subset of the black hole quasi-normal\nmode spectrum.",
        "This paper is devoted to prove existence of renormalized solutions for a\nclass of non--linear degenerate elliptic equations involving a non--linear\nconvection term, which satisfies a growth properties, and a Hardy potential.\nAdditionally, we assume that the right-hand side is an $L^m$ function, with\n$m\\geq 1$.",
        "We present Transit Timing Variations (TTVs) of HAT-P-12b, a low-density\nsub-Saturn mass planet orbiting a metal-poor K4 dwarf star. Using 14 years of\nobservational data (2009-2022), our study incorporates 7 new ground-based\nphotometric transit observations, three sectors of Transiting Exoplanet Survey\nSatellite (TESS) data, and 23 previously published light curves. A total of 46\nlight curves were analyzed using various analytical models, such as linear,\norbital decay, apsidal precession, and sinusoidal models to investigate the\npresence of additional planets. The stellar tidal quality factor ($Q_\\star'\n\\sim$ 28.4) is lower than the theoretical predictions, making the orbital decay\nmodel an unlikely explanation. The apsidal precession model with a $\\chi_r^2$\nof 4.2 revealed a slight orbital eccentricity (e = 0.0013) and a precession\nrate of 0.0045 rad\/epoch. Frequency analysis using the Generalized Lomb-Scargle\n(GLS) periodogram identified a significant periodic signal at 0.00415\ncycles\/day (FAP = 5.1$\\times$10$^{-6}$ %), suggesting the influence of an\nadditional planetary companion. The sinusoidal model provides the lowest\nreduced chi-squared value ($\\chi_r^2$) of 3.2. Sinusoidal fitting of the timing\nresiduals estimated this companion to have a mass of approximately 0.02 $M_J$ ,\nassuming it is in a 2:1 Mean-Motion Resonance (MMR) with HAT-P-12b.\nAdditionally, the Applegate mechanism, with an amplitude much smaller than the\nobserved TTV amplitude of 156 s, confirms that stellar activity is not\nresponsible for the observed variations.",
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions.",
        "The structural and magnetic properties of Fe$_{1-x}$Ni$_x$~($x$ = 0.32, 0.36,\n0.40, 0.50) alloys have been investigated using synchrotron based x-ray\ndiffraction (XRD) technique with x-rays of wavelength 0.63658 \\AA\\ down to 50 K\ntemperature, magnetic measurement using superconducting quantum interference\ndevice (SQUID) magnetometer and high resolution x-ray photoelectron\nspectroscopy (XPS) with monochromatic AlK$_\\alpha$ radiation. The XRD studies\nsuggest a single phase with fcc structure for $x$ = 0.36, 0.40, and 0.50\n~alloys and a mixed phase for $x$ = 0.32 alloy containing both bcc and fcc\nstructures. The lattice parameter of the alloys exhibits a linear dependence on\ntemperature giving rise to a temperature independent coefficient of thermal\nexpansion (CTE). The lowest CTE is observed for $x$ = 0.36 Invar alloy as\nexpected while $x$ = 0.50 alloy exhibits the highest CTE among the alloys\nstudied. The CTE of the fcc component of mixed phase alloy is close to that of\nInvar alloy. The temperature dependence of magnetization of the alloys down to\n2 K reveals an overall antiferromagnetic interactions within the ferromagnetic\nphase causing the magnetization decreasing with cooling. The field cooled and\nzero field cooled data show larger differences for the Invar compositions; this\nis also manifested in the magnetic hysteresis data at 2 K and 300 K."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm",
    "start_abstract":"In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Artificial Intelligence for Pediatric Ophthalmology"
      ],
      "abstract":[
        "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety\n  Detection",
        "SPPD: Self-training with Process Preference Learning Using Dynamic Value\n  Margin",
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Building Knowledge Graphs Towards a Global Food Systems Datahub",
        "Text Semantics to Flexible Design: A Residential Layout Generation\n  Method Based on Stable Diffusion Model",
        "Personalizing Education through an Adaptive LMS with Integrated LLMs",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Observer-Aware Probabilistic Planning Under Partial Observability",
        "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue",
        "ECLAIR: Enhanced Clarification for Interactive Responses",
        "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
        "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline\n  for Code Generation",
        "Decision Information Meets Large Language Models: The Future of\n  Explainable Operations Research",
        "Compressed Image Generation with Denoising Diffusion Codebook Models",
        "Juggling with Tensor Bases in Functional Approaches",
        "A Comprehensive Search for Leptoquarks Decaying into Top-$\\tau$ Final\n  States at the Future LHC",
        "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D\n  CT Image Synthesis",
        "Measuring Diversity in Synthetic Datasets",
        "Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to\n  Adversarial Attacks",
        "A Parareal in time numerical method for the collisional Vlasov equation\n  in the hyperbolic scaling",
        "POSMAC: Powering Up In-Network AR\/CG Traffic Classification with Online\n  Learning",
        "Evaluating Stenosis Detection with Grounding DINO, YOLO, and DINO-DETR",
        "Quantum superposition of boundary condition in $\\mathrm{PAdS}_2$",
        "Towards Automated Fact-Checking of Real-World Claims: Exploring Task\n  Formulation and Assessment with LLMs",
        "AUTOFRAME -- A Software-driven Integration Framework for Automotive\n  Systems",
        "Multi-Fidelity Policy Gradient Algorithms",
        "Structural Damping Identification Sensitivity in Flutter Speed\n  Estimation",
        "Comment on \"QCD factorization with multihadron fragmentation functions\""
      ],
      "abstract":[
        "The rapid advancements in Large Language Models (LLMs) have enabled their\ndeployment as autonomous agents for handling complex tasks in dynamic\nenvironments. These LLMs demonstrate strong problem-solving capabilities and\nadaptability to multifaceted scenarios. However, their use as agents also\nintroduces significant risks, including task-specific risks, which are\nidentified by the agent administrator based on the specific task requirements\nand constraints, and systemic risks, which stem from vulnerabilities in their\ndesign or interactions, potentially compromising confidentiality, integrity, or\navailability (CIA) of information and triggering security risks. Existing\ndefense agencies fail to adaptively and effectively mitigate these risks. In\nthis paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent\nsafety, which features adaptive safety check generation, effective safety check\noptimization, and tool compatibility and flexibility. Extensive experiments\ndemonstrate that AGrail not only achieves strong performance against\ntask-specific and system risks but also exhibits transferability across\ndifferent LLM agents' tasks.",
        "Recently, enhancing the numerical and logical reasoning capability of Large\nLanguage Models (LLMs) has emerged as a research hotspot. Existing methods face\nseveral limitations: inference-phase techniques (e.g., Chain of Thoughts) rely\non prompt selection and the pretrained knowledge; sentence-level Supervised\nFine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with\nstep-wise mathematical correctness and depend on stronger models distillation\nor human annotations; while Reinforcement Learning (RL) approaches incur high\nGPU memory costs and unstable training. To address these, we propose\n\\textbf{S}elf-training framework integrating \\textbf{P}rocess\n\\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPD\nleverages a process-based Markov Decision Process (MDP) and Bellman optimality\nequation to derive \\textbf{dynamic value margin} on step-level preference\noptimization, which employs tree-based self-sampling on model responses\n\\textbf{without any distillation} from other models. Furthermore, we\ntheoretically prove that SPPD is \\textbf{equivalent to on-policy policy\ngradient methods} under reward constraints. Experiments on 7B-scale models\ndemonstrate superior performance across in-domain and out-domain mathematical\nbenchmarks. We open-source our code at\n\\href{https:\/\/anonymous.4open.science\/r\/SSDPO-D-DCDD}{https:\/\/anonymous.4open.science\/r\/SPPD-DCDD}.",
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "Sustainable agricultural production aligns with several sustainability goals\nestablished by the United Nations (UN). However, there is a lack of studies\nthat comprehensively examine sustainable agricultural practices across various\nproducts and production methods. Such research could provide valuable insights\ninto the diverse factors influencing the sustainability of specific crops and\nproduce while also identifying practices and conditions that are universally\napplicable to all forms of agricultural production. While this research might\nhelp us better understand sustainability, the community would still need a\nconsistent set of vocabularies. These consistent vocabularies, which represent\nthe underlying datasets, can then be stored in a global food systems datahub.\nThe standardized vocabularies might help encode important information for\nfurther statistical analyses and AI\/ML approaches in the datasets, resulting in\nthe research targeting sustainable agricultural production. A structured method\nof representing information in sustainability, especially for wheat production,\nis currently unavailable. In an attempt to address this gap, we are building a\nset of ontologies and Knowledge Graphs (KGs) that encode knowledge associated\nwith sustainable wheat production using formal logic. The data for this set of\nknowledge graphs are collected from public data sources, experimental results\ncollected at our experiments at Kansas State University, and a Sustainability\nWorkshop that we organized earlier in the year, which helped us collect input\nfrom different stakeholders throughout the value chain of wheat. The modeling\nof the ontology (i.e., the schema) for the Knowledge Graph has been in progress\nwith the help of our domain experts, following a modular structure using KNARM\nmethodology. In this paper, we will present our preliminary results and schemas\nof our Knowledge Graph and ontologies.",
        "Flexibility in the AI-based residential layout design remains a significant\nchallenge, as traditional methods like rule-based heuristics and graph-based\ngeneration often lack flexibility and require substantial design knowledge from\nusers. To address these limitations, we propose a cross-modal design approach\nbased on the Stable Diffusion model for generating flexible residential\nlayouts. The method offers multiple input types for learning objectives,\nallowing users to specify both boundaries and layouts. It incorporates natural\nlanguage as design constraints and introduces ControlNet to enable stable\nlayout generation through two distinct pathways. We also present a scheme that\nencapsulates design expertise within a knowledge graph and translates it into\nnatural language, providing an interpretable representation of design\nknowledge. This comprehensibility and diversity of input options enable\nprofessionals and non-professionals to directly express design requirements,\nenhancing flexibility and controllability. Finally, experiments verify the\nflexibility of the proposed methods under multimodal constraints better than\nstate-of-the-art models, even when specific semantic information about room\nareas or connections is incomplete.",
        "The widespread adoption of large language models (LLMs) marks a\ntransformative era in technology, especially within the educational sector.\nThis paper explores the integration of LLMs within learning management systems\n(LMSs) to develop an adaptive learning management system (ALMS) personalized\nfor individual learners across various educational stages. Traditional LMSs,\nwhile facilitating the distribution of educational materials, fall short in\naddressing the nuanced needs of diverse student populations, particularly in\nsettings with limited instructor availability. Our proposed system leverages\nthe flexibility of AI to provide a customizable learning environment that\nadjusts to each user's evolving needs. By integrating a suite of\ngeneral-purpose and domain-specific LLMs, this system aims to minimize common\nissues such as factual inaccuracies and outdated information, characteristic of\ngeneral LLMs like OpenAI's ChatGPT. This paper details the development of an\nALMS that not only addresses privacy concerns and the limitations of existing\neducational tools but also enhances the learning experience by maintaining\nengagement through personalized educational content.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "In this article, we are interested in planning problems where the agent is\naware of the presence of an observer, and where this observer is in a partial\nobservability situation. The agent has to choose its strategy so as to optimize\nthe information transmitted by observations. Building on observer-aware Markov\ndecision processes (OAMDPs), we propose a framework to handle this type of\nproblems and thus formalize properties such as legibility, explicability and\npredictability. This extension of OAMDPs to partial observability can not only\nhandle more realistic problems, but also permits considering dynamic hidden\nvariables of interest. These dynamic target variables allow, for instance,\nworking with predictability, or with legibility problems where the goal might\nchange during execution. We discuss theoretical properties of PO-OAMDPs and,\nexperimenting with benchmark problems, we analyze HSVI's convergence behavior\nwith dedicated initializations and study the resulting strategies.",
        "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics.",
        "We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel\nunified and end-to-end framework for interactive disambiguation in enterprise\nAI assistants. ECLAIR generates clarification questions for ambiguous user\nqueries and resolves ambiguity based on the user's response.We introduce a\ngeneralized architecture capable of integrating ambiguity information from\nmultiple downstream agents, enhancing context-awareness in resolving\nambiguities and allowing enterprise specific definition of agents. We further\ndefine agents within our system that provide domain-specific grounding\ninformation. We conduct experiments comparing ECLAIR to few-shot prompting\ntechniques and demonstrate ECLAIR's superior performance in clarification\nquestion generation and ambiguity resolution.",
        "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
        "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, particularly in system 1 tasks, yet the intricacies of their\nproblem-solving mechanisms in system 2 tasks are not sufficiently explored.\nRecent research on System2-to-System1 methods surge, exploring the System 2\nreasoning knowledge via inference-time computation and compressing the explored\nknowledge into System 1 process. In this paper, we focus on code generation,\nwhich is a representative System 2 task, and identify two primary challenges:\n(1) the complex hidden reasoning processes and (2) the heterogeneous data\ndistributions that complicate the exploration and training of robust LLM\nsolvers. To tackle these issues, we propose a novel BDC framework that explores\ninsightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with\nmutual \\textbf{B}oosting, \\textbf{D}isentangles the heterogeneous training data\nfor composable LoRA-experts, and obtain \\textbf{C}ustomized problem solver for\neach data instance with an input-aware hypernetwork to weight over the\nLoRA-experts, offering effectiveness, flexibility, and robustness. This\nframework leverages multiple LLMs through mutual verification and boosting,\nintegrated into a Monte-Carlo Tree Search process enhanced by reflection-based\npruning and refinement. Additionally, we introduce the DisenLora algorithm,\nwhich clusters heterogeneous data to fine-tune LLMs into composable Lora\nexperts, enabling the adaptive generation of customized problem solvers through\nan input-aware hypernetwork. This work lays the groundwork for advancing LLM\ncapabilities in complex reasoning tasks, offering a novel System2-to-System1\nsolution.",
        "Operations Research (OR) is vital for decision-making in many industries.\nWhile recent OR methods have seen significant improvements in automation and\nefficiency through integrating Large Language Models (LLMs), they still\nstruggle to produce meaningful explanations. This lack of clarity raises\nconcerns about transparency and trustworthiness in OR applications. To address\nthese challenges, we propose a comprehensive framework, Explainable Operations\nResearch (EOR), emphasizing actionable and understandable explanations\naccompanying optimization. The core of EOR is the concept of Decision\nInformation, which emerges from what-if analysis and focuses on evaluating the\nimpact of complex constraints (or parameters) changes on decision-making.\nSpecifically, we utilize bipartite graphs to quantify the changes in the OR\nmodel and adopt LLMs to improve the explanation capabilities. Additionally, we\nintroduce the first industrial benchmark to rigorously evaluate the\neffectiveness of explanations and analyses in OR, establishing a new standard\nfor transparency and clarity in the field.",
        "We present a novel generative approach based on Denoising Diffusion Models\n(DDMs), which produces high-quality image samples along with their losslessly\ncompressed bit-stream representations. This is obtained by replacing the\nstandard Gaussian noise sampling in the reverse diffusion with a selection of\nnoise samples from pre-defined codebooks of fixed iid Gaussian vectors.\nSurprisingly, we find that our method, termed Denoising Diffusion Codebook\nModel (DDCM), retains sample quality and diversity of standard DDMs, even for\nextremely small codebooks. We leverage DDCM and pick the noises from the\ncodebooks that best match a given image, converting our generative model into a\nhighly effective lossy image codec achieving state-of-the-art perceptual image\ncompression results. More generally, by setting other noise selections rules,\nwe extend our compression method to any conditional image generation task\n(e.g., image restoration), where the generated images are produced jointly with\ntheir condensed bit-stream representations. Our work is accompanied by a\nmathematical interpretation of the proposed compressed conditional generation\nschemes, establishing a connection with score-based approximations of posterior\nsamplers for the tasks considered.",
        "Systematic expansion schemes in functional approaches require the inclusion\nof higher order vertices. These vertices are expanded in independent tensor\nbases with a rapidly increasing number of basis elements. Amongst the related\ntasks are the construction of bases and projection operators, the importance\nordering of their elements, and the optimisation of such tensor bases, as well\nas an analysis of their regularity in momentum space. We present progress in\nall these directions and introduce the Mathematica package TensorBases designed\nfor the aforementioned tasks.",
        "We studied the collider phenomenology of third-generation scalar leptoquarks\nat the Large Hadron Collider (LHC) with a 14 TeV center-of-mass energy. The\nanalysis focuses on leptoquarks decaying exclusively into top quarks and tau\nleptons, employing machine learning-based tagging techniques for identifying\nhadronically decaying boosted top quarks, W\/Z, and Higgs bosons, as well as a\nmultivariate classifier to distinguish signal events from Standard Model (SM)\nbackgrounds. The expected 95% confidence level (CL) upper limits on the\nleptoquark production cross-section are computed assuming integrated\nluminosities of 200 and 500 inverse femtobarns at the 14 TeV LHC. The results\ndemonstrate significant sensitivity improvements for detecting leptoquarks at\nmasses beyond the current experimental limits.",
        "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.",
        "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https:\/\/github.com\/BlueWhaleLab\/DCScore.",
        "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons\/TinyML (MLC\/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets.",
        "We present the design of a multiscale parareal method for kinetic equations\nin the fluid dynamic regime. The goal is to reduce the cost of a fully kinetic\nsimulation using a parallel in time procedure. Using the multiscale property of\nkinetic models, the cheap, coarse propagator consists in a fluid solver and the\nfine (expensive) propagation is achieved through a kinetic solver for a\ncollisional Vlasov equation. To validate our approach, we present simulations\nin the 1D in space, 3D in velocity settings over a wide range of initial data\nand kinetic regimes, showcasing the accuracy, efficiency, and the speedup\ncapabilities of our method.",
        "In this demonstration, we showcase POSMAC1, a platform designed to deploy\nDecision Tree (DT) and Random Forest (RF) models on the NVIDIA DOCA DPU,\nequipped with an ARM processor, for real-time network traffic classification.\nDeveloped specifically for Augmented Reality (AR) and Cloud Gaming (CG) traffic\nclassification, POSMAC streamlines model evaluation, and generalization while\noptimizing throughput to closely match line rates.",
        "Detecting stenosis in coronary angiography is vital for diagnosing and\nmanaging cardiovascular diseases. This study evaluates the performance of\nstate-of-the-art object detection models on the ARCADE dataset using the\nMMDetection framework. The models are assessed using COCO evaluation metrics,\nincluding Intersection over Union (IoU), Average Precision (AP), and Average\nRecall (AR). Results indicate variations in detection accuracy across different\nmodels, attributed to differences in algorithmic design, transformer-based vs.\nconvolutional architectures. Additionally, several challenges were encountered\nduring implementation, such as compatibility issues between PyTorch, CUDA, and\nMMDetection, as well as dataset inconsistencies in ARCADE. The findings provide\ninsights into model selection for stenosis detection and highlight areas for\nfurther improvement in deep learning-based coronary artery disease diagnosis.",
        "We explore the quantum superposition of boundary conditions in the context of\nthe Poincar\\'e patch of the two-dimensional Anti-de Sitter space\n($\\mathrm{PAdS}_2$). Focusing on Robin (mixed) boundary conditions (RBC), we\ninvestigate the response function of the Unruh-DeWitt (UDW) detector\ninteracting with two or more scalar fields, each respecting a different\nboundary condition. The role of this quantum superposition is two-fold: i) it\nmay represent different fields propagating on the same spacetime and\ninteracting with an UDW detector or ii) it may describe an UDW detector on a\nsuperposition of spacetimes, each one with an inequivalent propagating field.",
        "Fact-checking is necessary to address the increasing volume of\nmisinformation. Traditional fact-checking relies on manual analysis to verify\nclaims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models\n(LLMs) across multiple labeling schemes (binary, three-class, five-class) and\nextends traditional claim verification by incorporating analysis, verdict\nclassification, and explanation in a structured setup to provide comprehensive\njustifications for real-world claims. We evaluate Llama-3 models of varying\nsizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024)\nusing evidence retrieved via restricted web searches. We utilize TIGERScore as\na reference-free evaluation metric to score the justifications. Our results\nshow that larger LLMs consistently outperform smaller LLMs in classification\naccuracy and justification quality without fine-tuning. We find that smaller\nLLMs in a one-shot scenario provide comparable task performance to fine-tuned\nSmall Language Models (SLMs) with large context sizes, while larger LLMs\nconsistently surpass them. Evidence integration improves performance across all\nmodels, with larger LLMs benefiting most. Distinguishing between nuanced labels\nremains challenging, emphasizing the need for further exploration of labeling\nschemes and alignment with evidences. Our findings demonstrate the potential of\nretrieval-augmented AFC with LLMs.",
        "The evolution of automotive technologies towards more integrated and\nsophisticated systems requires a shift from traditional distributed\narchitectures to centralized vehicle architectures. This work presents a novel\nframework that addresses the increasing complexity of Software Defined Vehicles\n(SDV) through a centralized approach that optimizes software and hardware\nintegration. Our approach introduces a scalable, modular, and secure automotive\ndeployment framework that leverages a hardware abstraction layer and dynamic\nsoftware deployment capabilities to meet the growing demands of the industry.\nThe framework supports centralized computing of vehicle functions, making\nsoftware development more dynamic and easier to update and upgrade. We\ndemonstrate the capabilities of our framework by implementing it in a simulated\nenvironment where it effectively handles several automotive operations such as\nlane detection, motion planning, and vehicle control. Our results highlight the\nframework's potential to facilitate the development and maintenance of future\nvehicles, emphasizing its adaptability to different hardware configurations and\nits readiness for real-world applications. This work lays the foundation for\nfurther exploration of robust, scalable, and secure SDV systems, setting a new\nstandard for future automotive architectures.",
        "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.",
        "Predicting flutter remains a key challenge in aeroelastic research, with\ncertain models relying on modal parameters, such as natural frequencies and\ndamping ratios. These models are particularly useful in early design stages or\nfor the development of small UAVs (maximum take-off mass below 7 kg). This\nstudy evaluates two frequency-domain system identification methods, Fast\nRelaxed Vector Fitting (FRVF) and the Loewner Framework (LF), for predicting\nthe flutter onset speed of a flexible wing model. Both methods are applied to\nextract modal parameters from Ground Vibration Testing data, which are\nsubsequently used to develop a reduced-order model with two degrees of freedom.\nResults indicate that FRVF and LFinformed models provide reliable flutter\nspeed, with predictions deviating by no more than 3% (FRVF) and 5% (LF) from\nthe N4SID-informed benchmark. The findings highlight the sensitivity of flutter\nspeed predictions to damping ratio identification accuracy and demonstrate the\npotential of these methods as computationally efficient alternatives for\npreliminary aeroelastic assessments.",
        "We make several comments on the recent work in Ref.~\\cite{Rogers:2024nhb}\nwhile also reaffirming and adding to the work in Ref.~\\cite{Pitonyak:2023gjx}.\nWe show that the factorization formula for $e^+e^-\\to (h_1\\cdots h_n)\\, X$ in\nRef.~\\cite{Rogers:2024nhb} is equivalent to a version one can derive using the\ndefinition of a $n$-hadron fragmentation function (FF) introduced in\nRef.~\\cite{Pitonyak:2023gjx}. In addition, we scrutinize how to generalize the\nnumber density definition of a single-hadron FF to a $n$-hadron FF, arguing\nthat the definition given in Ref.~\\cite{Pitonyak:2023gjx} should be considered\nthe standard one. We also emphasize that the evolution equations for dihadron\nFFs~(DiFFs) in Ref.~\\cite{Pitonyak:2023gjx} have the same splitting functions\nas those for single-hadron FFs. Therefore, the DiFF (and $n$-hadron FF)\ndefinitions in Ref.~\\cite{Pitonyak:2023gjx} have a natural number density\ninterpretation and are consistent with collinear factorization using the\nstandard hard factors and evolution kernels. Moreover, we make clear that the\noperator definition for the DiFF $D_1^{h_1h_2}(\\xi,M_h)$ written down in\nRef.~\\cite{Rogers:2024nhb} agrees exactly with the one in\nRef.~\\cite{Pitonyak:2023gjx}. Contrary to what is implied in\nRef.~\\cite{Rogers:2024nhb}, this definition did not appear in the literature\nprior to the work in Ref.~\\cite{Pitonyak:2023gjx}. There also seem to be\ninconsistencies in how $D_1^{h_1h_2}(\\xi,M_h)$ appears in previous unpolarized\ncross section formulas in the literature."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b15",
    "start_title":"Learning Parities with Neural Networks",
    "start_abstract":"In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Testing conditional independence of discrete distributions"
      ],
      "abstract":[
        "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
      ],
      "categories":[
        "stat.CO"
      ]
    },
    "list":{
      "title":[
        "Splitting Regularized Wasserstein Proximal Algorithms for Nonsmooth\n  Sampling Problems",
        "Fast and light-weight energy statistics using the \\textit{R} package\n  \\textsf{Rfast}",
        "Deep P-Spline: Theory, Fast Tuning, and Application",
        "Parallel ADMM Algorithm with Gaussian Back Substitution for\n  High-Dimensional Quantile Regression and Classification",
        "Least squares variational inference",
        "MCMC for multi-modal distributions",
        "Design Principles for Architectures of Technical Smart Service Systems",
        "Resampling Methods that Generate Time Series Data to Enable Sensitivity\n  and Model Analysis in Energy Modeling",
        "Weighted Fisher divergence for high-dimensional Gaussian variational\n  inference",
        "Greedy Stein Variational Gradient Descent: An algorithmic approach for\n  wave prospection problems",
        "fastrerandomize: An R Package for Fast Rerandomization Using Accelerated\n  Computing",
        "Langevin Bi-fidelity Importance Sampling for Failure Probability\n  Estimation",
        "Geodesic Variational Bayes for Multiway Covariances",
        "CoHiRF: A Scalable and Interpretable Clustering Framework for\n  High-Dimensional Data",
        "A look-up table algorithm to model radiation damage effects in Monte\n  Carlo events for HL-LHC experiments",
        "Consistent crust-core interpolation and its effect on non-radial neutron\n  star oscillations",
        "Some Bohr-type inequalities with two parameters for bounded analytic\n  functions",
        "Spatially resolved dust properties over 50 kpc in a hyperluminous galaxy\n  merger at $z = 4.6$",
        "Inductive Construction of Variational Quantum Circuit for Constrained\n  Combinatorial Optimization",
        "Safety in safe Bayesian optimization and its ramifications for control",
        "Sample and Map from a Single Convex Potential: Generation using\n  Conjugate Moment Measures",
        "Improving Student Self-Efficacy in Quantum Computing with the Qubit\n  Touchdown Board Game",
        "A linearly-implicit energy preserving scheme for geometrically nonlinear\n  mechanics based on non-canonical Hamiltonian formulations",
        "Colimits of internal categories",
        "A Bayesian Multivariate Spatial Point Pattern Model: Application to Oral\n  Microbiome FISH Image Data",
        "Chiral Magnetic Effect enhancement at lower collision energies",
        "Rotational beta expansions and Schmidt games",
        "Constructing the low-temperature phase diagram for the $2+p$-quantum\n  spin glass using the nonperturbative renormalization group"
      ],
      "abstract":[
        "Sampling from nonsmooth target probability distributions is essential in\nvarious applications, including the Bayesian Lasso. We propose a\nsplitting-based sampling algorithm for the time-implicit discretization of the\nprobability flow for the Fokker-Planck equation, where the score function\ndefined as the gradient logarithm of the current probability density function,\nis approximated by the regularized Wasserstein proximal. When the prior\ndistribution is the Laplace prior, our algorithm is explicitly formulated as a\ndeterministic interacting particle system, incorporating softmax operators and\nshrinkage operations to efficiently compute the gradient drift vector field and\nthe score function. The proposed formulation introduces a particular class of\nattention layers in transformer structures, which can sample sparse target\ndistributions. We verify the convergence towards target distributions regarding\nR\\'enyi divergences under suitable conditions. Numerical experiments in\nhigh-dimensional nonsmooth sampling problems, such as sampling from mixed\nGaussian and Laplace distributions, logistic regressions, image restoration\nwith L1-TV regularization, and Bayesian neural networks, demonstrate the\nefficiency and robust performance of the proposed method.",
        "Energy statistics, also known as $\\mathcal{\\varepsilon}$-statistics, are\nfunctions of distances between statistical observations. This class of\nfunctions has enabled the development of non-linear statistical concepts, such\nas distance variance, distance covariance, and distance correlation. However,\nthe computational burden associated with $\\mathcal{\\varepsilon}$-statistics is\nsubstantial, particularly when the data reside in multivariate space. To\naddress this challenge, we have developed a method to significantly reduce\nmemory requirements and accelerate computations, thereby facilitating the\nanalysis of large data sets. The following cases are demonstrated: univariate\nand multivariate distance variance, distance covariance, partial distance\ncorrelation, energy distance, and hypothesis testing for the equality of\nunivariate distributions.",
        "Deep neural networks (DNNs) have been widely applied to solve real-world\nregression problems. However, selecting optimal network structures remains a\nsignificant challenge. This study addresses this issue by linking neuron\nselection in DNNs to knot placement in basis expansion techniques. We introduce\na difference penalty that automates knot selection, thereby simplifying the\ncomplexities of neuron selection. We name this method Deep P-Spline (DPS). This\napproach extends the class of models considered in conventional DNN modeling\nand forms the basis for a latent variable modeling framework using the\nExpectation-Conditional Maximization (ECM) algorithm for efficient network\nstructure tuning with theoretical guarantees. From a nonparametric regression\nperspective, DPS is proven to overcome the curse of dimensionality, enabling\nthe effective handling of datasets with a large number of input variable, a\nscenario where conventional nonparametric regression methods typically\nunderperform. This capability motivates the application of the proposed\nmethodology to computer experiments and image data analyses, where the\nassociated regression problems involving numerous inputs are common. Numerical\nresults validate the effectiveness of the model, underscoring its potential for\nadvanced nonlinear regression tasks.",
        "In the field of high-dimensional data analysis, modeling methods based on\nquantile loss function are highly regarded due to their ability to provide a\ncomprehensive statistical perspective and effective handling of heterogeneous\ndata. In recent years, many studies have focused on using the parallel\nalternating direction method of multipliers (P-ADMM) to solve high-dimensional\nquantile regression and classification problems. One efficient strategy is to\nreformulate the quantile loss function by introducing slack variables. However,\nthis reformulation introduces a theoretical challenge: even when the\nregularization term is convex, the convergence of the algorithm cannot be\nguaranteed. To address this challenge, this paper proposes the Gaussian\nBack-Substitution strategy, which requires only a simple and effective\ncorrection step that can be easily integrated into existing parallel algorithm\nframeworks, achieving a linear convergence rate. Furthermore, this paper\nextends the parallel algorithm to handle some novel quantile loss\nclassification models. Numerical simulations demonstrate that the proposed\nmodified P-ADMM algorithm exhibits excellent performance in terms of\nreliability and efficiency.",
        "Variational inference consists in finding the best approximation of a target\ndistribution within a certain family, where `best' means (typically) smallest\nKullback-Leiber divergence. We show that, when the approximation family is\nexponential, the best approximation is the solution of a fixed-point equation.\nWe introduce LSVI (Least-Squares Variational Inference), a Monte Carlo variant\nof the corresponding fixed-point recursion, where each iteration boils down to\nordinary least squares regression and does not require computing gradients. We\nshow that LSVI is equivalent to stochastic mirror descent; we use this insight\nto derive convergence guarantees. We introduce various ideas to improve LSVI\nfurther when the approximation family is Gaussian, leading to a $O(d^3)$\ncomplexity in the dimension $d$ of the target in the full-covariance case, and\na $O(d)$ complexity in the mean-field case. We show that LSVI outperforms\nstate-of-the-art methods in a range of examples, while remaining gradient-free,\nthat is, it does not require computing gradients.",
        "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces.",
        "Successful smart services require seamless integration into existing\ncorporate systems and an interdisciplinary approach that aligns the development\nof both business models and technical architectures. Multi-disciplinarity and\ncocreating with customers add a layer of complexity but are essential\ncollaboration schemes for validating the value proposition of smart services\nand building longterm customer loyalty. This paper explores these challenges\nand distills the design principles for the architectures of technical smart\nservice systems, based on empirical data from architecture projects in two\nmanufacturing companies. These principles contribute to the sparse academic\nliterature on this topic and help practitioners navigate several design\ntrade-offs commonly arising in smart service projects.",
        "Energy systems modeling frequently relies on time series data, whether\nobserved or forecast. This is particularly the case, for example, in capacity\nplanning models that use hourly production and load data forecast to occur over\nthe coming several decades. This paper addresses the attendant problem of\nperforming sensitivity, robustness, and other post-solution analyses using time\nseries data. We explore two efficient and relatively simple, non-parametric,\nbootstrapping methods for generating arbitrary numbers of time series from a\nsingle observed or forecast series. The paper presents and assesses each\nmethod. We find that the generated series are both visually and by statistical\nsummary measures close to the original observational data. In consequence these\nseries are credibly taken as stochastic instances from a common distribution,\nthat of the original series of observations. With climate change in mind, the\npaper further proposes and explores two general techniques for systematically\naltering (increasing or decreasing) time series. Both for the perturbed and\nunperturbed synthetic series data, we find that the generated series induce\nvariability in properties of the series that are important for energy modeling,\nin particular periods of under- and over-production, and periods of increased\nramping rates. In consequence, series produced in this way are apt for use in\nrobustness, sensitivity, and in general post-solution analysis of energy\nplanning models. These validity factors auger well for applications beyond\nenergy modeling.",
        "Bayesian inference has many advantages for complex models. However, standard\nMonte Carlo methods for summarizing the posterior can be computationally\ndemanding, and it is attractive to consider optimization-based variational\napproximations. Our work considers Gaussian approximations with sparse\nprecision matrices which are tractable to optimize in high-dimensional\nproblems. Although the optimal Gaussian approximation is usually defined as the\none closest to the target posterior in Kullback-Leibler divergence, it is\nuseful to consider other divergences when the Gaussian assumption is crude, in\norder to capture important features of the posterior for a given application.\nOur work studies the weighted Fisher divergence, which focuses on gradient\ndifferences between the target posterior and its approximation, with the Fisher\nand score-based divergences being special cases. We make three main\ncontributions. First, we compare approximations for weighted Fisher divergences\nunder mean-field assumptions for both Gaussian and non-Gaussian targets with\nKullback-Leibler approximations. Second, we go beyond mean-field and consider\napproximations with sparse precision matrices reflecting posterior conditional\nindependence structure for hierarchical models. Using stochastic gradient\ndescent to enforce sparsity, we develop two approaches to minimize the weighted\nFisher divergence, based on the reparametrization trick and a batch\napproximation of the objective. Finally, we examine the performance of our\nmethods for examples involving logistic regression, generalized linear mixed\nmodels and stochastic volatility models.",
        "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
        "The fastrerandomize R package provides hardware-accelerated tools for\nperforming rerandomization and randomization testing in experimental research.\nUsing a JAX backend, the package enables exact rerandomization inference even\nfor large experiments with hundreds of billions of possible randomizations. Key\nfunctionalities include generating pools of acceptable rerandomizations based\non covariate balance, conducting exact randomization tests, and performing\npre-analysis evaluations to determine optimal rerandomization acceptance\nthresholds. Through batched processing and GPU acceleration, fastrerandomize\nachieves substantial performance gains compared to existing implementations,\nmaking previously intractable designs computationally feasible. The package\ntherefore extends the randomization-based inference toolkit in R, allowing\nresearchers to efficiently implement more stringent rerandomization designs and\nconduct valid inference even with large sample sizes or in high-dimensional\nsettings.",
        "Estimating failure probability is one of the key tasks in the field of\nuncertainty quantification. In this domain, importance sampling has proven to\nbe an effective estimation strategy; however, its efficiency heavily depends on\nthe choice of the biasing distribution. An improperly selected biasing\ndistribution can significantly increase estimation error. One way to solve this\nproblem is to leverage a less expensive, lower-fidelity surrogate. Building on\nthe accessibility to such a model and its derivative on the random uncertain\ninputs, we introduce an importance-sampling-based estimator, termed the\nLangevin bi-fidelity importance sampling (L-BF-IS), which uses\nscore-function-based sampling algorithms to generate new samples and\nsubstantially reduces the mean square error (MSE) of failure probability\nestimation. The proposed method demonstrates lower estimation error, especially\nin high-dimensional ($\\geq 100$) input spaces and when limited high-fidelity\nevaluations are available. The L-BF-IS estimator's effectiveness is validated\nthrough experiments with two synthetic functions and two real-world\napplications governed by partial differential equations. These real-world\napplications involve a composite beam, which is represented using a simplified\nEuler-Bernoulli equation as a low-fidelity surrogate, and a steady-state\nstochastic heat equation, for which a pre-trained neural operator serves as the\nlow-fidelity surrogate.",
        "This article explores the optimization of variational approximations for\nposterior covariances of Gaussian multiway arrays. To achieve this, we\nestablish a natural differential geometric optimization framework on the space\nusing the pullback of the affine-invariant metric. In the case of a truly\nseparable covariance, we demonstrate a joint approximation in the multiway\nspace outperforms a mean-field approximation in optimization efficiency and\nprovides a superior approximation to an unstructured Inverse-Wishart posterior\nunder the average Mahalanobis distance of the data while maintaining a multiway\ninterpretation. We moreover establish efficient expressions for the Euclidean\nand Riemannian gradients in both cases of the joint and mean-field\napproximation. We end with an analysis of commodity trade data.",
        "Clustering high-dimensional data poses significant challenges due to the\ncurse of dimensionality, scalability issues, and the presence of noisy and\nirrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF),\na novel clustering method designed to address these challenges effectively.\nCoHiRF leverages random feature selection to mitigate noise and dimensionality\neffects, repeatedly applies K-Means clustering in reduced feature spaces, and\ncombines results through a unanimous consensus criterion. This iterative\napproach constructs a cluster assignment matrix, where each row records the\ncluster assignments of a sample across repetitions, enabling the identification\nof stable clusters by comparing identical rows. Clusters are organized\nhierarchically, enabling the interpretation of the hierarchy to gain insights\ninto the dataset. CoHiRF is computationally efficient with a running time\ncomparable to K-Means, scalable to massive datasets, and exhibits robust\nperformance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and\nOPTICS. Experimental results on synthetic and real-world datasets confirm the\nmethod's ability to reveal meaningful patterns while maintaining scalability,\nmaking it a powerful tool for high-dimensional data analysis.",
        "Radiation damage significantly impacts the performance of silicon tracking\ndetectors in Large Hadron Collider (LHC) experiments such as ATLAS and CMS,\nwith signal reduction being the most critical effect. Adjusting sensor bias\nvoltage and detection thresholds can help mitigate these effects, but\ngenerating simulated data that accurately mirror the performance evolution with\nthe accumulation of luminosity, hence fluence, is crucial. The ATLAS\ncollaboration has developed and implemented algorithms to correct simulated\nMonte Carlo (MC) events for radiation damage effects, achieving impressive\nagreement between collision data and simulated events. In preparation for the\nhigh-luminosity phase (HL-LHC), the demand for a faster ATLAS MC production\nalgorithm becomes imperative due to escalating collision, events, tracks, and\nparticle hit rates, imposing stringent constraints on available computing\nresources. This article outlines the philosophy behind the new algorithm, its\nimplementation strategy, and the essential components involved. The results\nfrom closure tests indicate that the events simulated using the new algorithm\nagree with fully simulated events at the level of few \\%. The first tests on\ncomputing performance show that the new algorithm is as fast as it is when no\nradiation damage corrections are applied.",
        "To model the structure of neutron stars (NSs) theoretically,it is common to\nconsider layers with different density regimes. Matching the equation of state\n(EoS) for the crust and core and obtaining a suitable description of these\nextreme conditions are crucial for understanding the properties of these\ncompact objects. In this work, we construct ten different NS EoSs incorporating\nthree distinct crust models, which are connected to the core using a\nthermodynamically and causally consistent formalism. For cold NSs, we propose a\nlinear relationship between pressure and energy density in a narrow region\nbetween the crust and core, effectively establishing an interpolation function\nin the pressure-baryonic chemical potential plane. We then compare this EoS\nmatching method with the classical approach, which neglects causal and\nthermodynamic consistency. We solve the Tolman-Oppenheimer-Volkoff equation to\nobtain the mass-radius relationship and compare our results with observational\nconstraints on NSs. Furthermore, we investigate the influence of the new\nmatching formalism on non-radial oscillation frequencies and damping times. Our\nfindings suggest that the method used to glue the crust and core EoS impacts NS\nobservables, such as the radius, oscillation frequencies, and damping times of\nnon-radial modes, which may be crucial for interpreting future gravitational\nwave observations from neutron star mergers or isolated pulsars. The effects\nare particularly noticeable for low-mass NSs, regardless of the specific EoS\nmodel chosen. In particular, we find that the $p_1$ oscillation mode exhibits\nsignificant differences in frequencies among alternative matching methods,\nwhereas the fundamental $f$-mode remains unaffected by changes in crust models\nor interpolation schemes.",
        "In this article, some Bohr inequalities for analytical functions on the unit\ndisk are generalized to the forms with two parameters. One of our results is\nsharp.",
        "We present spatially resolved dust-continuum ALMA observations from\nrest-frame $\\sim$60 to $\\sim$600 $\\mu$m (bands 3-10) of the hyperluminous hot\ndust-obscured galaxy (hot DOG) WISE J224607.6-052634.9 (W2246-0526), at\nredshift $z=4.6$. W2246-0526 is interacting with at least three companion\ngalaxies, forming a system connected by tidal streams. We model the\nmultiwavelength ALMA observations of the dust continuum using a modified\nblackbody, from which we derive the dust properties (mass, emissivity index,\narea of the emitting region, and temperature) in the hot DOG and resolved\nstructures across a region of nearly $\\sim$50 kpc. The peak temperature at the\nlocation of the hot DOG, $\\sim$110 K, is likely the consequence of heating by\nthe central quasar. The dust temperature drops to $\\sim$40 K at a radius of\n$\\sim$8 kpc, suggesting that heating by the quasar beyond that distance is\nnondominant. The dust in the connecting streams between the host and companion\ngalaxies is at temperatures between 30-40 K, typical of starburst galaxies,\nsuggesting it is most likely heated by recent, in-situ star formation. This is\nthe first time dust properties are spatially resolved over several tens of kpc\nin a galaxy system beyond Cosmic Noon --this is more than six times the scales\npreviously probed in galaxies at those redshifts.",
        "In this study, we propose a new method for constrained combinatorial\noptimization using variational quantum circuits. Quantum computers are\nconsidered to have the potential to solve large combinatorial optimization\nproblems faster than classical computers. Variational quantum algorithms, such\nas Variational Quantum Eigensolver (VQE), have been studied extensively because\nthey are expected to work on noisy intermediate scale devices. Unfortunately,\nmany optimization problems have constraints, which induces infeasible solutions\nduring VQE process. Recently, several methods for efficiently solving\nconstrained combinatorial optimization problems have been proposed by designing\na quantum circuit so as to output only the states that satisfy the constraints.\nHowever, the types of available constraints are still limited. Therefore, we\nhave started to develop variational quantum circuits that can handle a wider\nrange of constraints. The proposed method utilizes a forwarding operation that\nmaps from feasible states for subproblems to those for larger subproblems. As\nlong as appropriate forwarding operations can be defined, iteration of this\nprocess can inductively construct variational circuits outputting feasible\nstates even in the case of multiple and complex constraints. In this paper, the\nproposed method was applied to facility location problem and was found to\nincrease the probability for measuring feasible solutions or optimal solutions.\nIn addition, the cost of the obtained circuit was comparable to that of\nconventional variational circuits.",
        "A recurring and important task in control engineering is parameter tuning\nunder constraints, which conceptually amounts to optimization of a blackbox\nfunction accessible only through noisy evaluations. For example, in control\npractice parameters of a pre-designed controller are often tuned online in\nfeedback with a plant, and only safe parameter values should be tried, avoiding\nfor example instability. Recently, machine learning methods have been deployed\nfor this important problem, in particular, Bayesian optimization (BO). To\nhandle safety constraints, algorithms from safe BO have been utilized,\nespecially SafeOpt-type algorithms, which enjoy considerable popularity in\nlearning-based control, robotics, and adjacent fields. However, we identify two\nsignificant obstacles to practical safety. First, SafeOpt-type algorithms rely\non quantitative uncertainty bounds, and most implementations replace these by\ntheoretically unsupported heuristics. Second, the theoretically valid\nuncertainty bounds crucially depend on a quantity - the reproducing kernel\nHilbert space norm of the target function - that at present is impossible to\nreliably bound using established prior engineering knowledge. By careful\nnumerical experiments we show that these issues can indeed cause safety\nviolations. To overcome these problems, we propose Lipschitz-only Safe Bayesian\nOptimization (LoSBO), a safe BO algorithm that relies only on a known Lipschitz\nbound for its safety. Furthermore, we propose a variant (LoS-GP-UCB) that\navoids gridding of the search space and is therefore applicable even for\nmoderately high-dimensional problems.",
        "A common approach to generative modeling is to split model-fitting into two\nblocks: define first how to sample noise (e.g. Gaussian) and choose next what\nto do with it (e.g. using a single map or flows). We explore in this work an\nalternative route that ties sampling and mapping. We find inspiration in moment\nmeasures, a result that states that for any measure $\\rho$ supported on a\ncompact convex set of $\\mathbb{R}^d$, there exists a unique convex potential\n$u$ such that $\\rho=\\nabla u\\,\\sharp\\,e^{-u}$. While this does seem to tie\neffectively sampling (from log-concave distribution $e^{-u}$) and action\n(pushing particles through $\\nabla u$), we observe on simple examples (e.g.,\nGaussians or 1D distributions) that this choice is ill-suited for practical\ntasks. We study an alternative factorization, where $\\rho$ is factorized as\n$\\nabla w^*\\,\\sharp\\,e^{-w}$, where $w^*$ is the convex conjugate of $w$. We\ncall this approach conjugate moment measures, and show far more intuitive\nresults on these examples. Because $\\nabla w^*$ is the Monge map between the\nlog-concave distribution $e^{-w}$ and $\\rho$, we rely on optimal transport\nsolvers to propose an algorithm to recover $w$ from samples of $\\rho$, and\nparameterize $w$ as an input-convex neural network.",
        "Qubit Touchdown is a two-player, competitive board game that was developed to\nintroduce students to quantum computing. A quantum computer is a new kind of\ncomputer that is based on the laws of quantum physics, and it can solve certain\nproblems faster than normal computers because it follows a different set of\nrules. Qubit Touchdown's game play mirrors the rules of (American) football,\nwith players taking turns moving the football to score the most touchdowns, and\nno knowledge of quantum computing is needed to play the game. We evaluated the\ngame with 107 public high school students in Precalculus, Advanced Placement\n(AP) Statistics, and\/or AP Physics 1 courses, assessing whether their interest\nin and self-efficacy toward quantum computing changed as a result of playing\nthe game and learning about its connections to quantum computing. We also\nassessed whether the game was easy to learn and enjoyable. We found that\nstudents' self-efficacy was improved by 33.4%, and they widely considered the\ngame accessible and fun. Thus, Qubit Touchdown could be an effective resource\nto introduce students to Quantum Computing and boost their confidence in\nlearning about the field. Free printables of the game are available, and\nprofessionally produced copies can be purchased on demand.",
        "This work presents a novel formulation and numerical strategy for the\nsimulation of geometrically nonlinear structures. First, a non-canonical\nHamiltonian (Poisson) formulation is introduced by including the dynamics of\nthe stress tensor. This framework is developed for von-K\\'arm\\'an\nnonlinearities in beams and plates, as well as finite strain elasticity with\nSaint-Venant material behavior. In the case of plates, both negligible and\nnon-negligible membrane inertia are considered. For the former case the\ntwo-dimensional elasticity complex is leveraged to express the dynamics in\nterms of the Airy stress function. The finite element discretization employs a\nmixed approach, combining a conforming approximation for displacement and\nvelocity fields with a discontinuous stress tensor representation. A staggered,\nlinear implicit time integration scheme is proposed, establishing connections\nwith existing explicit-implicit energy-preserving methods. The stress degrees\nof freedom are statically condensed, reducing the computational complexity to\nsolving a system with a positive definite matrix. The methodology is validated\nthrough numerical experiments on the Duffing oscillator, a von-K\\'arm\\'an beam,\nand a column undergoing finite strain elasticity. Comparisons with fully\nimplicit energy-preserving method and the explicit Newmark scheme demonstrate\nthat the proposed approach achieves superior accuracy while maintaining energy\nstability. Additionally, it enables larger time steps compared to explicit\nschemes and exhibits computational efficiency comparable to the leapfrog\nmethod.",
        "We show that for a list-arithmetic pretopos $\\mathcal{E}$ with pullback\nstable coequalisers, the $2$-category $\\mathbf{Cat}(\\mathcal{E})$ of internal\ncategories, functors and natural transformations has finite $2$-colimits.",
        "Advances in cellular imaging technologies, especially those based on\nfluorescence in situ hybridization (FISH) now allow detailed visualization of\nthe spatial organization of human or bacterial cells. Quantifying this spatial\norganization is crucial for understanding the function of multicellular tissues\nor biofilms, with implications for human health and disease. To address the\nneed for better methods to achieve such quantification, we propose a flexible\nmultivariate point process model that characterizes and estimates complex\nspatial interactions among multiple cell types. The proposed Bayesian framework\nis appealing due to its unified estimation process and the ability to directly\nquantify uncertainty in key estimates of interest, such as those of inter-type\ncorrelation and the proportion of variance due to inter-type relationships. To\nensure stable and interpretable estimation, we consider shrinkage priors for\ncoefficients associated with latent processes. Model selection and comparison\nare conducted by using a deviance information criterion designed for models\nwith latent variables, effectively balancing the risk of overfitting with that\nof oversimplifying key quantities. Furthermore, we develop a hierarchical\nmodeling approach to integrate multiple image-specific estimates from a given\nsubject, allowing inference at both the global and subject-specific levels. We\napply the proposed method to microbial biofilm image data from the human tongue\ndorsum and find that specific taxon pairs, such as Streptococcus\nmitis-Streptococcus salivarius and Streptococcus mitis-Veillonella, exhibit\nstrong positive spatial correlations, while others, such as Actinomyces-Rothia,\nshow slight negative correlations. For most of the taxa, a substantial portion\nof spatial variance can be attributed to inter-taxon relationships.",
        "We extend previous holographic studies of the Chiral Magnetic Effect (CME) by\nincorporating a time-dependent magnetic field. Various magnetic field profiles\nproposed in the literature are implemented, and their impact on the CME signal\nis analyzed in both static and expanding backgrounds. Interestingly, the\nintegrated chiral magnetic current can exhibit a non-monotonic dependence on\nthe collision energy. Our results suggest that the CME signal is enhanced at\ncollision energies below $\\sqrt{s}=200$ GeV. In addition, we derive a\nquasi-equilibrium formula for the chiral magnetic effect in the expanding\nbackground that is valid at late times.",
        "We consider rotational beta expansions in dimensions 1, 2 and 4 and view them\nas expansions on real numbers, complex numbers, and quaternions, respectively.\nWe give sufficient conditions on the parameters $\\alpha, \\beta \\in (0,1)$ so\nthat particular cylinder sets arising from the expansions are winning or losing\nSchmidt $(\\alpha,\\beta)$-game.",
        "In this paper, we use a nonperturbative renormalization group approach to\nconstruct the dynamical phase space of a quantum spin glass in the large $N$\nlimit. The disordered Hamiltonian is of ``$2 + p$\" type, and we perform a\ncoarse-graining procedure over the Wigner spectrum for the matrix-like\ndisorder. The phase space reconstruction relies on phase transitions derived\nfrom the Luttinger-Ward functional, which accounts for interactions that are\nforbidden by perturbation theory. Various phases are identified, characterized\nby large correlations between replicas and\/or the breaking of time translation\nsymmetry."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b11",
    "start_title":"Testing conditional independence of discrete distributions",
    "start_abstract":"We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary.",
    "start_categories":[
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Learning Parities with Neural Networks"
      ],
      "abstract":[
        "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Pareto Optimization with Robust Evaluation for Noisy Subset Selection",
        "Dendritic Localized Learning: Toward Biologically Plausible Algorithm",
        "Extract-QD Framework: A Generic Approach for Quality-Diversity in Noisy,\n  Stochastic or Uncertain Domains",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "An Enhancement of Cuckoo Search Algorithm for Optimal Earthquake\n  Evacuation Space Allocation in Intramuros, Manila City",
        "Channel-wise Parallelizable Spiking Neuron with Multiplication-free\n  Dynamics and Large Temporal Receptive Fields",
        "Cascaded Large-Scale TSP Solving with Unified Neural Guidance: Bridging\n  Local and Population-based Search",
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for\n  Efficient Black-Box Neural Network Optimization",
        "Hardware-In-The-Loop Training of a 4f Optical Correlator with\n  Logarithmic Complexity Reduction for CNNs",
        "The working principles of model-based GAs fall within the PAC framework:\n  A mathematical theory of problem decomposition",
        "A RankNet-Inspired Surrogate-Assisted Hybrid Metaheuristic for Expensive\n  Coverage Optimization",
        "Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate\n  Tool for Ligand Strain Calculations",
        "Towards Fair and Robust Face Parsing for Generative AI: A\n  Multi-Objective Approach",
        "On the generalized eigenvalue problem in subspace-based excited state\n  methods for quantum computers",
        "Measuring Similarity in Causal Graphs: A Framework for Semantic and\n  Structural Analysis",
        "Hyperparameters in Score-Based Membership Inference Attacks",
        "Tensor renormalization group study of the two-dimensional lattice U(1)\n  gauge-Higgs model with a topological $\\theta$ term under L\\\"uscher's\n  admissibility condition",
        "Nonflat bands and chiral symmetry in magic-angle twisted bilayer\n  graphene",
        "Effect of imaginary gauge on wave transport in driven-dissipative\n  systems",
        "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
        "Explainable Reinforcement Learning via Temporal Policy Decomposition",
        "Towards Change Impact Analysis in Microservices-based System Evolution",
        "RIO EPICS device support application case study on an ion source control\n  system (ISHP)",
        "Migration of phthalate plasticisers in heritage objects made of\n  poly(vinyl chloride): mechanical and environmental aspects",
        "Near-extremal dumb holes and some aspects of the Hawking effect",
        "Can Safety Fine-Tuning Be More Principled? Lessons Learned from\n  Cybersecurity"
      ],
      "abstract":[
        "Subset selection is a fundamental problem in combinatorial optimization,\nwhich has a wide range of applications such as influence maximization and\nsparse regression. The goal is to select a subset of limited size from a ground\nset in order to maximize a given objective function. However, the evaluation of\nthe objective function in real-world scenarios is often noisy. Previous\nalgorithms, including the greedy algorithm and multi-objective evolutionary\nalgorithms POSS and PONSS, either struggle in noisy environments or consume\nexcessive computational resources. In this paper, we focus on the noisy subset\nselection problem with a cardinality constraint, where the evaluation of a\nsubset is noisy. We propose a novel approach based on Pareto Optimization with\nRobust Evaluation for noisy subset selection (PORE), which maximizes a robust\nevaluation function and minimizes the subset size simultaneously. PORE can\nefficiently identify well-structured solutions and handle computational\nresources, addressing the limitations observed in PONSS. Our experiments,\nconducted on real-world datasets for influence maximization and sparse\nregression, demonstrate that PORE significantly outperforms previous methods,\nincluding the classical greedy algorithm, POSS, and PONSS. Further validation\nthrough ablation studies confirms the effectiveness of our robust evaluation\nfunction.",
        "Backpropagation is the foundational algorithm for training neural networks\nand a key driver of deep learning's success. However, its biological\nplausibility has been challenged due to three primary limitations: weight\nsymmetry, reliance on global error signals, and the dual-phase nature of\ntraining, as highlighted by the existing literature. Although various\nalternative learning approaches have been proposed to address these issues,\nmost either fail to satisfy all three criteria simultaneously or yield\nsuboptimal results. Inspired by the dynamics and plasticity of pyramidal\nneurons, we propose Dendritic Localized Learning (DLL), a novel learning\nalgorithm designed to overcome these challenges. Extensive empirical\nexperiments demonstrate that DLL satisfies all three criteria of biological\nplausibility while achieving state-of-the-art performance among algorithms that\nmeet these requirements. Furthermore, DLL exhibits strong generalization across\na range of architectures, including MLPs, CNNs, and RNNs. These results,\nbenchmarked against existing biologically plausible learning algorithms, offer\nvaluable empirical insights for future research. We hope this study can inspire\nthe development of new biologically plausible algorithms for training\nmultilayer networks and advancing progress in both neuroscience and machine\nlearning.",
        "Quality-Diversity (QD) has demonstrated potential in discovering collections\nof diverse solutions to optimisation problems. Originally designed for\ndeterministic environments, QD has been extended to noisy, stochastic, or\nuncertain domains through various Uncertain-QD (UQD) methods. However, the\nlarge number of UQD methods, each with unique constraints, makes selecting the\nmost suitable one challenging. To remedy this situation, we present two\ncontributions: first, the Extract-QD Framework (EQD Framework), and second,\nExtract-ME (EME), a new method derived from it. The EQD Framework unifies\nexisting approaches within a modular view, and facilitates developing novel\nmethods by interchanging modules. We use it to derive EME, a novel method that\nconsistently outperforms or matches the best existing methods on standard\nbenchmarks, while previous methods show varying performance. In a second\nexperiment, we show how our EQD Framework can be used to augment existing QD\nalgorithms and in particular the well-established\nPolicy-Gradient-Assisted-MAP-Elites method, and demonstrate improved\nperformance in uncertain domains at no additional evaluation cost. For any new\nuncertain task, our contributions now provide EME as a reliable \"first guess\"\nmethod, and the EQD Framework as a tool for developing task-specific\napproaches. Together, these contributions aim to lower the cost of adopting UQD\ninsights in QD applications.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "The Cuckoo Search Algorithm (CSA), while effective in solving complex\noptimization problems, faces limitations in random population initialization\nand reliance on fixed parameters. Random initialization of the population often\nresults in clustered solutions, resulting in uneven exploration of the search\nspace and hindering effective global optimization. Furthermore, the use of\nfixed values for discovery rate and step size creates a trade-off between\nsolution accuracy and convergence speed. To address these limitations, an\nEnhanced Cuckoo Search Algorithm (ECSA) is proposed. This algorithm utilizes\nthe Sobol Sequence to generate a more uniformly distributed initial population\nand incorporates Cosine Annealing with Warm Restarts to dynamically adjust the\nparameters. The performance of the algorithms was evaluated on 13 benchmark\nfunctions (7 unimodal, 6 multimodal). Statistical analyses were conducted to\ndetermine the significance and consistency of the results. The ECSA outperforms\nthe CSA in 11 out of 13 benchmark functions with a mean fitness improvement of\n30% across all functions, achieving 35% for unimodal functions and 24% for\nmultimodal functions. The enhanced algorithm demonstrated increased convergence\nefficiency, indicating its superiority to the CSA in solving a variety of\noptimization problems. The ECSA is subsequently applied to optimize earthquake\nevacuation space allocation in Intramuros, Manila.",
        "Spiking Neural Networks (SNNs) are distinguished from Artificial Neural\nNetworks (ANNs) for their sophisticated neuronal dynamics and sparse binary\nactivations (spikes) inspired by the biological neural system. Traditional\nneuron models use iterative step-by-step dynamics, resulting in serial\ncomputation and slow training speed of SNNs. Recently, parallelizable spiking\nneuron models have been proposed to fully utilize the massive parallel\ncomputing ability of graphics processing units to accelerate the training of\nSNNs. However, existing parallelizable spiking neuron models involve dense\nfloating operations and can only achieve high long-term dependencies learning\nability with a large order at the cost of huge computational and memory costs.\nTo solve the dilemma of performance and costs, we propose the mul-free\nchannel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable\nfor SNNs' resource-restricted application scenarios. The proposed neuron\nimports the channel-wise convolution to enhance the learning ability, induces\nthe sawtooth dilations to reduce the neuron order, and employs the bit shift\noperation to avoid multiplications. The algorithm for design and implementation\nof acceleration methods is discussed meticulously. Our methods are validated in\nneuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and\nneuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.\nTraining speed results demonstrate the effectiveness of our acceleration\nmethods, providing a practical reference for future research.",
        "The traveling salesman problem (TSP) is a fundamental NP-hard optimization\nproblem. This work presents UNiCS, a novel unified neural-guided cascaded\nsolver for solving large-scale TSP instances. UNiCS comprises a local search\n(LS) phase and a population-based search (PBS) phase, both guided by a learning\ncomponent called unified neural guidance (UNG). Specifically, UNG guides\nsolution generation across both phases and determines appropriate phase\ntransition timing to effectively combine the complementary strengths of LS and\nPBS. While trained only on simple distributions with relatively small-scale TSP\ninstances, UNiCS generalizes effectively to challenging TSP benchmarks\ncontaining much larger instances (10,000-71,009 nodes) with diverse node\ndistributions entirely unseen during training. Experimental results on the\nlarge-scale TSP instances demonstrate that UNiCS consistently outperforms\nstate-of-the-art methods, with its advantage remaining consistent across\nvarious runtime budgets.",
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "Swarm intelligence optimization algorithms have gained significant attention\ndue to their ability to solve complex optimization problems. However, the\nefficiency of optimization in large-scale problems limits the use of related\nmethods. This paper presents a GPU-accelerated version of the Multi-Guiding\nSpark Fireworks Algorithm (MGFWA), which significantly improves the\ncomputational efficiency compared to its traditional CPU-based counterpart. We\nbenchmark the GPU-MGFWA on several neural network black-box optimization\nproblems and demonstrate its superior performance in terms of both speed and\nsolution quality. By leveraging the parallel processing power of modern GPUs,\nthe proposed GPU-MGFWA results in faster convergence and reduced computation\ntime for large-scale optimization tasks. The proposed implementation offers a\npromising approach to accelerate swarm intelligence algorithms, making them\nmore suitable for real-time applications and large-scale industrial problems.\nSource code is released at https:\/\/github.com\/mxxxr\/MGFWA.",
        "This work evaluates a forward-only learning algorithm on the MNIST dataset\nwith hardware-in-the-loop training of a 4f optical correlator, achieving 87.6%\naccuracy with O(n2) complexity, compared to backpropagation, which achieves\n88.8% accuracy with O(n2 log n) complexity.",
        "The concepts of linkage, building blocks, and problem decomposition have long\nexisted in the genetic algorithm (GA) field and have guided the development of\nmodel-based GAs for decades. However, their definitions are usually vague,\nmaking it difficult to develop theoretical support. This paper provides an\nalgorithm-independent definition to describe the concept of linkage. With this\ndefinition, the paper proves that any problems with a bounded degree of linkage\nare decomposable and that proper problem decomposition is possible via linkage\nlearning. The way of decomposition given in this paper also offers a new\nperspective on nearly decomposable problems with bounded difficulty and\nbuilding blocks from the theoretical aspect. Finally, this paper relates\nproblem decomposition to PAC learning and proves that the global optima of\nthese problems and the minimum decomposition blocks are PAC learnable under\ncertain conditions.",
        "Coverage optimization generally involves deploying a set of facilities to\nbest satisfy the demands of specified points, with broad applications in fields\nsuch as location science and sensor networks. Recent applications reveal that\nthe subset site selection coupled with continuous angular parameter\noptimization can be formulated as Mixed-Variable Optimization Problems (MVOPs).\nMeanwhile, high-fidelity discretization and visibility analysis significantly\nincrease computational cost and complexity, evolving the MVOP into an Expensive\nMixed-Variable Optimization Problem (EMVOP). While canonical Evolutionary\nAlgorithms have yielded promising results, their reliance on numerous fitness\nevaluations is too costly for our problem. Furthermore, most surrogate-assisted\nmethods face limitations due to their reliance on regression-based models. To\naddress these issues, we propose the RankNet-Inspired Surrogate-assisted Hybrid\nMetaheuristic (RI-SHM), an extension of our previous work. RI-SHM integrates\nthree key components: (1) a RankNet-based pairwise global surrogate that\ninnovatively predicts rankings between pairs of individuals, bypassing the\nchallenges of fitness estimation in discontinuous solution space; (2) a\nsurrogate-assisted local Estimation of Distribution Algorithm that enhances\nlocal exploitation and helps escape from local optima; and (3) a fitness\ndiversity-driven switching strategy that dynamically balances exploration and\nexploitation. Experiments demonstrate that our algorithm can effectively handle\nlarge-scale coverage optimization tasks of up to 300 dimensions and more than\n1,800 targets within desirable runtime. Compared to state-of-the-art algorithms\nfor EMVOPs, RI-SHM consistently outperforms them by up to 56.5$\\%$ across all\ntested instances.",
        "Ligand strain energy, the energy difference between the bound and unbound\nconformations of a ligand, is an important component of structure-based small\nmolecule drug design. A large majority of observed ligands in protein-small\nmolecule co-crystal structures bind in low-strain conformations, making strain\nenergy a useful filter for structure-based drug design. In this work we present\na tool for calculating ligand strain with a high accuracy. StrainRelief uses a\nMACE Neural Network Potential (NNP), trained on a large database of Density\nFunctional Theory (DFT) calculations to estimate ligand strain of neutral\nmolecules with quantum accuracy. We show that this tool estimates strain energy\ndifferences relative to DFT to within 1.4 kcal\/mol, more accurately than\nalternative NNPs. These results highlight the utility of NNPs in drug\ndiscovery, and provide a useful tool for drug discovery teams.",
        "Face parsing is a fundamental task in computer vision, enabling applications\nsuch as identity verification, facial editing, and controllable image\nsynthesis. However, existing face parsing models often lack fairness and\nrobustness, leading to biased segmentation across demographic groups and errors\nunder occlusions, noise, and domain shifts. These limitations affect downstream\nface synthesis, where segmentation biases can degrade generative model outputs.\nWe propose a multi-objective learning framework that optimizes accuracy,\nfairness, and robustness in face parsing. Our approach introduces a\nhomotopy-based loss function that dynamically adjusts the importance of these\nobjectives during training. To evaluate its impact, we compare multi-objective\nand single-objective U-Net models in a GAN-based face synthesis pipeline\n(Pix2PixHD). Our results show that fairness-aware and robust segmentation\nimproves photorealism and consistency in face generation. Additionally, we\nconduct preliminary experiments using ControlNet, a structured conditioning\nmodel for diffusion-based synthesis, to explore how segmentation quality\ninfluences guided image generation. Our findings demonstrate that\nmulti-objective face parsing improves demographic consistency and robustness,\nleading to higher-quality GAN-based synthesis.",
        "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers.",
        "Causal graphs are commonly used to understand and model complex systems.\nResearchers often construct these graphs from different perspectives, leading\nto significant variations for the same problem. Comparing causal graphs is,\ntherefore, essential for evaluating assumptions, integrating insights, and\nresolving disagreements. The rise of AI tools has further amplified this need,\nas they are increasingly used to generate hypothesized causal graphs by\nsynthesizing information from various sources such as prior research and\ncommunity inputs, providing the potential for automating and scaling causal\nmodeling for complex systems. Similar to humans, these tools also produce\ninconsistent results across platforms, versions, and iterations. Despite its\nimportance, research on causal graph comparison remains scarce. Existing\nmethods often focus solely on structural similarities, assuming identical\nvariable names, and fail to capture nuanced semantic relationships, which is\nessential for causal graph comparison. We address these gaps by investigating\nmethods for comparing causal graphs from both semantic and structural\nperspectives. First, we reviewed over 40 existing metrics and, based on\npredefined criteria, selected nine for evaluation from two threads of machine\nlearning: four semantic similarity metrics and five learning graph kernels. We\ndiscuss the usability of these metrics in simple examples to illustrate their\nstrengths and limitations. We then generated a synthetic dataset of 2,000\ncausal graphs using generative AI based on a reference diagram. Our findings\nreveal that each metric captures a different aspect of similarity, highlighting\nthe need to use multiple metrics.",
        "Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA.",
        "We investigate the two-dimensional lattice U(1) gauge-Higgs model with a\ntopological term, employing L\\\"uscher's admissibility condition. The standard\nMonte Carlo simulation for this model is hindered not only by the complex\naction problem due to the topological term but also by the topological freezing\nproblem originating from the admissibility condition. Resolving both obstacles\nsimultaneously with the tensor renormalization group approach, we show the\nadvantage of the admissibility condition in dealing with the topological term\ndiscretized with the so-called field-theoretical definition.",
        "In this work, we study an interacting tight-binding model of magic-angle\ntwisted bilayer graphene (MATBG), with a twist angle of $1.05^\\circ$. We derive\neffective theories based on a mean-field normal state at charge neutrality,\nthereby including the renormalizations coming from integrating out high-energy\nmodes. In these theories, the flat bands display a sizable increase of the\nbandwidth, suggesting a renormalization of the magic angle. Additionally, the\ncorresponding wavefunctions flow towards the limit of perfect particle-hole\nsymmetry and sublattice polarization (the 'chiral' limit). We further represent\nthe flat bands in the 'vortex Chern' basis and discuss the implications on the\ndynamics regarding the 'flat' and 'chiral' symmetries of MATBG, as manifested\nin the symmetry-broken states at neutrality.",
        "Wave transport in disordered media is a fundamental problem with direct\nimplications in condensed matter, materials science, optics, atomic physics,\nand even biology. The majority of studies are focused on Hermitian systems to\nunderstand disorder-induced localization. However, recent studies of\nnon-Hermitian disordered media have revealed unique behaviors, with a universal\nprinciple emerging that links the eigenvalue spectrum of the disordered\nHamiltonian and its statistics with its transport properties. In this work we\nshow that the situation can be very different in driven-dissipative lattices of\ncavities, where a uniform gain applied equally to all the components of the\nsystem can act as a knob for controlling the wave transport properties without\naltering the eigenvalue statistics of the underlying Hamiltonian. Our results\nopen a new avenue for developing a deeper insight into the transport properties\nin disordered media and will aid in building new devices as well. Our work\nwhich is presented in the context of optics generalizes to any physical\nplatforms where gain can be implemented. These include acoustics, electronics,\nand coupled quantum oscillators such as atoms, diamond centers and\nsuperconducting qubits.",
        "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
        "We investigate the explainability of Reinforcement Learning (RL) policies\nfrom a temporal perspective, focusing on the sequence of future outcomes\nassociated with individual actions. In RL, value functions compress information\nabout rewards collected across multiple trajectories and over an infinite\nhorizon, allowing a compact form of knowledge representation. However, this\ncompression obscures the temporal details inherent in sequential\ndecision-making, presenting a key challenge for interpretability. We present\nTemporal Policy Decomposition (TPD), a novel explainability approach that\nexplains individual RL actions in terms of their Expected Future Outcome (EFO).\nThese explanations decompose generalized value functions into a sequence of\nEFOs, one for each time step up to a prediction horizon of interest, revealing\ninsights into when specific outcomes are expected to occur. We leverage\nfixed-horizon temporal difference learning to devise an off-policy method for\nlearning EFOs for both optimal and suboptimal actions, enabling contrastive\nexplanations consisting of EFOs for different state-action pairs. Our\nexperiments demonstrate that TPD generates accurate explanations that (i)\nclarify the policy's future strategy and anticipated trajectory for a given\naction and (ii) improve understanding of the reward composition, facilitating\nfine-tuning of the reward function to align with human expectations.",
        "Cloud-native systems are the mainstream for enterprise solutions, given their\nscalability, resilience, and other benefits. While the benefits of cloud-native\nsystems fueled by microservices are known, less guidance exists on their\nevolution. One could assume that since microservices encapsulate their code,\ncode changes remain encapsulated as well; however, the community is becoming\nmore aware of the possible consequences of code change propagation across\nmicroservices. Moreover, an active mitigation instrument for negative\nconsequences of change propagation across microservices (i.e., ripple effect)\nis yet missing, but the microservice community would greatly benefit from it.\nThis paper introduces what it could look like to have an infrastructure to\nassist with change impact analysis across the entire microservice system and\nintends to facilitate advancements in laying out the foundations and building\nguidelines on microservice system evolution. It shares a new direction for\nincremental software architecture reconstruction that could serve as the\ninfrastructure concept and demonstrates early results from prototyping to\nillustrate the potential impact.",
        "Experimental Physics and Industrial Control System (EPICS) is a software tool\nthat during last years has become relevant as a main framework to deploy\ndistributed control systems in large scientific environments. At the moment,\nESS Bilbao uses this middleware to perform the control of their Ion Source\nHydrogen Positive (ISHP) project. The implementation of the control system was\nbased on: PXI Real Time controllers using the LabVIEW-RT and LabVIEW-EPICS\ntools; and RIO devices based on Field-Programmable Gate Array (FPGA)\ntechnology. Intended to provide a full compliant EPICS IOCs for RIO devices and\nto avoid additional efforts on the system maintainability, a migration of the\ncurrent system to a derivative Red Hat Linux (CentOS) environment has been\nconducted. This paper presents a real application case study for using the\nNIRIO EPICS device support (NIRIO-EDS) to give support to the ISHP. Although\nRIO FPGA configurations are particular solutions for ISHP performance, the\nNIRIO-EDS has permitted the control and monitoring of devices by applying a\nwell-defined design methodology into the previous FPGA configuration for\nRIO\/FlexRIO devices. This methodology has permitted a fast and easy deployment\nfor the new robust, scalable and maintainable software to support RIO devices\ninto the ISHP control architecture.",
        "To clean or not to clean? The solution to this dilemma is related to\nunderstanding the plasticiser migration which has a few practical implications\nfor the state of museum artefacts made of plasticised poly(vinyl chloride) -\nPVC and objects stored in their vicinity. The consequences of this process\nencompass aesthetic changes due to the presence of exudates and dust\ndeposition, an increase in air pollution and the development of mechanical\nstresses. Therefore, this paper discusses the plasticiser migration in PVC to\nprovide evidence and support the development of recommendations and guidelines\nfor conservators, collection managers and heritage scientists. Particularly,\nthe investigation is focused on the migration of the ortho-phthalates\nrepresenting the group of the most abundant plasticisers in PVC collections.\nThe predominance of inner diffusion or surface emission (evaporation)\ndetermining the rate-limiting step of the overall migration process is\nconsidered a fundament for understanding the potential environmental and\nmechanical risk. According to this concept, general correlations for various\northo-phthalates are proposed depending on their molar mass with the support of\nmolecular dynamics simulations and NMR diffusometry. The study reveals that for\nthe majority of the PVC objects in collections, the risk of accelerated\nmigration upon mild removal of surface plasticiser exudate is low. Thus,\nsurface cleaning would allow for diminishing dust deposition and air pollution\nby phthalate-emitting objects in a museum environment. Bearing in mind\nsimplicity and the need for fast decision-supporting solutions, the\nstep-by-step protocol for non-destructive identification and quantification of\nplasticisers in objects made of or containing plasticised PVC, determination of\nthe physical state of investigated artefacts and rate-limiting process of\nplasticiser migration is proposed.",
        "We propose novel non-relativistic fluid analogue models, that is dumb hole\nmodels, for extremal and near-extremal black holes. Further we study the\nback-reaction effects of analogue Hawking radiation emitted from these dumb\nholes. We discuss and quantify the reduction in the background fluid velocity\ncaused by radiation of Hawking phonons. In doing so, we speculate on the\nexistence of an emergent Hawking force which leads to the reduction in the\nbackground fluid velocity and which is produced as a consequence of phonon\nemission. In addition to the analogue gravity literature, our results might be\nof relevance to black hole pedagogy.",
        "As LLMs develop increasingly advanced capabilities, there is an increased\nneed to minimize the harm that could be caused to society by certain model\noutputs; hence, most LLMs have safety guardrails added, for example via\nfine-tuning. In this paper, we argue the position that current safety\nfine-tuning is very similar to a traditional cat-and-mouse game (or arms race)\nbetween attackers and defenders in cybersecurity. Model jailbreaks and attacks\nare patched with bandaids to target the specific attack mechanism, but many\nsimilar attack vectors might remain. When defenders are not proactively coming\nup with principled mechanisms, it becomes very easy for attackers to sidestep\nany new defenses. We show how current defenses are insufficient to prevent new\nadversarial jailbreak attacks, reward hacking, and loss of control problems. In\norder to learn from past mistakes in cybersecurity, we draw analogies with\nhistorical examples and develop lessons learned that can be applied to LLM\nsafety. These arguments support the need for new and more principled approaches\nto designing safe models, which are architected for security from the\nbeginning. We describe several such approaches from the AI literature."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cond-mat.dis-nn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
      ],
      "abstract":[
        "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "The timing and spectral properties of the 2022 outburst of SGR\n  J1935+2154 observed with NICER",
        "Evolution of Shock Structures and QPOs After Halting BHL Accretion onto\n  Kerr Black Hole",
        "Evolution of X-ray Gas in SN 1987A from 2007 to 2021: Ring Fading and\n  Ejecta Brightening Unveiled through Differential Emission Measure Analysis",
        "An Unusual Change in the Radio Jets of GRS 1915+105",
        "Populations of Neutron Star Ultraluminous X-ray Sources: Mind your b's\n  and B's",
        "Evidence for an Instability-Induced Binary Merger in the Double-Peaked,\n  Helium-Rich Type IIn Supernova 2023zkd",
        "Early Light Curve Excess in Type IIb Supernovae Observed by the ATLAS\n  Survey: Qualitative Constraints on Progenitor Systems",
        "The Galactic population of magnetars : a simulation-based inference\n  study",
        "Multi-messenger Emission by Magnetically Arrested Disks and Relativistic\n  Jets of Black Hole X-ray Binaries",
        "eRO-ExTra: eROSITA extragalactic non-AGN X-ray transients and variables\n  in eRASS1 and eRASS2",
        "Modeling fast X-ray variability around an accreting black hole",
        "Shock-cooling Constraints via Early-time Observations of the Type IIb SN\n  2022hnt",
        "Origin of holes and rings in the Green Monster of Cassiopeia A: Insights\n  from 3D magnetohydrodynamic simulations",
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection"
      ],
      "abstract":[
        "The magnetar SGR J1935+2154 entered a new active episode on October 10, 2022,\nwith X-ray bursts and enhanced persistent emission. At the tail of high burst\nrate interval, lasting several hours, radio bursts were detected, revealing the\nconnection between the X-ray activities and radio emissions. We analyzed\nobservations of SGR J1935+2154 for nearly three months, using data from Neutron\nStar Interior Composition Explorer (NICER). We report the timing and spectral\nresults following the onset of this outburst. In general, the X-ray flux of the\npersistent emission decays exponentially. While a flare is evident on the light\ncurve, a fast radio burst (FRB) was detected immediately following the peak of\nthis flare. We found a phase jump of pulse profile, with a deviation of\n$0.16\\pm0.03$ phase, which is related to the glitch. The spectra are well fit\nwith the combination of a blackbody and a power law model. The decay of the\noutburst is dominated by the drop of the non-thermal component, which also\nleads to the increase of thermal proportion. The photon index of the power law\nis inversely correlated with both the unabsorbed flux and the burst rate. We\nfind that unlike the large variety of the persistent emission around FRB\n221014, the X-ray properties are very stable when FRBs 221021 and 221201\nhappened. These results manifest the connection between glitch, phase jump,\nX-ray burst, and radio burst, crucial for studying the mutation in twisted\nmagnetic fields and constraining the trigger mechanism of radio bursts.",
        "One of the mechanisms responsible for disk formation around the black holes\nis Bondi-Hoyle-Lyttleton (BHL) accretion.The fact that BHL accretion can be\ninterrupted by various astrophysical phenomena, such as stellar winds or\nastrophysical jets, makes it crucial to study the behavior of shock cones\nformed by BHL accretion around black holes once the accretion process is\nhalted. Investigating the new plasma structures that emerge in these scenarios\ncan provide insights into observational results. In this context, a new plasma\nstructure forming around the Kerr black hole has been numerically modeled as a\nfunction of the black hole spin parameter and the asymptotic velocity of BHL\naccretion. The numerical analysis revealed that high spin (a\/M=0.9) and\nsupersonic flow ( M > 1) are necessary conditions for low-frequency\nquasi-periodic oscillations (LFQPOs) formation. On the other hand, the\nfundamental mode of the high-frequency quasi-periodic oscillations (HFQPOs) are\nfound to be independent of both the black hole spin and asymptotic velocity and\nare instead governed by general relativistic effects. Additionally, the study\ndemonstrated that for 3:2 and 2:1 resonance states to form, nonlinear couplings\nneeds to be occurred when the black hole rotates rapidly. These couplings\nproduce harmonic frequencies, providing an explanation for the observed\nquasi-periodic oscillation (QPO) resonances in black hole binaries. These\nfindings align with precession models and nonlinear resonance models, both of\nwhich play a crucial role in QPO generation. Finally, the LFQPOs and HFQPOs\nobtained from numerical simulations are consistent with the observed QPO\nfrequencies in the microquasars GRS 1915+105 and XTE J1550-564, as well as in\nthe AGN REJ1034+396, which harbors a supermassive black hole at its center.",
        "As the nearest supernova (SN) observed since Kepler's SN of 1604, SN 1987A\nprovides an unprecedented opportunity to study in detail the early evolution of\nsupernova remnants (SNRs). Despite extensive studies through both observations\nand simulations, there is still an urgent need for a more effective approach to\nintegrate the results from two sides. In this study, we conducted a detailed\ndifferential emission measure (DEM) analysis on the XMM-Newton observations\ntaken in 2007 to 2021 to characterize the continuous temperature structure of\nSN 1987A, which can be better compared with simulations. The X-ray plasma\nexhibit a temperature distribution with a major peak at $\\sim0.5$-$1$ keV and a\nhigh-temperature tail extending to $\\gtrsim5$ keV. The emission measure (EM) of\nthe major peak started to decline around 2014, while the EM of the tail\ncontinued increasing and appears to have formed a secondary peak at $\\sim3$-$5$\nkeV in recent years. Our DEM results consistent well with simulations, which\nhelp to further identify the major peak as originating from the equatorial ring\nand the secondary peak as arising from the newly shocked ejecta. Together with\nthe simulations, our DEM analysis reveals recent fading of the ring and\nbrightening of the ejecta in X-rays from SN 1987A. Additionally, we observed a\nrecent decrease in the centroid energy of Fe K line, providing further evidence\nof newly shocked ejecta.",
        "We compare Very Large Array observations of GRS 1915+105 made in 1994 and\n2023, with nearly three decades of difference. The source has experienced\nintriguing major changes. The position angle of the bipolar ejecta in the plane\nof the sky has increased counterclockwise by 24 degrees. The inclination angle\nof the flow with respect to the line of sight has increased by 17 degrees.\nAnalysis of GRS 1915+105 images over the years suggest that the observed\nchanges took place within a year or less. Our analysis indicates that during\n2023 the plane of the accretion disk was aligned with the line of sight, which\nmay explain the deep X-ray obscured state and the high mid-infrared luminosity\nobserved with JWST in that epoch. More recent 2024 observations imply that the\nposition angle of the ejecta has returned to its historic values. We suggest\nthat these abrupt changes could be due to the presence of an undetected\ntertiary component in the system. Future monitoring of the time evolution of\nthe source may further clarify the cause of these remarkable changes.",
        "Ultraluminous X-ray sources (ULXs) with neutron star (NS) accretors challenge\ntraditional accretion models, and have sparked a debate regarding the role of\ngeometrical beaming and strong magnetic fields (B). The reduction of the\nThomson cross-section in the presence of strong B, leads to a modification of\nthe Eddington limit, and therefore is expected to affect significantly the\nobservational appearance of NS-ULXs. We investigate the role of this\nmodification using population synthesis models, and explore its effects on the\nX-ray luminosity functions, spin-up rates, and outflow energetics of the\nobserved NS-ULXs. Our results show that the new prescription allows NS-ULXs to\nachieve super-Eddington luminosities with milder beaming compared to before,\nimproving the agreement with observations. In addition, it broadens the range\nof spin-up rates allowing for more diverse conditions in NS-ULXs in terms of\naccretion rates and magnetic fields. More importantly, the reduced beaming\nincreases the likelihood of observing the NS-ULXs within wind-powered nebulae\nsuch as NGC 5907 ULX-1. Our findings highlight the necessity of taking into\naccount B effects independently of the approach: geometrical beaming or strong\nB, and call for magnetospheric accretion prescriptions that can be integrated\nin population synthesis codes.",
        "We present ultraviolet to infrared observations of the extraordinary Type IIn\nsupernova 2023zkd (SN 2023zkd). Photometrically, it exhibits persistent and\nluminous precursor emission spanning $\\sim$4 years preceding discovery\n($M_r\\approx-15$ mag, 1,500~days in the observer frame), followed by a\nsecondary stage of gradual brightening in its final year. Post-discovery, it\nexhibits two photometric peaks of comparable brightness ($M_r\\lesssim-18.7$ mag\nand $M_r\\approx-18.4$ mag, respectively) separated by 240 days.\nSpectroscopically, SN 2023zkd exhibits highly asymmetric and multi-component\nBalmer and He I profiles that we attribute to ejecta interaction with\nfast-moving ($1,\\!000-2,\\!000\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$) He-rich polar\nmaterial and slow-moving ($\\sim$$400\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$)\nequatorially-distributed H-rich material. He II features also appear during the\nsecond light curve peak and evolve rapidly. Shock-driven models fit to the\nmulti-band photometry suggest that the event is powered by interaction with\n$\\sim$$5-6\\;M_{\\odot}$ of CSM, with $2-3\\;M_{\\odot}$ associated with each light\ncurve peak, expelled during mass-loss episodes $\\sim$$3-4$ and $\\sim$$1-2$\nyears prior to explosion. The observed precursor emission, combined with the\nextreme mass-loss rates required to power each light curve peak, favors either\nsuper-Eddington accretion onto a black hole or multiple long-lived eruptions\nfrom a massive star to luminosities that have not been previously observed. We\nconsider multiple progenitor scenarios for SN 2023zkd, and find that the\nbrightening optical precursor and inferred explosion properties are most\nconsistent with a massive ($M_{\\mathrm{ZAMS}}\\geq30\\;M_{\\odot}$) and\npartially-stripped He star undergoing an instability-induced merger with a\nblack hole companion.",
        "Type IIb supernovae (SNe IIb) often exhibit an early light curve excess (EE)\npreceding the main peak powered by radioactive nickel decay. The physical\norigin of this early emission remains an open question. Among the proposed\nscenarios, shock cooling emission-resulting from the interaction between the\nshockwave and extended envelopes-is considered the most plausible mechanism.\nThe frequency of these events remains unconstrained. This study aims to\nquantify the frequency of EE in SNe IIb and investigate its physical origin by\nanalyzing optical light curves from the Asteroid Terrestrial-impact Last Alert\nSystem (ATLAS) survey. We selected 74 SNe IIb from 153 spectroscopically\nclassified events in the Transient Name Server (TNS) database, observed by\nATLAS, with peak fluxes exceeding 150 {\\mu}Jy and explosion epoch uncertainties\nlower than six days. Using light curve model fitting and outlier analysis, we\nidentified SNe IIb exhibiting EE and analyzed their photometric properties. We\nfound 21 SNe IIb with EE, corresponding to a frequency of approximately 28-40%,\nwith the higher value obtained under the most stringent data cuts. The EE's\nduration and color evolution are consistent with shock cooling in extended\nhydrogen-rich envelopes. We also found that EE SNe IIb have longer rise times\nand faster post-peak decline rates than non-EE SNe IIb, while both groups share\nsimilar peak absolute magnitudes. Our findings suggest that EE and non-EE SNe\nIIb likely share similar initial progenitor masses but differ in ejecta mass\nproperties, potentially due to varying degrees of binary interaction. This\nstudy provides constraints on the evolutionary pathways of SNe IIb progenitors\nas compact stars with and without extended hydrogen envelopes.",
        "Population synthesis modeling of the observed dynamical and physical\nproperties of a population is a highly effective method for constraining the\nunderlying birth parameters and evolutionary tracks. In this work, we apply a\npopulation synthesis model to the canonical magnetar population to gain insight\ninto the parent population. We utilize simulation-based inference to reproduce\nthe observed magnetar population with a model which takes into account the\nsecular evolution of the force-free magnetosphere and magnetic field decay\nsimultaneously and self-consistently. Our observational constraints are such\nthat no magnetar is detected through their persistent emission when convolving\nthe simulated populations with the XMM-Newton EPIC-pn Galactic plane\nobservations, and that all of the $\\sim$30 known magnetars are discovered\nthrough their bursting activity in the last $\\sim50$ years. Under these\nconstraints, we find that, within 95 % credible intervals, the birth rate of\nmagnetars to be $1.8^{+2.6}_{-0.6}$ kyr$^{-1}$, and lead to having\n$10.7^{+18.8}_{-4.4}$ % of neutron stars born as magnetars. We also find a mean\nmagnetic field at birth ($\\mu_b$ is in T) $\\log\\left(\\mu_b\\right) =\n10.2^{+0.1}_{-0.2}$, a magnetic field decay slope $\\alpha_d = 1.9\n^{+0.9}_{-1.3}$, and timescale $\\tau_d = 17.9^{+24.1}_{-14.5}$ kyr, in broad\nagreement with previous estimates. We conclude this study by exploring\ndetection prospects: an all-sky survey with XMM-Newton would potentially allow\nto get around 7 periodic detections of magnetars, with approximately 150\nmagnetars exceeding XMM-Newton's flux threshold, and the upcoming AXIS\nexperiment should allow to double these detections.",
        "Black hole X-ray binaries (BHXBs) are observed in various wavelengths from\nradio to GeV gamma-ray. Several BHXBs, including MAXI J1820+070 and Cygnus X-1,\nare also found to emit ultrahigh-energy (UHE; photon energy $>$100 TeV) gamma\nrays. The origin and production mechanism of the multi-wavelength emission of\nBHXBs are under debate. We propose a scenario where relativistic particles from\nmagnetically arrested disks (MADs), which could form when BHXBs are in\nquiescent or hard states, produce UHE gamma rays, while electrons in the jets\nproduce GeV gamma-ray emission. Specifically, magnetic turbulence in MADs heats\nup and accelerates electrons and protons, while magnetic reconnection in jets\naccelerates electrons. Sub-PeV gamma rays and neutrinos are produced when\nrelativistic protons interact with the thermal protons and the radiation by\nthermal electrons in the disk. We discuss the perspectives of observing sub-PeV\nmulti-messenger signals from individual BHXBs. Finally, we evaluate the\nintegrated fluxes of the quiescent and hard-state BHXB population and find that\nBHXBs may contribute to the Galactic diffuse emission above $\\sim 100$ TeV.",
        "(Abridged) While previous X-ray studies showed the dominance of regular\nactive galactic nuclei (AGN) variability, a small fraction of sources arise\nfrom more exotic phenomena such as tidal disruption events (TDEs),\nquasi-periodic eruptions, or other short-lived events associated with\nsupermassive black hole accretion. This paper describes the systematic\nselection of X-ray extragalactic transients found in the first two eROSITA\nall-sky surveys (eRASS) that are not associated with known AGN prior to eROSITA\nobservations. We generated a variability sample from eRASS1 and eRASS2 (Dec.\n2019-Dec. 2020), which includes sources with a variability significance and a\nfractional amplitude larger than four, located in the Legacy Survey DR10 (LS10)\nfootprint. The properties of LS10 counterparts were used to exclude stars and\nknown AGN. The sample was additionally cleaned using pre-eROSITA\nclassifications, archival optical spectra, and archival X-ray data. The final\ncatalog eRO-ExTra includes 304 extragalactic eROSITA transients and variables\nnot associated with known AGN. More than 90% of sources have reliable LS10\noptical counterparts. For each source, we provide archival X-ray data from\nSwift, ROSAT, and XMM-Newton; the eROSITA long-term light curve (2-2.5 years)\nwith a light curve classification; as well as the best power law fit spectral\nresults at the peak eROSITA epoch. Reliable spectroscopic and photometric\nredshifts are provided for more than 80% of the sample. Several sources in the\ncatalog are known TDE candidates discovered by eROSITA. In addition, 31 sources\nare radio detected. The eRO-ExTra transients constitute a relatively clean\nparent sample of non-AGN variability phenomena associated with massive black\nholes. More than 95% of eRO-ExTra sources were discovered in X-rays with\neROSITA for the first time, which makes it a valuable resource for studying\nunique nuclear transients.",
        "X-ray inter-band time lags are observed during the outbursts of black hole\nX-ray binaries (BHXRBs). Timing analysis of fast variability in low Fourier\nfrequency bands shows that high-energy photons lag behind low-energy photons, a\nphenomenon referred to as hard lag. Conversely, in high Fourier frequency\nbands, low-energy photons lag behind high-energy photons, known as soft lag.\nThis frequency-dependent lag spectrum suggests that the lags arise from\ndifferent physical processes. Notably, a trend has been observed wherein the\nlags shift towards shorter timescales during the rising hard state, indicating\nan evolution in the inner accretion flow. In this study, we simulate these\ninter-band lags by conducting Monte Carlo simulations of the rapid variability\nwithin the geometry of a jet base corona. We consider both inward propagating\naccretion rate fluctuations and reverberation (light crossing) delays in our\nsimulations. We successfully reproduce both low-frequency hard lags and\nhigh-frequency soft lags in a self-consistent manner. We replicate the observed\nevolution of the frequency-dependent lag spectra by varying the geometrical\nscale of the corona and the viscous frequency of the disc. Finally, we discuss\nthe potential of a spherical corona and emphasize that polarization\nobservations from the Imaging X-ray Polarimetry Explorer (IXPE) and the\nenhanced X-ray Timing and Polarimetry mission (eXTP) will be crucial for\ndistinguishing the corona's geometry in future studies.",
        "We report the results of a rapid follow-up campaign on the Type IIb Supernova\n(SN) 2022hnt. We present a daily, multi-band, photometric follow-up using the\nLas Cumbres Observatory, the Zwicky Transient Facility, the orbiting\n\\textit{Swift} observatory, and the Asteroid Terrestrial-impact Last Alert\nSystem (ATLAS). A distinctive feature in the light curve of SN 2022hnt and\nother IIb SNe is an early narrow peak prior to the ${}^{56}$Ni peak caused by\nrapid shock cooling of the hydrogen envelope, which can serve as an important\nprobe of the properties of the massive progenitor star in the moments before\nexplosion. Using SN 2022hnt as a case study, we demonstrate a framework of\nconsiderations for the application of shock cooling models to type IIb SNe,\noutlining a consistent procedure for future surveys of Type IIb SNe progenitor\nand explosion properties. \\hll{We fit several recent models of shock-cooling\nemission and obtain progenitor radii between $\\sim50$ and $\\sim100$ $R_\\odot$,\nas well as hydrogen-enriched envelope masses between $\\sim0.01$ and $\\sim0.1$\n$M_\\odot$, both consistent with values for other IIb SNe. One of these models\nis the model of \\cite{Morag2023}, marking the first time this model has been\napplied to a Type IIb SN.} We evaluate contrasting predictions between\nshock-cooling models to construct a fiducial parameter set which can be used\nfor comparison to other SNe. Finally, we investigate the possibility of\nextended wind breakout or precursor emission captured in the earliest\ndetections.",
        "[Abridged] Cassiopeia A (Cas A) provides a unique opportunity to study\nsupernova (SN) dynamics and interactions with the circumstellar medium (CSM).\nRecent JWST observations revealed the \"Green Monster\" (GM), a structure with a\nlikely CSM origin. We investigate its pockmarked morphology, characterized by\ncircular holes and rings, by examining the role of small-scale ejecta\nstructures interacting with a dense circumstellar shell. We adopted a\nneutrino-driven SN model to trace the evolution of its explosion from core\ncollapse to the age of the Cas A remnant using high-resolution 3D\nmagnetohydrodynamic simulations. Besides other processes, the simulations\ninclude self-consistent calculations of radiative losses, accounting for\ndeviations from electron-proton temperature equilibration and ionization\nequilibrium, as well as the ejecta composition derived from the SN. The GM's\nmorphology is reproduced by dense ejecta clumps and fingers interacting with an\nasymmetric, forward-shocked circumstellar shell. The clumps and fingers form by\nhydrodynamic instabilities growing at the interface between SN ejecta and\nshocked CSM. Radiative cooling accounting for effects of non-equilibrium of\nionization enhances the ejecta fragmentation, forming dense knots and thin\nfilamentary structures that penetrate the shell, producing a network of holes\nand rings with properties similar to those observed. The origin of the holes\nand rings in the GM can be attributed to the interaction of ejecta with a\nshocked circumstellar shell. By constraining the timing of this interaction and\nanalyzing the properties of these structures, we provide a distinction of this\nscenario from an alternative hypothesis, which attributes these features to\nfast-moving ejecta knots penetrating the shell ahead of the forward shock.",
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning",
    "start_abstract":"Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cond-mat.dis-nn"
      ]
    },
    "list":{
      "title":[
        "Scale-free localization versus Anderson localization in unidirectional\n  quasiperiodic lattices",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "Impact of Nonreciprocal Hopping on Localization in Non-Hermitian\n  Quasiperiodic Systems",
        "Hopfield model with quasi-diagonal connection matrix",
        "Relationship between the shear moduli and defect-induced structural\n  relaxation ofhigh-entropy metallic glasses",
        "Exact mobility edges in quasiperiodic network models with slowly varying\n  potentials",
        "Electrical conductivity of conductive films based on random metallic\n  nanowire networks",
        "Screening and localization in the nonlinear Anderson problem",
        "Topological mechanical neural networks as classifiers through in situ\n  backpropagation learning",
        "The Capacity of Modern Hopfield Networks under the Data Manifold\n  Hypothesis",
        "Tentaclelike spectra and bound states in Hatano-Nelson chain with\n  long-range impurity coupling",
        "Isolating the hard core of phaseless inference: the Phase selection\n  formulation",
        "Critical exponents of the spin glass transition in a field at zero\n  temperature",
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection"
      ],
      "abstract":[
        "Scale-free localization emerging in non-Hermitian physics has recently\ngarnered significant attention. In this work, we explore the interplay between\nscale-free localization and Anderson localization by investigating a\nunidirectional quasiperiodic model with generalized boundary conditions. We\nderive analytical expressions of Lyapunov exponent from the bulk equations.\nTogether with the boundary equation, we can determine properties of eigenstates\nand spectrum and establish their exact relationships with the quasiperiodic\npotential strength and boundary parameter. While eigenstates exhibit scale-free\nlocalization in the weak disorder regime, they become localized in the strong\ndisorder regime. The scale-free and Anderson localized states satisfy the\nboundary equation in distinct ways, leading to different localization\nproperties and scaling behaviors. Generalizing our framework, we design a model\nwith exact energy edges separating the scale-free and Anderson localized states\nvia the mosaic modulation of quasiperiodic potentials. Our models can be\nrealized experimentally in electric circuits.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "We study the non-Hermitian Aubry-Andr\\'e-Harper model, incorporating complex\nphase modulation, unmodulated and modulated nonreciprocal hopping. Using\nAvila's global theory, we derive analytical phase boundaries and map out the\nphase diagrams, revealing extended, localized, critical, and skin phases unique\nto non-Hermitian systems. For complex phase modulation, we determine\nlocalization lengths through Lyapunov exponents and show that topological\ntransitions align with localization transitions. In the nonreciprocal case, we\nuse similarity transformations to confirm phase boundaries consistent with\nAvila's theory and uncover asymmetric localization behaviors. Importantly,\nmodulated nonreciprocal hopping transforms both extended and critical phases\ninto skin phases under open boundary conditions. These results highlight the\ninterplay between topology, localization, and non-Hermitian effects, offering\nnew perspectives on quasiperiodic systems.",
        "We analyze a Hopfield neural network with a quasi-diagonal connection matrix.\nWe use the term \"quasi-diagonal matrix\" to denote a matrix with all elements\nequal zero except the elements on the first super- and sub-diagonals of the\nprinciple diagonal. The nonzero elements are arbitrary real numbers. Such\nmatrix generalizes the well-known connection matrix of the one dimensional\nIsing model with open boundary conditions where all nonzero elements equal +1.\nWe present a simple description of the fixed points of the Hopfield neural\nnetwork and their dependence on the matrix elements. The obtained results also\nallow us to analyze the cases of a) the nonzero elements constitute arbitrary\nsuper- and sub-diagonals and b) periodic boundary conditions.",
        "We performed high-frequency shear modulus and calorimetry measurements on\nseven high-entropy metallic glasses (HEMGs) in the initial, relaxed and\ncrystalline states. It is shown that the shear modulus of HEMGs is\nintrinsically related with the concentration of defects responsible for\nstructural relaxation. In the absence of structural relaxation, temperature\ncoefficient of shear modulus of glass equals to that of the maternal crystal.\nAll found regularities are governed by a single equation.",
        "Quasiperiodic potentials without self-duality are always hard to derive the\nexact mobility edges (MEs). Here, we propose a new class of network models with\nexactly solvable MEs, characterized by quasiperiodic slowly varying potentials\nthat do not exhibit hidden self-duality. We present two methods to derive the\nMEs, the first involves integrating out the periodic sites to obtain an\neffective Hamiltonian with effective potential $g(E)V$ and effective\neigenenergy $f(E)$, which directly yields the MEs at $f(E) = \\pm(2t\\pm g(E)V)$,\nand the second is to connect the localized-delocalized transition points of the\nquasiperiodic slowly varying models and the real-complex transition points of\nthe eigenvalue equations. To illustrate this, we take quasiperiodic mosaic\nslowly varying models as examples, and we find that the MEs obtained from the\ntwo methods are the same. Furthermore, we generalize our methods to\nquasiperiodic network models and propose a possible experimental realization\nbased on optical waveguide systems, showing that the Anderson transition can be\nobserved even using small physical systems (with $L = 50 - 100$). Our results\nmay provide insight into understanding and realizing exact MEs in experiments.",
        "Using computer simulation, we investigated the dependence of the electrical\nconductivity of random two-dimensional systems of straight nanowires on the\nmain parameters. Both the resistance of the conductors and the resistance of\nthe contacts between them were taken into account. The dependence of the\nresistance, $R$, between network nodes on the distance between nodes, $r$, is\n$R(r) = R_\\Box\/\\pi \\ln r + \\mathrm{const}$, where $R_\\Box$ is the sheet\nresistance.",
        "We resolve an existing question concerning the localization of a wave packet\nby random potential in the presence of weak nonlinearity. The problem has\ngained considerable interest in the literature, and it continues to attract\nattention due to its connection with the general properties of behavior of\nsystems with competition between nonlinearity, nonlocality and randomness. We\nfind that the nonlinearly localized state occurs through a finite polarization\nresponse from the lattice well beyond the assumptions of a perturbation-theory\napproach. For the vanishing polarization response the nonlinear localization\nlength diverges permitting unlimited spreading of the nonlinear field.",
        "Recently, a new frontier in computing has emerged with physical neural\nnetworks(PNNs) harnessing intrinsic physical processes for learning. Here, we\nexplore topological mechanical neural networks(TMNNs) inspired by the quantum\nspin Hall effect(QSHE) in topological metamaterials, for machine learning\nclassification tasks. TMNNs utilize pseudospin states and the robustness of the\nQSHE, making them damage-tolerant for binary classification. We first\ndemonstrate data clustering using untrained TMNNs. Then, for specific tasks, we\nderive an in situ backpropagation algorithm - a two-step, local-rule method\nthat updates TMNNs using only local information, enabling in situ physical\nlearning. TMNNs achieve high accuracy in classifications of Iris flowers,\nPenguins, and Seeds while maintaining robustness against bond pruning.\nFurthermore, we demonstrate parallel classification via frequency-division\nmultiplexing, assigning different tasks to distinct frequencies for enhanced\nefficiency. Our work introduces in situ backpropagation for wave-based\nmechanical neural networks and positions TMNNs as promising neuromorphic\ncomputing hardware for classification tasks.",
        "We generalize the computation of the capacity of exponential Hopfield model\nfrom Lucibello and M\\'ezard (2024) to more generic pattern ensembles, including\nbinary patterns and patterns generated from a hidden manifold model.",
        "In non-Hermitian systems, the energy spectra and eigenstates exhibit high\nsensitivity to boundary conditions, lattice geometries, and local impurities.\nIn this paper, we study the effect of long-range impurity coupling, located far\nfrom the boundaries, on the paradigmatic non-Hermitian Hatano-Nelson model.\nThrough exact analytical treatment, we reveal the intriguing tentacle-like\nspectral structures that emerge from the otherwise Bloch or non-Bloch spectra\nunder periodic or open boundary conditions, respectively. We show that these\nspectral tentacles are associated with emergent bound states near the impurity,\nwith their number determined by the coupling range. We further determine the\nlocalization length of these tentacled states using the transfer matrix. Our\nwork indicates that the long-range impurity coupling cannot be treated as a\nmere perturbative effect and holds promise for state manipulations in\nnon-Hermitian systems.",
        "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency.",
        "We analyze the spin glass transition in a field in finite dimension $D$ below\nthe upper critical dimension directly at zero temperature using a recently\nintroduced perturbative loop expansion around the Bethe lattice solution. The\nexpansion is generated by the so-called $M$-layer construction, and it has\n$1\/M$ as the associated small parameter. Computing analytically and numerically\nthese non-standard diagrams at first order in the $1\/M$ expansion, we construct\nan $\\epsilon$-expansion around the upper critical dimension $D_\\text{uc}=8$,\nwith $\\epsilon=D_\\text{uc}-D$. Following standard field theoretical methods, we\ncan write a $\\beta$ function, finding a new zero-temperature fixed-point\nassociated with the spin glass transition in a field in dimensions $D<8$. We\nare also able to compute, at first order in the $\\epsilon$-expansion, the three\nindependent critical exponents characterizing the transition, plus the\ncorrection-to-scaling exponent.",
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b9",
    "start_title":"Kinetic description and convergence analysis of genetic algorithms for global optimization",
    "start_abstract":"Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Genetic Algorithms + Data Structures = Evolution Programs"
      ],
      "abstract":[
        "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
      ],
      "categories":[
        "cs.DS"
      ]
    },
    "list":{
      "title":[
        "QuaRs: A Transform for Better Lossless Compression of Integers",
        "Minimizers in Semi-Dynamic Strings",
        "Maximum Coverage $k$-Antichains and Chains: A Greedy Approach",
        "On Deleting Vertices to Reduce Density in Graphs and Supermodular\n  Functions",
        "PtrHash: Minimal Perfect Hashing at RAM Throughput",
        "Fine-Grained Complexity of Computing Degree-Constrained Spanning Trees",
        "Efficient parameterized approximation",
        "Faster parameterized algorithm for 3-Hitting Set",
        "Exploring Word-Representable Temporal Graphs",
        "Potential-Based Greedy Matching for Dynamic Delivery Pooling",
        "Tight Bounds for some Classical Problems Parameterized by Cutwidth",
        "Faster Approximation Algorithms for k-Center via Data Reduction",
        "Graph-Based Algorithms for Diverse Similarity Search",
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "Bargaining with transfers",
        "Constraints on extended axion structures from the lensing of fast radio\n  bursts",
        "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes",
        "Bangla Fake News Detection Based On Multichannel Combined CNN-LSTM",
        "MAIA: A new detector concept for a 10 TeV muon collider",
        "High-pressure growth effect on the properties of high-Tc iron-based\n  superconductors: A short review",
        "The Dead Internet Theory: A Survey on Artificial Interactions and the\n  Future of Social Media",
        "Talking to GDELT Through Knowledge Graphs",
        "The First Chemical Census the Milky Way's Nuclear Star Cluster",
        "Seismic wavefield solutions via physics-guided generative neural\n  operator",
        "Drop size distribution from laboratory experiments based on single-drop\n  fragmentation and comparison with aerial in-situ measurements",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Temperature-Distance Relations in Casimir Physics",
        "Formation of Singularity and Apparent Horizon for Dissipative Collapse\n  in $f(R,T)$ Theory of Gravity"
      ],
      "abstract":[
        "The rise of integer-valued data, partly driven by the Internet of Things\n(IoT), has increased demand for efficient compression methods to reduce storage\nand transmission costs. Existing, speed-oriented methods rely on the\n``smaller-numbers-less-bits'' principle, assuming unimodal distributions\ncentered around zero. This assumption is often violated in practice, leading to\nsuboptimal compression. We propose QuaRs, a transformation that reshapes\narbitrary distributions into unimodal ones centered around zero, improving\ncompatibility with fast integer compression methods. QuaRs remaps data based on\nquantiles, assigning smaller magnitudes to frequent values. The method is fast,\ninvertible, and has sub-quadratic complexity. QuaRs enhances compression\nefficiency, even for challenging distributions, while integrating seamlessly\nwith existing techniques.",
        "Minimizers sampling is one of the most widely-used mechanisms for sampling\nstrings. Let $S=S[0]\\ldots S[n-1]$ be a string over an alphabet $\\Sigma$. In\naddition, let $w\\geq 2$ and $k\\geq 1$ be two integers and\n$\\rho=(\\Sigma^k,\\leq)$ be a total order on $\\Sigma^k$. The minimizer of window\n$X=S[i\\mathinner{.\\,.} i+w+k-2]$ is the smallest position in $[i,i+w-1]$ where\nthe smallest length-$k$ substring of $S[i\\mathinner{.\\,.} i+w+k-2]$ based on\n$\\rho$ starts. The set of minimizers for all $i\\in[0,n-w-k+1]$ is the set\n$\\mathcal{M}_{w,k,\\rho}(S)$ of the minimizers of $S$. The set\n$\\mathcal{M}_{w,k,\\rho}(S)$ can be computed in $\\mathcal{O}(n)$ time. The\nfolklore algorithm for this computation computes the minimizer of every window\nin $\\mathcal{O}(1)$ amortized time using $\\mathcal{O}(w)$ working space. It is\nthus natural to pose the following two questions:\n  Question 1: Can we efficiently support other dynamic updates on the window?\n  Question 2: Can we improve on the $\\mathcal{O}(w)$ working space?\n  We answer both questions in the affirmative:\n  1. We term a string $X$ semi-dynamic when one is allowed to insert or delete\na letter at any of its ends. We show a data structure that maintains a\nsemi-dynamic string $X$ and supports minimizer queries in $X$ in\n$\\mathcal{O}(1)$ time with amortized $\\mathcal{O}(1)$ time per update\noperation.\n  2. We show that this data structure can be modified to occupy strongly\nsublinear space without increasing the asymptotic complexity of its operations.\nTo the best of our knowledge, this yields the first algorithm for computing\n$\\mathcal{M}_{w,k,\\rho}(S)$ in $\\mathcal{O}(n)$ time using\n$\\mathcal{O}(\\sqrt{w})$ working space.\n  We complement our theoretical results with a concrete application and an\nexperimental evaluation.",
        "Given an input acyclic digraph $G = (V,E)$ and a positive integer $k$, the\nproblem of Maximum Coverage $k$-Antichains (resp., Chains) denoted as MA-$k$\n(resp., MC-$k$) asks to find $k$ sets of pairwise unreachable vertices, known\nas antichains (resp., $k$ subsequences of paths, known as chains), maximizing\nthe number of vertices covered by these antichains (resp. chains). While MC-$k$\nhas been recently solved in (almost) optimal $O(|E|^{1+o(1)})$ time [Kogan and\nParter, ICALP 2022], the fastest known algorithm for MA-$k$ is a recent\n$(k|E|)^{1+o(1)}$-time solution [Kogan and Parter, ESA 2024] as well as a $1\/2$\napproximation running in $|E|^{1+o(1)}$ time in the same paper. In this paper,\nwe leverage a paths-based proof of the Greene-Kleitmann (GK) theorem with the\nhelp of the greedy algorithm for set cover and recent advances on fast\nalgorithms for flows and shortest paths to obtain the following results for\nMA-$k$:\n  - The first (exact) algorithm running in $|E|^{1+o(1)}$ time, hence\nindependent in $k$.\n  - A randomized algorithm running in $\\tilde{O}(\\alpha_k|E|)$ time, where\n$\\alpha_k$ is the size of the optimal solution. That is, a near-linear\nparameterized running time, generalizing the result of [M\\\"akinen et al., ACM\nTALG] obtained for $k=1$.\n  - An approximation algorithm running in time $O(\\alpha_1^2|V| +\n(\\alpha_1+k)|E|)$ with approximation ratio of $(1-1\/e) > 0.63 > 1\/2$.\n  Our last two solutions rely on the use of greedy set cover, first exploited\nin [Felsner et al., Order 2003] for chains, which we now apply to antichains.\nWe complement these results with two examples (one for chains and one for\nantichains) showing that, for every $k \\ge 2$, greedy misses a $1\/4$ portion of\nthe optimal coverage. We also show that greedy is a $\\Omega(\\log{|V|})$ factor\naway from minimality when required to cover all vertices: previously unknown\nfor sets of chains or antichains.",
        "We consider deletion problems in graphs and supermodular functions where the\ngoal is to reduce density. In Graph Density Deletion (GraphDD), we are given a\ngraph $G=(V,E)$ with non-negative vertex costs and a non-negative parameter\n$\\rho \\ge 0$ and the goal is to remove a minimum cost subset $S$ of vertices\nsuch that the densest subgraph in $G-S$ has density at most $\\rho$. This\nproblem has an underlying matroidal structure and generalizes several classical\nproblems such as vertex cover, feedback vertex set, and pseudoforest deletion\nset for appropriately chosen $\\rho \\le 1$ and all of these classical problems\nadmit a $2$-approximation. In sharp contrast, we prove that for every fixed\ninteger $\\rho > 1$, GraphDD is hard to approximate to within a logarithmic\nfactor via a reduction from Set Cover, thus showing a phase transition\nphenomenon. Next, we investigate a generalization of GraphDD to monotone\nsupermodular functions, termed Supermodular Density Deletion (SupmodDD). In\nSupmodDD, we are given a monotone supermodular function $f:2^V \\rightarrow\n\\mathbb{Z}_{\\ge 0}$ via an evaluation oracle with element costs and a\nnon-negative integer $\\rho \\ge 0$ and the goal is remove a minimum cost subset\n$S \\subseteq V$ such that the densest subset according to $f$ in $V-S$ has\ndensity at most $\\rho$. We show that SupmodDD is approximation equivalent to\nthe well-known Submodular Cover problem; this implies a tight logarithmic\napproximation and hardness for SupmodDD; it also implies a logarithmic\napproximation for GraphDD, thus matching our inapproximability bound. Motivated\nby these hardness results, we design bicriteria approximation algorithms for\nboth GraphDD and SupmodDD.",
        "Given a set $S$ of $n$ keys, a minimal perfect hash function (MPHF) is a\ncollision-free bijective map $\\mathsf{H_{mphf}}$ from $S$ to $\\{0, \\dots,\nn-1\\}$. This work presents a (minimal) perfect hash function that first\nprioritizes query throughput, while also allowing efficient construction for\n$10^9$ or more elements using 2.4 bits of memory per key.\n  Both PTHash and PHOBIC first map all $n$ keys to $n\/\\lambda < n$ buckets.\nThen, each bucket stores a pilot that controls the final hash value of the keys\nmapping to it. PtrHash builds on this by using 1) fixed-width (uncompressed)\n8-bit pilots, 2) a construction algorithm similar to cuckoo-hashing to find\nsuitable pilot values. Further, it 3) uses the same number of buckets and slots\nfor each part, with 4) a single remap table to map intermediate positions $\\geq\nn$ to $<n$, 5) encoded using per-cacheline Elias-Fano coding. Lastly, 6)\nPtrHash support streaming queries, where we use prefetching to answer a stream\nof multiple queries more efficiently than one-by-one processing.\n  With default parameters, PtrHash takes 2.40 bits per key. On 300 million\nstring keys, PtrHash is as fast or faster to build than other MPHFs, and at\nleast $1.75\\times$ faster to query. When streaming multiple queries, this\nimproves to $3.1\\times$ speedup over the fastest alternative, while also being\nsignificantly faster to construct. When using $10^9$ integer keys instead,\nquery times are as low as 12 ns\/key when iterating in a for loop, or even down\nto 8 ns\/key when using the streaming approach, within $10\\%$ of the maximum\nmemory-bound throughput.",
        "We investigate the computation of minimum-cost spanning trees satisfying\nprescribed vertex degree constraints: Given a graph $G$ and a constraint\nfunction $D$, we ask for a (minimum-cost) spanning tree $T$ such that for each\nvertex $v$, $T$ achieves a degree specified by $D(v)$. Specifically, we\nconsider three kinds of constraint functions ordered by their generality -- $D$\nmay either assign each vertex to a list of admissible degrees, an upper bound\non the degrees, or a specific degree. Using a combination of novel techniques\nand state-of-the-art machinery, we obtain an almost-complete overview of the\nfine-grained complexity of these problems taking into account the most\nclassical graph parameters of the input graph $G$. In particular, we present\nSETH-tight upper and lower bounds for these problems when parameterized by the\npathwidth and cutwidth, an ETH-tight algorithm parameterized by the\ncliquewidth, and a nearly SETH-tight algorithm parameterized by treewidth.",
        "Many problems are NP-hard and, unless P = NP, do not admit polynomial-time\nexact algorithms. The fastest known exact algorithms exactly usually take time\nexponential in the input size. Much research effort has gone into obtaining\nfaster exact algorithms for instances that are sufficiently well-structured,\ne.g., through parameterized algorithms with running time $f(k)\\cdot\nn^{\\mathcal{O}(1)}$ where n is the input size and k quantifies some structural\nproperty such as treewidth. When k is small, this is comparable to a\npolynomial-time exact algorithm and outperforms the fastest exact\nexponential-time algorithms for a large range of k.\n  In this work, we are interested instead in leveraging instance structure for\npolynomial-time approximation algorithms. We aim for polynomial-time algorithms\nthat produce a solution of value at most or at least (depending on minimization\nvs. maximization) $c\\mathrm{OPT}\\pm f(k)$ where c is a constant. Unlike for\nstandard parameterized algorithms, we do not assume that structural information\nis provided with the input. Ideally, we can obtain algorithms with small\nadditive error, i.e., $c=1$ and $f(k)$ is polynomial or even linear in $k$. For\nsmall k, this is similarly comparable to a polynomial-time exact algorithm and\nwill beat general case approximation for a large range of k.\n  We study Vertex Cover, Connected Vertex Cover, Chromatic Number, and Triangle\nPacking. The parameters we consider are the size of minimum modulators to graph\nclasses on which the respective problem is tractable. For most\nproblem-parameter combinations we give algorithms that compute a solution of\nsize at least or at most $\\mathrm{OPT}\\pm k$. In the case of Vertex Cover, most\nof our algorithms are tight under the Unique Games Conjecture and provide\nbetter approximation guarantees than standard 2-approximations if the modulator\nis smaller than the optimum solution.",
        "In the 3-Hitting Set problem, the input is a hypergraph $G$ such that the\nsize of every hyperedge of $G$ is at most 3, and an integers $k$, and the goal\nis to decide whether there is a set $S$ of at most $k$ vertices such that every\nhyperedge of $G$ contains at least one vertex from $S$. In this paper we give\nan $O^*(2.0409^k)$-time algorithm for 3-Hitting Set.",
        "Word-representable graphs are a subset of graphs that may be represented by a\nword $w$ over an alphabet composed of the vertices in the graph. In such\ngraphs, an edge exists if and only if the occurrences of the corresponding\nvertices alternate in the word $w$. We generalise this notion to temporal\ngraphs, constructing timesteps by partitioning the word into factors\n(contiguous subwords) such that no factor contains more than one copy of any\ngiven symbol. With this definition, we study the problem of \\emph{exploration},\nasking for the fastest schedule such that a given agent may explore all $n$\nvertices of the graph. We show that if the corresponding temporal graph is\nconnected in every timestep, we may explore the graph in $2\\delta n$ timesteps,\nwhere $\\delta$ is the lowest degree of any vertex in the graph. In general, we\nshow that, for any temporal graph represented by a word of length at least\n$n(2dn + d)$, with a connected underlying graph, the full graph can be explored\nin $2 d n$ timesteps, where $d$ is the diameter of the graph. We show this is\nasymptotically optimal by providing a class of graphs of diameter $d$ requiring\n$\\Omega(d n)$ timesteps to explore, for any $d \\in [1, n]$.",
        "We study the problem of pooling together delivery orders into a single trip,\na strategy widely adopted by platforms to reduce total travel distance. Similar\nto other dynamic matching settings, the pooling decisions involve a trade-off\nbetween immediate reward and holding jobs for potentially better opportunities\nin the future. In this paper, we introduce a new heuristic dubbed\npotential-based greedy (PB), which aims to keep longer-distance jobs in the\nsystem, as they have higher potential reward (distance savings) from being\npooled with other jobs in the future. This algorithm is simple in that it\ndepends solely on the topology of the space, and does not rely on forecasts or\npartial information about future demand arrivals. We prove that PB\nsignificantly improves upon a naive greedy approach in terms of worst-case\nperformance on the line. Moreover, we conduct extensive numerical experiments\nusing both synthetic and real-world order-level data from the Meituan platform.\nOur simulations show that PB consistently outperforms not only the naive greedy\nheuristic but a number of benchmark algorithms, including (i) batching-based\nheuristics that are widely used in practice, and (ii) forecast-aware heuristics\nthat are given the correct probability distributions (in synthetic data) or a\nbest-effort forecast (in real data). We attribute the surprising unbeatability\nof PB to the fact that it is specialized for rewards defined by distance saved\nin delivery pooling.",
        "Cutwidth is a widely studied parameter that quantifies how well a graph can\nbe decomposed along small edge-cuts. It complements pathwidth, which captures\ndecomposition by small vertex separators, and it is well-known that cutwidth\nupper-bounds pathwidth. The SETH-tight parameterized complexity of problems on\ngraphs of bounded pathwidth (and treewidth) has been actively studied over the\npast decade while for cutwidth the complexity of many classical problems\nremained open.\n  For Hamiltonian Cycle, it is known that a $(2+\\sqrt{2})^{\\operatorname{pw}}\nn^{O(1)}$ algorithm is optimal for pathwidth under SETH~[Cygan et al.\\ JACM\n2022]. Van Geffen et al.~[J.\\ Graph Algorithms Appl.\\ 2020] and Bojikian et\nal.~[STACS 2023] asked which running time is optimal for this problem\nparameterized by cutwidth. We answer this question with\n$(1+\\sqrt{2})^{\\operatorname{ctw}} n^{O(1)}$ by providing matching upper and\nlower bounds. Second, as our main technical contribution, we close the gap left\nby van Heck~[2018] for Partition Into Triangles (and Triangle Packing) by\nimproving both upper and lower bound and getting a tight bound of\n$\\sqrt[3]{3}^{\\operatorname{ctw}} n^{O(1)}$, which to our knowledge exhibits\nthe only known tight non-integral basis apart from Hamiltonian Cycle. We show\nthat cuts inducing a disjoint union of paths of length three (unions of\nso-called $Z$-cuts) lie at the core of the complexity of the problem -- usually\nlower-bound constructions use simpler cuts inducing either a matching or a\ndisjoint union of bicliques. Finally, we determine the optimal running times\nfor Max Cut ($2^{\\operatorname{ctw}} n^{O(1)}$) and Induced Matching\n($3^{\\operatorname{ctw}} n^{O(1)}$) by providing matching lower bounds for the\nexisting algorithms -- the latter result also answers an open question for\ntreewidth by Chaudhary and Zehavi~[WG 2023].",
        "We study efficient algorithms for the Euclidean $k$-Center problem, focusing\non the regime of large $k$. We take the approach of data reduction by\nconsidering $\\alpha$-coreset, which is a small subset $S$ of the dataset $P$\nsuch that any $\\beta$-approximation on $S$ is an $(\\alpha +\n\\beta)$-approximation on $P$. We give efficient algorithms to construct\ncoresets whose size is $k \\cdot o(n)$, which immediately speeds up existing\napproximation algorithms. Notably, we obtain a near-linear time\n$O(1)$-approximation when $k = n^c$ for any $0 < c < 1$. We validate the\nperformance of our coresets on real-world datasets with large $k$, and we\nobserve that the coreset speeds up the well-known Gonzalez algorithm by up to\n$4$ times, while still achieving similar clustering cost. Technically, one of\nour coreset results is based on a new efficient construction of consistent\nhashing with competitive parameters. This general tool may be of independent\ninterest for algorithm design in high dimensional Euclidean spaces.",
        "Nearest neighbor search is a fundamental data structure problem with many\napplications in machine learning, computer vision, recommendation systems and\nother fields. Although the main objective of the data structure is to quickly\nreport data points that are closest to a given query, it has long been noted\n(Carbonell and Goldstein, 1998) that without additional constraints the\nreported answers can be redundant and\/or duplicative. This issue is typically\naddressed in two stages: in the first stage, the algorithm retrieves a (large)\nnumber $r$ of points closest to the query, while in the second stage, the $r$\npoints are post-processed and a small subset is selected to maximize the\ndesired diversity objective. Although popular, this method suffers from a\nfundamental efficiency bottleneck, as the set of points retrieved in the first\nstage often needs to be much larger than the final output.\n  In this paper we present provably efficient algorithms for approximate\nnearest neighbor search with diversity constraints that bypass this two stage\nprocess. Our algorithms are based on popular graph-based methods, which allows\nus to \"piggy-back\" on the existing efficient implementations. These are the\nfirst graph-based algorithms for nearest neighbor search with diversity\nconstraints. For data sets with low intrinsic dimension, our data structures\nreport a diverse set of $k$ points approximately closest to the query, in time\nthat only depends on $k$ and $\\log \\Delta$, where $\\Delta$ is the ratio of the\ndiameter to the closest pair distance in the data set. This bound is\nqualitatively similar to the best known bounds for standard (non-diverse)\ngraph-based algorithms. Our experiments show that the search time of our\nalgorithms is substantially lower than that using the standard two-stage\napproach.",
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "We propose a solution to the problem of bargaining with transfers, along with\nan axiomatisation of the solution. Inefficient allocations in the bargaining\nset can influence the solution, but are discounted relative to more efficient\nones. The key axioms are additivity and a property we call \\emph{inverse\nmonotonicity}, which states that adding an allocation to the bargaining set\nthat is worse for a given player than the initial solution cannot benefit that\nplayer.",
        "Axions are hypothetical pseudoscalar particles that have been regarded as\npromising dark matter (DM) candidates. On the other hand, extended compact\nobjects such as axion stars, which are supported by gravity and axion self\ninteractions, may have also been formed in the early Universe and comprise part\nof DM. In this work, we consider the lensing of electromagnetic signals from\ndistant sources by axion stars, as a way to constrain the properties of axion\nstars and fundamental axion parameters. Accounting for the effect of the finite\nsize of the axion star, we study the lensing effect induced by gravity and the\naxion-photon coupling. The latter effect is frequency dependent, and is\nrelevant in the low frequency band, which motivates the use of fast radio burst\n(FRB) signals as a probe. We calculate the predicted number of lensed FRB\nevents by specifying the fundamental axion parameters, axion star radial\nprofile, fraction of DM residing in axion stars, and imposing lensing criteria\nbased on the flux ratio and time delay between the brightest images from\nlensing. Assuming an optimistic case of $10^4$ observed FRB events, and a\ntiming resolution of $1~\\mu{\\rm s}$, the lack of observed FRB lensing events in\nCHIME allows us to probe axion stars with mass $ \\gtrsim 2 \\times 10^{-2}\nM_\\odot$, corresponding to axion masses $\\lesssim 10^{-10}{\\rm eV}$. We obtain\nconstraints for even lighter axion stars up to $\\sim 10^{-3} M_\\odot$, when the\naxion-photon interactions are taken into account. Our results indicate that FRB\nlensing lead to constraints that are competitive with conventional microlensing\nsearches operating in the optical band.",
        "Images generated by text-to-image (T2I) models often exhibit visual biases\nand stereotypes of concepts such as culture and profession. Existing\nquantitative measures of stereotypes are based on statistical parity that does\nnot align with the sociological definition of stereotypes and, therefore,\nincorrectly categorizes biases as stereotypes. Instead of oversimplifying\nstereotypes as biases, we propose a quantitative measure of stereotypes that\naligns with its sociological definition. We then propose OASIS to measure the\nstereotypes in a generated dataset and understand their origins within the T2I\nmodel. OASIS includes two scores to measure stereotypes from a generated image\ndataset: (M1) Stereotype Score to measure the distributional violation of\nstereotypical attributes, and (M2) WALS to measure spectral variance in the\nimages along a stereotypical attribute. OASIS also includes two methods to\nunderstand the origins of stereotypes in T2I models: (U1) StOP to discover\nattributes that the T2I model internally associates with a given concept, and\n(U2) SPI to quantify the emergence of stereotypical attributes in the latent\nspace of the T2I model during image generation. Despite the considerable\nprogress in image fidelity, using OASIS, we conclude that newer T2I models such\nas FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts\nand still generate images with widespread stereotypical attributes.\nAdditionally, the quantity of stereotypes worsens for nationalities with lower\nInternet footprints.",
        "There have recently been many cases of unverified or misleading information\ncirculating quickly over bogus web networks and news portals. This false news\ncreates big damage to society and misleads people. For Example, in 2019, there\nwas a rumor that the Padma Bridge of Bangladesh needed 100,000 human heads for\nsacrifice. This rumor turns into a deadly position and this misleading\ninformation takes the lives of innocent people. There is a lot of work in\nEnglish but a few works in Bangla. In this study, we are going to identify the\nfake news from the unconsidered news source to provide the newsreader with\nnatural news or real news. The paper is based on the combination of\nconvolutional neural network (CNN) and long short-term memory (LSTM), where CNN\nis used for deep feature extraction and LSTM is used for detection using the\nextracted feature. The first thing we did to deploy this piece of work was data\ncollection. We compiled a data set from websites and attempted to deploy it\nusing the methodology of deep learning which contains about 50k of news. With\nthe proposed model of Multichannel combined CNN-LSTM architecture, our model\ngained an accuracy of 75.05%, which is a good sign for detecting fake news in\nBangla.",
        "Muon colliders offer a compelling opportunity to explore the TeV scale and\nconduct precision tests of the Standard Model, all within a relatively compact\ngeographical footprint. This paper introduces a new detector concept, MAIA\n(Muon Accelerator Instrumented Apparatus), optimized for $\\sqrt{s}=10$ TeV\n$\\mu\\mu$ collisions. The detector features an all-silicon tracker immersed in a\n5T solenoid field. High-granularity silicon-tungsten and iron-scintillator\ncalorimeters surrounding the solenoid capture high-energy electronic and\nhadronic showers, respectively, and support particle-flow reconstruction. The\noutermost subsystem comprises an air-gap muon spectrometer, which enables\nstandalone track reconstruction for high-momentum muons. The performance of the\nMAIA detector is evaluated in terms of differential particle reconstruction\nefficiencies and resolutions. Beam-induced background (BIB) simulations\ngenerated in FLUKA are overlaid with single particle gun samples to assess\ndetector reconstruction capabilities under realistic experimental conditions.\nEven with BIB, reconstruction efficiencies exceed 95% for energetic tracks,\nphotons, and neutrons in the central region of the detector. This paper\noutlines promising avenues of future work, including forward region\noptimization and opportunities for enhanced flavor\/boosted object tagging, and\naddresses the technological assumptions needed to achieve the desired detector\nperformance.",
        "The high-pressure growth technique is a vital approach that facilitates the\nstabilization of new phases and allows for meticulous control of structural\nparameters, which significantly impact electronic and magnetic properties. We\npresent a short review of our ongoing investigations into various families of\niron-based superconductors (IBS), employing the high-gas pressure and\nhigh-temperature synthesis (HP-HTS) method. This technique is capable of\nproducing the gas pressures up to 1.8 GPa and a heating temperature of up to\n1700 {\\deg}C through a three-zone furnace within a cylindrical chamber.\nDifferent kinds of IBS samples are prepared using HPHTS and characterized\nthrough various measurements to reach the final conclusions. The results\ndemonstrate that the high-pressure growth technique significantly enhances the\nproperties of IBS, including the transition temperature, critical current\ndensity, and pinning force. In addition, the quality of the samples and their\ndensity are improved through the intergrain connections. Furthermore, the\ncomprehensive evaluations and investigations prove that a growth pressure of\n0.5 GPa is sufficient for producing high-quality IBS bulks under the optimized\nsynthesis conditions.",
        "The Dead Internet Theory (DIT) suggests that much of today's internet,\nparticularly social media, is dominated by non-human activity, AI-generated\ncontent, and corporate agendas, leading to a decline in authentic human\ninteraction. This study explores the origins, core claims, and implications of\nDIT, emphasizing its relevance in the context of social media platforms. The\ntheory emerged as a response to the perceived homogenization of online spaces,\nhighlighting issues like the proliferation of bots, algorithmically generated\ncontent, and the prioritization of engagement metrics over genuine user\ninteraction. AI technologies play a central role in this phenomenon, as social\nmedia platforms increasingly use algorithms and machine learning to curate\ncontent, drive engagement, and maximize advertising revenue. While these tools\nenhance scalability and personalization, they also prioritize virality and\nconsumption over authentic communication, contributing to the erosion of trust,\nthe loss of content diversity, and a dehumanized internet experience. This\nstudy redefines DIT in the context of social media, proposing that the\ncommodification of content consumption for revenue has taken precedence over\nmeaningful human connectivity. By focusing on engagement metrics, platforms\nfoster a sense of artificiality and disconnection, underscoring the need for\nhuman-centric approaches to revive authentic online interaction and community\nbuilding.",
        "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
        "An important step in understanding the formation and evolution of the Nuclear\nStar Cluster (NSC) is to investigate its chemistry and chemical evolution.\nAdditionally, exploring the relationship of the NSC to the other structures in\nthe Galactic Center and the Milky Way disks is of great interest. Extreme\noptical extinction has previously prevented optical studies, but near-IR\nhigh-resolution spectroscopy is now possible. Here, we present a detailed\nchemical abundance analysis of 19 elements - more than four times as many as\npreviously published - for 9 stars in the NSC of the Milky Way, observed with\nthe IGRINS spectrometer on the Gemini South telescope. This study provides new,\ncrucial observational evidence to shed light on the origin of the NSC. We\ndemonstrate that it is possible to probe a variety of nucleosynthetic channels,\nreflecting different chemical evolution timescales. Our findings reveal that\nthe NSC trends for the elements F, Mg, Al, Si, S, K, Ca, Ti, Cr, Mn, Co, Ni,\nCu, and Zn, as well as the s-process elements Ba, Ce, Nd, and Yb, generally\nfollow the inner bulge trends within uncertainties. This suggests a likely\nshared evolutionary history and our results indicate that the NSC population is\nconsistent with the chemical sequence observed in the inner Galaxy (the\ninner-disk sequence). However, we identify a significant and unexplained\ndifference in the form of higher Na abundances in the NSC compared to the\ninner-bulge. This is also observed in few Galactic globular clusters, and may\nsuggest a common enrichment process at work in all these systems.",
        "Current neural operators often struggle to generalize to complex,\nout-of-distribution conditions, limiting their ability in seismic wavefield\nrepresentation. To address this, we propose a generative neural operator (GNO)\nthat leverages generative diffusion models (GDMs) to learn the underlying\nstatistical distribution of scattered wavefields while incorporating a\nphysics-guided sampling process at each inference step. This physics guidance\nenforces wave equation-based constraints corresponding to specific velocity\nmodels, driving the iteratively generated wavefields toward physically\nconsistent solutions. By training the diffusion model on wavefields\ncorresponding to a diverse dataset of velocity models, frequencies, and source\npositions, our GNO enables to rapidly synthesize high-fidelity wavefields at\ninference time. Numerical experiments demonstrate that our GNO not only\nproduces accurate wavefields matching numerical reference solutions, but also\ngeneralizes effectively to previously unseen velocity models and frequencies.",
        "Laboratory experiments and theoretical modelling are conducted to determine\nthe raindrop size distribution (DSD) resulting from distinct fragmentation\nprocesses under various upward airstreams. Since weather radar echoes are\nproportional to the sixth power of the average droplet diameter, understanding\nthe fragmentation mechanisms that lead to different breakup sizes is crucial\nfor accurate rainfall predictions. We utilize a two-parameter gamma\ndistribution for theoretical modelling and estimate the average droplet\ndiameter from the theoretically obtained characteristic sizes, often treated as\nassumed input parameters for different rain conditions in rainfall modelling.\nOur experimental and theoretical findings demonstrate a close agreement with\nthe DSD predicted by the Marshall and Palmer relationship for steady rain\nconditions. Additionally, in situ DSD measurements at different altitudes were\nobtained through research flights equipped with advanced sensors, further\nvalidating our rainfall model. This study underscores the effectiveness of\nlaboratory-scale experiments and the critical importance of accurately\ncharacterizing DSD to enhance rainfall predictions.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "The Casimir-Lifshitz force arises from thermal and quantum mechanical\nfluctuations between classical bodies and becomes significant below the micron\nscale. We explore temperature-distance relations based on the concepts of Wick\nand Bohr arising from energy-time uncertainty relations. We show that\ntemperature-distance relations similar to those arising from the uncertainty\nprinciple are found in various Casimir interactions, with an exact relation\noccurring in the low-temperature regime when the zero point energy contribution\ncancels the thermal radiation pressure contribution between two plates.",
        "In this paper, we consider the spherically symmetric gravitational collapse\nof isotropic matter undergoing dissipation in the form of heat flux, with a\ngeneralized Vaidya exterior, in the context of $f(R, T)$ gravity. Choosing\n$f(R, T)=R+2\\lambda T$, and applying the $f(R, T)$ junction conditions on the\nfield equations for the interior and exterior regions, we have obtained\nmatching conditions of the matter-Lagrangian and its derivatives across the\nboundary. The time of formation of singularity and the time of formation of\napparent horizon have been determined and constraints on the integration\nconstants are examined for which the final singularity is hidden behind the\nhorizon."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b25",
    "start_title":"Genetic Algorithms + Data Structures = Evolution Programs",
    "start_abstract":"Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation.",
    "start_categories":[
      "cs.DS"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Kinetic description and convergence analysis of genetic algorithms for global optimization"
      ],
      "abstract":[
        "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Carleman estimate for semi-discrete stochastic parabolic operators in\n  arbitrary dimension and applications to controllability",
        "Reachability for multiagent control systems via Lyapunov functions",
        "A Rank-One-Update Method for the Training of Support Vector Machines",
        "On Subdifferentials Via a Generalized Conjugation Scheme: An Application\n  to DC Problems and Optimality Conditions",
        "Existence and uniqueness of control sets with a nonempty interior for\n  linear control systems on solvable groups",
        "Convergence of projected stochastic approximation algorithm",
        "Asymptotic behavior of penalty dynamics for constrained variational\n  inequalities",
        "Local convergence analysis of a stabilized sequential quadratic\n  programming method for optimization problems in Banach spaces",
        "A Graph-Based Iterative Strategy for Solving the All-Line Transmission\n  Switching Problem",
        "Bi-Parameterized Two-Stage Stochastic Min-Max and Min-Min Mixed Integer\n  Programs",
        "A Smoothing Consensus-Based Optimization Algorithm for Nonsmooth\n  Nonconvex Optimization",
        "Solving Non-Monotone Inclusions Using Monotonicity of Pairs of Operators",
        "A New Lyapunov-like Stability Inequality with an \\textit{Asymmetric}\n  Matrix and Application to Suboptimal LQ Control Design",
        "Strongly nonlinear age structured equation,time-elapsed model and large\n  delays",
        "Benchmarking nuclear matrix elements of $0\\nu\\beta\\beta$ decay with\n  high-energy nuclear collisions",
        "Periodic orbits and their gravitational wave radiations around the\n  Schwarzschild-MOG black hole",
        "A positive product formula of integral kernels of $k$-Hankel transforms",
        "Floquet geometric squeezing in fast-rotating condensates",
        "Clarkson-McCarthy inequality on a locally compact group",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Low-rank variance reduction for uncertain radiative transfer with\n  control variates",
        "Lyapunov exponent for quantum graphs that are elements of a subshift of\n  finite type",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Nonlinear Spectroscopy as a Magnon Breakdown Diagnosis and its Efficient\n  Simulation",
        "Empirical Thermophotovoltaic Performance Predictions and Limits",
        "The waves-in-space Purcell effect for superconducting qubits",
        "Anomalous nuclear spin coherence in superconducting Nb$_3$Sn",
        "Effects of Initial Nucleon-Nucleon Correlations on Light Nuclei\n  Production in Au+Au Collisions at $\\sqrt{s_\\mathrm{NN}} = 3\\ $ GeV"
      ],
      "abstract":[
        "This paper considers a semi-discrete forward stochastic parabolic operator\nwith homogeneous Dirichlet conditions in arbitrary dimensions. We show the lack\nof null controllability for a spatial semi-discretization of a\nnull-controllable stochastic parabolic system from any initial datum. However,\nby proving a new Carleman estimate for its semi-discrete backward stochastic\nadjoint system, we achieve a relaxed observability inequality, which is applied\nto derivative $\\phi$-null controllability by duality arguments.",
        "This paper concerns the problem of reachability of a given state for a\nmultiagent control system in $\\mathbb{R}^d$. In such a system, at every time\neach agent can choose his\/her velocity which depends both on his\/her position\nand on the position of the whole crowd of agents (modeled by a probability\nmeasure on $ \\mathbb{R}^d$). The main contribution of the paper is to study the\nabove reachability problem with a given rate of attainability through a\nLyapunov method adapted to the Wasserstein space of probability measures. As a\nbyproduct we obtain a new comparison result for viscosity solutions of Hamilton\nJacobi equations in the Wasserstein space.",
        "This paper considers convex quadratic programs\n  associated with the training of support vector machines (SVM).\n  Exploiting the special structure of the SVM problem a new\n  type of active set method with long cycles and stable rank-one-updates\n  is proposed and tested (CMU: cycling method with updates).\n  The structure of the problem allows for a repeated simple increase\n  of the set of inactive constraints while controlling its size. This is\n  followed by minimization steps with cheap updates of a matrix factorization.\n  A widely used approach for solving SVM problems is the\n  alternating direction method SMO,\n  a method that is very efficient for low accuracy.\n  The new active set approach allows for higher accuracy\n  results at moderate computational cost. To relate both approaches,\n  the effect of the accuracy on the running time and on the\n  predictive quality of the SVM is compared with some numerical examples.\n  A surprising result of the numerical examples is that only a\n  very small number of cycles (each consisting of less than 2n\n  steps) was used for CMU.",
        "This paper studies properties of a subdifferential defined using a\ngeneralized conjugation scheme. We relate this subdifferential together with\nthe domain of an appropriate conjugate function and the {\\epsilon}-directional\nderivative. In addition, we also present necessary conditions for\n{\\epsilon}-optimality and global optimality in optimization problems involving\nthe difference of two convex functions. These conditions will be written via\nthis generalized notion of subdifferential studied in the first sections of the\npaper.",
        "In this paper, we obtain weak conditions for the existence of a control set\nwith a nonempty interior for a linear control system on a solvable Lie group.\nWe show that the Lie algebra rank condition together with the compactness of\nthe nilpotent part of the generalized kernel of the drift are enough to assure\nthe existence of such a control set. Moreover, this control set is unique and\ncontains the whole generalized kernel in its closure.",
        "We study the Robbins-Monro stochastic approximation algorithm with\nprojections on a hyperrectangle and prove its convergence. This work fills a\ngap in the convergence proof of the classic book by Kushner and Yin. Using the\nODE method, we show that the algorithm converges to stationary points of a\nrelated projected ODE. Our results provide a better theoretical foundation for\nstochastic optimization techniques, including stochastic gradient descent and\nits proximal version. These results extend the algorithm's applicability and\nrelax some assumptions of previous research.",
        "We propose a comprehensive framework for solving constrained variational\ninequalities via various classes of evolution equations displaying multi-scale\naspects. In a Hilbertian framework, the class of dynamical systems we propose\ncombine Tikhonov regularization and exterior penalization terms in order to\nyield simultaneously strong convergence of trajectories to least norm solutions\nin the constrained domain. Our construction thus unifies the literature on\nregularization methods and penalty-term based dynamical systems.",
        "This paper presents a stabilized sequential quadratic programming (SQP)\nmethod for solving optimization problems in Banach spaces. The optimization\nproblem considered in this study has a general form that enables us to\nrepresent various types of optimization problems. Several SQP methods have been\nproposed for optimization problems in Banach spaces with specific structures;\nhowever, research on the local analysis of SQP-type methods for general\nproblems, such as those considered in this study, is limited. We focus on the\nlocal behavior of the proposed stabilized SQP method and prove its local\nquadratic convergence under reasonable assumptions, without including any\nconstraint qualifications. Finally, numerical experiments are performed to\nconfirm the theoretical properties shown in the local analysis.",
        "The transmission switching problem aims to determine the optimal network\ntopology that minimizes the operating costs of a power system. This problem is\ntypically formulated as a mixed-integer optimization model, which involves\nbig-M constants that lead to weak relaxations and significant computational\nchallenges, particularly when all lines are switchable. In this paper, we\npropose a two-fold approach: first, using graph theory to derive tighter big-M\nvalues by solving a relaxed longest path problem; second, introducing an\niterative algorithm that incorporates a heuristic version of the switching\nproblem to efficiently generate low-cost feasible solutions, thereby\naccelerating the search for optimal solutions in the integer optimization\nsolver. Numerical results on the 118-bus network show that the proposed\nmethodology significantly reduces the computational burden compared to\nconventional approaches.",
        "We introduce two-stage stochastic min-max and min-min integer programs with\nbi-parameterized recourse (BTSPs), where the first-stage decisions affect both\nthe objective function and the feasible region of the second-stage problem. To\nsolve these programs efficiently, we introduce Lagrangian-integrated L-shaped\n($L^2$) methods, which guarantee exact solutions when the first-stage decisions\nare pure binary. For mixed-binary first-stage programs, we present a\nregularization-augmented variant of this method. We also introduce\ndistributionally robust bi-parameterized two-stage stochastic integer programs\nand present an extension of the $L^2$ method and a reformulation-based method\nfor programs with finite and continuous supports, respectively. Our\ncomputational results show that the $L^2$ method surpasses the benchmark method\nfor bi-parameterized stochastic network interdiction problems, solving all\ninstances in 23 seconds on average, whereas the benchmark method failed to\nsolve any instance within 3600 seconds. Additionally, it achieves optimal\nsolutions up to 18.4 and 1.7 times faster for instances of risk-neutral and\ndistributionally robust bi-parameterized stochastic facility location problems,\nrespectively. Furthermore, BTSPs have applications in solving stochastic\nproblems with decision-dependent probability distributions or sets of\ndistributions (ambiguity set). The $L^2$ method outperforms existing\napproaches, achieving optimal solutions 5.3 times faster for distributionally\nrobust facility location problem with a decision-dependent and non-relatively\ncomplete ambiguity set.",
        "Lately, a novel swarm intelligence model, namely the consensus-based\noptimization (CBO) algorithm, was introduced to deal with the global\noptimization problems. Limited by the conditions of Ito's formula, the\nconvergence analysis of the previous CBO finite particle system mainly focuses\non the problem with smooth objective function. With the help of smoothing\nmethod, this paper achieves a breakthrough by proposing an effective CBO\nalgorithm for solving the global solution of a nonconvex, nonsmooth, and\npossible non-Lipschitz continuous minimization problem with theoretical\nanalysis, which dose not rely on the mean-field limit. We indicate that the\nproposed algorithm exhibits a global consensus and converges to a common state\nwith any initial data. Then, we give a more detailed error estimation on the\nobjective function values along the state of the proposed algorithm towards the\nglobal minimum. Finally, some numerical examples are presented to illustrate\nthe appreciable performance of the proposed method on solving the nonsmooth,\nnonconvex minimization problems.",
        "In this paper, under the monotonicity of pairs of operators, we propose some\nGeneralized Proximal Point Algorithms to solve non-monotone inclusions using\nwarped resolvents and transformed resolvents. The weak, strong, and linear\nconvergence of the proposed algorithms are established under very mild\nconditions.",
        "The Lyapunov inequality is an indispensable tool for stability analysis in\nthe linear control theory. This work proposes a new variant of this inequality\nwhere-in the constituent matrix is allowed to be asymmetric. After developing\nthe stability conditions based on the proposed inequality for a class of linear\nsystems, we utilize these conditions to derive new results for the suboptimal\nlinear quadratic control problem where we characterize the cost of the\nstabilizing controllers. We also demonstrate, by a numerical example, that the\nproposed results can be easily molded for the structured suboptimal consensus\nprotocol design for multi-agent system where we also see that the asymmetry\ncondition of the design matrix turns up inherently.",
        "The time-elapsed model for neural networks is a nonlinear age structured\nequationwhere the renewal term describes the network activity and influences\nthe dischargerate, possibly with a delay due to the length of connections.We\nsolve a long standing question, namely that an inhibitory network withoutdelay\nwill converge to a steady state and thus the network is desynchonised.\nOurapproach is based on the observation that a non-expansion property holds\ntrue.However a non-degeneracy condition is needed and, besides the standard\none, weintroduce a new condition based on strict nonlinearity.When a delay is\nincluded, and following previous works for Fokker-Planck models,we prove that\nthe network may generate periodic solutions. We introduce a newformalism to\nestablish rigorously this property for large delays.The fundamental contraction\nproperty also holds for some other age structuredequations and systems.",
        "Reducing uncertainties in the nuclear matrix element (NME) remains a critical\nchallenge in designing and interpreting experiments aimed at discovering\nneutrinoless double beta ($0\\nu\\beta\\beta$) decay. Here, we identify a class of\nobservables, distinct from those employed in low-energy nuclear structure\napplications, that are strongly correlated with the NME: momentum correlations\namong hadrons produced in high-energy nuclear collisions. Focusing on the\n$^{150}$Nd$\\rightarrow$$^{150}$Sm transition, we combine a Bayesian analysis of\nthe structure of $^{150}$Nd with simulations of high-energy\n$^{150}$Nd+$^{150}$Nd collisions. We reveal prominent correlations between the\nNME and features of the quark-gluon plasma (QGP) formed in these processes,\nsuch as spatial gradients and anisotropies, which are accessible via collective\nflow measurements. Our findings demonstrate collider experiments involving\n$0\\nu\\beta\\beta$ decay candidates as a platform for benchmarking theoretical\npredictions of the NME.",
        "This article explores the motion of massive particles in the gravitational\nfield of a modified gravity (MOG) black hole (BH), characterized by the\nparameter $\\alpha$. Using the Hamiltonian formalism, the geodesic equations and\nthe effective potential governing particle trajectories are derived. Key\nfeatures, including the innermost stable circular orbit (ISCO) and the\ninnermost bound circular orbit (IBCO), are analyzed, revealing their dependence\non the particle's energy, angular momentum, and the MOG parameter. In the\nextremal case, where $\\alpha=-1$, the event horizon merges with the Cauchy\nhorizon, forming a distinctive BH configuration. Numerical methods are employed\nto compute periodic orbits in this spacetime, with a comparison drawn to the\nSchwarzschild BH. The findings indicate that for $\\alpha>0$, periodic orbits\naround Schwarzschild-MOG BH exhibit lower energy requirements than those in\nSchwarzschild spacetime, whereas for $-1<\\alpha<0$, the energy requirements are\nhigher. Precessing orbits near periodic trajectories are also examined,\noffering insights into their complex dynamical behavior. Finally, the\ngravitational wave (GW) radiation from the periodic orbits of a test particle\naround the Schwarzschild-MOG BH is examined, generating intricate waveforms\nthat provide insights into the gravitational structure of the system.",
        "Let $R$ be a root system in $\\mathbb R^N$ and $G$ be the finite subgroup\ngenerated by the reflections associated to the root system. We establish a\npositive radial product formula for the integral kernels $B_{k,1}(x,y)$ of\n$(k,1)$-generalized Fourier transforms (or the $k$-Hankel transforms) $F_{k,1}$\n$$B_{k,1}(x,z)j_{2\\left\\langle\nk\\right\\rangle+N-2}\\left(2\\sqrt{t\\left|z\\right|}\\right)=\\int_{\\mathbb R^N}\nB_{k,1}(\\xi,z)\\,d\\sigma_{x,t}^{k,1}(\\xi),$$ where $j_{\\lambda}$ is the\nnormalized Bessel function, and $\\sigma_{x,t}^{k,1}(\\xi)$ is the unique\nprobability measure. Such a product formula is equivalent to the following\nrepresentation of the generalized spherical mean operator $f\\mapsto M_f,\\;f\\in\nC_b(\\mathbb{R}^N)$ in $(k,1)$-generalized Fourier analysis \\begin{align*}\nM_f(x,t)=\\int_{\\mathbb{R}^N}f\\,d\\sigma_{x,t}^{k,1},\\;(x,t)\\in\\mathbb{R}^N\\times{\\mathbb{R}}_+.\\end{align*}\nWe will then analyze the representing measure $\\sigma_{x,t}^{k,1}(\\xi)$ and\nshow that the support of the measure is contained in\n$$\\left\\{\\xi\\in\\mathbb{R}^N:\\sqrt{\\vert\\xi\\vert}\\geq\\vert\\sqrt{\\vert\nx\\vert}-\\sqrt t\\vert\\right\\}\\cap\\left(\\bigcup_{g\\in\nG}\\{\\xi\\in\\mathbb{R}^N:d(\\xi,gx)\\leq\\sqrt t\\}\\right),$$ where\n$d\\left(x,y\\right)=\\sqrt{\\left|x\\right|+\\left|y\\right|-\\sqrt{2\\left(\\left|x\\right|\\left|y\\right|+\\left\\langle\nx,y\\right\\rangle\\right)}}$. Based on the support of the representing measure\n$\\sigma_{x,t}^{k,1}$, we will get a weak Huygens's principle for the deformed\nwave equation in $(k,1)$-generalized Fourier analysis. Moreover, for $N\\geq 2$,\nif we assume that $F_{k,1}\\left(\\mathcal S(\\mathbb{R}^N)\\right)$ consists of\nrapidly decreasing functions at infinity, then we get two different results on\n$\\text{supp}\\sigma_{x,t}^{k,1}$, which indirectly denies such assumption.",
        "Constructing and manipulating quantum states in fast-rotating Bose-Einstein\ncondensates (BEC) has long stood as a significant challenge as the rotating\nspeed approaching the critical velocity. Although the recent experiment\n[Science, 372, 1318 (2021)] has realized the geometrically squeezed state of\nthe guiding-center mode, the remaining degree of freedom, the cyclotron mode,\nremains unsqueezed due to the large energy gap of Landau levels. To overcome\nthis limitation, in this paper, we propose a Floquet-based state-preparation\nprotocol by periodically driving an anisotropic potential. This protocol not\nonly facilitates the single cyclotron-mode squeezing, but also enables a\ntwo-mode squeezing. Such two-mode squeezing offers a richer set of dynamics\ncompared to single-mode squeezing and can achieve wavepacket width well below\nthe lowest Landau level limit. Our work provides a highly controllable knob for\nrealizing diverse geometrically squeezed states in ultracold quantum gases\nwithin the quantum Hall regime.",
        "Let $G$ be a locally compact group, $\\mu$ its Haar measure, $\\hat G$ its\nPontryagin dual and $\\nu$ the dual measure. For any $A_\\theta\\in L^1(G;\\mathcal\nC_p)\\cap L^2(G;\\mathcal C_p)$, ($\\mathcal C_p$ is Schatten ideal), and\n$1<p\\le2$ we prove $$\\int_{\\hat\nG}\\left\\|\\int_GA_\\theta\\overline{\\xi(\\theta)}\\,\\mathrm\nd\\mu(\\theta)\\right\\|_p^q\\,\\mathrm d\\nu(\\xi)\\le\n  \\left(\\int_G\\|A_\\theta\\|_p^p\\,\\mathrm d\\mu(\\theta)\\right)^{q\/p}, $$ where\n$q=p\/(p-1)$. This appears to be a generalization of some earlier obtained\ninequalities, including Clarkson-McCarthy inequalities (in the case $G=\\mathbf\nZ_2$), and Hausdorff-Young inequality. Some corollaries are also given.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The radiative transfer equation models various physical processes ranging\nfrom plasma simulations to radiation therapy. In practice, these phenomena are\noften subject to uncertainties. Modeling and propagating these uncertainties\nrequires accurate and efficient solvers for the radiative transfer equations.\nDue to the equation's high-dimensional phase space, fine-grid solutions of the\nradiative transfer equation are computationally expensive and memory-intensive.\nIn recent years, dynamical low-rank approximation has become a popular method\nfor solving kinetic equations due to the development of computationally\ninexpensive, memory-efficient and robust algorithms like the augmented basis\nupdate \\& Galerkin integrator. In this work, we propose a low-rank Monte Carlo\nestimator and combine it with a control variate strategy based on\nmulti-fidelity low-rank approximations for variance reduction. We investigate\nthe error analytically and numerically and find that a joint approach to\nbalance rank and grid size is necessary. Numerical experiments further show\nthat the efficiency of estimators can be improved using dynamical low-rank\napproximation, especially in the context of control variates.",
        "We consider the Schr\\\"odinger operator on the quantum graph whose edges\nconnect the points of ${\\Bbb Z}$. The numbers of the edges connecting two\nconsecutive points $n$ and $n+1$ are read along the orbits of a shift of finite\ntype. We prove that the Lyapunov exponent is potitive for energies $E$ that do\nnot belong to a discrete subset of $[0,\\infty)$. The number of points $E$ of\nthis subset in $[(\\pi (j-1))^2, (\\pi j)^2]$ is the same for all $j\\in {\\Bbb\nN}$.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "Identifying quantum spin liquids, magnon breakdown, or fractionalized\nexcitations in quantum magnets is an ongoing challenge due to the ambiguity of\npossible origins of excitation continua occurring in linear response probes.\nRecently, it was proposed that techniques measuring higher-order response, such\nas two-dimensional coherent spectroscopy (2DCS), could resolve such\nambiguities. Numerically simulating nonlinear response functions can, however,\nbe computationally very demanding. We present an efficient Lanczos-based method\nto compute second-order susceptibilities $\\chi^{2}\\omega_t,\\omega_\\tau)$\ndirectly in the frequency domain. Applying this to extended Kitaev models\ndescribing $\\alpha$-RuCl$_3$, we find qualitatively different nonlinear\nresponses between intermediate magnetic field strengths and the high-field\nregime. To put these results into context, we derive the general 2DCS response\nof partially-polarized magnets within the linear spin-wave approximation,\nestablishing that $\\chi^2(\\omega_t,\\omega_\\tau)$ is restricted to a distinct\nuniversal form if the excitations are conventional magnons. Deviations from\nthis form, as predicted in our (Lanczos-based) simulations for\n$\\alpha$-RuCl$_3$, can hence serve in 2DCS experiments as direct criteria to\ndetermine whether an observed excitation continuum is of conventional\ntwo-magnon type or of different nature.",
        "Significant progress has been made in the field of thermophotovoltaics, with\nefficiency recently rising to over 40% due to improvements in cell design and\nmaterial quality, higher emitter temperatures, and better spectral management.\nHowever, inconsistencies in trends for efficiency with semiconductor bandgap\nenergy across various temperatures pose challenges in predicting optimal\nbandgaps or expected performance for different applications. To address these\nissues, here we present realistic performance predictions for various types of\nsingle-junction cells over a broad range of emitter temperatures using an\nempirical model based on past cell measurements. Our model is validated using\ndata from different authors with various bandgaps and emitter temperatures, and\nan excellent agreement is seen between the model and the experimental data.\nUsing our model, we show that in addition to spectral losses, it is important\nto consider practical electrical losses associated with series resistance and\ncell quality to avoid overestimation of system efficiency. We also show the\neffect of modifying various system parameters such as bandgap, above and\nbelow-bandgap reflectance, saturation current, and series resistance on the\nefficiency and power density of thermophotovoltaics at different temperatures.\nFinally, we predict the bandgap energies for best performance over a range of\nemitter temperatures for different cell material qualities.",
        "Quantum information processing, especially with quantum error correction,\nrequires both long-lived qubits and fast, quantum non-demolition readout. In\nsuperconducting circuits this leads to the requirement to both strongly couple\nqubits, such as transmons, to readout modes while also protecting them from\nassociated Purcell decay through the readout port. So-called Purcell filters\ncan provide this protection, at the cost of significant increases in circuit\ncomponents and complexity. However, as we demonstrate in this work, visualizing\nthe qubit fields in space reveals locations where the qubit fields are strong\nand cavity fields weak; simply placing ports at these locations provides\nintrinsic Purcell protection. For a $\\lambda\/2$ readout mode in the\n`chip-in-tube' geometry, we show both millisecond level Purcell protection and,\nconversely, greatly enhanced Purcell decay (qubit lifetime of 1~$\\mu$s) simply\nby relocating the readout port. This method of integrating the Purcell\nprotection into the qubit-cavity geometry can be generalized to other 3D\nimplementations, such as post-cavities, as well as planar geometries. For qubit\nfrequencies below the readout mode this effect is quite distinct from the\nmulti-mode Purcell effect, which we demonstrate in a 3D-post geometry where we\nshow both Purcell protection of the qubit while spoiling the quality factor of\nhigher cavity harmonics to protect against dephasing due to stray photons in\nthese modes.",
        "We have investigated the normal and superconducting states of the\ntechnologically important compound Nb$_3$Sn using $^{93}$Nb nuclear magnetic\nresonance. From spin-lattice relaxation we find strong suppression of the\nzero-temperature superconducting order parameter by magnetic field. We have\nidentified an anomalously large electron-nuclear exchange interaction from\nspin-spin relaxation measurements, an order of magnitude beyond that of the\ndipole-dipole interaction, and thereby sensitive to vortex dynamics and vortex\npinning.",
        "Light nuclei production in heavy-ion collisions serves as a sensitive probe\nof the QCD phase structure. In coalescence models, triton ($N_t$) and deuteron\n($N_d$) yields depend on the spatial separation of nucleon pairs ($\\Delta r$)\nin Wigner functions, yet the impact of initial two-nucleon correlations\n$\\rho(\\Delta r)$ remains underexplored. We develop a method to sample nucleons\nin $^{197}$Au nuclei that simultaneously satisfies both the single-particle\ndistribution $f(r)$ and the two-nucleon correlation $\\rho(\\Delta r)$. Using\nthese nuclei, we simulate Au+Au collisions at $\\sqrt{s_\\mathrm{NN}}=3$ GeV via\nthe SMASH transport model (mean-field mode) to calculate proton, deuteron, and\ntriton yields. Simulations reveal a 36% enhancement in mid-rapidity deuteron\nyields across all centrality ranges and a 33% rise in mid-rapidity triton\nproduction for 0-10% central collisions. Calculated transverse momentum of\nlight nuclei aligns with STAR data. We further analyze impacts of baryon\nconservation, spectator exclusion, and centrality determination via charged\nmultiplicity. Notably, observed discrepancies in the double yield ratio suggest\nunaccounted physical mechanisms, such as critical fluctuations or inaccuracies\nin coalescence parameters or light nuclei cross-sections. This underscores the\ncritical role of initial nucleon-nucleon correlations, linking microscopic\nnuclear structure to intermediate-energy collision dynamics."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques",
    "start_abstract":"Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b35"
      ],
      "title":[
        "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
      ],
      "abstract":[
        "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "Efficient Event-based Delay Learning in Spiking Neural Networks",
        "EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement\n  Learning",
        "A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for\n  Efficient Black-Box Neural Network Optimization",
        "Channel-wise Parallelizable Spiking Neuron with Multiplication-free\n  Dynamics and Large Temporal Receptive Fields",
        "Quantum Simplicial Neural Networks",
        "Runtime Analysis of the Compact Genetic Algorithm on the LeadingOnes\n  Benchmark",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Impact of Surrogate Model Accuracy on Performance and Model Management\n  Strategy in Surrogate-Assisted Evolutionary Algorithms",
        "Waves and symbols in neuromorphic hardware: from analog signal\n  processing to digital computing on the same computational substrate",
        "Comprehensive Benchmarking Environment for Worker Flexibility in\n  Flexible Job Shop Scheduling Problems",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "RL + Transformer = A General-Purpose Problem Solver",
        "Matter creation, adiabaticity and phantom behavior",
        "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone",
        "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing",
        "Tempo: Helping Data Scientists and Domain Experts Collaboratively\n  Specify Predictive Modeling Tasks",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Ultrafast Proton Delivery with Pin Ridge Filters (pRFs): A Novel\n  Approach for Motion Management in Proton Therapy",
        "Upper limits on the gamma-ray emission from the microquasar V4641 Sgr",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "The bright, dusty aftermath of giant eruptions & H-rich supernovae. Late\n  interaction of supernova shocks & dusty circumstellar shells",
        "Evolution and Pathogenicity of SARS-CoVs: A Microcanonical Analysis of\n  Receptor-Binding Motifs",
        "PVTree: Realistic and Controllable Palm Vein Generation for Recognition\n  Tasks",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "Generative Learning of Densities on Manifolds",
        "scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation\n  Model Knowledge"
      ],
      "abstract":[
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Spiking Neural Networks (SNNs) are attracting increased attention as a more\nenergy-efficient alternative to traditional Artificial Neural Networks. Spiking\nneurons are stateful and intrinsically recurrent, making them well-suited for\nspatio-temporal tasks. However, this intrinsic memory is limited by synaptic\nand membrane time constants. A powerful additional mechanism are delays. In\nthis paper, we propose a novel event-based training method for SNNs with\ndelays, grounded in the EventProp formalism and enabling the calculation of\nexact gradients with respect to weights and delays. Our method supports\nmultiple spikes per neuron and, to our best knowledge, is the first delay\nlearning algorithm to be applied to recurrent SNNs. We evaluate our method on a\nsimple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits and\nSpiking Speech Commands datasets, demonstrating that our algorithm can optimize\ndelays from suboptimal initial conditions and enhance classification accuracy\ncompared to architectures without delays. Finally, we show that our approach\nuses less than half the memory of the current state-of-the-art delay-learning\nmethod and is up to 26x faster.",
        "Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising\napproach to overcoming the limitations of traditional reinforcement learning\n(RL) by integrating the Evolutionary Computation (EC) paradigm with RL.\nHowever, the population-based nature of EC significantly increases\ncomputational costs, thereby restricting the exploration of algorithmic design\nchoices and scalability in large-scale settings. To address this challenge, we\nintroduce $\\texttt{$\\textbf{EvoRL}$}$, the first end-to-end EvoRL framework\noptimized for GPU acceleration. The framework executes the entire training\npipeline on accelerators, including environment simulations and EC processes,\nleveraging hierarchical parallelism through vectorization and compilation\ntechniques to achieve superior speed and scalability. This design enables the\nefficient training of large populations on a single machine. In addition to its\nperformance-oriented design, $\\texttt{$\\textbf{EvoRL}$}$ offers a comprehensive\nplatform for EvoRL research, encompassing implementations of traditional RL\nalgorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g.,\nCMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL\n(e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT). The framework's\nmodular architecture and user-friendly interface allow researchers to\nseamlessly integrate new components, customize algorithms, and conduct fair\nbenchmarking and ablation studies. The project is open-source and available at:\nhttps:\/\/github.com\/EMI-Group\/evorl.",
        "Swarm intelligence optimization algorithms have gained significant attention\ndue to their ability to solve complex optimization problems. However, the\nefficiency of optimization in large-scale problems limits the use of related\nmethods. This paper presents a GPU-accelerated version of the Multi-Guiding\nSpark Fireworks Algorithm (MGFWA), which significantly improves the\ncomputational efficiency compared to its traditional CPU-based counterpart. We\nbenchmark the GPU-MGFWA on several neural network black-box optimization\nproblems and demonstrate its superior performance in terms of both speed and\nsolution quality. By leveraging the parallel processing power of modern GPUs,\nthe proposed GPU-MGFWA results in faster convergence and reduced computation\ntime for large-scale optimization tasks. The proposed implementation offers a\npromising approach to accelerate swarm intelligence algorithms, making them\nmore suitable for real-time applications and large-scale industrial problems.\nSource code is released at https:\/\/github.com\/mxxxr\/MGFWA.",
        "Spiking Neural Networks (SNNs) are distinguished from Artificial Neural\nNetworks (ANNs) for their sophisticated neuronal dynamics and sparse binary\nactivations (spikes) inspired by the biological neural system. Traditional\nneuron models use iterative step-by-step dynamics, resulting in serial\ncomputation and slow training speed of SNNs. Recently, parallelizable spiking\nneuron models have been proposed to fully utilize the massive parallel\ncomputing ability of graphics processing units to accelerate the training of\nSNNs. However, existing parallelizable spiking neuron models involve dense\nfloating operations and can only achieve high long-term dependencies learning\nability with a large order at the cost of huge computational and memory costs.\nTo solve the dilemma of performance and costs, we propose the mul-free\nchannel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable\nfor SNNs' resource-restricted application scenarios. The proposed neuron\nimports the channel-wise convolution to enhance the learning ability, induces\nthe sawtooth dilations to reduce the neuron order, and employs the bit shift\noperation to avoid multiplications. The algorithm for design and implementation\nof acceleration methods is discussed meticulously. Our methods are validated in\nneuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and\nneuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.\nTraining speed results demonstrate the effectiveness of our acceleration\nmethods, providing a practical reference for future research.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "The compact genetic algorithm (cGA) is one of the simplest\nestimation-of-distribution algorithms (EDAs). Next to the univariate marginal\ndistribution algorithm (UMDA) -- another simple EDA -- , the cGA has been\nsubject to extensive mathematical runtime analyses, often showcasing a similar\nor even superior performance to competing approaches. Surprisingly though, up\nto date and in contrast to the UMDA and many other heuristics, we lack a\nrigorous runtime analysis of the cGA on the LeadingOnes benchmark -- one of the\nmost studied theory benchmarks in the domain of evolutionary computation.\n  We fill this gap in the literature by conducting a formal runtime analysis of\nthe cGA on LeadingOnes. For the cGA's single parameter -- called the\nhypothetical population size -- at least polylogarithmically larger than the\nproblem size, we prove that the cGA samples the optimum of LeadingOnes with\nhigh probability within a number of function evaluations quasi-linear in the\nproblem size and linear in the hypothetical population size. For the best\nhypothetical population size, our result matches, up to polylogarithmic\nfactors, the typical quadratic runtime that many randomized search heuristics\nexhibit on LeadingOnes. Our analysis exhibits some noteworthy differences in\nthe working principles of the two algorithms which were not visible in previous\nworks.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "Surrogate-assisted evolutionary algorithms (SAEAs) have been proposed to\nsolve expensive optimization problems. Although SAEAs use surrogate models that\napproximate the evaluations of solutions using machine learning techniques,\nprior research has not adequately investigated the impact of surrogate model\naccuracy on search performance and model management strategy in SAEAs. This\nstudy analyzes how surrogate model accuracy affects search performance and\nmodel management strategies. For this purpose, we construct a pseudo-surrogate\nmodel with adjustable prediction accuracy to ensure fair comparisons across\ndifferent model management strategies. We compared three model management\nstrategies: (1) pre-selection (PS), (2) individual-based (IB), and (3)\ngeneration-based (GB) on standard benchmark problems with a baseline model that\ndoes not use surrogates. The experimental results reveal that a higher\nsurrogate model accuracy improves the search performance. However, the impact\nvaries according to the strategy used. Specifically, PS demonstrates a clear\ntrend of improved performance as the estimation accuracy increases, whereas IB\nand GB exhibit robust performance when the accuracy surpasses a certain\nthreshold. In model strategy comparisons, GB exhibits superior performance\nacross a broad range of prediction accuracies, IB outperforms it at lower\naccuracies, and PS outperforms it at higher accuracies. The findings of this\nstudy clarify guidelines for selecting appropriate model management strategies\nbased on the surrogate model accuracy.",
        "Neural systems use the same underlying computational substrate to carry out\nanalog filtering and signal processing operations, as well as discrete symbol\nmanipulation and digital computation. Inspired by the computational principles\nof canonical cortical microcircuits, we propose a framework for using recurrent\nspiking neural networks to seamlessly and robustly switch between analog signal\nprocessing and categorical and discrete computation. We provide theoretical\nanalysis and practical neural network design tools to formally determine the\nconditions for inducing this switch. We demonstrate the robustness of this\nframework experimentally with hardware soft Winner-Take-All and mixed-feedback\nrecurrent spiking neural networks, implemented by appropriately configuring the\nanalog neuron and synapse circuits of a mixed-signal neuromorphic processor\nchip.",
        "In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP)\naims to optimize a sequence of operations and assign each to an eligible\nmachine with varying processing times. For integration of the workforce, each\nmachine also requires a worker to be present to process an operation which\nadditionally affects the processing times. The resulting problem is called\nFlexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The\nFJSSP has been approached with various problem representations, including Mixed\nInteger Linear Programming (MILP), Constrained Programming (CP), and\nSimulation-based Optimization (SBO). In the latter area in particular, there\nexists a large number of specialized Evolutionary Algorithms (EA) like Particle\nSwarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often\ndeveloped for single use cases only, and validated on a few selected test\ninstances, let alone compared with results from solvers using other problem\nrepresentations. While suitable approaches do also exist, the design of the\nFJSSP-W instances is not standardized and the algorithms are hardly comparable.\nThis calls for a systematic benchmarking environment that provides a\ncomprehensive set of FJSSP(-W) instances and supports targeted algorithm\ndevelopment. It will facilitate the comparison of algorithmic performance in\nthe face of different problem characteristics. The present paper presents a\ncollection of 402 commonly accepted FJSSP instances and proposes an approach to\nextend these with worker flexibility. In addition, we present a detailed\nprocedure for the evaluation of scheduling algorithms on these problem sets and\nprovide suitable model representations for this purpose. We provide complexity\ncharacteristics for all presented instances as well as baseline results of\ncommon commercial solvers to facilitate the validation of new algorithmic\ndevelopments.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.",
        "We present a novel cosmological framework that unifies matter creation\ndynamics with thermodynamic principles. Starting with a single-component fluid\ncharacterized by a constant equation of state parameter, $\\omega$, we introduce\na generalized second law of thermodynamics by considering the entropy\nassociated with the cosmic horizon. Imposing an adiabatic expansion condition\nuniquely determines the particle creation rate, $\\Gamma$, a feature\nunprecedented in previous matter creation models. This mechanism yields a\ncosmology featuring phantom-like expansion while relying solely on a single\nconstituent, which can be either a quintessence-like fluid or a non-exotic,\nnon-relativistic dark matter component. Remarkably, this framework avoids the\nneed for exotic physics while providing a consistent explanation for the\naccelerated expansion of the universe. Our results open new pathways for\nunderstanding the interplay between horizon thermodynamics, particle creation,\nand cosmic evolution, offering fresh insights into the nature of dark energy\nand its potential thermodynamic origins.",
        "Although 85% of sub-Saharan Africa's population is covered by mobile\nbroadband signal, only 37% use the internet, and those who do seldom use the\nweb. The most frequently cited reason for low internet usage is the cost of\ndata. We investigate whether AI can bridge this gap by analyzing 40,350 queries\nsubmitted to an AI chatbot by 469 teachers in Sierra Leone over 17 months.\nTeachers use AI for teaching assistance more frequently than web search. We\ncompare the AI responses to the corresponding top search results for the same\nqueries from the most popular local web search engine, google.com.sl. Only 2%\nof results for corresponding web searches contain content from in country.\nAdditionally, the average web search result consumes 3,107 times more data than\nan AI response. Bandwidth alone costs \\$2.41 per thousand web search results\nloaded, while the total cost of AI is \\$0.30 per thousand responses. As a\nresult, AI is 87% less expensive than web search. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
        "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.",
        "Temporal predictive models have the potential to improve decisions in health\ncare, public services, and other domains, yet they often fail to effectively\nsupport decision-makers. Prior literature shows that many misalignments between\nmodel behavior and decision-makers' expectations stem from issues of model\nspecification, namely how, when, and for whom predictions are made. However,\nmodel specifications for predictive tasks are highly technical and difficult\nfor non-data-scientist stakeholders to interpret and critique. To address this\nchallenge we developed Tempo, an interactive system that helps data scientists\nand domain experts collaboratively iterate on model specifications. Using\nTempo's simple yet precise temporal query language, data scientists can quickly\nprototype specifications with greater transparency about pre-processing\nchoices. Moreover, domain experts can assess performance within data subgroups\nto validate that models behave as expected. Through three case studies, we\ndemonstrate how Tempo helps multidisciplinary teams quickly prune infeasible\nspecifications and identify more promising directions to explore.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "Active breath-hold techniques effectively mitigate respiratory motion but\npose challenges for patients who are ineligible for the procedure. Conventional\ntreatment planning relies on multiple energy layers, extending delivery time\ndue to slow layer switching. We propose to use pin ridge filters (pRFs),\ninitially developed for FLASH radiotherapy, to construct a single energy beam\nplan and minimize dose delivery time. The conventional ITV--based\nfree--breathing treatment plan served as the reference. A GTV--based IMPT--DS\nplan with a downstream energy modulation strategy was developed based on a new\nbeam model that was commissioned using the maximum energy of the IMPT plan.\nConsequently, a nested iterative pencil beam direction (PBD) spot reduction\nprocess removed low--weighted spots along each PBD, generating pRFs with\ncoarser resolution. Finally, the IMPT--DS plan was then converted into an\nIMPT--pRF plan, using a monoenergetic beam with optimized spot positions and\nweights. This approach was validated on lung and liver SBRT cases (10 Gy RBE x\n5). For the lung case, the mean lung--GTV dose decreased from 10.3 Gy to 6.9\nGy, with delivery time reduced from 188.79 to 36.16 seconds. The largest time\nreduction was at 150{\\deg}, from 47.4 to 3.99 seconds. For the liver case, the\nmean liver--GTV dose decreased from 5.7 Gy to 3.8 Gy, with delivery time\nreduced from 111.13 to 30.54 seconds. The largest time reduction was at\n180{\\deg}, from 38.57 to 3.94 seconds. This method significantly reduces dose\ndelivery time and organ at risk dose. Further analysis is needed to validate\nits clinical feasibility.",
        "Following a recent detection of TeV radiation by the Large High Altitude Air\nShower Observatory (LHAASO) and the High-Altitude Water Cherenkov Observatory\n(HAWC), coincident with the direction of the microquasar V4641 Sgr, we search\nfor possible GeV emission from this source. We explored the morphology and\ntemporal features of the source as well as two nearby unassociated point\nsources which could be a part of extended structure of V4641 Sgr, and compared\nresults with corresponding X-ray and TeV emissions. The 95% confidence level\nupper limits for the flux from the source, assuming both point and extended\nsource models were 5.38$\\times$ 10$^{-13}$ erg cm$^{-2}$ s$^{-1}$ and\n1.12$\\times$ 10$^{-12}$ erg cm$^{-2}$ s$^{-1}$, respectively. Additionally, no\ncorrelation between gamma-ray light curve and X-ray outbursts was observed.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "The late-stage evolution of massive stars is marked by intense instability as\nthey approach core-collapse. During these phases, giant stellar eruptions lead\nto exceptionally high mass-loss rates, forming significant amounts of dust.\nHowever, the survival of these dust grains is challenged by the powerful shock\nwaves generated when the progenitor explodes as a supernova (SN). We explore\nthe impact of hydrogen-rich SN explosions from 45, 50, and 60 M$_\\odot$\nprogenitors on dust formed after these eruptions, focusing on interactions with\ncircumstellar shells occurring from a few years to centuries after the event.\nUsing 3D hydrodynamical simulations, we track the evolution of dust particles\nin a scenario that includes the progenitor's stellar wind, a giant eruption,\nand the subsequent SN explosion, following the mass budgets predicted by\nstellar evolution models. For a standard SN ejecta mass of 10 M$_\\odot$ and\nkinetic energy of $10^{51}$ erg, only 25% of the dust mass survives 250 years\npost-explosion in a spherical circumstellar medium (CSM), while merely 2%\nremains a century after the explosion in a bipolar CSM. If the SN follows the\neruption within a dozen years, 75% of the dust survives for a standard\nexplosion, dropping to 20% for more massive ejecta (15-20 M$_\\odot$) with\nkinetic energy of $5 \\times 10^{51}$ erg. The geometry of the CSM and the early\ntransition of the SN remnant into a radiative phase significantly influence\ndust survival. As the shock wave weakens and efficiently converts kinetic\nenergy into thermal radiation (up to half of the injected kinetic energy) the\nlikelihood of dust survival increases, affecting not only pre-existing dust in\nthe CSM but also SN-condensed dust and ambient interstellar dust. Contrary to\nexpectations, a larger fraction of the dust mass can survive if the SN occurs\nonly a few years after the eruption.",
        "The rapid evolution and global impact of coronaviruses, notably SARS-CoV-1\nand SARS-CoV-2, underscore the importance of understanding their molecular\nmechanisms in detail. This study focuses on the receptor-binding motif (RBM)\nwithin the Spike protein of these viruses, a critical element for viral entry\nthrough interaction with the ACE2 receptor. We investigate the sequence\nvariations in the RBM across SARS-CoV-1, SARS-CoV-2 and its early variants of\nconcern (VOCs). Utilizing multicanonical simulations and microcanonical\nanalysis, we examine how these variations influence the folding dynamics,\nthermostability, and solubility of the RBMs. Our methodology includes\ncalculating the density of states (DoS) to identify structural phase\ntransitions and assess thermodynamic properties. Furthermore, we solve the\nPoisson-Boltzmann equation to model the solubility of the RBMs in aqueous\nenvironments. This methodology is expected to elucidate structural and\nfunctional differences in viral evolution and pathogenicity, likely improving\ntargeted treatments and vaccines.",
        "Palm vein recognition is an emerging biometric technology that offers\nenhanced security and privacy. However, acquiring sufficient palm vein data for\ntraining deep learning-based recognition models is challenging due to the high\ncosts of data collection and privacy protection constraints. This has led to a\ngrowing interest in generating pseudo-palm vein data using generative models.\nExisting methods, however, often produce unrealistic palm vein patterns or\nstruggle with controlling identity and style attributes. To address these\nissues, we propose a novel palm vein generation framework named PVTree. First,\nthe palm vein identity is defined by a complex and authentic 3D palm vascular\ntree, created using an improved Constrained Constructive Optimization (CCO)\nalgorithm. Second, palm vein patterns of the same identity are generated by\nprojecting the same 3D vascular tree into 2D images from different views and\nconverting them into realistic images using a generative model. As a result,\nPVTree satisfies the need for both identity consistency and intra-class\ndiversity. Extensive experiments conducted on several publicly available\ndatasets demonstrate that our proposed palm vein generation method surpasses\nexisting methods and achieves a higher TAR@FAR=1e-4 under the 1:1 Open-set\nprotocol. To the best of our knowledge, this is the first time that the\nperformance of a recognition model trained on synthetic palm vein data exceeds\nthat of the recognition model trained on real data, which indicates that palm\nvein image generation research has a promising future.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "A generative modeling framework is proposed that combines diffusion models\nand manifold learning to efficiently sample data densities on manifolds. The\napproach utilizes Diffusion Maps to uncover possible low-dimensional underlying\n(latent) spaces in the high-dimensional data (ambient) space. Two approaches\nfor sampling from the latent data density are described. The first is a\nscore-based diffusion model, which is trained to map a standard normal\ndistribution to the latent data distribution using a neural network. The second\none involves solving an It\\^o stochastic differential equation in the latent\nspace. Additional realizations of the data are generated by lifting the samples\nback to the ambient space using Double Diffusion Maps, a recently introduced\ntechnique typically employed in studying dynamical system reduction; here the\nfocus lies in sampling densities rather than system dynamics. The proposed\napproaches enable sampling high dimensional data densities restricted to\nlow-dimensional, a priori unknown manifolds. The efficacy of the proposed\nframework is demonstrated through a benchmark problem and a material with\nmultiscale structure.",
        "Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of\nmillions of human cells across organs, diseases, development and perturbations\nto date. However, the high-dimensional sparsity, batch effect noise, category\nimbalance, and ever-increasing data scale of the original sequencing data pose\nsignificant challenges for multi-center knowledge transfer, data fusion, and\ncross-validation between scRNA-seq datasets. To address these barriers, (1) we\nfirst propose a latent codes-based scRNA-seq dataset distillation framework\nnamed scDD, which transfers and distills foundation model knowledge and\noriginal dataset information into a compact latent space and generates\nsynthetic scRNA-seq dataset by a generator to replace the original dataset.\nThen, (2) we propose a single-step conditional diffusion generator named SCDG,\nwhich perform single-step gradient back-propagation to help scDD optimize\ndistillation quality and avoid gradient decay caused by multi-step\nback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics\nand inter-class discriminability of the synthetic dataset through flexible\nconditional control and generation quality assurance. Finally, we propose a\ncomprehensive benchmark to evaluate the performance of scRNA-seq dataset\ndistillation in different data analysis tasks. It is validated that our\nproposed method can achieve 7.61% absolute and 15.70% relative improvement over\nprevious state-of-the-art methods on average task."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b35",
    "start_title":"Deep learning-based framework for automatic cranial defect reconstruction and implant modeling",
    "start_abstract":"This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
      ],
      "abstract":[
        "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Dynamics and control of maize infection by Busseola fusca:\n  multi-seasonal modeling and biocontrol strategies",
        "Constraining the curvature-induced quantum gravity scales via gamma-ray\n  bursts",
        "Neural Chaos: A Spectral Stochastic Neural Operator",
        "Constructing balanced datasets for predicting failure modes in\n  structural systems under seismic hazards",
        "On rigid regular graphs and a problem of Babai and Pultr",
        "Groupoids, equivalence bibundles and bimodules for noncommutative\n  solenoids",
        "The Solo Revolution: A Theory of AI-Enabled Individual Entrepreneurship",
        "Moduli of curves and moduli of sheaves",
        "Time-dependent global sensitivity analysis of the Doyle-Fuller-Newman\n  model",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Human fields and their impact on brain waves A pilot study",
        "Highly efficient field-free switching by orbital Hall torque in a\n  MoS2-based device operating at room temperature",
        "Ergodic optimization for beta-transformations",
        "A non-homogeneous, non-stationary and path-dependent Markov anomalous\n  diffusion model",
        "Computational Complexity of Covering Colored Mixed Multigraphs with\n  Simple Degree Partitions",
        "Multi-scale Energy Release Events in the Quiet Sun: A Possible Source\n  for Coronal Heating"
      ],
      "abstract":[
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Maize production in sub-Saharan Africa faces significant challenges due to\nthe maize stalk borer (Busseola fusca), a major pest that causes substantial\nyield losses. Chemical control methods have raised concerns about environmental\nimpact and pest resistance, making biological control a promising alternative.\nIn this study, we develop a multi-seasonal mathematical model using an\nimpulsive system of differential equations to describe stalk borer population\ndynamics and evaluate pest control strategies. We analyze the stability of the\npest-free solution using Floquet theory and study the effects of periodic\npredator releases on pest suppression. Numerical simulations illustrate the\nimpact of cultural practice and predator release frequency. Moreover, our\nsimulations show that, under good cultural practices, releasing predators once\nor three times a year is an effective biocontrol strategy. However, in cases of\npoor cultural practices, biocontrol has only a limited effect, and the best\noutcome is achieved when predators are released once a year at the beginning of\nthe cropping season.",
        "We constrain the parameters that govern curvature-induced quantum gravity\ntime-of-flight (TOF) effects. These TOF delays, which occur due to modified\ndispersion relations of particles in a vacuum, could be a phenomenological\nsignature of quantum gravity. Gamma-ray bursts (GRBs), short, high-energy\nevents from distant galaxies, offer a unique opportunity to impose\nobservational limits on TOF delays and, by extension, on the energy scales of\nquantum gravity. Using the standard Jacob-Piran relation, which assumes a\nlocally-flat spacetime, the analysis of quantum gravity-induced TOF effects\nestablishes a lower limit of approximately 10 Planck energies on the energy\nscale of these effects. However, curvature-induced quantum gravity effects may\nintroduce additional contributions. From current GRB observations, we find\nthat, at a 95% credibility level, in the symmetry-deformed scenario,\ncurvature-induced TOF effects may only arise at energies above 0.04 Planck\nenergy. If we consider only curvature-induced effects, this limit is an order\nof magnitude stronger. Observing more GRBs at different redshifts could improve\nthe constraints on the curvature-induced QG phenomena. However, given the\ncapabilities of current telescopes and the current understanding of GRBs, it is\nunlikely that these constraints will be significantly extended beyond the\npresent level.",
        "Building surrogate models with uncertainty quantification capabilities is\nessential for many engineering applications where randomness, such as\nvariability in material properties, is unavoidable. Polynomial Chaos Expansion\n(PCE) is widely recognized as a to-go method for constructing stochastic\nsolutions in both intrusive and non-intrusive ways. Its application becomes\nchallenging, however, with complex or high-dimensional processes, as achieving\naccuracy requires higher-order polynomials, which can increase computational\ndemands and or the risk of overfitting. Furthermore, PCE requires specialized\ntreatments to manage random variables that are not independent, and these\ntreatments may be problem-dependent or may fail with increasing complexity. In\nthis work, we adopt the spectral expansion formalism used in PCE; however, we\nreplace the classical polynomial basis functions with neural network (NN) basis\nfunctions to leverage their expressivity. To achieve this, we propose an\nalgorithm that identifies NN-parameterized basis functions in a purely\ndata-driven manner, without any prior assumptions about the joint distribution\nof the random variables involved, whether independent or dependent. The\nproposed algorithm identifies each NN-parameterized basis function\nsequentially, ensuring they are orthogonal with respect to the data\ndistribution. The basis functions are constructed directly on the joint\nstochastic variables without requiring a tensor product structure. This\napproach may offer greater flexibility for complex stochastic models, while\nsimplifying implementation compared to the tensor product structures typically\nused in PCE to handle random vectors. We demonstrate the effectiveness of the\nproposed scheme through several numerical examples of varying complexity and\nprovide comparisons with classical PCE.",
        "Accurate prediction of structural failure modes under seismic excitations is\nessential for seismic risk and resilience assessment. Traditional\nsimulation-based approaches often result in imbalanced datasets dominated by\nnon-failure or frequently observed failure scenarios, limiting the\neffectiveness in machine learning-based prediction. To address this challenge,\nthis study proposes a framework for constructing balanced datasets that include\ndistinct failure modes. The framework consists of three key steps. First,\ncritical ground motion features (GMFs) are identified to effectively represent\nground motion time histories. Second, an adaptive algorithm is employed to\nestimate the probability densities of various failure domains in the space of\ncritical GMFs and structural parameters. Third, samples generated from these\nprobability densities are transformed into ground motion time histories by\nusing a scaling factor optimization process. A balanced dataset is constructed\nby performing nonlinear response history analyses on structural systems with\nparameters matching the generated samples, subjected to corresponding\ntransformed ground motion time histories. Deep neural network models are\ntrained on balanced and imbalanced datasets to highlight the importance of\ndataset balancing. To further evaluate the framework's applicability, numerical\ninvestigations are conducted using two different structural models subjected to\nrecorded and synthetic ground motions. The results demonstrate the framework's\nrobustness and effectiveness in addressing dataset imbalance and improving\nmachine learning performance in seismic failure mode prediction.",
        "A graph is \\textit{rigid} if it only admits the identity endomorphism. We\nshow that for every $d\\ge 3$ there exist infinitely many mutually rigid\n$d$-regular graphs of arbitrary odd girth $g\\geq 7$. Moreover, we determine the\nminimum order of a rigid $d$-regular graph for every $d\\ge 3$. This provides\nstrong positive answers to a question of van der Zypen\n[https:\/\/mathoverflow.net\/q\/296483, https:\/\/mathoverflow.net\/q\/321108].\nFurther, we use our construction to show that every finite monoid is isomorphic\nto the endomorphism monoid of a regular graph. This solves a problem of Babai\nand Pultr [J. Comb.~Theory, Ser.~B, 1980].",
        "Let $p$ be a prime number and $\\mathcal{S}_p$ the $p$-solenoid. For\n$\\alpha\\in \\mathbb{R}\\times \\mathbb{Q}_p$ we consider in this paper a naturally\nassociated action groupoid $S_\\alpha:=\\mathbb{Z} [1\/p]\\ltimes_\\alpha\n\\mathcal{S}_p \\rightrightarrows \\mathcal{S}_p$ whose $C^*-$algebra is a model\nfor the noncommutative solenoid $\\mathcal{A}_\\alpha^\\mathscr{S}$ studied by\nLatremoli\\`ere and Packer. Following the geometric ideas of Connes and Rieffel\nto describe the Morita equivalences of noncommutative torus using the Kronecker\nfoliation on the torus, we give an explicit description of the\ngeometric\/topologic equivalence bibundle for groupoids $S_\\alpha$ and $S_\\beta$\nwhenever $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ are in the same orbit\nof the $GL_2(\\mathbb{Z}[1\/p])$ action by linear fractional transformations. As\na corollary, for $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ as above we\nget an explicit description of the imprimitivity bimodules for the associated\nnoncommutative solenoids.",
        "This paper presents the AI Enabled Individual Entrepreneurship Theory (AIET),\na theoretical framework explaining how artificial intelligence technologies\ntransform individual entrepreneurial capability. The theory identifies two\nfoundational premises: knowledge democratization and resource requirements\nevolution. Through three core mechanisms skill augmentation, capital structure\ntransformation, and risk profile modification AIET explains how individuals can\nnow undertake entrepreneurial activities at scales previously requiring\nsignificant organizational infrastructure. The theory presents five testable\npropositions addressing the changing relationship between organizational size\nand competitive advantage, the expansion of individual entrepreneurial\ncapacity, the transformation of market entry barriers, the evolution of\ntraditional firm advantages, and the modification of entrepreneurial risk\nprofiles. Boundary conditions related to task characteristics and market\nconditions define the theory's scope and applicability. The framework suggests\nsignificant implications for entrepreneurship theory, organizational design,\nand market structure as AI capabilities continue to advance. This theory\nprovides a foundation for understanding the evolving landscape of\nentrepreneurship in an AI-enabled world.",
        "Relationships between moduli spaces of curves and sheaves on 3-folds are\npresented starting with the Gromov-Witten\/Donaldson-Thomas correspondence\nproposed more than 20 years ago with D. Maulik, N. Nekrasov, and A. Okounkov.\nThe descendent and relative correspondences as developed with A. Pixton in the\ncontext of stable pairs led to the proof of the correspondence for the\nCalabi-Yau quintic 3-fold. More recently, the study of correspondences in\nfamilies has played an important role in connection with other basic moduli\nproblems in algebraic geometry. The full conjectural framework is presented\nhere in the context of families of 3-folds. This article accompanies my lecture\nat the ICBS in July 2024.",
        "The Doyle-Fuller-Newman model is arguably the most ubiquitous electrochemical\nmodel in lithium-ion battery research. Since it is a highly nonlinear model,\nits input-output relations are still poorly understood. Researchers therefore\noften employ sensitivity analyses to elucidate relative parametric importance\nfor certain use cases. However, some methods are ill-suited for the complexity\nof the model and appropriate methods often face the downside of only being\napplicable to scalar quantities of interest. We implement a novel framework for\nglobal sensitivity analysis of time-dependent model outputs and apply it to a\ndrive cycle simulation. We conduct a full and a subgroup sensitivity analysis\nto resolve lowly sensitive parameters and explore the model error when\nunimportant parameters are set to arbitrary values. Our findings suggest that\nthe method identifies insensitive parameters whose variations cause only small\ndeviations in the voltage response of the model. By providing the methodology,\nwe hope research questions related to parametric sensitivity for time-dependent\nquantities of interest, such as voltage responses, can be addressed more easily\nand adequately in simulative battery research and beyond.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "During brain function, groups of neurons fire synchronously. When these\ngroups are large enough, the resulting electrical signals can be measured on\nthe scalp using Electroencephalography (EEG). The amplitude of these signals\ncan be significant depending on the size and synchronization of the neural\nactivity. EEG waves exhibit distinct patterns based on the brain's state, such\nas whether it is asleep, awake, engaged in mental calculations, or performing\nother cognitive functions. Additionally, these patterns can be modified by\nexternal factors, such as transcranial magnetic stimulation (TMS). TMS involves\nbringing an antenna that generates variable electromagnetic fields close to\nspecific areas of the skull to treat certain pathologies. Given that the human\nbody naturally generates magnetic fields, a question arises: Can these fields\ninfluence the EEG by modulating neuronal function, causing a resonance effect,\nor through some unknown interaction? This study investigated whether\napproaching the palm of the hand to the top of the head (Intervention) could\ninduce effects in the EEG. Power Spectral Density (PSD) was obtained for the 30\nseconds preceding the intervention (PSD_pre) and the final 30 seconds of the\nintervention (PSD_last). The exact Wilcoxon signed-rank test suggests that the\nmedian of PSD_pre is greater than the median of PSD_last at the 95% confidence\nlevel (p-value = 0.004353). In contrast, in the control group, the test\nindicates that at the 95% confidence level (p-value = 0.7667), the median of\nPSD_pre is not greater than the median of PSD_last.",
        "Charge-to-spin and spin-to-charge conversion mechanisms in high spin-orbit\nmaterials are the new frontier of memory devices. They operate via spin-orbit\ntorque (SOT) switching of a magnetic electrode, driven by an applied charge\ncurrent. In this work, we propose a novel memory device based on the\nsemiconducting two-dimensional centrosymmetric transition metal dichalcogenide\n(TMD) MoS2, that operates as a SOT device in the writing process and a spin\nvalve in the reading process. We demonstrate that stable voltage states at room\ntemperature can be deterministically controlled by a switching current density\nas low as 3.2x10^4 A\/cm^2 even in zero field. An applied field 50-100 Oe can be\nused as a further or alternative control parameter for the state switching. Ab\ninitio calculations of spin Hall effect (SHE) and orbital Hall effect (OHE)\nindicate that the latter is the only one responsible for the generation of the\nSOT in the magnetic electrode. The large value of OHC in bulk MoS2 makes our\ndevice competitive in terms of energetic efficiency and could be integrated in\nTMD heterostructures to design memory devices with multiple magnetization\nstates for non-Boolean computation.",
        "Ergodic optimization for beta-transformations $T_\\beta(x)= \\beta x \\pmod 1$\nis developed. If $\\beta>1$ is a beta-number, or such that the orbit-closure of\n$1$ is not minimal, we show that the Typically Periodic Optimization Conjecture\nholds, establishing that there exists an open dense set of H\\\"{o}lder\ncontinuous functions such that for each function in this set, there exists a\nunique maximizing measure, this measure is supported on a periodic orbit, and\nthe periodic locking property holds. It follows that typical periodic\noptimization is typical among the class of beta-transformations: it holds for a\nset of parameters $\\beta>1$ that is residual, and has full Lebesgue measure.",
        "A novel probabilistic framework for modelling anomalous diffusion is\npresented. The resulting process is Markovian, non-homogeneous, non-stationary,\nnon-ergodic, and state-dependent. The fundamental law governing this process is\ndriven by two opposing forces: one proportional to the current state,\nrepresenting the intensity of autocorrelation or contagion, and another\ninversely proportional to the elapsed time, acting as a damping function. The\ninterplay between these forces determines the diffusion regime, characterized\nby the ratio of their proportionality coefficients. This framework encompasses\nvarious regimes, including subdiffusion, Brownian non-Gaussian, superdiffusion,\nballistic, and hyperballistic behaviours. The hyperballistic regime emerges\nwhen the correlation force dominates over damping, whereas a balance between\nthese mechanisms results in a ballistic regime, which is also stationary.\nCrucially, non-stationarity is shown to be necessary for regimes other than\nballistic. The model's ability to describe hyperballistic phenomena has been\ndemonstrated in applications such as epidemics, software reliability, and\nnetwork traffic. Furthermore, deviations from Gaussianity are explored and\nviolations of the Central Limit Theorem are highlighted, supported by\ntheoretical analysis and simulations. It will also be shown that the model\nexhibits a strong autocorrelation structure due to a position dependent jump\nprobability.",
        "The notion of graph covers (also referred to as locally bijective\nhomomorphisms) plays an important role in topological graph theory and has\nfound its computer science applications in models of local computation. For a\nfixed target graph $H$, the {\\sc $H$-Cover} problem asks if an input graph $G$\nallows a graph covering projection onto $H$. Despite the fact that the quest\nfor characterizing the computational complexity of {\\sc $H$-Cover} had been\nstarted more than 30 years ago, only a handful of general results have been\nknown so far.\n  In this paper, we present a complete characterization of the computational\ncomplexity of covering coloured graphs for the case that every equivalence\nclass in the degree partition of the target graph has at most two vertices. We\nprove this result in a very general form. Following the lines of current\ndevelopment of topological graph theory, we study graphs in the most relaxed\nsense of the definition. In particular, we consider graphs that are mixed (they\nmay have both directed and undirected edges), may have multiple edges, loops,\nand semi-edges. We show that a strong P\/NP-complete dichotomy holds true in the\nsense that for each such fixed target graph $H$, the {\\sc $H$-Cover} problem is\neither polynomial-time solvable for arbitrary inputs, or NP-complete even for\nsimple input graphs.",
        "The coronal heating problem remains one of the most challenging questions in\nsolar physics. The energy driving coronal heating is widely understood to be\nassociated with convective motions below the photosphere. Recent\nhigh-resolution observations reveal that photospheric magnetic fields in the\nquiet Sun undergo complex and rapid evolution. These photospheric dynamics are\nexpected to be reflected in the coronal magnetic field. Motivated by these\ninsights, our research aims to explore the relationship between magnetic energy\nand coronal heating. By combining observations from Solar Orbiter and SDO with\na magnetic field extrapolation technique, we estimate the magnetic free energy\nof multi-scale energy release events in the quiet Sun. Interestingly, our\nresults reveal a strong correlation between the evolution of free energy and\nthe integrated intensity of extreme ultraviolet emission at 171 \\AA~in these\nevents. We quantitatively assess the potential energy flux budget of these\nevents to evaluate their contribution to coronal heating. Our study implies a\nlink between photospheric magnetic field evolution and coronal temperature\nvariations, paving the way for further research into similar phenomena."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Trends in Phase II Trials for Cancer Therapies",
    "start_abstract":"Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "E(n) Equivariant Graph Neural Networks"
      ],
      "abstract":[
        "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Beyond the Hype: Benchmarking LLM-Evolved Heuristics for Bin Packing",
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Estimation of total body fat using symbolic regression and evolutionary\n  algorithms",
        "QGAIC: Quantum Inspired Genetic Algorithm for Image Classification",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "Runtime Analysis of the Compact Genetic Algorithm on the LeadingOnes\n  Benchmark",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "Efficient Event-based Delay Learning in Spiking Neural Networks",
        "Cascading CMA-ES Instances for Generating Input-diverse Solution Batches",
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights\n  LLMs",
        "Contextual bandits with entropy-based human feedback",
        "Rational Functions on the Projective Line from a Computational Viewpoint",
        "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and\n  Earth Surface Analysis",
        "Solar flares as electron accelerators: toward a resolution of the\n  acceleration efficiency issue",
        "Connectivity of Coxeter group Morse boundaries",
        "Passive Heart Rate Monitoring During Smartphone Use in Everyday Life",
        "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
        "Enhancing Transformers for Generalizable First-Order Logical Entailment",
        "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm\n  Perception in Large Vision-Language Models",
        "Monotonicity results in half spaces for quasilinear elliptic equations\n  involving a singular term",
        "On Fair Ordering and Differential Privacy",
        "Comparative Analysis of Efficient Adapter-Based Fine-Tuning of\n  State-of-the-Art Transformer Models",
        "Feedback cooling of fermionic atoms in optical lattices",
        "A Constraint-Preserving Neural Network Approach for Solving Mean-Field\n  Games Equilibrium"
      ],
      "abstract":[
        "Coupling Large Language Models (LLMs) with Evolutionary Algorithms has\nrecently shown significant promise as a technique to design new heuristics that\noutperform existing methods, particularly in the field of combinatorial\noptimisation. An escalating arms race is both rapidly producing new heuristics\nand improving the efficiency of the processes evolving them. However, driven by\nthe desire to quickly demonstrate the superiority of new approaches, evaluation\nof the new heuristics produced for a specific domain is often cursory: testing\non very few datasets in which instances all belong to a specific class from the\ndomain, and on few instances per class. Taking bin-packing as an example, to\nthe best of our knowledge we conduct the first rigorous benchmarking study of\nnew LLM-generated heuristics, comparing them to well-known existing heuristics\nacross a large suite of benchmark instances using three performance metrics.\nFor each heuristic, we then evolve new instances won by the heuristic and\nperform an instance space analysis to understand where in the feature space\neach heuristic performs well. We show that most of the LLM heuristics do not\ngeneralise well when evaluated across a broad range of benchmarks in contrast\nto existing simple heuristics, and suggest that any gains from generating very\nspecialist heuristics that only work in small areas of the instance space need\nto be weighed carefully against the considerable cost of generating these\nheuristics.",
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "Body fat percentage is an increasingly popular alternative to Body Mass Index\nto measure overweight and obesity, offering a more accurate representation of\nbody composition. In this work, we evaluate three evolutionary computation\ntechniques, Grammatical Evolution, Context-Free Grammar Genetic Programming,\nand Dynamic Structured Grammatical Evolution, to derive an interpretable\nmathematical expression to estimate the percentage of body fat that are also\naccurate. Our primary objective is to obtain a model that balances accuracy\nwith explainability, making it useful for clinical and health applications. We\ncompare the performance of the three variants on a public anthropometric\ndataset and compare the results obtained with the QLattice framework.\nExperimental results show that grammatical evolution techniques can obtain\ncompetitive results in performance and interpretability.",
        "This study uses two meta-heuristics methodologies to introduce two novel\nquantum-inspired meta heuristic approaches: quantum-inspired genetic algorithm\n(QIGA1) and quantum-inspired genetic algorithm with dynamic approach (QIGA2).\nThe two suggested methods combine a classical and quantum genetic algorithm\napproach. Both approaches use The correlation coefficient as an assessment\nfunction to identify the best (optimal) values for binary image. In quantum\ncomputing, they use simple ideas like qubits and state superposition. Due to\nthese characteristics, parallelism which uses the time discreteness of quantum\nmechanical systems, is exhibited. For five distinct MNIST datasets, the\nperformance of all participating algorithms has been assessed by comparing the\nsuggested approaches first with their traditional approach counterparts and\nthen with the proposed methods QIGA1 and QIGA2. Each method's ideal threshold\nvalue, associated fitness value (best and average), loss, and accuracy for each\nMNIST dataset have all been published. The outcomes demonstrate the superior\nefficiency of the suggested approaches over their traditional equivalents.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "The compact genetic algorithm (cGA) is one of the simplest\nestimation-of-distribution algorithms (EDAs). Next to the univariate marginal\ndistribution algorithm (UMDA) -- another simple EDA -- , the cGA has been\nsubject to extensive mathematical runtime analyses, often showcasing a similar\nor even superior performance to competing approaches. Surprisingly though, up\nto date and in contrast to the UMDA and many other heuristics, we lack a\nrigorous runtime analysis of the cGA on the LeadingOnes benchmark -- one of the\nmost studied theory benchmarks in the domain of evolutionary computation.\n  We fill this gap in the literature by conducting a formal runtime analysis of\nthe cGA on LeadingOnes. For the cGA's single parameter -- called the\nhypothetical population size -- at least polylogarithmically larger than the\nproblem size, we prove that the cGA samples the optimum of LeadingOnes with\nhigh probability within a number of function evaluations quasi-linear in the\nproblem size and linear in the hypothetical population size. For the best\nhypothetical population size, our result matches, up to polylogarithmic\nfactors, the typical quadratic runtime that many randomized search heuristics\nexhibit on LeadingOnes. Our analysis exhibits some noteworthy differences in\nthe working principles of the two algorithms which were not visible in previous\nworks.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "Spiking Neural Networks (SNNs) are attracting increased attention as a more\nenergy-efficient alternative to traditional Artificial Neural Networks. Spiking\nneurons are stateful and intrinsically recurrent, making them well-suited for\nspatio-temporal tasks. However, this intrinsic memory is limited by synaptic\nand membrane time constants. A powerful additional mechanism are delays. In\nthis paper, we propose a novel event-based training method for SNNs with\ndelays, grounded in the EventProp formalism and enabling the calculation of\nexact gradients with respect to weights and delays. Our method supports\nmultiple spikes per neuron and, to our best knowledge, is the first delay\nlearning algorithm to be applied to recurrent SNNs. We evaluate our method on a\nsimple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits and\nSpiking Speech Commands datasets, demonstrating that our algorithm can optimize\ndelays from suboptimal initial conditions and enhance classification accuracy\ncompared to architectures without delays. Finally, we show that our approach\nuses less than half the memory of the current state-of-the-art delay-learning\nmethod and is up to 26x faster.",
        "Rather than obtaining a single good solution for a given optimization\nproblem, users often seek alternative design choices, because the best-found\nsolution may perform poorly with respect to additional objectives or\nconstraints that are difficult to capture into the modeling process.\n  Aiming for batches of diverse solutions of high quality is often desirable,\nas it provides flexibility to accommodate post-hoc user preferences. At the\nsame time, it is crucial that the quality of the best solution found is not\ncompromised.\n  One particular problem setting balancing high quality and diversity is fixing\nthe required minimum distance between solutions while simultaneously obtaining\nthe best possible fitness. Recent work by Santoni et al. [arXiv 2024] revealed\nthat this setting is not well addressed by state-of-the-art algorithms,\nperforming in par or worse than pure random sampling.\n  Driven by this important limitation, we propose a new approach, where\nparallel runs of the covariance matrix adaptation evolution strategy (CMA-ES)\ninherit tabu regions in a cascading fashion. We empirically demonstrate that\nour CMA-ES-Diversity Search (CMA-ES-DS) algorithm generates trajectories that\nallow to extract high-quality solution batches that respect a given minimum\ndistance requirement, clearly outperforming those obtained from off-the-shelf\nrandom sampling, multi-modal optimization algorithms, and standard CMA-ES.",
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Large Language Models (LLMs) have demonstrated great potential as evaluators\nof NLG systems, allowing for high-quality, reference-free, and multi-aspect\nassessments. However, existing LLM-based metrics suffer from two major\ndrawbacks: reliance on proprietary models to generate training data or perform\nevaluations, and a lack of fine-grained, explanatory feedback. In this paper,\nwe introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation\nmetric that provides accurate explanations based on error spans. OpeNLGauge is\navailable as a two-stage ensemble of larger open-weight LLMs, or as a small\nfine-tuned evaluation model, with confirmed generalizability to unseen tasks,\ndomains and aspects. Our extensive meta-evaluation shows that OpeNLGauge\nachieves competitive correlation with human judgments, outperforming\nstate-of-the-art models on certain tasks while maintaining full reproducibility\nand providing explanations more than twice as accurate.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "We explore the moduli space $\\mathcal{M}_d^1$ of degree $d \\geq 3$ rational\nfunctions on the projective line using a machine learning approach, focusing on\nautomorphism group classification. For $d = 3$, where $\\mathcal{M}_3^1 =\n{\\mathbb P}_{\\mathbf{w}}^5 ({\\mathbb Q})$ with weights $\\mathbf{w} = (2, 2, 3,\n3, 4, 6)$, we generate a dataset of 2,078,697 rational functions over $\\Q$ with\nnaive height $\\leq 4$. Initial coefficient-based models achieved high overall\naccuracy but struggled with minority classes due to extreme class imbalance. By\nusing invariants $\\xi_0, \\ldots, \\xi_5$ as features in a Random Forest\nclassifier, we achieved approximately 99.992\\% accuracy, mirroring successes in\ngenus 2 curves \\cite{2024-03}. This highlights the transformative role of\ninvariants in arithmetic dynamics, yet for $d > 3$, unknown generators of\n$\\mathcal{R}_{(d+1, d-1)}$ pose scalability challenges. Our framework bridges\ndata-driven and algebraic methods, with potential extensions to higher degrees\nand $\\mathcal{M}_d^2$.",
        "Surficial geologic mapping is essential for understanding Earth surface\nprocesses, addressing modern challenges such as climate change and national\nsecurity, and supporting common applications in engineering and resource\nmanagement. However, traditional mapping methods are labor-intensive, limiting\nspatial coverage and introducing potential biases. To address these\nlimitations, we introduce EarthScape, a novel, AI-ready multimodal dataset\nspecifically designed for surficial geologic mapping and Earth surface\nanalysis. EarthScape integrates high-resolution aerial RGB and near-infrared\n(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain\nfeatures, and hydrologic and infrastructure vector data. The dataset provides\ndetailed annotations for seven distinct surficial geologic classes encompassing\nvarious geological processes. We present a comprehensive data processing\npipeline using open-sourced raw data and establish baseline benchmarks using\ndifferent spatial modalities to demonstrate the utility of EarthScape. As a\nliving dataset with a vision for expansion, EarthScape bridges the gap between\ncomputer vision and Earth sciences, offering a valuable resource for advancing\nresearch in multimodal learning, geospatial analysis, and geological mapping.\nOur code is available at https:\/\/github.com\/masseygeo\/earthscape.",
        "A major open issue concerning the active Sun is the effectiveness with which\nmagnetic reconnection accelerates electrons in flares. A paper published by\n{\\em{Nature}} in 2022 used microwave observations to conclude that the Sun is\nan almost ideal accelerator, energizing nearly all electrons within a coronal\nvolume to nonthermal energies. Shortly thereafter, a paper published in\n{\\em{Astrophysical Journal Letters}} used hard X-ray measurements \\emph{of the\nsame event} to reach the contradictory conclusion that less than 1\\% of the\navailable electrons were accelerated. Here we address this controversy by using\nspatially resolved observations of hard X-ray emission and a spectral inversion\nmethod to determine the evolution of the electron spectrum throughout the\nflare. So we estimated the density of the medium where electrons accelerate\nand, from this, the ratio of accelerated to ambient electron densities. Results\nshow that this ratio never exceeds a percent or so in the cases analyzed.",
        "We study the connectivity of Morse boundaries of Coxeter groups. We define\ntwo conditions on the defining graph of a Coxeter group: wide-avoidant and\nwide-spherical-avoidant. We show that wide-spherical-avoidant, one-ended,\naffine-free Coxeter groups have connected and locally connected Morse\nboundaries. On the other hand, one-ended Coxeter groups that are not\nwide-avoidant and not wide have disconnected Morse boundary. For the\nright-angled case, we get a full characterization: a one-ended right-angled\nCoxeter group has connected, non-empty Morse boundary if and only if it is\nwide-avoidant. Along the way we characterize Morse geodesic rays in affine-free\nCoxeter groups as those that spend uniformly bounded time in cosets of wide\nspecial subgroups.",
        "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
        "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
        "Transformers, as a fundamental deep learning architecture, have demonstrated\nremarkable capabilities in reasoning. This paper investigates the generalizable\nfirst-order logical reasoning ability of transformers with their parameterized\nknowledge and explores ways to improve it. The first-order reasoning capability\nof transformers is assessed through their ability to perform first-order\nlogical entailment, which is quantitatively measured by their performance in\nanswering knowledge graph queries. We establish connections between (1) two\ntypes of distribution shifts studied in out-of-distribution generalization and\n(2) the unseen knowledge and query settings discussed in the task of knowledge\ngraph query answering, enabling a characterization of fine-grained\ngeneralizability. Results on our comprehensive dataset show that transformers\noutperform previous methods specifically designed for this task and provide\ndetailed empirical evidence on the impact of input query syntax, token\nembedding, and transformer architectures on the reasoning capability of\ntransformers. Interestingly, our findings reveal a mismatch between positional\nencoding and other design choices in transformer architectures employed in\nprior practices. This discovery motivates us to propose a more sophisticated,\nlogic-aware architecture, TEGA, to enhance the capability for generalizable\nfirst-order logical entailment in transformers.",
        "With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps:\/\/github.com\/CoderChen01\/LVLMSarcasmAnalysis",
        "We consider positive solutions to $\\displaystyle -\\Delta_p\nu=\\frac{1}{u^\\gamma}+f(u)$ under zero Dirichlet condition in the half space.\nExploiting a prio-ri estimates and the moving plane technique, we prove that\nany solution is monotone increasing in the direction orthogonal to the\nboundary.",
        "In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.",
        "In this work, we investigate the efficacy of various adapter architectures on\nsupervised binary classification tasks from the SuperGLUE benchmark as well as\na supervised multi-class news category classification task from Kaggle.\nSpecifically, we compare classification performance and time complexity of\nthree transformer models, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art (SoTA) adapter\narchitectures. Our analysis reveals performance differences across adapter\narchitectures, highlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the training time. Similar\nresults are observed on the new classification task, further supporting our\nfindings and demonstrating adapters as efficient and flexible alternatives to\nfine-tuning. This study provides valuable insights and guidelines for selecting\nand implementing adapters in diverse natural language processing (NLP)\napplications.",
        "We discuss the preparation of topological insulator states with fermionic\nultracold atoms in optical lattices by means of measurement-based Markovian\nfeedback control. The designed measurement and feedback operators induce an\neffective dissipative channel that stabilizes the desired insulator state,\neither in an exact way or approximately in the case where additional\nexperimental constraints are assumed. Successful state preparation is\ndemonstrated in one-dimensional insulators as well as for Haldane's Chern\ninsulator, by calculating the fidelity between the target ground state and the\nsteady state of the feedback-modified master equation. The fidelity is obtained\nvia time evolution of the system with moderate sizes. For larger 2D systems, we\ncompare the mean occupation of the single-particle eigenstates for the ground\nand steady state computed through mean-field kinetic equations.",
        "Neural network-based methods have demonstrated effectiveness in solving\nhigh-dimensional Mean-Field Games (MFG) equilibria, yet ensuring mathematically\nconsistent density-coupled evolution remains a major challenge. This paper\nproposes the NF-MKV Net, a neural network approach that integrates\nprocess-regularized normalizing flow (NF) with state-policy-connected\ntime-series neural networks to solve MKV FBSDEs and their associated\nfixed-point formulations of MFG equilibria. The method first reformulates MFG\nequilibria as MKV FBSDEs, embedding density evolution into equation\ncoefficients within a probabilistic framework. Neural networks are then\nemployed to approximate value functions and their gradients. To enforce\nvolumetric invariance and temporal continuity, NF architectures impose loss\nconstraints on each density transfer function."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"E(n) Equivariant Graph Neural Networks",
    "start_abstract":"This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Trends in Phase II Trials for Cancer Therapies"
      ],
      "abstract":[
        "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Stability of difference equations with interspecific density dependence,\n  competition, and maturation delays",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "MorphoITH: A Framework for Deconvolving Intra-Tumor Heterogeneity Using\n  Tissue Morphology",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "R3F: An R package for evolutionary dates, rates, and priors using the\n  relative rate framework",
        "Multicellular self-organization in Escherichia coli",
        "Characterizing the Conformational States of G Protein Coupled Receptors\n  Generated with AlphaFold",
        "The time-dependent reproduction number for epidemics in heterogeneous\n  populations",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Mechanoreceptive A$\\beta$ primary afferents discriminate naturalistic\n  social touch inputs at a functionally relevant time scale",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "On the eternal non-Markovianity of qubit maps",
        "Micro Blossom: Accelerated Minimum-Weight Perfect Matching Decoding for\n  Quantum Error Correction",
        "Search for continuous gravitational wave signals from luminous dark\n  photon superradiance clouds with LVK O3 observations",
        "Asymptotics for multiple $q$-orthogonal polynomials from the RHP",
        "Scalable First-order Method for Certifying Optimal k-Sparse GLMs",
        "Towards Transparent and Accurate Plasma State Monitoring at JET",
        "Quark number susceptibility and conserved charge fluctuation for\n  (2+1)-flavor QCD with M\\\"obius domain wall fermions",
        "Galaxy-cluster-stacked Fermi-LAT III: substructure and radio-relic\n  counterparts",
        "Erosion of a dense molecular core by a strong outflow from a massive\n  protostar",
        "Reporting on pTP sublimation during evaporation deposition",
        "Parameter Invariance Analysis of Moment Equations Using\n  Dulmage-Mendelsohn Decomposition",
        "Singularity-Based Consistent QML Estimation of Multiple Breakpoints in\n  High-Dimensional Factor Models",
        "Spectral Analysis and Invariant Measure in Studies of the Dynamics of\n  the Hemostasis of a Blood Vessel",
        "The Evolution of Hypervelocity Supernova Survivors and the Outcomes of\n  Interacting Double White Dwarf Binaries",
        "$K$-theoretic pullbacks for Lagrangians on derived critical loci"
      ],
      "abstract":[
        "A general system of difference equations is presented for multispecies\ncommunities with density dependent population growth and delayed maturity.\nInterspecific competition, mutualism, predation, commensalism, and amensalism\nare accommodated. A sufficient condition for the local asymptotic stability of\na coexistence equilibrium in this system is then proven. Using this system, the\ngeneralisation of the Beverton-Holt and Leslie-Gower models of competition to\nmultispecies systems with possible maturation delays is presented and shown to\nyield interesting stability properties. The stability of coexistence depends on\nthe relative abundances of the species at the unique interior equilibrium. A\nsufficient condition for local stability is derived that only requires\nintraspecific competition to outweigh interspecific competition. The condition\ndoes not depend on maturation delays. The derived stability properties are used\nto develop a novel estimation approach for the coefficients of interspecific\ncompetition. This approach finds an optimal configuration given the conjecture\nthat coexisting species strive to outcompete competitors and are most likely\nobserved near the stable interior equilibrium with the greatest dampening of\nperturbations. The optimal solution is compared to estimates of niche overlap\nusing an empirical example of malaria mosquito vectors with delayed maturity in\nthe Anopheles gambiae sensu lato species complex.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "The ability of tumors to evolve and adapt by developing subclones in\ndifferent genetic and epigenetic states is a major challenge in oncology.\nTraditional tools like multi-regional sequencing used to study tumor evolution\nand the resultant intra-tumor heterogeneity (ITH) are often impractical because\nof their resource-intensiveness and limited scalability. Here, we present\nMorphoITH, a novel framework that leverages histopathology slides to deconvolve\nmolecular ITH through tissue morphology. MorphoITH integrates a self-supervised\ndeep learning similarity measure to capture phenotypic variation across\nmultiple dimensions (cytology, architecture, and microenvironment) with\nrigorous methods to eliminate spurious sources of variation. Using a prototype\nof ITH, clear cell renal cell carcinoma (ccRCC), we show that MorphoITH\ncaptures clinically-significant biological features, such as vascular\narchitecture and nuclear grades. Furthermore, we find that MorphoITH recognizes\ndifferential biological states corresponding to subclonal changes in key driver\ngenes (BAP1\/PBRM1\/SETD2). Finally, by applying MorphoITH to a multi-regional\nsequencing experiment, we postulate evolutionary trajectories that largely\nrecapitulate genetic evolution. In summary, MorphoITH provides a scalable\nphenotypic lens that bridges the gap between histopathology and genomics,\nadvancing precision oncology.",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "The relative rate framework (RRF) can estimate divergence times from branch\nlengths in a phylogeny, which is the theoretical basis of the RelTime method\nfrequently applied, a relaxed clock approach for molecular dating that scales\nwell for large phylogenies. The use of RRF has also enabled the development of\ncomputationally efficient and accurate methods for testing the autocorrelation\nof lineage rates in a phylogeny (CorrTest) and selecting data-driven parameters\nof the birth-death speciation model (ddBD), which can be used to specify priors\nin Bayesian molecular dating. We have developed R3F, an R package implementing\nRRF to estimate divergence times, infer lineage rates, conduct CorrTest, and\nbuild a ddBD tree prior for Bayesian dating in molecular phylogenies. Here, we\ndescribe R3F functionality and explain how to interpret and use its outputs in\nother visualization software and packages, such as MEGA, ggtree, and FigTree.\nUltimately, R3F is intended to enable the dating of the Tree of Life with\ngreater accuracy and precision, which would have important implications for\nstudies of organism evolution, diversification dynamics, phylogeography, and\nbiogeography. Availability and Implementation: The source codes and related\ninstructions for installing and implementing R3F are available from GitHub\n(https:\/\/github.com\/cathyqqtao\/R3F).",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "G-Protein Coupled Receptors (GPCRs) are integral to numerous physiological\nprocesses and are the target of approximately one-third of FDA-approved\ntherapeutics. Despite their significance, only a limited subset of GPCRs has\nbeen successfully targeted, primarily due to challenges in accurately modeling\ntheir structures. AlphaFold, a state-of-the-art deep learning model, has\ndemonstrated remarkable capability in predicting protein structures with high\naccuracy. This study conducts an evaluation of AlphaFold performance in\npredicting GPCR structures and their conformational states by comparing its\npredictions to experimentally determined structures using metrics such as\naverage deformation between alpha carbon atoms and the Helix 3 - Helix 6\n(H3-H6) distance. Our analysis reveals that both AlphaFold 2 (AF2) and\nAlphaFold 3 (AF3) produce more accurate predictions for GPCRs in inactive\nconformations, with lower activity levels correlating with smaller\ndeformations. Conversely, higher activity levels are associated with increased\nvariability in AlphaFold performance due to difficulties with accurately\npredicting conformational changes upon GPCR activation and ligand binding.\nAdditionally, AlphaFold performance varies across different GPCR classes,\ninfluenced by the availability and quality of training data as well as the\nstructural complexity and diversity of the receptors. These findings\ndemonstrate the potential of AlphaFold in advancing drug discovery efforts,\nwhile also highlighting the necessity for continued refinement to enhance\npredictive accuracy for active conformations.",
        "The time-dependent reproduction number Rt can be used to track pathogen\ntransmission and to assess the efficacy of interventions. This quantity can be\nestimated by fitting renewal equation models to time series of infectious\ndisease case counts. These models almost invariably assume a homogeneous\npopulation. Individuals are assumed not to differ systematically in the rates\nat which they come into contact with others. It is also assumed that the\ntypical time that elapses between one case and those it causes (known as the\ngeneration time distribution) does not differ across groups. But contact\npatterns are known to widely differ by age and according to other demographic\ngroupings, and infection risk and transmission rates have been shown to vary\nacross groups for a range of directly transmitted diseases. Here, we derive\nfrom first principles a renewal equation framework which accounts for these\ndifferences in transmission across groups. We use a generalisation of the\nclassic McKendrick-von Foerster equation to handle populations structured into\ninteracting groups. This system of partial differential equations allows us to\nderive a simple analytical expression for Rt which involves only group-level\ncontact patterns and infection risks. We show that the same expression emerges\nfrom both deterministic and stochastic discrete-time versions of the model and\ndemonstrate via simulations that our Rt expression governs the long-run fate of\nepidemics. Our renewal equation model provides a basis from which to account\nfor more realistic, diverse populations in epidemiological models and opens the\ndoor to inferential approaches which use known group characteristics to\nestimate Rt.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Interpersonal touch is an important channel of social emotional interaction.\nHow these physical skin-to-skin touch expressions are processed in the\nperipheral nervous system is not well understood. From microneurography\nrecordings in humans, we evaluated the capacity of six subtypes of cutaneous\nmechanoreceptive afferents to differentiate human-delivered social touch\nexpressions. Leveraging statistical and classification analyses, we found that\nsingle units of multiple mechanoreceptive A$\\beta$ subtypes, especially slowly\nadapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can\nreliably differentiate social touch expressions at accuracies similar to human\nrecognition. We then identified the most informative firing patterns of SA-II\nand HFA afferents, which indicate that average durations of 3-4 s of firing\nprovide sufficient discriminative information. Those two subtypes also exhibit\nrobust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch\nexpressions due to their specific firing properties. Greater shifts in\nspike-timing, however, can change a firing pattern's envelope to resemble that\nof another expression and drastically compromise an afferent's discrimination\ncapacity. Altogether, the findings indicate that SA-II and HFA afferents\ndifferentiate the skin contact of social touch at time scales relevant for such\ninteractions, which are 1-2 orders of magnitude longer than those for\nnon-social touch.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "As is well known, unital Pauli maps can be eternally non-CP-divisible. In\ncontrast, here we show that in the case of non-unital maps, eternal\nnon-Markovianity in the non-unital part is ruled out. In the unital case, the\neternal non-Markovianity can be obtained by a convex combination of two\ndephasing semigroups, but not all three of them. We study these results and the\nramifications arising from them.",
        "Minimum-Weight Perfect Matching (MWPM) decoding is important to quantum error\ncorrection decoding because of its accuracy. However, many believe that it is\ndifficult, if possible at all, to achieve the microsecond latency requirement\nposed by superconducting qubits. This work presents the first publicly known\nMWPM decoder, called Micro Blossom, that achieves sub-microsecond decoding\nlatency. Micro Blossom employs a heterogeneous architecture that carefully\npartitions a state-of-the-art MWPM decoder between software and a programmable\naccelerator with parallel processing units, one of each vertex\/edge of the\ndecoding graph. On a surface code with code distance $d$ and a circuit-level\nnoise model with physical error rate $p$, Micro Blossom's accelerator employs\n$O(d^3)$ parallel processing units to reduce the worst-case latency from\n$O(d^{12})$ to $O(d^9)$ and reduce the average latency from $O(p d^3+1)$ to\n$O(p^2 d^2+1)$ when $p \\ll 1$.\n  We report a prototype implementation of Micro Blossom using FPGA. Measured at\n$d=13$ and $p=0.1\\%$, the prototype achieves an average decoding latency of\n$0.8 \\mu s$ at a moderate clock frequency of $62 MHz$. Micro Blossom is the\nfirst publicly known hardware-accelerated exact MWPM decoder, and the decoding\nlatency of $0.8 \\mu s$ is 8 times shorter than the best latency of MWPM decoder\nimplementations reported in the literature.",
        "Superradiance clouds of kinetically-mixed dark photons around spinning black\nholes can produce observable multi-messenger electromagnetic and gravitational\nwave signals. The cloud generates electric fields of up to a\nTeravolt-per-meter, which lead to a cascade production of charged particles,\nyielding a turbulent quasi-equilibrium plasma around the black hole, and\nresulting in electromagnetic fluxes ranging from supernova to pulsar-like\nluminosities. For stellar mass black holes, such systems resemble millisecond\npulsars and are expected to emit pulsating radio waves and continuous\ngravitational waves (CWs) within the LIGO-Virgo-KAGRA (LVK) sensitivity band.\nWe select 44 sources with approximately coincident frequencies or positive\nfrequency drifts from existing pulsar catalogs as potential candidates of\nlong-lasting superradiance clouds around old galactic black holes. For a subset\nof 34 sources that are well measured and have not been previously targeted, we\nperform the first search for CW emission in LVK data from the third observing\nrun. We find no evidence of a CW signal and place 95% confidence level upper\nlimits on the emitted strain amplitude. We interpret these results, together\nwith limits from previous searches, in terms of the underlying dark photon\ntheory by performing an analysis of the expected signals from superradiance\nclouds from galactic black holes. We find that, even for moderately spinning\nblack holes, the absence of an observed CW signal disfavors a discrete set of\ndark photon masses between about $10^{-13}$ $\\rm{eV}\/c^2$ and $10^{-12}$\n$\\rm{eV}\/c^2$ and kinetic mixing couplings in the range of $10^{-9}$-$10^{-7}$,\nsubject to assumptions about the properties of the black hole population and\nthe cloud's electromagnetic emission.",
        "We deduce the asymptotic behaviour of a broad class of multiple\n$q$-orthogonal polynomials as their degree tends to infinity. We achieve this\nby rephrasing multiple $q$-orthogonal polynomials as part of a solution to a\nRiemann Hilbert Problem (RHP). In particular, we study multiple $q$-orthogonal\npolynomials of the first kind (see [12]), which are Type II orthogonal\npolynomials with weights given by\n  \\begin{equation}\n  w_1(x) = x^\\alpha \\omega(x)d_qx,\\qquad w_2(x) = x^\\beta \\omega(x)d_qx,\n\\nonumber \\end{equation} which satisfy the constraint \\begin{equation}\\nonumber\n  |\\omega(q^{2n})-1| = \\mathcal{O}(q^{2n}), \\end{equation} as $n\\to \\infty$.\nUsing $q$-calculus we obtain detailed asymptotics for these polynomials from\nthe RHP. This class of polynomials studied was chosen in part to their\nconnection to the work of [11,12], concerning the irrationality of $\\zeta_q(1)$\nand $\\zeta_q(2)$. To conduct our asymptotic analysis we will require the\nfollowing added restrictions on $w_1(x)$ and $w_2(x)$: $\\alpha \\notin\n\\mathbb{Z}$, $\\beta \\notin \\mathbb{Z}$ and $\\alpha \\neq \\beta \\mod \\mathbb{Z}$.\nThese restrictions are necessary for the asymptotic analysis but not the\nstatement of multiple $q$-orthogonal polynomials as solutions to a RHP.\n  The author wishes to extend special thanks to Prof. Walter Van Assche, who\nmotivated this studied and provided valuable discussion.",
        "This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.",
        "Controlling and monitoring plasma within a tokamak device is complex and\nchallenging. Plasma off-normal events, such as disruptions, are hindering\nsteady-state operation. For large devices, they can even endanger the machine's\nintegrity and it represents in general one of the most serious concerns for the\nexploitation of the tokamak concept for future power plants. Effective plasma\nstate monitoring carries the potential to enable an understanding of such\nphenomena and their evolution which is crucial for the successful operation of\ntokamaks. This paper presents the application of a transparent and data-driven\nmethodology to monitor the plasma state in a tokamak. Compared to previous\nstudies in the field, supervised and unsupervised learning techniques are\ncombined. The dataset consisted of 520 expert-validated discharges from JET.\nThe goal was to provide an interpretable plasma state representation for the\nJET operational space by leveraging multi-task learning for the first time in\nthe context of plasma state monitoring. When evaluated as disruption\npredictors, a sequence-based approach showed significant improvements compared\nto the state-based models. The best resulting network achieved a promising\ncross-validated success rate when combined with a physical indicator and\naccounting for nearby instabilities. Qualitative evaluations of the learned\nlatent space uncovered operational and disruptive regions as well as patterns\nrelated to learned dynamics and global feature importance. The applied\nmethodology provides novel possibilities for the definition of triggers to\nswitch between different control scenarios, data analysis, and learning as well\nas exploring latent dynamics for plasma state monitoring. It also showed\npromising quantitative and qualitative results with warning times suitable for\navoidance purposes and distributions that are consistent with known physical\nmechanisms.",
        "We present quark number susceptibilities and conserved charge fluctuations\nfor (2+1)-flavor QCD using M\\\"obius Domain Wall fermions with a pion mass of\n\\(135~\\rm{MeV}\\). Our results are compared with hadron resonance gas models\nbelow the QCD transition temperature and with \\(\\mathcal{O}(g^2)\\) perturbation\ntheory at high temperatures. Additionally, we compare our findings with results\nfrom staggered fermion discretizations. Furthermore, we also present results of\nleading order Kurtosis of electric charge and strangeness fluctuations.",
        "Faint $\\gamma$-ray signatures emerge in Fermi-LAT data stacked scaled to the\ncharacteristic $R_{500}$ radii of MCXC galaxy clusters. This third paper in a\nseries shows a $4.3\\sigma$ excess of discrete 4FGL-DR4 catalog $\\gamma$-ray\nsources at the $r<1.5R_{500}$ radii of 205 clusters, coincident with an $r\\sim\nR_{500}$ diffuse $2.6\\sigma$ excess of 1-100 GeV emission from 75 high-latitude\nclusters. The source excess becomes highly ($>5\\sigma$) significant when\nconsidering the substantial ($3.4\\sigma$) and unexpectedly rapid quenching of\n$\\gamma$-ray sources just inside the virial shock. The excess sources show\nradial, spectral, and luminosity distributions better matching radio-relic\ncounterparts or substructure than present tentative classifications as\nblazar-candidates. Their spectral distribution is bimodal: flat-spectrum\nsources are consistent with enhanced hadronic emission behind weak, Mach\n$\\sim2$ shocks, while softer sources may be phoenix counterparts.",
        "We present Atacama Large Millimeter\/submillimeter Array Band 3 observations\nof N$_2$H$^+$ (1-0) and CH$_3$CN (5-4), as well as Band 7 observations of the\nH$_2$CO molecular line emissions from the protostellar system GGD 27-MM2(E).\nThrough position-velocity diagrams along and across the outflow axis, we study\nthe kinematics and structure of the outflow. We also fit extracted spectra of\nthe CH$_3$CN emission to obtain the physical conditions of the gas. We use the\nresults to discuss the impact of the outflow on its surroundings. We find that\nN$_2$H$^+$ emission traces a dense molecular cloud surrounding GGD 27-MM2(E).\nWe estimate that the mass of this cloud is $\\sim$13.3-26.5 M$_\\odot$. The\nmolecular cloud contains an internal cavity aligned with the H$_2$CO-traced\nmolecular outflow. The outflow, also traced by $\\mathrm{CH_3 CN}$, shows\nevidence of a collision with a molecular core (MC), as indicated by the\ndistinctive increases in the distinct physical properties of the gas such as\nexcitation temperature, column density, line width, and velocity. This\ncollision results in an X-shape structure in the northern part of the outflow\naround the position of the MC, which produces spray-shocked material downstream\nin the north of MC as observed in position-velocity diagrams both along and\nacross of the outflow axis. The outflow has a mass of 1.7-2.1 M$_\\odot$, a\nmomentum of 7.8-10.1 M$_\\odot$ km s$^{-1}$, a kinetic energy of 5.0-6.6$\\times\n10^{44}$ erg, and a mass loss rate of 4.9--6.0$\\times10^{-4}$ M$_\\odot$\nyr$^{-1}$. The molecular outflow from GGD 27-MM2(E) significantly perturbs and\nerodes its parent cloud, compressing the gas of sources such as MC and ALMA 12.\nThe feedback from this powerful protostellar outflow contributes to maintain\nthe turbulence in the surrounding area.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Living organisms maintain stable functioning amid environmental fluctuations\nthrough homeostasis, a mechanism that preserves a system's behavior despite\nchanges in environmental conditions. To elucidate homeostasis in stochastic\nbiochemical reactions, theoretical tools for assessing population-level\ninvariance under parameter perturbations are crucial. In this paper, we propose\na systematic method for identifying the stationary moments that remain\ninvariant under parameter perturbations by leveraging the structural properties\nof the stationary moment equations. A key step in this development is\naddressing the underdetermined nature of moment equations, which has\ntraditionally made it difficult to characterize how stationary moments depend\non system parameters. To overcome this, we utilize the Dulmage-Mendelsohn (DM)\ndecomposition of the coefficient matrix to extract welldetermined subequations\nand reveal their hierarchical structure. Leveraging this structure, we identify\nstationary moments whose partial derivatives with respect to parameters are\nstructurally zero, facilitating the exploration of fundamental constraints that\ngovern homeostatic behavior in stochastic biochemical systems.",
        "This paper investigates the estimation of high-dimensional factor models in\nwhich factor loadings undergo an unknown number of structural changes over\ntime. Given that a model with multiple changes in factor loadings can be\nobservationally indistinguishable from one with constant loadings but varying\nfactor variances, this reduces the high-dimensional structural change problem\nto a lower-dimensional one. Due to the presence of multiple breakpoints, the\nfactor space may expand, potentially causing the pseudo factor covariance\nmatrix within some regimes to be singular. We define two types of breakpoints:\n{\\bf a singular change}, where the number of factors in the combined regime\nexceeds the minimum number of factors in the two separate regimes, and {\\bf a\nrotational change}, where the number of factors in the combined regime equals\nthat in each separate regime. Under a singular change, we derive the properties\nof the small eigenvalues and establish the consistency of the QML estimators.\nUnder a rotational change, unlike in the single-breakpoint case, the pseudo\nfactor covariance matrix within each regime can be either full rank or\nsingular, yet the QML estimation error for the breakpoints remains stably\nbounded. We further propose an information criterion (IC) to estimate the\nnumber of breakpoints and show that, with probability approaching one, it\naccurately identifies the true number of structural changes. Monte Carlo\nsimulations confirm strong finite-sample performance. Finally, we apply our\nmethod to the FRED-MD dataset, identifying five structural breaks in factor\nloadings between 1959 and 2024.",
        "A mathematical model of atherosclerosis of a blood vessel is advanced with\nregard for the entry of low-density lipoproteins (LDLs) into blood. For the\nfirst time, the influence of cytokines on the inflammation of a blood vessel at\nthe formation of atherosclerotic plaques is taken into account. With the help\nof the expansion in a Fourier series and the calculation of an invariant\nmeasure, the scenario of the appearance of strange attractors depending on a\nchange in the parameter of the dissipation of cholesterol is studied. The\nconclusion is made about the interconnection of the dynamics of the metabolic\nprocess in a blood vascular system and its physical state.",
        "The recent prediction and discovery of hypervelocity supernova survivors has\nprovided strong evidence that the \"dynamically driven double-degenerate\ndouble-detonation\" (D6) Type Ia supernova scenario occurs in Nature. In this\nmodel, the accretion stream from the secondary white dwarf in a double white\ndwarf binary strikes the primary white dwarf violently enough to trigger a\nhelium shell detonation, which in turn triggers a carbon\/oxygen core\ndetonation. If the secondary white dwarf survives the primary's explosion, it\nwill be flung away as a hypervelocity star. While previous work has shown that\nthe hotter observed D6 stars can be broadly understood as secondaries whose\nouter layers have been heated by their primaries' explosions, the properties of\nthe cooler D6 stars have proven difficult to reproduce. In this paper, we show\nthat the cool D6 stars can be explained by the Kelvin-Helmholtz contraction of\nhelium or carbon\/oxygen white dwarfs that underwent significant mass loss and\ncore heating prior to and during the explosion of their white dwarf companions.\nWe find that the current population of known D6 candidates is consistent with\n~2% of Type Ia supernovae leaving behind a hypervelocity surviving companion.\nWe also calculate the evolution of hot, low-mass oxygen\/neon stars and find\nreasonable agreement with the properties of the LP 40-365 class of\nhypervelocity survivors, suggesting that these stars are the kicked remnants of\nnear-Chandrasekhar-mass oxygen\/neon white dwarfs that were partially disrupted\nby oxygen deflagrations. We use these results as motivation for schematic\ndiagrams showing speculative outcomes of interacting double white dwarf\nbinaries, including long-lived merger remnants, Type Ia supernovae, and several\nkinds of peculiar transients.",
        "Given a regular function $\\phi$ on a smooth stack, and a $(-1)$-shifted\nLagrangian $M$ on the derived critical locus of $\\phi$, under fairly general\nhypotheses, we construct a pullback map from the Grothendieck group of coherent\nmatrix factorizations of $\\phi$ to that of coherent sheaves on $M$. This map\nsatisfies a functoriality property with respect to the composition of\nLagrangian correspondences, as well as the usual bivariance and base-change\nproperties.\n  We provide three applications of the construction, one in the definition of\nquantum $K$-theory of critical loci (Landau-Ginzburg models), paving the way to\ngeneralize works of Okounkov school from Nakajima quiver varieties to quivers\nwith potentials, one in establishing a degeneration formula for $K$-theoretic\nDonaldson-Thomas theory of local Calabi-Yau 4-folds, the other in confirming a\n$K$-theoretic version of Joyce-Safronov conjecture."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Human-Centric Interfaces for Ambient Intelligence",
    "start_abstract":"To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence.",
    "start_categories":[
      "physics.app-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Speaker Diarization with LSTM"
      ],
      "abstract":[
        "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate\n  personalized feedback",
        "From Screens to Scenes: A Survey of Embodied AI in Healthcare",
        "Economic Rationality under Specialization: Evidence of Decision Bias in\n  AI Agents",
        "Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through\n  Multi-Agent Reinforcement Learning",
        "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment",
        "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of\n  Role-Playing Language Agents",
        "A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large\n  Language Models Using Cross-Attention Signals",
        "Narrative-Driven Travel Planning: Geoculturally-Grounded Script\n  Generation with Evolutionary Itinerary Optimization",
        "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language\n  Models for Navigation Applications",
        "Representation Learning to Advance Multi-institutional Studies with\n  Electronic Health Record Data",
        "DeclareAligner: A Leap Towards Efficient Optimal Alignments for\n  Declarative Process Model Conformance Checking",
        "AlgoPilot: Fully Autonomous Program Synthesis Without Human-Written\n  Programs",
        "Multi-modal Speech Enhancement with Limited Electromyography Channels",
        "Spline Quantile Regression",
        "Custom Loss Functions in Fuel Moisture Modeling",
        "Interior control for surfaces with positive scalar curvature and its\n  application",
        "Estimates for short character sums evaluated at homogeneous polynomials",
        "An Explainable Pipeline for Machine Learning with Functional Data",
        "Mamba-Shedder: Post-Transformer Compression for Efficient Selective\n  Structured State Space Models",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Data-driven geometric parameter optimization for PD-GMRES",
        "Diagrammatic Categories which arise from Representation Graphs",
        "Detecting Heel Strike and toe off Events Using Kinematic Methods and\n  LSTM Models",
        "Approximation properties of neural ODEs",
        "Objective Metrics for Human-Subjects Evaluation in Explainable\n  Reinforcement Learning",
        "Unraveling Pedestrian Fatality Patterns: A Comparative Study with\n  Explainable AI",
        "Duoidal R-Matrices"
      ],
      "abstract":[
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Grading student assignments in STEM courses is a laborious and repetitive\ntask for tutors, often requiring a week to assess an entire class. For\nstudents, this delay of feedback prevents iterating on incorrect solutions,\nhampers learning, and increases stress when exercise scores determine admission\nto the final exam. Recent advances in AI-assisted education, such as automated\ngrading and tutoring systems, aim to address these challenges by providing\nimmediate feedback and reducing grading workload. However, existing solutions\noften fall short due to privacy concerns, reliance on proprietary closed-source\nmodels, lack of support for combining Markdown, LaTeX and Python code, or\nexcluding course tutors from the grading process. To overcome these\nlimitations, we introduce PyEvalAI, an AI-assisted evaluation system, which\nautomatically scores Jupyter notebooks using a combination of unit tests and a\nlocally hosted language model to preserve privacy. Our approach is free,\nopen-source, and ensures tutors maintain full control over the grading process.\nA case study demonstrates its effectiveness in improving feedback speed and\ngrading efficiency for exercises in a university-level course on numerics.",
        "Healthcare systems worldwide face persistent challenges in efficiency,\naccessibility, and personalization. Powered by modern AI technologies such as\nmultimodal large language models and world models, Embodied AI (EmAI)\nrepresents a transformative frontier, offering enhanced autonomy and the\nability to interact with the physical world to address these challenges. As an\ninterdisciplinary and rapidly evolving research domain, \"EmAI in healthcare\"\nspans diverse fields such as algorithms, robotics, and biomedicine. This\ncomplexity underscores the importance of timely reviews and analyses to track\nadvancements, address challenges, and foster cross-disciplinary collaboration.\nIn this paper, we provide a comprehensive overview of the \"brain\" of EmAI for\nhealthcare, wherein we introduce foundational AI algorithms for perception,\nactuation, planning, and memory, and focus on presenting the healthcare\napplications spanning clinical interventions, daily care & companionship,\ninfrastructure support, and biomedical research. Despite its promise, the\ndevelopment of EmAI for healthcare is hindered by critical challenges such as\nsafety concerns, gaps between simulation platforms and real-world applications,\nthe absence of standardized benchmarks, and uneven progress across\ninterdisciplinary domains. We discuss the technical barriers and explore\nethical considerations, offering a forward-looking perspective on the future of\nEmAI in healthcare. A hierarchical framework of intelligent levels for EmAI\nsystems is also introduced to guide further development. By providing\nsystematic insights, this work aims to inspire innovation and practical\napplications, paving the way for a new era of intelligent, patient-centered\nhealthcare.",
        "In the study by Chen et al. (2023) [01], the large language model GPT\ndemonstrated economic rationality comparable to or exceeding the average human\nlevel in tasks such as budget allocation and risk preference. Building on this\nfinding, this paper further incorporates specialized agents, such as\nbiotechnology experts and economists, for a horizontal comparison to explore\nwhether specialization can enhance or maintain economic rationality equivalent\nto that of GPT in similar decision-making scenarios. The results indicate that\nwhen agents invest more effort in specialized fields, their decision-making\nbehavior is more prone to 'rationality shift,' specifically manifested as\nincreased violations of GARP (Generalized Axiom of Revealed Preference),\ndecreased CCEI (Critical Cost Efficiency Index), and more significant decision\ndeviations under high-risk conditions. In contrast, GPT and more generalized\nbasic agents maintain a more stable and consistent level of rationality across\nmultiple tasks. This study reveals the inherent conflict between specialization\nand economic rationality, providing new insights for constructing AI\ndecision-making systems that balance specialization and generalization across\nvarious scenarios.",
        "The effective design of patrol strategies is a difficult and complex problem,\nespecially in medium and large areas. The objective is to plan, in a\ncoordinated manner, the optimal routes for a set of patrols in a given area, in\norder to achieve maximum coverage of the area, while also trying to minimize\nthe number of patrols. In this paper, we propose a multi-agent reinforcement\nlearning (MARL) model, based on a decentralized partially observable Markov\ndecision process, to plan unpredictable patrol routes within an urban\nenvironment represented as an undirected graph. The model attempts to maximize\na target function that characterizes the environment within a given time frame.\nOur model has been tested to optimize police patrol routes in three\nmedium-sized districts of the city of Malaga. The aim was to maximize\nsurveillance coverage of the most crime-prone areas, based on actual crime data\nin the city. To address this problem, several MARL algorithms have been\nstudied, and among these the Value Decomposition Proximal Policy Optimization\n(VDPPO) algorithm exhibited the best performance. We also introduce a novel\nmetric, the coverage index, for the evaluation of the coverage performance of\nthe routes generated by our model. This metric is inspired by the predictive\naccuracy index (PAI), which is commonly used in criminology to detect hotspots.\nUsing this metric, we have evaluated the model under various scenarios in which\nthe number of agents (or patrols), their starting positions, and the level of\ninformation they can observe in the environment have been modified. Results\nshow that the coordinated routes generated by our model achieve a coverage of\nmore than $90\\%$ of the $3\\%$ of graph nodes with the highest crime incidence,\nand $65\\%$ for $20\\%$ of these nodes; $3\\%$ and $20\\%$ represent the coverage\nstandards for police resource allocation.",
        "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
        "Recent advances in LLM-based role-playing language agents (RPLAs) have\nattracted broad attention in various applications. While chain-of-thought\nreasoning has shown importance in many tasks for LLMs, the internal thinking\nprocesses of RPLAs remain unexplored. Understanding characters' inner thoughts\nis crucial for developing advanced RPLAs. In this paper, we introduce\nROLETHINK, a novel benchmark constructed from literature for evaluating\ncharacter thought generation. We propose the task of inner thought reasoning,\nwhich includes two sets: the gold set that compares generated thoughts with\noriginal character monologues, and the silver set that uses expert synthesized\ncharacter analyses as references. To address this challenge, we propose MIRROR,\na chain-of-thought approach that generates character thoughts by retrieving\nmemories, predicting character reactions, and synthesizing motivations. Through\nextensive experiments, we demonstrate the importance of inner thought reasoning\nfor RPLAs, and MIRROR consistently outperforms existing methods. Resources are\navailable at https:\/\/github.com\/airaer1998\/RPA_Thought.",
        "We propose a novel reinforcement learning framework for post training large\nlanguage models that does not rely on human in the loop feedback. Instead, our\napproach uses cross attention signals within the model itself to derive a self\nsupervised reward, thereby guiding iterative fine tuning of the model policy.\nBy analyzing how the model attends to the input prompt during generation, we\nconstruct measures of prompt coverage, focus, and coherence. We then use these\nmeasures to rank or score candidate responses, providing a reward signal that\nencourages the model to produce well aligned, on topic text. In empirical\ncomparisons against standard policy gradient methods and RL fine tuning with\nsynthetic preference models, our method shows significant gains in prompt\nrelevance and consistency over a non RL baseline. While it does not yet match\nthe performance of fully human supervised RLHF systems, it highlights an\nimportant direction for scaling alignment with minimal human labeling. We\nprovide a detailed analysis, discuss potential limitations, and outline future\nwork for combining cross-attention based signals with smaller amounts of human\nfeedback.",
        "To enhance tourists' experiences and immersion, this paper proposes a\nnarrative-driven travel planning framework called NarrativeGuide, which\ngenerates a geoculturally-grounded narrative script for travelers, offering a\nnovel, role-playing experience for their journey. In the initial stage,\nNarrativeGuide constructs a knowledge graph for attractions within a city, then\nconfigures the worldview, character setting, and exposition based on the\nknowledge graph. Using this foundation, the knowledge graph is combined to\ngenerate an independent scene unit for each attraction. During the itinerary\nplanning stage, NarrativeGuide models narrative-driven travel planning as an\noptimization problem, utilizing a genetic algorithm (GA) to refine the\nitinerary. Before evaluating the candidate itinerary, transition scripts are\ngenerated for each pair of adjacent attractions, which, along with the scene\nunits, form a complete script. The weighted sum of script coherence, travel\ntime, and attraction scores is then used as the fitness value to update the\ncandidate solution set. Experimental results across four cities, i.e., Nanjing\nand Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate\nsignificant improvements in narrative coherence and cultural fit, alongside a\nnotable reduction in travel time and an increase in the quality of visited\nattractions. Our study highlights that incorporating external evolutionary\noptimization effectively addresses the limitations of large language models in\ntravel planning.",
        "Existing navigation decision support systems often perform poorly when\nhandling non-predefined navigation scenarios. Leveraging the generalization\ncapabilities of large language model (LLM) in handling unknown scenarios, this\nresearch proposes a dual-core framework for LLM applications to address this\nissue. Firstly, through ReAct-based prompt engineering, a larger LLM core\ndecomposes intricate navigation tasks into manageable sub-tasks, which\nautonomously invoke corresponding external tools to gather relevant\ninformation, using this feedback to mitigate the risk of LLM hallucinations.\nSubsequently, a fine-tuned and compact LLM core, acting like a first-mate is\ndesigned to process such information and unstructured external data, then to\ngenerates context-aware recommendations, ultimately delivering lookout insights\nand navigation hints that adhere to the International Regulations for\nPreventing Collisions at Sea (COLREGs) and other rules. Extensive experiments\ndemonstrate the proposed framework not only excels in traditional ship\ncollision avoidance tasks but also adapts effectively to unstructured,\nnon-predefined, and unpredictable scenarios. A comparative analysis with\nDeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and\nrationality of the proposed framework. This research bridges the gap between\nconventional navigation systems and LLMs, offering a framework to enhance\nsafety and operational efficiency across diverse navigation applications.",
        "The adoption of EHRs has expanded opportunities to leverage data-driven\nalgorithms in clinical care and research. A major bottleneck in effectively\nconducting multi-institutional EHR studies is the data heterogeneity across\nsystems with numerous codes that either do not exist or represent different\nclinical concepts across institutions. The need for data privacy further limits\nthe feasibility of including multi-institutional patient-level data required to\nstudy similarities and differences across patient subgroups. To address these\nchallenges, we developed the GAME algorithm. Tested and validated across 7\ninstitutions and 2 languages, GAME integrates data in several levels: (1) at\nthe institutional level with knowledge graphs to establish relationships\nbetween codes and existing knowledge sources, providing the medical context for\nstandard codes and their relationship to each other; (2) between institutions,\nleveraging language models to determine the relationships between\ninstitution-specific codes with established standard codes; and (3) quantifying\nthe strength of the relationships between codes using a graph attention\nnetwork. Jointly trained embeddings are created using transfer and federated\nlearning to preserve data privacy. In this study, we demonstrate the\napplicability of GAME in selecting relevant features as inputs for AI-driven\nalgorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.\nWe then highlight the application of GAME harmonized multi-institutional EHR\ndata in a study of Alzheimer's disease outcomes and suicide risk among patients\nwith mental health disorders, without sharing patient-level data outside\nindividual institutions.",
        "In many engineering applications, processes must be followed precisely,\nmaking conformance checking between event logs and declarative process models\ncrucial for ensuring adherence to desired behaviors. This is a critical area\nwhere Artificial Intelligence (AI) plays a pivotal role in driving effective\nprocess improvement. However, computing optimal alignments poses significant\ncomputational challenges due to the vast search space inherent in these models.\nConsequently, existing approaches often struggle with scalability and\nefficiency, limiting their applicability in real-world settings. This paper\nintroduces DeclareAligner, a novel algorithm that uses the A* search algorithm,\nan established AI pathfinding technique, to tackle the problem from a fresh\nperspective leveraging the flexibility of declarative models. Key features of\nDeclareAligner include only performing actions that actively contribute to\nfixing constraint violations, utilizing a tailored heuristic to navigate\ntowards optimal solutions, and employing early pruning to eliminate\nunproductive branches, while also streamlining the process through\npreprocessing and consolidating multiple fixes into unified actions. The\nproposed method is evaluated using 8,054 synthetic and real-life alignment\nproblems, demonstrating its ability to efficiently compute optimal alignments\nby significantly outperforming the current state of the art. By enabling\nprocess analysts to more effectively identify and understand conformance\nissues, DeclareAligner has the potential to drive meaningful process\nimprovement and management.",
        "Program synthesis has traditionally relied on human-provided specifications,\nexamples, or prior knowledge to generate functional algorithms. Existing\nmethods either emulate human-written algorithms or solve specific tasks without\ngenerating reusable programmatic logic, limiting their ability to create novel\nalgorithms. We introduce AlgoPilot, a groundbreaking approach for fully\nautomated program synthesis without human-written programs or trajectories.\nAlgoPilot leverages reinforcement learning (RL) guided by a Trajectory Language\nModel (TLM) to synthesize algorithms from scratch. The TLM, trained on\ntrajectories generated by random Python functions, serves as a soft constraint\nduring the RL process, aligning generated sequences with patterns likely to\nrepresent valid algorithms. Using sorting as a test case, AlgoPilot\ndemonstrates its ability to generate trajectories that are interpretable as\nclassical algorithms, such as Bubble Sort, while operating without prior\nalgorithmic knowledge. This work establishes a new paradigm for algorithm\ndiscovery and lays the groundwork for future advancements in autonomous program\nsynthesis.",
        "Speech enhancement (SE) aims to improve the clarity, intelligibility, and\nquality of speech signals for various speech enabled applications. However,\nair-conducted (AC) speech is highly susceptible to ambient noise, particularly\nin low signal-to-noise ratio (SNR) and non-stationary noise environments.\nIncorporating multi-modal information has shown promise in enhancing speech in\nsuch challenging scenarios. Electromyography (EMG) signals, which capture\nmuscle activity during speech production, offer noise-resistant properties\nbeneficial for SE in adverse conditions. Most previous EMG-based SE methods\nrequired 35 EMG channels, limiting their practicality. To address this, we\npropose a novel method that considers only 8-channel EMG signals with acoustic\nsignals using a modified SEMamba network with added cross-modality modules. Our\nexperiments demonstrate substantial improvements in speech quality and\nintelligibility over traditional approaches, especially in extremely low SNR\nsettings. Notably, compared to the SE (AC) approach, our method achieves a\nsignificant PESQ gain of 0.235 under matched low SNR conditions and 0.527 under\nmismatched conditions, highlighting its robustness.",
        "Quantile regression is a powerful tool capable of offering a richer view of\nthe data as compared to linear-squares regression. Quantile regression is\ntypically performed individually on a few quantiles or a grid of quantiles\nwithout considering the similarity of the underlying regression coefficients at\nnearby quantiles. When needed, an ad hoc post-processing procedure such as\nkernel smoothing is employed to smooth the estimated coefficients across\nquantiles and thereby improve the performance of these estimates. This paper\nintroduces a new method, called spline quantile regression (SQR), that unifies\nquantile regression with quantile smoothing and jointly estimates the\nregression coefficients across quantiles as smoothing splines. We discuss the\ncomputation of the SQR solution as a linear program (LP) using an\ninterior-point algorithm. We also experiment with some gradient algorithms that\nrequire less memory than the LP algorithm. The performance of the SQR method\nand these algorithms is evaluated using simulated and real-world data.",
        "Fuel moisture content (FMC) is a key predictor for wildfire rate of spread\n(ROS). Machine learning models of FMC are being used more in recent years,\naugmenting or replacing traditional physics-based approaches. Wildfire rate of\nspread (ROS) has a highly nonlinear relationship with FMC, where small\ndifferences in dry fuels lead to large differences in ROS. In this study,\ncustom loss functions that place more weight on dry fuels were examined with a\nvariety of machine learning models of FMC. The models were evaluated with a\nspatiotemporal cross-validation procedure to examine whether the custom loss\nfunctions led to more accurate forecasts of ROS. Results show that the custom\nloss functions improved accuracy for ROS forecasts by a small amount. Further\nresearch would be needed to establish whether the improvement in ROS forecasts\nleads to more accurate real-time wildfire simulations.",
        "Let $M^n$, $n\\in\\{3,4,5\\}$, be a closed aspherical $n$-manifold and $S\\subset\nM$ a subset consisting of disjoint incompressible embedded closed aspherical\nsubmanifolds (possibly with different dimensions). When $n =3,4$, we show that\n$M\\setminus S$ cannot admit any complete metric with positive scalar curvature.\nWhen $n=5$, we obtain the same result either when $S$ contains a submanifold of\ncodimension 1 or 2, or when $S$ itself is a connected submaifold of codimension\n$\\ge 3.$ The key ingredient is a new interior control for the extrinsic\ndiameter of surfaces with positive scalar curvature.",
        "Let $p$ be a prime. We prove bounds on short Dirichlet character sums\nevaluated at a class of homogeneous polynomials in arbitrary dimensions. In\nevery dimension, this bound is nontrivial for sums over boxes with side lengths\nas short as $p^{1\/4 + \\kappa}$ for any $\\kappa>0$. Our methods capitalize on\nthe relationship between characters mod $p$ and characters over finite field\nextensions as well as bounds on the multiplicative energy of sets in products\nof finite fields.",
        "Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.",
        "Large pre-trained models have achieved outstanding results in sequence\nmodeling. The Transformer block and its attention mechanism have been the main\ndrivers of the success of these models. Recently, alternative architectures,\nsuch as Selective Structured State Space Models (SSMs), have been proposed to\naddress the inefficiencies of Transformers. This paper explores the compression\nof SSM-based models, particularly Mamba and its hybrids. We study the\nsensitivity of these models to the removal of selected components at different\ngranularities to reduce the model size and computational overhead, thus\nimproving their efficiency while maintaining accuracy. The proposed solutions,\ncollectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x\nduring inference, demonstrating that model efficiency can be improved by\neliminating several redundancies with minimal impact on the overall model\nperformance. The code is available at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "Restarted GMRES is a robust and widely used iterative solver for linear\nsystems. The control of the restart parameter is a key task to accelerate\nconvergence and to prevent the well-known stagnation phenomenon. We focus on\nthe Proportional-Derivative GMRES (PD-GMRES), which has been derived using\ncontrol-theoretic ideas in [Cuevas N\\'u\\~nez, Schaerer, and Bhaya (2018)] as a\nversatile method for modifying the restart parameter. Several variants of a\nquadtree-based geometric optimization approach are proposed to find a best\nchoice of PD-GMRES parameters. We show that the optimized PD-GMRES performs\nwell across a large number of matrix types and we observe superior performance\nas compared to major other GMRES-based iterative solvers. Moreover, we propose\nan extension of the PD-GMRES algorithm to further improve performance by\ncontrolling the range of values for the restart parameter.",
        "The main result of this paper utilizes the representation graph of a group\n$G$, $R(V,G)$, and gives a general construction of a diagrammatic category\n$\\mathbf{Dgrams}_{R(V,G)}$. The proof of the main theorem shows that, given\nexplicit criteria, there is an equivalence of categories between a quotient\ncategory of $\\mathbf{Dgrams}_{R(V,G)}$ and a full subcategory of\n$G-\\textbf{mod}$ with objects being the tensor products of finitely many\nirreducible $G$-modules.",
        "Accurate gait event detection is crucial for gait analysis, rehabilitation,\nand assistive technology, particularly in exoskeleton control, where precise\nidentification of stance and swing phases is essential. This study evaluated\nthe performance of seven kinematics-based methods and a Long Short-Term Memory\n(LSTM) model for detecting heel strike and toe-off events across 4363 gait\ncycles from 588 able-bodied subjects. The results indicated that while the Zeni\net al. method achieved the highest accuracy among kinematics-based approaches,\nother methods exhibited systematic biases or required dataset-specific tuning.\nThe LSTM model performed comparably to Zeni et al., providing a data-driven\nalternative without systematic bias. These findings highlight the potential of\ndeep learning-based approaches for gait event detection while emphasizing the\nneed for further validation in clinical populations and across diverse gait\nconditions. Future research will explore the generalizability of these methods\nin pathological populations, such as individuals with post-stroke conditions\nand knee osteoarthritis, as well as their robustness across varied gait\nconditions and data collection settings to enhance their applicability in\nrehabilitation and exoskeleton control.",
        "We study the approximation properties of shallow neural networks whose\nactivation function is defined as the flow of a neural ordinary differential\nequation (neural ODE) at the final time of the integration interval. We prove\nthe universal approximation property (UAP) of such shallow neural networks in\nthe space of continuous functions. Furthermore, we investigate the\napproximation properties of shallow neural networks whose parameters are\nrequired to satisfy some constraints. In particular, we constrain the Lipschitz\nconstant of the flow of the neural ODE to increase the stability of the shallow\nneural network, and we restrict the norm of the weight matrices of the linear\nlayers to one to make sure that the restricted expansivity of the flow is not\ncompensated by the increased expansivity of the linear layers. For this\nsetting, we prove approximation bounds that tell us the accuracy to which we\ncan approximate a continuous function with a shallow neural network with such\nconstraints. We prove that the UAP holds if we consider only the constraint on\nthe Lipschitz constant of the flow or the unit norm constraint on the weight\nmatrices of the linear layers.",
        "Explanation is a fundamentally human process. Understanding the goal and\naudience of the explanation is vital, yet existing work on explainable\nreinforcement learning (XRL) routinely does not consult humans in their\nevaluations. Even when they do, they routinely resort to subjective metrics,\nsuch as confidence or understanding, that can only inform researchers of users'\nopinions, not their practical effectiveness for a given problem. This paper\ncalls on researchers to use objective human metrics for explanation evaluations\nbased on observable and actionable behaviour to build more reproducible,\ncomparable, and epistemically grounded research. To this end, we curate,\ndescribe, and compare several objective evaluation methodologies for applying\nexplanations to debugging agent behaviour and supporting human-agent teaming,\nillustrating our proposed methods using a novel grid-based environment. We\ndiscuss how subjective and objective metrics complement each other to provide\nholistic validation and how future work needs to utilise standardised\nbenchmarks for testing to enable greater comparisons between research.",
        "Road fatalities pose significant public safety and health challenges\nworldwide, with pedestrians being particularly vulnerable in vehicle-pedestrian\ncrashes due to disparities in physical and performance characteristics. This\nstudy employs explainable artificial intelligence (XAI) to identify key factors\ncontributing to pedestrian fatalities across the five U.S. states with the\nhighest crash rates (2018-2022). It compares them to the five states with the\nlowest fatality rates. Using data from the Fatality Analysis Reporting System\n(FARS), the study applies machine learning techniques-including Decision Trees,\nGradient Boosting Trees, Random Forests, and XGBoost-to predict contributing\nfactors to pedestrian fatalities. To address data imbalance, the Synthetic\nMinority Over-sampling Technique (SMOTE) is utilized, while SHapley Additive\nExplanations (SHAP) values enhance model interpretability. The results indicate\nthat age, alcohol and drug use, location, and environmental conditions are\nsignificant predictors of pedestrian fatalities. The XGBoost model outperformed\nothers, achieving a balanced accuracy of 98 %, accuracy of 90 %, precision of\n92 %, recall of 90 %, and an F1 score of 91 %. Findings reveal that pedestrian\nfatalities are more common in mid-block locations and areas with poor\nvisibility, with older adults and substance-impaired individuals at higher\nrisk. These insights can inform policymakers and urban planners in implementing\ntargeted safety measures, such as improved lighting, enhanced pedestrian\ninfrastructure, and stricter traffic law enforcement, to reduce fatalities and\nimprove public safety.",
        "In this note, we define an analogue of R-matrices for bialgebras in the\nsetting of a monad that is opmonoidal over two tensor products. Analogous to\nthe classical case, such structures bijectively correspond to duoidal\nstructures on the Eilenberg--Moore category of the monad. Further, we\ninvestigate how a cocommutative version of this lifts the linearly distributive\nstructure of a normal duoidal category."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Speaker Diarization with LSTM",
    "start_abstract":"For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Human-Centric Interfaces for Ambient Intelligence"
      ],
      "abstract":[
        "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
      ],
      "categories":[
        "physics.app-ph"
      ]
    },
    "list":{
      "title":[
        "Ultrawhite structural starch film for sustainable cooling",
        "Multi-time scale and high performance in-material reservoir computing\n  using graphene-based ion-gating reservoir",
        "Detection and Ranging Beyond the Canonical Resolution Limit",
        "Transfer ABCD Matrix for Time-Varying Media and Time Crystals",
        "Novel crossbar array of silicon nitride resistive memories on SOI\n  enables memristor rationed logic",
        "Practical implementation of a chiral phononic crystal demonstrator with\n  ultra-low frequency bandgap",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "Stochastic reconstruction of multiphase composite microstructures using\n  statistics-encoded neural network for poro\/micro-mechanical modelling",
        "Analytical modeling of laminated composite rings on nonreciprocal\n  elastic foundations under non-axisymmetric loading",
        "Tracking the creation of single photon emitters in AlN by implantation\n  and annealing",
        "Node-to-node contact-friction problems using run-time parameter updates\n  on a conventional force-deformation finite element",
        "Ultra-flexible silicon foils with seamless detachability: the effect of\n  porous multilayered structures prepared through modulated electrolyte\n  composition",
        "Experimental and Theoretical Study of Thin-covered Composite Dowels\n  considering Multiple Load Conditions",
        "Einstein-Maxwell-Dilaton Wormholes that meet the Energy Conditions",
        "On the comparison principle for a nonlocal infinity Laplacian",
        "Freeze-and-release direct optimization method for variational\n  calculations of excited electronic states",
        "$e$-product of distributions, with applications",
        "A nonlinear model of shearable elastic rod from an origami-like\n  microstructure displaying folding and faulting",
        "Galaxy infall models for arbitrary velocity directions",
        "Analysis of $q_\\mathrm{rec}^2$-distribution for $B\\to K M_X$ and $B\\to\n  K^* M_X$ decays in a scalar-mediator dark-matter scenario",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Geometric Gauss Sums and Gross-Koblitz Formulas over Function Fields",
        "Solar irradiance statistical analysis in Mexico City from 2018 to 2021",
        "On the axially symmetric solutions to the spatially homogeneous Landau\n  equation",
        "Electroweak baryogenesis from charged current anomalies in $B$ meson\n  decays",
        "Ferri- and Ferro-Electric Switching in Spontaneously Chiral Polar Liquid\n  Crystals",
        "A general quasilinear elliptic problem with variable exponents and\n  Neumann boundary conditions for image processing",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case"
      ],
      "abstract":[
        "Reducing human reliance on high-electricity-consuming cooling technologies\nlike air conditioning is crucial for reshaping the global energy paradigm.\nThrough utilizing natural starch gelatinization, freezedrying and densification\nprocesses, we fabricated an ultrawhite cooling starch film with an ultrahigh\nsolar reflectance of 0.96 and strong infrared emittance of 0.94. The porous\nstructure of the cooling starch film, systematically controlled by the\nmechanical pressing processing, allows for effective scattering of solar\nradiation while emitting strongly during the atmospheric transparency window,\nthereby contributing to high-efficiency daytime radiative cooling capacity.\nFurthermore, the cooling starch film exhibits excellent mechanical tensile\nstrength, measuring at up to 38.5 megapascals, which is more than twice the\nstrength of natural wood. The ultrawhite radiative cooling starch film holds\nsignificant promise for optimizing cooling energy usage, especially in hot and\narid climates.",
        "The rising energy demands of conventional AI systems underscore the need for\nefficient computing technologies like brain-inspired computing. Physical\nreservoir computing (PRC), leveraging the nonlinear dynamics of physical\nsystems for information processing, has emerged as a promising approach for\nneuromorphic computing. However, current PRC systems are constrained by narrow\noperating timescales and limited performance. To address these challenges, an\nion-gel\/graphene electric double layer transistor-based ion-gating reservoir\n(IGR) was developed, offering adaptability across multi-time scales with an\nexceptionally wide operating range from 1 MHz to 20 Hz and high information\nprocessing capacity. The IGR achieved deep learning (DL)-level accuracy in\nchaotic time series prediction tasks while reducing computational resource\nrequirements to 1\/100 of those needed by DL. Principal component analysis\nreveals the IGR's superior performance stems from its high-dimensionality,\ndriven by the ambipolar behavior of graphene and multiple relaxation processes.\nThe proposed IGR represents a significant step forward in providing low-power,\nhigh-performance computing solutions, particularly for resource-constrained\nedge environments.",
        "The canonical range resolution limit in radar, sonar, and lidar systems is\nfound to be a special case of a more general resolution limit. The general\nlimit indicates that it is possible to surpass the canonical limit in moderate\n(of order unity) signal-to-noise ratio (SNR) environments by using the signal\namplitude and phase information. The canonical limit only considers the\nbandwidth of the received signal without considering how SNR affects the range\nresolution. Details present in the signal amplitude, such as attenuation and\ngeometric spreading, can act as additional sources of range information.\nPrevious studies have taken advantage of the relationship between target\ndistance and signal amplitude or phase to achieve higher resolution ranging,\nand often employ unusual transmit waveforms for this purpose. These methods\neach provide distinct bounds on range resolution, rather than a unified bound\napplicable across different systems and applications. We apply ideas from\ninformation theory to determine a general lower bound to the smallest\nresolvable range bin size and corresponding target strength measurements.",
        "This paper introduces a formal definition of the transfer ABCD parameters in\ntime-varying electromagnetic systems. The formal definition comes after the\nrearrangement of the fields $D$ and $B$ at the inputs and outputs of the\ntemporal system based on the time-varying boundary conditions. Then, we derive\nthe ABCD parameters of a temporal transmission line, i.e., a temporal slab, and\ncompute the associated scattering parameters (reflection and transmission\ncoefficients). The results presented here open up an alternative way, based on\nnetwork theory, to analyze multilayer temporal configurations. Moreover, we\nshow that the ABCD parameters can be used to compute the dispersion diagram\n($\\omega$ vs $k$) of time crystals.",
        "In this work, the fabrication of crossbar arrays of silicon nitride resistive\nmemories on silicon-on-insulator substrate and their utilization to realize\nmulti-rationed logic circuits are presented. Typical electrical\ncharacterization of the memristors revealed their ability of multi-state\noperation by the presence of 12 well separated resistance levels. Through a\ndedicated modeling and fitting procedure of these levels, a reconfigurable\nlogic based on memristor rationed logic scheme is designed and a crossbar\nintegration methodology was proposed. Finally, several circuitry aspects were\nsimulated in SPICE with a silicon nitride SOI crossbar array calibrated model\nand power optimization prospects were discussed.",
        "The use of phononic crystals for vibration attenuation and isolation has been\nwidely studied, showing that the attenuation frequency range depends on their\nmass and stiffness. The concepts of chirality and tacticity have been\nintroduced into classical phononic crystals to enrich the dynamics of the mass\nelements and thereby achieve lower frequency ranges with high vibration\nattenuation. Although these concepts have demonstrated their effectiveness on\nlab-scale crystals, their implementation in industrial applications is still\nrare. Chiral phononic crystals require a complex geometry that complicates\ntheir manufacturing. Existing examples require to be fabricated by 3D printing,\nmaking them expensive to build on a large scale for demonstration purposes or\nin-situ applications. In this study, we redefine a chiral phononic crystal\ndesign for translational-rotational coupling in order to enable its\nmanufacturability using exclusively conventional processes. We then investigate\nthe design space of these newly designed phononic crystals, using a simplified\nunit cell FEM model that minimizes computation time. A parametric study is\nconducted to investigate the crystal's tunability by modifying the dimensions\nof the chiral links between the masses. A large crystal with ultra-low\nfrequency range attenuation -- starting at 60~Hz -- is then designed, with the\naim to demonstrate the influence of the crystal's tacticity on the vibration\nisolation by hand sensing. A crystal composed of 2 unit cells is manufactured\nand its measured transfer function is compared with numerical predictions, thus\nhighlighting the disparities between the behavior of the structure under\nreal-life and ideal excitation conditions.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "Understanding microstructure-property relationships (MPRs) is essential for\noptimising the performance of multiphase composites. Image-based\nporo\/micro-mechanical modelling provides a non-invasive approach to exploring\nMPRs, but the randomness of multiphase composites often necessitates extensive\n3D microstructure datasets for statistical reliability. This study introduces a\ncost-effective machine learning framework to reconstruct numerous virtual 3D\nmicrostructures from limited 2D exemplars, circumventing the high costs of\nvolumetric microscopy. Using feedforward neural networks, termed the\nstatistics-encoded neural network (SENN), the framework encodes 2D\nmorphological statistics and infers 3D morphological statistics via a 2D-to-3D\nintegration scheme. Statistically equivalent 3D microstructures are synthesised\nusing Gibbs sampling. Hierarchical characterisation enables seamless capture of\nfeatures across multiple scales. Validation on three composites demonstrates\nstrong statistical equivalence between reconstructed and reference\nmicrostructures, confirmed by morphological descriptors and simulated\nmacroscopic properties (e.g., stiffness, permeability). The SENN-based\nframework is a high-fidelity tool for efficiently and accurately reconstructing\nmultiphase microstructures.",
        "A mechanical model of a laminated composite ring on a nonreciprocal elastic\nfoundation is a valuable engineering tool during the early design stages of\nvarious applications, such as non-pneumatic wheels, flexible bearings,\nexpandable tubulars in oil wells, and vascular stents interacting with blood\nvessel linings, especially under non-axisymmetric loadings. Despite its\nimportance, limited research has focused on the interaction between laminated\ncomposite rings and nonreciprocal elastic foundations. Moreover, no\nquantitative studies have yet explored the influence of foundation stiffness on\nthe ring deformation. This work aims to develop an analytical framework for a\nlaminated composite ring supported by a nonreciprocal elastic foundation under\nnon-axisymmetric loading conditions. The model generates a design map that\ncorrelates the foundation stiffness with the ring deformation, accounting for\nring dimensions, laminate layup architecture, and lamina anisotropy. The\nclosed-form solution provides an efficient design tool for analyzing\nnon-axisymmetric and nonuniform loadings at a low computational cost. The\nresulting design map provides a valuable resource for exploring the interaction\nbetween the nonreciprocal foundation and the laminated ring. The proposed\nanalytical framework and design map hold broad potential applications in\nautomotive, mechanical, civil, and biomedical engineering fields.",
        "In this study, we inspect and analyze the effect of Al implantation into AlN\nby conducting confocal microscopy on the ion implanted regions, before and\nafter implantation, followed by an annealing step. The independent effect of\nannealing is studied in an unimplanted control region, which showed that\nannealing alone does not produce new emitters. We observed that point-like\nemitters are created in the implanted regions after annealing by tracking\nindividual locations in a lithographically patterned sample. The newly created\nquantum emitters show anti-bunching under ambient conditions and are spectrally\nsimilar to the previously discovered emitters in as-grown AlN.",
        "A novel implementation of the traditional node-to-node Coulomb\ncontact-friction problem is presented that utilizes run-time parameter updates\non conventional elasto-plastic elements. The two-noded elements are defined by\nan independent uniaxial force-deformation (or constitutive) relation in each\ndegree of freedom. The location of the two nodes may or may not be coincident.\nA parameter is a pointer to a value (nodal geometry, element property, material\nproperty, etc.) in the finite element domain that can be controlled by the\nuser. Parameters that control the frictional interface normal and tangential\nresponses are updated based on contact detection, and eliminate the need for\nadding new source code to the finite element library of a given software. The\nrun-time algorithm for updating both an elastic and elasto-plastic\nforce-deformation element to achieve a penalty-based contact-friction model is\npresented. Static and dynamic cases were investigated for a two-noded unit\nprocess and compared with the results obtained from closed-form solutions.\nMultiple interface elements were then used for the sliding, tipping, and\nrocking responses of a rigid block on rigid foundation. Finally, several case\nstudies were investigated, and the findings were compared with the experimental\nresults or solutions from the literature. The proposed friction-contact\nimplementation can be deployed in larger finite element models, and parameter\nupdates facilitate implementation of a wider array of friction models by\nchanging only the constitutive model.",
        "A comprehensive evaluation of the effect and limitations of variable current\ndensity and electrolyte composition on layer porosity and microstructure\nchanges of porous silicon (pSi) multilayer stacks is reported. Following these\nresults, the development and optimization of a four-layer stack architecture is\nreported through addition of super-low porosity layers (SLPL) on a low\/high\nporosity layer (LPL\/HLP) stack. Thermal treatment of these structures achieved\nexcellent top and bottom surface reconstruction to form sintered porous silicon\n(SPS) detachable foils, enabling direct foil separation using cello tape from a\nsmooth and specular parent substrate without the need to cut or damage it",
        "With the widespread application of composite structures in the fields of\nbuilding and bridge constructions, thin-covered composite dowels are\nincreasingly adopted in various engineering scenarios. This paper presents a\ndesign methodology for thin-covered composite dowels, supported by both\nexperimental and theoretical investigations. In the experiment, a novel test\nrig and specimens are designed to facilitate tensile-shear coupling loading.\nThe study identifies a new failure mode: Restricted Cone Failure (RCF) in\nthin-covered composite dowels under tensile-shear coupling load, which distinct\nfrom conventional composite dowels. This RCF mode is attributed to the thin\nthickness of the side concrete cover, which restricts the development of the\nfailure cone in the thickness direction. Additionally, a parametric analysis is\nconducted to evaluate the effects of key factors--such as steel dowel\nthickness, effective embedment depth, and the tensile strength of steel fiber\nreinforced concrete--on the bearing capacity and ductility of thin-covered\ncomposite dowels. Based on the theoretical findings, comprehensive tensile,\nshear, and tensile-shear coupling capacity models along with an engineering\ndesign model are developed to aid in the practical application of thin-covered\ncomposite dowels.",
        "One of the latest predictions of Einstein's theory is the existence of\nWormholes (WH). In this work, we present exact solutions of the\nEinstein-Maxwell-Dilaton equations representing traversable Wormholes. These\nsolutions satisfy the energy conditions and have a ring singularity satisfying\nthe cosmic censorship of WHs, i.e. we show that, as in previous solutions,\ngeodesics cannot touch the singularity. We find that the most optimal input\nregions for the first class of solutions traversing these wormholes are near\nthe poles and near the equatorial plane for the second class. We also find that\nthe solution associated with the first class is physically feasible, while for\nthe second class it presents the problem of not being asymptotically flat when\nconsidering a dilatonic-type scalar field. Finally, we give examples of\nrealistic astrophysical objects that could fulfill these conditions.",
        "In this article, we prove the uniqueness of viscosity solutions to\n$\\mathcal{L}_{\\infty} u =f$ in $\\Omega$, where $\\mathcal{L}_{\\infty}$ denotes\nthe nonlocal infinity Laplace operator, $\\Omega$ a bounded domain, and $f$ a\ncontinuous functions such that $f \\leq 0$. Uniqueness is established through a\ncomparison principle.",
        "Time-independent, orbital-optimized density functional approaches outperform\ntime-dependent density functional theory (TDDFT) in calculations of excited\nelectronic states involving a large rearrangement of the electron density, such\nas charge transfer excitations. However, optimizing orbitals for excited states\nremains challenging, as the latter typically correspond to saddle points on the\nelectronic energy surface. A simple and robust strategy for variational orbital\noptimization of excited states is presented. The approach involves two steps:\n(1) a constrained energy minimization, where a subset of orbitals changed by\nthe excitation are frozen, followed by (2) a fully unconstrained saddle point\noptimization. The constrained minimization step makes it possible to identify\nthe electronic degrees of freedom along which the energy needs to be maximized,\npreventing variational collapse. Both steps of this freeze-and-release strategy\nare carried out using direct optimization algorithms with a computational\nscaling comparable to ground state calculations. Numerical tests using a\nsemilocal functional are performed on intramolecular charge transfer states of\norganic molecules and intermolecular charge transfer states of molecular\ndimers. It is shown that the freeze-and-release direct optimization (FR-DO)\napproach can successfully converge challenging charge transfer states,\novercoming limitations of conventional algorithms based on the maximum overlap\nmethod, which either collapse to lower energy, charge-delocalized solutions or\nfail to converge. While FR-DO requires more iterations on average, the overall\nincrease in computational cost is small. For the NH3-F2 dimer, it is found that\nunlike TDDFT, orbital-optimized calculations reproduce the correct long-range\ndependency of the energy with respect to the donor-acceptor separation without\nthe need to include exact exchange in the long range.",
        "We consider and reformulate a recent definition of multiplication between\ndistributions. We show that this definition can be adopted, in particular, to\nprove biorthonormality of some distributions arising when looking to the\n(generalized) eigenvalues of a specific non self-adjoint number-like operator,\nconsidered in connection with the recently introduced {\\em weak pseudo-bosons}.\nSeveral examples are discussed in details.",
        "A new continuous model of shearable rod, subject to large elastic\ndeformation, is derived from nonlinear homogenization of a one-dimensional\nperiodic microstructured chain. As particular cases, the governing equations\nreduce to the Euler elastica and to the shearable elastica known as 'Engesser',\nthat has been scarcely analysed so far. The microstructure that is homogenized\nis made up of elastic hinges and four-bar linkages, which may be realized in\npractice using origami joints. The equivalent continuous rod is governed by a\nDifferential-Algebraic system of nonlinear Equations (DAE), containing an\ninternal length ratio, and showing a surprisingly rich mechanical landscape,\nwhich involves a twin sequence of bifurcation loads, separated by a\n'transition' mode. The latter occurs, for simply supported and cantilever rods\nin a 'bookshelf-like' mode and in a mode involving faulting (formation of a\nstep in displacement), respectively. The postcritical response of the simply\nsupported rod exhibits the emergence of folding, an infinite curvature\noccurring at a point of the rod axis, developing into a curvature jump at\nincreasing load. Faulting and folding, excluded for both Euler and Reissner\nmodels and so far unknown in the rod theory, represent 'signatures' revealing\nthe origami design of the microstructure. These two features are shown to be\nassociated with bifurcations and, in particular folding, with a secondary\nbifurcation of the corresponding discrete chain when the number of elements is\nodd. Beside the intrinsic theoretical relevance to the field of structural\nmechanics, our results can be applied to various technological contexts\ninvolving highly compliant mechanisms, such as the achievement of objective\ntrajectories with soft robot arms through folding and localized displacement of\norigami-inspired or multi-material mechanisms.",
        "For most galaxies in the cosmos, our knowledge of their motion is limited to\nline-of-sight velocities from redshift observations. Peculiar motions on the\nsky are only measured for a few cases. With increasingly detailed observations,\nthe assumption that line-of-sight velocities suffice for an accurate and\nprecise reconstruction of galaxy kinematics needs to be re-investigated and the\nimpact of perpendicular velocities to be quantified. We analyse the motion of\ntwo galaxies with arbitrary velocities, determine their mutual velocity on an\narbitrary background, and compare this general relative velocity to the one\nfrom line-of-sight components only. The latter are known as ``minor and major\ninfall models'' established by Karachentsev and Kashibadze (2006). Our\nderivations reveal that the infall models approximate the radial velocity\nbetween two galaxies by two different projections employing different\ninformation about the system. For galaxies with small angular separations, all\ninfall models agree that the radial velocity is the difference of their\nline-of-sight velocities. For larger angles, the minor infall model is mostly\nsuitable when perpendicular velocity components are negligible and there is no\ninformation about the tangential velocity of the binary. The major infall model\nis best suitable when the motion is mainly radial and symmetry assumptions\ncancel the tangential and one perpendicular component. The latter often\nrequires to transition from galaxy binaries to groups or clusters, as we show\nquantitatively. We give an encompassing overview how the infall models over-\nand under-estimate general binary or $N$-body motions. We quantify the impact\nof perpendicular velocity components, sparse sampling, and deviations of the\ntracer-galaxies from the motion in an embedding gravitational potential which\nare related to the angular momentum of the structure. (abridged)",
        "We demonstrate that the scalar-mediator dark-matter scenario is consistent\nwith the experimental data on the decay $B\\to K M_X$ and provides a good\ndescription of the shape of the observed excess. Within this scenario, the\ninteraction with dark-matter particles leads to approximately the same excess\nin $\\Gamma(B\\to K^* M_X)$ and $\\Gamma(B\\to K M_X)$ compared to the Standard\nModel; also the differential distributions of the excess events are similar in\nshape in the variable $q_\\mathrm{rec}^2$ measured by experiment.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "In this paper, we prove the Gross-Koblitz-Thakur formulas relating special\n$v$-adic gamma values to the newly introduced geometric Gauss sums in the\nfunction field setting. These are analogous to those for the $p$-adic gamma\nfunction in the classical setting due to Gross-Koblitz and the $v$-adic\narithmetic gamma function over function fields due to Thakur. For these new\nGauss sums, we establish their key arithmetic properties, including the\nuniformity of absolute values and prime factorizations. We also determine their\nsigns at infinite places, and derive two analogs of the Hasse-Davenport\nrelations.",
        "Solar radiation is made up of three components of electromagnetic waves:\ninfrared, visible and ultraviolet. The infrared component is the cause of\nthermal energy, the visible spectrum allows to see through the eyes and the\nultraviolet component is the most energetic and damaging. Solar radiation has\nseveral benefits, such as helping to synthesize vitamin D in the skin, favors\nblood circulation, among others benefits for the human body. In the Earth, it\nis the main source of energy for agriculture, also used as an alternative\nsource of energy to hydrocarbons, through solar cells. The solar irradiance\nrepresents the surface power density with units W\/m$^2$ in SI. Too much\nexposure can cause damage and an increase in value over the time can be can be\nalso damaging. In this work it was used an open data base provided by\nSecretar\\'ia del Medio Ambiente, from which a statistical analysis was\nperformed of the solar irradiance values measured at various meteorological\nstations in Mexico City and the so-called metropolitan area, from 2018 to 2021.\nThis analysis was carried out per years, months and days. From the solar\nirradiance values distributions, it was obtained the averages, maximums and\nmeans were it was found there was no variation in the solar irradiance values\nover this period of years.",
        "In this paper, we consider the spatially homogeneous Landau equation, which\nis a variation of the Boltzmann equation in the grazing collision limit. For\nthe Landau equation for hard potentials in the style of Desvillettes-Villani\n(Comm. Partial Differential Equations, 2000), we provide the proof of the\nexistence of axisymmetric measure-valued solution for any axisymmetric\n$\\mathcal{P}_p(\\mathbb{R}^3)$ initial profile for any $p\\ge 2$. Moreover, we\nprove that if the initial data is not a single Dirac mass, then the solution\ninstantaneously becomes analytic for any time $t>0$ in the hard potential case.\nIn the soft potential and the Maxwellian molecule cases, we show that there are\nno solutions whose support is contained in a fixed line even for any given\nline-concentrated data.",
        "We demonstrate for the first time that new physics explaining the long\nstanding charged $B$ meson anomalies, $R(D^{(*)})$, can be the source of CP\nviolation that explains the observed baryon asymmetry of the universe (BAU). We\nconsider the general two Higgs doublet model with complex Yukawa couplings and\ncompute the BAU in the semiclassical formalism, using a novel analytic\napproximation for the latter. After imposing constraints from both flavor\nobservables and the electron electric dipole moment (eEDM), we find that a\nsignificant BAU can still be generated for a variety of benchmark points in the\nparameter space, assuming the occurrence of a sufficiently strong first order\nelectroweak phase transition. These scenarios, which explain both the\n$R(D^{(*)})$ flavor anomalies and the BAU, can be probed with future eEDM\nexperiments and Higgs factories measurements.",
        "The recent discovery of spontaneous chiral symmetry breaking has demonstrated\nthe possibility of discovering the exotic textures of ferromagnetic systems in\nliquid crystalline fluid ferro-electrics. We show that the polar smectic\nmesophase exhibited by the first molecule discovered to exhibit a spontaneously\nchiral ferroelectric nematic phase is also helical has a strongly varied\ntextural morphology depending in its thermal history and phase ordering.\nElectro-optic studies demonstrate that the two spontaneously chiral phases\nexhibit field induced phase transitions. For the nematic variant, this process\nis threshold-less and has no hysteresis while for the smectic it has a clear\nthreshold and shows hysteresis meaning this phase exhibits pseudo-ferrielectric\nswitching, the first of its kind for ferroelectric nematic like phases. We show\nthat helix formation can be both 1st and 2nd order but when it is 1st it is\naccompanied by pre-transitional helix formation in the preceding ferroelectric\nnematic phase.",
        "The aim of this paper is to state and prove existence and uniqueness results\nfor a general elliptic problem with homogeneous Neumann boundary conditions,\noften associated with image processing tasks like denoising. The novelty is\nthat we surpass the lack of coercivity of the Euler-Lagrange functional with an\ninnovative technique that has at its core the idea of showing that the minimum\nof the energy functional over a subset of the space $W^{1,p(x)}(\\Omega)$\ncoincides with the global minimum. The obtained existence result applies to\nmultiple-phase elliptic problems under remarkably weak assumptions.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift."
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Turning single-molecule localization microscopy into a quantitative bioanalytical tool",
    "start_abstract":"Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "A framework for evaluating the performance of SMLM cluster analysis algorithms"
      ],
      "abstract":[
        "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Accelerated Cardiac Parametric Mapping using Deep Learning-Refined\n  Subspace Models",
        "Technical report of a DMD-based Characterization Method for Vision\n  Sensors",
        "Transferring between sparse and dense matching via probabilistic\n  reweighting",
        "Color Correction Meets Cross-Spectral Refinement: A Distribution-Aware\n  Diffusion for Underwater Image Restoration",
        "Balanced Opto-electronic Joint Transform Correlator for Enhanced\n  Real-Time Pattern Recognition",
        "Observation-only learning of neural mapping schemes for gappy\n  satellite-derived ocean colour parameters",
        "Gain-MLP: Improving HDR Gain Map Encoding via a Lightweight MLP",
        "ILACS-LGOT: A Multi-Layer Contrast Enhancement Approach for Palm-Vein\n  Images",
        "Deep Task-Based Beamforming and Channel Data Augmentations for Enhanced\n  Ultrasound Imaging",
        "A Synergy Scoring Filter for Unsupervised Anomaly Detection with Noisy\n  Data",
        "Skeletonisation Scale-Spaces",
        "Overview of Variable Rate Coding in JPEG AI",
        "DeepNuParc: A Novel Deep Clustering Framework for Fine-scale\n  Parcellation of Brain Nuclei Using Diffusion MRI Tractography",
        "MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length\n  Video Generation",
        "Out-of-distribution generalisation for learning quantum channels with\n  low-energy coherent states",
        "Formalising the intentional stance 2: a coinductive approach",
        "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep\n  Reinforcement Learning",
        "MastermindEval: A Simple But Scalable Reasoning Benchmark",
        "Critical Unstable Qubits: an Application to $B^0\\bar{B}^0$-Meson System",
        "MobileSteward: Integrating Multiple App-Oriented Agents with\n  Self-Evolution to Automate Cross-App Instructions",
        "Simple Hamiltonians for Matrix Product State models",
        "A Reinforcement Learning Approach to Quiet and Safe UAM Traffic\n  Management",
        "Learning Visual Proxy for Compositional Zero-Shot Learning",
        "k-LLMmeans: Summaries as Centroids for Interpretable and Scalable\n  LLM-Based Text Clustering",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "An overview of regularity results for the Laplacian and $p$-Laplacian in\n  metric spaces",
        "Temporal Feature Weaving for Neonatal Echocardiographic Viewpoint Video\n  Classification"
      ],
      "abstract":[
        "Cardiac parametric mapping is useful for evaluating cardiac fibrosis and\nedema. Parametric mapping relies on single-shot heartbeat-by-heartbeat imaging,\nwhich is susceptible to intra-shot motion during the imaging window. However,\nreducing the imaging window requires undersampled reconstruction techniques to\npreserve image fidelity and spatial resolution. The proposed approach is based\non a low-rank tensor model of the multi-dimensional data, which jointly\nestimates spatial basis images and temporal basis time-courses from an\nauxiliary parallel imaging reconstruction. The tensor-estimated spatial basis\nis then further refined using a deep neural network, trained in a fully\nsupervised fashion, improving the fidelity of the spatial basis using learned\nrepresentations of cardiac basis functions. This two-stage spatial basis\nestimation will be compared against Fourier-based reconstructions and parallel\nimaging alone to demonstrate the sharpening and denoising properties of the\ndeep learning-based subspace analysis.",
        "This technical report presents a novel DMD-based characterization method for\nvision sensors, particularly neuromorphic sensors such as event-based vision\nsensors (EVS) and Tianmouc, a complementary vision sensor. Traditional image\nsensor characterization standards, such as EMVA1288, are unsuitable for BVS due\nto their dynamic response characteristics. To address this, we propose a\nhigh-speed, high-precision testing system using a Digital Micromirror Device\n(DMD) to modulate spatial and temporal light intensity. This approach enables\nquantitative analysis of key parameters such as event latency, signal-to-noise\nratio (SNR), and dynamic range (DR) under controlled conditions. Our method\nprovides a standardized and reproducible testing framework, overcoming the\nlimitations of existing evaluation techniques for neuromorphic sensors.\nFurthermore, we discuss the potential of this method for large-scale BVS\ndataset generation and conversion, paving the way for more consistent\nbenchmarking of bio-inspired vision technologies.",
        "Detector-based and detector-free matchers are only applicable within their\nrespective sparsity ranges. To improve adaptability of existing matchers, this\npaper introduces a novel probabilistic reweighting method. Our method is\napplicable to Transformer-based matching networks and adapts them to different\nsparsity levels without altering network parameters. The reweighting approach\nadjusts attention weights and matching scores using detection probabilities of\nfeatures. And we prove that the reweighted matching network is the asymptotic\nlimit of detector-based matching network. Furthermore, we propose a sparse\ntraining and pruning pipeline for detector-free networks based on reweighting.\nReweighted versions of SuperGlue, LightGlue, and LoFTR are implemented and\nevaluated across different levels of sparsity. Experiments show that the\nreweighting method improves pose accuracy of detector-based matchers on dense\nfeatures. And the performance of reweighted sparse LoFTR is comparable to\ndetector-based matchers, demonstrating good flexibility in balancing accuracy\nand computational complexity.",
        "Underwater imaging often suffers from significant visual degradation, which\nlimits its suitability for subsequent applications. While recent underwater\nimage enhancement (UIE) methods rely on the current advances in deep neural\nnetwork architecture designs, there is still considerable room for improvement\nin terms of cross-scene robustness and computational efficiency. Diffusion\nmodels have shown great success in image generation, prompting us to consider\ntheir application to UIE tasks. However, directly applying them to UIE tasks\nwill pose two challenges, \\textit{i.e.}, high computational budget and color\nunbalanced perturbations. To tackle these issues, we propose DiffColor, a\ndistribution-aware diffusion and cross-spectral refinement model for efficient\nUIE. Instead of diffusing in the raw pixel space, we transfer the image into\nthe wavelet domain to obtain such low-frequency and high-frequency spectra, it\ninherently reduces the image spatial dimensions by half after each\ntransformation. Unlike single-noise image restoration tasks, underwater imaging\nexhibits unbalanced channel distributions due to the selective absorption of\nlight by water. To address this, we design the Global Color Correction (GCC)\nmodule to handle the diverse color shifts, thereby avoiding potential global\ndegradation disturbances during the denoising process. For the sacrificed image\ndetails caused by underwater scattering, we further present the Cross-Spectral\nDetail Refinement (CSDR) to enhance the high-frequency details, which are\nintegrated with the low-frequency signal as input conditions for guiding the\ndiffusion. This way not only ensures the high-fidelity of sampled content but\nalso compensates for the sacrificed details. Comprehensive experiments\ndemonstrate the superior performance of DiffColor over state-of-the-art methods\nin both quantitative and qualitative evaluations.",
        "Opto-electronic joint transform correlators (JTCs) use a focal plane array\n(FPA) to detect the joint power spectrum (JPS) of two input images, projecting\nit onto a spatial light modulator (SLM) to be optically Fourier transformed.\nThe JPS is composed of two self-intensities and two conjugate-products, where\nonly the latter produce the cross-correlation. However, the self-intensity\nterms are typically much stronger than the conjugate-products, consuming most\nof the available bit-depth on the FPA and SLM. Here we propose and demonstrate,\nthrough simulation and experiment, a balanced opto-electronic JTC that\nelectronically processes the JPS to remove the self-intensity terms, thereby\nenhancing the quality of the cross-correlation result.",
        "Monitoring optical properties of coastal and open ocean waters is crucial to\nassessing the health of marine ecosystems. Deep learning offers a promising\napproach to address these ecosystem dynamics, especially in scenarios where\ngap-free ground-truth data is lacking, which poses a challenge for designing\neffective training frameworks. Using an advanced neural variational data\nassimilation scheme (called 4DVarNet), we introduce a comprehensive training\nframework designed to effectively train directly on gappy data sets. Using the\nMediterranean Sea as a case study, our experiments not only highlight the high\nperformance of the chosen neural network in reconstructing gap-free images from\ngappy datasets but also demonstrate its superior performance over\nstate-of-the-art algorithms such as DInEOF and Direct Inversion, whether using\nCNN or UNet architectures.",
        "While most images shared on the web and social media platforms are encoded in\nstandard dynamic range (SDR), many displays now can accommodate high dynamic\nrange (HDR) content. Additionally, modern cameras can capture images in an HDR\nformat but convert them to SDR to ensure maximum compatibility with existing\nworkflows and legacy displays. To support both SDR and HDR, new encoding\nformats are emerging that store additional metadata in SDR images in the form\nof a gain map. When applied to the SDR image, the gain map recovers the HDR\nversion of the image as needed. These gain maps, however, are typically\ndown-sampled and encoded using standard image compression, such as JPEG and\nHEIC, which can result in unwanted artifacts. In this paper, we propose to use\na lightweight multi-layer perceptron (MLP) network to encode the gain map. The\nMLP is optimized using the SDR image information as input and provides superior\nperformance in terms of HDR reconstruction. Moreover, the MLP-based approach\nuses a fixed memory footprint (10 KB) and requires no additional adjustments to\naccommodate different image sizes or encoding parameters. We conduct extensive\nexperiments on various MLP based HDR embedding strategies and demonstrate that\nour approach outperforms the current state-of-the-art.",
        "This article presents an extended author's version based on our previous\nwork, where we introduced the Multiple Overlapping Tiles (MOT) method for palm\nvein image enhancement. To better reflect the specific operations involved, we\nrename MOT to ILACS-LGOT (Intensity-Limited Adaptive Contrast Stretching with\nLayered Gaussian-weighted Overlapping Tiles). This revised terminology more\naccurately represents the method's approach to contrast enhancement and blocky\neffect mitigation. Additionally, this article provides a more detailed\nanalysis, including expanded evaluations, graphical representations, and\nsample-based comparisons, demonstrating the effectiveness of ILACS-LGOT over\nexisting methods.",
        "This paper introduces a deep learning (DL)-based framework for task-based\nultrasound (US) beamforming, aiming to enhance clinical outcomes by integrating\nspecific clinical tasks directly into the beamforming process. Task-based\nbeamforming optimizes the beamformer not only for image quality but also for\nperformance on a particular clinical task, such as lesion classification. The\nproposed framework explores two approaches: (1) a Joint Beamformer and\nClassifier (JBC) that classifies the US images generated by the beamformer to\nprovide feedback for image quality improvement; and (2) a Channel Data\nClassifier Beamformer (CDCB) that incorporates classification directly at the\nchannel data representation within the beamformer's bottleneck layer.\nAdditionally, we introduce channel data augmentations to address challenges\nposed by noisy and limited in-vivo data. Numerical evaluations demonstrate that\ntraining with channel data augmentations significantly improves image quality.\nThe proposed methods were evaluated against conventional Delay-and-Sum (DAS)\nand Minimum Variance (MV) beamforming techniques, demonstrating superior\nperformance in terms of both image contrast and clinical relevance. Among all\nmethods, the CDCB approach achieves the best results, outperforming others in\nterms of image quality and clinical relevance. These approaches exhibit\nsignificant potential for improving clinical relevance and image quality in\nultrasound imaging.",
        "Noise-inclusive fully unsupervised anomaly detection (FUAD) holds significant\npractical relevance. Although various methods exist to address this problem,\nthey are limited in both performance and scalability. Our work seeks to\novercome these obstacles, enabling broader adaptability of unsupervised anomaly\ndetection (UAD) models to FUAD. To achieve this, we introduce the Synergy\nScoring Filter (SSFilter), the first fully unsupervised anomaly detection\napproach to leverage sample-level filtering. SSFilter facilitates end-to-end\nrobust training and applies filtering to the complete training set\npost-training, offering a model-agnostic solution for FUAD. Specifically,\nSSFilter integrates a batch-level anomaly scoring mechanism based on mutual\npatch comparison and utilizes regression errors in anomalous regions, alongside\nprediction uncertainty, to estimate sample-level uncertainty scores that\ncalibrate the anomaly scoring mechanism. This design produces a synergistic,\nrobust filtering approach. Furthermore, we propose a realistic anomaly\nsynthesis method and an integrity enhancement strategy to improve model\ntraining and mitigate missed noisy samples. Our method establishes\nstate-of-the-art performance on the FUAD benchmark of the recent large-scale\nindustrial anomaly detection dataset, Real-IAD. Additionally, dataset-level\nfiltering enhances the performance of various UAD methods on the FUAD\nbenchmark, and the high scalability of our approach significantly boosts its\npractical applicability.",
        "The medial axis transform is a well-known tool for shape recognition. Instead\nof the object contour, it equivalently describes a binary object in terms of a\nskeleton containing all centres of maximal inscribed discs. While this shape\ndescriptor is useful for many applications, it is also sensitive to noise:\nSmall boundary perturbations can result in large unwanted expansions of the\nskeleton. Pruning offers a remedy by removing unwanted skeleton parts. In our\ncontribution, we generalise this principle to skeleton sparsification: We show\nthat subsequently removing parts of the skeleton simplifies the associated\nshape in a hierarchical manner that obeys scale-space properties.\n  To this end, we provide both a continuous and discrete theory that\nincorporates architectural and simplification statements as well as\ninvariances. We illustrate how our skeletonisation scale-spaces can be employed\nfor practical applications with two proof-of-concept implementations for\npruning and compression.",
        "Empirical evidence has demonstrated that learning-based image compression can\noutperform classical compression frameworks. This has led to the ongoing\nstandardization of learned-based image codecs, namely Joint Photographic\nExperts Group (JPEG) AI. The objective of JPEG AI is to enhance compression\nefficiency and provide a software and hardwarefriendly solution. Based on our\nresearch, JPEG AI represents the first standardization that can facilitate the\nimplementation of a learned image codec on a mobile device. This article\npresents an overview of the variable rate coding functionality in JPEG AI,\nwhich includes three variable rate adaptations: a threedimensional quality map,\na fast bit rate matching algorithm, and a training strategy. The variable rate\nadaptations offer a continuous rate function up to 2.0 bpp, exhibiting a high\nlevel of performance, a flexible bit allocation between different color\ncomponents, and a region of interest function for the specified use case. The\nevaluation of performance encompasses both objective and subjective results.\nWith regard to the objective bit rate matching, the main profile with low\ncomplexity yielded a 13.1% BD-rate gain over VVC intra, while the high profile\nwith high complexity achieved a 19.2% BD-rate gain over VVC intra. The BD-rate\nresult is calculated as the mean of the seven perceptual metrics defined in the\nJPEG AI common test conditions. With respect to subjective results, the example\nof improving the quality of the region of interest is illustrated.",
        "Brain nuclei are clusters of anatomically distinct neurons that serve as\nimportant hubs for processing and relaying information in various neural\ncircuits. Fine-scale parcellation of the brain nuclei is vital for a\ncomprehensive understanding of its anatomico-functional correlations. Diffusion\nMRI tractography is an advanced imaging technique that can estimate the brain's\nwhite matter structural connectivity to potentially reveal the topography of\nthe nuclei of interest for studying its subdivisions. In this work, we present\na deep clustering pipeline, namely DeepNuParc, to perform automated, fine-scale\nparcellation of brain nuclei using diffusion MRI tractography. First, we\nincorporate a newly proposed deep learning approach to enable accurate\nsegmentation of the nuclei of interest directly on the dMRI data. Next, we\ndesign a novel streamline clustering-based structural connectivity feature for\na robust representation of voxels within the nuclei. Finally, we improve the\npopular joint dimensionality reduction and k-means clustering approach to\nenable nuclei parcellation at a finer scale. We demonstrate DeepNuParc on two\nimportant brain structures, i.e. the amygdala and the thalamus, that are known\nto have multiple anatomically and functionally distinct nuclei subdivisions.\nExperimental results show that DeepNuParc enables consistent parcellation of\nthe nuclei into multiple parcels across multiple subjects and achieves good\ncorrespondence with the widely used coarse-scale atlases. Our codes are\navailable at https:\/\/github.com\/HarlandZZC\/deep_nuclei_parcellation.",
        "Diffusion models are successful for synthesizing high-quality videos but are\nlimited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained\nfootage (e.g. over minutes) still remains an open research question. In this\npaper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers),\na new diffusion model specialized for long video generation. MALT Diffusion (or\njust MALT) handles long videos by subdividing them into short segments and\ndoing segment-level autoregressive generation. To achieve this, we first\npropose recurrent attention layers that encode multiple segments into a compact\nmemory latent vector; by maintaining this memory vector over time, MALT is able\nto condition on it and continuously generate new footage based on a long\ntemporal context. We also present several training techniques that enable the\nmodel to generate frames over a long horizon with consistent quality and\nminimal degradation. We validate the effectiveness of MALT through experiments\non long video benchmarks. We first perform extensive analysis of MALT in\nlong-contextual understanding capability and stability using popular long video\nbenchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video\ngeneration on UCF-101, outperforming the previous state-of-the-art of 648.4.\nFinally, we explore MALT's capabilities in a text-to-video generation setting\nand show that it can produce long videos compared with recent techniques for\nlong text-to-video generation.",
        "When experimentally learning the action of a continuous variable quantum\nprocess by probing it with inputs, there will often be some restriction on the\ninput states used. One experimentally simple way to probe the channel is using\nlow-energy coherent states. Learning a quantum channel in this way presents\ndifficulties, due to the fact that two channels may act similarly on low energy\ninputs but very differently for high energy inputs. They may also act similarly\non coherent state inputs but differently on non-classical inputs. Extrapolating\nthe behaviour of a channel for more general input states from its action on the\nfar more limited set of low energy coherent states is a case of\nout-of-distribution generalisation. To be sure that such generalisation gives\nmeaningful results, one needs to relate error bounds for the training set to\nbounds that are valid for all inputs. We show that for any pair of channels\nthat act sufficiently similarly on low energy coherent state inputs, one can\nbound how different the input-output relations are for any (high energy or\nhighly non-classical) input. This proves out-of-distribution generalisation is\nalways possible for learning quantum channels using low energy coherent states.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single\/multiple agents,\nsingle\/multiple backdoors, discrete\/continuous action spaces, and sparse\/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps:\/\/github.com\/maoubo\/UNIDOOR.",
        "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
        "We extend our previous work on a novel class of unstable qubits which we have\nidentified recently and called them Critical Unstable Qubits (CUQs). The\ncharacteristic property of CUQs is that the energy-level and decay-width\nvectors, ${\\bf E}$ and ${\\bf \\Gamma}$, are orthogonal to one another, and the\nkey parameter $r = |{\\bf \\Gamma}|\/|2{\\bf E}|$ is less than 1. Most remarkably,\nCUQs exhibit two atypical behaviours: (i) they display coherence-decoherence\noscillations in a co-decaying frame of the system described by a unit Bloch\nvector ${\\bf b}$, and (ii) the unit Bloch vector ${\\bf b}$ describing a pure\nCUQ sweeps out unequal areas during equal intervals of time, while rotating\nabout the vector ${\\bf E}$. The latter anharmonic phenomenon emerges beyond the\nusual oscillatory pattern due to the energy-level difference of the two-level\nquantum system, which governs an ordinary qubit. By making use of a Fourier\nseries decomposition, we define anharmonicity observables that quantify the\ndegree of non-sinusoidal oscillation of a CUQ. We apply the results of our\nformalism to the $B^0\\bar{B}^0$-meson system and derive, for the first time,\ngeneric upper limits on these new observables.",
        "Mobile phone agents can assist people in automating daily tasks on their\nphones, which have emerged as a pivotal research spotlight. However, existing\nprocedure-oriented agents struggle with cross-app instructions, due to the\nfollowing challenges: (1) complex task relationships, (2) diverse app\nenvironment, and (3) error propagation and information loss in multi-step\nexecution. Drawing inspiration from object-oriented programming principles, we\nrecognize that object-oriented solutions is more suitable for cross-app\ninstruction. To address these challenges, we propose a self-evolving\nmulti-agent framework named MobileSteward, which integrates multiple\napp-oriented StaffAgents coordinated by a centralized StewardAgent. We design\nthree specialized modules in MobileSteward: (1) Dynamic Recruitment generates a\nscheduling graph guided by information flow to explicitly associate tasks among\napps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each\nequipped with app-specialized expertise to address the diversity between apps.\n(3) Adjusted Evaluation conducts evaluation to provide reflection tips or\ndeliver key information, which alleviates error propagation and information\nloss during multi-step execution. To continuously improve the performance of\nMobileSteward, we develop a Memory-based Self-evolution mechanism, which\nsummarizes the experience from successful execution, to improve the performance\nof MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench)\nin the real-world environment to evaluate the agents' capabilities of solving\ncomplex cross-app instructions. Experimental results demonstrate that\nMobileSteward achieves the best performance compared to both single-agent and\nmulti-agent frameworks, highlighting the superiority of MobileSteward in better\nhandling user instructions with diverse complexity.",
        "Matrix Product States (MPS) and Tensor Networks provide a general framework\nfor the construction of solvable models. The best-known example is the\nAffleck-Kennedy-Lieb-Tasaki (AKLT) model, which is the ground state of a 2-body\nnearest-neighbor parent Hamiltonian. We show that such simple parent\nHamiltonians for MPS models are, in fact, much more prevalent than hitherto\nknown: The existence of a single example with a simple Hamiltonian for a given\nchoice of dimensions already implies that any generic MPS with those dimensions\npossesses an equally simple Hamiltonian. We illustrate our finding by\ndiscussing a number of models with nearest-neighbor parent Hamiltonians, which\ngeneralize the AKLT model on various levels.",
        "Urban air mobility (UAM) is a transformative system that operates various\nsmall aerial vehicles in urban environments to reshape urban transportation.\nHowever, integrating UAM into existing urban environments presents a variety of\ncomplex challenges. Recent analyses of UAM's operational constraints highlight\naircraft noise and system safety as key hurdles to UAM system implementation.\nFuture UAM air traffic management schemes must ensure that the system is both\nquiet and safe. We propose a multi-agent reinforcement learning approach to\nmanage UAM traffic, aiming at both vertical separation assurance and noise\nmitigation. Through extensive training, the reinforcement learning agent learns\nto balance the two primary objectives by employing altitude adjustments in a\nmulti-layer UAM network. The results reveal the tradeoffs among noise impact,\ntraffic congestion, and separation. Overall, our findings demonstrate the\npotential of reinforcement learning in mitigating UAM's noise impact while\nmaintaining safe separation using altitude adjustments",
        "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.",
        "We introduce k-LLMmeans, a novel modification of the k-means clustering\nalgorithm that utilizes LLMs to generate textual summaries as cluster\ncentroids, thereby capturing contextual and semantic nuances often lost when\nrelying on purely numerical means of document embeddings. This modification\npreserves the properties of k-means while offering greater interpretability:\nthe cluster centroid is represented by an LLM-generated summary, whose\nembedding guides cluster assignments. We also propose a mini-batch variant,\nenabling efficient online clustering for streaming text data and providing\nreal-time interpretability of evolving cluster centroids. Through extensive\nsimulations, we show that our methods outperform vanilla k-means on multiple\nmetrics while incurring only modest LLM usage that does not scale with dataset\nsize. Finally, We present a case study showcasing the interpretability of\nevolving cluster centroids in sequential text streams. As part of our\nevaluation, we compile a new dataset from StackExchange, offering a benchmark\nfor text-stream clustering.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "We review some regularity results for the Laplacian and $p$-Laplacian in\nmetric measure spaces. The focus is mainly on interior H\\\"older, Lipschitz and\nsecond-regularity estimates and on spaces supporting a Poincar\\'e inequality or\nhaving Ricci curvature bounded below.",
        "Automated viewpoint classification in echocardiograms can help\nunder-resourced clinics and hospitals in providing faster diagnosis and\nscreening when expert technicians may not be available. We propose a novel\napproach towards echocardiographic viewpoint classification. We show that\ntreating viewpoint classification as video classification rather than image\nclassification yields advantage. We propose a CNN-GRU architecture with a novel\ntemporal feature weaving method, which leverages both spatial and temporal\ninformation to yield a 4.33\\% increase in accuracy over baseline image\nclassification while using only four consecutive frames. The proposed approach\nincurs minimal computational overhead. Additionally, we publish the Neonatal\nEchocardiogram Dataset (NED), a professionally-annotated dataset providing\nsixteen viewpoints and associated echocardipgraphy videos to encourage future\nwork and development in this field. Code available at:\nhttps:\/\/github.com\/satchelfrench\/NED"
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"A framework for evaluating the performance of SMLM cluster analysis algorithms",
    "start_abstract":"This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
      ],
      "abstract":[
        "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Progress of the anti-obesity of Berberine",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Silicon is the next frontier in plant synthetic biology",
        "An Evaluation on the Role of Non-Coding RNA in HIV Transcription and\n  Latency: A Review",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "A Comprehensive Review of Protein Language Models",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "Mechanism of Shape Symmetry Breaking in Surfactant Mediated Crystal\n  Growth",
        "A New Interpretation for the Hot Corona in Active Galactic Nuclei",
        "Dynamics of Quantum Correlations within the double Caldeira-Leggett\n  formalism",
        "Comparative Time-Series Analysis of Hip and Shoulder Rotation in\n  Baseball Bat Swings",
        "Nearly tight weighted 2-designs in complex projective spaces of every\n  dimension",
        "On the viability of higher order theories",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Exceptional-point-controlled mode interaction in three-dimensional\n  microcavities represented by generalized Husimi functions",
        "Proxy Control Barrier Functions: Integrating Barrier-Based and\n  Lyapunov-Based Safety-Critical Control Design",
        "New exact spatially localized solutions of the (3 + 1) -dimensional\n  nonlinear non-dissipative quasi-geostrophic potential vorticity equation for\n  an exponential atmosphere",
        "Hierarchies from deterministic non-locality in theory space Anderson\n  localisation",
        "Efficient detection of entanglement by stimulated disentanglement",
        "Fractional Sobolev spaces related to an ultraparabolic operator",
        "Geography of irreducible 4-manifolds with order two fundamental group",
        "Evaluation codes arising from symmetric polynomials"
      ],
      "abstract":[
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "Obesity is defined as the excessive accumulation or abnormal distribution of\nbody fat. According to data from World Obesity Atlas 2024, the increase in\nprevalence of obesity has become a major worldwide health problem in adults as\nwell as among children and adolescents. Although an increasing number of drugs\nhave been approved for the treatment of obesity in recent years, many of these\ndrugs have inevitable side effects which have increased the demand for new\nsafe, accessible and effective drugs for obesity and prompt interest in natural\nproducts. Berberine (BBR) and its metabolites, known for their multiple\npharmacological effects. Recent studies have emphatically highlighted the\nanti-obesity benefits of BBR and the underlying mechanisms have been gradually\nelucidated. However, its clinical application is limited by poor oral\nabsorption and low bioavailability. Based on this, this review summarizes\ncurrent research on the anti-obesity effects of BBR and its metabolites,\nincluding advancements in clinical trail results, understanding potential\nmolecular mechanisms and absorption and bioavailability. As a natural compound\nderived from plants, BBR holds potential as an alternative approach for\nmanaging obesity.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "The existence of latent cellular reservoirs is recognized as the major\nbarrier to an HIV cure. Reactivating and eliminating \"shock and kill\" or\npermanently silencing \"block and lock\" the latent HIV reservoir, as well as\ngene editing, remain promising approaches, but so far have proven to be only\npartially successful. Moreover, using latency reversing agents or \"block and\nlock\" drugs pose additional considerations, including the ability to cause\ncellular toxicity, a potential lack of specificity for HIV, or low potency when\neach agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long\nnon-coding RNAs (lncRNAs) are becoming increasingly recognized as important\nregulators of gene expression. RNA-based approaches for combatting HIV latency\nrepresent a promising strategy since both miRNAs and lncRNAs are more cell-type\nand tissue specific than protein coding genes. Thus, a higher specificity of\ntargeting the latent HIV reservoir with less overall cellular toxicity can\nlikely be achieved. In this review, we summarize current knowledge about HIV\ngene expression regulation by miRNAs and lncRNAs encoded in the human genome,\nas well as regulatory molecules encoded in the HIV genome. We discuss both the\ntranscriptional and post-transcriptional regulation of HIV gene expression to\nalign with the current definition of latency, and describe RNA molecules that\neither promote HIV latency or have anti-latency properties. Finally, we provide\nperspectives on using each class of RNAs as potential targets for combatting\nHIV latency, and describe the complexity of the interactions between different\nRNA molecules, their protein targets, and HIV.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "At the intersection of the rapidly growing biological data landscape and\nadvancements in Natural Language Processing (NLP), protein language models\n(PLMs) have emerged as a transformative force in modern research. These models\nhave achieved remarkable progress, highlighting the need for timely and\ncomprehensive overviews. However, much of the existing literature focuses\nnarrowly on specific domains, often missing a broader analysis of PLMs. This\nstudy provides a systematic review of PLMs from a macro perspective, covering\nkey historical milestones and current mainstream trends. We focus on the models\nthemselves and their evaluation metrics, exploring aspects such as model\narchitectures, positional encoding, scaling laws, and datasets. In the\nevaluation section, we discuss benchmarks and downstream applications. To\nfurther support ongoing research, we introduce relevant mainstream tools.\nLastly, we critically examine the key challenges and limitations in this\nrapidly evolving field.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "We present a dynamical model of crystal growth, in which it is possible to\nreliably achieve asymmetric products, beginning from symmetric initial\nconditions and growing within an isotropic environment. The asymmetric growth\nis the result of a positive feedback mechanism that amplifies the effect of\nthermal fluctuations in the coverage of surfactants on the growing crystalline\nfacets. Within our simple model, we are able to understand the kinetic and\nthermodynamic factors involved in both the onset of symmetry breaking and the\npersistence of anisotropic growth. We demonstrate that the mechanism is general\nby studying models with increasing complexity. We argue that this mechanism of\nsymmetry breaking underpins observations of colloidal, seed-mediated syntheses\nof single crystalline metal nanorods capped with strongly interacting\nsurfactants. The parameters within our model are related to experimental\nobservables such as the concentration, hydrophobicity, and binding strength of\nthe surfactants, which suggests a potential route to optimize the yield of\nasymmetric products in colloidal nanoparticle syntheses.",
        "This work attempts to provide a new interpretation for the hot corona in\nactive galactic nuclei (AGNs). A thin\n  parabolic magnetic reconnection layer, anchored at the innermost disk and\nextending along the boundary of the\n  magnetic tower for a few tens of gravitational radii, serves as a hard-X-ray\nsource above the disk. Within this\n  reconnection layer, the tearing instability leads to the formation of a chain\nof plasmoids, which contain relativistic\n  electrons that generate X-ray radiation through inverse-Compton (IC)\nscattering of soft photons emitted by the\n  accretion disk. Based on previous theoretical works and numerical\nsimulations, we develop a heuristic framework\n  to parameterize the geometry and magnetization of the reconnection layer, as\nwell as to compute both the power of\n  the IC-scattering radiation and the height of the reconnection layer. Our\nmodel allows for a quantitative\n  investigation of the relation between the height of the corona and the X-ray\nradiation luminosity, which can be\n  directly compared against the observed relation from X-ray reverberation\nmapping of individual AGNs. The\n  theoretical results are in good agreement with the observations of IRAS\n13224-3809, indicating the validation of\n  our model.",
        "This study investigates the effects of decoherence and squeezing on the\ndynamics of various kinds of quantum features--local quantum coherence, local\nentropy, EPR correlations, and entanglement--in the high-temperature limit of\nthe double Caldeira-Leggett model, focusing on initially squeezed states. We\ncompare two scenarios: (1) particles interacting with distinct environments and\n(2) particles coupled to a common environment. Our analysis reveals that common\nenvironments better preserve local coherence over time, whereas distinct\nenvironments accelerate decoherence. Temperature enhances decoherence and\nsuppresses coherence revivals, while squeezing affects transient dynamics but\nnot long-term coherence saturation. Local entropy increases with temperature\nand squeezing, though their underlying physical mechanisms differ. EPR\ncorrelations degrade due to environmental interactions, with squeezing\ninitially enhancing them but failing to prevent their eventual loss.\nEntanglement exhibits distinct behaviors: in separate environments, it\nundergoes sudden death, whereas in common environments, it experiences a dark\nperiod whose duration shortens with stronger squeezing. These findings provide\na comprehensive understanding of how decoherence and squeezing influence\nquantum correlations in open quantum systems.",
        "This study focuses on the rotation of the hips and shoulders during a\nbaseball bat swing, analyzing the time-series changes in rotational angles,\nrotational velocities, and axes using marker position data obtained from a\nmotion capture system with 12 infrared cameras. Previous studies have examined\nfactors such as ground reaction forces, muscle activation patterns, rotational\nenergy, angular velocity, and angles during a swing. However, to the best of\nour knowledge, the hip and shoulder rotational motions have not been adequately\nvisualized or compared. In particular, there is a lack of analysis regarding\nthe coordination and timing differences between hip and shoulder movements\nduring the swing. Therefore, this study aims to quantitatively compare the hip\nand shoulder rotational movements during the swing between skilled and\nunskilled players and visualizes the differences between them. Based on the\nobtained data, the study aims to improve the understanding of bat swing\nmechanics by visualizing the coordinated body movements during the swing.",
        "We use dense Sidon sets to construct small weighted projective 2-designs.\nThis represents quantitative progress on Zauner's conjecture.",
        "In physics, all dynamical equations that describe fundamental interactions\nare second order ordinary differential equations in the time derivatives. In\nthe literature, this property is traced back to a result obtained by\nOstrogradski in the mid 19th century, which is the technical basis of a 'no-go'\ntheorem for higher order theories. In this work, we review the connection of\nsymmetry properties with the order of dynamical equations, before reconsidering\nOstrogradski's result. Then, we show how Ostrogradski's conclusion is reached\nby applying to higher order theories concepts and method that have been\nspecifically developed for second order theories. We discuss a potential lack\nof consistency in this approach, to support the claim that Ostrogradski's\nresult applies to a class of higher order theories that is nowhere\nrepresentative of generic ones: we support this claim by giving an example of a\nhigher-order Lagrangian that is asymptotically stable, but that would be\nunstable under Ostrogradski's criterion. We also conclude that, when\nconsidering higher order theories as fundamental, we may need to reconsider and\nextend the conceptual framework on which our standard treatment of second order\ntheories is based.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Non-Hermitian photonics has attracted significant interest and influences\nseveral key areas such as optical metamaterials, laser physics, and nonlinear\noptics. While non-Hermitian effects have been widely addressed in\ntwo-dimensional systems, we focus on realistic three-dimensional devices. To\nthis end we generalize established phase space methods from mesoscopic optics\nand introduce Husimi functions for three-dimensional systems that deepen the\ninsight and access to the mode morphology and their dynamics. We illustrate\nthat four-dimensional Husimi functions can be represented using a specific\nprojection in two dimensions and illustrate it for (conical) cylindrical\ncavities. The non-Hermitian character of the intrinsically open photonic\nsystems is in particular revealed when examining the TE and TM polarization\ncharacter of the resonance modes. Unlike the 2D case, polarization is not\nconserved in three-dimensional cavities, and we use generalized Husimi function\nto represent the interaction of polarization modes. We find their dynamics to\nbe ruled by a network of exceptional points in the parameter space spanned by\nthe refractive index and the cavity geometry tilt angle. This approach not only\nenhances our understanding of cavity modes but also aids in the design of more\nefficient photonic devices and systems.",
        "This work introduces a novel Proxy Control Barrier Function (PCBF) scheme\nthat integrates barrier-based and Lyapunov-based safety-critical control\nstrategies for strict-feedback systems with potentially unknown dynamics. The\nproposed method employs a modular design procedure, decomposing the original\nsystem into a proxy subsystem and a virtual tracking subsystem that are\ncontrolled by the control barrier function (CBF)-based and Lyapunov-based\ncontrollers, respectively. By integrating these separately designed\ncontrollers, the overall system's safety is ensured. Moreover, a new\nfilter-based disturbance observer is utilized to design a PCBF-based safe\ncontroller for strict-feedback systems subject to mismatched disturbances. This\napproach broadens the class of systems to which CBF-based methods can be\napplied and significantly simplifies CBF construction by requiring only the\nmodel of the proxy subsystem. The effectiveness of the proposed method is\ndemonstrated through numerical simulations.",
        "New exact spatially localized stationary solutions against the background of\na zonal flow are found for the (3+1)-dimensional nonlinear non-dissipative\nquasi-geostrophic potential vorticity equation, which describes Rossby waves\nand vortices in an exponential atmosphere. In total, three solutions are\npresented. The nonlinear boundary conditions with a flat bottom and a rigid lid\ngenerate an infinite discrete set of baroclinic modes for each solution. The\nsolutions show the possibility of existence of baroclinic dipoles in the\nexponential atmosphere, similar to baroclinic dipoles in the ocean. It is shown\nthat: a) a pair of vortices in the baroclinic dipole appears and disappears\nwhen the velocity of stationary motion changes; b) the baroclinic dipoles show\nthe ability to transfer warm or cold air depending on the polarity of the\nvortices in the dipole.",
        "The nearest-neighbour or local mass terms in theory space among quantum\nfields, with their generic disordered values, are known to lead to the\nlocalisation of mass eigenstates, analogous to Anderson localisation in a\none-dimensional lattice. This mechanism can be used to create an exponential\nhierarchy in the coupling between two fields by placing them at opposite ends\nof the lattice chain. Extending this mechanism, we show that when copies of\nsuch fields are appropriately attached to the lattice chain, it leads to the\nemergence of multiple massless modes. These vanishing masses are a direct\nconsequence of the locality of interactions in theory space. The latter may\nbreak down in an ordered and deterministic manner through quantum effects if\nadditional interactions exist among the chain fields. Such non-locality can\ninduce small masses for the otherwise massless modes without necessarily\ndelocalising the mass eigenstates. We provide examples of interactions that\npreserve or even enhance localisation. Applications to flavour hierarchies,\nneutrino mass, and the $\\mu$-problem in supersymmetric theories are discussed.",
        "Standard detection of entanglement relies on local measurements of the\nindividual particles, evaluating their correlations in post-processing. For\ntime-energy entangled photons, either times $(t_{1},t_{2})$ or energies\n$(E_{1},E_{2})$ are measured, but not both due to the mutual quantum\nuncertainty, providing only partial information of the entanglement. In\nprinciple, a global detector could recover the complete information of\nentanglement in a single shot if it could measure the combined correlated\nvariables $(t_{1}-t_{2})$ and $(E_{1}+E_{2})$ without measuring the individual\nenergies or times. Such a global measurement is possible using the reverse\ndisentangling interaction, like sum-frequency generation (SFG), but nonlinear\ninteractions at the single-photon level are exceedingly inefficient. We\novercome this barrier by stimulating the nonlinear SFG interaction with a\nstrong pump, thereby measuring both the energy-sum (SFG spectrum) and the\ntime-difference (response to group delay\/dispersion) simultaneously and\nefficiently. We generate bi-photons with extreme time-energy entanglement\n(octave-spanning spectrum of 113THz) and measure a relative uncertainty between\ntime-difference and energy-sum of\n$\\Delta(t_1-t_2)\\Delta(E_1+E_2)\\approx\\!2\\!\\cdot\\!10^{-13}h$, violating the\nclassical bound by more than 12 orders of magnitude. The presented coherent SFG\ndramatically enhances the detection SNR compared to standard methods since it\nideally rejects erroneous coincidences in both time and energy, paving the way\nfor sensing applications, such as quantum illumination (radar) and more.",
        "We propose a functional framework of fractional Sobolev spaces for a class of\nultra-parabolic Kolmogorov type operators satisfying the weak H\\\"ormander\ncondition. We characterize these spaces as real interpolation of natural order\nintrinic Sobolev spaces recently introduced in [27], and prove continuous\nembeddings into $L^p$ and intrinsic H\\\"older spaces from [24]. These embeddings\nnaturally extend the standard Euclidean ones, coherently with the homogeneous\nstructure of the associated Kolmogorov group. Our approach to interpolation is\nbased on approximation of intrinsically regular functions, the latter heavily\nrelying on integral estimates of the intrinsic Taylor remainder. The embeddings\nexploit the aforementioned interpolation property and the corresponding\nembeddings of natural order intrinsic spaces.",
        "Let $R$ be a closed, oriented topological 4-manifold whose Euler\ncharacteristic and signature are denoted by $e$ and $\\sigma$. We show that if\n$R$ has order two $\\pi_1$, odd intersection form, and $2e + 3\\sigma \\geq 0$,\nthen for all but seven $(e, \\sigma)$ coordinates, $R$ admits an irreducible\nsmooth structure. We accomplish this by performing a variety of operations on\nirreducible simply-connected 4-manifolds to build 4-manifolds with order two\n$\\pi_1$. These techniques include torus surgeries, symplectic fiber sums,\nrational blow-downs, and numerous constructions of Lefschetz fibrations,\nincluding a new approach to equivariant fiber summing.",
        "Datta and Johnsen (Des. Codes and Cryptogr., {\\bf{91}} (2023), 747-761)\nintroduced a new family of evalutation codes in an affine space of dimension\n$\\ge 2$ over a finite field $\\mathbb{F}_q$ where linear combinations of\nelementary symmetric polynomials are evaluated on the set of all points with\npairwise distinct coordinates. In this paper, we propose a generalization by\ntaking low dimensional linear systems of symmetric polynomials. Computation for\nsmall values of $q=7,9$ shows that carefully chosen generalized Datta-Johnsen\ncodes $\\left[\\frac{1}{2}q(q-1),3,d\\right]$ have minimum distance $d$ equal to\nthe optimal value minus 1."
      ]
    }
  },
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "start_abstract":"It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products.",
    "start_categories":[
      "physics.gen-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Auto-Encoding Variational Bayes"
      ],
      "abstract":[
        "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Theorem Prover as a Judge for Synthetic Data Generation",
        "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!",
        "Empirical Evaluation of the Implicit Hitting Set Approach for Weighted\n  CSPs",
        "NS-Gym: Open-Source Simulation Environments and Benchmarks for\n  Non-Stationary Markov Decision Processes",
        "A3: Android Agent Arena for Mobile GUI Agents",
        "Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the\n  ARC Task",
        "Seeding for Success: Skill and Stochasticity in Tabletop Games",
        "The Internet of Large Language Models: An Orchestration Framework for\n  LLM Training and Knowledge Exchange Toward Artificial General Intelligence",
        "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol",
        "Knowledge is Power: Harnessing Large Language Models for Enhanced\n  Cognitive Diagnosis",
        "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning",
        "GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular\n  Representation and Diffusion Generation",
        "Intelligence Sequencing and the Path-Dependence of Intelligence\n  Evolution: AGI-First vs. DCI-First as Irreversible Attractors",
        "ALPET: Active Few-shot Learning for Citation Worthiness Detection in\n  Low-Resource Wikipedia Languages",
        "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity",
        "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource\n  Awareness",
        "D3PO: Preference-Based Alignment of Discrete Diffusion Models",
        "Pre-Equalization Aided Grant-Free Massive Access in Massive MIMO System",
        "MSV-Mamba: A Multiscale Vision Mamba Network for Echocardiography\n  Segmentation",
        "A stochastic programming approach for the scheduling of medical\n  interpreting service under uncertainty",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "A dynamical proof of Matui's absorption theorem",
        "Semantic Communications Services within Generalist Operated Networks",
        "Unified Approaches in Self-Supervised Event Stream Modeling: Progress\n  and Prospects",
        "A note on finiteness properties of vertex stabilisers",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "GameFactory: Creating New Games with Generative Interactive Videos",
        "Handling Uncertainty in Health Data using Generative Algorithms"
      ],
      "abstract":[
        "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
        "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https:\/\/github.com\/NovaSky-AI\/SkyThought.",
        "SAT technology has proven to be surprisingly effective in a large variety of\ndomains. However, for the Weighted CSP problem dedicated algorithms have always\nbeen superior. One approach not well-studied so far is the use of SAT in\nconjunction with the Implicit Hitting Set approach. In this work, we explore\nsome alternatives to the existing algorithm of reference. The alternatives,\nmostly borrowed from related boolean frameworks, consider trade-offs for the\ntwo main components of the IHS approach: the computation of low-cost hitting\nvectors, and their transformation into high-cost cores. For each one, we\npropose 4 levels of intensity. Since we also test the usefulness of cost\nfunction merging, our experiments consider 32 different implementations. Our\nempirical study shows that for WCSP it is not easy to identify the best\nalternative. Nevertheless, the cost-function merging encoding and extracting\nmaximal cores seems to be a robust approach.",
        "In many real-world applications, agents must make sequential decisions in\nenvironments where conditions are subject to change due to various exogenous\nfactors. These non-stationary environments pose significant challenges to\ntraditional decision-making models, which typically assume stationary dynamics.\nNon-stationary Markov decision processes (NS-MDPs) offer a framework to model\nand solve decision problems under such changing conditions. However, the lack\nof standardized benchmarks and simulation tools has hindered systematic\nevaluation and advance in this field. We present NS-Gym, the first simulation\ntoolkit designed explicitly for NS-MDPs, integrated within the popular\nGymnasium framework. In NS-Gym, we segregate the evolution of the environmental\nparameters that characterize non-stationarity from the agent's decision-making\nmodule, allowing for modular and flexible adaptations to dynamic environments.\nWe review prior work in this domain and present a toolkit encapsulating key\nproblem characteristics and types in NS-MDPs. This toolkit is the first effort\nto develop a set of standardized interfaces and benchmark problems to enable\nconsistent and reproducible evaluation of algorithms under non-stationary\nconditions. We also benchmark six algorithmic approaches from prior work on\nNS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to\nassess the adaptability and robustness of their decision-making algorithms to\nnon-stationary conditions.",
        "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at https:\/\/yuxiangchai.github.io\/Android-Agent-Arena\/.",
        "While LLMs have exhibited strong performance on various NLP tasks, it is\nnoteworthy that most of these tasks rely on utilizing the vast amount of\nknowledge encoded in LLMs' parameters, rather than solving new problems without\nprior knowledge. In cognitive research, the latter ability is referred to as\nfluid intelligence, which is considered to be critical for assessing human\nintelligence. Recent research on fluid intelligence assessments has highlighted\nsignificant deficiencies in LLMs' abilities. In this paper, we analyze the\nchallenges LLMs face in demonstrating fluid intelligence through controlled\nexperiments, using the most representative ARC task as an example. Our study\nrevealed three major limitations in existing LLMs: limited ability for skill\ncomposition, unfamiliarity with abstract input formats, and the intrinsic\ndeficiency of left-to-right decoding. Our data and code can be found in\nhttps:\/\/wujunjie1998.github.io\/araoc-benchmark.github.io\/.",
        "Games often incorporate random elements in the form of dice or shuffled card\ndecks. This randomness is a key contributor to the player experience and the\nvariety of game situations encountered. There is a tension between a level of\nrandomness that makes the game interesting and contributes to the player\nenjoyment of a game, and a level at which the outcome itself is effectively\nrandom and the game becomes dull. The optimal level for a game will depend on\nthe design goals and target audience. We introduce a new technique to quantify\nthe level of randomness in game outcome and use it to compare 15 tabletop games\nand disentangle the different contributions to the overall randomness from\nspecific parts of some games. We further explore the interaction between game\nrandomness and player skill, and how this innate randomness can affect error\nanalysis in common game experiments.",
        "This paper explores the multi-dimensional challenges faced during the\ndevelopment of Large Language Models (LLMs), including the massive scale of\nmodel parameters and file sizes, the complexity of development environment\nconfiguration, the singularity of model functionality, and the high costs of\ncomputational resources. To address these challenges, this paper proposes three\ncore technical solutions: LLM sharing protocol, LLM universal environment\nframework, and Agent optimal path module. To solve the computational resource\nconstraints in the early stages of research, we further innovatively propose a\njoint mining mechanism, achieving bilateral value sharing between computing\npower providers and model designers, including breakthrough rewards for optimal\nmodel paths and long-term profit distribution, thereby providing researchers\nwith cost-optimized computational resource support and promoting the continuous\ndevelopment of LLM research and applications.",
        "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings.",
        "Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive\nstates by analyzing their performance across a series of exercises. However,\nexisting CDMs often struggle with diagnosing infrequent students and exercises\ndue to a lack of rich prior knowledge. With the advancement in large language\nmodels (LLMs), which possess extensive domain knowledge, their integration into\ncognitive diagnosis presents a promising opportunity. Despite this potential,\nintegrating LLMs with CDMs poses significant challenges. LLMs are not\nwell-suited for capturing the fine-grained collaborative interactions between\nstudents and exercises, and the disparity between the semantic space of LLMs\nand the behavioral space of CDMs hinders effective integration. To address\nthese issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD)\nframework, which is a model-agnostic framework utilizing LLMs to enhance CDMs\nand compatible with various CDM architectures. The KCD framework operates in\ntwo stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis\nstage, both students and exercises are diagnosed to achieve comprehensive and\ndetailed modeling. In the Cognitive Level Alignment stage, we bridge the gap\nbetween the CDMs' behavioral space and the LLMs' semantic space using\ncontrastive learning and mask-reconstruction approaches. Experiments on several\nreal-world datasets demonstrate the effectiveness of our proposed framework.",
        "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.",
        "Retrosynthesis prediction focuses on identifying reactants capable of\nsynthesizing a target product. Typically, the retrosynthesis prediction\ninvolves two phases: Reaction Center Identification and Reactant Generation.\nHowever, we argue that most existing methods suffer from two limitations in the\ntwo phases: (i) Existing models do not adequately capture the ``face''\ninformation in molecular graphs for the reaction center identification. (ii)\nCurrent approaches for the reactant generation predominantly use sequence\ngeneration in a 2D space, which lacks versatility in generating reasonable\ndistributions for completed reactive groups and overlooks molecules' inherent\n3D properties. To overcome the above limitations, we propose GDiffRetro. For\nthe reaction center identification, GDiffRetro uniquely integrates the original\ngraph with its corresponding dual graph to represent molecular structures,\nwhich helps guide the model to focus more on the faces in the graph. For the\nreactant generation, GDiffRetro employs a conditional diffusion model in 3D to\nfurther transform the obtained synthon into a complete reactant. Our\nexperimental findings reveal that GDiffRetro outperforms state-of-the-art\nsemi-template models across various evaluative metrics.",
        "The trajectory of intelligence evolution is often framed around the emergence\nof artificial general intelligence (AGI) and its alignment with human values.\nThis paper challenges that framing by introducing the concept of intelligence\nsequencing: the idea that the order in which AGI and decentralized collective\nintelligence (DCI) emerge determines the long-term attractor basin of\nintelligence. Using insights from dynamical systems, evolutionary game theory,\nand network models, it argues that intelligence follows a path-dependent,\nirreversible trajectory. Once development enters a centralized (AGI-first) or\ndecentralized (DCI-first) regime, transitions become structurally infeasible\ndue to feedback loops and resource lock-in. Intelligence attractors are modeled\nin functional state space as the co-navigation of conceptual and adaptive\nfitness spaces. Early-phase structuring constrains later dynamics, much like\nrenormalization in physics. This has major implications for AI safety:\ntraditional alignment assumes AGI will emerge and must be controlled after the\nfact, but this paper argues that intelligence sequencing is more foundational.\nIf AGI-first architectures dominate before DCI reaches critical mass,\nhierarchical monopolization and existential risk become locked in. If DCI-first\nemerges, intelligence stabilizes around decentralized cooperative equilibrium.\nThe paper further explores whether intelligence structurally biases itself\ntoward an attractor based on its self-modeling method -- externally imposed\naxioms (favoring AGI) vs. recursive internal visualization (favoring DCI).\nFinally, it proposes methods to test this theory via simulations, historical\nlock-in case studies, and intelligence network analysis. The findings suggest\nthat intelligence sequencing is a civilizational tipping point: determining\nwhether the future is shaped by unbounded competition or unbounded cooperation.",
        "Citation Worthiness Detection (CWD) consists in determining which sentences,\nwithin an article or collection, should be backed up with a citation to\nvalidate the information it provides. This study, introduces ALPET, a framework\ncombining Active Learning (AL) and Pattern-Exploiting Training (PET), to\nenhance CWD for languages with limited data resources. Applied to Catalan,\nBasque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW\nbaseline while reducing the amount of labeled data in some cases above 80\\%.\nALPET's performance plateaus after 300 labeled samples, showing it suitability\nfor low-resource scenarios where large, labeled datasets are not common. While\nspecific active learning query strategies, like those employing K-Means\nclustering, can offer advantages, their effectiveness is not universal and\noften yields marginal gains over random sampling, particularly with smaller\ndatasets. This suggests that random sampling, despite its simplicity, remains a\nstrong baseline for CWD in constraint resource environments. Overall, ALPET's\nability to achieve high performance with fewer labeled samples makes it a\npromising tool for enhancing the verifiability of online content in\nlow-resource language settings.",
        "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.",
        "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
        "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D3PO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D3PO on a structured binary sequence generation task,\ndemonstrating that the method effectively aligns model outputs with preferences\nwhile maintaining structural validity. Our results highlight that D3PO enables\ncontrolled fine-tuning without requiring explicit reward models, making it a\npractical alternative to reinforcement learning-based approaches. Future\nresearch will explore extending D3PO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
        "The spatial diversity and multiplexing advantages of massive\nmulti-input-multi-output (mMIMO) can significantly improve the capacity of\nmassive non-orthogonal multiple access (NOMA) in machine type communications.\nHowever, state-of-the-art grant-free massive NOMA schemes for mMIMO systems\nrequire accurate estimation of random access channels to perform activity\ndetection and the following coherent data demodulation, which suffers from\nexcessive pilot overhead and access latency. To address this, we propose a\npre-equalization aided grant-free massive access scheme for mMIMO systems,\nwhere an iterative detection scheme is conceived. Specifically, the base\nstation (BS) firstly activates one of its antennas (i.e., beacon antenna) to\nbroadcast a beacon signal, which facilitates the user equipment (UEs) to\nperform downlink channel estimation and pre-equalize the uplink random access\nsignal with respect to the channels associated with the beacon antenna. During\nthe uplink transmission stage, the BS detects UEs' activity and data by using\nthe proposed iterative detection algorithm, which consists of three modules:\ncoarse data detection (DD), data-aided channel estimation (CE), and fine DD. In\nthe proposed algorithm, the joint activity and DD is firstly performed based on\nthe signals received by the beacon antenna. Subsequently, the DD is further\nrefined by iteratively performing data-aided CE module and fine DD module using\nsignals received by all BS antennas. Our simulation results demonstrate that\nthe proposed scheme outperforms state-of-the-art mMIMO-based grant-free massive\nNOMA schemes with the same access latency. Simulation codes are provided to\nreproduce the results in this article: https:\/\/github.com\/owenwang517\/tvt-2025.",
        "Ultrasound imaging frequently encounters challenges, such as those related to\nelevated noise levels, diminished spatiotemporal resolution, and the complexity\nof anatomical structures. These factors significantly hinder the model's\nability to accurately capture and analyze structural relationships and dynamic\npatterns across various regions of the heart. Mamba, an emerging model, is one\nof the most cutting-edge approaches that is widely applied to diverse vision\nand language tasks. To this end, this paper introduces a U-shaped deep learning\nmodel incorporating a large-window Mamba scale (LMS) module and a hierarchical\nfeature fusion approach for echocardiographic segmentation. First, a cascaded\nresidual block serves as an encoder and is employed to incrementally extract\nmultiscale detailed features. Second, a large-window multiscale mamba module is\nintegrated into the decoder to capture global dependencies across regions and\nenhance the segmentation capability for complex anatomical structures.\nFurthermore, our model introduces auxiliary losses at each decoder layer and\nemploys a dual attention mechanism to fuse multilayer features both spatially\nand across channels. This approach enhances segmentation performance and\naccuracy in delineating complex anatomical structures. Finally, the\nexperimental results using the EchoNet-Dynamic and CAMUS datasets demonstrate\nthat the model outperforms other methods in terms of both accuracy and\nrobustness. For the segmentation of the left ventricular endocardium\n(${LV}_{endo}$), the model achieved optimal values of 95.01 and 93.36,\nrespectively, while for the left ventricular epicardium (${LV}_{epi}$), values\nof 87.35 and 87.80, respectively, were achieved. This represents an improvement\nranging between 0.54 and 1.11 compared with the best-performing model.",
        "Limited English Proficiency (LEP) patients face higher risks of adverse\nhealth outcomes due to communication barriers, making timely medical\ninterpreting services essential for mitigating those risks. This paper\naddresses the scheduling of medical interpreting services under uncertainty.\nThe problem is formulated as a two-stage stochastic programming model that\naccounts for uncertainties in emergency patients' arrival and service time. The\nmodel handles the hiring decisions of part-time interpreters and the assignment\nof full-time and hired part-time interpreters. The objective is to minimize the\ntotal cost, which encompasses full-time interpreters' overtime cost, the fixed\nand variable costs of part-time interpreters, and the penalty cost for not\nserving LEP patients on time. The model is solved using the Sample Average\nApproximation (SAA) algorithm. To overcome the computational burden of the SAA\nalgorithm, a Tabu Search (TS) algorithm was used to solve the model. A\nreal-life case study is used to validate and evaluate the proposed solution\nalgorithms. The results demonstrate the effectiveness of the proposed\nstochastic programming-based solutions in concurrently reducing both the total\ncost and the waiting time. Further, sensitivity analysis reveals how the\nincrease in some key parameters, such as the arrival rate of emergency patients\nwith LEP, impacts scheduling outcomes.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "We give a dynamical, relatively elementary proof of an \"absorption theorem\"\nwhich is closely related to a well-known result due to Matui. The construction\nis in the spirit of an earlier joint work of the author and S. Robert. In an\nappendix we explain how to use this result to correct the dynamical proof given\nby Melleray--Robert of a classification theorem for orbit equivalence of\nminimal ample groups due to Giordano, Putnam and Skau (the original argument\nhad a gap).",
        "This paper addresses the challenge of integrating semantic communication\nprinciples into operated networks, traditionally optimized based on\nnetwork-centric metrics rather than application-specific needs. Operated\nnetworks strongly adhere to the principle of ``separation of concerns\", which\nemphasizes a clear distinction between network operation and application.\nDespite the initial perceived incompatibility between semantic communication\nand the principles of operated networks, this paper provides solutions to\nreconcile them. The foundations of these solutions include the adoption of\nnon-arbitrary semantic representations as a standard encoding for\ncommunications, the establishment of a standard interface between the\napplication and network, and a dedicated network control plane. These enable\nthe application to describe the data typology and the nature of the task, and\nto agree upon a transmission scheme tailored to the supported task. Through\nthree scenarios involving an application transmitting text representations, we\nillustrate the implementation of the proposal and demonstrate the potential of\nthe approach.",
        "The proliferation of digital interactions across diverse domains, such as\nhealthcare, e-commerce, gaming, and finance, has resulted in the generation of\nvast volumes of event stream (ES) data. ES data comprises continuous sequences\nof timestamped events that encapsulate detailed contextual information relevant\nto each domain. While ES data holds significant potential for extracting\nactionable insights and enhancing decision-making, its effective utilization is\nhindered by challenges such as the scarcity of labeled data and the fragmented\nnature of existing research efforts. Self-Supervised Learning (SSL) has emerged\nas a promising paradigm to address these challenges by enabling the extraction\nof meaningful representations from unlabeled ES data. In this survey, we\nsystematically review and synthesize SSL methodologies tailored for ES modeling\nacross multiple domains, bridging the gaps between domain-specific approaches\nthat have traditionally operated in isolation. We present a comprehensive\ntaxonomy of SSL techniques, encompassing both predictive and contrastive\nparadigms, and analyze their applicability and effectiveness within different\napplication contexts. Furthermore, we identify critical gaps in current\nresearch and propose a future research agenda aimed at developing scalable,\ndomain-agnostic SSL frameworks for ES modeling. By unifying disparate research\nefforts and highlighting cross-domain synergies, this survey aims to accelerate\ninnovation, improve reproducibility, and expand the applicability of SSL to\ndiverse real-world ES challenges.",
        "We prove a criterion for the geometric and algebraic finiteness properties of\nvertex stabilisers of $G$-CW-complexes, given the finiteness properties of the\ngroup $G$ and of the stabilisers of positive dimensional cells. This\ngeneralises a result of Haglund--Wise for groups acting on trees to higher\ndimensions. As an application, for $n\\ge 2$, we deduce the existence of\nuncountably many quasi-isometry classes of one-ended groups that are of type\n$\\mathsf{FP}_n$ and not of type $\\mathsf{FP}_{n+1}$.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "Generative game engines have the potential to revolutionize game development\nby autonomously creating new content and reducing manual workload. However,\nexisting video-based game generation methods fail to address the critical\nchallenge of scene generalization, limiting their applicability to existing\ngames with fixed styles and scenes. In this paper, we present GameFactory, a\nframework focused on exploring scene generalization in game video generation.\nTo enable the creation of entirely new and diverse games, we leverage\npre-trained video diffusion models trained on open-domain video data. To bridge\nthe domain gap between open-domain priors and small-scale game dataset, we\npropose a multi-phase training strategy that decouples game style learning from\naction control, preserving open-domain generalization while achieving action\ncontrollability. Using Minecraft as our data source, we release GF-Minecraft, a\nhigh-quality and diversity action-annotated video dataset for research.\nFurthermore, we extend our framework to enable autoregressive\naction-controllable game video generation, allowing the production of\nunlimited-length interactive game videos. Experimental results demonstrate that\nGameFactory effectively generates open-domain, diverse, and action-controllable\ngame videos, representing a significant step forward in AI-driven game\ngeneration. Our dataset and project page are publicly available at\n\\url{https:\/\/vvictoryuki.github.io\/gamefactory\/}.",
        "Understanding and managing uncertainty is crucial in machine learning,\nespecially in high-stakes domains like healthcare, where class imbalance can\nimpact predictions. This paper introduces RIGA, a novel pipeline that mitigates\nclass imbalance using generative AI. By converting tabular healthcare data into\nimages, RIGA leverages models like cGAN, VQVAE, and VQGAN to generate balanced\nsamples, improving classification performance. These representations are\nprocessed by CNNs and later transformed back into tabular format for seamless\nintegration. This approach enhances traditional classifiers like XGBoost,\nimproves Bayesian structure learning, and strengthens ML model robustness by\ngenerating realistic synthetic data for underrepresented classes."
      ]
    }
  }
]